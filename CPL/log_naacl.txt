#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 2 0 0 1 0 0 0]
Losses:  18.15954822860658 1.4161680936813354 1.5775365829467773 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 18.1595482Losses:  19.484691370278597 1.9326664209365845 1.9884347915649414 5.833449114114046
CurrentTrain: epoch  0, batch     1 | loss: 19.4846914Losses:  17.746089674532413 1.6278927326202393 1.7181609869003296 4.395899511873722
CurrentTrain: epoch  0, batch     2 | loss: 17.7460897Losses:  18.17402466572821 1.8575948476791382 1.9360754489898682 5.152892196550965
CurrentTrain: epoch  0, batch     3 | loss: 18.1740247Losses:  16.067860882729292 1.6802583932876587 1.7847683429718018 2.502943318337202
CurrentTrain: epoch  0, batch     4 | loss: 16.0678609Losses:  16.373739317059517 1.5590217113494873 1.6780784130096436 4.398820951581001
CurrentTrain: epoch  0, batch     5 | loss: 16.3737393Losses:  21.417053814977407 1.7519546747207642 1.7090275287628174 8.27966558560729
CurrentTrain: epoch  0, batch     6 | loss: 21.4170538Losses:  17.409098625183105 1.8425514698028564 1.9120049476623535 3.8551626205444336
CurrentTrain: epoch  0, batch     7 | loss: 17.4090986Losses:  24.183430083096027 1.7097153663635254 1.6211116313934326 10.513400442898273
CurrentTrain: epoch  0, batch     8 | loss: 24.1834301Losses:  15.300930052995682 1.6746113300323486 1.7610507011413574 2.892889052629471
CurrentTrain: epoch  0, batch     9 | loss: 15.3009301Losses:  16.401279281824827 1.6205899715423584 1.7688238620758057 3.8637264482676983
CurrentTrain: epoch  0, batch    10 | loss: 16.4012793Losses:  15.91254884749651 1.50306236743927 1.3817121982574463 3.0614011958241463
CurrentTrain: epoch  0, batch    11 | loss: 15.9125488Losses:  13.59333610534668 1.711456298828125 1.7675395011901855 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 13.5933361Losses:  13.43955135345459 1.7717539072036743 1.7012113332748413 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 13.4395514Losses:  18.627808466553688 1.5759872198104858 1.6004878282546997 5.3342303186655045
CurrentTrain: epoch  0, batch    14 | loss: 18.6278085Losses:  17.292185144498944 1.6054942607879639 1.490584373474121 4.989485101774335
CurrentTrain: epoch  0, batch    15 | loss: 17.2921851Losses:  16.486608006060123 1.709338903427124 1.6652803421020508 4.024260975420475
CurrentTrain: epoch  0, batch    16 | loss: 16.4866080Losses:  16.336933612823486 1.5661613941192627 1.5338608026504517 3.7753710746765137
CurrentTrain: epoch  0, batch    17 | loss: 16.3369336Losses:  18.004900321364403 1.624449610710144 1.3738322257995605 5.059612616896629
CurrentTrain: epoch  0, batch    18 | loss: 18.0049003Losses:  13.843977510929108 1.4918882846832275 1.4096100330352783 1.4325156807899475
CurrentTrain: epoch  0, batch    19 | loss: 13.8439775Losses:  15.994688633829355 1.7919704914093018 1.6394981145858765 4.265522602945566
CurrentTrain: epoch  0, batch    20 | loss: 15.9946886Losses:  16.536829801276326 1.499699354171753 1.2819501161575317 5.050193639472127
CurrentTrain: epoch  0, batch    21 | loss: 16.5368298Losses:  21.01198612526059 1.6680575609207153 1.641351342201233 8.70601974800229
CurrentTrain: epoch  0, batch    22 | loss: 21.0119861Losses:  14.217634048312902 1.6592223644256592 1.5673494338989258 1.4388559721410275
CurrentTrain: epoch  0, batch    23 | loss: 14.2176340Losses:  17.979508712887764 1.5585293769836426 1.6403249502182007 5.698576286435127
CurrentTrain: epoch  0, batch    24 | loss: 17.9795087Losses:  12.548966407775879 1.665771245956421 1.6013047695159912 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 12.5489664Losses:  12.006401062011719 1.648781180381775 1.422465443611145 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 12.0064011Losses:  17.04648069292307 1.6014254093170166 1.462891697883606 4.807432688772678
CurrentTrain: epoch  0, batch    27 | loss: 17.0464807Losses:  16.341935677453876 1.6391215324401855 1.436078667640686 4.636672539636493
CurrentTrain: epoch  0, batch    28 | loss: 16.3419357Losses:  12.218143463134766 1.7436859607696533 1.5153002738952637 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 12.2181435Losses:  10.89187240600586 1.552618384361267 1.465986728668213 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 10.8918724Losses:  15.97052613645792 1.553410291671753 1.3813495635986328 5.202720083296299
CurrentTrain: epoch  0, batch    31 | loss: 15.9705261Losses:  11.119872093200684 1.4256870746612549 1.3339312076568604 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 11.1198721Losses:  11.73359414935112 1.4683688879013062 1.4491770267486572 1.4029552638530731
CurrentTrain: epoch  0, batch    33 | loss: 11.7335941Losses:  15.206649385392666 1.6343563795089722 1.4023984670639038 3.2776532992720604
CurrentTrain: epoch  0, batch    34 | loss: 15.2066494Losses:  15.473522890359163 1.7323883771896362 1.4697784185409546 3.349485147744417
CurrentTrain: epoch  0, batch    35 | loss: 15.4735229Losses:  13.093660987913609 1.417106032371521 1.387856125831604 1.8154961243271828
CurrentTrain: epoch  0, batch    36 | loss: 13.0936610Losses:  12.90829935669899 1.6495040655136108 1.443812608718872 1.4250954687595367
CurrentTrain: epoch  0, batch    37 | loss: 12.9082994Losses:  11.100032806396484 1.7078195810317993 1.5114339590072632 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 11.1000328Losses:  17.10400678217411 1.6665810346603394 1.193182349205017 5.078615203499794
CurrentTrain: epoch  0, batch    39 | loss: 17.1040068Losses:  18.837386578321457 1.642833948135376 1.4650497436523438 7.347438305616379
CurrentTrain: epoch  0, batch    40 | loss: 18.8373866Losses:  17.845928110182285 1.5646183490753174 1.289030909538269 5.714345850050449
CurrentTrain: epoch  0, batch    41 | loss: 17.8459281Losses:  15.510177414864302 1.5422024726867676 1.5349867343902588 3.704284470528364
CurrentTrain: epoch  0, batch    42 | loss: 15.5101774Losses:  17.458369759842753 1.4340965747833252 1.3965799808502197 6.516099480912089
CurrentTrain: epoch  0, batch    43 | loss: 17.4583698Losses:  13.045042663812637 1.6438661813735962 1.2985211610794067 1.785264641046524
CurrentTrain: epoch  0, batch    44 | loss: 13.0450427Losses:  10.652173042297363 1.4845678806304932 1.3429380655288696 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 10.6521730Losses:  16.271216206252575 1.5068436861038208 1.2673712968826294 4.9434469267725945
CurrentTrain: epoch  0, batch    46 | loss: 16.2712162Losses:  16.562947548925877 1.5575740337371826 1.3622132539749146 4.973884858191013
CurrentTrain: epoch  0, batch    47 | loss: 16.5629475Losses:  13.923504462465644 1.5909850597381592 1.5536341667175293 2.2476984169334173
CurrentTrain: epoch  0, batch    48 | loss: 13.9235045Losses:  13.075267739593983 1.4705572128295898 1.2424943447113037 1.5092172101140022
CurrentTrain: epoch  0, batch    49 | loss: 13.0752677Losses:  20.303265511989594 1.738760232925415 1.5059669017791748 8.533011376857758
CurrentTrain: epoch  0, batch    50 | loss: 20.3032655Losses:  14.63608656078577 1.4918162822723389 1.348388671875 4.667176343500614
CurrentTrain: epoch  0, batch    51 | loss: 14.6360866Losses:  12.171684265136719 1.706247329711914 1.420906901359558 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 12.1716843Losses:  16.375279162544757 1.3953512907028198 1.277927041053772 5.378041957039386
CurrentTrain: epoch  0, batch    53 | loss: 16.3752792Losses:  17.431077875196934 1.4447021484375 1.239619493484497 6.959647096693516
CurrentTrain: epoch  0, batch    54 | loss: 17.4310779Losses:  12.087029870599508 1.6957669258117676 1.415172815322876 1.5362676940858364
CurrentTrain: epoch  0, batch    55 | loss: 12.0870299Losses:  23.01578824222088 1.3518648147583008 1.1688369512557983 12.0328618735075
CurrentTrain: epoch  0, batch    56 | loss: 23.0157882Losses:  13.341715924441814 1.4430656433105469 1.408735990524292 2.998951070010662
CurrentTrain: epoch  0, batch    57 | loss: 13.3417159Losses:  14.824894897639751 1.6773730516433716 1.5743601322174072 3.7108631059527397
CurrentTrain: epoch  0, batch    58 | loss: 14.8248949Losses:  12.768366415053606 1.7295441627502441 1.3644442558288574 1.528040487319231
CurrentTrain: epoch  0, batch    59 | loss: 12.7683664Losses:  26.171674966812134 1.4311437606811523 1.4099702835083008 15.970665216445923
CurrentTrain: epoch  0, batch    60 | loss: 26.1716750Losses:  14.73749104514718 1.6982402801513672 1.1981457471847534 4.656868372112513
CurrentTrain: epoch  0, batch    61 | loss: 14.7374910Losses:  12.623944699764252 1.3354575634002686 1.4867825508117676 1.5878176093101501
CurrentTrain: epoch  0, batch    62 | loss: 12.6239447Losses:  11.159332275390625 1.6847130060195923 1.2948986291885376 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 11.1593323Losses:  14.561788302380592 1.5535579919815063 1.4874156713485718 4.132759791333228
CurrentTrain: epoch  1, batch     1 | loss: 14.5617883Losses:  16.687690433114767 1.518570065498352 1.2129327058792114 6.650329288095236
CurrentTrain: epoch  1, batch     2 | loss: 16.6876904Losses:  11.895420037209988 1.4238300323486328 1.436851978302002 1.4668845757842064
CurrentTrain: epoch  1, batch     3 | loss: 11.8954200Losses:  12.977788049727678 1.6284531354904175 1.5301356315612793 1.8232584781944752
CurrentTrain: epoch  1, batch     4 | loss: 12.9777880Losses:  11.046381413936615 1.556575059890747 1.3890633583068848 1.402643620967865
CurrentTrain: epoch  1, batch     5 | loss: 11.0463814Losses:  10.653894424438477 1.4740676879882812 1.334348440170288 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 10.6538944Losses:  12.214360665529966 1.381484031677246 1.240364670753479 1.7557386867702007
CurrentTrain: epoch  1, batch     7 | loss: 12.2143607Losses:  11.856700256466866 1.599068522453308 1.3203052282333374 1.5684150010347366
CurrentTrain: epoch  1, batch     8 | loss: 11.8567003Losses:  16.47071409225464 1.5604560375213623 1.3439247608184814 6.545516490936279
CurrentTrain: epoch  1, batch     9 | loss: 16.4707141Losses:  16.113270975649357 1.492995262145996 1.1392157077789307 5.221313692629337
CurrentTrain: epoch  1, batch    10 | loss: 16.1132710Losses:  13.60924682021141 1.4987449645996094 1.491312861442566 3.200342744588852
CurrentTrain: epoch  1, batch    11 | loss: 13.6092468Losses:  12.829827725887299 1.391371250152588 1.1983115673065186 2.8486656546592712
CurrentTrain: epoch  1, batch    12 | loss: 12.8298277Losses:  9.923561096191406 1.4679222106933594 1.4007989168167114 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 9.9235611Losses:  14.49601836130023 1.4816890954971313 1.1217955350875854 3.6392364017665386
CurrentTrain: epoch  1, batch    14 | loss: 14.4960184Losses:  10.465740203857422 1.597839593887329 1.2882767915725708 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 10.4657402Losses:  14.23041295260191 1.433135747909546 1.3684732913970947 3.1094994470477104
CurrentTrain: epoch  1, batch    16 | loss: 14.2304130Losses:  15.279668595641851 1.2987664937973022 1.1769676208496094 4.441113259643316
CurrentTrain: epoch  1, batch    17 | loss: 15.2796686Losses:  16.021261455491185 1.4669995307922363 1.317780613899231 6.084716083481908
CurrentTrain: epoch  1, batch    18 | loss: 16.0212615Losses:  10.278553009033203 1.4844709634780884 1.3134623765945435 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 10.2785530Losses:  9.952409744262695 1.280739426612854 1.3466687202453613 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 9.9524097Losses:  20.746833480894566 1.4485039710998535 1.2849804162979126 9.857946075499058
CurrentTrain: epoch  1, batch    21 | loss: 20.7468335Losses:  10.382868766784668 1.3546077013015747 1.3301500082015991 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 10.3828688Losses:  9.870052337646484 1.5920393466949463 1.3491289615631104 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 9.8700523Losses:  13.237141475081444 1.4611127376556396 1.3177695274353027 4.374333247542381
CurrentTrain: epoch  1, batch    24 | loss: 13.2371415Losses:  11.900915890932083 1.5170106887817383 1.329482078552246 2.8882425129413605
CurrentTrain: epoch  1, batch    25 | loss: 11.9009159Losses:  10.453478813171387 1.3346680402755737 1.237174153327942 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 10.4534788Losses:  9.957710266113281 1.438193917274475 1.2285581827163696 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 9.9577103Losses:  10.352690696716309 1.4791969060897827 1.3181698322296143 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 10.3526907Losses:  14.228682585060596 1.3761003017425537 1.369565486907959 4.332463331520557
CurrentTrain: epoch  1, batch    29 | loss: 14.2286826Losses:  14.12350234016776 1.5028513669967651 1.181463599205017 3.6353517435491085
CurrentTrain: epoch  1, batch    30 | loss: 14.1235023Losses:  13.859168868511915 1.408476710319519 1.3523966073989868 3.262240272015333
CurrentTrain: epoch  1, batch    31 | loss: 13.8591689Losses:  14.25030280649662 1.4719618558883667 1.2743717432022095 3.951246753334999
CurrentTrain: epoch  1, batch    32 | loss: 14.2503028Losses:  9.351544380187988 1.3918213844299316 1.2379828691482544 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 9.3515444Losses:  9.283535957336426 1.4324142932891846 1.321043848991394 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 9.2835360Losses:  17.1985801756382 1.5535964965820312 1.3298234939575195 6.542270094156265
CurrentTrain: epoch  1, batch    35 | loss: 17.1985802Losses:  12.238177798688412 1.6260664463043213 1.3727412223815918 1.6053996309638023
CurrentTrain: epoch  1, batch    36 | loss: 12.2381778Losses:  10.389236457645893 1.2306108474731445 1.2102208137512207 1.6042194440960884
CurrentTrain: epoch  1, batch    37 | loss: 10.3892365Losses:  12.180137179791927 1.489979863166809 1.3459253311157227 2.9884658083319664
CurrentTrain: epoch  1, batch    38 | loss: 12.1801372Losses:  13.927021220326424 1.5781954526901245 1.2724264860153198 4.703357890248299
CurrentTrain: epoch  1, batch    39 | loss: 13.9270212Losses:  11.043610095977783 1.4327399730682373 1.3585280179977417 1.4673333168029785
CurrentTrain: epoch  1, batch    40 | loss: 11.0436101Losses:  12.255074322223663 1.4528093338012695 1.1873753070831299 2.956116497516632
CurrentTrain: epoch  1, batch    41 | loss: 12.2550743Losses:  11.561181474477053 1.5198372602462769 1.1909352540969849 1.8549217469990253
CurrentTrain: epoch  1, batch    42 | loss: 11.5611815Losses:  14.227617524564266 1.4163410663604736 1.371048927307129 4.503932259976864
CurrentTrain: epoch  1, batch    43 | loss: 14.2276175Losses:  14.481022665277123 1.3520011901855469 1.3182203769683838 3.893675634637475
CurrentTrain: epoch  1, batch    44 | loss: 14.4810227Losses:  13.256519202142954 1.5059847831726074 1.3504729270935059 2.96854579821229
CurrentTrain: epoch  1, batch    45 | loss: 13.2565192Losses:  10.5294828414917 1.5729023218154907 1.28804612159729 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 10.5294828Losses:  9.759831428527832 1.5256459712982178 1.1045076847076416 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 9.7598314Losses:  11.409556478261948 1.3801400661468506 1.1898794174194336 1.4076920449733734
CurrentTrain: epoch  1, batch    48 | loss: 11.4095565Losses:  19.497820168733597 1.3892827033996582 1.2106451988220215 9.359789162874222
CurrentTrain: epoch  1, batch    49 | loss: 19.4978202Losses:  11.18434277176857 1.208479881286621 1.128179907798767 1.4396242201328278
CurrentTrain: epoch  1, batch    50 | loss: 11.1843428Losses:  9.965991973876953 1.5054974555969238 1.2648506164550781 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 9.9659920Losses:  16.05435499176383 1.4798450469970703 1.227407693862915 6.3691609762609005
CurrentTrain: epoch  1, batch    52 | loss: 16.0543550Losses:  9.050774574279785 1.4300750494003296 1.3620892763137817 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 9.0507746Losses:  12.465514436364174 1.2823028564453125 1.2544350624084473 3.3343308120965958
CurrentTrain: epoch  1, batch    54 | loss: 12.4655144Losses:  12.120046451687813 1.3472161293029785 1.3374676704406738 2.9133528023958206
CurrentTrain: epoch  1, batch    55 | loss: 12.1200465Losses:  9.757993698120117 1.5236945152282715 1.1529860496520996 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 9.7579937Losses:  10.137706756591797 1.4623134136199951 1.2674477100372314 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 10.1377068Losses:  9.034796714782715 1.3090825080871582 1.270840048789978 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 9.0347967Losses:  13.368269670754671 1.3784136772155762 1.2760034799575806 3.2383267767727375
CurrentTrain: epoch  1, batch    59 | loss: 13.3682697Losses:  9.513507843017578 1.5013962984085083 1.3541772365570068 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 9.5135078Losses:  10.856292724609375 1.448828935623169 1.2032241821289062 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 10.8562927Losses:  12.390957687050104 1.5860507488250732 1.5034499168395996 4.35913548246026
CurrentTrain: epoch  1, batch    62 | loss: 12.3909577Losses:  8.299339294433594 1.4631812572479248 1.2422661781311035 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 8.2993393Losses:  8.998124122619629 1.354837417602539 1.2967939376831055 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 8.9981241Losses:  9.98938861489296 1.487715721130371 1.2688744068145752 1.4037248194217682
CurrentTrain: epoch  2, batch     2 | loss: 9.9893886Losses:  14.139057997614145 1.3303842544555664 1.3553481101989746 3.054484251886606
CurrentTrain: epoch  2, batch     3 | loss: 14.1390580Losses:  11.667106863111258 1.377474069595337 1.2318062782287598 1.468473669141531
CurrentTrain: epoch  2, batch     4 | loss: 11.6671069Losses:  11.658132433891296 1.3318355083465576 1.1114802360534668 2.12087619304657
CurrentTrain: epoch  2, batch     5 | loss: 11.6581324Losses:  9.27345085144043 1.475107192993164 1.2066960334777832 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 9.2734509Losses:  14.29615867882967 1.3525549173355103 1.2084898948669434 4.98982322961092
CurrentTrain: epoch  2, batch     7 | loss: 14.2961587Losses:  12.47496484965086 1.3321115970611572 1.2300971746444702 3.4226634427905083
CurrentTrain: epoch  2, batch     8 | loss: 12.4749648Losses:  13.63291361182928 1.4354264736175537 1.23061203956604 3.9028053507208824
CurrentTrain: epoch  2, batch     9 | loss: 13.6329136Losses:  10.611761942505836 1.3306964635849 1.2698296308517456 1.5006407648324966
CurrentTrain: epoch  2, batch    10 | loss: 10.6117619Losses:  10.030204594135284 1.3526225090026855 1.2054979801177979 1.5410030484199524
CurrentTrain: epoch  2, batch    11 | loss: 10.0302046Losses:  15.479369141161442 1.5504486560821533 1.2676805257797241 5.89241311699152
CurrentTrain: epoch  2, batch    12 | loss: 15.4793691Losses:  10.619140058755875 1.258885145187378 1.2365957498550415 1.4867099821567535
CurrentTrain: epoch  2, batch    13 | loss: 10.6191401Losses:  13.015613473951817 1.4371962547302246 1.3593332767486572 2.8791760578751564
CurrentTrain: epoch  2, batch    14 | loss: 13.0156135Losses:  11.199856388382614 1.217402696609497 1.1196328401565552 2.301546680741012
CurrentTrain: epoch  2, batch    15 | loss: 11.1998564Losses:  12.137417197227478 1.3547019958496094 1.148820400238037 2.8511356115341187
CurrentTrain: epoch  2, batch    16 | loss: 12.1374172Losses:  12.289928246289492 1.5288441181182861 1.2810695171356201 2.9223735816776752
CurrentTrain: epoch  2, batch    17 | loss: 12.2899282Losses:  20.06841879710555 1.5905108451843262 1.3194283246994019 11.620302494615316
CurrentTrain: epoch  2, batch    18 | loss: 20.0684188Losses:  11.397434884682298 1.29817533493042 1.1457080841064453 1.8709456268697977
CurrentTrain: epoch  2, batch    19 | loss: 11.3974349Losses:  8.576071739196777 1.3223395347595215 1.2835335731506348 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 8.5760717Losses:  9.401657104492188 1.4670579433441162 1.354366421699524 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 9.4016571Losses:  13.287453413009644 1.367300033569336 1.0927187204360962 4.371161222457886
CurrentTrain: epoch  2, batch    22 | loss: 13.2874534Losses:  11.058695368468761 1.1301422119140625 1.1791365146636963 1.6153989359736443
CurrentTrain: epoch  2, batch    23 | loss: 11.0586954Losses:  9.647219382226467 1.362262487411499 1.2192308902740479 1.5136324986815453
CurrentTrain: epoch  2, batch    24 | loss: 9.6472194Losses:  14.125863287597895 1.5074098110198975 1.3154658079147339 4.870211813598871
CurrentTrain: epoch  2, batch    25 | loss: 14.1258633Losses:  10.539736956357956 1.4522790908813477 1.094868779182434 1.4146330058574677
CurrentTrain: epoch  2, batch    26 | loss: 10.5397370Losses:  10.498350940644741 1.267437219619751 1.1513099670410156 1.7941101416945457
CurrentTrain: epoch  2, batch    27 | loss: 10.4983509Losses:  10.237884316593409 1.3957582712173462 1.2579994201660156 1.4172094203531742
CurrentTrain: epoch  2, batch    28 | loss: 10.2378843Losses:  20.83341731503606 1.4270403385162354 1.2150402069091797 12.051979441195726
CurrentTrain: epoch  2, batch    29 | loss: 20.8334173Losses:  10.732436891645193 1.5504639148712158 1.4206583499908447 1.4695832692086697
CurrentTrain: epoch  2, batch    30 | loss: 10.7324369Losses:  9.860230255872011 1.4561976194381714 1.0978143215179443 1.486910630017519
CurrentTrain: epoch  2, batch    31 | loss: 9.8602303Losses:  11.395576443523169 1.351021647453308 1.1969523429870605 2.9120082519948483
CurrentTrain: epoch  2, batch    32 | loss: 11.3955764Losses:  12.029002584517002 1.3295824527740479 1.2748219966888428 2.2780651226639748
CurrentTrain: epoch  2, batch    33 | loss: 12.0290026Losses:  14.404372032731771 1.4813148975372314 1.075352668762207 5.7006719671189785
CurrentTrain: epoch  2, batch    34 | loss: 14.4043720Losses:  15.399156425148249 1.2945268154144287 1.0073106288909912 5.897940490394831
CurrentTrain: epoch  2, batch    35 | loss: 15.3991564Losses:  11.685860995203257 1.1714637279510498 1.1137123107910156 2.900945071130991
CurrentTrain: epoch  2, batch    36 | loss: 11.6858610Losses:  11.268865566700697 1.4642927646636963 1.1942577362060547 1.7800063900649548
CurrentTrain: epoch  2, batch    37 | loss: 11.2688656Losses:  10.39602056145668 1.468296766281128 1.312943458557129 1.4043718874454498
CurrentTrain: epoch  2, batch    38 | loss: 10.3960206Losses:  12.890997931361198 1.3569107055664062 1.199195146560669 3.357255980372429
CurrentTrain: epoch  2, batch    39 | loss: 12.8909979Losses:  12.888429522514343 1.3261280059814453 1.2361750602722168 4.450490832328796
CurrentTrain: epoch  2, batch    40 | loss: 12.8884295Losses:  11.488805398344994 1.2693915367126465 1.2374343872070312 1.4902416318655014
CurrentTrain: epoch  2, batch    41 | loss: 11.4888054Losses:  10.01162764430046 1.2958177328109741 1.2222541570663452 1.3993105590343475
CurrentTrain: epoch  2, batch    42 | loss: 10.0116276Losses:  9.275636672973633 1.3815265893936157 1.2248926162719727 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 9.2756367Losses:  9.53016858920455 1.1895475387573242 1.1612952947616577 1.4762745462357998
CurrentTrain: epoch  2, batch    44 | loss: 9.5301686Losses:  12.234316850081086 1.319739580154419 1.25470769405365 3.276134515181184
CurrentTrain: epoch  2, batch    45 | loss: 12.2343169Losses:  8.460195541381836 1.2828150987625122 1.2050046920776367 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 8.4601955Losses:  8.916975975036621 1.2436414957046509 1.2180300951004028 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 8.9169760Losses:  8.344931602478027 1.3818551301956177 1.2580188512802124 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 8.3449316Losses:  11.014931198209524 1.4312692880630493 1.2743430137634277 1.4321331940591335
CurrentTrain: epoch  2, batch    49 | loss: 11.0149312Losses:  18.249076448380947 1.4937069416046143 1.146498203277588 9.178798280656338
CurrentTrain: epoch  2, batch    50 | loss: 18.2490764Losses:  12.012512849643826 1.403605341911316 1.3801032304763794 3.555651353672147
CurrentTrain: epoch  2, batch    51 | loss: 12.0125128Losses:  9.897020572796464 1.2309235334396362 1.1644809246063232 1.7212316934019327
CurrentTrain: epoch  2, batch    52 | loss: 9.8970206Losses:  11.274209320545197 1.244718074798584 1.2339413166046143 2.839560806751251
CurrentTrain: epoch  2, batch    53 | loss: 11.2742093Losses:  10.286228090524673 1.363999605178833 1.1235636472702026 1.4013737738132477
CurrentTrain: epoch  2, batch    54 | loss: 10.2862281Losses:  11.136843029409647 1.3595051765441895 1.230446457862854 2.9173148311674595
CurrentTrain: epoch  2, batch    55 | loss: 11.1368430Losses:  10.289379300549626 1.3872064352035522 1.3246526718139648 1.769201459363103
CurrentTrain: epoch  2, batch    56 | loss: 10.2893793Losses:  9.079568862915039 1.3273470401763916 1.0809085369110107 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 9.0795689Losses:  8.776670455932617 1.3125487565994263 1.2684272527694702 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 8.7766705Losses:  19.74055342748761 1.4942582845687866 1.1737650632858276 11.275174666196108
CurrentTrain: epoch  2, batch    59 | loss: 19.7405534Losses:  8.801861763000488 1.3863317966461182 1.2121541500091553 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 8.8018618Losses:  11.531712792813778 1.2896918058395386 1.180862307548523 1.9980528578162193
CurrentTrain: epoch  2, batch    61 | loss: 11.5317128Losses:  8.373750686645508 1.3199493885040283 1.2748987674713135 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 8.3737507Losses:  9.75995609164238 1.3656399250030518 1.3108408451080322 1.4246909320354462
CurrentTrain: epoch  3, batch     0 | loss: 9.7599561Losses:  9.64663752913475 1.249135136604309 1.1921597719192505 1.417348474264145
CurrentTrain: epoch  3, batch     1 | loss: 9.6466375Losses:  8.772783279418945 1.352999210357666 1.2355148792266846 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 8.7727833Losses:  12.84994949400425 1.2835628986358643 1.1775293350219727 4.3347069174051285
CurrentTrain: epoch  3, batch     3 | loss: 12.8499495Losses:  10.934734918177128 1.2574793100357056 1.1276053190231323 3.3337813392281532
CurrentTrain: epoch  3, batch     4 | loss: 10.9347349Losses:  13.23183199763298 1.3964648246765137 1.1298067569732666 4.309707134962082
CurrentTrain: epoch  3, batch     5 | loss: 13.2318320Losses:  9.663406401872635 1.288285732269287 1.0378646850585938 1.4037857353687286
CurrentTrain: epoch  3, batch     6 | loss: 9.6634064Losses:  12.808973662555218 1.2889645099639893 1.296482801437378 4.252410285174847
CurrentTrain: epoch  3, batch     7 | loss: 12.8089737Losses:  9.549569599330425 1.2432116270065308 1.1950709819793701 1.4414400979876518
CurrentTrain: epoch  3, batch     8 | loss: 9.5495696Losses:  8.679771423339844 1.2632637023925781 1.0572080612182617 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 8.6797714Losses:  8.411014556884766 1.3411552906036377 1.1202759742736816 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 8.4110146Losses:  11.61084657534957 1.395317554473877 1.274113655090332 3.0280104242265224
CurrentTrain: epoch  3, batch    11 | loss: 11.6108466Losses:  12.666829854249954 1.3868579864501953 1.439087986946106 4.2461831867694855
CurrentTrain: epoch  3, batch    12 | loss: 12.6668299Losses:  12.777043998241425 1.5601208209991455 1.3118176460266113 2.873269736766815
CurrentTrain: epoch  3, batch    13 | loss: 12.7770440Losses:  19.126049757003784 1.1352486610412598 1.2349494695663452 10.367085218429565
CurrentTrain: epoch  3, batch    14 | loss: 19.1260498Losses:  10.096644967794418 1.2814879417419434 1.1187177896499634 1.4134975373744965
CurrentTrain: epoch  3, batch    15 | loss: 10.0966450Losses:  14.877248153090477 1.3038349151611328 1.184192419052124 6.0689090341329575
CurrentTrain: epoch  3, batch    16 | loss: 14.8772482Losses:  9.642907418310642 1.3882858753204346 1.3140864372253418 1.4439576044678688
CurrentTrain: epoch  3, batch    17 | loss: 9.6429074Losses:  12.426075831055641 1.280527114868164 1.1645256280899048 4.362721338868141
CurrentTrain: epoch  3, batch    18 | loss: 12.4260758Losses:  9.108965516090393 1.2973062992095947 1.3290802240371704 1.480683445930481
CurrentTrain: epoch  3, batch    19 | loss: 9.1089655Losses:  11.604993425309658 1.2472944259643555 1.0739201307296753 4.2378517016768456
CurrentTrain: epoch  3, batch    20 | loss: 11.6049934Losses:  11.443988256156445 1.2917916774749756 1.2257745265960693 3.307838849723339
CurrentTrain: epoch  3, batch    21 | loss: 11.4439883Losses:  15.132546965032816 1.2868340015411377 1.135741114616394 6.410708013921976
CurrentTrain: epoch  3, batch    22 | loss: 15.1325470Losses:  10.292033523321152 1.390851616859436 1.2319585084915161 1.4244788587093353
CurrentTrain: epoch  3, batch    23 | loss: 10.2920335Losses:  9.947969403117895 1.1578763723373413 1.1007273197174072 1.4436883591115475
CurrentTrain: epoch  3, batch    24 | loss: 9.9479694Losses:  11.58998166397214 1.1120554208755493 1.176086664199829 2.9386011250317097
CurrentTrain: epoch  3, batch    25 | loss: 11.5899817Losses:  10.771128173917532 1.401615858078003 1.3732638359069824 2.9115619622170925
CurrentTrain: epoch  3, batch    26 | loss: 10.7711282Losses:  8.850719813257456 1.1808130741119385 1.1434882879257202 1.4471310414373875
CurrentTrain: epoch  3, batch    27 | loss: 8.8507198Losses:  13.515976421535015 1.1229609251022339 1.0947235822677612 4.576631061732769
CurrentTrain: epoch  3, batch    28 | loss: 13.5159764Losses:  13.946285467594862 1.3408124446868896 1.2847175598144531 5.8482782654464245
CurrentTrain: epoch  3, batch    29 | loss: 13.9462855Losses:  8.056844711303711 1.2509394884109497 1.3091931343078613 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 8.0568447Losses:  7.5602827072143555 1.1711056232452393 1.0032150745391846 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 7.5602827Losses:  13.52150672674179 1.1504656076431274 1.1595239639282227 5.765693128108978
CurrentTrain: epoch  3, batch    32 | loss: 13.5215067Losses:  10.977618664503098 1.0791456699371338 1.2079954147338867 2.8887819945812225
CurrentTrain: epoch  3, batch    33 | loss: 10.9776187Losses:  8.407991409301758 1.171152114868164 1.2428112030029297 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 8.4079914Losses:  10.190712738782167 1.1353919506072998 1.1934707164764404 1.5130260474979877
CurrentTrain: epoch  3, batch    35 | loss: 10.1907127Losses:  7.771641731262207 1.3255364894866943 1.1553761959075928 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 7.7716417Losses:  8.450601577758789 1.4342222213745117 1.4136707782745361 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 8.4506016Losses:  10.876436710357666 1.278625726699829 1.290160894393921 2.817234516143799
CurrentTrain: epoch  3, batch    38 | loss: 10.8764367Losses:  12.462473392486572 1.3706541061401367 1.2833704948425293 4.256575107574463
CurrentTrain: epoch  3, batch    39 | loss: 12.4624734Losses:  17.46851620078087 1.0973844528198242 1.1488665342330933 7.767885059118271
CurrentTrain: epoch  3, batch    40 | loss: 17.4685162Losses:  8.855305671691895 1.3044073581695557 1.1998014450073242 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 8.8553057Losses:  8.737105667591095 1.2470314502716064 1.089233160018921 1.3962062001228333
CurrentTrain: epoch  3, batch    42 | loss: 8.7371057Losses:  10.101933389902115 1.3836801052093506 1.4248700141906738 1.7683877050876617
CurrentTrain: epoch  3, batch    43 | loss: 10.1019334Losses:  11.864472649991512 1.3170177936553955 1.2918205261230469 3.1409952864050865
CurrentTrain: epoch  3, batch    44 | loss: 11.8644726Losses:  10.66479017958045 1.1193184852600098 1.1211669445037842 2.83516314253211
CurrentTrain: epoch  3, batch    45 | loss: 10.6647902Losses:  12.16000522300601 1.2795873880386353 1.295905590057373 3.2698837630450726
CurrentTrain: epoch  3, batch    46 | loss: 12.1600052Losses:  11.037940174341202 1.5624107122421265 1.4382153749465942 2.8107320368289948
CurrentTrain: epoch  3, batch    47 | loss: 11.0379402Losses:  8.167301177978516 1.3117042779922485 1.2848374843597412 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 8.1673012Losses:  18.988911412656307 1.3992804288864136 1.2829411029815674 10.174151204526424
CurrentTrain: epoch  3, batch    49 | loss: 18.9889114Losses:  8.879243850708008 1.5107333660125732 1.3581817150115967 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 8.8792439Losses:  9.634778618812561 1.1972945928573608 1.1297848224639893 1.4280868768692017
CurrentTrain: epoch  3, batch    51 | loss: 9.6347786Losses:  12.065193742513657 1.208972454071045 1.3032221794128418 4.378840535879135
CurrentTrain: epoch  3, batch    52 | loss: 12.0651937Losses:  13.386411484330893 1.3393501043319702 1.3018510341644287 4.327435310930014
CurrentTrain: epoch  3, batch    53 | loss: 13.3864115Losses:  8.83400422334671 1.2542920112609863 1.1402082443237305 1.3986476063728333
CurrentTrain: epoch  3, batch    54 | loss: 8.8340042Losses:  11.160950846970081 1.4987214803695679 1.1521821022033691 2.8239995911717415
CurrentTrain: epoch  3, batch    55 | loss: 11.1609508Losses:  7.529947280883789 1.1983684301376343 1.1184427738189697 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 7.5299473Losses:  7.929262161254883 1.1602091789245605 1.0535097122192383 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.9292622Losses:  13.162346720695496 1.2742291688919067 1.1639002561569214 5.44904887676239
CurrentTrain: epoch  3, batch    58 | loss: 13.1623467Losses:  18.181393265724182 1.2357900142669678 1.0534337759017944 9.820963501930237
CurrentTrain: epoch  3, batch    59 | loss: 18.1813933Losses:  8.676602363586426 1.4142606258392334 1.1845862865447998 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 8.6766024Losses:  13.289930012077093 1.4229516983032227 1.1629550457000732 4.671457912772894
CurrentTrain: epoch  3, batch    61 | loss: 13.2899300Losses:  9.158672422170639 1.347182273864746 0.9895972609519958 1.4095922410488129
CurrentTrain: epoch  3, batch    62 | loss: 9.1586724Losses:  12.42655785381794 1.1738598346710205 1.1890416145324707 4.338177040219307
CurrentTrain: epoch  4, batch     0 | loss: 12.4265579Losses:  13.059512693434954 1.2882004976272583 1.2481489181518555 4.950821477919817
CurrentTrain: epoch  4, batch     1 | loss: 13.0595127Losses:  7.458761692047119 1.3338305950164795 1.160331130027771 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 7.4587617Losses:  11.476979047060013 1.2803294658660889 1.2204248905181885 4.257527142763138
CurrentTrain: epoch  4, batch     3 | loss: 11.4769790Losses:  16.985011130571365 1.4653066396713257 1.4130423069000244 8.623617202043533
CurrentTrain: epoch  4, batch     4 | loss: 16.9850111Losses:  9.148351222276688 1.1160434484481812 1.316314935684204 1.4109196960926056
CurrentTrain: epoch  4, batch     5 | loss: 9.1483512Losses:  9.001950822770596 1.154486894607544 1.1147377490997314 1.520795427262783
CurrentTrain: epoch  4, batch     6 | loss: 9.0019508Losses:  7.023066520690918 1.1403405666351318 1.2207984924316406 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 7.0230665Losses:  11.126537915319204 1.1277695894241333 1.1933903694152832 2.856611844152212
CurrentTrain: epoch  4, batch     8 | loss: 11.1265379Losses:  9.840716496109962 1.0856444835662842 1.1247341632843018 1.5222445875406265
CurrentTrain: epoch  4, batch     9 | loss: 9.8407165Losses:  12.377748597413301 1.2717411518096924 1.1666125059127808 4.4209967739880085
CurrentTrain: epoch  4, batch    10 | loss: 12.3777486Losses:  11.206548072397709 1.2240077257156372 1.4129369258880615 2.8676818385720253
CurrentTrain: epoch  4, batch    11 | loss: 11.2065481Losses:  9.513943009078503 1.235792875289917 1.1553611755371094 1.4384673163294792
CurrentTrain: epoch  4, batch    12 | loss: 9.5139430Losses:  7.882030010223389 1.3205289840698242 1.271233081817627 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 7.8820300Losses:  9.44921050593257 1.104240894317627 1.0776227712631226 1.4807704500854015
CurrentTrain: epoch  4, batch    14 | loss: 9.4492105Losses:  9.095844209194183 0.9601958394050598 1.166738510131836 1.3968443274497986
CurrentTrain: epoch  4, batch    15 | loss: 9.0958442Losses:  11.767395853996277 1.3744447231292725 1.1831140518188477 3.1760443449020386
CurrentTrain: epoch  4, batch    16 | loss: 11.7673959Losses:  7.749658107757568 1.295686960220337 1.2679029703140259 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 7.7496581Losses:  12.232596039772034 1.3116307258605957 1.0994317531585693 4.359196305274963
CurrentTrain: epoch  4, batch    18 | loss: 12.2325960Losses:  16.622117161750793 1.2532570362091064 1.0995965003967285 8.767714142799377
CurrentTrain: epoch  4, batch    19 | loss: 16.6221172Losses:  11.311448603868484 1.247532606124878 1.2465379238128662 2.853650599718094
CurrentTrain: epoch  4, batch    20 | loss: 11.3114486Losses:  10.911690775305033 1.1402959823608398 1.3444361686706543 2.872145716100931
CurrentTrain: epoch  4, batch    21 | loss: 10.9116908Losses:  9.294403601437807 1.362527847290039 1.2288832664489746 1.4827175624668598
CurrentTrain: epoch  4, batch    22 | loss: 9.2944036Losses:  14.510359827429056 1.1180418729782104 1.1009352207183838 7.136070314794779
CurrentTrain: epoch  4, batch    23 | loss: 14.5103598Losses:  8.93065943941474 1.1390055418014526 1.1826781034469604 1.4660522006452084
CurrentTrain: epoch  4, batch    24 | loss: 8.9306594Losses:  8.406061172485352 1.1990270614624023 1.0872271060943604 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 8.4060612Losses:  8.812737077474594 1.2934486865997314 1.1860463619232178 1.4365144670009613
CurrentTrain: epoch  4, batch    26 | loss: 8.8127371Losses:  10.748251954093575 1.3138223886489868 1.290984869003296 2.0248041544109583
CurrentTrain: epoch  4, batch    27 | loss: 10.7482520Losses:  9.617841988801956 1.3493348360061646 1.2760443687438965 1.4146445095539093
CurrentTrain: epoch  4, batch    28 | loss: 9.6178420Losses:  13.959102232009172 1.277976155281067 1.2175800800323486 5.976199705153704
CurrentTrain: epoch  4, batch    29 | loss: 13.9591022Losses:  13.27896224707365 1.3179357051849365 1.3466925621032715 5.734184853732586
CurrentTrain: epoch  4, batch    30 | loss: 13.2789622Losses:  7.731898307800293 1.3078839778900146 1.154062271118164 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 7.7318983Losses:  10.053724452853203 1.0892170667648315 1.0931971073150635 2.9782001227140427
CurrentTrain: epoch  4, batch    32 | loss: 10.0537245Losses:  12.329602899029851 1.0583128929138184 1.2084147930145264 5.018702210858464
CurrentTrain: epoch  4, batch    33 | loss: 12.3296029Losses:  7.551130294799805 1.3094301223754883 1.2852928638458252 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 7.5511303Losses:  9.145097106695175 1.2478623390197754 0.9537325501441956 1.4011724889278412
CurrentTrain: epoch  4, batch    35 | loss: 9.1450971Losses:  12.667708307504654 1.310464859008789 1.2898797988891602 4.6798485815525055
CurrentTrain: epoch  4, batch    36 | loss: 12.6677083Losses:  12.099626591429114 1.2870360612869263 1.3861976861953735 3.6538334395736456
CurrentTrain: epoch  4, batch    37 | loss: 12.0996266Losses:  7.294114589691162 1.1943678855895996 1.1550530195236206 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 7.2941146Losses:  11.82248842716217 1.094881296157837 1.1184556484222412 4.372782349586487
CurrentTrain: epoch  4, batch    39 | loss: 11.8224884Losses:  12.480103053152561 1.2367838621139526 1.2098188400268555 4.6595940962433815
CurrentTrain: epoch  4, batch    40 | loss: 12.4801031Losses:  7.818617820739746 1.430814504623413 1.3981645107269287 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 7.8186178Losses:  13.888698123395443 1.192655086517334 1.124912977218628 6.2772326692938805
CurrentTrain: epoch  4, batch    42 | loss: 13.8886981Losses:  10.23128554597497 1.154099464416504 1.2682948112487793 2.8669700361788273
CurrentTrain: epoch  4, batch    43 | loss: 10.2312855Losses:  7.876368522644043 1.3913378715515137 1.1781165599822998 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 7.8763685Losses:  8.729694738984108 1.0947542190551758 0.9262596368789673 1.495472326874733
CurrentTrain: epoch  4, batch    45 | loss: 8.7296947Losses:  10.938676495105028 0.9707158803939819 1.1361202001571655 2.9722677655518055
CurrentTrain: epoch  4, batch    46 | loss: 10.9386765Losses:  9.44454675912857 1.456769585609436 1.336515188217163 1.621239721775055
CurrentTrain: epoch  4, batch    47 | loss: 9.4445468Losses:  13.708039790391922 1.1891690492630005 1.1330527067184448 5.856861621141434
CurrentTrain: epoch  4, batch    48 | loss: 13.7080398Losses:  8.130768775939941 1.6128559112548828 1.4798364639282227 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 8.1307688Losses:  10.683801263570786 1.418594479560852 1.2409379482269287 2.992532342672348
CurrentTrain: epoch  4, batch    50 | loss: 10.6838013Losses:  6.982614994049072 1.010862112045288 1.1701769828796387 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 6.9826150Losses:  11.497916046530008 1.1811797618865967 1.2507829666137695 4.266201797872782
CurrentTrain: epoch  4, batch    52 | loss: 11.4979160Losses:  10.715963900089264 1.1629544496536255 1.1558259725570679 1.524862825870514
CurrentTrain: epoch  4, batch    53 | loss: 10.7159639Losses:  8.415072590112686 1.2010631561279297 1.2298359870910645 1.402578979730606
CurrentTrain: epoch  4, batch    54 | loss: 8.4150726Losses:  7.7816290855407715 1.0180341005325317 1.192451000213623 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 7.7816291Losses:  18.17164671048522 1.3672791719436646 1.2166837453842163 10.32703697308898
CurrentTrain: epoch  4, batch    56 | loss: 18.1716467Losses:  11.69332018494606 1.1145234107971191 1.3904622793197632 4.258099466562271
CurrentTrain: epoch  4, batch    57 | loss: 11.6933202Losses:  11.30679327994585 1.3890771865844727 1.2031975984573364 3.1726341918110847
CurrentTrain: epoch  4, batch    58 | loss: 11.3067933Losses:  10.464631155133247 1.088330626487732 1.1315442323684692 3.0573573857545853
CurrentTrain: epoch  4, batch    59 | loss: 10.4646312Losses:  8.995782643556595 1.3250902891159058 1.1143178939819336 1.415408879518509
CurrentTrain: epoch  4, batch    60 | loss: 8.9957826Losses:  9.071367867290974 1.1962765455245972 1.051001787185669 1.46099341660738
CurrentTrain: epoch  4, batch    61 | loss: 9.0713679Losses:  13.08700604736805 1.0947426557540894 1.2854812145233154 6.091894581913948
CurrentTrain: epoch  4, batch    62 | loss: 13.0870060Losses:  7.509315013885498 1.1755892038345337 0.9885196685791016 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.5093150Losses:  10.185895901173353 0.9985082745552063 1.2092392444610596 2.972337704151869
CurrentTrain: epoch  5, batch     1 | loss: 10.1858959Losses:  10.314315140247345 1.225698709487915 1.1493186950683594 2.8154662251472473
CurrentTrain: epoch  5, batch     2 | loss: 10.3143151Losses:  10.597266357392073 1.2006089687347412 1.177713394165039 2.9197522811591625
CurrentTrain: epoch  5, batch     3 | loss: 10.5972664Losses:  11.541613698005676 1.0875368118286133 1.1945388317108154 4.328274846076965
CurrentTrain: epoch  5, batch     4 | loss: 11.5416137Losses:  12.961566478013992 1.1465628147125244 1.2837131023406982 5.806368380784988
CurrentTrain: epoch  5, batch     5 | loss: 12.9615665Losses:  7.799494743347168 1.2709102630615234 1.1120879650115967 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 7.7994947Losses:  8.81608361005783 1.1897794008255005 1.0851091146469116 1.4300701022148132
CurrentTrain: epoch  5, batch     7 | loss: 8.8160836Losses:  12.19469489902258 1.231918215751648 1.1549969911575317 4.368152521550655
CurrentTrain: epoch  5, batch     8 | loss: 12.1946949Losses:  13.229341387748718 0.9480445384979248 1.0530849695205688 6.02446973323822
CurrentTrain: epoch  5, batch     9 | loss: 13.2293414Losses:  11.18678268790245 1.4404829740524292 1.3135675191879272 2.803389400243759
CurrentTrain: epoch  5, batch    10 | loss: 11.1867827Losses:  31.782559007406235 1.1772968769073486 1.3208063840866089 24.229424089193344
CurrentTrain: epoch  5, batch    11 | loss: 31.7825590Losses:  10.230507880449295 1.129093885421753 1.1645591259002686 2.819128066301346
CurrentTrain: epoch  5, batch    12 | loss: 10.2305079Losses:  9.029602259397507 1.3016844987869263 1.3770339488983154 1.4012815654277802
CurrentTrain: epoch  5, batch    13 | loss: 9.0296023Losses:  9.047231435775757 1.1331946849822998 1.095940351486206 1.7028911113739014
CurrentTrain: epoch  5, batch    14 | loss: 9.0472314Losses:  9.000415533781052 1.0495293140411377 1.1543121337890625 1.4947697818279266
CurrentTrain: epoch  5, batch    15 | loss: 9.0004155Losses:  8.862370282411575 1.0962778329849243 1.275341510772705 1.4670789539813995
CurrentTrain: epoch  5, batch    16 | loss: 8.8623703Losses:  11.226186595857143 1.4111087322235107 1.4832099676132202 2.8664501532912254
CurrentTrain: epoch  5, batch    17 | loss: 11.2261866Losses:  11.55369571223855 1.237492561340332 1.0371198654174805 4.532222781330347
CurrentTrain: epoch  5, batch    18 | loss: 11.5536957Losses:  8.63089406490326 1.1920981407165527 1.2082325220108032 1.407528281211853
CurrentTrain: epoch  5, batch    19 | loss: 8.6308941Losses:  9.577731251716614 1.1457446813583374 1.1224015951156616 2.8076354265213013
CurrentTrain: epoch  5, batch    20 | loss: 9.5777313Losses:  12.748566210269928 1.207188606262207 1.348219633102417 5.74016147851944
CurrentTrain: epoch  5, batch    21 | loss: 12.7485662Losses:  9.900024835020304 1.209074854850769 1.1323795318603516 3.0237001813948154
CurrentTrain: epoch  5, batch    22 | loss: 9.9000248Losses:  10.9096799492836 1.1748043298721313 1.1381287574768066 3.2864919304847717
CurrentTrain: epoch  5, batch    23 | loss: 10.9096799Losses:  9.848254837095737 1.2254774570465088 1.154460072517395 2.8980671539902687
CurrentTrain: epoch  5, batch    24 | loss: 9.8482548Losses:  22.926384929567575 1.0743168592453003 1.2087745666503906 14.880913738161325
CurrentTrain: epoch  5, batch    25 | loss: 22.9263849Losses:  10.382117077708244 1.2433686256408691 1.2324002981185913 3.0716665238142014
CurrentTrain: epoch  5, batch    26 | loss: 10.3821171Losses:  8.86734253168106 1.105134129524231 1.07094407081604 1.3997684121131897
CurrentTrain: epoch  5, batch    27 | loss: 8.8673425Losses:  8.30514271184802 1.1339064836502075 1.216132640838623 1.4736316911876202
CurrentTrain: epoch  5, batch    28 | loss: 8.3051427Losses:  11.847098845988512 0.9683237075805664 1.152224063873291 4.3827514834702015
CurrentTrain: epoch  5, batch    29 | loss: 11.8470988Losses:  9.390760641545057 1.1485891342163086 1.0519731044769287 1.5237142853438854
CurrentTrain: epoch  5, batch    30 | loss: 9.3907606Losses:  11.628292210400105 1.2075519561767578 0.993094801902771 4.443668015301228
CurrentTrain: epoch  5, batch    31 | loss: 11.6282922Losses:  8.561081796884537 1.138289451599121 1.1327335834503174 1.422675997018814
CurrentTrain: epoch  5, batch    32 | loss: 8.5610818Losses:  22.960831835865974 1.1908297538757324 1.4540784358978271 14.455993846058846
CurrentTrain: epoch  5, batch    33 | loss: 22.9608318Losses:  10.48963487148285 1.0466171503067017 1.2276628017425537 2.8997997045516968
CurrentTrain: epoch  5, batch    34 | loss: 10.4896349Losses:  11.673040717840195 1.2512812614440918 1.3124616146087646 4.294250816106796
CurrentTrain: epoch  5, batch    35 | loss: 11.6730407Losses:  14.559900008141994 1.2748433351516724 1.1485174894332886 6.886554919183254
CurrentTrain: epoch  5, batch    36 | loss: 14.5599000Losses:  8.8366480730474 1.3511234521865845 1.3233788013458252 1.4356661699712276
CurrentTrain: epoch  5, batch    37 | loss: 8.8366481Losses:  8.821453277021646 1.1970806121826172 1.170697808265686 1.4660479463636875
CurrentTrain: epoch  5, batch    38 | loss: 8.8214533Losses:  8.449367642402649 1.0733445882797241 1.1788413524627686 1.4031811952590942
CurrentTrain: epoch  5, batch    39 | loss: 8.4493676Losses:  8.59794619679451 1.0132689476013184 1.3858757019042969 1.391008883714676
CurrentTrain: epoch  5, batch    40 | loss: 8.5979462Losses:  7.952544212341309 1.2878100872039795 1.2885351181030273 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 7.9525442Losses:  9.04330911487341 0.9630675911903381 1.146918773651123 1.4901384338736534
CurrentTrain: epoch  5, batch    42 | loss: 9.0433091Losses:  11.858731921762228 1.1637439727783203 1.2064905166625977 4.26748913154006
CurrentTrain: epoch  5, batch    43 | loss: 11.8587319Losses:  7.608529090881348 1.0607073307037354 1.3305236101150513 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 7.6085291Losses:  6.937536716461182 1.143412470817566 1.2063164710998535 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 6.9375367Losses:  10.589556280523539 1.5585055351257324 1.4314584732055664 2.987173620611429
CurrentTrain: epoch  5, batch    46 | loss: 10.5895563Losses:  9.808816969394684 1.2319085597991943 1.2154216766357422 1.402208387851715
CurrentTrain: epoch  5, batch    47 | loss: 9.8088170Losses:  11.067153301090002 1.4401899576187134 1.235437273979187 2.865481700748205
CurrentTrain: epoch  5, batch    48 | loss: 11.0671533Losses:  9.224959880113602 1.2315988540649414 1.2076170444488525 1.4074845612049103
CurrentTrain: epoch  5, batch    49 | loss: 9.2249599Losses:  8.863853126764297 1.1188191175460815 1.2081738710403442 1.4025899469852448
CurrentTrain: epoch  5, batch    50 | loss: 8.8638531Losses:  17.334047697484493 1.2067382335662842 1.2497928142547607 10.222960852086544
CurrentTrain: epoch  5, batch    51 | loss: 17.3340477Losses:  11.73383042961359 1.4664714336395264 1.373234510421753 4.305141426622868
CurrentTrain: epoch  5, batch    52 | loss: 11.7338304Losses:  15.544529631733894 1.0638493299484253 1.1711573600769043 8.500124171376228
CurrentTrain: epoch  5, batch    53 | loss: 15.5445296Losses:  8.64480448141694 1.0414522886276245 1.1694014072418213 1.4876551665365696
CurrentTrain: epoch  5, batch    54 | loss: 8.6448045Losses:  9.745273530483246 0.947769820690155 1.3515249490737915 2.8630189299583435
CurrentTrain: epoch  5, batch    55 | loss: 9.7452735Losses:  10.44105851650238 1.0899416208267212 1.2312045097351074 2.832950472831726
CurrentTrain: epoch  5, batch    56 | loss: 10.4410585Losses:  8.411072950810194 1.247804880142212 1.2491616010665894 1.4738867096602917
CurrentTrain: epoch  5, batch    57 | loss: 8.4110730Losses:  12.344282649457455 1.3458571434020996 1.4456603527069092 4.458355449140072
CurrentTrain: epoch  5, batch    58 | loss: 12.3442826Losses:  12.94340968132019 1.2173941135406494 1.1811237335205078 4.908174276351929
CurrentTrain: epoch  5, batch    59 | loss: 12.9434097Losses:  8.684266988188028 1.142985463142395 1.046821117401123 1.566001359373331
CurrentTrain: epoch  5, batch    60 | loss: 8.6842670Losses:  8.638840854167938 1.2789767980575562 1.0896304845809937 1.4345700144767761
CurrentTrain: epoch  5, batch    61 | loss: 8.6388409Losses:  11.912583768367767 1.3916187286376953 1.2138025760650635 4.3133081793785095
CurrentTrain: epoch  5, batch    62 | loss: 11.9125838Losses:  13.860763743519783 1.155870795249939 1.2194178104400635 6.662795260548592
CurrentTrain: epoch  6, batch     0 | loss: 13.8607637Losses:  9.978702012449503 1.199621319770813 1.1435543298721313 2.8349980749189854
CurrentTrain: epoch  6, batch     1 | loss: 9.9787020Losses:  8.739339523017406 1.1045541763305664 1.2884893417358398 1.439376525580883
CurrentTrain: epoch  6, batch     2 | loss: 8.7393395Losses:  8.961318291723728 1.268678903579712 1.2339441776275635 1.456829346716404
CurrentTrain: epoch  6, batch     3 | loss: 8.9613183Losses:  8.356159124523401 1.194684386253357 1.211852788925171 1.4437436200678349
CurrentTrain: epoch  6, batch     4 | loss: 8.3561591Losses:  8.845860466361046 1.1650722026824951 0.9706984758377075 1.5773706287145615
CurrentTrain: epoch  6, batch     5 | loss: 8.8458605Losses:  7.1310930252075195 1.1103074550628662 1.2442845106124878 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 7.1310930Losses:  8.538029942661524 1.2118330001831055 1.2978761196136475 1.4515755511820316
CurrentTrain: epoch  6, batch     7 | loss: 8.5380299Losses:  13.465511441230774 1.0814825296401978 1.1774734258651733 6.228003144264221
CurrentTrain: epoch  6, batch     8 | loss: 13.4655114Losses:  10.023172970861197 1.2712961435317993 1.217968463897705 3.0419360361993313
CurrentTrain: epoch  6, batch     9 | loss: 10.0231730Losses:  11.371420126408339 1.0712249279022217 1.1947224140167236 3.234678488224745
CurrentTrain: epoch  6, batch    10 | loss: 11.3714201Losses:  11.09372990950942 1.3845953941345215 1.3194031715393066 3.051386769860983
CurrentTrain: epoch  6, batch    11 | loss: 11.0937299Losses:  8.942670673131943 1.2192151546478271 1.098984718322754 1.4420870244503021
CurrentTrain: epoch  6, batch    12 | loss: 8.9426707Losses:  8.661492496728897 1.1199764013290405 1.3146759271621704 1.4015046656131744
CurrentTrain: epoch  6, batch    13 | loss: 8.6614925Losses:  8.694839108735323 1.2930330038070679 1.3777713775634766 1.4632407315075397
CurrentTrain: epoch  6, batch    14 | loss: 8.6948391Losses:  10.433357022702694 1.044421672821045 1.198899745941162 3.0292566046118736
CurrentTrain: epoch  6, batch    15 | loss: 10.4333570Losses:  6.837829113006592 1.1613389253616333 1.157382845878601 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 6.8378291Losses:  9.674411803483963 0.9487367868423462 1.0998668670654297 2.9116397202014923
CurrentTrain: epoch  6, batch    17 | loss: 9.6744118Losses:  8.479595478624105 1.1554620265960693 1.1657088994979858 1.5617492757737637
CurrentTrain: epoch  6, batch    18 | loss: 8.4795955Losses:  7.080244064331055 1.1890642642974854 1.1508262157440186 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 7.0802441Losses:  12.050865963101387 1.1699753999710083 1.274043083190918 4.627976253628731
CurrentTrain: epoch  6, batch    20 | loss: 12.0508660Losses:  8.602342665195465 1.2384440898895264 1.1242616176605225 1.4258108735084534
CurrentTrain: epoch  6, batch    21 | loss: 8.6023427Losses:  9.787403233349323 1.2868156433105469 1.141840934753418 2.8634554222226143
CurrentTrain: epoch  6, batch    22 | loss: 9.7874032Losses:  8.504366278648376 0.978281557559967 1.2241612672805786 1.4254173040390015
CurrentTrain: epoch  6, batch    23 | loss: 8.5043663Losses:  11.373693715780973 1.1484639644622803 1.0926527976989746 4.2999751679599285
CurrentTrain: epoch  6, batch    24 | loss: 11.3736937Losses:  10.232848819345236 1.200693130493164 1.3192822933197021 2.9026266895234585
CurrentTrain: epoch  6, batch    25 | loss: 10.2328488Losses:  8.987937696278095 1.3911573886871338 1.3038349151611328 1.457112081348896
CurrentTrain: epoch  6, batch    26 | loss: 8.9879377Losses:  8.377842336893082 1.1048794984817505 1.188377022743225 1.444782167673111
CurrentTrain: epoch  6, batch    27 | loss: 8.3778423Losses:  12.738393846899271 1.1014646291732788 1.2255942821502686 5.84557444229722
CurrentTrain: epoch  6, batch    28 | loss: 12.7383938Losses:  13.031703799962997 1.200850486755371 1.1160739660263062 5.750943511724472
CurrentTrain: epoch  6, batch    29 | loss: 13.0317038Losses:  11.21965605020523 1.1884479522705078 1.2601686716079712 4.231836378574371
CurrentTrain: epoch  6, batch    30 | loss: 11.2196561Losses:  8.561203390359879 1.1854345798492432 1.189509391784668 1.4131292402744293
CurrentTrain: epoch  6, batch    31 | loss: 8.5612034Losses:  10.160434380173683 1.1815226078033447 1.2960067987442017 2.957797184586525
CurrentTrain: epoch  6, batch    32 | loss: 10.1604344Losses:  8.018450558185577 1.0897669792175293 1.0360805988311768 1.399750530719757
CurrentTrain: epoch  6, batch    33 | loss: 8.0184506Losses:  7.1708550453186035 1.032888412475586 1.1118847131729126 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 7.1708550Losses:  10.897091254591942 0.9741134643554688 1.0732953548431396 3.3145097345113754
CurrentTrain: epoch  6, batch    35 | loss: 10.8970913Losses:  10.081867422908545 1.1048970222473145 1.1534956693649292 2.9206711910665035
CurrentTrain: epoch  6, batch    36 | loss: 10.0818674Losses:  7.299647331237793 1.2432246208190918 1.1584599018096924 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.2996473Losses:  14.156542606651783 0.8024277687072754 1.1726665496826172 7.551420517265797
CurrentTrain: epoch  6, batch    38 | loss: 14.1565426Losses:  6.770068168640137 0.973164439201355 1.0400047302246094 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 6.7700682Losses:  7.325327396392822 1.2763817310333252 1.2543940544128418 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 7.3253274Losses:  8.346540778875351 1.143740177154541 1.2876098155975342 1.4024756848812103
CurrentTrain: epoch  6, batch    41 | loss: 8.3465408Losses:  10.662829034030437 1.1543705463409424 1.278452754020691 2.840557686984539
CurrentTrain: epoch  6, batch    42 | loss: 10.6628290Losses:  10.379234194755554 1.2719690799713135 1.2789297103881836 3.0596483945846558
CurrentTrain: epoch  6, batch    43 | loss: 10.3792342Losses:  8.659444242715836 1.264639973640442 1.4509694576263428 1.4601081907749176
CurrentTrain: epoch  6, batch    44 | loss: 8.6594442Losses:  8.637466043233871 1.2676751613616943 1.343930959701538 1.399317353963852
CurrentTrain: epoch  6, batch    45 | loss: 8.6374660Losses:  10.479141652584076 1.1166322231292725 1.187061071395874 2.3127302527427673
CurrentTrain: epoch  6, batch    46 | loss: 10.4791417Losses:  9.657532967627048 0.999665379524231 1.1722102165222168 2.8704188242554665
CurrentTrain: epoch  6, batch    47 | loss: 9.6575330Losses:  8.380570497363806 1.1209200620651245 1.2720650434494019 1.4690047167241573
CurrentTrain: epoch  6, batch    48 | loss: 8.3805705Losses:  8.763800293207169 1.2171963453292847 1.2581455707550049 1.4278055727481842
CurrentTrain: epoch  6, batch    49 | loss: 8.7638003Losses:  8.176998108625412 1.0129215717315674 1.1115211248397827 1.4089417159557343
CurrentTrain: epoch  6, batch    50 | loss: 8.1769981Losses:  7.076786994934082 1.183104395866394 1.2547340393066406 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 7.0767870Losses:  7.023859977722168 0.9244104027748108 1.214921474456787 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 7.0238600Losses:  11.085119303315878 1.0765395164489746 1.2277653217315674 4.376175936311483
CurrentTrain: epoch  6, batch    53 | loss: 11.0851193Losses:  6.71843147277832 1.150578498840332 1.1446266174316406 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 6.7184315Losses:  11.418903313577175 1.2441799640655518 1.4183924198150635 4.292221985757351
CurrentTrain: epoch  6, batch    55 | loss: 11.4189033Losses:  8.199578173458576 0.9619072675704956 1.1892510652542114 1.464498408138752
CurrentTrain: epoch  6, batch    56 | loss: 8.1995782Losses:  9.69466257840395 1.1283923387527466 1.060734748840332 2.925450809299946
CurrentTrain: epoch  6, batch    57 | loss: 9.6946626Losses:  8.394152253866196 1.0879309177398682 1.3356776237487793 1.432725042104721
CurrentTrain: epoch  6, batch    58 | loss: 8.3941523Losses:  8.622276447713375 1.0704073905944824 1.1848571300506592 1.4289647564291954
CurrentTrain: epoch  6, batch    59 | loss: 8.6222764Losses:  6.6796698570251465 1.1171408891677856 1.264388918876648 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 6.6796699Losses:  8.019861705601215 1.0705630779266357 1.253335952758789 1.405776508152485
CurrentTrain: epoch  6, batch    61 | loss: 8.0198617Losses:  10.955358982086182 0.9506576061248779 1.116051197052002 4.249547004699707
CurrentTrain: epoch  6, batch    62 | loss: 10.9553590Losses:  8.357313599437475 1.0325875282287598 1.3129985332489014 1.4514202736318111
CurrentTrain: epoch  7, batch     0 | loss: 8.3573136Losses:  9.4265196621418 0.9029536247253418 1.1158580780029297 2.815809041261673
CurrentTrain: epoch  7, batch     1 | loss: 9.4265197Losses:  6.733937740325928 0.9353506565093994 1.2159513235092163 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 6.7339377Losses:  14.708896443247795 1.2846237421035767 1.2870066165924072 7.646098896861076
CurrentTrain: epoch  7, batch     3 | loss: 14.7088964Losses:  14.037602186203003 0.9952141046524048 1.1976563930511475 7.54874587059021
CurrentTrain: epoch  7, batch     4 | loss: 14.0376022Losses:  9.340783447027206 0.9399697780609131 1.138392448425293 2.802248328924179
CurrentTrain: epoch  7, batch     5 | loss: 9.3407834Losses:  6.684425354003906 0.9424538612365723 1.0242588520050049 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 6.6844254Losses:  12.195804983377457 0.9417738914489746 1.153481364250183 5.651175409555435
CurrentTrain: epoch  7, batch     7 | loss: 12.1958050Losses:  8.07948249578476 0.9937047362327576 1.1970314979553223 1.4333028197288513
CurrentTrain: epoch  7, batch     8 | loss: 8.0794825Losses:  8.332939267158508 1.23567533493042 1.2211954593658447 1.433583378791809
CurrentTrain: epoch  7, batch     9 | loss: 8.3329393Losses:  11.576961815357208 1.0912590026855469 1.3432304859161377 4.4648526310920715
CurrentTrain: epoch  7, batch    10 | loss: 11.5769618Losses:  6.7862772941589355 1.0150282382965088 1.2277626991271973 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 6.7862773Losses:  10.434967935085297 1.0682454109191895 1.1421724557876587 2.8228959441184998
CurrentTrain: epoch  7, batch    12 | loss: 10.4349679Losses:  8.051340848207474 1.055300235748291 1.336530089378357 1.401802808046341
CurrentTrain: epoch  7, batch    13 | loss: 8.0513408Losses:  10.22167119383812 1.2339563369750977 1.2394044399261475 3.35995015501976
CurrentTrain: epoch  7, batch    14 | loss: 10.2216712Losses:  10.094603210687637 1.1064043045043945 1.3813912868499756 2.804544121026993
CurrentTrain: epoch  7, batch    15 | loss: 10.0946032Losses:  14.181300681084394 1.1543060541152954 1.320222020149231 7.257804911583662
CurrentTrain: epoch  7, batch    16 | loss: 14.1813007Losses:  8.210497289896011 1.0895264148712158 1.3564486503601074 1.3976906836032867
CurrentTrain: epoch  7, batch    17 | loss: 8.2104973Losses:  6.816960334777832 1.0132970809936523 1.177283525466919 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 6.8169603Losses:  12.217738799750805 0.9856685400009155 1.1185919046401978 5.844944648444653
CurrentTrain: epoch  7, batch    19 | loss: 12.2177388Losses:  8.408667534589767 1.1749927997589111 1.378184199333191 1.4054746329784393
CurrentTrain: epoch  7, batch    20 | loss: 8.4086675Losses:  10.910811621695757 1.0347435474395752 1.2136130332946777 4.299864489585161
CurrentTrain: epoch  7, batch    21 | loss: 10.9108116Losses:  9.39771881699562 1.0591591596603394 1.210697889328003 2.8066467344760895
CurrentTrain: epoch  7, batch    22 | loss: 9.3977188Losses:  7.210463523864746 1.2169857025146484 1.4521479606628418 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 7.2104635Losses:  7.061781883239746 1.1716325283050537 1.4135921001434326 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 7.0617819Losses:  8.86419465765357 1.206193208694458 1.3392443656921387 1.4374659322202206
CurrentTrain: epoch  7, batch    25 | loss: 8.8641947Losses:  9.931264135986567 1.2424988746643066 1.2334423065185547 2.8489200808107853
CurrentTrain: epoch  7, batch    26 | loss: 9.9312641Losses:  8.036104649305344 1.1573925018310547 1.1010551452636719 1.419906109571457
CurrentTrain: epoch  7, batch    27 | loss: 8.0361046Losses:  9.162515729665756 1.005394697189331 1.1148455142974854 2.7941990792751312
CurrentTrain: epoch  7, batch    28 | loss: 9.1625157Losses:  7.930533409118652 1.083230972290039 1.1310551166534424 1.3952093124389648
CurrentTrain: epoch  7, batch    29 | loss: 7.9305334Losses:  6.340753078460693 1.0883750915527344 0.98127681016922 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 6.3407531Losses:  13.970686197280884 1.3334637880325317 1.3638454675674438 7.071227788925171
CurrentTrain: epoch  7, batch    31 | loss: 13.9706862Losses:  9.297901932150126 1.015186071395874 1.180271029472351 2.8515403904020786
CurrentTrain: epoch  7, batch    32 | loss: 9.2979019Losses:  14.534840352833271 1.3393375873565674 1.183030366897583 7.698230512440205
CurrentTrain: epoch  7, batch    33 | loss: 14.5348404Losses:  10.653106175363064 0.807750940322876 1.3395917415618896 4.214182339608669
CurrentTrain: epoch  7, batch    34 | loss: 10.6531062Losses:  6.252440929412842 0.84589022397995 1.0955711603164673 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 6.2524409Losses:  11.541012790054083 1.0726783275604248 1.1822423934936523 4.340670611709356
CurrentTrain: epoch  7, batch    36 | loss: 11.5410128Losses:  10.008262317627668 1.302647590637207 1.2555238008499146 2.8879038505256176
CurrentTrain: epoch  7, batch    37 | loss: 10.0082623Losses:  9.51446944475174 1.1092896461486816 1.3075106143951416 2.826930344104767
CurrentTrain: epoch  7, batch    38 | loss: 9.5144694Losses:  6.8669962882995605 1.1016557216644287 1.192166805267334 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 6.8669963Losses:  9.507582157850266 1.054093599319458 1.2957758903503418 2.843354195356369
CurrentTrain: epoch  7, batch    40 | loss: 9.5075822Losses:  9.831826031208038 1.3675987720489502 1.2483904361724854 2.8185442090034485
CurrentTrain: epoch  7, batch    41 | loss: 9.8318260Losses:  6.765366554260254 1.306377649307251 1.184010624885559 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 6.7653666Losses:  8.066538393497467 0.9411786794662476 1.1752104759216309 1.4152866005897522
CurrentTrain: epoch  7, batch    43 | loss: 8.0665384Losses:  10.770340718328953 0.9713746309280396 1.0061413049697876 4.379324235022068
CurrentTrain: epoch  7, batch    44 | loss: 10.7703407Losses:  9.401731848716736 0.9987751245498657 1.279822826385498 2.8175166845321655
CurrentTrain: epoch  7, batch    45 | loss: 9.4017318Losses:  12.406769782304764 1.0687199831008911 1.2651667594909668 5.633416682481766
CurrentTrain: epoch  7, batch    46 | loss: 12.4067698Losses:  8.039606720209122 1.0534335374832153 1.3012216091156006 1.4004017412662506
CurrentTrain: epoch  7, batch    47 | loss: 8.0396067Losses:  7.989766716957092 1.0278499126434326 1.1154625415802002 1.4395605325698853
CurrentTrain: epoch  7, batch    48 | loss: 7.9897667Losses:  8.697473496198654 1.3506815433502197 1.301040530204773 1.39800164103508
CurrentTrain: epoch  7, batch    49 | loss: 8.6974735Losses:  7.334013938903809 1.0622122287750244 1.1565790176391602 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 7.3340139Losses:  8.234014179557562 1.0341169834136963 1.237626314163208 1.4235855601727962
CurrentTrain: epoch  7, batch    51 | loss: 8.2340142Losses:  10.886830478906631 1.181746482849121 1.1950393915176392 4.2726947367191315
CurrentTrain: epoch  7, batch    52 | loss: 10.8868305Losses:  7.943999290466309 1.0213758945465088 1.3222386837005615 1.4172039031982422
CurrentTrain: epoch  7, batch    53 | loss: 7.9439993Losses:  10.737919181585312 1.048896312713623 1.1913673877716064 4.240044921636581
CurrentTrain: epoch  7, batch    54 | loss: 10.7379192Losses:  6.78766393661499 1.156837821006775 1.181412696838379 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 6.7876639Losses:  6.939805507659912 1.3620314598083496 1.3667253255844116 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 6.9398055Losses:  6.976223945617676 0.9869683980941772 1.1514670848846436 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 6.9762239Losses:  12.260203305631876 0.9511691331863403 1.1687040328979492 5.882236424833536
CurrentTrain: epoch  7, batch    58 | loss: 12.2602033Losses:  6.340974807739258 0.8613957762718201 1.1974456310272217 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 6.3409748Losses:  8.201405826956034 1.2703018188476562 1.2411037683486938 1.4194467701017857
CurrentTrain: epoch  7, batch    60 | loss: 8.2014058Losses:  8.272301018238068 1.3406765460968018 1.237076997756958 1.411863625049591
CurrentTrain: epoch  7, batch    61 | loss: 8.2723010Losses:  6.2747039794921875 0.8555673360824585 1.1920690536499023 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 6.2747040Losses:  8.235967572778463 1.1606006622314453 1.186946988105774 1.4361686073243618
CurrentTrain: epoch  8, batch     0 | loss: 8.2359676Losses:  9.684460557997227 1.0170652866363525 1.2735544443130493 3.072328008711338
CurrentTrain: epoch  8, batch     1 | loss: 9.6844606Losses:  9.325524032115936 1.0298155546188354 1.1573660373687744 2.9072243571281433
CurrentTrain: epoch  8, batch     2 | loss: 9.3255240Losses:  7.666065484285355 1.0298668146133423 0.9584767818450928 1.427698403596878
CurrentTrain: epoch  8, batch     3 | loss: 7.6660655Losses:  7.977003693580627 1.0197654962539673 1.047478437423706 1.4117532968521118
CurrentTrain: epoch  8, batch     4 | loss: 7.9770037Losses:  6.784434795379639 1.3144292831420898 1.2276538610458374 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 6.7844348Losses:  6.545299053192139 1.0690743923187256 1.1992650032043457 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 6.5452991Losses:  9.54274833202362 1.1216691732406616 1.328196406364441 2.8205350637435913
CurrentTrain: epoch  8, batch     7 | loss: 9.5427483Losses:  9.438323222100735 1.0703980922698975 1.1862456798553467 2.841789446771145
CurrentTrain: epoch  8, batch     8 | loss: 9.4383232Losses:  10.934124439954758 1.2180867195129395 1.3802704811096191 4.204034775495529
CurrentTrain: epoch  8, batch     9 | loss: 10.9341244Losses:  6.732752799987793 1.2771135568618774 1.184828281402588 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 6.7327528Losses:  7.670157693326473 0.9481997489929199 1.0909382104873657 1.4459693655371666
CurrentTrain: epoch  8, batch    11 | loss: 7.6701577Losses:  7.9815628826618195 0.9509669542312622 1.1366539001464844 1.398246556520462
CurrentTrain: epoch  8, batch    12 | loss: 7.9815629Losses:  8.205536387860775 1.2375112771987915 1.2399295568466187 1.499375842511654
CurrentTrain: epoch  8, batch    13 | loss: 8.2055364Losses:  7.749417424201965 0.9425305128097534 1.1505955457687378 1.4001103639602661
CurrentTrain: epoch  8, batch    14 | loss: 7.7494174Losses:  8.440185397863388 1.2126035690307617 1.4412550926208496 1.4130381047725677
CurrentTrain: epoch  8, batch    15 | loss: 8.4401854Losses:  12.067461557686329 1.2673672437667847 1.0096542835235596 4.369023866951466
CurrentTrain: epoch  8, batch    16 | loss: 12.0674616Losses:  6.502068519592285 1.1505342721939087 1.0745391845703125 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 6.5020685Losses:  7.872936725616455 0.8619004487991333 1.2066704034805298 1.537099838256836
CurrentTrain: epoch  8, batch    18 | loss: 7.8729367Losses:  10.542273689061403 0.8480435013771057 1.0993385314941406 4.317222286015749
CurrentTrain: epoch  8, batch    19 | loss: 10.5422737Losses:  6.8985772132873535 1.207165241241455 1.4090566635131836 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 6.8985772Losses:  6.819308757781982 1.1421643495559692 1.365752100944519 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 6.8193088Losses:  12.840890146791935 1.2768781185150146 1.2703429460525513 5.871380068361759
CurrentTrain: epoch  8, batch    22 | loss: 12.8408901Losses:  16.446661181747913 0.9357234239578247 1.277031660079956 9.953067012131214
CurrentTrain: epoch  8, batch    23 | loss: 16.4466612Losses:  6.572025775909424 1.1359100341796875 1.1752163171768188 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 6.5720258Losses:  18.911676838994026 0.9253247380256653 1.1331024169921875 12.717224076390266
CurrentTrain: epoch  8, batch    25 | loss: 18.9116768Losses:  7.811968922615051 0.8789162635803223 1.2796647548675537 1.3937712907791138
CurrentTrain: epoch  8, batch    26 | loss: 7.8119689Losses:  17.282538149505854 1.3609342575073242 1.2790400981903076 10.067075464874506
CurrentTrain: epoch  8, batch    27 | loss: 17.2825381Losses:  6.228861331939697 0.8253790140151978 1.2262754440307617 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 6.2288613Losses:  10.829290300607681 1.0096241235733032 1.2593469619750977 4.2826036512851715
CurrentTrain: epoch  8, batch    29 | loss: 10.8292903Losses:  7.901425123214722 1.0839085578918457 1.1454976797103882 1.4566876888275146
CurrentTrain: epoch  8, batch    30 | loss: 7.9014251Losses:  12.566407985985279 0.8500576019287109 1.1959612369537354 6.2923563197255135
CurrentTrain: epoch  8, batch    31 | loss: 12.5664080Losses:  9.348380506038666 1.169703722000122 1.1497464179992676 2.7841500639915466
CurrentTrain: epoch  8, batch    32 | loss: 9.3483805Losses:  6.256670951843262 0.9302799105644226 1.105175495147705 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 6.2566710Losses:  9.022103186696768 1.2927801609039307 1.2005598545074463 1.4232987128198147
CurrentTrain: epoch  8, batch    34 | loss: 9.0221032Losses:  8.068592764437199 1.2486008405685425 1.1380420923233032 1.4542600885033607
CurrentTrain: epoch  8, batch    35 | loss: 8.0685928Losses:  8.103904843330383 1.212268352508545 1.2026352882385254 1.3940783739089966
CurrentTrain: epoch  8, batch    36 | loss: 8.1039048Losses:  6.330487251281738 0.9383362531661987 1.205871820449829 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.3304873Losses:  15.639954086393118 1.353087067604065 1.3137587308883667 8.659873958677053
CurrentTrain: epoch  8, batch    38 | loss: 15.6399541Losses:  10.920073568820953 1.2288527488708496 1.3329660892486572 4.216162741184235
CurrentTrain: epoch  8, batch    39 | loss: 10.9200736Losses:  7.6470703184604645 0.9304720759391785 1.0879625082015991 1.4028905928134918
CurrentTrain: epoch  8, batch    40 | loss: 7.6470703Losses:  7.909353703260422 1.0044622421264648 1.2192738056182861 1.4086184203624725
CurrentTrain: epoch  8, batch    41 | loss: 7.9093537Losses:  9.34958678483963 1.1214709281921387 1.1156067848205566 2.8387873768806458
CurrentTrain: epoch  8, batch    42 | loss: 9.3495868Losses:  16.075054861605167 1.3594155311584473 1.5386507511138916 9.01339790970087
CurrentTrain: epoch  8, batch    43 | loss: 16.0750549Losses:  7.888386577367783 1.1204016208648682 1.1480815410614014 1.3888987004756927
CurrentTrain: epoch  8, batch    44 | loss: 7.8883866Losses:  7.596068978309631 0.8462448120117188 1.061414122581482 1.3905373811721802
CurrentTrain: epoch  8, batch    45 | loss: 7.5960690Losses:  10.448163352906704 0.7751690149307251 1.1244316101074219 4.401836238801479
CurrentTrain: epoch  8, batch    46 | loss: 10.4481634Losses:  7.569015558809042 0.980542004108429 0.9990886449813843 1.4223142229020596
CurrentTrain: epoch  8, batch    47 | loss: 7.5690156Losses:  7.001211166381836 1.0631420612335205 1.0896508693695068 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 7.0012112Losses:  6.278077125549316 0.8483855724334717 1.2171213626861572 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 6.2780771Losses:  9.545111149549484 1.0852727890014648 1.3868229389190674 2.815614193677902
CurrentTrain: epoch  8, batch    50 | loss: 9.5451111Losses:  7.950544118881226 1.0643067359924316 1.1870659589767456 1.434091329574585
CurrentTrain: epoch  8, batch    51 | loss: 7.9505441Losses:  7.691560506820679 0.8057280778884888 1.1843879222869873 1.4344737529754639
CurrentTrain: epoch  8, batch    52 | loss: 7.6915605Losses:  6.402414798736572 0.9800100326538086 1.2541300058364868 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 6.4024148Losses:  10.585080049932003 0.9889717102050781 0.9686077833175659 4.381654642522335
CurrentTrain: epoch  8, batch    54 | loss: 10.5850800Losses:  7.70080754160881 0.8497024178504944 1.1550016403198242 1.441524475812912
CurrentTrain: epoch  8, batch    55 | loss: 7.7008075Losses:  11.1284144744277 1.2115939855575562 1.2063500881195068 4.488394103944302
CurrentTrain: epoch  8, batch    56 | loss: 11.1284145Losses:  10.409154370427132 0.918595016002655 1.0753562450408936 4.230251744389534
CurrentTrain: epoch  8, batch    57 | loss: 10.4091544Losses:  7.8805937469005585 0.9621206521987915 1.2342547178268433 1.3943838775157928
CurrentTrain: epoch  8, batch    58 | loss: 7.8805937Losses:  7.826139003038406 0.9698795080184937 1.2077090740203857 1.4112334549427032
CurrentTrain: epoch  8, batch    59 | loss: 7.8261390Losses:  6.147351264953613 0.84052574634552 1.1704518795013428 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 6.1473513Losses:  12.52601145207882 1.3864333629608154 1.3282568454742432 5.609339699149132
CurrentTrain: epoch  8, batch    61 | loss: 12.5260115Losses:  8.105074375867844 1.0226472616195679 1.536923885345459 1.4077491462230682
CurrentTrain: epoch  8, batch    62 | loss: 8.1050744Losses:  9.873205434530973 1.1387913227081299 1.288265347480774 3.293961774557829
CurrentTrain: epoch  9, batch     0 | loss: 9.8732054Losses:  6.363988876342773 1.1150939464569092 1.114713430404663 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 6.3639889Losses:  6.278884410858154 0.8765116333961487 1.2062002420425415 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 6.2788844Losses:  9.280020862817764 1.0665593147277832 1.2306747436523438 2.8251821100711823
CurrentTrain: epoch  9, batch     3 | loss: 9.2800209Losses:  9.551603078842163 1.2315342426300049 1.3204536437988281 2.809821844100952
CurrentTrain: epoch  9, batch     4 | loss: 9.5516031Losses:  10.463142074644566 0.8586798906326294 1.1448394060134888 4.270756401121616
CurrentTrain: epoch  9, batch     5 | loss: 10.4631421Losses:  7.658283859491348 0.9836485385894775 1.099823236465454 1.402127891778946
CurrentTrain: epoch  9, batch     6 | loss: 7.6582839Losses:  9.47229915857315 1.2481683492660522 1.2210230827331543 2.8328352570533752
CurrentTrain: epoch  9, batch     7 | loss: 9.4722992Losses:  9.333861380815506 1.0525705814361572 1.34987473487854 2.8008337318897247
CurrentTrain: epoch  9, batch     8 | loss: 9.3338614Losses:  6.505610942840576 1.0284950733184814 1.3593286275863647 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 6.5056109Losses:  9.64513824135065 1.129673957824707 1.4280366897583008 2.960230328142643
CurrentTrain: epoch  9, batch    10 | loss: 9.6451382Losses:  8.704143315553665 1.168809175491333 1.2952736616134644 1.4039542973041534
CurrentTrain: epoch  9, batch    11 | loss: 8.7041433Losses:  9.807365238666534 1.3983720541000366 1.3722565174102783 2.829852879047394
CurrentTrain: epoch  9, batch    12 | loss: 9.8073652Losses:  6.58150577545166 1.1430236101150513 1.2587368488311768 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 6.5815058Losses:  9.017064891755581 0.9324696063995361 1.0461735725402832 2.86305745691061
CurrentTrain: epoch  9, batch    14 | loss: 9.0170649Losses:  10.243506290018559 0.9187766313552856 1.033407211303711 4.2030681148171425
CurrentTrain: epoch  9, batch    15 | loss: 10.2435063Losses:  9.195076704025269 0.8846877813339233 1.2157520055770874 2.854722738265991
CurrentTrain: epoch  9, batch    16 | loss: 9.1950767Losses:  6.371268272399902 1.038172721862793 1.2169930934906006 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 6.3712683Losses:  7.756924957036972 0.9964890480041504 1.1828322410583496 1.4016850888729095
CurrentTrain: epoch  9, batch    18 | loss: 7.7569250Losses:  13.730934225022793 1.1997935771942139 1.2782306671142578 7.133358083665371
CurrentTrain: epoch  9, batch    19 | loss: 13.7309342Losses:  7.800732344388962 1.0412490367889404 1.2377369403839111 1.4071376025676727
CurrentTrain: epoch  9, batch    20 | loss: 7.8007323Losses:  8.132295995950699 0.9603490233421326 1.292280912399292 1.3965992033481598
CurrentTrain: epoch  9, batch    21 | loss: 8.1322960Losses:  6.188495635986328 1.038949966430664 1.0422790050506592 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 6.1884956Losses:  9.35374217107892 1.0215742588043213 1.26151442527771 2.882339049130678
CurrentTrain: epoch  9, batch    23 | loss: 9.3537422Losses:  6.868582725524902 1.0150481462478638 1.1244494915008545 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 6.8685827Losses:  10.549480982124805 1.0374925136566162 1.1291022300720215 4.304258413612843
CurrentTrain: epoch  9, batch    25 | loss: 10.5494810Losses:  6.447527885437012 0.8277331590652466 1.3101871013641357 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 6.4475279Losses:  9.201004207134247 1.0512186288833618 1.2402935028076172 2.823074996471405
CurrentTrain: epoch  9, batch    27 | loss: 9.2010042Losses:  9.348081730306149 1.1840251684188843 1.2056965827941895 2.870917461812496
CurrentTrain: epoch  9, batch    28 | loss: 9.3480817Losses:  9.160636220127344 0.7969222068786621 1.3494513034820557 2.857998166233301
CurrentTrain: epoch  9, batch    29 | loss: 9.1606362Losses:  7.790986564010382 1.0134150981903076 1.1812118291854858 1.4152450822293758
CurrentTrain: epoch  9, batch    30 | loss: 7.7909866Losses:  10.741456508636475 1.115814208984375 1.2636874914169312 4.270272731781006
CurrentTrain: epoch  9, batch    31 | loss: 10.7414565Losses:  15.066626965999603 0.9357386827468872 1.1777160167694092 8.785620152950287
CurrentTrain: epoch  9, batch    32 | loss: 15.0666270Losses:  10.619759939610958 1.0298912525177002 1.2045392990112305 4.287108801305294
CurrentTrain: epoch  9, batch    33 | loss: 10.6197599Losses:  6.451892852783203 1.1542813777923584 1.2146728038787842 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 6.4518929Losses:  12.196193918585777 1.0270237922668457 1.3297064304351807 5.691218599677086
CurrentTrain: epoch  9, batch    35 | loss: 12.1961939Losses:  9.375872552394867 0.9343612790107727 1.5334333181381226 2.7998008131980896
CurrentTrain: epoch  9, batch    36 | loss: 9.3758726Losses:  6.527924060821533 1.11179518699646 1.103905200958252 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.5279241Losses:  7.855617731809616 0.9245941638946533 1.2361631393432617 1.426993578672409
CurrentTrain: epoch  9, batch    38 | loss: 7.8556177Losses:  6.141209602355957 0.8061105012893677 1.1684179306030273 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 6.1412096Losses:  7.723380323499441 0.9965634942054749 1.1480790376663208 1.448592897504568
CurrentTrain: epoch  9, batch    40 | loss: 7.7233803Losses:  6.519393444061279 1.1689643859863281 1.2362117767333984 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 6.5193934Losses:  11.21289324760437 1.2705252170562744 1.2661685943603516 4.230994939804077
CurrentTrain: epoch  9, batch    42 | loss: 11.2128932Losses:  9.189926445484161 0.8806855082511902 1.3777596950531006 2.8231833577156067
CurrentTrain: epoch  9, batch    43 | loss: 9.1899264Losses:  7.7174612283706665 0.8975050449371338 1.2806861400604248 1.399762749671936
CurrentTrain: epoch  9, batch    44 | loss: 7.7174612Losses:  8.12586823105812 1.2781614065170288 1.2704545259475708 1.4222668707370758
CurrentTrain: epoch  9, batch    45 | loss: 8.1258682Losses:  9.556568618863821 1.1236844062805176 1.2736353874206543 2.858901973813772
CurrentTrain: epoch  9, batch    46 | loss: 9.5565686Losses:  9.221056014299393 1.0439187288284302 1.1899099349975586 2.822759658098221
CurrentTrain: epoch  9, batch    47 | loss: 9.2210560Losses:  6.271361827850342 0.9720779061317444 1.1571203470230103 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 6.2713618Losses:  9.146256744861603 0.9208672046661377 1.2068026065826416 2.8183215260505676
CurrentTrain: epoch  9, batch    49 | loss: 9.1462567Losses:  10.560656793415546 0.9176135063171387 1.1758055686950684 4.290107972919941
CurrentTrain: epoch  9, batch    50 | loss: 10.5606568Losses:  6.48996639251709 1.1166354417800903 1.2217848300933838 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 6.4899664Losses:  9.351818859577179 1.0330122709274292 1.3246721029281616 2.8540833592414856
CurrentTrain: epoch  9, batch    52 | loss: 9.3518189Losses:  8.986512877047062 0.885251522064209 1.0828197002410889 2.856653906404972
CurrentTrain: epoch  9, batch    53 | loss: 8.9865129Losses:  9.068935304880142 0.9805070757865906 1.18636155128479 2.819263368844986
CurrentTrain: epoch  9, batch    54 | loss: 9.0689353Losses:  10.597550094127655 0.9309036731719971 1.1717422008514404 4.284928023815155
CurrentTrain: epoch  9, batch    55 | loss: 10.5975501Losses:  10.695221859961748 1.038297176361084 1.143041968345642 4.353764493018389
CurrentTrain: epoch  9, batch    56 | loss: 10.6952219Losses:  15.341973222792149 1.2473992109298706 1.3422472476959229 8.697115816175938
CurrentTrain: epoch  9, batch    57 | loss: 15.3419732Losses:  7.6157176196575165 1.012999415397644 1.0863068103790283 1.4037043750286102
CurrentTrain: epoch  9, batch    58 | loss: 7.6157176Losses:  10.49822124838829 0.8014339208602905 1.2088780403137207 4.198696464300156
CurrentTrain: epoch  9, batch    59 | loss: 10.4982212Losses:  7.685831904411316 0.9772604703903198 1.0922632217407227 1.42379891872406
CurrentTrain: epoch  9, batch    60 | loss: 7.6858319Losses:  11.765145298093557 1.1303606033325195 1.1962623596191406 4.214241024106741
CurrentTrain: epoch  9, batch    61 | loss: 11.7651453Losses:  12.177136480808258 0.8073164224624634 1.326615333557129 5.883568346500397
CurrentTrain: epoch  9, batch    62 | loss: 12.1771365
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  8  clusters
Clusters:  [5 1 0 6 4 2 0 1 1 1 1 1 1 4 7 1 3 4 1 1]
Losses:  10.838106751441956 1.4253406524658203 1.2927749156951904 1.7140451669692993
CurrentTrain: epoch  0, batch     0 | loss: 10.8381068Losses:  15.026332333683968 1.5105617046356201 1.5243430137634277 6.159621670842171
CurrentTrain: epoch  0, batch     1 | loss: 15.0263323Losses:  11.7219697535038 1.4744229316711426 1.1492791175842285 2.9107324182987213
CurrentTrain: epoch  0, batch     2 | loss: 11.7219698Losses:  10.703906700015068 1.2696328163146973 0.7176060676574707 1.4198776930570602
CurrentTrain: epoch  0, batch     3 | loss: 10.7039067Losses:  9.332195281982422 1.4238431453704834 1.3963284492492676 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.3321953Losses:  10.428635060787201 1.4655053615570068 1.364598035812378 1.5812057852745056
CurrentTrain: epoch  1, batch     1 | loss: 10.4286351Losses:  9.327006481587887 1.5462110042572021 1.234169363975525 1.4949728474020958
CurrentTrain: epoch  1, batch     2 | loss: 9.3270065Losses:  10.191163145005703 1.5467400550842285 1.438981533050537 1.4555502757430077
CurrentTrain: epoch  1, batch     3 | loss: 10.1911631Losses:  7.549440383911133 1.3983408212661743 1.2430636882781982 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.5494404Losses:  11.19940185546875 1.4773545265197754 1.4060428142547607 3.06134033203125
CurrentTrain: epoch  2, batch     1 | loss: 11.1994019Losses:  8.172513961791992 1.451647400856018 1.3007628917694092 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 8.1725140Losses:  7.5821433290839195 1.611802577972412 1.5999884605407715 1.4737243875861168
CurrentTrain: epoch  2, batch     3 | loss: 7.5821433Losses:  11.005798935890198 1.3617873191833496 1.1628001928329468 3.1355921030044556
CurrentTrain: epoch  3, batch     0 | loss: 11.0057989Losses:  9.825546380132437 1.4197227954864502 1.4177707433700562 3.025523778051138
CurrentTrain: epoch  3, batch     1 | loss: 9.8255464Losses:  8.674735184758902 1.486918330192566 1.2932201623916626 1.4127517901360989
CurrentTrain: epoch  3, batch     2 | loss: 8.6747352Losses:  8.78216204047203 1.814537525177002 1.4444241523742676 1.4290011823177338
CurrentTrain: epoch  3, batch     3 | loss: 8.7821620Losses:  6.7174506187438965 1.4648613929748535 1.589665412902832 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 6.7174506Losses:  12.087044907733798 1.4834983348846436 1.303782343864441 5.178074074909091
CurrentTrain: epoch  4, batch     1 | loss: 12.0870449Losses:  6.737706184387207 1.4168641567230225 1.2651216983795166 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.7377062Losses:  8.467706959694624 1.2997303009033203 0.8066425323486328 1.5301421098411083
CurrentTrain: epoch  4, batch     3 | loss: 8.4677070Losses:  6.853482246398926 1.4358718395233154 1.4276793003082275 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.8534822Losses:  6.430538177490234 1.4470106363296509 1.3999593257904053 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 6.4305382Losses:  10.690953105688095 1.4118798971176147 1.304052710533142 4.146491378545761
CurrentTrain: epoch  5, batch     2 | loss: 10.6909531Losses:  7.17638086527586 1.2553324699401855 0.9573416709899902 1.4349234029650688
CurrentTrain: epoch  5, batch     3 | loss: 7.1763809Losses:  6.271773815155029 1.5170986652374268 1.395897388458252 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 6.2717738Losses:  6.142883777618408 1.4302175045013428 1.277747631072998 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 6.1428838Losses:  6.037971019744873 1.3772348165512085 1.3050403594970703 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 6.0379710Losses:  6.814175345003605 1.1882143020629883 1.406888484954834 1.5078217759728432
CurrentTrain: epoch  6, batch     3 | loss: 6.8141753Losses:  8.216288972645998 1.4728165864944458 1.2950536012649536 2.100105691701174
CurrentTrain: epoch  7, batch     0 | loss: 8.2162890Losses:  7.106819365173578 1.41891610622406 1.3187966346740723 1.4813339449465275
CurrentTrain: epoch  7, batch     1 | loss: 7.1068194Losses:  6.874989118427038 1.3461098670959473 1.3751513957977295 1.4532800577580929
CurrentTrain: epoch  7, batch     2 | loss: 6.8749891Losses:  5.846915677189827 1.2554435729980469 0.5756235122680664 1.4469675570726395
CurrentTrain: epoch  7, batch     3 | loss: 5.8469157Losses:  9.791200544685125 1.5611172914505005 1.300881266593933 4.283486749976873
CurrentTrain: epoch  8, batch     0 | loss: 9.7912005Losses:  8.395796716213226 1.38970947265625 1.462878704071045 2.8263019919395447
CurrentTrain: epoch  8, batch     1 | loss: 8.3957967Losses:  5.338685035705566 1.3116720914840698 1.3598039150238037 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 5.3386850Losses:  6.282178029417992 1.3371047973632812 1.1114091873168945 1.4223872274160385
CurrentTrain: epoch  8, batch     3 | loss: 6.2821780Losses:  6.796730607748032 1.3829946517944336 1.4249365329742432 1.4019523561000824
CurrentTrain: epoch  9, batch     0 | loss: 6.7967306Losses:  6.62988668680191 1.4384429454803467 1.2808113098144531 1.4364309906959534
CurrentTrain: epoch  9, batch     1 | loss: 6.6298867Losses:  7.152170650660992 1.3916728496551514 1.144087791442871 2.027239315211773
CurrentTrain: epoch  9, batch     2 | loss: 7.1521707Losses:  6.035947240889072 1.1495542526245117 1.239121437072754 1.4877328053116798
CurrentTrain: epoch  9, batch     3 | loss: 6.0359472
Losses:  4.904191017150879 1.1421760320663452 1.3935275077819824 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 4.9041910Losses:  9.911141935735941 1.357589840888977 1.350470781326294 5.804833952337503
MemoryTrain:  epoch  0, batch     1 | loss: 9.9111419Losses:  4.645066261291504 1.068345308303833 1.3446006774902344 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 4.6450663Losses:  9.16968721151352 1.5495227575302124 1.4261811971664429 5.63166469335556
MemoryTrain:  epoch  1, batch     1 | loss: 9.1696872Losses:  3.596757411956787 1.2018051147460938 1.3056272268295288 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.5967574Losses:  10.60681214183569 1.0016114711761475 1.7040746212005615 5.63396992534399
MemoryTrain:  epoch  2, batch     1 | loss: 10.6068121Losses:  3.8757104873657227 1.1403377056121826 1.4541128873825073 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.8757105Losses:  8.65474333241582 1.2978947162628174 1.2792117595672607 5.7884346432983875
MemoryTrain:  epoch  3, batch     1 | loss: 8.6547433Losses:  3.6489675045013428 1.2338851690292358 1.2074954509735107 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 3.6489675Losses:  8.449749637395144 0.9058492183685303 1.379656434059143 5.86394302919507
MemoryTrain:  epoch  4, batch     1 | loss: 8.4497496Losses:  3.1328225135803223 1.124395728111267 1.3919007778167725 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 3.1328225Losses:  10.258507613092661 1.3846200704574585 1.2552670240402222 5.729882124811411
MemoryTrain:  epoch  5, batch     1 | loss: 10.2585076Losses:  3.3361573219299316 1.1306984424591064 1.197861671447754 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 3.3361573Losses:  8.518367618322372 1.3676387071609497 1.3061634302139282 5.662475198507309
MemoryTrain:  epoch  6, batch     1 | loss: 8.5183676Losses:  3.5472824573516846 1.2010090351104736 1.368605136871338 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 3.5472825Losses:  8.155564274638891 1.0002483129501343 1.4106354713439941 5.648801293224096
MemoryTrain:  epoch  7, batch     1 | loss: 8.1555643Losses:  3.3631019592285156 1.1770906448364258 1.183729887008667 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 3.3631020Losses:  8.108396623283625 1.017229437828064 1.2417162656784058 5.712952706962824
MemoryTrain:  epoch  8, batch     1 | loss: 8.1083966Losses:  3.30122447013855 1.1819140911102295 1.1816954612731934 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 3.3012245Losses:  8.579910542815924 1.1120318174362183 1.6620086431503296 5.734365250915289
MemoryTrain:  epoch  9, batch     1 | loss: 8.5799105
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 96.02%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 78.28%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 75.69%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 73.94%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 73.31%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 72.45%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.72%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.74%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.05%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.88%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.99%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 92.85%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.78%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.63%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.41%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 92.03%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.72%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.62%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.50%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.24%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 91.04%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 91.14%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.96%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 90.59%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 90.16%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 89.61%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 89.13%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.79%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 88.27%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.19%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 87.07%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 86.65%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 86.19%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.63%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 85.19%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 84.69%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 84.26%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 83.84%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 83.35%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.55%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.70%   
cur_acc:  ['0.9464', '0.7639']
his_acc:  ['0.9464', '0.8470']
Clustering into  14  clusters
Clusters:  [13  2 11 10  3  1 12  0  6  6  2  0  0  3  9  2  7  1  2  2  0  2  8  2
  4  1  2  5  2  2]
Losses:  9.378551483154297 1.458021640777588 1.3336125612258911 1.5504951477050781
CurrentTrain: epoch  0, batch     0 | loss: 9.3785515Losses:  9.298735618591309 1.1436374187469482 1.1214666366577148 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 9.2987356Losses:  12.406054221093655 1.364454984664917 1.3997693061828613 4.189860068261623
CurrentTrain: epoch  0, batch     2 | loss: 12.4060542Losses:  12.489783611148596 1.2278375625610352 1.1096773147583008 1.6007207296788692
CurrentTrain: epoch  0, batch     3 | loss: 12.4897836Losses:  9.699014157056808 1.315426230430603 1.2654272317886353 1.424798458814621
CurrentTrain: epoch  1, batch     0 | loss: 9.6990142Losses:  15.280449155718088 1.3180111646652222 1.1860860586166382 8.266540292650461
CurrentTrain: epoch  1, batch     1 | loss: 15.2804492Losses:  10.146169662475586 1.2374942302703857 1.2974073886871338 1.4374656677246094
CurrentTrain: epoch  1, batch     2 | loss: 10.1461697Losses:  10.938479140400887 1.4989409446716309 1.3042607307434082 1.4633490592241287
CurrentTrain: epoch  1, batch     3 | loss: 10.9384791Losses:  15.539047837257385 1.297011137008667 1.439353585243225 7.822879910469055
CurrentTrain: epoch  2, batch     0 | loss: 15.5390478Losses:  10.096947073936462 1.3059544563293457 1.225150227546692 2.312749743461609
CurrentTrain: epoch  2, batch     1 | loss: 10.0969471Losses:  8.885798294097185 1.2998085021972656 1.0428845882415771 1.468491394072771
CurrentTrain: epoch  2, batch     2 | loss: 8.8857983Losses:  9.203829489648342 1.2316737174987793 0.8226628303527832 1.499114714562893
CurrentTrain: epoch  2, batch     3 | loss: 9.2038295Losses:  7.320046901702881 1.3735733032226562 1.210421085357666 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.3200469Losses:  10.034548316150904 1.2707879543304443 1.1390795707702637 3.423978839069605
CurrentTrain: epoch  3, batch     1 | loss: 10.0345483Losses:  10.44155215844512 1.197129487991333 1.3288369178771973 2.8597168885171413
CurrentTrain: epoch  3, batch     2 | loss: 10.4415522Losses:  9.95826805755496 1.1326637268066406 1.620863914489746 1.806257139891386
CurrentTrain: epoch  3, batch     3 | loss: 9.9582681Losses:  10.359099313616753 1.2903581857681274 1.1341828107833862 3.0973514765501022
CurrentTrain: epoch  4, batch     0 | loss: 10.3590993Losses:  7.856418818235397 1.2383002042770386 1.1707241535186768 1.4215433299541473
CurrentTrain: epoch  4, batch     1 | loss: 7.8564188Losses:  10.64339479804039 1.2448606491088867 1.2656407356262207 3.147350162267685
CurrentTrain: epoch  4, batch     2 | loss: 10.6433948Losses:  6.945193842053413 1.5851802825927734 1.1083221435546875 1.4933648854494095
CurrentTrain: epoch  4, batch     3 | loss: 6.9451938Losses:  6.924508094787598 1.254659652709961 1.4491982460021973 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.9245081Losses:  7.441258646547794 1.3035199642181396 1.186009407043457 1.4718687310814857
CurrentTrain: epoch  5, batch     1 | loss: 7.4412586Losses:  8.50877445936203 1.2408816814422607 1.1814312934875488 1.4127313494682312
CurrentTrain: epoch  5, batch     2 | loss: 8.5087745Losses:  9.0492612272501 1.325188159942627 1.3074183464050293 1.4670869261026382
CurrentTrain: epoch  5, batch     3 | loss: 9.0492612Losses:  9.18360111117363 1.356776237487793 1.1748812198638916 2.884813040494919
CurrentTrain: epoch  6, batch     0 | loss: 9.1836011Losses:  9.949901919811964 1.1575593948364258 1.3732542991638184 2.940337996929884
CurrentTrain: epoch  6, batch     1 | loss: 9.9499019Losses:  11.22514894604683 1.2425317764282227 1.0259125232696533 4.731110364198685
CurrentTrain: epoch  6, batch     2 | loss: 11.2251489Losses:  8.520752690732479 1.3509931564331055 0.8774223327636719 1.6383350118994713
CurrentTrain: epoch  6, batch     3 | loss: 8.5207527Losses:  5.742487907409668 1.2888431549072266 1.250089168548584 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.7424879Losses:  8.29867273569107 1.2935800552368164 1.2991905212402344 1.4088621735572815
CurrentTrain: epoch  7, batch     1 | loss: 8.2986727Losses:  11.356306057423353 1.1858835220336914 1.2300286293029785 4.764789562672377
CurrentTrain: epoch  7, batch     2 | loss: 11.3563061Losses:  8.230437502264977 1.0461649894714355 0.8174376487731934 1.5916006416082382
CurrentTrain: epoch  7, batch     3 | loss: 8.2304375Losses:  10.817161466926336 1.1760562658309937 1.188220500946045 4.602341081947088
CurrentTrain: epoch  8, batch     0 | loss: 10.8171615Losses:  9.511796656996012 1.231877088546753 1.0955467224121094 2.8740822710096836
CurrentTrain: epoch  8, batch     1 | loss: 9.5117967Losses:  9.769007943570614 1.2698630094528198 1.327870488166809 4.32062079757452
CurrentTrain: epoch  8, batch     2 | loss: 9.7690079Losses:  7.598418831825256 1.3031625747680664 0.8471345901489258 1.395516037940979
CurrentTrain: epoch  8, batch     3 | loss: 7.5984188Losses:  6.0720624923706055 1.1999773979187012 1.1846888065338135 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 6.0720625Losses:  7.11530014872551 1.2870793342590332 1.2847732305526733 1.4020733535289764
CurrentTrain: epoch  9, batch     1 | loss: 7.1153001Losses:  7.882205933332443 1.2114715576171875 0.984357476234436 1.4733910262584686
CurrentTrain: epoch  9, batch     2 | loss: 7.8822059Losses:  5.5960848107934 1.224710464477539 0.8701486587524414 1.4164278283715248
CurrentTrain: epoch  9, batch     3 | loss: 5.5960848
Losses:  3.7428221702575684 1.0802106857299805 1.333113193511963 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.7428222Losses:  3.567598342895508 1.1789069175720215 1.317380428314209 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 3.5675983Losses:  3.475653648376465 1.0068693161010742 1.2029656171798706 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.4756536Losses:  4.177667617797852 1.2905089855194092 1.4335553646087646 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 4.1776676Losses:  4.0272698402404785 1.1107560396194458 1.4550750255584717 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 4.0272698Losses:  2.888730525970459 1.1338269710540771 1.4399663209915161 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.8887305Losses:  3.2095112800598145 1.0779473781585693 1.5633662939071655 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.2095113Losses:  3.058980941772461 1.1491726636886597 1.1615606546401978 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 3.0589809Losses:  2.4845170974731445 1.0113362073898315 1.271073341369629 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.4845171Losses:  3.4885849952697754 1.2407824993133545 1.2839016914367676 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 3.4885850Losses:  2.859252691268921 1.0593408346176147 1.3703844547271729 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.8592527Losses:  3.0038018226623535 1.1754229068756104 1.3998724222183228 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 3.0038018Losses:  3.172595500946045 1.1266472339630127 1.398754358291626 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 3.1725955Losses:  2.6952476501464844 1.1121653318405151 1.4751619100570679 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.6952477Losses:  2.4906630516052246 1.1518089771270752 1.230265736579895 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.4906631Losses:  3.243588447570801 1.0380163192749023 1.5627951622009277 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 3.2435884Losses:  2.5511441230773926 1.088280200958252 1.323331356048584 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.5511441Losses:  2.8463382720947266 1.0822237730026245 1.5110862255096436 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.8463383Losses:  2.449589252471924 0.9254815578460693 1.3818936347961426 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.4495893Losses:  2.4325196743011475 1.216217279434204 1.1389625072479248 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.4325197
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.32%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 70.44%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 69.90%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 69.56%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.15%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 91.27%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.02%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.08%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.19%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.67%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.67%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.71%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.54%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 90.43%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 90.16%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 90.09%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 89.61%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 89.36%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 88.97%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 88.88%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 88.51%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.27%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 88.06%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 87.63%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.23%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 86.78%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 86.39%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 86.08%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 85.84%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 85.73%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 85.50%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 85.27%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 84.99%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 84.71%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.29%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.70%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 83.04%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.45%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 81.76%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 81.19%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.75%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.48%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.50%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 81.20%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 80.42%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 80.04%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 79.62%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 79.15%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.37%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.48%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 79.36%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 79.02%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 78.68%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 78.35%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.10%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 77.73%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 78.39%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 78.02%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 77.78%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.11%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.20%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.73%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 79.01%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.90%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 78.80%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 78.49%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 78.19%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 77.96%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 77.74%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 77.59%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 77.36%   
cur_acc:  ['0.9464', '0.7639', '0.6915']
his_acc:  ['0.9464', '0.8470', '0.7736']
Clustering into  19  clusters
Clusters:  [15  3 13 16  1 18 12  8  2  2  0  8  5  1  3  0 17 14  2  2  6  2 10  2
  9 14  0 11  3  2  2  6  6  0  7  2  5  4  2  3]
Losses:  8.191886901855469 1.4430840015411377 1.2845032215118408 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 8.1918869Losses:  9.420569751411676 1.475730299949646 1.4241712093353271 1.463612888008356
CurrentTrain: epoch  0, batch     1 | loss: 9.4205698Losses:  15.396922584623098 1.462204933166504 1.347529649734497 8.052603241056204
CurrentTrain: epoch  0, batch     2 | loss: 15.3969226Losses:  9.571333151310682 1.655759334564209 1.6014752388000488 1.5299970917403698
CurrentTrain: epoch  0, batch     3 | loss: 9.5713332Losses:  10.778708428144455 1.4513205289840698 1.5885648727416992 2.9436826407909393
CurrentTrain: epoch  1, batch     0 | loss: 10.7787084Losses:  6.873455047607422 1.4651200771331787 1.4372799396514893 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 6.8734550Losses:  7.891583766788244 1.4774715900421143 1.388089656829834 1.4485329054296017
CurrentTrain: epoch  1, batch     2 | loss: 7.8915838Losses:  7.293682813644409 1.3430604934692383 1.040562629699707 1.4234893321990967
CurrentTrain: epoch  1, batch     3 | loss: 7.2936828Losses:  11.15740242600441 1.529167652130127 1.4410688877105713 4.239219099283218
CurrentTrain: epoch  2, batch     0 | loss: 11.1574024Losses:  9.230349127203226 1.3350799083709717 1.2403218746185303 2.8745742477476597
CurrentTrain: epoch  2, batch     1 | loss: 9.2303491Losses:  8.85067505761981 1.417051076889038 1.4507944583892822 2.9168095104396343
CurrentTrain: epoch  2, batch     2 | loss: 8.8506751Losses:  10.492580145597458 0.8268928527832031 1.0 6.635585516691208
CurrentTrain: epoch  2, batch     3 | loss: 10.4925801Losses:  8.00350771099329 1.3167929649353027 1.1555132865905762 2.0101505294442177
CurrentTrain: epoch  3, batch     0 | loss: 8.0035077Losses:  10.329698421061039 1.5309867858886719 1.3463668823242188 4.287987567484379
CurrentTrain: epoch  3, batch     1 | loss: 10.3296984Losses:  8.558920193463564 1.3519287109375 1.5009536743164062 2.928251553326845
CurrentTrain: epoch  3, batch     2 | loss: 8.5589202Losses:  5.911020740866661 0.9694733619689941 1.2534050941467285 1.5108570903539658
CurrentTrain: epoch  3, batch     3 | loss: 5.9110207Losses:  7.303354844450951 1.3645617961883545 1.3210740089416504 1.4787407964468002
CurrentTrain: epoch  4, batch     0 | loss: 7.3033548Losses:  5.881453037261963 1.3622567653656006 1.390184760093689 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 5.8814530Losses:  7.116421163082123 1.388843297958374 1.173377513885498 1.4342994093894958
CurrentTrain: epoch  4, batch     2 | loss: 7.1164212Losses:  5.344808757305145 1.188812255859375 0.6493406295776367 1.423393189907074
CurrentTrain: epoch  4, batch     3 | loss: 5.3448088Losses:  8.077143393456936 1.3142701387405396 1.2598686218261719 2.898322306573391
CurrentTrain: epoch  5, batch     0 | loss: 8.0771434Losses:  5.4498186111450195 1.3733359575271606 1.267625093460083 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.4498186Losses:  7.047408256679773 1.3474302291870117 1.0721465349197388 1.557443294674158
CurrentTrain: epoch  5, batch     2 | loss: 7.0474083Losses:  7.334080219268799 1.3447413444519043 1.8580927848815918 1.5740966796875
CurrentTrain: epoch  5, batch     3 | loss: 7.3340802Losses:  5.54629373550415 1.361875295639038 1.4371752738952637 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.5462937Losses:  6.785879135131836 1.357384443283081 1.286746859550476 1.4163236618041992
CurrentTrain: epoch  6, batch     1 | loss: 6.7858791Losses:  10.953747913241386 1.3024975061416626 0.9236175417900085 5.940832778811455
CurrentTrain: epoch  6, batch     2 | loss: 10.9537479Losses:  6.668689034879208 1.3682737350463867 1.7992095947265625 1.549375794827938
CurrentTrain: epoch  6, batch     3 | loss: 6.6686890Losses:  6.552944533526897 1.2559744119644165 1.170137643814087 1.4311069175601006
CurrentTrain: epoch  7, batch     0 | loss: 6.5529445Losses:  5.246906280517578 1.2722784280776978 1.1632273197174072 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.2469063Losses:  9.430237397551537 1.427282452583313 1.529919981956482 4.198819741606712
CurrentTrain: epoch  7, batch     2 | loss: 9.4302374Losses:  5.909515962004662 1.2201600074768066 1.3183188438415527 1.4980951398611069
CurrentTrain: epoch  7, batch     3 | loss: 5.9095160Losses:  6.266170352697372 1.3383101224899292 1.1245673894882202 1.4089806973934174
CurrentTrain: epoch  8, batch     0 | loss: 6.2661704Losses:  6.287963781505823 1.2390270233154297 1.1977834701538086 1.4813546277582645
CurrentTrain: epoch  8, batch     1 | loss: 6.2879638Losses:  4.487783908843994 1.2529468536376953 1.0391595363616943 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 4.4877839Losses:  6.483555719256401 1.3681817054748535 1.278883457183838 1.450049802660942
CurrentTrain: epoch  8, batch     3 | loss: 6.4835557Losses:  7.893691182136536 1.3153126239776611 1.3633460998535156 2.8794108629226685
CurrentTrain: epoch  9, batch     0 | loss: 7.8936912Losses:  4.727938652038574 1.2147157192230225 1.1833288669586182 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.7279387Losses:  10.187650624662638 1.268430471420288 1.0253267288208008 5.644952241331339
CurrentTrain: epoch  9, batch     2 | loss: 10.1876506Losses:  5.245557151734829 1.2618584632873535 0.7624950408935547 1.4475181922316551
CurrentTrain: epoch  9, batch     3 | loss: 5.2455572
Losses:  3.0927014350891113 0.9814822673797607 1.1277334690093994 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.0927014Losses:  2.8716442584991455 1.1773550510406494 1.1067664623260498 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.8716443Losses:  3.9897966384887695 1.25156569480896 1.180009365081787 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 3.9897966Losses:  3.428020477294922 1.245323896408081 1.4200040102005005 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.4280205Losses:  3.6936898231506348 1.0513865947723389 1.2550017833709717 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.6936898Losses:  3.1519360542297363 0.9146131277084351 1.079846739768982 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 3.1519361Losses:  2.7880611419677734 1.0863664150238037 1.1237519979476929 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.7880611Losses:  2.815166473388672 1.051987886428833 1.2483317852020264 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.8151665Losses:  3.4391798973083496 1.276432991027832 1.4712538719177246 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 3.4391799Losses:  2.7220139503479004 0.9755617380142212 1.497704029083252 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.7220140Losses:  2.521878242492676 1.156564712524414 1.1691527366638184 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.5218782Losses:  3.4093379974365234 1.262308955192566 1.2194795608520508 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 3.4093380Losses:  2.939673900604248 1.188714623451233 1.4243347644805908 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.9396739Losses:  2.3416244983673096 1.0032782554626465 1.0601401329040527 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.3416245Losses:  2.6581668853759766 1.0823251008987427 1.289817452430725 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.6581669Losses:  2.5427803993225098 1.0150216817855835 1.3885115385055542 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.5427804Losses:  2.7972307205200195 1.1330325603485107 1.4078996181488037 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.7972307Losses:  2.617600440979004 1.0802239179611206 1.3005449771881104 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.6176004Losses:  2.430741310119629 1.081600546836853 1.2269819974899292 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.4307413Losses:  2.7582712173461914 1.119802713394165 1.483304738998413 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.7582712Losses:  2.267375946044922 1.0247639417648315 1.1778521537780762 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.2673759Losses:  2.258314371109009 0.928002655506134 1.2353944778442383 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.2583144Losses:  2.4483304023742676 1.0572463274002075 1.2149969339370728 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.4483304Losses:  2.8842997550964355 1.3189674615859985 1.4750722646713257 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.8842998Losses:  2.2911911010742188 1.007904291152954 1.2170429229736328 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.2911911Losses:  2.738409996032715 1.1858594417572021 1.4499964714050293 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.7384100Losses:  2.143359661102295 0.9493685960769653 1.0397100448608398 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.1433597Losses:  2.406313896179199 1.0639899969100952 1.2808692455291748 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.4063139Losses:  2.757953643798828 1.0896632671356201 1.5814869403839111 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.7579536Losses:  2.282784938812256 1.0553297996520996 1.175006628036499 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.2827849
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 50.45%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 54.55%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 52.08%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 51.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 52.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 54.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.76%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 59.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 60.48%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 61.33%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 61.93%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 62.32%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 61.98%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 61.82%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 74.19%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.81%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 81.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.69%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 89.19%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.93%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.87%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.15%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.61%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.69%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.48%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 89.38%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.02%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 88.40%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 88.02%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 87.57%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 87.28%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 86.85%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 86.58%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 86.38%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 86.11%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 86.13%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 85.17%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 84.74%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 84.24%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 83.96%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 83.61%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 83.40%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 83.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.80%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.60%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 82.40%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.21%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 82.08%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 82.02%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 81.60%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 80.90%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 80.28%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.60%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 79.00%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 78.46%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.21%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 79.49%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.12%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.79%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 78.27%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 77.86%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 77.36%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 76.86%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.20%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 76.83%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.51%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.28%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.05%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.78%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 76.49%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 76.15%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 75.98%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 75.65%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 75.48%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 75.59%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 75.74%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 76.22%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.59%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 76.58%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 76.60%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 76.55%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 76.51%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.57%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.49%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 76.34%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 76.23%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.19%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 76.11%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 75.79%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 75.61%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 75.48%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 75.27%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 75.20%   [EVAL] batch:  186 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 75.03%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 75.10%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  194 | acc: 6.25%,  total acc: 74.87%   [EVAL] batch:  195 | acc: 12.50%,  total acc: 74.55%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 73.86%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 73.56%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 73.19%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.20%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 73.27%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 73.31%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 73.25%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 73.02%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 72.88%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 72.65%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 72.48%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 72.23%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 73.02%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  222 | acc: 50.00%,  total acc: 72.87%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 72.85%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 72.78%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 74.19%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 74.27%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 74.25%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 74.28%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.44%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.80%   
cur_acc:  ['0.9464', '0.7639', '0.6915', '0.7381']
his_acc:  ['0.9464', '0.8470', '0.7736', '0.7480']
Clustering into  24  clusters
Clusters:  [19  1 21  8  5 17 16 11 22 22  0 10 10  5 20  0 15  1  2  2  3  2 14  2
  9  1  0 13  1  2  2  4  4  0 23  2  6  7  2  1  1 18 11  2  8 12  2  2
 11  3]
Losses:  10.054847940802574 1.3567909002304077 1.5334419012069702 1.6357290595769882
CurrentTrain: epoch  0, batch     0 | loss: 10.0548479Losses:  14.863002829253674 1.2845962047576904 1.3704187870025635 6.566287092864513
CurrentTrain: epoch  0, batch     1 | loss: 14.8630028Losses:  11.002436570823193 1.3506386280059814 1.312643051147461 2.9405230805277824
CurrentTrain: epoch  0, batch     2 | loss: 11.0024366Losses:  10.228747513145208 1.1414995193481445 1.0759763717651367 1.5246154330670834
CurrentTrain: epoch  0, batch     3 | loss: 10.2287475Losses:  14.223221726715565 1.3633930683135986 1.2927322387695312 7.194153733551502
CurrentTrain: epoch  1, batch     0 | loss: 14.2232217Losses:  7.3257527351379395 1.2263565063476562 1.2026724815368652 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 7.3257527Losses:  8.143595695495605 1.320023775100708 1.270991325378418 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 8.1435957Losses:  8.210335869342089 1.1992192268371582 1.4599900245666504 1.577812809497118
CurrentTrain: epoch  1, batch     3 | loss: 8.2103359Losses:  9.514222241938114 1.2158910036087036 1.2917011976242065 1.525933362543583
CurrentTrain: epoch  2, batch     0 | loss: 9.5142222Losses:  8.214266743510962 1.2909965515136719 1.202303409576416 1.561341729015112
CurrentTrain: epoch  2, batch     1 | loss: 8.2142667Losses:  15.084480501711369 1.2752525806427002 1.2715129852294922 8.912771441042423
CurrentTrain: epoch  2, batch     2 | loss: 15.0844805Losses:  7.879720166325569 1.3680496215820312 1.0019216537475586 1.4522609263658524
CurrentTrain: epoch  2, batch     3 | loss: 7.8797202Losses:  7.293794631958008 1.238432765007019 1.3621597290039062 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.2937946Losses:  7.5382813811302185 1.2519900798797607 1.1367855072021484 1.4246119856834412
CurrentTrain: epoch  3, batch     1 | loss: 7.5382814Losses:  9.498608272522688 1.2533459663391113 1.1274044513702393 3.1545702628791332
CurrentTrain: epoch  3, batch     2 | loss: 9.4986083Losses:  5.915790010243654 1.1846346855163574 0.7363944053649902 1.5347990281879902
CurrentTrain: epoch  3, batch     3 | loss: 5.9157900Losses:  12.675250928848982 1.1779261827468872 1.085997462272644 6.09561912342906
CurrentTrain: epoch  4, batch     0 | loss: 12.6752509Losses:  7.793484311550856 1.2893949747085571 1.2348583936691284 1.5783577971160412
CurrentTrain: epoch  4, batch     1 | loss: 7.7934843Losses:  7.152618106454611 1.2121014595031738 1.1537599563598633 1.4978133998811245
CurrentTrain: epoch  4, batch     2 | loss: 7.1526181Losses:  6.967284105718136 1.0849404335021973 1.1206040382385254 1.4378198608756065
CurrentTrain: epoch  4, batch     3 | loss: 6.9672841Losses:  9.652686595916748 1.1971417665481567 1.2011992931365967 3.139822483062744
CurrentTrain: epoch  5, batch     0 | loss: 9.6526866Losses:  5.297186374664307 1.1932215690612793 0.9661407470703125 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.2971864Losses:  5.621679782867432 1.2135570049285889 1.2267813682556152 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 5.6216798Losses:  5.64848554879427 1.193105697631836 0.9672279357910156 1.4394186809659004
CurrentTrain: epoch  5, batch     3 | loss: 5.6484855Losses:  7.701167847961187 1.115147590637207 0.9809876084327698 1.9205425046384335
CurrentTrain: epoch  6, batch     0 | loss: 7.7011678Losses:  8.398249596357346 1.1693873405456543 1.223562240600586 3.0600156486034393
CurrentTrain: epoch  6, batch     1 | loss: 8.3982496Losses:  7.159123092889786 1.2248789072036743 1.2344542741775513 1.4074551165103912
CurrentTrain: epoch  6, batch     2 | loss: 7.1591231Losses:  6.002559117972851 1.1646513938903809 1.1560568809509277 1.4359144493937492
CurrentTrain: epoch  6, batch     3 | loss: 6.0025591Losses:  8.252624720335007 1.120650053024292 1.066927433013916 2.900893896818161
CurrentTrain: epoch  7, batch     0 | loss: 8.2526247Losses:  5.520136833190918 1.227823257446289 1.1463944911956787 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.5201368Losses:  6.642731070518494 1.1742452383041382 1.0348436832427979 1.4238466024398804
CurrentTrain: epoch  7, batch     2 | loss: 6.6427311Losses:  7.281007267534733 1.1560282707214355 1.0518889427185059 1.4260477796196938
CurrentTrain: epoch  7, batch     3 | loss: 7.2810073Losses:  5.383842468261719 1.0552446842193604 1.1138451099395752 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 5.3838425Losses:  9.937819242477417 1.2565391063690186 1.2310421466827393 4.733810186386108
CurrentTrain: epoch  8, batch     1 | loss: 9.9378192Losses:  8.252216394990683 1.1272175312042236 1.2499446868896484 2.9326601587235928
CurrentTrain: epoch  8, batch     2 | loss: 8.2522164Losses:  6.183748789131641 1.2940406799316406 1.3318510055541992 1.459737367928028
CurrentTrain: epoch  8, batch     3 | loss: 6.1837488Losses:  8.0797202847898 1.1857802867889404 1.2323331832885742 2.903099801391363
CurrentTrain: epoch  9, batch     0 | loss: 8.0797203Losses:  6.4873447977006435 1.103893756866455 1.1314067840576172 1.4337115846574306
CurrentTrain: epoch  9, batch     1 | loss: 6.4873448Losses:  5.188936233520508 1.1567200422286987 1.2025320529937744 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.1889362Losses:  5.805497258901596 0.9892845153808594 0.7274513244628906 1.4787856042385101
CurrentTrain: epoch  9, batch     3 | loss: 5.8054973
Losses:  3.19812273979187 1.0534722805023193 1.3008840084075928 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.1981227Losses:  2.621215343475342 1.0426609516143799 1.1846182346343994 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.6212153Losses:  2.5861153602600098 1.1437034606933594 1.299333095550537 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.5861154Losses:  5.528847784735262 1.320281982421875 1.1176557540893555 2.1036494681611657
MemoryTrain:  epoch  0, batch     3 | loss: 5.5288478Losses:  3.1858034133911133 1.0187294483184814 1.330775499343872 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.1858034Losses:  3.007659912109375 0.973832368850708 1.3691198825836182 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.0076599Losses:  3.1058595180511475 1.2123258113861084 1.2931947708129883 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 3.1058595Losses:  5.848092161118984 1.4202194213867188 1.9626655578613281 1.4484234675765038
MemoryTrain:  epoch  1, batch     3 | loss: 5.8480922Losses:  3.1050875186920166 1.0815935134887695 1.5109742879867554 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.1050875Losses:  2.7072763442993164 1.0479602813720703 1.2499727010726929 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.7072763Losses:  2.4868786334991455 1.0320558547973633 1.1393256187438965 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.4868786Losses:  5.690112076699734 1.7462897300720215 2.4114718437194824 1.4295029267668724
MemoryTrain:  epoch  2, batch     3 | loss: 5.6901121Losses:  2.8947598934173584 1.0300288200378418 1.4266903400421143 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.8947599Losses:  2.282924175262451 0.9335484504699707 1.0897197723388672 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2829242Losses:  2.8443799018859863 1.2220137119293213 1.3809850215911865 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.8443799Losses:  3.442214511334896 1.0731468200683594 0.8478803634643555 1.4343986734747887
MemoryTrain:  epoch  3, batch     3 | loss: 3.4422145Losses:  2.9244813919067383 1.2922486066818237 1.1767269372940063 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.9244814Losses:  2.4842591285705566 0.8245084881782532 1.544008731842041 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.4842591Losses:  2.615673542022705 1.1109209060668945 1.2914268970489502 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.6156735Losses:  3.9214247465133667 1.04410982131958 1.2370209693908691 1.4220036268234253
MemoryTrain:  epoch  4, batch     3 | loss: 3.9214247Losses:  2.406196117401123 1.1234493255615234 1.1511452198028564 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.4061961Losses:  2.4966979026794434 1.0704423189163208 1.208418846130371 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.4966979Losses:  2.430478572845459 0.9990928173065186 1.306753158569336 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.4304786Losses:  5.310633163899183 1.151155948638916 2.4396004676818848 1.6133365444839
MemoryTrain:  epoch  5, batch     3 | loss: 5.3106332Losses:  2.545485258102417 1.0691475868225098 1.3186345100402832 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.5454853Losses:  2.3944756984710693 0.9823249578475952 1.2836518287658691 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.3944757Losses:  2.5192983150482178 1.1251603364944458 1.257601022720337 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.5192983Losses:  3.3474648892879486 0.9468536376953125 0.8274240493774414 1.5444951951503754
MemoryTrain:  epoch  6, batch     3 | loss: 3.3474649Losses:  2.5045528411865234 1.0478531122207642 1.3704357147216797 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.5045528Losses:  2.374683380126953 1.0326308012008667 1.2034854888916016 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.3746834Losses:  2.8404440879821777 1.1274361610412598 1.542564868927002 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.8404441Losses:  4.045037843286991 0.9856476783752441 1.5416383743286133 1.4226428046822548
MemoryTrain:  epoch  7, batch     3 | loss: 4.0450378Losses:  2.517090320587158 1.1565748453140259 1.2649414539337158 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.5170903Losses:  2.5573537349700928 1.1372170448303223 1.300222635269165 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.5573537Losses:  2.2728731632232666 0.8710311055183411 1.3221107721328735 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.2728732Losses:  3.431350663304329 0.9157290458679199 0.9653782844543457 1.4803940802812576
MemoryTrain:  epoch  8, batch     3 | loss: 3.4313507Losses:  2.3214111328125 1.0655823945999146 1.1533534526824951 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.3214111Losses:  2.378040075302124 1.0387736558914185 1.2135828733444214 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.3780401Losses:  2.5049705505371094 0.9562364220619202 1.4344207048416138 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.5049706Losses:  3.9275037422776222 1.359541416168213 0.9738812446594238 1.4422568455338478
MemoryTrain:  epoch  9, batch     3 | loss: 3.9275037
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 63.54%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 61.99%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.91%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.65%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.66%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 85.96%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 85.78%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 85.59%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 85.45%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 85.52%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 85.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 85.77%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 85.91%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 86.03%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 86.14%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 86.53%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.74%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 86.04%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 85.42%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 84.97%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 84.53%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 84.34%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 83.92%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 83.43%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 83.26%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 82.87%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 82.78%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 82.40%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 82.09%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 82.01%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 81.72%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 81.32%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 80.40%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 80.15%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 79.85%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 79.67%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 79.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.15%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 78.76%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 78.61%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.51%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.36%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 78.04%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 77.37%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 76.78%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 76.14%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 75.56%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 75.06%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 74.83%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 76.31%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 75.94%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 75.59%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 75.10%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 74.76%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 74.18%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 73.71%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 74.19%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 73.84%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 73.54%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 73.33%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.12%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.87%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.34%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 73.55%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 73.19%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 72.92%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 72.42%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 72.20%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 72.35%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 72.44%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.76%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 73.14%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 73.24%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 73.43%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 73.41%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 73.38%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 73.34%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 73.26%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 73.20%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 73.01%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 72.86%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 72.70%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 72.68%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 72.59%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 72.67%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 72.69%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 72.63%   [EVAL] batch:  195 | acc: 12.50%,  total acc: 72.32%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 72.02%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 71.65%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 71.36%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 71.00%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 71.14%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 71.21%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 71.04%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.82%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 70.60%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 70.30%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 70.11%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 69.87%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 69.81%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 70.47%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 72.11%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.48%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 72.66%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 72.47%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 72.31%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 72.19%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 71.84%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 71.92%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 71.59%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 71.49%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  276 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 72.10%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 72.11%   [EVAL] batch:  279 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 72.18%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 72.15%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 71.99%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 71.89%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 71.70%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 71.47%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 71.44%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.47%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 71.67%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:  294 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  297 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.08%   
cur_acc:  ['0.9464', '0.7639', '0.6915', '0.7381', '0.7391']
his_acc:  ['0.9464', '0.8470', '0.7736', '0.7480', '0.7308']
Clustering into  29  clusters
Clusters:  [15  2 25  8  3 28 20 12 26 26  0  6  6  3 17  0 18 10  2  2  4  2 19  2
  9 10  0 23  2  2  2  5  5  0 27  2  1 16  2  2  2 21 12  2  8  7  2  2
 12  4 14  2 22  1  2  2 24 11  2 13]
Losses:  8.642558097839355 1.251866340637207 1.174290657043457 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 8.6425581Losses:  10.051151648163795 1.3070331811904907 1.3431007862091064 1.6027034670114517
CurrentTrain: epoch  0, batch     1 | loss: 10.0511516Losses:  10.27760785445571 1.407365322113037 1.3166913986206055 1.6619061790406704
CurrentTrain: epoch  0, batch     2 | loss: 10.2776079Losses:  11.74380531348288 1.2399110794067383 1.394658088684082 1.8612932208925486
CurrentTrain: epoch  0, batch     3 | loss: 11.7438053Losses:  9.035917717963457 1.2644761800765991 1.1975241899490356 1.442165333777666
CurrentTrain: epoch  1, batch     0 | loss: 9.0359177Losses:  9.547218203544617 1.2824456691741943 1.2904841899871826 1.6451300382614136
CurrentTrain: epoch  1, batch     1 | loss: 9.5472182Losses:  6.855983734130859 1.2686572074890137 1.1574422121047974 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 6.8559837Losses:  7.917480647563934 1.2842583656311035 1.070295810699463 1.5459277033805847
CurrentTrain: epoch  1, batch     3 | loss: 7.9174806Losses:  8.174510881304741 1.2252415418624878 1.1904304027557373 1.5229863375425339
CurrentTrain: epoch  2, batch     0 | loss: 8.1745109Losses:  6.853318691253662 1.233647108078003 0.9980396628379822 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 6.8533187Losses:  9.821129031479359 1.2589497566223145 1.1544086933135986 3.0038701966404915
CurrentTrain: epoch  2, batch     2 | loss: 9.8211290Losses:  14.10691013932228 1.0 0.9428586959838867 8.978005796670914
CurrentTrain: epoch  2, batch     3 | loss: 14.1069101Losses:  11.401732943952084 1.1974551677703857 1.3298747539520264 4.748262904584408
CurrentTrain: epoch  3, batch     0 | loss: 11.4017329Losses:  7.986830197274685 1.2149338722229004 1.062826156616211 1.778219185769558
CurrentTrain: epoch  3, batch     1 | loss: 7.9868302Losses:  8.008366983383894 1.1721160411834717 1.0079987049102783 1.7051471881568432
CurrentTrain: epoch  3, batch     2 | loss: 8.0083670Losses:  9.180122785270214 1.192615032196045 1.5973153114318848 1.4359940811991692
CurrentTrain: epoch  3, batch     3 | loss: 9.1801228Losses:  7.594487391412258 1.220731258392334 1.1692912578582764 1.52000542730093
CurrentTrain: epoch  4, batch     0 | loss: 7.5944874Losses:  6.663438320159912 1.1296041011810303 1.11942720413208 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 6.6634383Losses:  7.772814519703388 1.208341121673584 1.110727310180664 1.5115916803479195
CurrentTrain: epoch  4, batch     2 | loss: 7.7728145Losses:  5.370132647454739 1.3400325775146484 0.6604061126708984 1.4419593438506126
CurrentTrain: epoch  4, batch     3 | loss: 5.3701326Losses:  14.105177700519562 1.286208987236023 1.2004395723342896 8.752209961414337
CurrentTrain: epoch  5, batch     0 | loss: 14.1051777Losses:  7.264381378889084 1.121742606163025 0.888024091720581 1.5895461738109589
CurrentTrain: epoch  5, batch     1 | loss: 7.2643814Losses:  12.951693519949913 1.1531656980514526 1.3226805925369263 6.044703468680382
CurrentTrain: epoch  5, batch     2 | loss: 12.9516935Losses:  6.0202565267682076 1.0109834671020508 1.2050342559814453 1.5645318105816841
CurrentTrain: epoch  5, batch     3 | loss: 6.0202565Losses:  8.605899602174759 1.2137501239776611 1.300478219985962 2.9979350864887238
CurrentTrain: epoch  6, batch     0 | loss: 8.6058996Losses:  9.026117444038391 1.111676812171936 1.1443525552749634 2.87626850605011
CurrentTrain: epoch  6, batch     1 | loss: 9.0261174Losses:  7.086406998336315 1.1240103244781494 1.1102044582366943 1.4343712851405144
CurrentTrain: epoch  6, batch     2 | loss: 7.0864070Losses:  7.408152714371681 1.146010398864746 1.2396488189697266 1.4574114233255386
CurrentTrain: epoch  6, batch     3 | loss: 7.4081527Losses:  8.855632424354553 0.9877147674560547 1.049863576889038 2.8547884225845337
CurrentTrain: epoch  7, batch     0 | loss: 8.8556324Losses:  8.551233232021332 1.203404426574707 1.2685590982437134 2.8903068900108337
CurrentTrain: epoch  7, batch     1 | loss: 8.5512332Losses:  11.53464575484395 1.1992573738098145 1.1799612045288086 5.974086482077837
CurrentTrain: epoch  7, batch     2 | loss: 11.5346458Losses:  5.847374901175499 1.2028369903564453 1.0242090225219727 1.4383554309606552
CurrentTrain: epoch  7, batch     3 | loss: 5.8473749Losses:  7.025918785482645 1.0466585159301758 1.0074715614318848 1.427509132772684
CurrentTrain: epoch  8, batch     0 | loss: 7.0259188Losses:  8.352477252483368 1.0924251079559326 1.214231014251709 2.8556572794914246
CurrentTrain: epoch  8, batch     1 | loss: 8.3524773Losses:  8.072826776653528 1.1432607173919678 0.9717330932617188 2.895963106304407
CurrentTrain: epoch  8, batch     2 | loss: 8.0728268Losses:  5.821087472140789 1.3449444770812988 0.9643864631652832 1.4015690013766289
CurrentTrain: epoch  8, batch     3 | loss: 5.8210875Losses:  8.1805578507483 1.1859341859817505 1.1241804361343384 2.940415982156992
CurrentTrain: epoch  9, batch     0 | loss: 8.1805579Losses:  6.654017053544521 1.1375877857208252 1.2259268760681152 1.4186020717024803
CurrentTrain: epoch  9, batch     1 | loss: 6.6540171Losses:  5.47706413269043 0.9925245642662048 1.1383484601974487 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.4770641Losses:  10.334922567009926 1.0 1.2624731063842773 6.257635846734047
CurrentTrain: epoch  9, batch     3 | loss: 10.3349226
Losses:  3.3608450889587402 1.1612600088119507 1.2308127880096436 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.3608451Losses:  2.6554505825042725 1.0329296588897705 1.155846357345581 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.6554506Losses:  3.358116388320923 0.9183080792427063 1.5397601127624512 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 3.3581164Losses:  3.2060647010803223 1.2265965938568115 1.4618353843688965 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 3.2060647Losses:  3.7208003997802734 0.981160044670105 1.4469194412231445 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.7208004Losses:  3.823698043823242 1.0103440284729004 1.3846689462661743 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.8236980Losses:  3.293813943862915 1.2699992656707764 1.565310001373291 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 3.2938139Losses:  3.0238759517669678 1.037334680557251 1.1357437372207642 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 3.0238760Losses:  3.0582940578460693 1.156454086303711 1.3627524375915527 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.0582941Losses:  3.117884635925293 1.0555706024169922 1.3865137100219727 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 3.1178846Losses:  2.721470832824707 0.9928512573242188 1.3835151195526123 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.7214708Losses:  3.148711919784546 1.088113784790039 1.3110358715057373 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 3.1487119Losses:  2.4362306594848633 0.874484121799469 1.2382440567016602 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.4362307Losses:  2.28715181350708 0.8490296602249146 1.1150048971176147 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2871518Losses:  3.168576717376709 1.363620638847351 1.4226595163345337 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 3.1685767Losses:  2.9949769973754883 1.171631932258606 1.6220381259918213 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.9949770Losses:  2.656022548675537 0.9661561846733093 1.4312433004379272 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.6560225Losses:  2.450622081756592 1.0238561630249023 1.1783740520477295 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.4506221Losses:  2.544114112854004 1.1697165966033936 1.267702579498291 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.5441141Losses:  2.5313808917999268 0.9979899525642395 1.3219568729400635 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.5313809Losses:  2.526428461074829 1.0454274415969849 1.2078261375427246 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.5264285Losses:  2.335414409637451 1.0762748718261719 1.1371302604675293 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.3354144Losses:  2.38144588470459 0.9513106346130371 1.3483796119689941 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.3814459Losses:  2.64062762260437 1.1206499338150024 1.2644212245941162 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.6406276Losses:  2.2914648056030273 0.9971246719360352 1.1066267490386963 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.2914648Losses:  2.4852283000946045 1.1483255624771118 1.216812252998352 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.4852283Losses:  2.387566089630127 0.9152315855026245 1.3653534650802612 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.3875661Losses:  2.9595022201538086 1.2057853937149048 1.624656319618225 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 2.9595022Losses:  2.2263009548187256 0.9248706102371216 1.155440330505371 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.2263010Losses:  2.486212968826294 1.0294315814971924 1.3885607719421387 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.4862130Losses:  2.649214506149292 1.1768155097961426 1.4083514213562012 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.6492145Losses:  2.2087178230285645 1.0553864240646362 1.0653430223464966 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 2.2087178Losses:  2.5286288261413574 1.0371878147125244 1.4077874422073364 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.5286288Losses:  2.2729973793029785 1.0448312759399414 1.134596824645996 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.2729974Losses:  2.448105573654175 0.9616646766662598 1.3640422821044922 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.4481056Losses:  2.325160503387451 1.0438134670257568 1.2238049507141113 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 2.3251605Losses:  2.6448516845703125 1.1957885026931763 1.3621532917022705 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.6448517Losses:  2.2837109565734863 0.9616180062294006 1.2747516632080078 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.2837110Losses:  2.487250804901123 1.0588946342468262 1.349581003189087 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.4872508Losses:  2.063948631286621 0.830592930316925 1.1378624439239502 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 2.0639486
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 73.24%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 70.91%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 69.07%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 68.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 82.65%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.37%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 83.30%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 82.44%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 80.76%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 80.78%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 80.69%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 81.08%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 80.52%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 79.97%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 79.59%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 79.30%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 79.01%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 78.66%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 78.24%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 77.79%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 77.83%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 77.51%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 77.18%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 76.61%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 76.20%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 75.79%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 75.39%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 75.19%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 74.87%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 74.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.50%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 74.15%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 73.93%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.82%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.54%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 72.92%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.42%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 71.82%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.28%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 71.95%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 71.43%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 71.01%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 70.51%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 70.16%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 69.13%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 69.55%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.92%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 68.66%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 69.54%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 69.20%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 68.59%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 68.39%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 68.07%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 68.83%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 68.53%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 68.24%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 67.81%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 67.62%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 67.55%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 67.51%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 67.45%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 67.52%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 67.56%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 67.63%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 67.98%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 67.28%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 66.94%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 66.60%   [EVAL] batch:  198 | acc: 6.25%,  total acc: 66.30%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 65.97%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 66.06%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 65.84%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 65.61%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 65.36%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 65.17%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 64.95%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 64.91%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 65.75%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 67.54%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 68.45%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 68.11%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 67.82%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 67.84%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 67.79%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 67.43%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 67.35%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 67.24%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 67.93%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 67.90%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 67.88%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 67.78%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.56%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.24%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 66.95%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 67.12%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 67.24%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 67.26%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 67.29%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 67.32%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 68.41%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 68.27%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 68.06%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 67.94%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 67.89%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 67.93%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.03%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 67.95%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 67.80%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 68.95%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 68.71%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 68.54%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 68.43%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 68.89%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 68.78%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 68.72%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.58%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 68.57%   
cur_acc:  ['0.9464', '0.7639', '0.6915', '0.7381', '0.7391', '0.6825']
his_acc:  ['0.9464', '0.8470', '0.7736', '0.7480', '0.7308', '0.6857']
Clustering into  34  clusters
Clusters:  [31  1 19 33 32 21 20  6  4  4  3  5  5 26 25 27 17  1  0  0 15  0 18  0
  9  1  3 23  0  0  0  2  2 29 30  0 14 12  0  1  1 24  6  0  0 16  7  0
  6  7 28  0  8 22  0  0 13 11  0 10  1  0  3  0  1  0  7  0  7  0]
Losses:  12.312569305300713 1.4036736488342285 1.453163981437683 3.184407874941826
CurrentTrain: epoch  0, batch     0 | loss: 12.3125693Losses:  11.447828971315175 1.35564386844635 1.3559229373931885 2.241222106385976
CurrentTrain: epoch  0, batch     1 | loss: 11.4478290Losses:  14.503743253648281 1.2389934062957764 1.046060562133789 5.8375502452254295
CurrentTrain: epoch  0, batch     2 | loss: 14.5037433Losses:  14.23667980544269 1.6652860641479492 1.5249156951904297 2.0229509007185698
CurrentTrain: epoch  0, batch     3 | loss: 14.2366798Losses:  9.732126474380493 1.348203182220459 1.3629074096679688 1.4297239780426025
CurrentTrain: epoch  1, batch     0 | loss: 9.7321265Losses:  8.46678638458252 1.3871400356292725 1.4693024158477783 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 8.4667864Losses:  9.538936611264944 1.256966471672058 1.186199426651001 1.6791524849832058
CurrentTrain: epoch  1, batch     2 | loss: 9.5389366Losses:  8.85597425699234 1.4792933464050293 1.2261109352111816 1.413185179233551
CurrentTrain: epoch  1, batch     3 | loss: 8.8559743Losses:  10.547729093581438 1.2814948558807373 1.2460660934448242 3.2628436870872974
CurrentTrain: epoch  2, batch     0 | loss: 10.5477291Losses:  9.33190667629242 1.3166754245758057 1.2650783061981201 1.4346765279769897
CurrentTrain: epoch  2, batch     1 | loss: 9.3319067Losses:  10.090968664735556 1.3433539867401123 1.4352596998214722 1.4825435243546963
CurrentTrain: epoch  2, batch     2 | loss: 10.0909687Losses:  10.429700255393982 1.6270232200622559 1.1844115257263184 1.3911861181259155
CurrentTrain: epoch  2, batch     3 | loss: 10.4297003Losses:  7.641595840454102 1.3751482963562012 1.4245966672897339 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.6415958Losses:  13.507949590682983 1.293661117553711 1.3402516841888428 5.128185987472534
CurrentTrain: epoch  3, batch     1 | loss: 13.5079496Losses:  8.99944855645299 1.2383487224578857 1.3736557960510254 1.7767851538956165
CurrentTrain: epoch  3, batch     2 | loss: 8.9994486Losses:  11.832071118056774 1.6309914588928223 1.3527379035949707 1.5079935118556023
CurrentTrain: epoch  3, batch     3 | loss: 11.8320711Losses:  11.436688613146544 1.395503044128418 1.5967050790786743 3.194932173937559
CurrentTrain: epoch  4, batch     0 | loss: 11.4366886Losses:  8.702928952872753 1.2416534423828125 1.2674670219421387 1.7829112336039543
CurrentTrain: epoch  4, batch     1 | loss: 8.7029290Losses:  7.784141540527344 1.2776877880096436 1.2890386581420898 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 7.7841415Losses:  7.0435614585876465 1.2672843933105469 0.783503532409668 1.5917448997497559
CurrentTrain: epoch  4, batch     3 | loss: 7.0435615Losses:  7.634350776672363 1.363825798034668 1.3703606128692627 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.6343508Losses:  12.107237443327904 1.3419092893600464 1.375536322593689 4.660449609160423
CurrentTrain: epoch  5, batch     1 | loss: 12.1072374Losses:  6.876325607299805 1.2009624242782593 1.1608495712280273 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 6.8763256Losses:  8.823507957160473 1.1333093643188477 1.0311470031738281 1.4589926525950432
CurrentTrain: epoch  5, batch     3 | loss: 8.8235080Losses:  10.433769594877958 1.2616662979125977 1.3995206356048584 3.2387603633105755
CurrentTrain: epoch  6, batch     0 | loss: 10.4337696Losses:  17.689433608204126 1.446657657623291 1.5086790323257446 9.842003379017115
CurrentTrain: epoch  6, batch     1 | loss: 17.6894336Losses:  12.681611061096191 1.196274995803833 1.2270643711090088 5.97426700592041
CurrentTrain: epoch  6, batch     2 | loss: 12.6816111Losses:  7.84207621216774 1.2140703201293945 0.9391231536865234 1.4129108488559723
CurrentTrain: epoch  6, batch     3 | loss: 7.8420762Losses:  9.537431426346302 1.2358514070510864 1.145992636680603 3.0404769852757454
CurrentTrain: epoch  7, batch     0 | loss: 9.5374314Losses:  8.723861780017614 1.2817941904067993 1.337770700454712 1.5491763018071651
CurrentTrain: epoch  7, batch     1 | loss: 8.7238618Losses:  10.162634678184986 1.3553451299667358 1.5668760538101196 3.3720687106251717
CurrentTrain: epoch  7, batch     2 | loss: 10.1626347Losses:  7.653660759329796 1.0335721969604492 0.9909601211547852 1.5609426349401474
CurrentTrain: epoch  7, batch     3 | loss: 7.6536608Losses:  9.790553897619247 1.2166647911071777 1.3527038097381592 2.7812508046627045
CurrentTrain: epoch  8, batch     0 | loss: 9.7905539Losses:  9.173198785632849 1.20115327835083 1.2592313289642334 2.888527002185583
CurrentTrain: epoch  8, batch     1 | loss: 9.1731988Losses:  14.619710266590118 1.3399457931518555 1.4351449012756348 8.146158993244171
CurrentTrain: epoch  8, batch     2 | loss: 14.6197103Losses:  7.982330277562141 1.3785810470581055 1.5013599395751953 1.5047649890184402
CurrentTrain: epoch  8, batch     3 | loss: 7.9823303Losses:  6.361117362976074 1.2292721271514893 1.2790601253509521 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 6.3611174Losses:  6.325366973876953 1.3168718814849854 1.4167616367340088 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 6.3253670Losses:  8.228521171957254 1.2276270389556885 1.2709617614746094 1.4517276920378208
CurrentTrain: epoch  9, batch     2 | loss: 8.2285212Losses:  7.382471643388271 1.2160334587097168 1.8302264213562012 1.4347258433699608
CurrentTrain: epoch  9, batch     3 | loss: 7.3824716
Losses:  2.762341022491455 1.0793628692626953 1.2905166149139404 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.7623410Losses:  3.326700210571289 1.1452847719192505 1.4298007488250732 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 3.3267002Losses:  2.5756125450134277 0.9648737907409668 1.4642713069915771 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.5756125Losses:  2.526395320892334 0.9829341769218445 1.321828842163086 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 2.5263953Losses:  7.364551160484552 1.0735187530517578 1.4374727010726929 4.288990829139948
MemoryTrain:  epoch  0, batch     4 | loss: 7.3645512Losses:  2.9833626747131348 1.0223051309585571 1.2789669036865234 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.9833627Losses:  2.975541114807129 1.1544854640960693 1.3579270839691162 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.9755411Losses:  2.8483986854553223 0.914214015007019 1.6097673177719116 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.8483987Losses:  3.1605634689331055 1.0546298027038574 1.5100908279418945 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 3.1605635Losses:  6.88494249433279 1.0286651849746704 1.0424717664718628 4.312911473214626
MemoryTrain:  epoch  1, batch     4 | loss: 6.8849425Losses:  2.506535053253174 0.9663437604904175 1.3348987102508545 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.5065351Losses:  2.6423730850219727 0.8780343532562256 1.3029320240020752 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.6423731Losses:  2.6494574546813965 1.0726604461669922 1.276764988899231 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.6494575Losses:  2.6225271224975586 1.1061811447143555 1.118431806564331 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 2.6225271Losses:  6.967976927757263 1.1237640380859375 1.3980152606964111 4.262670397758484
MemoryTrain:  epoch  2, batch     4 | loss: 6.9679769Losses:  2.528468132019043 0.9242720007896423 1.32472562789917 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.5284681Losses:  2.757998466491699 1.1698614358901978 1.3996824026107788 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.7579985Losses:  2.410930871963501 1.0471659898757935 1.085564374923706 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.4109309Losses:  2.6564183235168457 0.8908949494361877 1.5286238193511963 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.6564183Losses:  7.370840173214674 1.2701481580734253 1.5671650171279907 4.330531697720289
MemoryTrain:  epoch  3, batch     4 | loss: 7.3708402Losses:  2.3659849166870117 0.9937422871589661 1.180158257484436 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.3659849Losses:  2.293043613433838 0.8648279905319214 1.2363654375076294 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.2930436Losses:  2.584454298019409 1.105713963508606 1.2997441291809082 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.5844543Losses:  2.564600706100464 1.0484063625335693 1.3941848278045654 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.5646007Losses:  7.086509980261326 1.1440067291259766 1.569510579109192 4.236067093908787
MemoryTrain:  epoch  4, batch     4 | loss: 7.0865100Losses:  2.2974987030029297 0.8431217670440674 1.3308038711547852 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.2974987Losses:  2.546027183532715 1.0310888290405273 1.3793452978134155 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.5460272Losses:  2.6679863929748535 1.0937881469726562 1.4854347705841064 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.6679864Losses:  2.513333797454834 1.0648144483566284 1.2926658391952515 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.5133338Losses:  7.28577397018671 1.0052152872085571 1.6873400211334229 4.405051447451115
MemoryTrain:  epoch  5, batch     4 | loss: 7.2857740Losses:  2.25459623336792 0.8883881568908691 1.2283453941345215 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.2545962Losses:  2.4146647453308105 1.0017369985580444 1.32436203956604 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.4146647Losses:  2.4234046936035156 1.0937535762786865 1.2206451892852783 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.4234047Losses:  2.5074830055236816 0.9751091003417969 1.4043614864349365 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 2.5074830Losses:  6.664223276078701 1.078957438468933 1.2089844942092896 4.302551351487637
MemoryTrain:  epoch  6, batch     4 | loss: 6.6642233Losses:  2.394317626953125 0.9655239582061768 1.3272532224655151 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.3943176Losses:  2.5629653930664062 1.0869112014770508 1.3699960708618164 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.5629654Losses:  2.6154186725616455 1.1069166660308838 1.4257984161376953 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.6154187Losses:  2.1104800701141357 0.804485559463501 1.2318617105484009 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 2.1104801Losses:  6.90652633830905 1.1220593452453613 1.216672658920288 4.531651269644499
MemoryTrain:  epoch  7, batch     4 | loss: 6.9065263Losses:  2.2242934703826904 0.8525327444076538 1.312508225440979 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.2242935Losses:  2.1929216384887695 0.848015546798706 1.2625508308410645 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.1929216Losses:  2.8339452743530273 1.246274709701538 1.4793399572372437 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.8339453Losses:  2.4153826236724854 0.9422139525413513 1.3692493438720703 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 2.4153826Losses:  6.772880997508764 1.0562547445297241 1.3364919424057007 4.320325579494238
MemoryTrain:  epoch  8, batch     4 | loss: 6.7728810Losses:  2.3323793411254883 1.0026006698608398 1.2351378202438354 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.3323793Losses:  2.3458762168884277 0.9254138469696045 1.3361485004425049 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.3458762Losses:  2.598453998565674 1.1112109422683716 1.4062072038650513 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.5984540Losses:  2.3400721549987793 0.9183332920074463 1.3276320695877075 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 2.3400722Losses:  6.366368655115366 0.7955112457275391 1.2424441576004028 4.263339880853891
MemoryTrain:  epoch  9, batch     4 | loss: 6.3663687
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 58.59%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 57.75%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 55.53%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 53.70%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 52.01%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 50.65%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 49.17%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 47.78%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 51.29%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 52.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 53.82%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 54.73%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 55.76%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 56.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 57.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 58.69%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 59.23%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 61.53%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 61.68%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 62.76%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 62.14%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 61.79%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 61.70%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 61.73%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 61.12%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 61.19%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 60.52%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 82.95%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.79%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 82.44%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.98%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 81.45%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 81.15%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 81.16%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 81.69%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 82.17%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 81.83%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 80.77%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 80.46%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 80.23%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 79.94%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 79.73%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 79.37%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 79.32%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 79.04%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 79.07%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 78.66%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 78.48%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 78.23%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 77.99%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 77.38%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 77.02%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 76.60%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 76.18%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 75.65%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 75.39%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 75.06%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 74.87%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 74.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 74.33%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 74.09%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 73.98%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 73.87%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 73.54%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 72.92%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 72.36%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 71.76%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.23%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.81%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 70.63%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 71.90%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 71.90%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 71.23%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 70.77%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 70.21%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 69.72%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 69.23%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 68.70%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.88%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.47%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.71%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.53%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 69.08%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 68.67%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 68.34%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 67.98%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 67.70%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 67.35%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:  157 | acc: 31.25%,  total acc: 67.09%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 67.02%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:  160 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:  162 | acc: 18.75%,  total acc: 66.22%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 65.89%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 65.10%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 64.71%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 64.32%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 64.05%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 63.93%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 64.00%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 63.81%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 63.69%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 63.61%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 63.52%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 63.38%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 63.44%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 63.36%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 63.32%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 63.49%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 63.96%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 64.21%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 63.88%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 63.55%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 63.23%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 62.91%   [EVAL] batch:  198 | acc: 0.00%,  total acc: 62.59%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 62.28%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 62.28%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 62.38%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 62.44%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 62.23%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 61.99%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 61.73%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 61.49%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 61.23%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 61.21%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.39%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 61.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 62.24%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 62.39%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 62.42%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 62.39%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 63.05%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  230 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 64.70%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 64.92%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.22%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 65.56%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 65.24%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 65.11%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 64.95%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 64.78%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.77%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 64.73%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 64.70%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 64.60%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 64.45%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 64.37%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 64.30%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 64.31%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 64.92%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 64.98%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 64.87%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 64.82%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 64.57%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 64.45%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 64.29%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 64.07%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 63.89%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 63.78%   [EVAL] batch:  289 | acc: 31.25%,  total acc: 63.66%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 63.55%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 63.42%   [EVAL] batch:  292 | acc: 25.00%,  total acc: 63.29%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 63.20%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 63.23%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 63.29%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.18%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 64.33%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 64.41%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 64.51%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 64.46%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 64.40%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 64.35%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 64.27%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 64.41%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.65%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 64.55%   [EVAL] batch:  327 | acc: 31.25%,  total acc: 64.44%   [EVAL] batch:  328 | acc: 43.75%,  total acc: 64.38%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 64.34%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 64.26%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 65.63%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 65.79%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 65.51%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 65.38%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 65.23%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 65.10%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 65.55%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 65.49%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 65.48%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 65.44%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 65.37%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 65.26%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 65.09%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 64.94%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 64.85%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 64.74%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 64.70%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 64.57%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 64.76%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 64.80%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 64.84%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  391 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 65.12%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 64.82%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 64.75%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 64.59%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 64.44%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 64.30%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 64.17%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 64.03%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 63.89%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  407 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 64.83%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 64.86%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  427 | acc: 43.75%,  total acc: 64.79%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 64.77%   [EVAL] batch:  430 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 64.71%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 64.67%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 64.54%   
cur_acc:  ['0.9464', '0.7639', '0.6915', '0.7381', '0.7391', '0.6825', '0.6052']
his_acc:  ['0.9464', '0.8470', '0.7736', '0.7480', '0.7308', '0.6857', '0.6454']
Clustering into  39  clusters
Clusters:  [25  3 21 22 37 23 26  6  4  4 13  6  2 30 20 31 32  3  1  1 36  1 27  1
 24  3 13 19  1  1  1  0  0 33 34  1 35 17  1  3  3 29  1  1  6 38  2  1
 16  2 18  1  9 14  1  1  8 12  1 15  3  1 13  1  3  1  2  1  2  1 28  7
 11  1  1  1  5 10  1 10]
Losses:  12.78795599937439 1.162376880645752 1.3542581796646118 4.305173635482788
CurrentTrain: epoch  0, batch     0 | loss: 12.7879560Losses:  13.900520581752062 1.107284426689148 1.348785161972046 6.421818990260363
CurrentTrain: epoch  0, batch     1 | loss: 13.9005206Losses:  11.426159679889679 1.199796438217163 1.239675521850586 1.634442150592804
CurrentTrain: epoch  0, batch     2 | loss: 11.4261597Losses:  19.611425440758467 1.0 1.472212791442871 11.836607974022627
CurrentTrain: epoch  0, batch     3 | loss: 19.6114254Losses:  14.30151735432446 1.0345661640167236 1.4011238813400269 6.2568148244172335
CurrentTrain: epoch  1, batch     0 | loss: 14.3015174Losses:  9.532017141580582 1.1782634258270264 1.2963871955871582 1.414572149515152
CurrentTrain: epoch  1, batch     1 | loss: 9.5320171Losses:  9.029469788074493 1.2118147611618042 1.2893017530441284 1.4224651455879211
CurrentTrain: epoch  1, batch     2 | loss: 9.0294698Losses:  10.530660279095173 1.281569480895996 1.4372615814208984 1.6376682594418526
CurrentTrain: epoch  1, batch     3 | loss: 10.5306603Losses:  12.403467953205109 1.098400354385376 1.2358046770095825 4.72148209810257
CurrentTrain: epoch  2, batch     0 | loss: 12.4034680Losses:  7.625363539904356 1.0743703842163086 1.1638938188552856 1.5109030716121197
CurrentTrain: epoch  2, batch     1 | loss: 7.6253635Losses:  14.659005746245384 1.1454792022705078 1.3896632194519043 6.158675774931908
CurrentTrain: epoch  2, batch     2 | loss: 14.6590057Losses:  6.7075106501579285 1.2547459602355957 1.5553479194641113 1.4227535128593445
CurrentTrain: epoch  2, batch     3 | loss: 6.7075107Losses:  7.368797779083252 1.1553571224212646 1.3601222038269043 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.3687978Losses:  8.205236131325364 1.0546534061431885 1.076122522354126 1.7018477264791727
CurrentTrain: epoch  3, batch     1 | loss: 8.2052361Losses:  7.223077774047852 1.1115403175354004 1.2155120372772217 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 7.2230778Losses:  11.271361276507378 1.079878807067871 1.1493825912475586 1.5736989229917526
CurrentTrain: epoch  3, batch     3 | loss: 11.2713613Losses:  8.917784485965967 1.0373164415359497 1.1931551694869995 1.4211509563028812
CurrentTrain: epoch  4, batch     0 | loss: 8.9177845Losses:  6.567142486572266 1.143642544746399 1.261706829071045 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 6.5671425Losses:  12.020372070372105 1.0919127464294434 1.2420928478240967 5.600353874266148
CurrentTrain: epoch  4, batch     2 | loss: 12.0203721Losses:  10.78300429880619 1.118769645690918 1.1787195205688477 1.4733653217554092
CurrentTrain: epoch  4, batch     3 | loss: 10.7830043Losses:  8.307570908218622 1.0285043716430664 1.1888096332550049 1.453726265579462
CurrentTrain: epoch  5, batch     0 | loss: 8.3075709Losses:  10.481220055371523 1.125968337059021 1.316342830657959 4.409632492810488
CurrentTrain: epoch  5, batch     1 | loss: 10.4812201Losses:  8.748895347118378 1.1229239702224731 1.3029460906982422 1.4204174876213074
CurrentTrain: epoch  5, batch     2 | loss: 8.7488953Losses:  8.015365734696388 1.0305390357971191 0.7890782356262207 1.551690235733986
CurrentTrain: epoch  5, batch     3 | loss: 8.0153657Losses:  6.967000961303711 1.1010217666625977 1.3612053394317627 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 6.9670010Losses:  9.079829037189484 1.0426454544067383 1.2689151763916016 2.906665623188019
CurrentTrain: epoch  6, batch     1 | loss: 9.0798290Losses:  6.333141326904297 1.0942001342773438 1.3970892429351807 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 6.3331413Losses:  7.690100744366646 0.9529352188110352 1.1118831634521484 1.428149774670601
CurrentTrain: epoch  6, batch     3 | loss: 7.6901007Losses:  5.729042053222656 0.9722779989242554 1.2584956884384155 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.7290421Losses:  13.647525127977133 1.0627005100250244 1.222074031829834 6.558740433305502
CurrentTrain: epoch  7, batch     1 | loss: 13.6475251Losses:  10.711587402969599 1.084085464477539 1.434313178062439 4.341626618057489
CurrentTrain: epoch  7, batch     2 | loss: 10.7115874Losses:  6.362170197069645 1.1860032081604004 1.4575886726379395 1.4831156507134438
CurrentTrain: epoch  7, batch     3 | loss: 6.3621702Losses:  14.886421173810959 1.1258764266967773 1.338929533958435 8.720814675092697
CurrentTrain: epoch  8, batch     0 | loss: 14.8864212Losses:  11.302807008847594 1.054553747177124 1.220933198928833 4.911139642819762
CurrentTrain: epoch  8, batch     1 | loss: 11.3028070Losses:  7.3099935837090015 0.9816046357154846 1.1830940246582031 1.6715048141777515
CurrentTrain: epoch  8, batch     2 | loss: 7.3099936Losses:  8.987871184945107 1.269599437713623 1.5630507469177246 1.4224672466516495
CurrentTrain: epoch  8, batch     3 | loss: 8.9878712Losses:  9.355591662228107 1.0854988098144531 1.3142645359039307 2.8908671215176582
CurrentTrain: epoch  9, batch     0 | loss: 9.3555917Losses:  6.909216530621052 0.993259072303772 1.155600666999817 1.4383131340146065
CurrentTrain: epoch  9, batch     1 | loss: 6.9092165Losses:  7.356105595827103 1.0393283367156982 1.2932472229003906 1.4166124165058136
CurrentTrain: epoch  9, batch     2 | loss: 7.3561056Losses:  8.379629664123058 1.1721830368041992 1.8483505249023438 1.4415641352534294
CurrentTrain: epoch  9, batch     3 | loss: 8.3796297
Losses:  2.468903064727783 1.0598633289337158 1.2127379179000854 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.4689031Losses:  2.910673141479492 0.959357500076294 1.3923721313476562 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.9106731Losses:  2.560967445373535 0.9216005802154541 1.3134428262710571 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.5609674Losses:  2.497873306274414 0.996303141117096 1.296755313873291 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 2.4978733Losses:  2.9221696853637695 0.9267890453338623 1.4003586769104004 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 2.9221697Losses:  2.947544574737549 1.0880537033081055 1.3976528644561768 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.9475446Losses:  2.9533138275146484 1.0210978984832764 1.1576324701309204 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.9533138Losses:  3.136307954788208 0.9692327976226807 1.5226466655731201 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 3.1363080Losses:  2.8398566246032715 0.913571834564209 1.5005309581756592 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 2.8398566Losses:  2.5207409858703613 0.8785375356674194 1.3575977087020874 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 2.5207410Losses:  2.854311466217041 0.8165476322174072 1.3537896871566772 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.8543115Losses:  2.7657630443573 0.9477527141571045 1.435673475265503 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.7657630Losses:  2.547254800796509 0.9352173209190369 1.3456830978393555 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.5472548Losses:  2.7974069118499756 1.1068010330200195 1.5185139179229736 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 2.7974069Losses:  2.700927257537842 1.1200242042541504 1.3281248807907104 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 2.7009273Losses:  2.725165367126465 1.0320942401885986 1.361440658569336 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.7251654Losses:  2.2779016494750977 0.8436021208763123 1.3100395202636719 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2779016Losses:  2.7667083740234375 0.9705138206481934 1.4665600061416626 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.7667084Losses:  2.7968297004699707 1.1075115203857422 1.4412105083465576 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.7968297Losses:  2.161644458770752 0.8965641260147095 1.170663833618164 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 2.1616445Losses:  2.3900904655456543 0.8599417209625244 1.401068925857544 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.3900905Losses:  2.4777028560638428 0.9982630610466003 1.3101744651794434 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.4777029Losses:  2.4769034385681152 0.866961658000946 1.4154689311981201 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.4769034Losses:  2.4775850772857666 0.9207672476768494 1.4572882652282715 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.4775851Losses:  2.54172420501709 1.1169967651367188 1.3473780155181885 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 2.5417242Losses:  2.3592615127563477 0.933775782585144 1.3536691665649414 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.3592615Losses:  2.6694703102111816 1.116987943649292 1.4449224472045898 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.6694703Losses:  2.414555788040161 0.9701170921325684 1.3012065887451172 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.4145558Losses:  2.6168336868286133 0.9546760320663452 1.5403670072555542 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.6168337Losses:  1.9825979471206665 0.7305030822753906 1.1347379684448242 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 1.9825979Losses:  2.408608913421631 1.0196177959442139 1.3071094751358032 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.4086089#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 2 0 0 1 0 0 0]
Losses:  18.15954822860658 1.4161680936813354 1.5775365829467773 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 18.1595482Losses:  19.484691370278597 1.9326664209365845 1.9884347915649414 5.833449114114046
CurrentTrain: epoch  0, batch     1 | loss: 19.4846914Losses:  17.746089674532413 1.6278927326202393 1.7181609869003296 4.395899511873722
CurrentTrain: epoch  0, batch     2 | loss: 17.7460897Losses:  18.17402466572821 1.8575948476791382 1.9360754489898682 5.152892196550965
CurrentTrain: epoch  0, batch     3 | loss: 18.1740247Losses:  16.067860882729292 1.6802583932876587 1.7847683429718018 2.502943318337202
CurrentTrain: epoch  0, batch     4 | loss: 16.0678609Losses:  16.373739317059517 1.5590217113494873 1.6780784130096436 4.398820951581001
CurrentTrain: epoch  0, batch     5 | loss: 16.3737393Losses:  21.417053814977407 1.7519546747207642 1.7090275287628174 8.27966558560729
CurrentTrain: epoch  0, batch     6 | loss: 21.4170538Losses:  17.409098625183105 1.8425514698028564 1.9120049476623535 3.8551626205444336
CurrentTrain: epoch  0, batch     7 | loss: 17.4090986Losses:  24.183430083096027 1.7097153663635254 1.6211116313934326 10.513400442898273
CurrentTrain: epoch  0, batch     8 | loss: 24.1834301Losses:  15.300930052995682 1.6746113300323486 1.7610507011413574 2.892889052629471
CurrentTrain: epoch  0, batch     9 | loss: 15.3009301Losses:  16.401279281824827 1.6205899715423584 1.7688238620758057 3.8637264482676983
CurrentTrain: epoch  0, batch    10 | loss: 16.4012793Losses:  15.91254884749651 1.50306236743927 1.3817121982574463 3.0614011958241463
CurrentTrain: epoch  0, batch    11 | loss: 15.9125488Losses:  13.59333610534668 1.711456298828125 1.7675395011901855 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 13.5933361Losses:  13.43955135345459 1.7717539072036743 1.7012113332748413 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 13.4395514Losses:  18.627808466553688 1.5759872198104858 1.6004878282546997 5.3342303186655045
CurrentTrain: epoch  0, batch    14 | loss: 18.6278085Losses:  17.292185144498944 1.6054942607879639 1.490584373474121 4.989485101774335
CurrentTrain: epoch  0, batch    15 | loss: 17.2921851Losses:  16.486608006060123 1.709338903427124 1.6652803421020508 4.024260975420475
CurrentTrain: epoch  0, batch    16 | loss: 16.4866080Losses:  16.336933612823486 1.5661613941192627 1.5338608026504517 3.7753710746765137
CurrentTrain: epoch  0, batch    17 | loss: 16.3369336Losses:  18.004900321364403 1.624449610710144 1.3738322257995605 5.059612616896629
CurrentTrain: epoch  0, batch    18 | loss: 18.0049003Losses:  13.843977510929108 1.4918882846832275 1.4096100330352783 1.4325156807899475
CurrentTrain: epoch  0, batch    19 | loss: 13.8439775Losses:  15.994688633829355 1.7919704914093018 1.6394981145858765 4.265522602945566
CurrentTrain: epoch  0, batch    20 | loss: 15.9946886Losses:  16.536829801276326 1.499699354171753 1.2819501161575317 5.050193639472127
CurrentTrain: epoch  0, batch    21 | loss: 16.5368298Losses:  21.01198612526059 1.6680575609207153 1.641351342201233 8.70601974800229
CurrentTrain: epoch  0, batch    22 | loss: 21.0119861Losses:  14.217634048312902 1.6592223644256592 1.5673494338989258 1.4388559721410275
CurrentTrain: epoch  0, batch    23 | loss: 14.2176340Losses:  17.979508712887764 1.5585293769836426 1.6403249502182007 5.698576286435127
CurrentTrain: epoch  0, batch    24 | loss: 17.9795087Losses:  12.548966407775879 1.665771245956421 1.6013047695159912 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 12.5489664Losses:  12.006401062011719 1.648781180381775 1.422465443611145 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 12.0064011Losses:  17.04648069292307 1.6014254093170166 1.462891697883606 4.807432688772678
CurrentTrain: epoch  0, batch    27 | loss: 17.0464807Losses:  16.341935677453876 1.6391215324401855 1.436078667640686 4.636672539636493
CurrentTrain: epoch  0, batch    28 | loss: 16.3419357Losses:  12.218143463134766 1.7436859607696533 1.5153002738952637 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 12.2181435Losses:  10.89187240600586 1.552618384361267 1.465986728668213 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 10.8918724Losses:  15.97052613645792 1.553410291671753 1.3813495635986328 5.202720083296299
CurrentTrain: epoch  0, batch    31 | loss: 15.9705261Losses:  11.119872093200684 1.4256870746612549 1.3339312076568604 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 11.1198721Losses:  11.73359414935112 1.4683688879013062 1.4491770267486572 1.4029552638530731
CurrentTrain: epoch  0, batch    33 | loss: 11.7335941Losses:  15.206649385392666 1.6343563795089722 1.4023984670639038 3.2776532992720604
CurrentTrain: epoch  0, batch    34 | loss: 15.2066494Losses:  15.473522890359163 1.7323883771896362 1.4697784185409546 3.349485147744417
CurrentTrain: epoch  0, batch    35 | loss: 15.4735229Losses:  13.093660987913609 1.417106032371521 1.387856125831604 1.8154961243271828
CurrentTrain: epoch  0, batch    36 | loss: 13.0936610Losses:  12.90829935669899 1.6495040655136108 1.443812608718872 1.4250954687595367
CurrentTrain: epoch  0, batch    37 | loss: 12.9082994Losses:  11.100032806396484 1.7078195810317993 1.5114339590072632 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 11.1000328Losses:  17.10400678217411 1.6665810346603394 1.193182349205017 5.078615203499794
CurrentTrain: epoch  0, batch    39 | loss: 17.1040068Losses:  18.837386578321457 1.642833948135376 1.4650497436523438 7.347438305616379
CurrentTrain: epoch  0, batch    40 | loss: 18.8373866Losses:  17.845928110182285 1.5646183490753174 1.289030909538269 5.714345850050449
CurrentTrain: epoch  0, batch    41 | loss: 17.8459281Losses:  15.510177414864302 1.5422024726867676 1.5349867343902588 3.704284470528364
CurrentTrain: epoch  0, batch    42 | loss: 15.5101774Losses:  17.458369759842753 1.4340965747833252 1.3965799808502197 6.516099480912089
CurrentTrain: epoch  0, batch    43 | loss: 17.4583698Losses:  13.045042663812637 1.6438661813735962 1.2985211610794067 1.785264641046524
CurrentTrain: epoch  0, batch    44 | loss: 13.0450427Losses:  10.652173042297363 1.4845678806304932 1.3429380655288696 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 10.6521730Losses:  16.271216206252575 1.5068436861038208 1.2673712968826294 4.9434469267725945
CurrentTrain: epoch  0, batch    46 | loss: 16.2712162Losses:  16.562947548925877 1.5575740337371826 1.3622132539749146 4.973884858191013
CurrentTrain: epoch  0, batch    47 | loss: 16.5629475Losses:  13.923504462465644 1.5909850597381592 1.5536341667175293 2.2476984169334173
CurrentTrain: epoch  0, batch    48 | loss: 13.9235045Losses:  13.075267739593983 1.4705572128295898 1.2424943447113037 1.5092172101140022
CurrentTrain: epoch  0, batch    49 | loss: 13.0752677Losses:  20.303265511989594 1.738760232925415 1.5059669017791748 8.533011376857758
CurrentTrain: epoch  0, batch    50 | loss: 20.3032655Losses:  14.63608656078577 1.4918162822723389 1.348388671875 4.667176343500614
CurrentTrain: epoch  0, batch    51 | loss: 14.6360866Losses:  12.171684265136719 1.706247329711914 1.420906901359558 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 12.1716843Losses:  16.375279162544757 1.3953512907028198 1.277927041053772 5.378041957039386
CurrentTrain: epoch  0, batch    53 | loss: 16.3752792Losses:  17.431077875196934 1.4447021484375 1.239619493484497 6.959647096693516
CurrentTrain: epoch  0, batch    54 | loss: 17.4310779Losses:  12.087029870599508 1.6957669258117676 1.415172815322876 1.5362676940858364
CurrentTrain: epoch  0, batch    55 | loss: 12.0870299Losses:  23.01578824222088 1.3518648147583008 1.1688369512557983 12.0328618735075
CurrentTrain: epoch  0, batch    56 | loss: 23.0157882Losses:  13.341715924441814 1.4430656433105469 1.408735990524292 2.998951070010662
CurrentTrain: epoch  0, batch    57 | loss: 13.3417159Losses:  14.824894897639751 1.6773730516433716 1.5743601322174072 3.7108631059527397
CurrentTrain: epoch  0, batch    58 | loss: 14.8248949Losses:  12.768366415053606 1.7295441627502441 1.3644442558288574 1.528040487319231
CurrentTrain: epoch  0, batch    59 | loss: 12.7683664Losses:  26.171674966812134 1.4311437606811523 1.4099702835083008 15.970665216445923
CurrentTrain: epoch  0, batch    60 | loss: 26.1716750Losses:  14.73749104514718 1.6982402801513672 1.1981457471847534 4.656868372112513
CurrentTrain: epoch  0, batch    61 | loss: 14.7374910Losses:  12.623944699764252 1.3354575634002686 1.4867825508117676 1.5878176093101501
CurrentTrain: epoch  0, batch    62 | loss: 12.6239447Losses:  11.159332275390625 1.6847130060195923 1.2948986291885376 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 11.1593323Losses:  14.561788302380592 1.5535579919815063 1.4874156713485718 4.132759791333228
CurrentTrain: epoch  1, batch     1 | loss: 14.5617883Losses:  16.687690433114767 1.518570065498352 1.2129327058792114 6.650329288095236
CurrentTrain: epoch  1, batch     2 | loss: 16.6876904Losses:  11.895420037209988 1.4238300323486328 1.436851978302002 1.4668845757842064
CurrentTrain: epoch  1, batch     3 | loss: 11.8954200Losses:  12.977788049727678 1.6284531354904175 1.5301356315612793 1.8232584781944752
CurrentTrain: epoch  1, batch     4 | loss: 12.9777880Losses:  11.046381413936615 1.556575059890747 1.3890633583068848 1.402643620967865
CurrentTrain: epoch  1, batch     5 | loss: 11.0463814Losses:  10.653894424438477 1.4740676879882812 1.334348440170288 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 10.6538944Losses:  12.214360665529966 1.381484031677246 1.240364670753479 1.7557386867702007
CurrentTrain: epoch  1, batch     7 | loss: 12.2143607Losses:  11.856700256466866 1.599068522453308 1.3203052282333374 1.5684150010347366
CurrentTrain: epoch  1, batch     8 | loss: 11.8567003Losses:  16.47071409225464 1.5604560375213623 1.3439247608184814 6.545516490936279
CurrentTrain: epoch  1, batch     9 | loss: 16.4707141Losses:  16.113270975649357 1.492995262145996 1.1392157077789307 5.221313692629337
CurrentTrain: epoch  1, batch    10 | loss: 16.1132710Losses:  13.60924682021141 1.4987449645996094 1.491312861442566 3.200342744588852
CurrentTrain: epoch  1, batch    11 | loss: 13.6092468Losses:  12.829827725887299 1.391371250152588 1.1983115673065186 2.8486656546592712
CurrentTrain: epoch  1, batch    12 | loss: 12.8298277Losses:  9.923561096191406 1.4679222106933594 1.4007989168167114 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 9.9235611Losses:  14.49601836130023 1.4816890954971313 1.1217955350875854 3.6392364017665386
CurrentTrain: epoch  1, batch    14 | loss: 14.4960184Losses:  10.465740203857422 1.597839593887329 1.2882767915725708 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 10.4657402Losses:  14.23041295260191 1.433135747909546 1.3684732913970947 3.1094994470477104
CurrentTrain: epoch  1, batch    16 | loss: 14.2304130Losses:  15.279668595641851 1.2987664937973022 1.1769676208496094 4.441113259643316
CurrentTrain: epoch  1, batch    17 | loss: 15.2796686Losses:  16.021261455491185 1.4669995307922363 1.317780613899231 6.084716083481908
CurrentTrain: epoch  1, batch    18 | loss: 16.0212615Losses:  10.278553009033203 1.4844709634780884 1.3134623765945435 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 10.2785530Losses:  9.952409744262695 1.280739426612854 1.3466687202453613 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 9.9524097Losses:  20.746833480894566 1.4485039710998535 1.2849804162979126 9.857946075499058
CurrentTrain: epoch  1, batch    21 | loss: 20.7468335Losses:  10.382868766784668 1.3546077013015747 1.3301500082015991 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 10.3828688Losses:  9.870052337646484 1.5920393466949463 1.3491289615631104 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 9.8700523Losses:  13.237141475081444 1.4611127376556396 1.3177695274353027 4.374333247542381
CurrentTrain: epoch  1, batch    24 | loss: 13.2371415Losses:  11.900915890932083 1.5170106887817383 1.329482078552246 2.8882425129413605
CurrentTrain: epoch  1, batch    25 | loss: 11.9009159Losses:  10.453478813171387 1.3346680402755737 1.237174153327942 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 10.4534788Losses:  9.957710266113281 1.438193917274475 1.2285581827163696 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 9.9577103Losses:  10.352690696716309 1.4791969060897827 1.3181698322296143 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 10.3526907Losses:  14.228682585060596 1.3761003017425537 1.369565486907959 4.332463331520557
CurrentTrain: epoch  1, batch    29 | loss: 14.2286826Losses:  14.12350234016776 1.5028513669967651 1.181463599205017 3.6353517435491085
CurrentTrain: epoch  1, batch    30 | loss: 14.1235023Losses:  13.859168868511915 1.408476710319519 1.3523966073989868 3.262240272015333
CurrentTrain: epoch  1, batch    31 | loss: 13.8591689Losses:  14.25030280649662 1.4719618558883667 1.2743717432022095 3.951246753334999
CurrentTrain: epoch  1, batch    32 | loss: 14.2503028Losses:  9.351544380187988 1.3918213844299316 1.2379828691482544 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 9.3515444Losses:  9.283535957336426 1.4324142932891846 1.321043848991394 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 9.2835360Losses:  17.1985801756382 1.5535964965820312 1.3298234939575195 6.542270094156265
CurrentTrain: epoch  1, batch    35 | loss: 17.1985802Losses:  12.238177798688412 1.6260664463043213 1.3727412223815918 1.6053996309638023
CurrentTrain: epoch  1, batch    36 | loss: 12.2381778Losses:  10.389236457645893 1.2306108474731445 1.2102208137512207 1.6042194440960884
CurrentTrain: epoch  1, batch    37 | loss: 10.3892365Losses:  12.180137179791927 1.489979863166809 1.3459253311157227 2.9884658083319664
CurrentTrain: epoch  1, batch    38 | loss: 12.1801372Losses:  13.927021220326424 1.5781954526901245 1.2724264860153198 4.703357890248299
CurrentTrain: epoch  1, batch    39 | loss: 13.9270212Losses:  11.043610095977783 1.4327399730682373 1.3585280179977417 1.4673333168029785
CurrentTrain: epoch  1, batch    40 | loss: 11.0436101Losses:  12.255074322223663 1.4528093338012695 1.1873753070831299 2.956116497516632
CurrentTrain: epoch  1, batch    41 | loss: 12.2550743Losses:  11.561181474477053 1.5198372602462769 1.1909352540969849 1.8549217469990253
CurrentTrain: epoch  1, batch    42 | loss: 11.5611815Losses:  14.227617524564266 1.4163410663604736 1.371048927307129 4.503932259976864
CurrentTrain: epoch  1, batch    43 | loss: 14.2276175Losses:  14.481022665277123 1.3520011901855469 1.3182203769683838 3.893675634637475
CurrentTrain: epoch  1, batch    44 | loss: 14.4810227Losses:  13.256519202142954 1.5059847831726074 1.3504729270935059 2.96854579821229
CurrentTrain: epoch  1, batch    45 | loss: 13.2565192Losses:  10.5294828414917 1.5729023218154907 1.28804612159729 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 10.5294828Losses:  9.759831428527832 1.5256459712982178 1.1045076847076416 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 9.7598314Losses:  11.409556478261948 1.3801400661468506 1.1898794174194336 1.4076920449733734
CurrentTrain: epoch  1, batch    48 | loss: 11.4095565Losses:  19.497820168733597 1.3892827033996582 1.2106451988220215 9.359789162874222
CurrentTrain: epoch  1, batch    49 | loss: 19.4978202Losses:  11.18434277176857 1.208479881286621 1.128179907798767 1.4396242201328278
CurrentTrain: epoch  1, batch    50 | loss: 11.1843428Losses:  9.965991973876953 1.5054974555969238 1.2648506164550781 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 9.9659920Losses:  16.05435499176383 1.4798450469970703 1.227407693862915 6.3691609762609005
CurrentTrain: epoch  1, batch    52 | loss: 16.0543550Losses:  9.050774574279785 1.4300750494003296 1.3620892763137817 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 9.0507746Losses:  12.465514436364174 1.2823028564453125 1.2544350624084473 3.3343308120965958
CurrentTrain: epoch  1, batch    54 | loss: 12.4655144Losses:  12.120046451687813 1.3472161293029785 1.3374676704406738 2.9133528023958206
CurrentTrain: epoch  1, batch    55 | loss: 12.1200465Losses:  9.757993698120117 1.5236945152282715 1.1529860496520996 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 9.7579937Losses:  10.137706756591797 1.4623134136199951 1.2674477100372314 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 10.1377068Losses:  9.034796714782715 1.3090825080871582 1.270840048789978 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 9.0347967Losses:  13.368269670754671 1.3784136772155762 1.2760034799575806 3.2383267767727375
CurrentTrain: epoch  1, batch    59 | loss: 13.3682697Losses:  9.513507843017578 1.5013962984085083 1.3541772365570068 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 9.5135078Losses:  10.856292724609375 1.448828935623169 1.2032241821289062 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 10.8562927Losses:  12.390957687050104 1.5860507488250732 1.5034499168395996 4.35913548246026
CurrentTrain: epoch  1, batch    62 | loss: 12.3909577Losses:  8.299339294433594 1.4631812572479248 1.2422661781311035 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 8.2993393Losses:  8.998124122619629 1.354837417602539 1.2967939376831055 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 8.9981241Losses:  9.98938861489296 1.487715721130371 1.2688744068145752 1.4037248194217682
CurrentTrain: epoch  2, batch     2 | loss: 9.9893886Losses:  14.139057997614145 1.3303842544555664 1.3553481101989746 3.054484251886606
CurrentTrain: epoch  2, batch     3 | loss: 14.1390580Losses:  11.667106863111258 1.377474069595337 1.2318062782287598 1.468473669141531
CurrentTrain: epoch  2, batch     4 | loss: 11.6671069Losses:  11.658132433891296 1.3318355083465576 1.1114802360534668 2.12087619304657
CurrentTrain: epoch  2, batch     5 | loss: 11.6581324Losses:  9.27345085144043 1.475107192993164 1.2066960334777832 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 9.2734509Losses:  14.29615867882967 1.3525549173355103 1.2084898948669434 4.98982322961092
CurrentTrain: epoch  2, batch     7 | loss: 14.2961587Losses:  12.47496484965086 1.3321115970611572 1.2300971746444702 3.4226634427905083
CurrentTrain: epoch  2, batch     8 | loss: 12.4749648Losses:  13.63291361182928 1.4354264736175537 1.23061203956604 3.9028053507208824
CurrentTrain: epoch  2, batch     9 | loss: 13.6329136Losses:  10.611761942505836 1.3306964635849 1.2698296308517456 1.5006407648324966
CurrentTrain: epoch  2, batch    10 | loss: 10.6117619Losses:  10.030204594135284 1.3526225090026855 1.2054979801177979 1.5410030484199524
CurrentTrain: epoch  2, batch    11 | loss: 10.0302046Losses:  15.479369141161442 1.5504486560821533 1.2676805257797241 5.89241311699152
CurrentTrain: epoch  2, batch    12 | loss: 15.4793691Losses:  10.619140058755875 1.258885145187378 1.2365957498550415 1.4867099821567535
CurrentTrain: epoch  2, batch    13 | loss: 10.6191401Losses:  13.015613473951817 1.4371962547302246 1.3593332767486572 2.8791760578751564
CurrentTrain: epoch  2, batch    14 | loss: 13.0156135Losses:  11.199856388382614 1.217402696609497 1.1196328401565552 2.301546680741012
CurrentTrain: epoch  2, batch    15 | loss: 11.1998564Losses:  12.137417197227478 1.3547019958496094 1.148820400238037 2.8511356115341187
CurrentTrain: epoch  2, batch    16 | loss: 12.1374172Losses:  12.289928246289492 1.5288441181182861 1.2810695171356201 2.9223735816776752
CurrentTrain: epoch  2, batch    17 | loss: 12.2899282Losses:  20.06841879710555 1.5905108451843262 1.3194283246994019 11.620302494615316
CurrentTrain: epoch  2, batch    18 | loss: 20.0684188Losses:  11.397434884682298 1.29817533493042 1.1457080841064453 1.8709456268697977
CurrentTrain: epoch  2, batch    19 | loss: 11.3974349Losses:  8.576071739196777 1.3223395347595215 1.2835335731506348 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 8.5760717Losses:  9.401657104492188 1.4670579433441162 1.354366421699524 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 9.4016571Losses:  13.287453413009644 1.367300033569336 1.0927187204360962 4.371161222457886
CurrentTrain: epoch  2, batch    22 | loss: 13.2874534Losses:  11.058695368468761 1.1301422119140625 1.1791365146636963 1.6153989359736443
CurrentTrain: epoch  2, batch    23 | loss: 11.0586954Losses:  9.647219382226467 1.362262487411499 1.2192308902740479 1.5136324986815453
CurrentTrain: epoch  2, batch    24 | loss: 9.6472194Losses:  14.125863287597895 1.5074098110198975 1.3154658079147339 4.870211813598871
CurrentTrain: epoch  2, batch    25 | loss: 14.1258633Losses:  10.539736956357956 1.4522790908813477 1.094868779182434 1.4146330058574677
CurrentTrain: epoch  2, batch    26 | loss: 10.5397370Losses:  10.498350940644741 1.267437219619751 1.1513099670410156 1.7941101416945457
CurrentTrain: epoch  2, batch    27 | loss: 10.4983509Losses:  10.237884316593409 1.3957582712173462 1.2579994201660156 1.4172094203531742
CurrentTrain: epoch  2, batch    28 | loss: 10.2378843Losses:  20.83341731503606 1.4270403385162354 1.2150402069091797 12.051979441195726
CurrentTrain: epoch  2, batch    29 | loss: 20.8334173Losses:  10.732436891645193 1.5504639148712158 1.4206583499908447 1.4695832692086697
CurrentTrain: epoch  2, batch    30 | loss: 10.7324369Losses:  9.860230255872011 1.4561976194381714 1.0978143215179443 1.486910630017519
CurrentTrain: epoch  2, batch    31 | loss: 9.8602303Losses:  11.395576443523169 1.351021647453308 1.1969523429870605 2.9120082519948483
CurrentTrain: epoch  2, batch    32 | loss: 11.3955764Losses:  12.029002584517002 1.3295824527740479 1.2748219966888428 2.2780651226639748
CurrentTrain: epoch  2, batch    33 | loss: 12.0290026Losses:  14.404372032731771 1.4813148975372314 1.075352668762207 5.7006719671189785
CurrentTrain: epoch  2, batch    34 | loss: 14.4043720Losses:  15.399156425148249 1.2945268154144287 1.0073106288909912 5.897940490394831
CurrentTrain: epoch  2, batch    35 | loss: 15.3991564Losses:  11.685860995203257 1.1714637279510498 1.1137123107910156 2.900945071130991
CurrentTrain: epoch  2, batch    36 | loss: 11.6858610Losses:  11.268865566700697 1.4642927646636963 1.1942577362060547 1.7800063900649548
CurrentTrain: epoch  2, batch    37 | loss: 11.2688656Losses:  10.39602056145668 1.468296766281128 1.312943458557129 1.4043718874454498
CurrentTrain: epoch  2, batch    38 | loss: 10.3960206Losses:  12.890997931361198 1.3569107055664062 1.199195146560669 3.357255980372429
CurrentTrain: epoch  2, batch    39 | loss: 12.8909979Losses:  12.888429522514343 1.3261280059814453 1.2361750602722168 4.450490832328796
CurrentTrain: epoch  2, batch    40 | loss: 12.8884295Losses:  11.488805398344994 1.2693915367126465 1.2374343872070312 1.4902416318655014
CurrentTrain: epoch  2, batch    41 | loss: 11.4888054Losses:  10.01162764430046 1.2958177328109741 1.2222541570663452 1.3993105590343475
CurrentTrain: epoch  2, batch    42 | loss: 10.0116276Losses:  9.275636672973633 1.3815265893936157 1.2248926162719727 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 9.2756367Losses:  9.53016858920455 1.1895475387573242 1.1612952947616577 1.4762745462357998
CurrentTrain: epoch  2, batch    44 | loss: 9.5301686Losses:  12.234316850081086 1.319739580154419 1.25470769405365 3.276134515181184
CurrentTrain: epoch  2, batch    45 | loss: 12.2343169Losses:  8.460195541381836 1.2828150987625122 1.2050046920776367 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 8.4601955Losses:  8.916975975036621 1.2436414957046509 1.2180300951004028 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 8.9169760Losses:  8.344931602478027 1.3818551301956177 1.2580188512802124 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 8.3449316Losses:  11.014931198209524 1.4312692880630493 1.2743430137634277 1.4321331940591335
CurrentTrain: epoch  2, batch    49 | loss: 11.0149312Losses:  18.249076448380947 1.4937069416046143 1.146498203277588 9.178798280656338
CurrentTrain: epoch  2, batch    50 | loss: 18.2490764Losses:  12.012512849643826 1.403605341911316 1.3801032304763794 3.555651353672147
CurrentTrain: epoch  2, batch    51 | loss: 12.0125128Losses:  9.897020572796464 1.2309235334396362 1.1644809246063232 1.7212316934019327
CurrentTrain: epoch  2, batch    52 | loss: 9.8970206Losses:  11.274209320545197 1.244718074798584 1.2339413166046143 2.839560806751251
CurrentTrain: epoch  2, batch    53 | loss: 11.2742093Losses:  10.286228090524673 1.363999605178833 1.1235636472702026 1.4013737738132477
CurrentTrain: epoch  2, batch    54 | loss: 10.2862281Losses:  11.136843029409647 1.3595051765441895 1.230446457862854 2.9173148311674595
CurrentTrain: epoch  2, batch    55 | loss: 11.1368430Losses:  10.289379300549626 1.3872064352035522 1.3246526718139648 1.769201459363103
CurrentTrain: epoch  2, batch    56 | loss: 10.2893793Losses:  9.079568862915039 1.3273470401763916 1.0809085369110107 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 9.0795689Losses:  8.776670455932617 1.3125487565994263 1.2684272527694702 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 8.7766705Losses:  19.74055342748761 1.4942582845687866 1.1737650632858276 11.275174666196108
CurrentTrain: epoch  2, batch    59 | loss: 19.7405534Losses:  8.801861763000488 1.3863317966461182 1.2121541500091553 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 8.8018618Losses:  11.531712792813778 1.2896918058395386 1.180862307548523 1.9980528578162193
CurrentTrain: epoch  2, batch    61 | loss: 11.5317128Losses:  8.373750686645508 1.3199493885040283 1.2748987674713135 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 8.3737507Losses:  9.75995609164238 1.3656399250030518 1.3108408451080322 1.4246909320354462
CurrentTrain: epoch  3, batch     0 | loss: 9.7599561Losses:  9.64663752913475 1.249135136604309 1.1921597719192505 1.417348474264145
CurrentTrain: epoch  3, batch     1 | loss: 9.6466375Losses:  8.772783279418945 1.352999210357666 1.2355148792266846 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 8.7727833Losses:  12.84994949400425 1.2835628986358643 1.1775293350219727 4.3347069174051285
CurrentTrain: epoch  3, batch     3 | loss: 12.8499495Losses:  10.934734918177128 1.2574793100357056 1.1276053190231323 3.3337813392281532
CurrentTrain: epoch  3, batch     4 | loss: 10.9347349Losses:  13.23183199763298 1.3964648246765137 1.1298067569732666 4.309707134962082
CurrentTrain: epoch  3, batch     5 | loss: 13.2318320Losses:  9.663406401872635 1.288285732269287 1.0378646850585938 1.4037857353687286
CurrentTrain: epoch  3, batch     6 | loss: 9.6634064Losses:  12.808973662555218 1.2889645099639893 1.296482801437378 4.252410285174847
CurrentTrain: epoch  3, batch     7 | loss: 12.8089737Losses:  9.549569599330425 1.2432116270065308 1.1950709819793701 1.4414400979876518
CurrentTrain: epoch  3, batch     8 | loss: 9.5495696Losses:  8.679771423339844 1.2632637023925781 1.0572080612182617 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 8.6797714Losses:  8.411014556884766 1.3411552906036377 1.1202759742736816 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 8.4110146Losses:  11.61084657534957 1.395317554473877 1.274113655090332 3.0280104242265224
CurrentTrain: epoch  3, batch    11 | loss: 11.6108466Losses:  12.666829854249954 1.3868579864501953 1.439087986946106 4.2461831867694855
CurrentTrain: epoch  3, batch    12 | loss: 12.6668299Losses:  12.777043998241425 1.5601208209991455 1.3118176460266113 2.873269736766815
CurrentTrain: epoch  3, batch    13 | loss: 12.7770440Losses:  19.126049757003784 1.1352486610412598 1.2349494695663452 10.367085218429565
CurrentTrain: epoch  3, batch    14 | loss: 19.1260498Losses:  10.096644967794418 1.2814879417419434 1.1187177896499634 1.4134975373744965
CurrentTrain: epoch  3, batch    15 | loss: 10.0966450Losses:  14.877248153090477 1.3038349151611328 1.184192419052124 6.0689090341329575
CurrentTrain: epoch  3, batch    16 | loss: 14.8772482Losses:  9.642907418310642 1.3882858753204346 1.3140864372253418 1.4439576044678688
CurrentTrain: epoch  3, batch    17 | loss: 9.6429074Losses:  12.426075831055641 1.280527114868164 1.1645256280899048 4.362721338868141
CurrentTrain: epoch  3, batch    18 | loss: 12.4260758Losses:  9.108965516090393 1.2973062992095947 1.3290802240371704 1.480683445930481
CurrentTrain: epoch  3, batch    19 | loss: 9.1089655Losses:  11.604993425309658 1.2472944259643555 1.0739201307296753 4.2378517016768456
CurrentTrain: epoch  3, batch    20 | loss: 11.6049934Losses:  11.443988256156445 1.2917916774749756 1.2257745265960693 3.307838849723339
CurrentTrain: epoch  3, batch    21 | loss: 11.4439883Losses:  15.132546965032816 1.2868340015411377 1.135741114616394 6.410708013921976
CurrentTrain: epoch  3, batch    22 | loss: 15.1325470Losses:  10.292033523321152 1.390851616859436 1.2319585084915161 1.4244788587093353
CurrentTrain: epoch  3, batch    23 | loss: 10.2920335Losses:  9.947969403117895 1.1578763723373413 1.1007273197174072 1.4436883591115475
CurrentTrain: epoch  3, batch    24 | loss: 9.9479694Losses:  11.58998166397214 1.1120554208755493 1.176086664199829 2.9386011250317097
CurrentTrain: epoch  3, batch    25 | loss: 11.5899817Losses:  10.771128173917532 1.401615858078003 1.3732638359069824 2.9115619622170925
CurrentTrain: epoch  3, batch    26 | loss: 10.7711282Losses:  8.850719813257456 1.1808130741119385 1.1434882879257202 1.4471310414373875
CurrentTrain: epoch  3, batch    27 | loss: 8.8507198Losses:  13.515976421535015 1.1229609251022339 1.0947235822677612 4.576631061732769
CurrentTrain: epoch  3, batch    28 | loss: 13.5159764Losses:  13.946285467594862 1.3408124446868896 1.2847175598144531 5.8482782654464245
CurrentTrain: epoch  3, batch    29 | loss: 13.9462855Losses:  8.056844711303711 1.2509394884109497 1.3091931343078613 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 8.0568447Losses:  7.5602827072143555 1.1711056232452393 1.0032150745391846 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 7.5602827Losses:  13.52150672674179 1.1504656076431274 1.1595239639282227 5.765693128108978
CurrentTrain: epoch  3, batch    32 | loss: 13.5215067Losses:  10.977618664503098 1.0791456699371338 1.2079954147338867 2.8887819945812225
CurrentTrain: epoch  3, batch    33 | loss: 10.9776187Losses:  8.407991409301758 1.171152114868164 1.2428112030029297 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 8.4079914Losses:  10.190712738782167 1.1353919506072998 1.1934707164764404 1.5130260474979877
CurrentTrain: epoch  3, batch    35 | loss: 10.1907127Losses:  7.771641731262207 1.3255364894866943 1.1553761959075928 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 7.7716417Losses:  8.450601577758789 1.4342222213745117 1.4136707782745361 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 8.4506016Losses:  10.876436710357666 1.278625726699829 1.290160894393921 2.817234516143799
CurrentTrain: epoch  3, batch    38 | loss: 10.8764367Losses:  12.462473392486572 1.3706541061401367 1.2833704948425293 4.256575107574463
CurrentTrain: epoch  3, batch    39 | loss: 12.4624734Losses:  17.46851620078087 1.0973844528198242 1.1488665342330933 7.767885059118271
CurrentTrain: epoch  3, batch    40 | loss: 17.4685162Losses:  8.855305671691895 1.3044073581695557 1.1998014450073242 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 8.8553057Losses:  8.737105667591095 1.2470314502716064 1.089233160018921 1.3962062001228333
CurrentTrain: epoch  3, batch    42 | loss: 8.7371057Losses:  10.101933389902115 1.3836801052093506 1.4248700141906738 1.7683877050876617
CurrentTrain: epoch  3, batch    43 | loss: 10.1019334Losses:  11.864472649991512 1.3170177936553955 1.2918205261230469 3.1409952864050865
CurrentTrain: epoch  3, batch    44 | loss: 11.8644726Losses:  10.66479017958045 1.1193184852600098 1.1211669445037842 2.83516314253211
CurrentTrain: epoch  3, batch    45 | loss: 10.6647902Losses:  12.16000522300601 1.2795873880386353 1.295905590057373 3.2698837630450726
CurrentTrain: epoch  3, batch    46 | loss: 12.1600052Losses:  11.037940174341202 1.5624107122421265 1.4382153749465942 2.8107320368289948
CurrentTrain: epoch  3, batch    47 | loss: 11.0379402Losses:  8.167301177978516 1.3117042779922485 1.2848374843597412 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 8.1673012Losses:  18.988911412656307 1.3992804288864136 1.2829411029815674 10.174151204526424
CurrentTrain: epoch  3, batch    49 | loss: 18.9889114Losses:  8.879243850708008 1.5107333660125732 1.3581817150115967 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 8.8792439Losses:  9.634778618812561 1.1972945928573608 1.1297848224639893 1.4280868768692017
CurrentTrain: epoch  3, batch    51 | loss: 9.6347786Losses:  12.065193742513657 1.208972454071045 1.3032221794128418 4.378840535879135
CurrentTrain: epoch  3, batch    52 | loss: 12.0651937Losses:  13.386411484330893 1.3393501043319702 1.3018510341644287 4.327435310930014
CurrentTrain: epoch  3, batch    53 | loss: 13.3864115Losses:  8.83400422334671 1.2542920112609863 1.1402082443237305 1.3986476063728333
CurrentTrain: epoch  3, batch    54 | loss: 8.8340042Losses:  11.160950846970081 1.4987214803695679 1.1521821022033691 2.8239995911717415
CurrentTrain: epoch  3, batch    55 | loss: 11.1609508Losses:  7.529947280883789 1.1983684301376343 1.1184427738189697 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 7.5299473Losses:  7.929262161254883 1.1602091789245605 1.0535097122192383 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.9292622Losses:  13.162346720695496 1.2742291688919067 1.1639002561569214 5.44904887676239
CurrentTrain: epoch  3, batch    58 | loss: 13.1623467Losses:  18.181393265724182 1.2357900142669678 1.0534337759017944 9.820963501930237
CurrentTrain: epoch  3, batch    59 | loss: 18.1813933Losses:  8.676602363586426 1.4142606258392334 1.1845862865447998 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 8.6766024Losses:  13.289930012077093 1.4229516983032227 1.1629550457000732 4.671457912772894
CurrentTrain: epoch  3, batch    61 | loss: 13.2899300Losses:  9.158672422170639 1.347182273864746 0.9895972609519958 1.4095922410488129
CurrentTrain: epoch  3, batch    62 | loss: 9.1586724Losses:  12.42655785381794 1.1738598346710205 1.1890416145324707 4.338177040219307
CurrentTrain: epoch  4, batch     0 | loss: 12.4265579Losses:  13.059512693434954 1.2882004976272583 1.2481489181518555 4.950821477919817
CurrentTrain: epoch  4, batch     1 | loss: 13.0595127Losses:  7.458761692047119 1.3338305950164795 1.160331130027771 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 7.4587617Losses:  11.476979047060013 1.2803294658660889 1.2204248905181885 4.257527142763138
CurrentTrain: epoch  4, batch     3 | loss: 11.4769790Losses:  16.985011130571365 1.4653066396713257 1.4130423069000244 8.623617202043533
CurrentTrain: epoch  4, batch     4 | loss: 16.9850111Losses:  9.148351222276688 1.1160434484481812 1.316314935684204 1.4109196960926056
CurrentTrain: epoch  4, batch     5 | loss: 9.1483512Losses:  9.001950822770596 1.154486894607544 1.1147377490997314 1.520795427262783
CurrentTrain: epoch  4, batch     6 | loss: 9.0019508Losses:  7.023066520690918 1.1403405666351318 1.2207984924316406 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 7.0230665Losses:  11.126537915319204 1.1277695894241333 1.1933903694152832 2.856611844152212
CurrentTrain: epoch  4, batch     8 | loss: 11.1265379Losses:  9.840716496109962 1.0856444835662842 1.1247341632843018 1.5222445875406265
CurrentTrain: epoch  4, batch     9 | loss: 9.8407165Losses:  12.377748597413301 1.2717411518096924 1.1666125059127808 4.4209967739880085
CurrentTrain: epoch  4, batch    10 | loss: 12.3777486Losses:  11.206548072397709 1.2240077257156372 1.4129369258880615 2.8676818385720253
CurrentTrain: epoch  4, batch    11 | loss: 11.2065481Losses:  9.513943009078503 1.235792875289917 1.1553611755371094 1.4384673163294792
CurrentTrain: epoch  4, batch    12 | loss: 9.5139430Losses:  7.882030010223389 1.3205289840698242 1.271233081817627 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 7.8820300Losses:  9.44921050593257 1.104240894317627 1.0776227712631226 1.4807704500854015
CurrentTrain: epoch  4, batch    14 | loss: 9.4492105Losses:  9.095844209194183 0.9601958394050598 1.166738510131836 1.3968443274497986
CurrentTrain: epoch  4, batch    15 | loss: 9.0958442Losses:  11.767395853996277 1.3744447231292725 1.1831140518188477 3.1760443449020386
CurrentTrain: epoch  4, batch    16 | loss: 11.7673959Losses:  7.749658107757568 1.295686960220337 1.2679029703140259 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 7.7496581Losses:  12.232596039772034 1.3116307258605957 1.0994317531585693 4.359196305274963
CurrentTrain: epoch  4, batch    18 | loss: 12.2325960Losses:  16.622117161750793 1.2532570362091064 1.0995965003967285 8.767714142799377
CurrentTrain: epoch  4, batch    19 | loss: 16.6221172Losses:  11.311448603868484 1.247532606124878 1.2465379238128662 2.853650599718094
CurrentTrain: epoch  4, batch    20 | loss: 11.3114486Losses:  10.911690775305033 1.1402959823608398 1.3444361686706543 2.872145716100931
CurrentTrain: epoch  4, batch    21 | loss: 10.9116908Losses:  9.294403601437807 1.362527847290039 1.2288832664489746 1.4827175624668598
CurrentTrain: epoch  4, batch    22 | loss: 9.2944036Losses:  14.510359827429056 1.1180418729782104 1.1009352207183838 7.136070314794779
CurrentTrain: epoch  4, batch    23 | loss: 14.5103598Losses:  8.93065943941474 1.1390055418014526 1.1826781034469604 1.4660522006452084
CurrentTrain: epoch  4, batch    24 | loss: 8.9306594Losses:  8.406061172485352 1.1990270614624023 1.0872271060943604 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 8.4060612Losses:  8.812737077474594 1.2934486865997314 1.1860463619232178 1.4365144670009613
CurrentTrain: epoch  4, batch    26 | loss: 8.8127371Losses:  10.748251954093575 1.3138223886489868 1.290984869003296 2.0248041544109583
CurrentTrain: epoch  4, batch    27 | loss: 10.7482520Losses:  9.617841988801956 1.3493348360061646 1.2760443687438965 1.4146445095539093
CurrentTrain: epoch  4, batch    28 | loss: 9.6178420Losses:  13.959102232009172 1.277976155281067 1.2175800800323486 5.976199705153704
CurrentTrain: epoch  4, batch    29 | loss: 13.9591022Losses:  13.27896224707365 1.3179357051849365 1.3466925621032715 5.734184853732586
CurrentTrain: epoch  4, batch    30 | loss: 13.2789622Losses:  7.731898307800293 1.3078839778900146 1.154062271118164 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 7.7318983Losses:  10.053724452853203 1.0892170667648315 1.0931971073150635 2.9782001227140427
CurrentTrain: epoch  4, batch    32 | loss: 10.0537245Losses:  12.329602899029851 1.0583128929138184 1.2084147930145264 5.018702210858464
CurrentTrain: epoch  4, batch    33 | loss: 12.3296029Losses:  7.551130294799805 1.3094301223754883 1.2852928638458252 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 7.5511303Losses:  9.145097106695175 1.2478623390197754 0.9537325501441956 1.4011724889278412
CurrentTrain: epoch  4, batch    35 | loss: 9.1450971Losses:  12.667708307504654 1.310464859008789 1.2898797988891602 4.6798485815525055
CurrentTrain: epoch  4, batch    36 | loss: 12.6677083Losses:  12.099626591429114 1.2870360612869263 1.3861976861953735 3.6538334395736456
CurrentTrain: epoch  4, batch    37 | loss: 12.0996266Losses:  7.294114589691162 1.1943678855895996 1.1550530195236206 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 7.2941146Losses:  11.82248842716217 1.094881296157837 1.1184556484222412 4.372782349586487
CurrentTrain: epoch  4, batch    39 | loss: 11.8224884Losses:  12.480103053152561 1.2367838621139526 1.2098188400268555 4.6595940962433815
CurrentTrain: epoch  4, batch    40 | loss: 12.4801031Losses:  7.818617820739746 1.430814504623413 1.3981645107269287 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 7.8186178Losses:  13.888698123395443 1.192655086517334 1.124912977218628 6.2772326692938805
CurrentTrain: epoch  4, batch    42 | loss: 13.8886981Losses:  10.23128554597497 1.154099464416504 1.2682948112487793 2.8669700361788273
CurrentTrain: epoch  4, batch    43 | loss: 10.2312855Losses:  7.876368522644043 1.3913378715515137 1.1781165599822998 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 7.8763685Losses:  8.729694738984108 1.0947542190551758 0.9262596368789673 1.495472326874733
CurrentTrain: epoch  4, batch    45 | loss: 8.7296947Losses:  10.938676495105028 0.9707158803939819 1.1361202001571655 2.9722677655518055
CurrentTrain: epoch  4, batch    46 | loss: 10.9386765Losses:  9.44454675912857 1.456769585609436 1.336515188217163 1.621239721775055
CurrentTrain: epoch  4, batch    47 | loss: 9.4445468Losses:  13.708039790391922 1.1891690492630005 1.1330527067184448 5.856861621141434
CurrentTrain: epoch  4, batch    48 | loss: 13.7080398Losses:  8.130768775939941 1.6128559112548828 1.4798364639282227 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 8.1307688Losses:  10.683801263570786 1.418594479560852 1.2409379482269287 2.992532342672348
CurrentTrain: epoch  4, batch    50 | loss: 10.6838013Losses:  6.982614994049072 1.010862112045288 1.1701769828796387 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 6.9826150Losses:  11.497916046530008 1.1811797618865967 1.2507829666137695 4.266201797872782
CurrentTrain: epoch  4, batch    52 | loss: 11.4979160Losses:  10.715963900089264 1.1629544496536255 1.1558259725570679 1.524862825870514
CurrentTrain: epoch  4, batch    53 | loss: 10.7159639Losses:  8.415072590112686 1.2010631561279297 1.2298359870910645 1.402578979730606
CurrentTrain: epoch  4, batch    54 | loss: 8.4150726Losses:  7.7816290855407715 1.0180341005325317 1.192451000213623 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 7.7816291Losses:  18.17164671048522 1.3672791719436646 1.2166837453842163 10.32703697308898
CurrentTrain: epoch  4, batch    56 | loss: 18.1716467Losses:  11.69332018494606 1.1145234107971191 1.3904622793197632 4.258099466562271
CurrentTrain: epoch  4, batch    57 | loss: 11.6933202Losses:  11.30679327994585 1.3890771865844727 1.2031975984573364 3.1726341918110847
CurrentTrain: epoch  4, batch    58 | loss: 11.3067933Losses:  10.464631155133247 1.088330626487732 1.1315442323684692 3.0573573857545853
CurrentTrain: epoch  4, batch    59 | loss: 10.4646312Losses:  8.995782643556595 1.3250902891159058 1.1143178939819336 1.415408879518509
CurrentTrain: epoch  4, batch    60 | loss: 8.9957826Losses:  9.071367867290974 1.1962765455245972 1.051001787185669 1.46099341660738
CurrentTrain: epoch  4, batch    61 | loss: 9.0713679Losses:  13.08700604736805 1.0947426557540894 1.2854812145233154 6.091894581913948
CurrentTrain: epoch  4, batch    62 | loss: 13.0870060Losses:  7.509315013885498 1.1755892038345337 0.9885196685791016 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.5093150Losses:  10.185895901173353 0.9985082745552063 1.2092392444610596 2.972337704151869
CurrentTrain: epoch  5, batch     1 | loss: 10.1858959Losses:  10.314315140247345 1.225698709487915 1.1493186950683594 2.8154662251472473
CurrentTrain: epoch  5, batch     2 | loss: 10.3143151Losses:  10.597266357392073 1.2006089687347412 1.177713394165039 2.9197522811591625
CurrentTrain: epoch  5, batch     3 | loss: 10.5972664Losses:  11.541613698005676 1.0875368118286133 1.1945388317108154 4.328274846076965
CurrentTrain: epoch  5, batch     4 | loss: 11.5416137Losses:  12.961566478013992 1.1465628147125244 1.2837131023406982 5.806368380784988
CurrentTrain: epoch  5, batch     5 | loss: 12.9615665Losses:  7.799494743347168 1.2709102630615234 1.1120879650115967 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 7.7994947Losses:  8.81608361005783 1.1897794008255005 1.0851091146469116 1.4300701022148132
CurrentTrain: epoch  5, batch     7 | loss: 8.8160836Losses:  12.19469489902258 1.231918215751648 1.1549969911575317 4.368152521550655
CurrentTrain: epoch  5, batch     8 | loss: 12.1946949Losses:  13.229341387748718 0.9480445384979248 1.0530849695205688 6.02446973323822
CurrentTrain: epoch  5, batch     9 | loss: 13.2293414Losses:  11.18678268790245 1.4404829740524292 1.3135675191879272 2.803389400243759
CurrentTrain: epoch  5, batch    10 | loss: 11.1867827Losses:  31.782559007406235 1.1772968769073486 1.3208063840866089 24.229424089193344
CurrentTrain: epoch  5, batch    11 | loss: 31.7825590Losses:  10.230507880449295 1.129093885421753 1.1645591259002686 2.819128066301346
CurrentTrain: epoch  5, batch    12 | loss: 10.2305079Losses:  9.029602259397507 1.3016844987869263 1.3770339488983154 1.4012815654277802
CurrentTrain: epoch  5, batch    13 | loss: 9.0296023Losses:  9.047231435775757 1.1331946849822998 1.095940351486206 1.7028911113739014
CurrentTrain: epoch  5, batch    14 | loss: 9.0472314Losses:  9.000415533781052 1.0495293140411377 1.1543121337890625 1.4947697818279266
CurrentTrain: epoch  5, batch    15 | loss: 9.0004155Losses:  8.862370282411575 1.0962778329849243 1.275341510772705 1.4670789539813995
CurrentTrain: epoch  5, batch    16 | loss: 8.8623703Losses:  11.226186595857143 1.4111087322235107 1.4832099676132202 2.8664501532912254
CurrentTrain: epoch  5, batch    17 | loss: 11.2261866Losses:  11.55369571223855 1.237492561340332 1.0371198654174805 4.532222781330347
CurrentTrain: epoch  5, batch    18 | loss: 11.5536957Losses:  8.63089406490326 1.1920981407165527 1.2082325220108032 1.407528281211853
CurrentTrain: epoch  5, batch    19 | loss: 8.6308941Losses:  9.577731251716614 1.1457446813583374 1.1224015951156616 2.8076354265213013
CurrentTrain: epoch  5, batch    20 | loss: 9.5777313Losses:  12.748566210269928 1.207188606262207 1.348219633102417 5.74016147851944
CurrentTrain: epoch  5, batch    21 | loss: 12.7485662Losses:  9.900024835020304 1.209074854850769 1.1323795318603516 3.0237001813948154
CurrentTrain: epoch  5, batch    22 | loss: 9.9000248Losses:  10.9096799492836 1.1748043298721313 1.1381287574768066 3.2864919304847717
CurrentTrain: epoch  5, batch    23 | loss: 10.9096799Losses:  9.848254837095737 1.2254774570465088 1.154460072517395 2.8980671539902687
CurrentTrain: epoch  5, batch    24 | loss: 9.8482548Losses:  22.926384929567575 1.0743168592453003 1.2087745666503906 14.880913738161325
CurrentTrain: epoch  5, batch    25 | loss: 22.9263849Losses:  10.382117077708244 1.2433686256408691 1.2324002981185913 3.0716665238142014
CurrentTrain: epoch  5, batch    26 | loss: 10.3821171Losses:  8.86734253168106 1.105134129524231 1.07094407081604 1.3997684121131897
CurrentTrain: epoch  5, batch    27 | loss: 8.8673425Losses:  8.30514271184802 1.1339064836502075 1.216132640838623 1.4736316911876202
CurrentTrain: epoch  5, batch    28 | loss: 8.3051427Losses:  11.847098845988512 0.9683237075805664 1.152224063873291 4.3827514834702015
CurrentTrain: epoch  5, batch    29 | loss: 11.8470988Losses:  9.390760641545057 1.1485891342163086 1.0519731044769287 1.5237142853438854
CurrentTrain: epoch  5, batch    30 | loss: 9.3907606Losses:  11.628292210400105 1.2075519561767578 0.993094801902771 4.443668015301228
CurrentTrain: epoch  5, batch    31 | loss: 11.6282922Losses:  8.561081796884537 1.138289451599121 1.1327335834503174 1.422675997018814
CurrentTrain: epoch  5, batch    32 | loss: 8.5610818Losses:  22.960831835865974 1.1908297538757324 1.4540784358978271 14.455993846058846
CurrentTrain: epoch  5, batch    33 | loss: 22.9608318Losses:  10.48963487148285 1.0466171503067017 1.2276628017425537 2.8997997045516968
CurrentTrain: epoch  5, batch    34 | loss: 10.4896349Losses:  11.673040717840195 1.2512812614440918 1.3124616146087646 4.294250816106796
CurrentTrain: epoch  5, batch    35 | loss: 11.6730407Losses:  14.559900008141994 1.2748433351516724 1.1485174894332886 6.886554919183254
CurrentTrain: epoch  5, batch    36 | loss: 14.5599000Losses:  8.8366480730474 1.3511234521865845 1.3233788013458252 1.4356661699712276
CurrentTrain: epoch  5, batch    37 | loss: 8.8366481Losses:  8.821453277021646 1.1970806121826172 1.170697808265686 1.4660479463636875
CurrentTrain: epoch  5, batch    38 | loss: 8.8214533Losses:  8.449367642402649 1.0733445882797241 1.1788413524627686 1.4031811952590942
CurrentTrain: epoch  5, batch    39 | loss: 8.4493676Losses:  8.59794619679451 1.0132689476013184 1.3858757019042969 1.391008883714676
CurrentTrain: epoch  5, batch    40 | loss: 8.5979462Losses:  7.952544212341309 1.2878100872039795 1.2885351181030273 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 7.9525442Losses:  9.04330911487341 0.9630675911903381 1.146918773651123 1.4901384338736534
CurrentTrain: epoch  5, batch    42 | loss: 9.0433091Losses:  11.858731921762228 1.1637439727783203 1.2064905166625977 4.26748913154006
CurrentTrain: epoch  5, batch    43 | loss: 11.8587319Losses:  7.608529090881348 1.0607073307037354 1.3305236101150513 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 7.6085291Losses:  6.937536716461182 1.143412470817566 1.2063164710998535 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 6.9375367Losses:  10.589556280523539 1.5585055351257324 1.4314584732055664 2.987173620611429
CurrentTrain: epoch  5, batch    46 | loss: 10.5895563Losses:  9.808816969394684 1.2319085597991943 1.2154216766357422 1.402208387851715
CurrentTrain: epoch  5, batch    47 | loss: 9.8088170Losses:  11.067153301090002 1.4401899576187134 1.235437273979187 2.865481700748205
CurrentTrain: epoch  5, batch    48 | loss: 11.0671533Losses:  9.224959880113602 1.2315988540649414 1.2076170444488525 1.4074845612049103
CurrentTrain: epoch  5, batch    49 | loss: 9.2249599Losses:  8.863853126764297 1.1188191175460815 1.2081738710403442 1.4025899469852448
CurrentTrain: epoch  5, batch    50 | loss: 8.8638531Losses:  17.334047697484493 1.2067382335662842 1.2497928142547607 10.222960852086544
CurrentTrain: epoch  5, batch    51 | loss: 17.3340477Losses:  11.73383042961359 1.4664714336395264 1.373234510421753 4.305141426622868
CurrentTrain: epoch  5, batch    52 | loss: 11.7338304Losses:  15.544529631733894 1.0638493299484253 1.1711573600769043 8.500124171376228
CurrentTrain: epoch  5, batch    53 | loss: 15.5445296Losses:  8.64480448141694 1.0414522886276245 1.1694014072418213 1.4876551665365696
CurrentTrain: epoch  5, batch    54 | loss: 8.6448045Losses:  9.745273530483246 0.947769820690155 1.3515249490737915 2.8630189299583435
CurrentTrain: epoch  5, batch    55 | loss: 9.7452735Losses:  10.44105851650238 1.0899416208267212 1.2312045097351074 2.832950472831726
CurrentTrain: epoch  5, batch    56 | loss: 10.4410585Losses:  8.411072950810194 1.247804880142212 1.2491616010665894 1.4738867096602917
CurrentTrain: epoch  5, batch    57 | loss: 8.4110730Losses:  12.344282649457455 1.3458571434020996 1.4456603527069092 4.458355449140072
CurrentTrain: epoch  5, batch    58 | loss: 12.3442826Losses:  12.94340968132019 1.2173941135406494 1.1811237335205078 4.908174276351929
CurrentTrain: epoch  5, batch    59 | loss: 12.9434097Losses:  8.684266988188028 1.142985463142395 1.046821117401123 1.566001359373331
CurrentTrain: epoch  5, batch    60 | loss: 8.6842670Losses:  8.638840854167938 1.2789767980575562 1.0896304845809937 1.4345700144767761
CurrentTrain: epoch  5, batch    61 | loss: 8.6388409Losses:  11.912583768367767 1.3916187286376953 1.2138025760650635 4.3133081793785095
CurrentTrain: epoch  5, batch    62 | loss: 11.9125838Losses:  13.860763743519783 1.155870795249939 1.2194178104400635 6.662795260548592
CurrentTrain: epoch  6, batch     0 | loss: 13.8607637Losses:  9.978702012449503 1.199621319770813 1.1435543298721313 2.8349980749189854
CurrentTrain: epoch  6, batch     1 | loss: 9.9787020Losses:  8.739339523017406 1.1045541763305664 1.2884893417358398 1.439376525580883
CurrentTrain: epoch  6, batch     2 | loss: 8.7393395Losses:  8.961318291723728 1.268678903579712 1.2339441776275635 1.456829346716404
CurrentTrain: epoch  6, batch     3 | loss: 8.9613183Losses:  8.356159124523401 1.194684386253357 1.211852788925171 1.4437436200678349
CurrentTrain: epoch  6, batch     4 | loss: 8.3561591Losses:  8.845860466361046 1.1650722026824951 0.9706984758377075 1.5773706287145615
CurrentTrain: epoch  6, batch     5 | loss: 8.8458605Losses:  7.1310930252075195 1.1103074550628662 1.2442845106124878 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 7.1310930Losses:  8.538029942661524 1.2118330001831055 1.2978761196136475 1.4515755511820316
CurrentTrain: epoch  6, batch     7 | loss: 8.5380299Losses:  13.465511441230774 1.0814825296401978 1.1774734258651733 6.228003144264221
CurrentTrain: epoch  6, batch     8 | loss: 13.4655114Losses:  10.023172970861197 1.2712961435317993 1.217968463897705 3.0419360361993313
CurrentTrain: epoch  6, batch     9 | loss: 10.0231730Losses:  11.371420126408339 1.0712249279022217 1.1947224140167236 3.234678488224745
CurrentTrain: epoch  6, batch    10 | loss: 11.3714201Losses:  11.09372990950942 1.3845953941345215 1.3194031715393066 3.051386769860983
CurrentTrain: epoch  6, batch    11 | loss: 11.0937299Losses:  8.942670673131943 1.2192151546478271 1.098984718322754 1.4420870244503021
CurrentTrain: epoch  6, batch    12 | loss: 8.9426707Losses:  8.661492496728897 1.1199764013290405 1.3146759271621704 1.4015046656131744
CurrentTrain: epoch  6, batch    13 | loss: 8.6614925Losses:  8.694839108735323 1.2930330038070679 1.3777713775634766 1.4632407315075397
CurrentTrain: epoch  6, batch    14 | loss: 8.6948391Losses:  10.433357022702694 1.044421672821045 1.198899745941162 3.0292566046118736
CurrentTrain: epoch  6, batch    15 | loss: 10.4333570Losses:  6.837829113006592 1.1613389253616333 1.157382845878601 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 6.8378291Losses:  9.674411803483963 0.9487367868423462 1.0998668670654297 2.9116397202014923
CurrentTrain: epoch  6, batch    17 | loss: 9.6744118Losses:  8.479595478624105 1.1554620265960693 1.1657088994979858 1.5617492757737637
CurrentTrain: epoch  6, batch    18 | loss: 8.4795955Losses:  7.080244064331055 1.1890642642974854 1.1508262157440186 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 7.0802441Losses:  12.050865963101387 1.1699753999710083 1.274043083190918 4.627976253628731
CurrentTrain: epoch  6, batch    20 | loss: 12.0508660Losses:  8.602342665195465 1.2384440898895264 1.1242616176605225 1.4258108735084534
CurrentTrain: epoch  6, batch    21 | loss: 8.6023427Losses:  9.787403233349323 1.2868156433105469 1.141840934753418 2.8634554222226143
CurrentTrain: epoch  6, batch    22 | loss: 9.7874032Losses:  8.504366278648376 0.978281557559967 1.2241612672805786 1.4254173040390015
CurrentTrain: epoch  6, batch    23 | loss: 8.5043663Losses:  11.373693715780973 1.1484639644622803 1.0926527976989746 4.2999751679599285
CurrentTrain: epoch  6, batch    24 | loss: 11.3736937Losses:  10.232848819345236 1.200693130493164 1.3192822933197021 2.9026266895234585
CurrentTrain: epoch  6, batch    25 | loss: 10.2328488Losses:  8.987937696278095 1.3911573886871338 1.3038349151611328 1.457112081348896
CurrentTrain: epoch  6, batch    26 | loss: 8.9879377Losses:  8.377842336893082 1.1048794984817505 1.188377022743225 1.444782167673111
CurrentTrain: epoch  6, batch    27 | loss: 8.3778423Losses:  12.738393846899271 1.1014646291732788 1.2255942821502686 5.84557444229722
CurrentTrain: epoch  6, batch    28 | loss: 12.7383938Losses:  13.031703799962997 1.200850486755371 1.1160739660263062 5.750943511724472
CurrentTrain: epoch  6, batch    29 | loss: 13.0317038Losses:  11.21965605020523 1.1884479522705078 1.2601686716079712 4.231836378574371
CurrentTrain: epoch  6, batch    30 | loss: 11.2196561Losses:  8.561203390359879 1.1854345798492432 1.189509391784668 1.4131292402744293
CurrentTrain: epoch  6, batch    31 | loss: 8.5612034Losses:  10.160434380173683 1.1815226078033447 1.2960067987442017 2.957797184586525
CurrentTrain: epoch  6, batch    32 | loss: 10.1604344Losses:  8.018450558185577 1.0897669792175293 1.0360805988311768 1.399750530719757
CurrentTrain: epoch  6, batch    33 | loss: 8.0184506Losses:  7.1708550453186035 1.032888412475586 1.1118847131729126 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 7.1708550Losses:  10.897091254591942 0.9741134643554688 1.0732953548431396 3.3145097345113754
CurrentTrain: epoch  6, batch    35 | loss: 10.8970913Losses:  10.081867422908545 1.1048970222473145 1.1534956693649292 2.9206711910665035
CurrentTrain: epoch  6, batch    36 | loss: 10.0818674Losses:  7.299647331237793 1.2432246208190918 1.1584599018096924 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.2996473Losses:  14.156542606651783 0.8024277687072754 1.1726665496826172 7.551420517265797
CurrentTrain: epoch  6, batch    38 | loss: 14.1565426Losses:  6.770068168640137 0.973164439201355 1.0400047302246094 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 6.7700682Losses:  7.325327396392822 1.2763817310333252 1.2543940544128418 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 7.3253274Losses:  8.346540778875351 1.143740177154541 1.2876098155975342 1.4024756848812103
CurrentTrain: epoch  6, batch    41 | loss: 8.3465408Losses:  10.662829034030437 1.1543705463409424 1.278452754020691 2.840557686984539
CurrentTrain: epoch  6, batch    42 | loss: 10.6628290Losses:  10.379234194755554 1.2719690799713135 1.2789297103881836 3.0596483945846558
CurrentTrain: epoch  6, batch    43 | loss: 10.3792342Losses:  8.659444242715836 1.264639973640442 1.4509694576263428 1.4601081907749176
CurrentTrain: epoch  6, batch    44 | loss: 8.6594442Losses:  8.637466043233871 1.2676751613616943 1.343930959701538 1.399317353963852
CurrentTrain: epoch  6, batch    45 | loss: 8.6374660Losses:  10.479141652584076 1.1166322231292725 1.187061071395874 2.3127302527427673
CurrentTrain: epoch  6, batch    46 | loss: 10.4791417Losses:  9.657532967627048 0.999665379524231 1.1722102165222168 2.8704188242554665
CurrentTrain: epoch  6, batch    47 | loss: 9.6575330Losses:  8.380570497363806 1.1209200620651245 1.2720650434494019 1.4690047167241573
CurrentTrain: epoch  6, batch    48 | loss: 8.3805705Losses:  8.763800293207169 1.2171963453292847 1.2581455707550049 1.4278055727481842
CurrentTrain: epoch  6, batch    49 | loss: 8.7638003Losses:  8.176998108625412 1.0129215717315674 1.1115211248397827 1.4089417159557343
CurrentTrain: epoch  6, batch    50 | loss: 8.1769981Losses:  7.076786994934082 1.183104395866394 1.2547340393066406 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 7.0767870Losses:  7.023859977722168 0.9244104027748108 1.214921474456787 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 7.0238600Losses:  11.085119303315878 1.0765395164489746 1.2277653217315674 4.376175936311483
CurrentTrain: epoch  6, batch    53 | loss: 11.0851193Losses:  6.71843147277832 1.150578498840332 1.1446266174316406 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 6.7184315Losses:  11.418903313577175 1.2441799640655518 1.4183924198150635 4.292221985757351
CurrentTrain: epoch  6, batch    55 | loss: 11.4189033Losses:  8.199578173458576 0.9619072675704956 1.1892510652542114 1.464498408138752
CurrentTrain: epoch  6, batch    56 | loss: 8.1995782Losses:  9.69466257840395 1.1283923387527466 1.060734748840332 2.925450809299946
CurrentTrain: epoch  6, batch    57 | loss: 9.6946626Losses:  8.394152253866196 1.0879309177398682 1.3356776237487793 1.432725042104721
CurrentTrain: epoch  6, batch    58 | loss: 8.3941523Losses:  8.622276447713375 1.0704073905944824 1.1848571300506592 1.4289647564291954
CurrentTrain: epoch  6, batch    59 | loss: 8.6222764Losses:  6.6796698570251465 1.1171408891677856 1.264388918876648 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 6.6796699Losses:  8.019861705601215 1.0705630779266357 1.253335952758789 1.405776508152485
CurrentTrain: epoch  6, batch    61 | loss: 8.0198617Losses:  10.955358982086182 0.9506576061248779 1.116051197052002 4.249547004699707
CurrentTrain: epoch  6, batch    62 | loss: 10.9553590Losses:  8.357313599437475 1.0325875282287598 1.3129985332489014 1.4514202736318111
CurrentTrain: epoch  7, batch     0 | loss: 8.3573136Losses:  9.4265196621418 0.9029536247253418 1.1158580780029297 2.815809041261673
CurrentTrain: epoch  7, batch     1 | loss: 9.4265197Losses:  6.733937740325928 0.9353506565093994 1.2159513235092163 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 6.7339377Losses:  14.708896443247795 1.2846237421035767 1.2870066165924072 7.646098896861076
CurrentTrain: epoch  7, batch     3 | loss: 14.7088964Losses:  14.037602186203003 0.9952141046524048 1.1976563930511475 7.54874587059021
CurrentTrain: epoch  7, batch     4 | loss: 14.0376022Losses:  9.340783447027206 0.9399697780609131 1.138392448425293 2.802248328924179
CurrentTrain: epoch  7, batch     5 | loss: 9.3407834Losses:  6.684425354003906 0.9424538612365723 1.0242588520050049 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 6.6844254Losses:  12.195804983377457 0.9417738914489746 1.153481364250183 5.651175409555435
CurrentTrain: epoch  7, batch     7 | loss: 12.1958050Losses:  8.07948249578476 0.9937047362327576 1.1970314979553223 1.4333028197288513
CurrentTrain: epoch  7, batch     8 | loss: 8.0794825Losses:  8.332939267158508 1.23567533493042 1.2211954593658447 1.433583378791809
CurrentTrain: epoch  7, batch     9 | loss: 8.3329393Losses:  11.576961815357208 1.0912590026855469 1.3432304859161377 4.4648526310920715
CurrentTrain: epoch  7, batch    10 | loss: 11.5769618Losses:  6.7862772941589355 1.0150282382965088 1.2277626991271973 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 6.7862773Losses:  10.434967935085297 1.0682454109191895 1.1421724557876587 2.8228959441184998
CurrentTrain: epoch  7, batch    12 | loss: 10.4349679Losses:  8.051340848207474 1.055300235748291 1.336530089378357 1.401802808046341
CurrentTrain: epoch  7, batch    13 | loss: 8.0513408Losses:  10.22167119383812 1.2339563369750977 1.2394044399261475 3.35995015501976
CurrentTrain: epoch  7, batch    14 | loss: 10.2216712Losses:  10.094603210687637 1.1064043045043945 1.3813912868499756 2.804544121026993
CurrentTrain: epoch  7, batch    15 | loss: 10.0946032Losses:  14.181300681084394 1.1543060541152954 1.320222020149231 7.257804911583662
CurrentTrain: epoch  7, batch    16 | loss: 14.1813007Losses:  8.210497289896011 1.0895264148712158 1.3564486503601074 1.3976906836032867
CurrentTrain: epoch  7, batch    17 | loss: 8.2104973Losses:  6.816960334777832 1.0132970809936523 1.177283525466919 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 6.8169603Losses:  12.217738799750805 0.9856685400009155 1.1185919046401978 5.844944648444653
CurrentTrain: epoch  7, batch    19 | loss: 12.2177388Losses:  8.408667534589767 1.1749927997589111 1.378184199333191 1.4054746329784393
CurrentTrain: epoch  7, batch    20 | loss: 8.4086675Losses:  10.910811621695757 1.0347435474395752 1.2136130332946777 4.299864489585161
CurrentTrain: epoch  7, batch    21 | loss: 10.9108116Losses:  9.39771881699562 1.0591591596603394 1.210697889328003 2.8066467344760895
CurrentTrain: epoch  7, batch    22 | loss: 9.3977188Losses:  7.210463523864746 1.2169857025146484 1.4521479606628418 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 7.2104635Losses:  7.061781883239746 1.1716325283050537 1.4135921001434326 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 7.0617819Losses:  8.86419465765357 1.206193208694458 1.3392443656921387 1.4374659322202206
CurrentTrain: epoch  7, batch    25 | loss: 8.8641947Losses:  9.931264135986567 1.2424988746643066 1.2334423065185547 2.8489200808107853
CurrentTrain: epoch  7, batch    26 | loss: 9.9312641Losses:  8.036104649305344 1.1573925018310547 1.1010551452636719 1.419906109571457
CurrentTrain: epoch  7, batch    27 | loss: 8.0361046Losses:  9.162515729665756 1.005394697189331 1.1148455142974854 2.7941990792751312
CurrentTrain: epoch  7, batch    28 | loss: 9.1625157Losses:  7.930533409118652 1.083230972290039 1.1310551166534424 1.3952093124389648
CurrentTrain: epoch  7, batch    29 | loss: 7.9305334Losses:  6.340753078460693 1.0883750915527344 0.98127681016922 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 6.3407531Losses:  13.970686197280884 1.3334637880325317 1.3638454675674438 7.071227788925171
CurrentTrain: epoch  7, batch    31 | loss: 13.9706862Losses:  9.297901932150126 1.015186071395874 1.180271029472351 2.8515403904020786
CurrentTrain: epoch  7, batch    32 | loss: 9.2979019Losses:  14.534840352833271 1.3393375873565674 1.183030366897583 7.698230512440205
CurrentTrain: epoch  7, batch    33 | loss: 14.5348404Losses:  10.653106175363064 0.807750940322876 1.3395917415618896 4.214182339608669
CurrentTrain: epoch  7, batch    34 | loss: 10.6531062Losses:  6.252440929412842 0.84589022397995 1.0955711603164673 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 6.2524409Losses:  11.541012790054083 1.0726783275604248 1.1822423934936523 4.340670611709356
CurrentTrain: epoch  7, batch    36 | loss: 11.5410128Losses:  10.008262317627668 1.302647590637207 1.2555238008499146 2.8879038505256176
CurrentTrain: epoch  7, batch    37 | loss: 10.0082623Losses:  9.51446944475174 1.1092896461486816 1.3075106143951416 2.826930344104767
CurrentTrain: epoch  7, batch    38 | loss: 9.5144694Losses:  6.8669962882995605 1.1016557216644287 1.192166805267334 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 6.8669963Losses:  9.507582157850266 1.054093599319458 1.2957758903503418 2.843354195356369
CurrentTrain: epoch  7, batch    40 | loss: 9.5075822Losses:  9.831826031208038 1.3675987720489502 1.2483904361724854 2.8185442090034485
CurrentTrain: epoch  7, batch    41 | loss: 9.8318260Losses:  6.765366554260254 1.306377649307251 1.184010624885559 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 6.7653666Losses:  8.066538393497467 0.9411786794662476 1.1752104759216309 1.4152866005897522
CurrentTrain: epoch  7, batch    43 | loss: 8.0665384Losses:  10.770340718328953 0.9713746309280396 1.0061413049697876 4.379324235022068
CurrentTrain: epoch  7, batch    44 | loss: 10.7703407Losses:  9.401731848716736 0.9987751245498657 1.279822826385498 2.8175166845321655
CurrentTrain: epoch  7, batch    45 | loss: 9.4017318Losses:  12.406769782304764 1.0687199831008911 1.2651667594909668 5.633416682481766
CurrentTrain: epoch  7, batch    46 | loss: 12.4067698Losses:  8.039606720209122 1.0534335374832153 1.3012216091156006 1.4004017412662506
CurrentTrain: epoch  7, batch    47 | loss: 8.0396067Losses:  7.989766716957092 1.0278499126434326 1.1154625415802002 1.4395605325698853
CurrentTrain: epoch  7, batch    48 | loss: 7.9897667Losses:  8.697473496198654 1.3506815433502197 1.301040530204773 1.39800164103508
CurrentTrain: epoch  7, batch    49 | loss: 8.6974735Losses:  7.334013938903809 1.0622122287750244 1.1565790176391602 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 7.3340139Losses:  8.234014179557562 1.0341169834136963 1.237626314163208 1.4235855601727962
CurrentTrain: epoch  7, batch    51 | loss: 8.2340142Losses:  10.886830478906631 1.181746482849121 1.1950393915176392 4.2726947367191315
CurrentTrain: epoch  7, batch    52 | loss: 10.8868305Losses:  7.943999290466309 1.0213758945465088 1.3222386837005615 1.4172039031982422
CurrentTrain: epoch  7, batch    53 | loss: 7.9439993Losses:  10.737919181585312 1.048896312713623 1.1913673877716064 4.240044921636581
CurrentTrain: epoch  7, batch    54 | loss: 10.7379192Losses:  6.78766393661499 1.156837821006775 1.181412696838379 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 6.7876639Losses:  6.939805507659912 1.3620314598083496 1.3667253255844116 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 6.9398055Losses:  6.976223945617676 0.9869683980941772 1.1514670848846436 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 6.9762239Losses:  12.260203305631876 0.9511691331863403 1.1687040328979492 5.882236424833536
CurrentTrain: epoch  7, batch    58 | loss: 12.2602033Losses:  6.340974807739258 0.8613957762718201 1.1974456310272217 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 6.3409748Losses:  8.201405826956034 1.2703018188476562 1.2411037683486938 1.4194467701017857
CurrentTrain: epoch  7, batch    60 | loss: 8.2014058Losses:  8.272301018238068 1.3406765460968018 1.237076997756958 1.411863625049591
CurrentTrain: epoch  7, batch    61 | loss: 8.2723010Losses:  6.2747039794921875 0.8555673360824585 1.1920690536499023 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 6.2747040Losses:  8.235967572778463 1.1606006622314453 1.186946988105774 1.4361686073243618
CurrentTrain: epoch  8, batch     0 | loss: 8.2359676Losses:  9.684460557997227 1.0170652866363525 1.2735544443130493 3.072328008711338
CurrentTrain: epoch  8, batch     1 | loss: 9.6844606Losses:  9.325524032115936 1.0298155546188354 1.1573660373687744 2.9072243571281433
CurrentTrain: epoch  8, batch     2 | loss: 9.3255240Losses:  7.666065484285355 1.0298668146133423 0.9584767818450928 1.427698403596878
CurrentTrain: epoch  8, batch     3 | loss: 7.6660655Losses:  7.977003693580627 1.0197654962539673 1.047478437423706 1.4117532968521118
CurrentTrain: epoch  8, batch     4 | loss: 7.9770037Losses:  6.784434795379639 1.3144292831420898 1.2276538610458374 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 6.7844348Losses:  6.545299053192139 1.0690743923187256 1.1992650032043457 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 6.5452991Losses:  9.54274833202362 1.1216691732406616 1.328196406364441 2.8205350637435913
CurrentTrain: epoch  8, batch     7 | loss: 9.5427483Losses:  9.438323222100735 1.0703980922698975 1.1862456798553467 2.841789446771145
CurrentTrain: epoch  8, batch     8 | loss: 9.4383232Losses:  10.934124439954758 1.2180867195129395 1.3802704811096191 4.204034775495529
CurrentTrain: epoch  8, batch     9 | loss: 10.9341244Losses:  6.732752799987793 1.2771135568618774 1.184828281402588 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 6.7327528Losses:  7.670157693326473 0.9481997489929199 1.0909382104873657 1.4459693655371666
CurrentTrain: epoch  8, batch    11 | loss: 7.6701577Losses:  7.9815628826618195 0.9509669542312622 1.1366539001464844 1.398246556520462
CurrentTrain: epoch  8, batch    12 | loss: 7.9815629Losses:  8.205536387860775 1.2375112771987915 1.2399295568466187 1.499375842511654
CurrentTrain: epoch  8, batch    13 | loss: 8.2055364Losses:  7.749417424201965 0.9425305128097534 1.1505955457687378 1.4001103639602661
CurrentTrain: epoch  8, batch    14 | loss: 7.7494174Losses:  8.440185397863388 1.2126035690307617 1.4412550926208496 1.4130381047725677
CurrentTrain: epoch  8, batch    15 | loss: 8.4401854Losses:  12.067461557686329 1.2673672437667847 1.0096542835235596 4.369023866951466
CurrentTrain: epoch  8, batch    16 | loss: 12.0674616Losses:  6.502068519592285 1.1505342721939087 1.0745391845703125 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 6.5020685Losses:  7.872936725616455 0.8619004487991333 1.2066704034805298 1.537099838256836
CurrentTrain: epoch  8, batch    18 | loss: 7.8729367Losses:  10.542273689061403 0.8480435013771057 1.0993385314941406 4.317222286015749
CurrentTrain: epoch  8, batch    19 | loss: 10.5422737Losses:  6.8985772132873535 1.207165241241455 1.4090566635131836 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 6.8985772Losses:  6.819308757781982 1.1421643495559692 1.365752100944519 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 6.8193088Losses:  12.840890146791935 1.2768781185150146 1.2703429460525513 5.871380068361759
CurrentTrain: epoch  8, batch    22 | loss: 12.8408901Losses:  16.446661181747913 0.9357234239578247 1.277031660079956 9.953067012131214
CurrentTrain: epoch  8, batch    23 | loss: 16.4466612Losses:  6.572025775909424 1.1359100341796875 1.1752163171768188 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 6.5720258Losses:  18.911676838994026 0.9253247380256653 1.1331024169921875 12.717224076390266
CurrentTrain: epoch  8, batch    25 | loss: 18.9116768Losses:  7.811968922615051 0.8789162635803223 1.2796647548675537 1.3937712907791138
CurrentTrain: epoch  8, batch    26 | loss: 7.8119689Losses:  17.282538149505854 1.3609342575073242 1.2790400981903076 10.067075464874506
CurrentTrain: epoch  8, batch    27 | loss: 17.2825381Losses:  6.228861331939697 0.8253790140151978 1.2262754440307617 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 6.2288613Losses:  10.829290300607681 1.0096241235733032 1.2593469619750977 4.2826036512851715
CurrentTrain: epoch  8, batch    29 | loss: 10.8292903Losses:  7.901425123214722 1.0839085578918457 1.1454976797103882 1.4566876888275146
CurrentTrain: epoch  8, batch    30 | loss: 7.9014251Losses:  12.566407985985279 0.8500576019287109 1.1959612369537354 6.2923563197255135
CurrentTrain: epoch  8, batch    31 | loss: 12.5664080Losses:  9.348380506038666 1.169703722000122 1.1497464179992676 2.7841500639915466
CurrentTrain: epoch  8, batch    32 | loss: 9.3483805Losses:  6.256670951843262 0.9302799105644226 1.105175495147705 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 6.2566710Losses:  9.022103186696768 1.2927801609039307 1.2005598545074463 1.4232987128198147
CurrentTrain: epoch  8, batch    34 | loss: 9.0221032Losses:  8.068592764437199 1.2486008405685425 1.1380420923233032 1.4542600885033607
CurrentTrain: epoch  8, batch    35 | loss: 8.0685928Losses:  8.103904843330383 1.212268352508545 1.2026352882385254 1.3940783739089966
CurrentTrain: epoch  8, batch    36 | loss: 8.1039048Losses:  6.330487251281738 0.9383362531661987 1.205871820449829 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.3304873Losses:  15.639954086393118 1.353087067604065 1.3137587308883667 8.659873958677053
CurrentTrain: epoch  8, batch    38 | loss: 15.6399541Losses:  10.920073568820953 1.2288527488708496 1.3329660892486572 4.216162741184235
CurrentTrain: epoch  8, batch    39 | loss: 10.9200736Losses:  7.6470703184604645 0.9304720759391785 1.0879625082015991 1.4028905928134918
CurrentTrain: epoch  8, batch    40 | loss: 7.6470703Losses:  7.909353703260422 1.0044622421264648 1.2192738056182861 1.4086184203624725
CurrentTrain: epoch  8, batch    41 | loss: 7.9093537Losses:  9.34958678483963 1.1214709281921387 1.1156067848205566 2.8387873768806458
CurrentTrain: epoch  8, batch    42 | loss: 9.3495868Losses:  16.075054861605167 1.3594155311584473 1.5386507511138916 9.01339790970087
CurrentTrain: epoch  8, batch    43 | loss: 16.0750549Losses:  7.888386577367783 1.1204016208648682 1.1480815410614014 1.3888987004756927
CurrentTrain: epoch  8, batch    44 | loss: 7.8883866Losses:  7.596068978309631 0.8462448120117188 1.061414122581482 1.3905373811721802
CurrentTrain: epoch  8, batch    45 | loss: 7.5960690Losses:  10.448163352906704 0.7751690149307251 1.1244316101074219 4.401836238801479
CurrentTrain: epoch  8, batch    46 | loss: 10.4481634Losses:  7.569015558809042 0.980542004108429 0.9990886449813843 1.4223142229020596
CurrentTrain: epoch  8, batch    47 | loss: 7.5690156Losses:  7.001211166381836 1.0631420612335205 1.0896508693695068 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 7.0012112Losses:  6.278077125549316 0.8483855724334717 1.2171213626861572 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 6.2780771Losses:  9.545111149549484 1.0852727890014648 1.3868229389190674 2.815614193677902
CurrentTrain: epoch  8, batch    50 | loss: 9.5451111Losses:  7.950544118881226 1.0643067359924316 1.1870659589767456 1.434091329574585
CurrentTrain: epoch  8, batch    51 | loss: 7.9505441Losses:  7.691560506820679 0.8057280778884888 1.1843879222869873 1.4344737529754639
CurrentTrain: epoch  8, batch    52 | loss: 7.6915605Losses:  6.402414798736572 0.9800100326538086 1.2541300058364868 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 6.4024148Losses:  10.585080049932003 0.9889717102050781 0.9686077833175659 4.381654642522335
CurrentTrain: epoch  8, batch    54 | loss: 10.5850800Losses:  7.70080754160881 0.8497024178504944 1.1550016403198242 1.441524475812912
CurrentTrain: epoch  8, batch    55 | loss: 7.7008075Losses:  11.1284144744277 1.2115939855575562 1.2063500881195068 4.488394103944302
CurrentTrain: epoch  8, batch    56 | loss: 11.1284145Losses:  10.409154370427132 0.918595016002655 1.0753562450408936 4.230251744389534
CurrentTrain: epoch  8, batch    57 | loss: 10.4091544Losses:  7.8805937469005585 0.9621206521987915 1.2342547178268433 1.3943838775157928
CurrentTrain: epoch  8, batch    58 | loss: 7.8805937Losses:  7.826139003038406 0.9698795080184937 1.2077090740203857 1.4112334549427032
CurrentTrain: epoch  8, batch    59 | loss: 7.8261390Losses:  6.147351264953613 0.84052574634552 1.1704518795013428 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 6.1473513Losses:  12.52601145207882 1.3864333629608154 1.3282568454742432 5.609339699149132
CurrentTrain: epoch  8, batch    61 | loss: 12.5260115Losses:  8.105074375867844 1.0226472616195679 1.536923885345459 1.4077491462230682
CurrentTrain: epoch  8, batch    62 | loss: 8.1050744Losses:  9.873205434530973 1.1387913227081299 1.288265347480774 3.293961774557829
CurrentTrain: epoch  9, batch     0 | loss: 9.8732054Losses:  6.363988876342773 1.1150939464569092 1.114713430404663 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 6.3639889Losses:  6.278884410858154 0.8765116333961487 1.2062002420425415 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 6.2788844Losses:  9.280020862817764 1.0665593147277832 1.2306747436523438 2.8251821100711823
CurrentTrain: epoch  9, batch     3 | loss: 9.2800209Losses:  9.551603078842163 1.2315342426300049 1.3204536437988281 2.809821844100952
CurrentTrain: epoch  9, batch     4 | loss: 9.5516031Losses:  10.463142074644566 0.8586798906326294 1.1448394060134888 4.270756401121616
CurrentTrain: epoch  9, batch     5 | loss: 10.4631421Losses:  7.658283859491348 0.9836485385894775 1.099823236465454 1.402127891778946
CurrentTrain: epoch  9, batch     6 | loss: 7.6582839Losses:  9.47229915857315 1.2481683492660522 1.2210230827331543 2.8328352570533752
CurrentTrain: epoch  9, batch     7 | loss: 9.4722992Losses:  9.333861380815506 1.0525705814361572 1.34987473487854 2.8008337318897247
CurrentTrain: epoch  9, batch     8 | loss: 9.3338614Losses:  6.505610942840576 1.0284950733184814 1.3593286275863647 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 6.5056109Losses:  9.64513824135065 1.129673957824707 1.4280366897583008 2.960230328142643
CurrentTrain: epoch  9, batch    10 | loss: 9.6451382Losses:  8.704143315553665 1.168809175491333 1.2952736616134644 1.4039542973041534
CurrentTrain: epoch  9, batch    11 | loss: 8.7041433Losses:  9.807365238666534 1.3983720541000366 1.3722565174102783 2.829852879047394
CurrentTrain: epoch  9, batch    12 | loss: 9.8073652Losses:  6.58150577545166 1.1430236101150513 1.2587368488311768 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 6.5815058Losses:  9.017064891755581 0.9324696063995361 1.0461735725402832 2.86305745691061
CurrentTrain: epoch  9, batch    14 | loss: 9.0170649Losses:  10.243506290018559 0.9187766313552856 1.033407211303711 4.2030681148171425
CurrentTrain: epoch  9, batch    15 | loss: 10.2435063Losses:  9.195076704025269 0.8846877813339233 1.2157520055770874 2.854722738265991
CurrentTrain: epoch  9, batch    16 | loss: 9.1950767Losses:  6.371268272399902 1.038172721862793 1.2169930934906006 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 6.3712683Losses:  7.756924957036972 0.9964890480041504 1.1828322410583496 1.4016850888729095
CurrentTrain: epoch  9, batch    18 | loss: 7.7569250Losses:  13.730934225022793 1.1997935771942139 1.2782306671142578 7.133358083665371
CurrentTrain: epoch  9, batch    19 | loss: 13.7309342Losses:  7.800732344388962 1.0412490367889404 1.2377369403839111 1.4071376025676727
CurrentTrain: epoch  9, batch    20 | loss: 7.8007323Losses:  8.132295995950699 0.9603490233421326 1.292280912399292 1.3965992033481598
CurrentTrain: epoch  9, batch    21 | loss: 8.1322960Losses:  6.188495635986328 1.038949966430664 1.0422790050506592 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 6.1884956Losses:  9.35374217107892 1.0215742588043213 1.26151442527771 2.882339049130678
CurrentTrain: epoch  9, batch    23 | loss: 9.3537422Losses:  6.868582725524902 1.0150481462478638 1.1244494915008545 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 6.8685827Losses:  10.549480982124805 1.0374925136566162 1.1291022300720215 4.304258413612843
CurrentTrain: epoch  9, batch    25 | loss: 10.5494810Losses:  6.447527885437012 0.8277331590652466 1.3101871013641357 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 6.4475279Losses:  9.201004207134247 1.0512186288833618 1.2402935028076172 2.823074996471405
CurrentTrain: epoch  9, batch    27 | loss: 9.2010042Losses:  9.348081730306149 1.1840251684188843 1.2056965827941895 2.870917461812496
CurrentTrain: epoch  9, batch    28 | loss: 9.3480817Losses:  9.160636220127344 0.7969222068786621 1.3494513034820557 2.857998166233301
CurrentTrain: epoch  9, batch    29 | loss: 9.1606362Losses:  7.790986564010382 1.0134150981903076 1.1812118291854858 1.4152450822293758
CurrentTrain: epoch  9, batch    30 | loss: 7.7909866Losses:  10.741456508636475 1.115814208984375 1.2636874914169312 4.270272731781006
CurrentTrain: epoch  9, batch    31 | loss: 10.7414565Losses:  15.066626965999603 0.9357386827468872 1.1777160167694092 8.785620152950287
CurrentTrain: epoch  9, batch    32 | loss: 15.0666270Losses:  10.619759939610958 1.0298912525177002 1.2045392990112305 4.287108801305294
CurrentTrain: epoch  9, batch    33 | loss: 10.6197599Losses:  6.451892852783203 1.1542813777923584 1.2146728038787842 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 6.4518929Losses:  12.196193918585777 1.0270237922668457 1.3297064304351807 5.691218599677086
CurrentTrain: epoch  9, batch    35 | loss: 12.1961939Losses:  9.375872552394867 0.9343612790107727 1.5334333181381226 2.7998008131980896
CurrentTrain: epoch  9, batch    36 | loss: 9.3758726Losses:  6.527924060821533 1.11179518699646 1.103905200958252 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.5279241Losses:  7.855617731809616 0.9245941638946533 1.2361631393432617 1.426993578672409
CurrentTrain: epoch  9, batch    38 | loss: 7.8556177Losses:  6.141209602355957 0.8061105012893677 1.1684179306030273 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 6.1412096Losses:  7.723380323499441 0.9965634942054749 1.1480790376663208 1.448592897504568
CurrentTrain: epoch  9, batch    40 | loss: 7.7233803Losses:  6.519393444061279 1.1689643859863281 1.2362117767333984 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 6.5193934Losses:  11.21289324760437 1.2705252170562744 1.2661685943603516 4.230994939804077
CurrentTrain: epoch  9, batch    42 | loss: 11.2128932Losses:  9.189926445484161 0.8806855082511902 1.3777596950531006 2.8231833577156067
CurrentTrain: epoch  9, batch    43 | loss: 9.1899264Losses:  7.7174612283706665 0.8975050449371338 1.2806861400604248 1.399762749671936
CurrentTrain: epoch  9, batch    44 | loss: 7.7174612Losses:  8.12586823105812 1.2781614065170288 1.2704545259475708 1.4222668707370758
CurrentTrain: epoch  9, batch    45 | loss: 8.1258682Losses:  9.556568618863821 1.1236844062805176 1.2736353874206543 2.858901973813772
CurrentTrain: epoch  9, batch    46 | loss: 9.5565686Losses:  9.221056014299393 1.0439187288284302 1.1899099349975586 2.822759658098221
CurrentTrain: epoch  9, batch    47 | loss: 9.2210560Losses:  6.271361827850342 0.9720779061317444 1.1571203470230103 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 6.2713618Losses:  9.146256744861603 0.9208672046661377 1.2068026065826416 2.8183215260505676
CurrentTrain: epoch  9, batch    49 | loss: 9.1462567Losses:  10.560656793415546 0.9176135063171387 1.1758055686950684 4.290107972919941
CurrentTrain: epoch  9, batch    50 | loss: 10.5606568Losses:  6.48996639251709 1.1166354417800903 1.2217848300933838 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 6.4899664Losses:  9.351818859577179 1.0330122709274292 1.3246721029281616 2.8540833592414856
CurrentTrain: epoch  9, batch    52 | loss: 9.3518189Losses:  8.986512877047062 0.885251522064209 1.0828197002410889 2.856653906404972
CurrentTrain: epoch  9, batch    53 | loss: 8.9865129Losses:  9.068935304880142 0.9805070757865906 1.18636155128479 2.819263368844986
CurrentTrain: epoch  9, batch    54 | loss: 9.0689353Losses:  10.597550094127655 0.9309036731719971 1.1717422008514404 4.284928023815155
CurrentTrain: epoch  9, batch    55 | loss: 10.5975501Losses:  10.695221859961748 1.038297176361084 1.143041968345642 4.353764493018389
CurrentTrain: epoch  9, batch    56 | loss: 10.6952219Losses:  15.341973222792149 1.2473992109298706 1.3422472476959229 8.697115816175938
CurrentTrain: epoch  9, batch    57 | loss: 15.3419732Losses:  7.6157176196575165 1.012999415397644 1.0863068103790283 1.4037043750286102
CurrentTrain: epoch  9, batch    58 | loss: 7.6157176Losses:  10.49822124838829 0.8014339208602905 1.2088780403137207 4.198696464300156
CurrentTrain: epoch  9, batch    59 | loss: 10.4982212Losses:  7.685831904411316 0.9772604703903198 1.0922632217407227 1.42379891872406
CurrentTrain: epoch  9, batch    60 | loss: 7.6858319Losses:  11.765145298093557 1.1303606033325195 1.1962623596191406 4.214241024106741
CurrentTrain: epoch  9, batch    61 | loss: 11.7651453Losses:  12.177136480808258 0.8073164224624634 1.326615333557129 5.883568346500397
CurrentTrain: epoch  9, batch    62 | loss: 12.1771365
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  8  clusters
Clusters:  [5 1 0 6 4 2 0 1 1 1 1 1 1 4 7 1 3 4 1 1]
Losses:  10.838106751441956 1.4253406524658203 1.2927749156951904 1.7140451669692993
CurrentTrain: epoch  0, batch     0 | loss: 10.8381068Losses:  15.026332333683968 1.5105617046356201 1.5243430137634277 6.159621670842171
CurrentTrain: epoch  0, batch     1 | loss: 15.0263323Losses:  11.7219697535038 1.4744229316711426 1.1492791175842285 2.9107324182987213
CurrentTrain: epoch  0, batch     2 | loss: 11.7219698Losses:  10.703906700015068 1.2696328163146973 0.7176060676574707 1.4198776930570602
CurrentTrain: epoch  0, batch     3 | loss: 10.7039067Losses:  9.332195281982422 1.4238431453704834 1.3963284492492676 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.3321953Losses:  10.428635060787201 1.4655053615570068 1.364598035812378 1.5812057852745056
CurrentTrain: epoch  1, batch     1 | loss: 10.4286351Losses:  9.327006481587887 1.5462110042572021 1.234169363975525 1.4949728474020958
CurrentTrain: epoch  1, batch     2 | loss: 9.3270065Losses:  10.191163145005703 1.5467400550842285 1.438981533050537 1.4555502757430077
CurrentTrain: epoch  1, batch     3 | loss: 10.1911631Losses:  7.549440383911133 1.3983408212661743 1.2430636882781982 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.5494404Losses:  11.19940185546875 1.4773545265197754 1.4060428142547607 3.06134033203125
CurrentTrain: epoch  2, batch     1 | loss: 11.1994019Losses:  8.172513961791992 1.451647400856018 1.3007628917694092 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 8.1725140Losses:  7.5821433290839195 1.611802577972412 1.5999884605407715 1.4737243875861168
CurrentTrain: epoch  2, batch     3 | loss: 7.5821433Losses:  11.005798935890198 1.3617873191833496 1.1628001928329468 3.1355921030044556
CurrentTrain: epoch  3, batch     0 | loss: 11.0057989Losses:  9.825546380132437 1.4197227954864502 1.4177707433700562 3.025523778051138
CurrentTrain: epoch  3, batch     1 | loss: 9.8255464Losses:  8.674735184758902 1.486918330192566 1.2932201623916626 1.4127517901360989
CurrentTrain: epoch  3, batch     2 | loss: 8.6747352Losses:  8.78216204047203 1.814537525177002 1.4444241523742676 1.4290011823177338
CurrentTrain: epoch  3, batch     3 | loss: 8.7821620Losses:  6.7174506187438965 1.4648613929748535 1.589665412902832 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 6.7174506Losses:  12.087044907733798 1.4834983348846436 1.303782343864441 5.178074074909091
CurrentTrain: epoch  4, batch     1 | loss: 12.0870449Losses:  6.737706184387207 1.4168641567230225 1.2651216983795166 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.7377062Losses:  8.467706959694624 1.2997303009033203 0.8066425323486328 1.5301421098411083
CurrentTrain: epoch  4, batch     3 | loss: 8.4677070Losses:  6.853482246398926 1.4358718395233154 1.4276793003082275 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.8534822Losses:  6.430538177490234 1.4470106363296509 1.3999593257904053 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 6.4305382Losses:  10.690953105688095 1.4118798971176147 1.304052710533142 4.146491378545761
CurrentTrain: epoch  5, batch     2 | loss: 10.6909531Losses:  7.17638086527586 1.2553324699401855 0.9573416709899902 1.4349234029650688
CurrentTrain: epoch  5, batch     3 | loss: 7.1763809Losses:  6.271773815155029 1.5170986652374268 1.395897388458252 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 6.2717738Losses:  6.142883777618408 1.4302175045013428 1.277747631072998 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 6.1428838Losses:  6.037971019744873 1.3772348165512085 1.3050403594970703 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 6.0379710Losses:  6.814175345003605 1.1882143020629883 1.406888484954834 1.5078217759728432
CurrentTrain: epoch  6, batch     3 | loss: 6.8141753Losses:  8.216288972645998 1.4728165864944458 1.2950536012649536 2.100105691701174
CurrentTrain: epoch  7, batch     0 | loss: 8.2162890Losses:  7.106819365173578 1.41891610622406 1.3187966346740723 1.4813339449465275
CurrentTrain: epoch  7, batch     1 | loss: 7.1068194Losses:  6.874989118427038 1.3461098670959473 1.3751513957977295 1.4532800577580929
CurrentTrain: epoch  7, batch     2 | loss: 6.8749891Losses:  5.846915677189827 1.2554435729980469 0.5756235122680664 1.4469675570726395
CurrentTrain: epoch  7, batch     3 | loss: 5.8469157Losses:  9.791200544685125 1.5611172914505005 1.300881266593933 4.283486749976873
CurrentTrain: epoch  8, batch     0 | loss: 9.7912005Losses:  8.395796716213226 1.38970947265625 1.462878704071045 2.8263019919395447
CurrentTrain: epoch  8, batch     1 | loss: 8.3957967Losses:  5.338685035705566 1.3116720914840698 1.3598039150238037 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 5.3386850Losses:  6.282178029417992 1.3371047973632812 1.1114091873168945 1.4223872274160385
CurrentTrain: epoch  8, batch     3 | loss: 6.2821780Losses:  6.796730607748032 1.3829946517944336 1.4249365329742432 1.4019523561000824
CurrentTrain: epoch  9, batch     0 | loss: 6.7967306Losses:  6.62988668680191 1.4384429454803467 1.2808113098144531 1.4364309906959534
CurrentTrain: epoch  9, batch     1 | loss: 6.6298867Losses:  7.152170650660992 1.3916728496551514 1.144087791442871 2.027239315211773
CurrentTrain: epoch  9, batch     2 | loss: 7.1521707Losses:  6.035947240889072 1.1495542526245117 1.239121437072754 1.4877328053116798
CurrentTrain: epoch  9, batch     3 | loss: 6.0359472
Losses:  4.904191017150879 1.1421760320663452 1.3935275077819824 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 4.9041910Losses:  9.911141935735941 1.357589840888977 1.350470781326294 5.804833952337503
MemoryTrain:  epoch  0, batch     1 | loss: 9.9111419Losses:  4.645066261291504 1.068345308303833 1.3446006774902344 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 4.6450663Losses:  9.16968721151352 1.5495227575302124 1.4261811971664429 5.63166469335556
MemoryTrain:  epoch  1, batch     1 | loss: 9.1696872Losses:  3.596757411956787 1.2018051147460938 1.3056272268295288 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.5967574Losses:  10.60681214183569 1.0016114711761475 1.7040746212005615 5.63396992534399
MemoryTrain:  epoch  2, batch     1 | loss: 10.6068121Losses:  3.8757104873657227 1.1403377056121826 1.4541128873825073 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.8757105Losses:  8.65474333241582 1.2978947162628174 1.2792117595672607 5.7884346432983875
MemoryTrain:  epoch  3, batch     1 | loss: 8.6547433Losses:  3.6489675045013428 1.2338851690292358 1.2074954509735107 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 3.6489675Losses:  8.449749637395144 0.9058492183685303 1.379656434059143 5.86394302919507
MemoryTrain:  epoch  4, batch     1 | loss: 8.4497496Losses:  3.1328225135803223 1.124395728111267 1.3919007778167725 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 3.1328225Losses:  10.258507613092661 1.3846200704574585 1.2552670240402222 5.729882124811411
MemoryTrain:  epoch  5, batch     1 | loss: 10.2585076Losses:  3.3361573219299316 1.1306984424591064 1.197861671447754 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 3.3361573Losses:  8.518367618322372 1.3676387071609497 1.3061634302139282 5.662475198507309
MemoryTrain:  epoch  6, batch     1 | loss: 8.5183676Losses:  3.5472824573516846 1.2010090351104736 1.368605136871338 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 3.5472825Losses:  8.155564274638891 1.0002483129501343 1.4106354713439941 5.648801293224096
MemoryTrain:  epoch  7, batch     1 | loss: 8.1555643Losses:  3.3631019592285156 1.1770906448364258 1.183729887008667 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 3.3631020Losses:  8.108396623283625 1.017229437828064 1.2417162656784058 5.712952706962824
MemoryTrain:  epoch  8, batch     1 | loss: 8.1083966Losses:  3.30122447013855 1.1819140911102295 1.1816954612731934 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 3.3012245Losses:  8.579910542815924 1.1120318174362183 1.6620086431503296 5.734365250915289
MemoryTrain:  epoch  9, batch     1 | loss: 8.5799105
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 96.02%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 78.28%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 75.69%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 73.94%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 73.31%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 72.45%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.39%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.72%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.74%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.05%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.88%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.99%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 92.85%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.78%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.63%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.41%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 92.03%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.72%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.62%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.50%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.24%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 91.04%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 91.14%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.96%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 90.59%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 90.16%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 89.61%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 89.13%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.79%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 88.27%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.19%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 87.07%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 86.65%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 86.19%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.63%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 85.19%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 84.69%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 84.26%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 83.84%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 83.35%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 84.35%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 84.43%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.55%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.70%   
cur_acc:  ['0.9464', '0.7639']
his_acc:  ['0.9464', '0.8470']
Clustering into  14  clusters
Clusters:  [13  2 11 10  3  1 12  0  6  6  2  0  0  3  9  2  7  1  2  2  0  2  8  2
  4  1  2  5  2  2]
Losses:  9.378551483154297 1.458021640777588 1.3336125612258911 1.5504951477050781
CurrentTrain: epoch  0, batch     0 | loss: 9.3785515Losses:  9.298735618591309 1.1436374187469482 1.1214666366577148 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 9.2987356Losses:  12.406054221093655 1.364454984664917 1.3997693061828613 4.189860068261623
CurrentTrain: epoch  0, batch     2 | loss: 12.4060542Losses:  12.489783611148596 1.2278375625610352 1.1096773147583008 1.6007207296788692
CurrentTrain: epoch  0, batch     3 | loss: 12.4897836Losses:  9.699014157056808 1.315426230430603 1.2654272317886353 1.424798458814621
CurrentTrain: epoch  1, batch     0 | loss: 9.6990142Losses:  15.280449155718088 1.3180111646652222 1.1860860586166382 8.266540292650461
CurrentTrain: epoch  1, batch     1 | loss: 15.2804492Losses:  10.146169662475586 1.2374942302703857 1.2974073886871338 1.4374656677246094
CurrentTrain: epoch  1, batch     2 | loss: 10.1461697Losses:  10.938479140400887 1.4989409446716309 1.3042607307434082 1.4633490592241287
CurrentTrain: epoch  1, batch     3 | loss: 10.9384791Losses:  15.539047837257385 1.297011137008667 1.439353585243225 7.822879910469055
CurrentTrain: epoch  2, batch     0 | loss: 15.5390478Losses:  10.096947073936462 1.3059544563293457 1.225150227546692 2.312749743461609
CurrentTrain: epoch  2, batch     1 | loss: 10.0969471Losses:  8.885798294097185 1.2998085021972656 1.0428845882415771 1.468491394072771
CurrentTrain: epoch  2, batch     2 | loss: 8.8857983Losses:  9.203829489648342 1.2316737174987793 0.8226628303527832 1.499114714562893
CurrentTrain: epoch  2, batch     3 | loss: 9.2038295Losses:  7.320046901702881 1.3735733032226562 1.210421085357666 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.3200469Losses:  10.034548316150904 1.2707879543304443 1.1390795707702637 3.423978839069605
CurrentTrain: epoch  3, batch     1 | loss: 10.0345483Losses:  10.44155215844512 1.197129487991333 1.3288369178771973 2.8597168885171413
CurrentTrain: epoch  3, batch     2 | loss: 10.4415522Losses:  9.95826805755496 1.1326637268066406 1.620863914489746 1.806257139891386
CurrentTrain: epoch  3, batch     3 | loss: 9.9582681Losses:  10.359099313616753 1.2903581857681274 1.1341828107833862 3.0973514765501022
CurrentTrain: epoch  4, batch     0 | loss: 10.3590993Losses:  7.856418818235397 1.2383002042770386 1.1707241535186768 1.4215433299541473
CurrentTrain: epoch  4, batch     1 | loss: 7.8564188Losses:  10.64339479804039 1.2448606491088867 1.2656407356262207 3.147350162267685
CurrentTrain: epoch  4, batch     2 | loss: 10.6433948Losses:  6.945193842053413 1.5851802825927734 1.1083221435546875 1.4933648854494095
CurrentTrain: epoch  4, batch     3 | loss: 6.9451938Losses:  6.924508094787598 1.254659652709961 1.4491982460021973 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.9245081Losses:  7.441258646547794 1.3035199642181396 1.186009407043457 1.4718687310814857
CurrentTrain: epoch  5, batch     1 | loss: 7.4412586Losses:  8.50877445936203 1.2408816814422607 1.1814312934875488 1.4127313494682312
CurrentTrain: epoch  5, batch     2 | loss: 8.5087745Losses:  9.0492612272501 1.325188159942627 1.3074183464050293 1.4670869261026382
CurrentTrain: epoch  5, batch     3 | loss: 9.0492612Losses:  9.18360111117363 1.356776237487793 1.1748812198638916 2.884813040494919
CurrentTrain: epoch  6, batch     0 | loss: 9.1836011Losses:  9.949901919811964 1.1575593948364258 1.3732542991638184 2.940337996929884
CurrentTrain: epoch  6, batch     1 | loss: 9.9499019Losses:  11.22514894604683 1.2425317764282227 1.0259125232696533 4.731110364198685
CurrentTrain: epoch  6, batch     2 | loss: 11.2251489Losses:  8.520752690732479 1.3509931564331055 0.8774223327636719 1.6383350118994713
CurrentTrain: epoch  6, batch     3 | loss: 8.5207527Losses:  5.742487907409668 1.2888431549072266 1.250089168548584 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.7424879Losses:  8.29867273569107 1.2935800552368164 1.2991905212402344 1.4088621735572815
CurrentTrain: epoch  7, batch     1 | loss: 8.2986727Losses:  11.356306057423353 1.1858835220336914 1.2300286293029785 4.764789562672377
CurrentTrain: epoch  7, batch     2 | loss: 11.3563061Losses:  8.230437502264977 1.0461649894714355 0.8174376487731934 1.5916006416082382
CurrentTrain: epoch  7, batch     3 | loss: 8.2304375Losses:  10.817161466926336 1.1760562658309937 1.188220500946045 4.602341081947088
CurrentTrain: epoch  8, batch     0 | loss: 10.8171615Losses:  9.511796656996012 1.231877088546753 1.0955467224121094 2.8740822710096836
CurrentTrain: epoch  8, batch     1 | loss: 9.5117967Losses:  9.769007943570614 1.2698630094528198 1.327870488166809 4.32062079757452
CurrentTrain: epoch  8, batch     2 | loss: 9.7690079Losses:  7.598418831825256 1.3031625747680664 0.8471345901489258 1.395516037940979
CurrentTrain: epoch  8, batch     3 | loss: 7.5984188Losses:  6.0720624923706055 1.1999773979187012 1.1846888065338135 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 6.0720625Losses:  7.11530014872551 1.2870793342590332 1.2847732305526733 1.4020733535289764
CurrentTrain: epoch  9, batch     1 | loss: 7.1153001Losses:  7.882205933332443 1.2114715576171875 0.984357476234436 1.4733910262584686
CurrentTrain: epoch  9, batch     2 | loss: 7.8822059Losses:  5.5960848107934 1.224710464477539 0.8701486587524414 1.4164278283715248
CurrentTrain: epoch  9, batch     3 | loss: 5.5960848
Losses:  3.7428221702575684 1.0802106857299805 1.333113193511963 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.7428222Losses:  3.567598342895508 1.1789069175720215 1.317380428314209 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 3.5675983Losses:  3.475653648376465 1.0068693161010742 1.2029656171798706 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.4756536Losses:  4.177667617797852 1.2905089855194092 1.4335553646087646 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 4.1776676Losses:  4.0272698402404785 1.1107560396194458 1.4550750255584717 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 4.0272698Losses:  2.888730525970459 1.1338269710540771 1.4399663209915161 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.8887305Losses:  3.2095112800598145 1.0779473781585693 1.5633662939071655 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.2095113Losses:  3.058980941772461 1.1491726636886597 1.1615606546401978 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 3.0589809Losses:  2.4845170974731445 1.0113362073898315 1.271073341369629 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.4845171Losses:  3.4885849952697754 1.2407824993133545 1.2839016914367676 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 3.4885850Losses:  2.859252691268921 1.0593408346176147 1.3703844547271729 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.8592527Losses:  3.0038018226623535 1.1754229068756104 1.3998724222183228 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 3.0038018Losses:  3.172595500946045 1.1266472339630127 1.398754358291626 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 3.1725955Losses:  2.6952476501464844 1.1121653318405151 1.4751619100570679 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.6952477Losses:  2.4906630516052246 1.1518089771270752 1.230265736579895 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.4906631Losses:  3.243588447570801 1.0380163192749023 1.5627951622009277 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 3.2435884Losses:  2.5511441230773926 1.088280200958252 1.323331356048584 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.5511441Losses:  2.8463382720947266 1.0822237730026245 1.5110862255096436 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.8463383Losses:  2.449589252471924 0.9254815578460693 1.3818936347961426 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.4495893Losses:  2.4325196743011475 1.216217279434204 1.1389625072479248 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.4325197
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 56.64%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.32%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 70.44%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 69.90%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 69.56%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.15%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 91.27%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.02%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.08%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.19%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.26%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.67%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.67%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.71%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.54%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 90.43%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 90.16%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 90.09%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 89.61%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 89.36%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 88.97%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 88.88%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 88.51%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.27%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 88.06%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 87.63%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.23%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 86.78%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 86.39%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 86.08%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 85.84%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 85.73%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 85.50%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 85.27%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 84.99%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 84.71%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.29%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.70%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 83.04%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.45%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 81.76%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 81.19%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.75%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.48%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.50%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 81.20%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 80.42%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 80.04%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 79.62%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 79.15%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.37%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.48%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 79.36%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 79.02%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 78.68%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 78.35%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.10%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 77.73%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 78.39%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 78.12%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 78.02%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 77.78%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.11%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.20%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.73%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.03%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 79.01%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 79.00%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.90%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 78.80%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 78.58%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 78.49%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 78.19%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 77.96%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 77.74%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 77.59%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 77.36%   
cur_acc:  ['0.9464', '0.7639', '0.6915']
his_acc:  ['0.9464', '0.8470', '0.7736']
Clustering into  19  clusters
Clusters:  [15  3 13 16  1 18 12  8  2  2  0  8  5  1  3  0 17 14  2  2  6  2 10  2
  9 14  0 11  3  2  2  6  6  0  7  2  5  4  2  3]
Losses:  8.191886901855469 1.4430840015411377 1.2845032215118408 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 8.1918869Losses:  9.420569751411676 1.475730299949646 1.4241712093353271 1.463612888008356
CurrentTrain: epoch  0, batch     1 | loss: 9.4205698Losses:  15.396922584623098 1.462204933166504 1.347529649734497 8.052603241056204
CurrentTrain: epoch  0, batch     2 | loss: 15.3969226Losses:  9.571333151310682 1.655759334564209 1.6014752388000488 1.5299970917403698
CurrentTrain: epoch  0, batch     3 | loss: 9.5713332Losses:  10.778708428144455 1.4513205289840698 1.5885648727416992 2.9436826407909393
CurrentTrain: epoch  1, batch     0 | loss: 10.7787084Losses:  6.873455047607422 1.4651200771331787 1.4372799396514893 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 6.8734550Losses:  7.891583766788244 1.4774715900421143 1.388089656829834 1.4485329054296017
CurrentTrain: epoch  1, batch     2 | loss: 7.8915838Losses:  7.293682813644409 1.3430604934692383 1.040562629699707 1.4234893321990967
CurrentTrain: epoch  1, batch     3 | loss: 7.2936828Losses:  11.15740242600441 1.529167652130127 1.4410688877105713 4.239219099283218
CurrentTrain: epoch  2, batch     0 | loss: 11.1574024Losses:  9.230349127203226 1.3350799083709717 1.2403218746185303 2.8745742477476597
CurrentTrain: epoch  2, batch     1 | loss: 9.2303491Losses:  8.85067505761981 1.417051076889038 1.4507944583892822 2.9168095104396343
CurrentTrain: epoch  2, batch     2 | loss: 8.8506751Losses:  10.492580145597458 0.8268928527832031 1.0 6.635585516691208
CurrentTrain: epoch  2, batch     3 | loss: 10.4925801Losses:  8.00350771099329 1.3167929649353027 1.1555132865905762 2.0101505294442177
CurrentTrain: epoch  3, batch     0 | loss: 8.0035077Losses:  10.329698421061039 1.5309867858886719 1.3463668823242188 4.287987567484379
CurrentTrain: epoch  3, batch     1 | loss: 10.3296984Losses:  8.558920193463564 1.3519287109375 1.5009536743164062 2.928251553326845
CurrentTrain: epoch  3, batch     2 | loss: 8.5589202Losses:  5.911020740866661 0.9694733619689941 1.2534050941467285 1.5108570903539658
CurrentTrain: epoch  3, batch     3 | loss: 5.9110207Losses:  7.303354844450951 1.3645617961883545 1.3210740089416504 1.4787407964468002
CurrentTrain: epoch  4, batch     0 | loss: 7.3033548Losses:  5.881453037261963 1.3622567653656006 1.390184760093689 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 5.8814530Losses:  7.116421163082123 1.388843297958374 1.173377513885498 1.4342994093894958
CurrentTrain: epoch  4, batch     2 | loss: 7.1164212Losses:  5.344808757305145 1.188812255859375 0.6493406295776367 1.423393189907074
CurrentTrain: epoch  4, batch     3 | loss: 5.3448088Losses:  8.077143393456936 1.3142701387405396 1.2598686218261719 2.898322306573391
CurrentTrain: epoch  5, batch     0 | loss: 8.0771434Losses:  5.4498186111450195 1.3733359575271606 1.267625093460083 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.4498186Losses:  7.047408256679773 1.3474302291870117 1.0721465349197388 1.557443294674158
CurrentTrain: epoch  5, batch     2 | loss: 7.0474083Losses:  7.334080219268799 1.3447413444519043 1.8580927848815918 1.5740966796875
CurrentTrain: epoch  5, batch     3 | loss: 7.3340802Losses:  5.54629373550415 1.361875295639038 1.4371752738952637 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.5462937Losses:  6.785879135131836 1.357384443283081 1.286746859550476 1.4163236618041992
CurrentTrain: epoch  6, batch     1 | loss: 6.7858791Losses:  10.953747913241386 1.3024975061416626 0.9236175417900085 5.940832778811455
CurrentTrain: epoch  6, batch     2 | loss: 10.9537479Losses:  6.668689034879208 1.3682737350463867 1.7992095947265625 1.549375794827938
CurrentTrain: epoch  6, batch     3 | loss: 6.6686890Losses:  6.552944533526897 1.2559744119644165 1.170137643814087 1.4311069175601006
CurrentTrain: epoch  7, batch     0 | loss: 6.5529445Losses:  5.246906280517578 1.2722784280776978 1.1632273197174072 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.2469063Losses:  9.430237397551537 1.427282452583313 1.529919981956482 4.198819741606712
CurrentTrain: epoch  7, batch     2 | loss: 9.4302374#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  18.295831149443984 1.5524510145187378 1.5775365829467773 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 18.2958311Losses:  19.329860411584377 1.7774994373321533 1.9880342483520508 5.83479567617178
CurrentTrain: epoch  0, batch     1 | loss: 19.3298604Losses:  17.82791530713439 1.712453842163086 1.717653751373291 4.393430825322866
CurrentTrain: epoch  0, batch     2 | loss: 17.8279153Losses:  18.01407589018345 1.7004165649414062 1.9347906112670898 5.150220528244972
CurrentTrain: epoch  0, batch     3 | loss: 18.0140759Losses:  16.125554379075766 1.7402275800704956 1.7835633754730225 2.500932987779379
CurrentTrain: epoch  0, batch     4 | loss: 16.1255544Losses:  16.479349620640278 1.6612745523452759 1.676901936531067 4.399185664951801
CurrentTrain: epoch  0, batch     5 | loss: 16.4793496Losses:  21.420855797827244 1.7184674739837646 1.7442914247512817 8.278360642492771
CurrentTrain: epoch  0, batch     6 | loss: 21.4208558Losses:  17.415121980011463 1.8553433418273926 1.9073048830032349 3.8523644879460335
CurrentTrain: epoch  0, batch     7 | loss: 17.4151220Losses:  24.254942439496517 1.7878482341766357 1.6029136180877686 10.522154353559017
CurrentTrain: epoch  0, batch     8 | loss: 24.2549424Losses:  15.318220175802708 1.6844685077667236 1.7583390474319458 2.8937578573822975
CurrentTrain: epoch  0, batch     9 | loss: 15.3182202Losses:  16.5000927336514 1.7198998928070068 1.7648414373397827 3.8615848906338215
CurrentTrain: epoch  0, batch    10 | loss: 16.5000927Losses:  16.042084857821465 1.6333351135253906 1.3800948858261108 3.0612374991178513
CurrentTrain: epoch  0, batch    11 | loss: 16.0420849Losses:  13.65581226348877 1.7740113735198975 1.762507677078247 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 13.6558123Losses:  13.430277824401855 1.7550334930419922 1.699263572692871 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 13.4302778Losses:  18.74189656227827 1.6987485885620117 1.5929794311523438 5.328183107078075
CurrentTrain: epoch  0, batch    14 | loss: 18.7418966Losses:  17.14369836449623 1.4759492874145508 1.4891884326934814 4.963307052850723
CurrentTrain: epoch  0, batch    15 | loss: 17.1436984Losses:  16.19427127018571 1.4594173431396484 1.6570197343826294 3.9717828668653965
CurrentTrain: epoch  0, batch    16 | loss: 16.1942713Losses:  16.459271132946014 1.6903033256530762 1.5330939292907715 3.772894561290741
CurrentTrain: epoch  0, batch    17 | loss: 16.4592711Losses:  17.998283490538597 1.6119736433029175 1.3695379495620728 5.051900014281273
CurrentTrain: epoch  0, batch    18 | loss: 17.9982835Losses:  13.982996344566345 1.6295504570007324 1.4092522859573364 1.4333728551864624
CurrentTrain: epoch  0, batch    19 | loss: 13.9829963Losses:  15.940610405057669 1.6465280055999756 1.7016754150390625 4.266829963773489
CurrentTrain: epoch  0, batch    20 | loss: 15.9406104Losses:  16.59988392703235 1.5615501403808594 1.2718346118927002 5.049848450347781
CurrentTrain: epoch  0, batch    21 | loss: 16.5998839Losses:  20.975914299488068 1.6412009000778198 1.6325050592422485 8.69694834947586
CurrentTrain: epoch  0, batch    22 | loss: 20.9759143Losses:  14.228049516677856 1.625218391418457 1.6052501201629639 1.4379818439483643
CurrentTrain: epoch  0, batch    23 | loss: 14.2280495Losses:  18.024254232645035 1.5927014350891113 1.6466702222824097 5.693576246500015
CurrentTrain: epoch  0, batch    24 | loss: 18.0242542Losses:  12.60379409790039 1.7335578203201294 1.5859875679016113 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 12.6037941Losses:  12.096501350402832 1.732974886894226 1.4160926342010498 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 12.0965014Losses:  17.054589487612247 1.5616824626922607 1.4994038343429565 4.797547556459904
CurrentTrain: epoch  0, batch    27 | loss: 17.0545895Losses:  16.390363551676273 1.70139479637146 1.427811622619629 4.619444705545902
CurrentTrain: epoch  0, batch    28 | loss: 16.3903636Losses:  12.18915843963623 1.7167983055114746 1.494136095046997 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 12.1891584Losses:  10.99421501159668 1.6499271392822266 1.4652628898620605 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 10.9942150Losses:  15.835686855018139 1.4102174043655396 1.3574326038360596 5.206738643348217
CurrentTrain: epoch  0, batch    31 | loss: 15.8356869Losses:  11.24498462677002 1.5517385005950928 1.332444190979004 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 11.2449846Losses:  11.816437512636185 1.5825302600860596 1.4061193466186523 1.4026439487934113
CurrentTrain: epoch  0, batch    33 | loss: 11.8164375Losses:  15.296016965061426 1.7143665552139282 1.3828608989715576 3.294786725193262
CurrentTrain: epoch  0, batch    34 | loss: 15.2960170Losses:  15.39129101857543 1.7215101718902588 1.4211487770080566 3.332077380269766
CurrentTrain: epoch  0, batch    35 | loss: 15.3912910Losses:  13.10887160897255 1.4274721145629883 1.38375985622406 1.8146783411502838
CurrentTrain: epoch  0, batch    36 | loss: 13.1088716Losses:  12.86116898059845 1.533203125 1.4806569814682007 1.4248601198196411
CurrentTrain: epoch  0, batch    37 | loss: 12.8611690Losses:  11.141552925109863 1.7806439399719238 1.473483681678772 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 11.1415529Losses:  17.14931089244783 1.7120749950408936 1.193345546722412 5.065311258658767
CurrentTrain: epoch  0, batch    39 | loss: 17.1493109Losses:  18.82152432203293 1.6515350341796875 1.4515761137008667 7.337578475475311
CurrentTrain: epoch  0, batch    40 | loss: 18.8215243Losses:  17.933140575885773 1.5421390533447266 1.2707442045211792 5.800894558429718
CurrentTrain: epoch  0, batch    41 | loss: 17.9331406Losses:  15.553423404693604 1.618023157119751 1.5200161933898926 3.7010064125061035
CurrentTrain: epoch  0, batch    42 | loss: 15.5534234Losses:  17.550185814499855 1.5355591773986816 1.3936097621917725 6.51725734770298
CurrentTrain: epoch  0, batch    43 | loss: 17.5501858Losses:  13.05034089833498 1.6200251579284668 1.2910566329956055 1.7921116426587105
CurrentTrain: epoch  0, batch    44 | loss: 13.0503409Losses:  10.673988342285156 1.5527437925338745 1.3062148094177246 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 10.6739883Losses:  16.416116513311863 1.5782968997955322 1.2555859088897705 5.021334446966648
CurrentTrain: epoch  0, batch    46 | loss: 16.4161165Losses:  16.657158970832825 1.6417276859283447 1.333008050918579 5.0029460191726685
CurrentTrain: epoch  0, batch    47 | loss: 16.6571590Losses:  13.874900238588452 1.5660016536712646 1.536705493927002 2.24065531976521
CurrentTrain: epoch  0, batch    48 | loss: 13.8749002Losses:  13.22936674579978 1.6115608215332031 1.2452976703643799 1.5107969902455807
CurrentTrain: epoch  0, batch    49 | loss: 13.2293667Losses:  20.247485876083374 1.756054401397705 1.4355950355529785 8.522754430770874
CurrentTrain: epoch  0, batch    50 | loss: 20.2474859Losses:  14.649360999464989 1.5320076942443848 1.3310917615890503 4.656272277235985
CurrentTrain: epoch  0, batch    51 | loss: 14.6493610Losses:  12.217631340026855 1.7569512128829956 1.411876916885376 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 12.2176313Losses:  16.3597314869985 1.3929154872894287 1.2600656747817993 5.383874705992639
CurrentTrain: epoch  0, batch    53 | loss: 16.3597315Losses:  17.43934639543295 1.4518592357635498 1.2146692276000977 6.969484411180019
CurrentTrain: epoch  0, batch    54 | loss: 17.4393464Losses:  12.017191175371408 1.6764376163482666 1.353994369506836 1.5433752574026585
CurrentTrain: epoch  0, batch    55 | loss: 12.0171912Losses:  23.195627246052027 1.4516292810440063 1.1958755254745483 12.083624873310328
CurrentTrain: epoch  0, batch    56 | loss: 23.1956272Losses:  13.370115045458078 1.5321203470230103 1.355932593345642 2.9875428713858128
CurrentTrain: epoch  0, batch    57 | loss: 13.3701150Losses:  14.726118110120296 1.6295305490493774 1.50569486618042 3.6979084238409996
CurrentTrain: epoch  0, batch    58 | loss: 14.7261181Losses:  12.642955970019102 1.5205774307250977 1.4143633842468262 1.5359403602778912
CurrentTrain: epoch  0, batch    59 | loss: 12.6429560Losses:  26.29970743507147 1.5915570259094238 1.3878599405288696 15.95385267585516
CurrentTrain: epoch  0, batch    60 | loss: 26.2997074Losses:  14.65648278966546 1.4286924600601196 1.3322588205337524 4.651965234428644
CurrentTrain: epoch  0, batch    61 | loss: 14.6564828Losses:  12.73984619602561 1.4071484804153442 1.5308727025985718 1.5929994247853756
CurrentTrain: epoch  0, batch    62 | loss: 12.7398462Losses:  11.049299240112305 1.6392271518707275 1.2039532661437988 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 11.0492992Losses:  14.513740872498602 1.5230870246887207 1.475461483001709 4.1077407351695
CurrentTrain: epoch  1, batch     1 | loss: 14.5137409Losses:  16.760661989450455 1.610174536705017 1.211686611175537 6.625322252511978
CurrentTrain: epoch  1, batch     2 | loss: 16.7606620Losses:  12.011689141392708 1.575453281402588 1.4145853519439697 1.4615716487169266
CurrentTrain: epoch  1, batch     3 | loss: 12.0116891Losses:  12.949869759380817 1.6718199253082275 1.4629669189453125 1.8150154426693916
CurrentTrain: epoch  1, batch     4 | loss: 12.9498698Losses:  10.918079763650894 1.4276149272918701 1.382519006729126 1.4018453657627106
CurrentTrain: epoch  1, batch     5 | loss: 10.9180798Losses:  10.530649185180664 1.4448142051696777 1.2442078590393066 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 10.5306492Losses:  12.228604067116976 1.4364027976989746 1.1983826160430908 1.7634026892483234
CurrentTrain: epoch  1, batch     7 | loss: 12.2286041Losses:  11.896008852869272 1.6082978248596191 1.3380703926086426 1.5662416256964207
CurrentTrain: epoch  1, batch     8 | loss: 11.8960089Losses:  16.51483140140772 1.613991379737854 1.3310613632202148 6.552713252604008
CurrentTrain: epoch  1, batch     9 | loss: 16.5148314Losses:  16.14615824818611 1.5149470567703247 1.1210243701934814 5.244886428117752
CurrentTrain: epoch  1, batch    10 | loss: 16.1461582Losses:  13.581847190856934 1.4555997848510742 1.4937623739242554 3.2060365676879883
CurrentTrain: epoch  1, batch    11 | loss: 13.5818472Losses:  12.84196999669075 1.359908103942871 1.224454641342163 2.853235751390457
CurrentTrain: epoch  1, batch    12 | loss: 12.8419700Losses:  9.961973190307617 1.5380510091781616 1.3647533655166626 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 9.9619732Losses:  14.576653987169266 1.5465240478515625 1.106323003768921 3.669224292039871
CurrentTrain: epoch  1, batch    14 | loss: 14.5766540Losses:  10.320655822753906 1.493705153465271 1.2526543140411377 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 10.3206558Losses:  14.247427396476269 1.441608190536499 1.3635225296020508 3.1024898812174797
CurrentTrain: epoch  1, batch    16 | loss: 14.2474274Losses:  15.388365648686886 1.4609413146972656 1.1561483144760132 4.412315271794796
CurrentTrain: epoch  1, batch    17 | loss: 15.3883656Losses:  15.966370148584247 1.3538334369659424 1.362977385520935 6.091546578332782
CurrentTrain: epoch  1, batch    18 | loss: 15.9663701Losses:  10.327869415283203 1.4714312553405762 1.3494200706481934 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 10.3278694Losses:  10.019128799438477 1.3092643022537231 1.3857500553131104 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 10.0191288Losses:  20.794529974460602 1.503576636314392 1.1903648376464844 9.90791803598404
CurrentTrain: epoch  1, batch    21 | loss: 20.7945300Losses:  10.464920043945312 1.4271936416625977 1.312514305114746 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 10.4649200Losses:  9.72526741027832 1.4806890487670898 1.296433687210083 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 9.7252674Losses:  13.230500534176826 1.371295690536499 1.3662471771240234 4.39165623486042
CurrentTrain: epoch  1, batch    24 | loss: 13.2305005Losses:  11.92927185073495 1.3867177963256836 1.4721343517303467 2.8857051469385624
CurrentTrain: epoch  1, batch    25 | loss: 11.9292719Losses:  10.567845344543457 1.4648785591125488 1.2354323863983154 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 10.5678453Losses:  9.941400527954102 1.3939491510391235 1.2449355125427246 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 9.9414005Losses:  10.256209373474121 1.4295258522033691 1.2644436359405518 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 10.2562094Losses:  14.20852841064334 1.2791129350662231 1.4250746965408325 4.345324408262968
CurrentTrain: epoch  1, batch    29 | loss: 14.2085284Losses:  14.152454793453217 1.4854323863983154 1.179297924041748 3.6775593161582947
CurrentTrain: epoch  1, batch    30 | loss: 14.1524548Losses:  14.023129977285862 1.5755305290222168 1.3461389541625977 3.2550549879670143
CurrentTrain: epoch  1, batch    31 | loss: 14.0231300Losses:  14.221425898373127 1.4409584999084473 1.2421622276306152 3.9624270275235176
CurrentTrain: epoch  1, batch    32 | loss: 14.2214259Losses:  9.301426887512207 1.403914213180542 1.1787621974945068 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 9.3014269Losses:  9.35541820526123 1.3777296543121338 1.4355638027191162 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 9.3554182Losses:  17.180300682783127 1.643484115600586 1.2944484949111938 6.466891258955002
CurrentTrain: epoch  1, batch    35 | loss: 17.1803007Losses:  12.018892891705036 1.4440977573394775 1.309298038482666 1.6310182884335518
CurrentTrain: epoch  1, batch    36 | loss: 12.0188929Losses:  10.503224201500416 1.338194727897644 1.2082570791244507 1.6161792948842049
CurrentTrain: epoch  1, batch    37 | loss: 10.5032242Losses:  12.182230457663536 1.3808166980743408 1.4338995218276978 2.987596020102501
CurrentTrain: epoch  1, batch    38 | loss: 12.1822305Losses:  13.838148981332779 1.3881101608276367 1.3607263565063477 4.709102541208267
CurrentTrain: epoch  1, batch    39 | loss: 13.8381490Losses:  10.94052591919899 1.3629767894744873 1.311089277267456 1.4750098288059235
CurrentTrain: epoch  1, batch    40 | loss: 10.9405259Losses:  12.248049464076757 1.4184974431991577 1.198445439338684 2.960593905299902
CurrentTrain: epoch  1, batch    41 | loss: 12.2480495Losses:  11.559126902371645 1.4990341663360596 1.2200727462768555 1.8467092998325825
CurrentTrain: epoch  1, batch    42 | loss: 11.5591269Losses:  14.24298670142889 1.4643160104751587 1.3522624969482422 4.48911764472723
CurrentTrain: epoch  1, batch    43 | loss: 14.2429867Losses:  14.64264740049839 1.489929437637329 1.3191232681274414 3.90914024412632
CurrentTrain: epoch  1, batch    44 | loss: 14.6426474Losses:  13.28252412006259 1.606487512588501 1.2518584728240967 2.995995532721281
CurrentTrain: epoch  1, batch    45 | loss: 13.2825241Losses:  10.5880126953125 1.6538662910461426 1.2676317691802979 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 10.5880127Losses:  9.78378677368164 1.5018665790557861 1.1559069156646729 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 9.7837868Losses:  11.442215323448181 1.3752939701080322 1.2162387371063232 1.4088634252548218
CurrentTrain: epoch  1, batch    48 | loss: 11.4422153Losses:  19.570449955761433 1.546952486038208 1.1092569828033447 9.380500920116901
CurrentTrain: epoch  1, batch    49 | loss: 19.5704500Losses:  11.373548839241266 1.3728500604629517 1.1560823917388916 1.4431527592241764
CurrentTrain: epoch  1, batch    50 | loss: 11.3735488Losses:  10.013238906860352 1.4869502782821655 1.3195581436157227 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 10.0132389Losses:  16.01511261984706 1.3838834762573242 1.2114250659942627 6.449584703892469
CurrentTrain: epoch  1, batch    52 | loss: 16.0151126Losses:  9.07345962524414 1.4788804054260254 1.2995843887329102 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 9.0734596Losses:  12.403032772243023 1.2424206733703613 1.258040428161621 3.3191857263445854
CurrentTrain: epoch  1, batch    54 | loss: 12.4030328Losses:  12.000563196837902 1.1900992393493652 1.3696433305740356 2.9100909754633904
CurrentTrain: epoch  1, batch    55 | loss: 12.0005632Losses:  9.718274116516113 1.4638326168060303 1.172204613685608 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 9.7182741Losses:  10.010223388671875 1.368406057357788 1.238810658454895 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 10.0102234Losses:  9.057519912719727 1.3195466995239258 1.2488460540771484 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 9.0575199Losses:  13.477404829114676 1.4892722368240356 1.2951371669769287 3.2362596951425076
CurrentTrain: epoch  1, batch    59 | loss: 13.4774048Losses:  9.363260269165039 1.433820366859436 1.29091215133667 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 9.3632603Losses:  10.872664451599121 1.5178842544555664 1.1364250183105469 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 10.8726645Losses:  12.031925056129694 1.3978745937347412 1.2993440628051758 4.380245063453913
CurrentTrain: epoch  1, batch    62 | loss: 12.0319251Losses:  8.248437881469727 1.3159000873565674 1.3427330255508423 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 8.2484379Losses:  8.963604927062988 1.380641222000122 1.2419925928115845 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 8.9636049Losses:  9.861109733581543 1.4170820713043213 1.2058813571929932 1.4025115966796875
CurrentTrain: epoch  2, batch     2 | loss: 9.8611097Losses:  14.161868926137686 1.35386061668396 1.3435384035110474 3.075908537954092
CurrentTrain: epoch  2, batch     3 | loss: 14.1618689Losses:  11.807638823986053 1.5125631093978882 1.2558249235153198 1.4667794108390808
CurrentTrain: epoch  2, batch     4 | loss: 11.8076388Losses:  11.770410642027855 1.4074785709381104 1.1423165798187256 2.137560948729515
CurrentTrain: epoch  2, batch     5 | loss: 11.7704106Losses:  9.337520599365234 1.4927191734313965 1.2558655738830566 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 9.3375206Losses:  14.252853490412235 1.4050794839859009 1.1849620342254639 4.94812498241663
CurrentTrain: epoch  2, batch     7 | loss: 14.2528535Losses:  12.364547850564122 1.288743019104004 1.179340124130249 3.4187289495021105
CurrentTrain: epoch  2, batch     8 | loss: 12.3645479Losses:  13.521268159151077 1.480844259262085 1.1389514207839966 3.840366631746292
CurrentTrain: epoch  2, batch     9 | loss: 13.5212682Losses:  10.562671966850758 1.3112798929214478 1.2445571422576904 1.4960778430104256
CurrentTrain: epoch  2, batch    10 | loss: 10.5626720Losses:  9.891526345163584 1.2614386081695557 1.1667110919952393 1.5252438820898533
CurrentTrain: epoch  2, batch    11 | loss: 9.8915263Losses:  15.416691180318594 1.3202650547027588 1.375471830368042 5.918378230184317
CurrentTrain: epoch  2, batch    12 | loss: 15.4166912Losses:  10.769387748092413 1.3563441038131714 1.2560607194900513 1.4960274957120419
CurrentTrain: epoch  2, batch    13 | loss: 10.7693877Losses:  12.948951337486506 1.373359203338623 1.3505072593688965 2.881837461143732
CurrentTrain: epoch  2, batch    14 | loss: 12.9489513Losses:  11.32152475323528 1.3484792709350586 1.088395595550537 2.3229991337284446
CurrentTrain: epoch  2, batch    15 | loss: 11.3215248Losses:  12.177801512181759 1.479127049446106 1.0632013082504272 2.851902388036251
CurrentTrain: epoch  2, batch    16 | loss: 12.1778015Losses:  12.07031100615859 1.3768627643585205 1.2250652313232422 2.9046177230775356
CurrentTrain: epoch  2, batch    17 | loss: 12.0703110Losses:  19.639330327510834 1.3795552253723145 1.3240361213684082 11.385263860225677
CurrentTrain: epoch  2, batch    18 | loss: 19.6393303Losses:  11.470880506560206 1.4274988174438477 1.1461968421936035 1.8590431194752455
CurrentTrain: epoch  2, batch    19 | loss: 11.4708805Losses:  8.4832124710083 1.2474292516708374 1.2730227708816528 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 8.4832125Losses:  9.221352577209473 1.4516444206237793 1.179465651512146 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 9.2213526Losses:  13.282384373247623 1.3503576517105103 1.100637674331665 4.367215611040592
CurrentTrain: epoch  2, batch    22 | loss: 13.2823844Losses:  11.130938649177551 1.249220848083496 1.1327804327011108 1.6556435823440552
CurrentTrain: epoch  2, batch    23 | loss: 11.1309386Losses:  9.585058070719242 1.2260769605636597 1.276482343673706 1.5162342563271523
CurrentTrain: epoch  2, batch    24 | loss: 9.5850581Losses:  14.033825978636742 1.4339604377746582 1.328282356262207 4.850653752684593
CurrentTrain: epoch  2, batch    25 | loss: 14.0338260Losses:  10.478882633149624 1.4401484727859497 1.053170919418335 1.4177378043532372
CurrentTrain: epoch  2, batch    26 | loss: 10.4788826Losses:  10.528503328561783 1.3047394752502441 1.1416585445404053 1.7908925116062164
CurrentTrain: epoch  2, batch    27 | loss: 10.5285033Losses:  10.090908885002136 1.3211510181427002 1.1796324253082275 1.4184702634811401
CurrentTrain: epoch  2, batch    28 | loss: 10.0909089Losses:  20.736173417419195 1.3348133563995361 1.2101874351501465 12.052432801574469
CurrentTrain: epoch  2, batch    29 | loss: 20.7361734Losses:  10.457002934068441 1.371234655380249 1.2768301963806152 1.4839671216905117
CurrentTrain: epoch  2, batch    30 | loss: 10.4570029Losses:  9.790660884231329 1.3459789752960205 1.1075400114059448 1.489919688552618
CurrentTrain: epoch  2, batch    31 | loss: 9.7906609Losses:  11.301922965794802 1.3147993087768555 1.1428511142730713 2.900979209691286
CurrentTrain: epoch  2, batch    32 | loss: 11.3019230Losses:  11.954568222165108 1.3607797622680664 1.1650514602661133 2.2522547990083694
CurrentTrain: epoch  2, batch    33 | loss: 11.9545682Losses:  14.395729966461658 1.4047659635543823 1.0537618398666382 5.777185387909412
CurrentTrain: epoch  2, batch    34 | loss: 14.3957300Losses:  15.476850554347038 1.3610541820526123 1.010667085647583 5.892125174403191
CurrentTrain: epoch  2, batch    35 | loss: 15.4768506Losses:  11.654059078544378 1.1858900785446167 1.0954309701919556 2.8884903453290462
CurrentTrain: epoch  2, batch    36 | loss: 11.6540591Losses:  11.18432068824768 1.4233485460281372 1.1120903491973877 1.793916940689087
CurrentTrain: epoch  2, batch    37 | loss: 11.1843207Losses:  10.408796072006226 1.5052913427352905 1.2517911195755005 1.4068858623504639
CurrentTrain: epoch  2, batch    38 | loss: 10.4087961Losses:  12.827568408101797 1.2967641353607178 1.2017768621444702 3.36889111623168
CurrentTrain: epoch  2, batch    39 | loss: 12.8275684Losses:  12.894401714205742 1.2946856021881104 1.2341548204421997 4.487290546298027
CurrentTrain: epoch  2, batch    40 | loss: 12.8944017Losses:  11.62629920989275 1.3906348943710327 1.2116421461105347 1.4907105639576912
CurrentTrain: epoch  2, batch    41 | loss: 11.6262992Losses:  9.84948843717575 1.2442936897277832 1.1040416955947876 1.4010221362113953
CurrentTrain: epoch  2, batch    42 | loss: 9.8494884Losses:  9.225945472717285 1.3008816242218018 1.2494375705718994 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 9.2259455Losses:  9.45927283540368 1.139646291732788 1.1308389902114868 1.471419308334589
CurrentTrain: epoch  2, batch    44 | loss: 9.4592728Losses:  12.317897468805313 1.4350521564483643 1.1950702667236328 3.280381828546524
CurrentTrain: epoch  2, batch    45 | loss: 12.3178975Losses:  8.413504600524902 1.230424165725708 1.1946042776107788 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 8.4135046Losses:  9.0422945022583 1.3373234272003174 1.229056477546692 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 9.0422945Losses:  8.156171798706055 1.2955176830291748 1.1542816162109375 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 8.1561718Losses:  10.96618366241455 1.387939691543579 1.2626750469207764 1.4289617538452148
CurrentTrain: epoch  2, batch    49 | loss: 10.9661837Losses:  18.232363037765026 1.4611889123916626 1.1867135763168335 9.14414244145155
CurrentTrain: epoch  2, batch    50 | loss: 18.2323630Losses:  11.88264144398272 1.3115644454956055 1.3095266819000244 3.5768076274544
CurrentTrain: epoch  2, batch    51 | loss: 11.8826414Losses:  9.91568206436932 1.2908430099487305 1.1236717700958252 1.7310259211808443
CurrentTrain: epoch  2, batch    52 | loss: 9.9156821Losses:  11.207812398672104 1.209261417388916 1.2158392667770386 2.840134710073471
CurrentTrain: epoch  2, batch    53 | loss: 11.2078124Losses:  10.205624490976334 1.3506380319595337 1.0658180713653564 1.4031218588352203
CurrentTrain: epoch  2, batch    54 | loss: 10.2056245Losses:  10.95865622535348 1.374314546585083 1.045365333557129 2.92133704200387
CurrentTrain: epoch  2, batch    55 | loss: 10.9586562Losses:  10.151726542040706 1.2491049766540527 1.2976306676864624 1.785577593371272
CurrentTrain: epoch  2, batch    56 | loss: 10.1517265Losses:  9.033191680908203 1.3753366470336914 1.0122010707855225 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 9.0331917Losses:  8.777139663696289 1.3935587406158447 1.17719304561615 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 8.7771397Losses:  19.712728533893824 1.4823229312896729 1.1556899547576904 11.27516845241189
CurrentTrain: epoch  2, batch    59 | loss: 19.7127285Losses:  8.695704460144043 1.3655016422271729 1.0961650609970093 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 8.6957045Losses:  11.658752985298634 1.3486957550048828 1.2170658111572266 2.021450586616993
CurrentTrain: epoch  2, batch    61 | loss: 11.6587530Losses:  8.384790420532227 1.440119743347168 1.1788763999938965 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 8.3847904Losses:  9.707192089408636 1.3761241436004639 1.24738609790802 1.426028873771429
CurrentTrain: epoch  3, batch     0 | loss: 9.7071921Losses:  9.602515399456024 1.1969842910766602 1.2034268379211426 1.4210731387138367
CurrentTrain: epoch  3, batch     1 | loss: 9.6025154Losses:  8.762608528137207 1.3566107749938965 1.216117024421692 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 8.7626085Losses:  12.87322797998786 1.3594273328781128 1.139875888824463 4.324775602668524
CurrentTrain: epoch  3, batch     3 | loss: 12.8732280Losses:  10.790876515209675 1.2000327110290527 1.1107301712036133 3.2751308754086494
CurrentTrain: epoch  3, batch     4 | loss: 10.7908765Losses:  13.266826201230288 1.394587516784668 1.1591074466705322 4.31613402441144
CurrentTrain: epoch  3, batch     5 | loss: 13.2668262Losses:  9.711530774831772 1.4225742816925049 0.9559953212738037 1.403850644826889
CurrentTrain: epoch  3, batch     6 | loss: 9.7115308Losses:  12.660618215799332 1.306863784790039 1.1311860084533691 4.249205023050308
CurrentTrain: epoch  3, batch     7 | loss: 12.6606182Losses:  9.523831531405449 1.2493667602539062 1.1412774324417114 1.4587814062833786
CurrentTrain: epoch  3, batch     8 | loss: 9.5238315Losses:  8.762823104858398 1.3010956048965454 1.117854356765747 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 8.7628231Losses:  8.396058082580566 1.3589550256729126 1.0969969034194946 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 8.3960581Losses:  11.697976127266884 1.5123863220214844 1.2365038394927979 3.025141730904579
CurrentTrain: epoch  3, batch    11 | loss: 11.6979761Losses:  12.500811785459518 1.2172349691390991 1.4151347875595093 4.270035952329636
CurrentTrain: epoch  3, batch    12 | loss: 12.5008118Losses:  12.550402250140905 1.393394112586975 1.2211239337921143 2.8637596033513546
CurrentTrain: epoch  3, batch    13 | loss: 12.5504023Losses:  19.227716263383627 1.2427103519439697 1.224565029144287 10.369697388261557
CurrentTrain: epoch  3, batch    14 | loss: 19.2277163Losses:  10.073858678340912 1.2700974941253662 1.1004328727722168 1.4133228659629822
CurrentTrain: epoch  3, batch    15 | loss: 10.0738587Losses:  14.881768129765987 1.3837833404541016 1.0978844165802002 6.07643499225378
CurrentTrain: epoch  3, batch    16 | loss: 14.8817681Losses:  9.411154724657536 1.1869162321090698 1.2624876499176025 1.4573726430535316
CurrentTrain: epoch  3, batch    17 | loss: 9.4111547Losses:  12.471613831818104 1.3116185665130615 1.1645550727844238 4.380270905792713
CurrentTrain: epoch  3, batch    18 | loss: 12.4716138Losses:  8.807145979255438 1.1292232275009155 1.1951050758361816 1.4842971824109554
CurrentTrain: epoch  3, batch    19 | loss: 8.8071460Losses:  11.476577259600163 1.137343168258667 1.0545148849487305 4.234217144548893
CurrentTrain: epoch  3, batch    20 | loss: 11.4765773Losses:  11.321674168109894 1.3666000366210938 1.0586905479431152 3.2825058102607727
CurrentTrain: epoch  3, batch    21 | loss: 11.3216742Losses:  15.114991180598736 1.4374871253967285 1.0548009872436523 6.314913742244244
CurrentTrain: epoch  3, batch    22 | loss: 15.1149912Losses:  10.12947815656662 1.3542829751968384 1.1054697036743164 1.4173266291618347
CurrentTrain: epoch  3, batch    23 | loss: 10.1294782Losses:  10.039594165980816 1.2931839227676392 1.0465370416641235 1.4467072412371635
CurrentTrain: epoch  3, batch    24 | loss: 10.0395942Losses:  11.66112145036459 1.252218246459961 1.1197447776794434 2.9347630366683006
CurrentTrain: epoch  3, batch    25 | loss: 11.6611215Losses:  10.65917008370161 1.2583502531051636 1.3944928646087646 2.9117240235209465
CurrentTrain: epoch  3, batch    26 | loss: 10.6591701Losses:  8.902245171368122 1.2818374633789062 1.088578224182129 1.4485770538449287
CurrentTrain: epoch  3, batch    27 | loss: 8.9022452Losses:  13.504013814032078 1.1783826351165771 1.0737403631210327 4.556353367865086
CurrentTrain: epoch  3, batch    28 | loss: 13.5040138Losses:  13.871193401515484 1.3816076517105103 1.2107956409454346 5.81311558932066
CurrentTrain: epoch  3, batch    29 | loss: 13.8711934Losses:  7.950893402099609 1.2572762966156006 1.2001988887786865 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 7.9508934Losses:  7.6573686599731445 1.213247537612915 1.0581204891204834 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 7.6573687Losses:  13.439300406724215 1.2143340110778809 1.0537641048431396 5.747877944260836
CurrentTrain: epoch  3, batch    32 | loss: 13.4393004Losses:  11.04481602832675 1.1209957599639893 1.2154924869537354 2.910356532782316
CurrentTrain: epoch  3, batch    33 | loss: 11.0448160Losses:  8.313801765441895 1.1838111877441406 1.1443984508514404 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 8.3138018Losses:  10.212594583630562 1.1589831113815308 1.194043755531311 1.5157762318849564
CurrentTrain: epoch  3, batch    35 | loss: 10.2125946Losses:  7.826258659362793 1.3738317489624023 1.186697006225586 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 7.8262587Losses:  8.3194580078125 1.3158316612243652 1.4031492471694946 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 8.3194580Losses:  10.720304816961288 1.123572587966919 1.3000727891921997 2.816644996404648
CurrentTrain: epoch  3, batch    38 | loss: 10.7203048Losses:  12.210026923567057 1.1005089282989502 1.2778288125991821 4.258505526930094
CurrentTrain: epoch  3, batch    39 | loss: 12.2100269Losses:  17.689438682049513 1.2690842151641846 1.173919439315796 7.792933326214552
CurrentTrain: epoch  3, batch    40 | loss: 17.6894387Losses:  8.901817321777344 1.398282766342163 1.1586337089538574 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 8.9018173Losses:  8.592923730611801 1.2584954500198364 0.9347336292266846 1.3966737687587738
CurrentTrain: epoch  3, batch    42 | loss: 8.5929237Losses:  9.666702449321747 1.0458780527114868 1.3735065460205078 1.746165931224823
CurrentTrain: epoch  3, batch    43 | loss: 9.6667024Losses:  11.980889216065407 1.4338266849517822 1.2804204225540161 3.1513432413339615
CurrentTrain: epoch  3, batch    44 | loss: 11.9808892Losses:  10.730918109416962 1.2716680765151978 1.071629524230957 2.8269469141960144
CurrentTrain: epoch  3, batch    45 | loss: 10.7309181Losses:  11.965987972915173 1.323923945426941 1.0732042789459229 3.2652853056788445
CurrentTrain: epoch  3, batch    46 | loss: 11.9659880Losses:  10.713915765285492 1.2794623374938965 1.4142062664031982 2.812415063381195
CurrentTrain: epoch  3, batch    47 | loss: 10.7139158Losses:  8.038530349731445 1.3094229698181152 1.1755214929580688 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 8.0385303Losses:  18.84997148811817 1.307958722114563 1.220196008682251 10.195132926106453
CurrentTrain: epoch  3, batch    49 | loss: 18.8499715Losses:  8.68522834777832 1.366438627243042 1.3205231428146362 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 8.6852283Losses:  9.667465060949326 1.333137035369873 1.043774127960205 1.43204864859581
CurrentTrain: epoch  3, batch    51 | loss: 9.6674651Losses:  11.807081054896116 1.0479295253753662 1.240546703338623 4.344013523310423
CurrentTrain: epoch  3, batch    52 | loss: 11.8070811Losses:  13.254806749522686 1.251641035079956 1.2560093402862549 4.340115778148174
CurrentTrain: epoch  3, batch    53 | loss: 13.2548067Losses:  8.837366700172424 1.156829833984375 1.2569458484649658 1.3983951807022095
CurrentTrain: epoch  3, batch    54 | loss: 8.8373667Losses:  11.084117971360683 1.5016623735427856 1.06827974319458 2.8277703151106834
CurrentTrain: epoch  3, batch    55 | loss: 11.0841180Losses:  7.420012474060059 1.1338422298431396 1.0774372816085815 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 7.4200125Losses:  7.919833183288574 1.133967638015747 1.0738565921783447 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.9198332Losses:  12.986586302518845 1.265199899673462 1.045506238937378 5.4061019122600555
CurrentTrain: epoch  3, batch    58 | loss: 12.9865863Losses:  18.322352647781372 1.3207719326019287 0.9876896739006042 9.93282151222229
CurrentTrain: epoch  3, batch    59 | loss: 18.3223526Losses:  8.627762794494629 1.2723519802093506 1.2725403308868408 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 8.6277628Losses:  13.182624097913504 1.4309090375900269 1.149814248085022 4.569402929395437
CurrentTrain: epoch  3, batch    61 | loss: 13.1826241Losses:  9.088214933872223 1.3407313823699951 0.9382069110870361 1.4053784012794495
CurrentTrain: epoch  3, batch    62 | loss: 9.0882149Losses:  12.466073300689459 1.256494402885437 1.1535112857818604 4.347016599029303
CurrentTrain: epoch  4, batch     0 | loss: 12.4660733Losses:  12.836391039192677 1.027735948562622 1.2378612756729126 5.006060667335987
CurrentTrain: epoch  4, batch     1 | loss: 12.8363910Losses:  7.311639308929443 1.1488006114959717 1.201404094696045 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 7.3116393Losses:  11.247634645551443 1.2805519104003906 0.9868289232254028 4.2602722607553005
CurrentTrain: epoch  4, batch     3 | loss: 11.2476346Losses:  16.643495321273804 1.2354698181152344 1.2446788549423218 8.656955003738403
CurrentTrain: epoch  4, batch     4 | loss: 16.6434953Losses:  9.150328993797302 1.1590343713760376 1.2831542491912842 1.4080899953842163
CurrentTrain: epoch  4, batch     5 | loss: 9.1503290Losses:  8.98127530515194 1.0719355344772339 1.1811447143554688 1.5213639587163925
CurrentTrain: epoch  4, batch     6 | loss: 8.9812753Losses:  6.94965934753418 1.150936484336853 1.141607642173767 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 6.9496593Losses:  11.21506866067648 1.2140111923217773 1.1918864250183105 2.8606279715895653
CurrentTrain: epoch  4, batch     8 | loss: 11.2150687Losses:  10.011069100350142 1.2508916854858398 1.1107511520385742 1.544195931404829
CurrentTrain: epoch  4, batch     9 | loss: 10.0110691Losses:  12.136982019990683 1.1766681671142578 1.0546188354492188 4.385425623506308
CurrentTrain: epoch  4, batch    10 | loss: 12.1369820Losses:  11.071575790643692 1.141059398651123 1.387642502784729 2.877774864435196
CurrentTrain: epoch  4, batch    11 | loss: 11.0715758Losses:  9.539664562791586 1.3278448581695557 1.0835047960281372 1.4378703199326992
CurrentTrain: epoch  4, batch    12 | loss: 9.5396646Losses:  7.5504069328308105 1.1651182174682617 1.0786536931991577 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 7.5504069Losses:  9.396012872457504 1.2414019107818604 0.9289093613624573 1.4771228730678558
CurrentTrain: epoch  4, batch    14 | loss: 9.3960129Losses:  9.077255576848984 1.0759658813476562 1.0269020795822144 1.399369090795517
CurrentTrain: epoch  4, batch    15 | loss: 9.0772556Losses:  11.640094416216016 1.2378783226013184 1.1521068811416626 3.2206674981862307
CurrentTrain: epoch  4, batch    16 | loss: 11.6400944Losses:  7.6730637550354 1.2842433452606201 1.191118597984314 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 7.6730638Losses:  12.291311122477055 1.4174286127090454 1.0587401390075684 4.3537028804421425
CurrentTrain: epoch  4, batch    18 | loss: 12.2913111Losses:  16.410596378147602 1.307666301727295 0.8953027129173279 8.720197685062885
CurrentTrain: epoch  4, batch    19 | loss: 16.4105964Losses:  11.33284068480134 1.2720816135406494 1.2425869703292847 2.8502643145620823
CurrentTrain: epoch  4, batch    20 | loss: 11.3328407Losses:  10.739344447851181 1.0124903917312622 1.335747480392456 2.8641060292720795
CurrentTrain: epoch  4, batch    21 | loss: 10.7393444Losses:  9.258771821856499 1.388073205947876 1.1663486957550049 1.4819306582212448
CurrentTrain: epoch  4, batch    22 | loss: 9.2587718Losses:  14.506308801472187 1.1993436813354492 1.0330758094787598 7.122299917042255
CurrentTrain: epoch  4, batch    23 | loss: 14.5063088Losses:  8.826863922178745 1.1561510562896729 1.0377306938171387 1.4900228157639503
CurrentTrain: epoch  4, batch    24 | loss: 8.8268639Losses:  8.472599029541016 1.2720897197723389 1.0597898960113525 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 8.4725990Losses:  8.692092631012201 1.1426478624343872 1.2168058156967163 1.4344141222536564
CurrentTrain: epoch  4, batch    26 | loss: 8.6920926Losses:  10.524921545758843 1.2835862636566162 1.142335295677185 1.9983779285103083
CurrentTrain: epoch  4, batch    27 | loss: 10.5249215Losses:  9.476391106843948 1.4123834371566772 1.072908878326416 1.4148085415363312
CurrentTrain: epoch  4, batch    28 | loss: 9.4763911Losses:  14.044212386012077 1.2503033876419067 1.2761335372924805 6.047119662165642
CurrentTrain: epoch  4, batch    29 | loss: 14.0442124Losses:  13.106929115951061 1.165053367614746 1.3382656574249268 5.720759682357311
CurrentTrain: epoch  4, batch    30 | loss: 13.1069291Losses:  7.689499855041504 1.2816628217697144 1.1379728317260742 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 7.6894999Losses:  9.984750635921955 1.2364940643310547 0.8947098851203918 2.973078139126301
CurrentTrain: epoch  4, batch    32 | loss: 9.9847506Losses:  12.451510401442647 1.1570442914962769 1.1730964183807373 5.088614435866475
CurrentTrain: epoch  4, batch    33 | loss: 12.4515104Losses:  7.364890098571777 1.3214659690856934 1.0809171199798584 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 7.3648901Losses:  9.242823719978333 1.2913579940795898 1.0181808471679688 1.4047833681106567
CurrentTrain: epoch  4, batch    35 | loss: 9.2428237Losses:  12.762580879032612 1.4446871280670166 1.260176420211792 4.668925292789936
CurrentTrain: epoch  4, batch    36 | loss: 12.7625809Losses:  12.127961649559438 1.350313663482666 1.2675015926361084 3.731013788841665
CurrentTrain: epoch  4, batch    37 | loss: 12.1279616Losses:  7.308113098144531 1.2045342922210693 1.1544101238250732 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 7.3081131Losses:  11.94938462972641 1.2066028118133545 1.1589696407318115 4.3625306487083435
CurrentTrain: epoch  4, batch    39 | loss: 11.9493846Losses:  12.409529581665993 1.1515617370605469 1.2365609407424927 4.639408007264137
CurrentTrain: epoch  4, batch    40 | loss: 12.4095296Losses:  7.695475101470947 1.3606228828430176 1.3416876792907715 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 7.6954751Losses:  14.034747827798128 1.2772248983383179 1.182342767715454 6.288018930703402
CurrentTrain: epoch  4, batch    42 | loss: 14.0347478Losses:  10.20473999157548 1.068537950515747 1.2904061079025269 2.891503755003214
CurrentTrain: epoch  4, batch    43 | loss: 10.2047400Losses:  7.948176383972168 1.445139765739441 1.1800906658172607 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 7.9481764Losses:  8.875732325017452 1.284152865409851 0.8860411643981934 1.4980119690299034
CurrentTrain: epoch  4, batch    45 | loss: 8.8757323Losses:  10.973564703017473 1.0230696201324463 1.0923004150390625 2.982832510024309
CurrentTrain: epoch  4, batch    46 | loss: 10.9735647Losses:  9.248172532767057 1.3482108116149902 1.2988135814666748 1.5811565034091473
CurrentTrain: epoch  4, batch    47 | loss: 9.2481725Losses:  13.71056342869997 1.3314919471740723 1.049530267715454 5.815353162586689
CurrentTrain: epoch  4, batch    48 | loss: 13.7105634Losses:  7.858846187591553 1.2968025207519531 1.5008569955825806 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 7.8588462Losses:  10.505680825561285 1.3579496145248413 1.1339813470840454 2.9864018224179745
CurrentTrain: epoch  4, batch    50 | loss: 10.5056808Losses:  6.945909023284912 0.9929178953170776 1.1502442359924316 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 6.9459090Losses:  11.490080326795578 1.209096908569336 1.215823769569397 4.268341034650803
CurrentTrain: epoch  4, batch    52 | loss: 11.4900803Losses:  10.640380471944809 1.113452434539795 1.121492862701416 1.5533472001552582
CurrentTrain: epoch  4, batch    53 | loss: 10.6403805Losses:  8.330134958028793 1.0704889297485352 1.2861840724945068 1.3977218568325043
CurrentTrain: epoch  4, batch    54 | loss: 8.3301350Losses:  7.8529887199401855 1.1385717391967773 1.1264160871505737 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 7.8529887Losses:  18.173223100602627 1.4024832248687744 1.1565219163894653 10.346851907670498
CurrentTrain: epoch  4, batch    56 | loss: 18.1732231Losses:  11.507841292768717 0.9610902667045593 1.3718392848968506 4.2578498758375645
CurrentTrain: epoch  4, batch    57 | loss: 11.5078413Losses:  11.315490670502186 1.4600775241851807 1.1049541234970093 3.198717065155506
CurrentTrain: epoch  4, batch    58 | loss: 11.3154907Losses:  10.417530428618193 1.1669788360595703 0.9907229542732239 3.0401266925036907
CurrentTrain: epoch  4, batch    59 | loss: 10.4175304Losses:  8.841915309429169 1.1382358074188232 1.151522159576416 1.4102217555046082
CurrentTrain: epoch  4, batch    60 | loss: 8.8419153Losses:  9.331208817660809 1.4641553163528442 1.065773606300354 1.4575587436556816
CurrentTrain: epoch  4, batch    61 | loss: 9.3312088Losses:  13.028187345713377 1.2186647653579712 1.1288046836853027 6.087683271616697
CurrentTrain: epoch  4, batch    62 | loss: 13.0281873Losses:  7.716534614562988 1.382725715637207 0.9862014055252075 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.7165346Losses:  10.315938115119934 1.1785640716552734 1.1610896587371826 2.9728699922561646
CurrentTrain: epoch  5, batch     1 | loss: 10.3159381Losses:  10.421621978282928 1.3075400590896606 1.167144536972046 2.816632926464081
CurrentTrain: epoch  5, batch     2 | loss: 10.4216220Losses:  10.653291370719671 1.2750473022460938 1.120694875717163 2.923579838126898
CurrentTrain: epoch  5, batch     3 | loss: 10.6532914Losses:  11.44568856060505 1.1074559688568115 1.0697097778320312 4.3322509080171585
CurrentTrain: epoch  5, batch     4 | loss: 11.4456886Losses:  12.65612444281578 0.9838567972183228 1.1145778894424438 5.8283370435237885
CurrentTrain: epoch  5, batch     5 | loss: 12.6561244Losses:  7.816025733947754 1.2942360639572144 1.104749321937561 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 7.8160257Losses:  8.879625707864761 1.3234070539474487 1.0177044868469238 1.421370416879654
CurrentTrain: epoch  5, batch     7 | loss: 8.8796257Losses:  12.04552637040615 1.2222694158554077 0.9772032499313354 4.393525466322899
CurrentTrain: epoch  5, batch     8 | loss: 12.0455264Losses:  13.378222603350878 1.0960928201675415 1.0256640911102295 6.0597382970154285
CurrentTrain: epoch  5, batch     9 | loss: 13.3782226Losses:  11.09595650434494 1.4612854719161987 1.1463351249694824 2.8042046427726746
CurrentTrain: epoch  5, batch    10 | loss: 11.0959565Losses:  32.01665258407593 1.1078325510025024 1.271003246307373 24.537275314331055
CurrentTrain: epoch  5, batch    11 | loss: 32.0166526Losses:  10.392215363681316 1.256986141204834 1.1671953201293945 2.8261848613619804
CurrentTrain: epoch  5, batch    12 | loss: 10.3922154Losses:  8.814473748207092 1.0900623798370361 1.3888195753097534 1.3993202447891235
CurrentTrain: epoch  5, batch    13 | loss: 8.8144737Losses:  9.205811776220798 1.315996766090393 1.0547798871994019 1.7145393267273903
CurrentTrain: epoch  5, batch    14 | loss: 9.2058118Losses:  9.081096228212118 1.2664796113967896 1.0354127883911133 1.4645982347428799
CurrentTrain: epoch  5, batch    15 | loss: 9.0810962Losses:  8.890800662338734 1.0587348937988281 1.3346095085144043 1.4617964699864388
CurrentTrain: epoch  5, batch    16 | loss: 8.8908007Losses:  10.816089563071728 1.1945407390594482 1.3446955680847168 2.8498749062418938
CurrentTrain: epoch  5, batch    17 | loss: 10.8160896Losses:  11.619137842208147 1.301642894744873 1.029965877532959 4.532311040908098
CurrentTrain: epoch  5, batch    18 | loss: 11.6191378Losses:  8.576051503419876 1.236005187034607 1.1178276538848877 1.408211499452591
CurrentTrain: epoch  5, batch    19 | loss: 8.5760515Losses:  9.45139530301094 1.1577576398849487 0.9765190482139587 2.806498795747757
CurrentTrain: epoch  5, batch    20 | loss: 9.4513953Losses:  12.465808779001236 1.078174352645874 1.1761388778686523 5.748297601938248
CurrentTrain: epoch  5, batch    21 | loss: 12.4658088Losses:  9.883798565715551 1.0368962287902832 1.2935686111450195 3.019422974437475
CurrentTrain: epoch  5, batch    22 | loss: 9.8837986Losses:  11.254004925489426 1.381592035293579 1.1131703853607178 3.4427275359630585
CurrentTrain: epoch  5, batch    23 | loss: 11.2540049Losses:  9.593518763780594 1.0411133766174316 1.087808609008789 2.904864341020584
CurrentTrain: epoch  5, batch    24 | loss: 9.5935188Losses:  22.788702372461557 1.0117981433868408 1.2402677536010742 14.70581853762269
CurrentTrain: epoch  5, batch    25 | loss: 22.7887024Losses:  10.346926346421242 1.2498931884765625 1.2350165843963623 3.039283409714699
CurrentTrain: epoch  5, batch    26 | loss: 10.3469263Losses:  8.942206770181656 1.1823725700378418 1.0411118268966675 1.3991545736789703
CurrentTrain: epoch  5, batch    27 | loss: 8.9422068Losses:  8.411209378391504 1.229881763458252 1.2301874160766602 1.4743273593485355
CurrentTrain: epoch  5, batch    28 | loss: 8.4112094Losses:  12.128967106342316 1.1505210399627686 1.2356255054473877 4.379270374774933
CurrentTrain: epoch  5, batch    29 | loss: 12.1289671Losses:  9.512535277754068 1.2233070135116577 1.0056798458099365 1.5253745950758457
CurrentTrain: epoch  5, batch    30 | loss: 9.5125353Losses:  11.833833120763302 1.41151762008667 1.0114421844482422 4.4264344200491905
CurrentTrain: epoch  5, batch    31 | loss: 11.8338331Losses:  8.768368482589722 1.3581156730651855 1.0917648077011108 1.424842119216919
CurrentTrain: epoch  5, batch    32 | loss: 8.7683685Losses:  22.7272804453969 1.0893592834472656 1.14873206615448 14.608892269432545
CurrentTrain: epoch  5, batch    33 | loss: 22.7272804Losses:  10.700157463550568 1.1755188703536987 1.2613061666488647 2.9027894139289856
CurrentTrain: epoch  5, batch    34 | loss: 10.7001575Losses:  11.490373995155096 1.1560180187225342 1.2361533641815186 4.2864660285413265
CurrentTrain: epoch  5, batch    35 | loss: 11.4903740Losses:  14.57043056562543 1.3925695419311523 1.0762083530426025 6.8380659110844135
CurrentTrain: epoch  5, batch    36 | loss: 14.5704306Losses:  8.635264985263348 1.0956474542617798 1.3623758554458618 1.4293209239840508
CurrentTrain: epoch  5, batch    37 | loss: 8.6352650Losses:  8.911199476569891 1.2177481651306152 1.2258987426757812 1.4688924811780453
CurrentTrain: epoch  5, batch    38 | loss: 8.9111995Losses:  8.370145916938782 1.064293384552002 1.0988638401031494 1.4023514986038208
CurrentTrain: epoch  5, batch    39 | loss: 8.3701459Losses:  8.608293175697327 0.9429925084114075 1.4561307430267334 1.3912006616592407
CurrentTrain: epoch  5, batch    40 | loss: 8.6082932Losses:  8.030030250549316 1.3211989402770996 1.3304975032806396 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 8.0300303Losses:  9.114244803786278 1.0752700567245483 1.0868146419525146 1.4877141565084457
CurrentTrain: epoch  5, batch    42 | loss: 9.1142448Losses:  11.876178681850433 1.1302002668380737 1.2151515483856201 4.294754922389984
CurrentTrain: epoch  5, batch    43 | loss: 11.8761787Losses:  7.475511074066162 0.9869983792304993 1.2449603080749512 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 7.4755111Losses:  6.950076103210449 1.303077220916748 1.0580155849456787 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 6.9500761Losses:  10.179599080234766 1.3031331300735474 1.299662470817566 2.9665119983255863
CurrentTrain: epoch  5, batch    46 | loss: 10.1795991Losses:  9.891177952289581 1.3137757778167725 1.125255823135376 1.4044016003608704
CurrentTrain: epoch  5, batch    47 | loss: 9.8911780Losses:  10.904023107141256 1.2068936824798584 1.2869081497192383 2.871537145227194
CurrentTrain: epoch  5, batch    48 | loss: 10.9040231Losses:  9.237372517585754 1.241389274597168 1.1981256008148193 1.4053813219070435
CurrentTrain: epoch  5, batch    49 | loss: 9.2373725Losses:  8.92283883690834 1.0838723182678223 1.2531156539916992 1.4042045176029205
CurrentTrain: epoch  5, batch    50 | loss: 8.9228388Losses:  17.050091199576855 1.0687644481658936 1.2010884284973145 10.090029649436474
CurrentTrain: epoch  5, batch    51 | loss: 17.0500912Losses:  11.427775226533413 1.2091786861419678 1.2860867977142334 4.319565616548061
CurrentTrain: epoch  5, batch    52 | loss: 11.4277752Losses:  15.736440107226372 1.1540250778198242 1.2192596197128296 8.511190339922905
CurrentTrain: epoch  5, batch    53 | loss: 15.7364401Losses:  8.72744670510292 1.1200568675994873 1.056087613105774 1.5444280207157135
CurrentTrain: epoch  5, batch    54 | loss: 8.7274467Losses:  9.93784049153328 1.0518349409103394 1.381466269493103 2.865050822496414
CurrentTrain: epoch  5, batch    55 | loss: 9.9378405Losses:  10.36797821521759 1.182218313217163 1.014642596244812 2.8298598527908325
CurrentTrain: epoch  5, batch    56 | loss: 10.3679782Losses:  8.262915827333927 1.1987335681915283 1.1392731666564941 1.475007750093937
CurrentTrain: epoch  5, batch    57 | loss: 8.2629158Losses:  12.475379392504692 1.3951606750488281 1.4169776439666748 4.525291845202446
CurrentTrain: epoch  5, batch    58 | loss: 12.4753794Losses:  12.706670813262463 1.1304552555084229 1.039768934249878 4.895017676055431
CurrentTrain: epoch  5, batch    59 | loss: 12.7066708Losses:  8.588184598833323 1.1013003587722778 0.9400686025619507 1.5643947161734104
CurrentTrain: epoch  5, batch    60 | loss: 8.5881846Losses:  8.76011598110199 1.4039491415023804 1.054991602897644 1.4345802068710327
CurrentTrain: epoch  5, batch    61 | loss: 8.7601160Losses:  11.903816767036915 1.5172700881958008 1.0928356647491455 4.30234581977129
CurrentTrain: epoch  5, batch    62 | loss: 11.9038168Losses:  13.784499168395996 1.2734415531158447 1.0481480360031128 6.607548713684082
CurrentTrain: epoch  6, batch     0 | loss: 13.7844992Losses:  9.858063012361526 1.2442336082458496 0.9419947862625122 2.8420574963092804
CurrentTrain: epoch  6, batch     1 | loss: 9.8580630Losses:  8.89052352681756 1.2077677249908447 1.2554340362548828 1.4544263817369938
CurrentTrain: epoch  6, batch     2 | loss: 8.8905235Losses:  8.95423349738121 1.3210391998291016 1.1399234533309937 1.4539812505245209
CurrentTrain: epoch  6, batch     3 | loss: 8.9542335Losses:  8.261778768151999 1.229644775390625 1.0751479864120483 1.4420437179505825
CurrentTrain: epoch  6, batch     4 | loss: 8.2617788Losses:  9.100394576787949 1.3591833114624023 1.0194542407989502 1.550541251897812
CurrentTrain: epoch  6, batch     5 | loss: 9.1003946Losses:  7.072932243347168 1.101845622062683 1.1671440601348877 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 7.0729322Losses:  8.384110145270824 1.1948351860046387 1.1174404621124268 1.4566036984324455
CurrentTrain: epoch  6, batch     7 | loss: 8.3841101Losses:  13.535614758729935 1.0411499738693237 1.277828574180603 6.19368389248848
CurrentTrain: epoch  6, batch     8 | loss: 13.5356148Losses:  9.973154533654451 1.2951724529266357 1.120682954788208 3.0560479052364826
CurrentTrain: epoch  6, batch     9 | loss: 9.9731545Losses:  11.545426204800606 1.1368547677993774 1.1936395168304443 3.289398029446602
CurrentTrain: epoch  6, batch    10 | loss: 11.5454262Losses:  10.873715449124575 1.0783436298370361 1.374455213546753 3.052105952054262
CurrentTrain: epoch  6, batch    11 | loss: 10.8737154Losses:  8.995585501194 1.3197567462921143 1.027712106704712 1.453698217868805
CurrentTrain: epoch  6, batch    12 | loss: 8.9955855Losses:  8.62837764620781 1.0425033569335938 1.3496978282928467 1.4020077884197235
CurrentTrain: epoch  6, batch    13 | loss: 8.6283776Losses:  8.55852036178112 1.184368371963501 1.3348402976989746 1.461556002497673
CurrentTrain: epoch  6, batch    14 | loss: 8.5585204Losses:  10.48990573734045 1.1664166450500488 1.1486165523529053 2.9919675812125206
CurrentTrain: epoch  6, batch    15 | loss: 10.4899057Losses:  6.790223121643066 1.0982027053833008 1.1728403568267822 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 6.7902231Losses:  9.92828106880188 1.0618226528167725 1.216930866241455 2.9131534099578857
CurrentTrain: epoch  6, batch    17 | loss: 9.9282811Losses:  8.392477061599493 1.141492247581482 1.0877015590667725 1.5630026124417782
CurrentTrain: epoch  6, batch    18 | loss: 8.3924771Losses:  7.021526336669922 1.1326836347579956 1.1216810941696167 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 7.0215263Losses:  12.048459138721228 1.122826099395752 1.2501106262207031 4.653101529926062
CurrentTrain: epoch  6, batch    20 | loss: 12.0484591Losses:  8.545511156320572 1.2227990627288818 1.0895966291427612 1.4222773611545563
CurrentTrain: epoch  6, batch    21 | loss: 8.5455112Losses:  9.616118788719177 1.2276277542114258 1.0362849235534668 2.8419750928878784
CurrentTrain: epoch  6, batch    22 | loss: 9.6161188Losses:  8.591166615486145 1.1599252223968506 1.1276952028274536 1.421479344367981
CurrentTrain: epoch  6, batch    23 | loss: 8.5911666Losses:  11.48621079325676 1.3230150938034058 1.0055387020111084 4.300150841474533
CurrentTrain: epoch  6, batch    24 | loss: 11.4862108Losses:  10.153135783970356 1.1408607959747314 1.2781093120574951 2.9081111028790474
CurrentTrain: epoch  6, batch    25 | loss: 10.1531358Losses:  8.83492624759674 1.2870979309082031 1.2112650871276855 1.4713784456253052
CurrentTrain: epoch  6, batch    26 | loss: 8.8349262Losses:  8.265984326601028 1.2192403078079224 0.9547640085220337 1.4474914371967316
CurrentTrain: epoch  6, batch    27 | loss: 8.2659843Losses:  12.685190822929144 1.2666527032852173 0.9992952942848206 5.835766937583685
CurrentTrain: epoch  6, batch    28 | loss: 12.6851908Losses:  13.02205627784133 1.173112154006958 1.0979924201965332 5.796938117593527
CurrentTrain: epoch  6, batch    29 | loss: 13.0220563Losses:  11.043957144021988 1.1084367036819458 1.1709258556365967 4.2140001356601715
CurrentTrain: epoch  6, batch    30 | loss: 11.0439571Losses:  8.457605957984924 1.314768671989441 0.9320900440216064 1.4220463037490845
CurrentTrain: epoch  6, batch    31 | loss: 8.4576060Losses:  9.931264527142048 1.0357290506362915 1.205517053604126 2.954910881817341
CurrentTrain: epoch  6, batch    32 | loss: 9.9312645Losses:  8.166023671627045 1.2370786666870117 1.0204172134399414 1.4077271819114685
CurrentTrain: epoch  6, batch    33 | loss: 8.1660237Losses:  7.300436973571777 1.0031871795654297 1.1708695888519287 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 7.3004370Losses:  11.34865664318204 1.2076592445373535 1.101920485496521 3.31697653606534
CurrentTrain: epoch  6, batch    35 | loss: 11.3486566Losses:  10.114271599799395 1.121942400932312 1.1526250839233398 2.9197244234383106
CurrentTrain: epoch  6, batch    36 | loss: 10.1142716Losses:  7.451140403747559 1.3347512483596802 1.161926507949829 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.4511404Losses:  14.469081349670887 0.9038694500923157 1.1354591846466064 7.559098668396473
CurrentTrain: epoch  6, batch    38 | loss: 14.4690813Losses:  7.00905179977417 1.208020806312561 0.9390677213668823 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 7.0090518Losses:  7.335043430328369 1.3055859804153442 1.140768051147461 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 7.3350434Losses:  8.349154204130173 1.2133045196533203 1.1756010055541992 1.3992931544780731
CurrentTrain: epoch  6, batch    41 | loss: 8.3491542Losses:  10.965431936085224 1.3894641399383545 1.2408398389816284 2.8419511392712593
CurrentTrain: epoch  6, batch    42 | loss: 10.9654319Losses:  10.309464164078236 1.1702618598937988 1.2606451511383057 3.0201317742466927
CurrentTrain: epoch  6, batch    43 | loss: 10.3094642Losses:  8.721927586942911 1.2838793992996216 1.4281868934631348 1.4674839414656162
CurrentTrain: epoch  6, batch    44 | loss: 8.7219276Losses:  8.545232146978378 1.1670730113983154 1.276362419128418 1.4023593366146088
CurrentTrain: epoch  6, batch    45 | loss: 8.5452321Losses:  10.462303146719933 1.1190110445022583 1.080639362335205 2.2601165622472763
CurrentTrain: epoch  6, batch    46 | loss: 10.4623031Losses:  9.838058024644852 1.1095402240753174 1.1413123607635498 2.865629702806473
CurrentTrain: epoch  6, batch    47 | loss: 9.8380580Losses:  8.39432354643941 1.186450719833374 1.1878541707992554 1.4850184507668018
CurrentTrain: epoch  6, batch    48 | loss: 8.3943235Losses:  8.619602233171463 1.0480852127075195 1.1808745861053467 1.4367904961109161
CurrentTrain: epoch  6, batch    49 | loss: 8.6196022Losses:  8.398709028959274 1.0623507499694824 1.1895079612731934 1.405436247587204
CurrentTrain: epoch  6, batch    50 | loss: 8.3987090Losses:  7.020636558532715 1.1511693000793457 1.151602029800415 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 7.0206366Losses:  7.210477828979492 0.9982675313949585 1.23923921585083 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 7.2104778Losses:  11.223869938403368 1.202115535736084 1.2059239149093628 4.37578596547246
CurrentTrain: epoch  6, batch    53 | loss: 11.2238699Losses:  6.737049102783203 1.2163403034210205 1.056370496749878 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 6.7370491Losses:  11.197764843702316 1.2501088380813599 1.1961535215377808 4.2895388305187225
CurrentTrain: epoch  6, batch    55 | loss: 11.1977648Losses:  8.432723991572857 1.192196011543274 1.1339540481567383 1.4592842981219292
CurrentTrain: epoch  6, batch    56 | loss: 8.4327240Losses:  9.873935617506504 1.3044202327728271 1.0631687641143799 2.9101723805069923
CurrentTrain: epoch  6, batch    57 | loss: 9.8739356Losses:  8.447145741432905 1.1767152547836304 1.2549076080322266 1.4455078057944775
CurrentTrain: epoch  6, batch    58 | loss: 8.4471457Losses:  8.757077604532242 1.1109700202941895 1.2113066911697388 1.432704359292984
CurrentTrain: epoch  6, batch    59 | loss: 8.7570776Losses:  6.632674217224121 1.1146748065948486 1.2006566524505615 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 6.6326742Losses:  7.8896054327487946 0.8850838541984558 1.2651870250701904 1.4056004583835602
CurrentTrain: epoch  6, batch    61 | loss: 7.8896054Losses:  11.129547093063593 1.1603801250457764 1.1422808170318604 4.2360782362520695
CurrentTrain: epoch  6, batch    62 | loss: 11.1295471Losses:  8.405671626329422 1.08619225025177 1.2218291759490967 1.4449000656604767
CurrentTrain: epoch  7, batch     0 | loss: 8.4056716Losses:  9.751162201166153 1.1625534296035767 1.1014050245285034 2.820911556482315
CurrentTrain: epoch  7, batch     1 | loss: 9.7511622Losses:  6.747706890106201 0.9795339107513428 1.118425965309143 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 6.7477069Losses:  14.74875956773758 1.3043721914291382 1.2747013568878174 7.643108665943146
CurrentTrain: epoch  7, batch     3 | loss: 14.7487596Losses:  14.059647798538208 1.0899497270584106 1.118966817855835 7.550678491592407
CurrentTrain: epoch  7, batch     4 | loss: 14.0596478Losses:  9.370358675718307 0.9656195640563965 1.0822765827178955 2.8030559718608856
CurrentTrain: epoch  7, batch     5 | loss: 9.3703587Losses:  6.88259220123291 1.1756339073181152 1.0248278379440308 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 6.8825922Losses:  12.438578840345144 1.1066118478775024 1.1933530569076538 5.645282980054617
CurrentTrain: epoch  7, batch     7 | loss: 12.4385788Losses:  8.255664676427841 1.1892683506011963 0.9827371835708618 1.425250381231308
CurrentTrain: epoch  7, batch     8 | loss: 8.2556647Losses:  8.280008167028427 1.375168800354004 1.0028268098831177 1.4435871541500092
CurrentTrain: epoch  7, batch     9 | loss: 8.2800082Losses:  11.57458931952715 1.1563775539398193 1.2237411737442017 4.470781393349171
CurrentTrain: epoch  7, batch    10 | loss: 11.5745893Losses:  6.83447265625 1.1177390813827515 1.1541540622711182 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 6.8344727Losses:  10.593781977891922 1.2650872468948364 1.10403573513031 2.8151645958423615
CurrentTrain: epoch  7, batch    12 | loss: 10.5937820Losses:  8.086971372365952 1.1401573419570923 1.2771646976470947 1.3980055749416351
CurrentTrain: epoch  7, batch    13 | loss: 8.0869714Losses:  10.49072289839387 1.3902523517608643 1.1896076202392578 3.517549756914377
CurrentTrain: epoch  7, batch    14 | loss: 10.4907229Losses:  10.027807146310806 1.1863772869110107 1.194960117340088 2.808469206094742
CurrentTrain: epoch  7, batch    15 | loss: 10.0278071Losses:  14.146595306694508 1.1300569772720337 1.2703149318695068 7.260906524956226
CurrentTrain: epoch  7, batch    16 | loss: 14.1465953Losses:  8.18803259730339 1.1062023639678955 1.3174158334732056 1.3969506919384003
CurrentTrain: epoch  7, batch    17 | loss: 8.1880326Losses:  6.826102256774902 1.1087381839752197 1.0755882263183594 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 6.8261023Losses:  12.314980864524841 1.1369092464447021 1.0579396486282349 5.835646033287048
CurrentTrain: epoch  7, batch    19 | loss: 12.3149809Losses:  8.373292118310928 1.1637905836105347 1.2959699630737305 1.4065629541873932
CurrentTrain: epoch  7, batch    20 | loss: 8.3732921Losses:  10.932219948619604 1.1636707782745361 1.0755921602249146 4.322518792003393
CurrentTrain: epoch  7, batch    21 | loss: 10.9322199Losses:  9.472220629453659 1.1354050636291504 1.2016409635543823 2.801061362028122
CurrentTrain: epoch  7, batch    22 | loss: 9.4722206Losses:  6.916321754455566 1.0557193756103516 1.276503562927246 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 6.9163218Losses:  7.045431137084961 1.2773182392120361 1.2888667583465576 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 7.0454311Losses:  8.639047797769308 1.0605909824371338 1.225369930267334 1.4421431384980679
CurrentTrain: epoch  7, batch    25 | loss: 8.6390478Losses:  9.739584151655436 1.1722114086151123 1.096605658531189 2.8604237474501133
CurrentTrain: epoch  7, batch    26 | loss: 9.7395842Losses:  8.109367221593857 1.2358306646347046 1.079089641571045 1.4235332906246185
CurrentTrain: epoch  7, batch    27 | loss: 8.1093672Losses:  9.227438151836395 1.0770525932312012 1.1042859554290771 2.7922279238700867
CurrentTrain: epoch  7, batch    28 | loss: 9.2274382Losses:  8.079389214515686 1.2586631774902344 1.090864896774292 1.3969780206680298
CurrentTrain: epoch  7, batch    29 | loss: 8.0793892Losses:  6.3202805519104 1.1422806978225708 0.8982561230659485 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 6.3202806Losses:  13.719614826142788 1.0829846858978271 1.3382651805877686 7.076900325715542
CurrentTrain: epoch  7, batch    31 | loss: 13.7196148Losses:  9.31017941236496 0.9877218008041382 1.2067687511444092 2.841322600841522
CurrentTrain: epoch  7, batch    32 | loss: 9.3101794Losses:  14.422781333327293 1.3713129758834839 1.1025906801223755 7.636583670973778
CurrentTrain: epoch  7, batch    33 | loss: 14.4227813Losses:  10.64959192276001 0.8102512359619141 1.3329834938049316 4.208276748657227
CurrentTrain: epoch  7, batch    34 | loss: 10.6495919Losses:  6.453606605529785 0.904304027557373 1.228081226348877 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 6.4536066Losses:  11.46691009402275 1.1878764629364014 1.0238181352615356 4.321876257658005
CurrentTrain: epoch  7, batch    36 | loss: 11.4669101Losses:  10.044186763465405 1.3212480545043945 1.2591603994369507 2.887712173163891
CurrentTrain: epoch  7, batch    37 | loss: 10.0441868Losses:  9.278942078351974 0.980967104434967 1.1862492561340332 2.830327957868576
CurrentTrain: epoch  7, batch    38 | loss: 9.2789421Losses:  6.733416557312012 1.0752108097076416 1.1015875339508057 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 6.7334166Losses:  9.288351893424988 0.9636305570602417 1.1718828678131104 2.834868311882019
CurrentTrain: epoch  7, batch    40 | loss: 9.2883519Losses:  9.568729043006897 1.1869038343429565 1.1318509578704834 2.8256155252456665
CurrentTrain: epoch  7, batch    41 | loss: 9.5687290Losses:  6.507890224456787 1.1280062198638916 1.0972734689712524 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 6.5078902Losses:  8.201044768095016 1.104766845703125 1.106260061264038 1.4132620990276337
CurrentTrain: epoch  7, batch    43 | loss: 8.2010448Losses:  11.049693025648594 1.1674699783325195 1.021141529083252 4.423596777021885
CurrentTrain: epoch  7, batch    44 | loss: 11.0496930Losses:  9.278951399028301 1.1294918060302734 1.0081554651260376 2.8328835889697075
CurrentTrain: epoch  7, batch    45 | loss: 9.2789514Losses:  12.30619391053915 1.0387444496154785 1.1402745246887207 5.645398698747158
CurrentTrain: epoch  7, batch    46 | loss: 12.3061939Losses:  8.04596084356308 1.2025281190872192 1.1559730768203735 1.4005932211875916
CurrentTrain: epoch  7, batch    47 | loss: 8.0459608Losses:  8.033766690641642 1.1817985773086548 0.996015727519989 1.4350065626204014
CurrentTrain: epoch  7, batch    48 | loss: 8.0337667Losses:  8.633954048156738 1.3311436176300049 1.2328906059265137 1.3986353874206543
CurrentTrain: epoch  7, batch    49 | loss: 8.6339540Losses:  7.490238189697266 1.1675217151641846 1.173152208328247 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 7.4902382Losses:  8.306295726448298 1.2732927799224854 1.0503482818603516 1.426132533699274
CurrentTrain: epoch  7, batch    51 | loss: 8.3062957Losses:  10.77751249819994 1.203857421875 1.0673350095748901 4.266693063080311
CurrentTrain: epoch  7, batch    52 | loss: 10.7775125Losses:  7.825576573610306 0.9199988842010498 1.2898576259613037 1.4270437061786652
CurrentTrain: epoch  7, batch    53 | loss: 7.8255766Losses:  10.7885802090168 1.0528209209442139 1.205441951751709 4.252642422914505
CurrentTrain: epoch  7, batch    54 | loss: 10.7885802Losses:  6.958982467651367 1.284245491027832 1.2611827850341797 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 6.9589825Losses:  6.724095344543457 1.3823432922363281 1.1287776231765747 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 6.7240953Losses:  7.096007823944092 1.2065930366516113 1.0324941873550415 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 7.0960078Losses:  12.39200995489955 1.1345958709716797 1.1101417541503906 5.874160032719374
CurrentTrain: epoch  7, batch    58 | loss: 12.3920100Losses:  6.475886821746826 0.9713627099990845 1.2104054689407349 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 6.4758868Losses:  8.168191522359848 1.2719643115997314 1.20475172996521 1.4247022569179535
CurrentTrain: epoch  7, batch    60 | loss: 8.1681915Losses:  8.092400670051575 1.178283929824829 1.2066504955291748 1.4161816835403442
CurrentTrain: epoch  7, batch    61 | loss: 8.0924007Losses:  6.190687656402588 0.8343174457550049 1.137901782989502 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 6.1906877Losses:  8.282214406877756 1.17989182472229 1.1953085660934448 1.43960976973176
CurrentTrain: epoch  8, batch     0 | loss: 8.2822144Losses:  9.69681933708489 0.992557942867279 1.2836090326309204 3.0825682003051043
CurrentTrain: epoch  8, batch     1 | loss: 9.6968193Losses:  9.358225677162409 1.0815844535827637 1.1354496479034424 2.8987472988665104
CurrentTrain: epoch  8, batch     2 | loss: 9.3582257Losses:  7.858971029520035 1.2623639106750488 0.908017635345459 1.4250167906284332
CurrentTrain: epoch  8, batch     3 | loss: 7.8589710Losses:  8.118241250514984 1.1521430015563965 1.0850565433502197 1.4097327589988708
CurrentTrain: epoch  8, batch     4 | loss: 8.1182413Losses:  6.66613245010376 1.2246794700622559 1.193136215209961 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 6.6661325Losses:  6.492161273956299 1.0672335624694824 1.146317958831787 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 6.4921613Losses:  9.44642859697342 0.9426686763763428 1.4078186750411987 2.8233073353767395
CurrentTrain: epoch  8, batch     7 | loss: 9.4464286Losses:  9.445984330028296 1.1446939706802368 1.083795189857483 2.8391418121755123
CurrentTrain: epoch  8, batch     8 | loss: 9.4459843Losses:  10.43924119323492 0.8929729461669922 1.2119457721710205 4.208003781735897
CurrentTrain: epoch  8, batch     9 | loss: 10.4392412Losses:  6.693425178527832 1.3876683712005615 1.032327651977539 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 6.6934252Losses:  7.769187334924936 1.1546552181243896 0.9783260822296143 1.4465068615972996
CurrentTrain: epoch  8, batch    11 | loss: 7.7691873Losses:  8.199463814496994 1.369921326637268 0.9451318979263306 1.3974909484386444
CurrentTrain: epoch  8, batch    12 | loss: 8.1994638Losses:  7.9692517295479774 1.2677587270736694 0.9853317141532898 1.489216424524784
CurrentTrain: epoch  8, batch    13 | loss: 7.9692517Losses:  7.888007491827011 1.0775882005691528 1.163724660873413 1.3999942243099213
CurrentTrain: epoch  8, batch    14 | loss: 7.8880075Losses:  8.332607805728912 1.1523876190185547 1.4038530588150024 1.4033861756324768
CurrentTrain: epoch  8, batch    15 | loss: 8.3326078Losses:  12.241704225540161 1.4289233684539795 0.9999664425849915 4.386286973953247
CurrentTrain: epoch  8, batch    16 | loss: 12.2417042Losses:  6.545609474182129 1.2594707012176514 0.960803747177124 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 6.5456095Losses:  8.236732892692089 1.1443854570388794 1.2514317035675049 1.561822347342968
CurrentTrain: epoch  8, batch    18 | loss: 8.2367329Losses:  10.57120081037283 0.9709453582763672 0.9736371040344238 4.342223130166531
CurrentTrain: epoch  8, batch    19 | loss: 10.5712008Losses:  6.7343339920043945 1.1188910007476807 1.3272743225097656 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 6.7343340Losses:  6.811258316040039 1.211510419845581 1.2758255004882812 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 6.8112583Losses:  12.837838165462017 1.3685517311096191 1.1701960563659668 5.886563770473003
CurrentTrain: epoch  8, batch    22 | loss: 12.8378382Losses:  16.43638165295124 0.9738144278526306 1.231218695640564 9.942356422543526
CurrentTrain: epoch  8, batch    23 | loss: 16.4363817Losses:  6.363712787628174 1.053049087524414 1.0424102544784546 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 6.3637128Losses:  19.123048923909664 1.188366413116455 1.0950241088867188 12.706756733357906
CurrentTrain: epoch  8, batch    25 | loss: 19.1230489Losses:  7.856501340866089 1.0002448558807373 1.190071702003479 1.393855333328247
CurrentTrain: epoch  8, batch    26 | loss: 7.8565013Losses:  16.992592988535762 0.9075585603713989 1.4357762336730957 10.047036347910762
CurrentTrain: epoch  8, batch    27 | loss: 16.9925930Losses:  6.245068073272705 0.9341429471969604 1.115572452545166 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 6.2450681Losses:  10.72964745759964 0.9876263737678528 1.1987371444702148 4.267051517963409
CurrentTrain: epoch  8, batch    29 | loss: 10.7296475Losses:  7.8847341276705265 1.2450149059295654 0.9925053119659424 1.4278716780245304
CurrentTrain: epoch  8, batch    30 | loss: 7.8847341Losses:  12.779524110257626 0.9125703573226929 1.2471644878387451 6.382657311856747
CurrentTrain: epoch  8, batch    31 | loss: 12.7795241Losses:  9.1194366812706 1.0883678197860718 0.9991170167922974 2.7841214537620544
CurrentTrain: epoch  8, batch    32 | loss: 9.1194367Losses:  6.458266735076904 1.2019318342208862 1.0322775840759277 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 6.4582667Losses:  9.049559533596039 1.3735792636871338 1.1322433948516846 1.4270972609519958
CurrentTrain: epoch  8, batch    34 | loss: 9.0495595Losses:  7.975205443799496 1.2537795305252075 1.038981556892395 1.4548130258917809
CurrentTrain: epoch  8, batch    35 | loss: 7.9752054Losses:  7.7969473004341125 1.1255747079849243 0.9776288270950317 1.3967517018318176
CurrentTrain: epoch  8, batch    36 | loss: 7.7969473Losses:  6.482237815856934 1.0797990560531616 1.2101471424102783 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.4822378Losses:  15.484128322452307 1.2699803113937378 1.2928202152252197 8.64827474579215
CurrentTrain: epoch  8, batch    38 | loss: 15.4841283Losses:  10.654177278280258 1.0381765365600586 1.2603728771209717 4.219565957784653
CurrentTrain: epoch  8, batch    39 | loss: 10.6541773Losses:  7.735163599252701 1.058153510093689 1.048317790031433 1.4033474028110504
CurrentTrain: epoch  8, batch    40 | loss: 7.7351636Losses:  8.118435442447662 1.2440743446350098 1.1831440925598145 1.4032111763954163
CurrentTrain: epoch  8, batch    41 | loss: 8.1184354Losses:  9.211500499397516 1.1328566074371338 0.9749652743339539 2.8346389271318913
CurrentTrain: epoch  8, batch    42 | loss: 9.2115005Losses:  15.480823758989573 0.905651330947876 1.4199903011322021 8.991939786821604
CurrentTrain: epoch  8, batch    43 | loss: 15.4808238Losses:  7.877569824457169 1.1251060962677002 1.1436538696289062 1.3890348970890045
CurrentTrain: epoch  8, batch    44 | loss: 7.8775698Losses:  7.7708330154418945 1.2655998468399048 0.8227652311325073 1.3914971351623535
CurrentTrain: epoch  8, batch    45 | loss: 7.7708330Losses:  10.624437410384417 0.9539837837219238 1.124354362487793 4.394803125411272
CurrentTrain: epoch  8, batch    46 | loss: 10.6244374Losses:  7.744339734315872 1.1424074172973633 1.004414439201355 1.4233310520648956
CurrentTrain: epoch  8, batch    47 | loss: 7.7443397Losses:  7.23050594329834 1.3428089618682861 1.0058822631835938 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 7.2305059Losses:  6.396663665771484 0.9933600425720215 1.1588225364685059 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 6.3966637Losses:  9.419358760118484 1.0544655323028564 1.2541131973266602 2.8159346878528595
CurrentTrain: epoch  8, batch    50 | loss: 9.4193588Losses:  7.90427090972662 1.050472378730774 1.150812029838562 1.4303777441382408
CurrentTrain: epoch  8, batch    51 | loss: 7.9042709Losses:  7.894707024097443 1.0171329975128174 1.1136555671691895 1.438641369342804
CurrentTrain: epoch  8, batch    52 | loss: 7.8947070Losses:  6.456806182861328 1.0582894086837769 1.2135608196258545 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 6.4568062Losses:  10.850031271576881 1.225292682647705 0.9432405829429626 4.388135328888893
CurrentTrain: epoch  8, batch    54 | loss: 10.8500313Losses:  7.973108943551779 1.1782009601593018 1.0473136901855469 1.4384400211274624
CurrentTrain: epoch  8, batch    55 | loss: 7.9731089Losses:  11.02125819772482 1.2129217386245728 1.139899730682373 4.4163182601332664
CurrentTrain: epoch  8, batch    56 | loss: 11.0212582Losses:  10.632642082870007 1.1692290306091309 1.0444623231887817 4.227243237197399
CurrentTrain: epoch  8, batch    57 | loss: 10.6326421Losses:  8.028952658176422 1.1466184854507446 1.1801986694335938 1.3943915963172913
CurrentTrain: epoch  8, batch    58 | loss: 8.0289527Losses:  8.031828254461288 1.2157095670700073 1.1347227096557617 1.407773345708847
CurrentTrain: epoch  8, batch    59 | loss: 8.0318283Losses:  6.253791332244873 0.9649260640144348 1.1248551607131958 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 6.2537913Losses:  12.372918043285608 1.2538347244262695 1.2894082069396973 5.617416772991419
CurrentTrain: epoch  8, batch    61 | loss: 12.3729180Losses:  7.800912976264954 0.7177808284759521 1.5399563312530518 1.4089223146438599
CurrentTrain: epoch  8, batch    62 | loss: 7.8009130Losses:  9.728781905025244 1.2150168418884277 1.080700159072876 3.2443077228963375
CurrentTrain: epoch  9, batch     0 | loss: 9.7287819Losses:  6.518117904663086 1.340773105621338 1.039710521697998 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 6.5181179Losses:  6.3038787841796875 1.0603313446044922 1.0411027669906616 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 6.3038788Losses:  9.413119159638882 1.2465038299560547 1.160226583480835 2.818682514131069
CurrentTrain: epoch  9, batch     3 | loss: 9.4131192Losses:  9.25734356045723 1.0747617483139038 1.1805884838104248 2.8103144466876984
CurrentTrain: epoch  9, batch     4 | loss: 9.2573436Losses:  10.762579709291458 1.1305490732192993 1.1242220401763916 4.29586198925972
CurrentTrain: epoch  9, batch     5 | loss: 10.7625797Losses:  7.7418785989284515 1.1324405670166016 1.012685775756836 1.4005294740200043
CurrentTrain: epoch  9, batch     6 | loss: 7.7418786Losses:  9.328557722270489 1.2832543849945068 1.058557152748108 2.8275053426623344
CurrentTrain: epoch  9, batch     7 | loss: 9.3285577Losses:  9.158922731876373 0.8593961000442505 1.3708550930023193 2.7963138222694397
CurrentTrain: epoch  9, batch     8 | loss: 9.1589227Losses:  6.53378963470459 1.1089997291564941 1.3000646829605103 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 6.5337896Losses:  9.170696713030338 1.036414623260498 1.085862636566162 2.9125303998589516
CurrentTrain: epoch  9, batch    10 | loss: 9.1706967Losses:  8.73925769329071 1.3026809692382812 1.1369807720184326 1.401880145072937
CurrentTrain: epoch  9, batch    11 | loss: 8.7392577Losses:  9.509711436927319 1.220625638961792 1.2381409406661987 2.8364412114024162
CurrentTrain: epoch  9, batch    12 | loss: 9.5097114Losses:  6.598324775695801 1.1678569316864014 1.2489917278289795 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 6.5983248Losses:  9.201066631823778 1.2063981294631958 0.9411371350288391 2.8692118115723133
CurrentTrain: epoch  9, batch    14 | loss: 9.2010666Losses:  10.395070157945156 1.074950933456421 1.0200834274291992 4.202237211167812
CurrentTrain: epoch  9, batch    15 | loss: 10.3950702Losses:  9.271557927131653 1.1426751613616943 1.0150539875030518 2.8520079851150513
CurrentTrain: epoch  9, batch    16 | loss: 9.2715579Losses:  6.337018013000488 1.1237168312072754 1.0799617767333984 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 6.3370180Losses:  7.807972699403763 1.059985876083374 1.1324208974838257 1.4226906597614288
CurrentTrain: epoch  9, batch    18 | loss: 7.8079727Losses:  13.668128553777933 1.2171409130096436 1.1800751686096191 7.1371822990477085
CurrentTrain: epoch  9, batch    19 | loss: 13.6681286Losses:  7.7846570909023285 1.0596297979354858 1.185625433921814 1.415646642446518
CurrentTrain: epoch  9, batch    20 | loss: 7.7846571Losses:  8.090866029262543 0.9245705604553223 1.2264481782913208 1.3995322585105896
CurrentTrain: epoch  9, batch    21 | loss: 8.0908660Losses:  6.165115833282471 0.9302465915679932 1.12004816532135 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 6.1651158Losses:  9.177514635026455 0.8192538022994995 1.3258858919143677 2.8473888263106346
CurrentTrain: epoch  9, batch    23 | loss: 9.1775146Losses:  7.0306396484375 1.2366245985031128 1.0731687545776367 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 7.0306396Losses:  10.7223717905581 1.1619112491607666 1.168626070022583 4.311503145843744
CurrentTrain: epoch  9, batch    25 | loss: 10.7223718Losses:  6.658195495605469 1.0603857040405273 1.2923061847686768 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 6.6581955Losses:  9.137432634830475 1.0619120597839355 1.1673424243927002 2.818218767642975
CurrentTrain: epoch  9, batch    27 | loss: 9.1374326Losses:  9.245013624429703 1.2051340341567993 1.0945754051208496 2.853591352701187
CurrentTrain: epoch  9, batch    28 | loss: 9.2450136Losses:  9.13920933380723 0.8508371114730835 1.2726081609725952 2.8540406860411167
CurrentTrain: epoch  9, batch    29 | loss: 9.1392093Losses:  7.833140462636948 1.1814637184143066 1.0325119495391846 1.4152832925319672
CurrentTrain: epoch  9, batch    30 | loss: 7.8331405Losses:  10.741795808076859 1.2153230905532837 1.1355111598968506 4.294568330049515
CurrentTrain: epoch  9, batch    31 | loss: 10.7417958Losses:  15.486105352640152 1.217276930809021 1.0657238960266113 9.020170122385025
CurrentTrain: epoch  9, batch    32 | loss: 15.4861054Losses:  10.597104221582413 1.0381298065185547 1.1662867069244385 4.294027477502823
CurrentTrain: epoch  9, batch    33 | loss: 10.5971042Losses:  6.465032577514648 1.2722227573394775 1.1068174839019775 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 6.4650326Losses:  12.22166532278061 1.1312954425811768 1.2585572004318237 5.687125146389008
CurrentTrain: epoch  9, batch    35 | loss: 12.2216653Losses:  9.301348239183426 0.8985134363174438 1.5026512145996094 2.7979498207569122
CurrentTrain: epoch  9, batch    36 | loss: 9.3013482Losses:  6.562230110168457 1.128065824508667 1.065362811088562 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.5622301Losses:  7.908168375492096 1.1098254919052124 1.0977768898010254 1.424824297428131
CurrentTrain: epoch  9, batch    38 | loss: 7.9081684Losses:  6.37916374206543 1.0142338275909424 1.1945207118988037 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 6.3791637Losses:  7.668439861387014 1.076056957244873 1.0110372304916382 1.4417858086526394
CurrentTrain: epoch  9, batch    40 | loss: 7.6684399Losses:  6.357631683349609 1.093467354774475 1.157895565032959 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 6.3576317Losses:  11.318238139152527 1.3825774192810059 1.1978046894073486 4.231918215751648
CurrentTrain: epoch  9, batch    42 | loss: 11.3182381Losses:  9.140864044427872 0.843346357345581 1.363996982574463 2.8193351328372955
CurrentTrain: epoch  9, batch    43 | loss: 9.1408640Losses:  7.653978884220123 0.889785885810852 1.2274091243743896 1.3974986672401428
CurrentTrain: epoch  9, batch    44 | loss: 7.6539789Losses:  7.8136336505413055 1.0210235118865967 1.2479133605957031 1.4105689227581024
CurrentTrain: epoch  9, batch    45 | loss: 7.8136337Losses:  9.615353733301163 1.2336723804473877 1.2242684364318848 2.850804954767227
CurrentTrain: epoch  9, batch    46 | loss: 9.6153537Losses:  9.372681260108948 1.1489312648773193 1.1986169815063477 2.812201142311096
CurrentTrain: epoch  9, batch    47 | loss: 9.3726813Losses:  6.389657020568848 1.064922571182251 1.1710307598114014 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 6.3896570Losses:  9.209395706653595 1.0946555137634277 1.0590264797210693 2.813021957874298
CurrentTrain: epoch  9, batch    49 | loss: 9.2093957Losses:  10.655448641628027 1.1470816135406494 1.0561316013336182 4.267771448940039
CurrentTrain: epoch  9, batch    50 | loss: 10.6554486Losses:  6.296838760375977 0.9239022731781006 1.2218416929244995 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 6.2968388Losses:  9.400327023118734 1.1125539541244507 1.2754948139190674 2.8643239103257656
CurrentTrain: epoch  9, batch    52 | loss: 9.4003270Losses:  9.268007576465607 1.2132740020751953 1.0454097986221313 2.8235371708869934
CurrentTrain: epoch  9, batch    53 | loss: 9.2680076Losses:  9.028308689594269 0.9895596504211426 1.1357014179229736 2.8229721188545227
CurrentTrain: epoch  9, batch    54 | loss: 9.0283087Losses:  10.705892123281956 1.2333992719650269 0.9562712907791138 4.29379228502512
CurrentTrain: epoch  9, batch    55 | loss: 10.7058921Losses:  10.727039810270071 1.2551482915878296 0.9718133211135864 4.3286714516580105
CurrentTrain: epoch  9, batch    56 | loss: 10.7270398Losses:  14.90360201895237 1.0251221656799316 1.0776996612548828 8.703757181763649
CurrentTrain: epoch  9, batch    57 | loss: 14.9036020Losses:  7.593194156885147 1.1031520366668701 0.9648471474647522 1.4082066118717194
CurrentTrain: epoch  9, batch    58 | loss: 7.5931942Losses:  10.44154268503189 1.2825069427490234 0.7053003311157227 4.201510488986969
CurrentTrain: epoch  9, batch    59 | loss: 10.4415427Losses:  7.712049573659897 1.1464354991912842 0.9328315854072571 1.4287548959255219
CurrentTrain: epoch  9, batch    60 | loss: 7.7120496Losses:  11.787790536880493 1.1712636947631836 1.1769181489944458 4.209481477737427
CurrentTrain: epoch  9, batch    61 | loss: 11.7877905Losses:  12.40436577051878 1.0405895709991455 1.2955435514450073 5.901660196483135
CurrentTrain: epoch  9, batch    62 | loss: 12.4043658
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  10.88020658120513 1.4014137983322144 1.3236225843429565 1.7766690216958523
CurrentTrain: epoch  0, batch     0 | loss: 10.8802066Losses:  14.981237083673477 1.5045584440231323 1.5774153470993042 5.963316589593887
CurrentTrain: epoch  0, batch     1 | loss: 14.9812371Losses:  11.588874023407698 1.459080457687378 1.171694040298462 2.902658622711897
CurrentTrain: epoch  0, batch     2 | loss: 11.5888740Losses:  10.690958641469479 1.2644696235656738 0.7313570976257324 1.4060598835349083
CurrentTrain: epoch  0, batch     3 | loss: 10.6909586Losses:  9.20311450958252 1.3975117206573486 1.2900097370147705 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.2031145Losses:  10.441185098141432 1.447791576385498 1.4206740856170654 1.568197350949049
CurrentTrain: epoch  1, batch     1 | loss: 10.4411851Losses:  9.35371083393693 1.5269858837127686 1.3374571800231934 1.5150710977613926
CurrentTrain: epoch  1, batch     2 | loss: 9.3537108Losses:  10.35912737250328 1.5052776336669922 1.7297849655151367 1.4408267438411713
CurrentTrain: epoch  1, batch     3 | loss: 10.3591274Losses:  7.468104839324951 1.3880555629730225 1.2140661478042603 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.4681048Losses:  11.206619955599308 1.4593572616577148 1.4470618963241577 3.0638577714562416
CurrentTrain: epoch  2, batch     1 | loss: 11.2066200Losses:  8.166701316833496 1.4295952320098877 1.3342067003250122 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 8.1667013Losses:  7.628772437572479 1.5958781242370605 1.5994391441345215 1.5290471911430359
CurrentTrain: epoch  2, batch     3 | loss: 7.6287724Losses:  11.04393906518817 1.3392797708511353 1.1634441614151 3.229219388216734
CurrentTrain: epoch  3, batch     0 | loss: 11.0439391Losses:  9.749176558107138 1.415441632270813 1.3697295188903809 3.0049505792558193
CurrentTrain: epoch  3, batch     1 | loss: 9.7491766Losses:  8.609725505113602 1.4711531400680542 1.2778100967407227 1.417106181383133
CurrentTrain: epoch  3, batch     2 | loss: 8.6097255Losses:  8.509655050933361 1.7914543151855469 1.443282127380371 1.44855647534132
CurrentTrain: epoch  3, batch     3 | loss: 8.5096551Losses:  6.703130722045898 1.4607090950012207 1.6007568836212158 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 6.7031307Losses:  12.216551374644041 1.4711395502090454 1.3816803693771362 5.31226546689868
CurrentTrain: epoch  4, batch     1 | loss: 12.2165514Losses:  6.514758586883545 1.4010506868362427 1.2552376985549927 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.5147586Losses:  8.639138370752335 1.3268980979919434 0.8455538749694824 1.4978887140750885
CurrentTrain: epoch  4, batch     3 | loss: 8.6391384Losses:  6.763972759246826 1.4312701225280762 1.4344288110733032 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.7639728Losses:  6.325355529785156 1.4439393281936646 1.365204095840454 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 6.3253555Losses:  10.368355110287666 1.3952627182006836 1.4264371395111084 3.8981917649507523
CurrentTrain: epoch  5, batch     2 | loss: 10.3683551Losses:  7.240101411938667 1.2137484550476074 0.9399294853210449 1.4335227757692337
CurrentTrain: epoch  5, batch     3 | loss: 7.2401014Losses:  6.200108528137207 1.5068422555923462 1.413135051727295 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 6.2001085Losses:  6.033482551574707 1.4320405721664429 1.3083746433258057 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 6.0334826Losses:  5.92115592956543 1.3692001104354858 1.2821362018585205 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 5.9211559Losses:  6.733958788216114 1.1841387748718262 1.369279384613037 1.5182114318013191
CurrentTrain: epoch  6, batch     3 | loss: 6.7339588Losses:  8.330155070871115 1.4674198627471924 1.3695722818374634 2.3105594478547573
CurrentTrain: epoch  7, batch     0 | loss: 8.3301551Losses:  7.026994552463293 1.405106782913208 1.3133792877197266 1.5024965666234493
CurrentTrain: epoch  7, batch     1 | loss: 7.0269946Losses:  6.8712780103087425 1.3470431566238403 1.406493067741394 1.467963419854641
CurrentTrain: epoch  7, batch     2 | loss: 6.8712780Losses:  5.790614053606987 1.2747540473937988 0.5749402046203613 1.4610113352537155
CurrentTrain: epoch  7, batch     3 | loss: 5.7906141Losses:  9.733511563390493 1.546639084815979 1.2705371379852295 4.307541485875845
CurrentTrain: epoch  8, batch     0 | loss: 9.7335116Losses:  8.311412364244461 1.3830924034118652 1.4106684923171997 2.820518523454666
CurrentTrain: epoch  8, batch     1 | loss: 8.3114124Losses:  5.1318583488464355 1.318089485168457 1.2379125356674194 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 5.1318583Losses:  5.914916254580021 1.313173770904541 0.8960843086242676 1.4262635484337807
CurrentTrain: epoch  8, batch     3 | loss: 5.9149163Losses:  6.610645290464163 1.3784871101379395 1.3007190227508545 1.4069251976907253
CurrentTrain: epoch  9, batch     0 | loss: 6.6106453Losses:  6.600492510944605 1.4383976459503174 1.3550024032592773 1.4352169372141361
CurrentTrain: epoch  9, batch     1 | loss: 6.6004925Losses:  7.119423186406493 1.3742806911468506 1.1397864818572998 2.019113814458251
CurrentTrain: epoch  9, batch     2 | loss: 7.1194232Losses:  5.951165936887264 1.1232643127441406 1.2095627784729004 1.4648297056555748
CurrentTrain: epoch  9, batch     3 | loss: 5.9511659
Losses:  4.7508544921875 1.1153690814971924 1.3108105659484863 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 4.7508545Losses:  9.865436855703592 1.5118787288665771 0.9871323108673096 5.795736614614725
MemoryTrain:  epoch  0, batch     1 | loss: 9.8654369Losses:  4.50177526473999 1.1155425310134888 1.1403851509094238 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 4.5017753Losses:  8.94497886300087 1.523297905921936 1.318158745765686 5.661337524652481
MemoryTrain:  epoch  1, batch     1 | loss: 8.9449789Losses:  3.4904277324676514 1.2648736238479614 1.278412103652954 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.4904277Losses:  10.443957660347223 0.9027221202850342 1.6774256229400635 5.667046878486872
MemoryTrain:  epoch  2, batch     1 | loss: 10.4439577Losses:  3.591273069381714 1.176496982574463 1.2599232196807861 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.5912731Losses:  8.508661434054375 1.2602767944335938 1.2620422840118408 5.743442699313164
MemoryTrain:  epoch  3, batch     1 | loss: 8.5086614Losses:  3.780299186706543 1.2942640781402588 1.2476884126663208 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 3.7802992Losses:  8.359260458499193 0.8599804639816284 1.42996346950531 5.808633703738451
MemoryTrain:  epoch  4, batch     1 | loss: 8.3592605Losses:  3.104428291320801 1.1635689735412598 1.3400275707244873 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 3.1044283Losses:  9.968214251101017 1.3862783908843994 1.0234625339508057 5.723706461489201
MemoryTrain:  epoch  5, batch     1 | loss: 9.9682143Losses:  3.1949822902679443 1.1568900346755981 1.0534734725952148 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 3.1949823Losses:  8.530300945043564 1.3539327383041382 1.4235601425170898 5.633589595556259
MemoryTrain:  epoch  6, batch     1 | loss: 8.5303009Losses:  3.472825050354004 1.2369825839996338 1.2969416379928589 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 3.4728251Losses:  7.9810970723629 1.0067414045333862 1.264822006225586 5.644862979650497
MemoryTrain:  epoch  7, batch     1 | loss: 7.9810971Losses:  3.3864684104919434 1.2167388200759888 1.22702956199646 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 3.3864684Losses:  8.038396194577217 0.9904301166534424 1.2304842472076416 5.718699529767036
MemoryTrain:  epoch  8, batch     1 | loss: 8.0383962Losses:  3.2444028854370117 1.1662644147872925 1.1363672018051147 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 3.2444029Losses:  8.527155220508575 1.2567180395126343 1.5038317441940308 5.725684702396393
MemoryTrain:  epoch  9, batch     1 | loss: 8.5271552
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 96.59%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 96.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 94.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 94.14%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.84%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 88.94%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.22%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 73.75%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 71.94%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 71.35%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.46%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 91.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.02%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.83%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 92.87%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.20%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.40%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 93.41%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 93.41%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 93.34%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 93.26%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 93.11%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.96%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.66%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 92.32%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 92.21%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 92.08%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.81%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 91.57%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 91.11%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 91.21%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 90.90%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 90.52%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 90.09%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 89.54%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 89.06%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.72%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 88.33%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 88.01%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.62%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 87.19%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 86.89%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 86.41%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 86.18%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 85.77%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.11%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 84.61%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 84.06%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 83.64%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 83.22%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.92%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 82.74%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.83%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 83.91%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.20%   
cur_acc:  ['0.9484', '0.7500']
his_acc:  ['0.9484', '0.8420']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  9.487022213637829 1.4245529174804688 1.425213098526001 1.5774457976222038
CurrentTrain: epoch  0, batch     0 | loss: 9.4870222Losses:  9.431822776794434 1.2610403299331665 1.1746044158935547 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 9.4318228Losses:  12.365134373307228 1.244447112083435 1.4170122146606445 4.279824391007423
CurrentTrain: epoch  0, batch     2 | loss: 12.3651344Losses:  12.470434956252575 1.1388921737670898 1.0853824615478516 1.586976818740368
CurrentTrain: epoch  0, batch     3 | loss: 12.4704350Losses:  9.698645923286676 1.288043737411499 1.2677990198135376 1.4289792515337467
CurrentTrain: epoch  1, batch     0 | loss: 9.6986459Losses:  15.053027883172035 1.2655574083328247 1.138832926750183 8.127123609185219
CurrentTrain: epoch  1, batch     1 | loss: 15.0530279Losses:  10.10214975476265 1.239478588104248 1.212756633758545 1.439975529909134
CurrentTrain: epoch  1, batch     2 | loss: 10.1021498Losses:  10.986026920378208 1.4775114059448242 1.2996501922607422 1.5197793617844582
CurrentTrain: epoch  1, batch     3 | loss: 10.9860269Losses:  15.931717418134212 1.232287883758545 1.450972080230713 8.275837443768978
CurrentTrain: epoch  2, batch     0 | loss: 15.9317174Losses:  10.066131383180618 1.239534854888916 1.2164292335510254 2.2897556126117706
CurrentTrain: epoch  2, batch     1 | loss: 10.0661314Losses:  9.028310421854258 1.339045524597168 1.0995969772338867 1.4584299363195896
CurrentTrain: epoch  2, batch     2 | loss: 9.0283104Losses:  9.758668154478073 1.305976390838623 1.1924376487731934 1.4937431514263153
CurrentTrain: epoch  2, batch     3 | loss: 9.7586682Losses:  7.2269158363342285 1.354379653930664 1.1563349962234497 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.2269158Losses:  10.124209441244602 1.2538150548934937 1.1323624849319458 3.4528937712311745
CurrentTrain: epoch  3, batch     1 | loss: 10.1242094Losses:  10.498165749013424 1.1386611461639404 1.367741346359253 2.8913113102316856
CurrentTrain: epoch  3, batch     2 | loss: 10.4981657Losses:  9.863988276571035 1.0748591423034668 1.7044644355773926 1.8497060500085354
CurrentTrain: epoch  3, batch     3 | loss: 9.8639883Losses:  10.361890345811844 1.3156723976135254 1.0568616390228271 3.136044055223465
CurrentTrain: epoch  4, batch     0 | loss: 10.3618903Losses:  7.9501011967659 1.2252496480941772 1.2085716724395752 1.419711410999298
CurrentTrain: epoch  4, batch     1 | loss: 7.9501012Losses:  10.656090997159481 1.1451444625854492 1.3746813535690308 3.1638453230261803
CurrentTrain: epoch  4, batch     2 | loss: 10.6560910Losses:  6.904431521892548 1.4711661338806152 1.1454253196716309 1.4761683344841003
CurrentTrain: epoch  4, batch     3 | loss: 6.9044315Losses:  6.910510540008545 1.180856466293335 1.4396311044692993 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.9105105Losses:  7.501865230500698 1.3335659503936768 1.1819922924041748 1.4782360419631004
CurrentTrain: epoch  5, batch     1 | loss: 7.5018652Losses:  8.381002306938171 1.1518805027008057 1.1360770463943481 1.4218796491622925
CurrentTrain: epoch  5, batch     2 | loss: 8.3810023Losses:  8.860641486942768 1.1747784614562988 1.3045268058776855 1.439179427921772
CurrentTrain: epoch  5, batch     3 | loss: 8.8606415Losses:  9.239657700061798 1.2923648357391357 1.2776963710784912 2.9004090428352356
CurrentTrain: epoch  6, batch     0 | loss: 9.2396577Losses:  9.89703617990017 1.118377447128296 1.405791997909546 2.8927541822195053
CurrentTrain: epoch  6, batch     1 | loss: 9.8970362Losses:  11.432360894978046 1.233086347579956 1.025794267654419 4.939367540180683
CurrentTrain: epoch  6, batch     2 | loss: 11.4323609Losses:  8.802421476691961 1.3465404510498047 1.2047348022460938 1.6332377456128597
CurrentTrain: epoch  6, batch     3 | loss: 8.8024215Losses:  5.7416791915893555 1.3045244216918945 1.2359607219696045 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.7416792Losses:  8.231577634811401 1.2265219688415527 1.3316528797149658 1.412261724472046
CurrentTrain: epoch  7, batch     1 | loss: 8.2315776Losses:  11.447892364114523 1.0904955863952637 1.3374338150024414 4.826986011117697
CurrentTrain: epoch  7, batch     2 | loss: 11.4478924Losses:  8.462844889611006 1.1211848258972168 0.9442582130432129 1.5808949880301952
CurrentTrain: epoch  7, batch     3 | loss: 8.4628449Losses:  10.87454728409648 1.1721127033233643 1.2423162460327148 4.593741696327925
CurrentTrain: epoch  8, batch     0 | loss: 10.8745473Losses:  9.540713600814342 1.1206932067871094 1.1452754735946655 2.9219902083277702
CurrentTrain: epoch  8, batch     1 | loss: 9.5407136Losses:  9.819488868117332 1.2461094856262207 1.3118622303009033 4.380634650588036
CurrentTrain: epoch  8, batch     2 | loss: 9.8194889Losses:  7.4848374128341675 1.3372893333435059 0.697476863861084 1.3939474821090698
CurrentTrain: epoch  8, batch     3 | loss: 7.4848374Losses:  6.111321926116943 1.092937707901001 1.2843575477600098 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 6.1113219Losses:  7.164990276098251 1.2602967023849487 1.2872803211212158 1.4043005406856537
CurrentTrain: epoch  9, batch     1 | loss: 7.1649903Losses:  8.07922488451004 1.2026901245117188 1.0775225162506104 1.4882853627204895
CurrentTrain: epoch  9, batch     2 | loss: 8.0792249Losses:  5.609457731246948 1.1660170555114746 0.8669877052307129 1.4405629634857178
CurrentTrain: epoch  9, batch     3 | loss: 5.6094577
Losses:  3.6046547889709473 1.158511757850647 1.3504217863082886 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.6046548Losses:  3.47916841506958 1.1862229108810425 1.347863793373108 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 3.4791684Losses:  3.3722660541534424 1.090606451034546 1.2286183834075928 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.3722661Losses:  4.047525405883789 1.251866102218628 1.535919189453125 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 4.0475254Losses:  3.833109140396118 1.177838683128357 1.3579223155975342 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.8331091Losses:  2.819365978240967 1.125543475151062 1.4040509462356567 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.8193660Losses:  3.273955821990967 1.2038582563400269 1.5589590072631836 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.2739558Losses:  2.889988899230957 1.0703495740890503 1.1360071897506714 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.8899889Losses:  2.482083320617676 1.0709035396575928 1.2568089962005615 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.4820833Losses:  3.3725461959838867 1.2326725721359253 1.3303126096725464 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 3.3725462Losses:  2.787554979324341 1.104156494140625 1.3418673276901245 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.7875550Losses:  2.855438709259033 1.165118932723999 1.3602405786514282 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.8554387Losses:  3.043192148208618 1.130204200744629 1.4312877655029297 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 3.0431921Losses:  2.608281135559082 1.1476759910583496 1.372244954109192 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.6082811Losses:  2.6064090728759766 1.2813423871994019 1.2158770561218262 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.6064091Losses:  2.9907913208007812 0.916327178478241 1.6182135343551636 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.9907913Losses:  2.5091712474823 1.1493096351623535 1.238318681716919 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.5091712Losses:  2.693953037261963 1.0578988790512085 1.4891026020050049 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.6939530Losses:  2.5882701873779297 1.0691821575164795 1.3841183185577393 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.5882702Losses:  2.4963715076446533 1.1304469108581543 1.276133418083191 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.4963715
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.83%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 73.04%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 73.20%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 73.03%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 72.74%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 72.56%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.43%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.55%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 92.21%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 91.59%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 91.21%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 90.83%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.57%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.42%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.43%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.58%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 90.72%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 91.06%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.10%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.87%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 90.74%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 90.55%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.59%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 90.55%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 90.29%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 89.93%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 89.83%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 89.51%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 89.28%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 88.83%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 88.40%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 88.11%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 86.91%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 86.59%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 86.28%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 86.16%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 86.05%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 85.88%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.58%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.29%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.95%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.74%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 84.35%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.70%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 83.04%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.45%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 81.76%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.80%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.53%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.70%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 81.46%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.56%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.66%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.55%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 81.20%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 80.37%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 79.99%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 79.52%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 79.10%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 79.54%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 79.80%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 79.36%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 78.97%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 78.68%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 78.35%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 78.15%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 77.86%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.62%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 78.52%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 78.25%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 78.15%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 78.04%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 77.86%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 77.80%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 77.87%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.07%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.16%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.92%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 79.19%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.20%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 79.18%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 79.15%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 79.09%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 79.08%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 79.10%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 78.97%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 78.88%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 78.76%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 78.48%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 78.39%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 78.16%   
cur_acc:  ['0.9484', '0.7500', '0.7143']
his_acc:  ['0.9484', '0.8420', '0.7816']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  8.169928550720215 1.511452078819275 1.209075689315796 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 8.1699286Losses:  9.480765663087368 1.4206266403198242 1.4421706199645996 1.4593880996108055
CurrentTrain: epoch  0, batch     1 | loss: 9.4807657Losses:  15.250174149870872 1.4877855777740479 1.257400393486023 8.033889397978783
CurrentTrain: epoch  0, batch     2 | loss: 15.2501741Losses:  9.274948511272669 1.4203171730041504 1.633558750152588 1.4926890470087528
CurrentTrain: epoch  0, batch     3 | loss: 9.2749485Losses:  10.703659549355507 1.4268016815185547 1.5817111730575562 2.9660391956567764
CurrentTrain: epoch  1, batch     0 | loss: 10.7036595Losses:  6.883195877075195 1.5280616283416748 1.4016859531402588 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 6.8831959Losses:  7.68902650848031 1.4768375158309937 1.2741923332214355 1.4488770104944706
CurrentTrain: epoch  1, batch     2 | loss: 7.6890265Losses:  7.711204849183559 1.447664737701416 1.3588261604309082 1.4822476729750633
CurrentTrain: epoch  1, batch     3 | loss: 7.7112048Losses:  11.031970299780369 1.5852512121200562 1.3603380918502808 4.242119587957859
CurrentTrain: epoch  2, batch     0 | loss: 11.0319703Losses:  9.06274413317442 1.3580548763275146 1.2213754653930664 2.870421402156353
CurrentTrain: epoch  2, batch     1 | loss: 9.0627441Losses:  8.785945475101471 1.400564193725586 1.488491415977478 2.9223843216896057
CurrentTrain: epoch  2, batch     2 | loss: 8.7859455Losses:  10.036864072084427 0.6574325561523438 1.0 6.467395097017288
CurrentTrain: epoch  2, batch     3 | loss: 10.0368641Losses:  7.910990618169308 1.3312499523162842 1.1828348636627197 1.9908827766776085
CurrentTrain: epoch  3, batch     0 | loss: 7.9109906Losses:  10.13857650756836 1.5475208759307861 1.272870421409607 4.280144214630127
CurrentTrain: epoch  3, batch     1 | loss: 10.1385765Losses:  8.285427041351795 1.3853734731674194 1.321231722831726 2.899805970489979
CurrentTrain: epoch  3, batch     2 | loss: 8.2854270Losses:  6.214573182165623 0.8770713806152344 1.6621441841125488 1.5066969767212868
CurrentTrain: epoch  3, batch     3 | loss: 6.2145732Losses:  7.0023155845701694 1.3848602771759033 1.1767120361328125 1.4580860771238804
CurrentTrain: epoch  4, batch     0 | loss: 7.0023156Losses:  5.694180488586426 1.3392736911773682 1.3878135681152344 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 5.6941805Losses:  7.02188353985548 1.4246509075164795 1.1477689743041992 1.4726076647639275
CurrentTrain: epoch  4, batch     2 | loss: 7.0218835Losses:  5.248448371887207 1.2701926231384277 0.6700825691223145 1.4117493629455566
CurrentTrain: epoch  4, batch     3 | loss: 5.2484484Losses:  7.893754802644253 1.3098163604736328 1.2505548000335693 2.881163440644741
CurrentTrain: epoch  5, batch     0 | loss: 7.8937548Losses:  5.359249114990234 1.4196994304656982 1.3136844635009766 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.3592491Losses:  6.928394839167595 1.3662326335906982 1.1205641031265259 1.5382008999586105
CurrentTrain: epoch  5, batch     2 | loss: 6.9283948Losses:  7.027791067957878 1.1880669593811035 1.800626277923584 1.626048132777214
CurrentTrain: epoch  5, batch     3 | loss: 7.0277911Losses:  5.380629062652588 1.358088493347168 1.4642424583435059 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.3806291Losses:  6.653902143239975 1.3552937507629395 1.3313493728637695 1.4121647775173187
CurrentTrain: epoch  6, batch     1 | loss: 6.6539021Losses:  10.95141065120697 1.309302806854248 1.0455842018127441 5.952385783195496
CurrentTrain: epoch  6, batch     2 | loss: 10.9514107Losses:  6.627755202353001 1.4253897666931152 1.8181300163269043 1.481374777853489
CurrentTrain: epoch  6, batch     3 | loss: 6.6277552Losses:  6.402772955596447 1.2285773754119873 1.1963732242584229 1.4373808428645134
CurrentTrain: epoch  7, batch     0 | loss: 6.4027730Losses:  5.178479194641113 1.320507287979126 1.2412657737731934 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.1784792Losses:  9.427951656281948 1.445826768875122 1.583780288696289 4.212603889405727
CurrentTrain: epoch  7, batch     2 | loss: 9.4279517Losses:  6.223989002406597 1.2565269470214844 1.6808042526245117 1.4637694284319878
CurrentTrain: epoch  7, batch     3 | loss: 6.2239890Losses:  6.137601315975189 1.32847261428833 1.16865873336792 1.4063910841941833
CurrentTrain: epoch  8, batch     0 | loss: 6.1376013Losses:  6.2501019313931465 1.2733501195907593 1.2543818950653076 1.5031346157193184
CurrentTrain: epoch  8, batch     1 | loss: 6.2501019Losses:  4.615090847015381 1.2530715465545654 1.2665570974349976 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 4.6150908Losses:  6.201646536588669 1.361767292022705 1.1760354042053223 1.4390952289104462
CurrentTrain: epoch  8, batch     3 | loss: 6.2016465Losses:  7.769121639430523 1.3140360116958618 1.3913772106170654 2.86815594881773
CurrentTrain: epoch  9, batch     0 | loss: 7.7691216Losses:  4.574810028076172 1.2826917171478271 1.1381938457489014 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.5748100Losses:  10.166182052344084 1.2633483409881592 1.105955958366394 5.6501784436404705
CurrentTrain: epoch  9, batch     2 | loss: 10.1661821Losses:  4.9021502658724785 1.1952238082885742 0.5011882781982422 1.4423004314303398
CurrentTrain: epoch  9, batch     3 | loss: 4.9021503
Losses:  3.2041234970092773 1.0461225509643555 1.1981821060180664 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.2041235Losses:  2.977468729019165 1.0965657234191895 1.2999086380004883 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.9774687Losses:  4.025038242340088 1.3009910583496094 1.0009833574295044 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 4.0250382Losses:  3.1483962535858154 1.2058820724487305 1.2431553602218628 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.1483963Losses:  3.8995628356933594 1.0725390911102295 1.252497673034668 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.8995628Losses:  3.022728443145752 1.007283329963684 1.043332815170288 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 3.0227284Losses:  2.7478585243225098 1.0323563814163208 1.182751178741455 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.7478585Losses:  2.9952268600463867 1.1537226438522339 1.2376341819763184 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.9952269Losses:  3.2559633255004883 1.205885887145996 1.4189679622650146 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 3.2559633Losses:  2.5989785194396973 1.0139358043670654 1.2666901350021362 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.5989785Losses:  2.530442237854004 1.1902602910995483 1.131019115447998 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.5304422Losses:  3.0810189247131348 1.1121790409088135 1.1487970352172852 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 3.0810189Losses:  2.85750150680542 1.1321442127227783 1.3981337547302246 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.8575015Losses:  2.321974277496338 1.0315685272216797 1.0424736738204956 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.3219743Losses:  2.7255778312683105 1.157779574394226 1.2492722272872925 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.7255778Losses:  2.5149452686309814 1.0403748750686646 1.2920236587524414 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.5149453Losses:  2.7740554809570312 1.1176187992095947 1.361853003501892 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.7740555Losses:  2.532195568084717 1.07688570022583 1.2473098039627075 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.5321956Losses:  2.4651975631713867 1.0841621160507202 1.2519598007202148 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.4651976Losses:  2.657200574874878 1.0858098268508911 1.3872345685958862 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.6572006Losses:  2.420290946960449 1.0921010971069336 1.2710211277008057 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.4202909Losses:  2.339340925216675 0.99774169921875 1.2568697929382324 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.3393409Losses:  2.3168153762817383 1.063075065612793 1.1212244033813477 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.3168154Losses:  2.8572301864624023 1.220928430557251 1.5379843711853027 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.8572302Losses:  2.329406261444092 1.0012167692184448 1.2718727588653564 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.3294063Losses:  2.781266212463379 1.1498115062713623 1.546115756034851 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.7812662Losses:  2.464445114135742 1.1238611936569214 1.2580995559692383 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.4644451Losses:  2.4998044967651367 1.071777582168579 1.3689720630645752 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.4998045Losses:  2.620797634124756 1.0622828006744385 1.479461669921875 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.6207976Losses:  2.409109592437744 1.1420643329620361 1.2099461555480957 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.4091096
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 53.98%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 51.90%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 50.78%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 49.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 51.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 53.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 55.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 56.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 59.77%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 59.47%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 59.56%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 59.64%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.73%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.32%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 87.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 86.85%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 86.55%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 86.56%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 86.43%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.74%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 86.94%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 86.95%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.59%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 87.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.82%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.74%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 87.66%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 87.66%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 87.20%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 86.98%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.54%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.48%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 86.21%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 85.60%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 85.21%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 84.99%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 84.68%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 84.18%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 83.75%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 83.27%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 82.99%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 82.84%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 82.83%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 82.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.30%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.11%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.86%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 81.67%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 81.49%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 81.37%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.96%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.32%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.76%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 78.55%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.07%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.82%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.01%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 79.23%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 78.77%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.44%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 77.98%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 77.57%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 77.07%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 76.57%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.31%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 76.84%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 76.47%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.15%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 75.92%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.70%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.43%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 76.16%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 75.78%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 75.20%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 75.08%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 76.09%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.21%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 76.20%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 76.16%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 76.15%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.21%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 76.17%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.05%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 76.15%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 76.03%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 75.84%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 75.77%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 75.99%   [EVAL] batch:  195 | acc: 12.50%,  total acc: 75.67%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 75.35%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 74.97%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 74.65%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 74.28%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 74.25%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 74.29%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.18%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 74.03%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 73.83%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 73.62%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 73.33%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 73.13%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 72.85%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.80%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 73.52%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 73.41%   [EVAL] batch:  220 | acc: 68.75%,  total acc: 73.39%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:  222 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:  223 | acc: 62.50%,  total acc: 73.13%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 72.94%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.32%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 74.35%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 74.40%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 74.41%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.92%   
cur_acc:  ['0.9484', '0.7500', '0.7143', '0.7173']
his_acc:  ['0.9484', '0.8420', '0.7816', '0.7492']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  9.927556961774826 1.3723080158233643 1.3420774936676025 1.6335344016551971
CurrentTrain: epoch  0, batch     0 | loss: 9.9275570Losses:  14.487102009356022 1.268075942993164 1.3211779594421387 6.286149479448795
CurrentTrain: epoch  0, batch     1 | loss: 14.4871020Losses:  10.959727600216866 1.3383004665374756 1.2825114727020264 2.979205444455147
CurrentTrain: epoch  0, batch     2 | loss: 10.9597276Losses:  10.3263477422297 1.18701171875 1.0892858505249023 1.605116281658411
CurrentTrain: epoch  0, batch     3 | loss: 10.3263477Losses:  14.441434539854527 1.354036569595337 1.4041616916656494 7.27727285772562
CurrentTrain: epoch  1, batch     0 | loss: 14.4414345Losses:  7.312443733215332 1.2479138374328613 1.1193480491638184 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 7.3124437Losses:  8.106263160705566 1.3323057889938354 1.2499679327011108 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 8.1062632Losses:  7.886365465819836 1.1796207427978516 1.0651483535766602 1.55768733471632
CurrentTrain: epoch  1, batch     3 | loss: 7.8863655Losses:  9.345645647495985 1.2408623695373535 1.0412992238998413 1.525370817631483
CurrentTrain: epoch  2, batch     0 | loss: 9.3456456Losses:  8.328872200101614 1.2963923215866089 1.3324910402297974 1.5621771775186062
CurrentTrain: epoch  2, batch     1 | loss: 8.3288722Losses:  15.138452008366585 1.2855454683303833 1.1955971717834473 9.047098115086555
CurrentTrain: epoch  2, batch     2 | loss: 15.1384520Losses:  7.876056760549545 1.3506550788879395 0.9611163139343262 1.4703942239284515
CurrentTrain: epoch  2, batch     3 | loss: 7.8760568Losses:  7.313091278076172 1.2608133554458618 1.3157343864440918 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.3130913Losses:  7.5896265506744385 1.253840446472168 1.239260196685791 1.4087789058685303
CurrentTrain: epoch  3, batch     1 | loss: 7.5896266Losses:  9.488516956567764 1.2785248756408691 1.1420371532440186 3.155515819787979
CurrentTrain: epoch  3, batch     2 | loss: 9.4885170Losses:  6.017598606646061 1.2186031341552734 0.7993125915527344 1.524265743792057
CurrentTrain: epoch  3, batch     3 | loss: 6.0175986Losses:  12.684020105749369 1.201595425605774 0.9925537109375 6.218308035284281
CurrentTrain: epoch  4, batch     0 | loss: 12.6840201Losses:  7.612734019756317 1.3153409957885742 1.1191558837890625 1.53988379240036
CurrentTrain: epoch  4, batch     1 | loss: 7.6127340Losses:  7.086916033178568 1.2172470092773438 1.1142277717590332 1.4826670326292515
CurrentTrain: epoch  4, batch     2 | loss: 7.0869160Losses:  6.866804279386997 1.1174907684326172 1.0797176361083984 1.424249328672886
CurrentTrain: epoch  4, batch     3 | loss: 6.8668043Losses:  9.735361091792583 1.2197117805480957 1.2824435234069824 3.179195396602154
CurrentTrain: epoch  5, batch     0 | loss: 9.7353611Losses:  5.4785685539245605 1.2131435871124268 1.142576813697815 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.4785686Losses:  5.806848049163818 1.2357335090637207 1.3352618217468262 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 5.8068480Losses:  5.710932932794094 1.2364048957824707 0.9986729621887207 1.4183590039610863
CurrentTrain: epoch  5, batch     3 | loss: 5.7109329Losses:  7.741397822275758 1.138137936592102 1.069510579109192 1.869539225474
CurrentTrain: epoch  6, batch     0 | loss: 7.7413978Losses:  8.459703236818314 1.1801886558532715 1.2468445301055908 3.0636031925678253
CurrentTrain: epoch  6, batch     1 | loss: 8.4597032Losses:  7.283138573169708 1.264665961265564 1.2894155979156494 1.448428452014923
CurrentTrain: epoch  6, batch     2 | loss: 7.2831386Losses:  6.063460297882557 1.151010513305664 1.2327995300292969 1.4485015347599983
CurrentTrain: epoch  6, batch     3 | loss: 6.0634603Losses:  8.160807847976685 1.136124610900879 1.0591063499450684 2.872422933578491
CurrentTrain: epoch  7, batch     0 | loss: 8.1608078Losses:  5.55768346786499 1.2483999729156494 1.1836400032043457 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.5576835Losses:  6.9142115116119385 1.2193078994750977 1.25551176071167 1.4223277568817139
CurrentTrain: epoch  7, batch     2 | loss: 6.9142115Losses:  7.399562984704971 1.1729092597961426 1.1289734840393066 1.4081059992313385
CurrentTrain: epoch  7, batch     3 | loss: 7.3995630Losses:  5.393950462341309 1.1003199815750122 1.0902348756790161 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 5.3939505Losses:  9.750440120697021 1.2450087070465088 1.2487266063690186 4.550478458404541
CurrentTrain: epoch  8, batch     1 | loss: 9.7504401Losses:  8.3411171361804 1.1487761735916138 1.2783578634262085 3.03604768961668
CurrentTrain: epoch  8, batch     2 | loss: 8.3411171Losses:  6.173856288194656 1.221719741821289 1.3676643371582031 1.5051575005054474
CurrentTrain: epoch  8, batch     3 | loss: 6.1738563Losses:  8.003593064844608 1.1746876239776611 1.2345150709152222 2.867565728724003
CurrentTrain: epoch  9, batch     0 | loss: 8.0035931Losses:  6.490697745233774 1.1384186744689941 1.0738906860351562 1.4456952847540379
CurrentTrain: epoch  9, batch     1 | loss: 6.4906977Losses:  5.214998722076416 1.189896821975708 1.191943645477295 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.2149987Losses:  5.8765156753361225 1.0061702728271484 0.6907186508178711 1.497199822217226
CurrentTrain: epoch  9, batch     3 | loss: 5.8765157
Losses:  2.9790079593658447 1.0012006759643555 1.2048399448394775 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.9790080Losses:  2.751194477081299 1.038747787475586 1.3491891622543335 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.7511945Losses:  2.641774892807007 1.2114348411560059 1.2937273979187012 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.6417749Losses:  5.39659109711647 1.3658685684204102 1.462681770324707 1.7232803404331207
MemoryTrain:  epoch  0, batch     3 | loss: 5.3965911Losses:  3.3144960403442383 1.061605453491211 1.4441858530044556 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.3144960Losses:  2.9866108894348145 1.0159775018692017 1.2848684787750244 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.9866109Losses:  2.950356960296631 1.2130200862884521 1.1487421989440918 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.9503570Losses:  5.311450123786926 1.0673513412475586 1.9938678741455078 1.437338948249817
MemoryTrain:  epoch  1, batch     3 | loss: 5.3114501Losses:  3.0127696990966797 1.0734021663665771 1.4773746728897095 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.0127697Losses:  2.7221312522888184 1.137492299079895 1.2361770868301392 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.7221313Losses:  2.663571834564209 1.0264095067977905 1.2822558879852295 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.6635718Losses:  5.122135877609253 1.3132810592651367 2.2983646392822266 1.4061987400054932
MemoryTrain:  epoch  2, batch     3 | loss: 5.1221359Losses:  2.8156697750091553 1.0084949731826782 1.4911750555038452 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.8156698Losses:  2.3425371646881104 1.034822702407837 1.1195061206817627 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.3425372Losses:  2.68612003326416 1.1895394325256348 1.3426108360290527 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.6861200Losses:  3.5617397353053093 1.0129518508911133 1.0469732284545898 1.4390757605433464
MemoryTrain:  epoch  3, batch     3 | loss: 3.5617397Losses:  2.910196304321289 1.2508821487426758 1.3687069416046143 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.9101963Losses:  2.3088619709014893 0.859271764755249 1.3396239280700684 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.3088620Losses:  2.4693431854248047 1.1383188962936401 1.1723822355270386 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.4693432Losses:  3.6104863584041595 1.2264132499694824 0.8579497337341309 1.414101928472519
MemoryTrain:  epoch  4, batch     3 | loss: 3.6104864Losses:  2.430589437484741 1.146806240081787 1.175048589706421 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.4305894Losses:  2.6052608489990234 1.1497176885604858 1.2853994369506836 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.6052608Losses:  2.410417079925537 0.9092684984207153 1.4075406789779663 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.4104171Losses:  5.254944026470184 1.0707902908325195 2.51747989654541 1.5844137072563171
MemoryTrain:  epoch  5, batch     3 | loss: 5.2549440Losses:  2.6298999786376953 1.1240816116333008 1.383644700050354 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.6299000Losses:  2.4821853637695312 0.9910725951194763 1.3741600513458252 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.4821854Losses:  2.414358615875244 1.0865488052368164 1.2243292331695557 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.4143586Losses:  3.4379022419452667 1.0634217262268066 0.8149905204772949 1.5336211025714874
MemoryTrain:  epoch  6, batch     3 | loss: 3.4379022Losses:  2.297557830810547 0.988344669342041 1.2318143844604492 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.2975578Losses:  2.250483512878418 1.048255443572998 1.1275238990783691 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.2504835Losses:  2.675727367401123 1.1598029136657715 1.424863576889038 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.6757274Losses:  4.248018838465214 1.1684226989746094 1.5835771560668945 1.4231036677956581
MemoryTrain:  epoch  7, batch     3 | loss: 4.2480188Losses:  2.4750428199768066 1.1675419807434082 1.2234879732131958 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.4750428Losses:  2.4599452018737793 1.1065154075622559 1.2490721940994263 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.4599452Losses:  2.2765679359436035 0.9226876497268677 1.2845325469970703 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.2765679Losses:  3.4156855568289757 0.9614615440368652 0.9519963264465332 1.444169782102108
MemoryTrain:  epoch  8, batch     3 | loss: 3.4156856Losses:  2.2975058555603027 1.0817267894744873 1.1332917213439941 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.2975059Losses:  2.464540481567383 1.0762200355529785 1.294580101966858 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.4645405Losses:  2.4637985229492188 0.9589927196502686 1.4175889492034912 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.4637985Losses:  3.765781544148922 1.0609498023986816 1.1342463493347168 1.4476486667990685
MemoryTrain:  epoch  9, batch     3 | loss: 3.7657815
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 64.36%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.90%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 82.47%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 82.02%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.45%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 82.73%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 82.68%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 82.66%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 83.43%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 83.68%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 84.55%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 84.67%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 84.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.70%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 84.17%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 83.73%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 83.31%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 82.97%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 82.72%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 82.47%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 82.23%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 82.07%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 81.76%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 81.76%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 81.54%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 81.39%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 81.11%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 80.76%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 80.05%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 79.61%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 78.93%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 78.70%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 78.54%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 78.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.09%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 77.94%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 77.61%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 77.46%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 77.32%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 77.18%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.81%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 76.22%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 75.69%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 75.06%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 74.55%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 74.11%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.89%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 75.21%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 75.40%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 74.95%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 74.56%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 74.07%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 73.64%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 73.08%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 72.57%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 73.07%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 72.72%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 72.43%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 72.23%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.79%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 72.56%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 72.20%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.94%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 71.67%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 71.49%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 71.31%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.03%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 72.38%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 72.52%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 72.55%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 72.58%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 72.64%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 72.56%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 72.47%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 72.42%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 72.44%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 72.35%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 72.36%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 72.76%   [EVAL] batch:  195 | acc: 12.50%,  total acc: 72.45%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 72.14%   [EVAL] batch:  197 | acc: 6.25%,  total acc: 71.81%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 71.51%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 71.16%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 71.23%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.28%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 71.14%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.91%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 70.39%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 70.20%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 69.93%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 70.60%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  222 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  223 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 70.19%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 71.85%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.26%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 72.44%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 72.26%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 72.15%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 71.96%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 71.95%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 71.76%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 71.70%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 71.62%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 72.28%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 72.27%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  278 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 72.33%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 72.14%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 72.02%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 71.83%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 71.58%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.56%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 71.61%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 71.72%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:  294 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 71.85%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  297 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.10%   
cur_acc:  ['0.9484', '0.7500', '0.7143', '0.7173', '0.7490']
his_acc:  ['0.9484', '0.8420', '0.7816', '0.7492', '0.7310']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  8.50925350189209 1.2297476530075073 1.1821728944778442 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 8.5092535Losses:  9.926456414163113 1.2620247602462769 1.3162106275558472 1.5628699883818626
CurrentTrain: epoch  0, batch     1 | loss: 9.9264564Losses:  10.385244837030768 1.3617238998413086 1.4329497814178467 1.6837325002998114
CurrentTrain: epoch  0, batch     2 | loss: 10.3852448Losses:  11.44734794832766 1.1717982292175293 1.3289895057678223 1.7543566916137934
CurrentTrain: epoch  0, batch     3 | loss: 11.4473479Losses:  8.96017611026764 1.2233420610427856 1.2170218229293823 1.4420486688613892
CurrentTrain: epoch  1, batch     0 | loss: 8.9601761Losses:  9.47181549668312 1.245809555053711 1.2784620523452759 1.598424345254898
CurrentTrain: epoch  1, batch     1 | loss: 9.4718155Losses:  6.882065773010254 1.2310817241668701 1.3006904125213623 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 6.8820658Losses:  8.17118476331234 1.230849266052246 1.2167882919311523 1.5806076377630234
CurrentTrain: epoch  1, batch     3 | loss: 8.1711848Losses:  8.19899233803153 1.2004058361053467 1.1491374969482422 1.5899673365056515
CurrentTrain: epoch  2, batch     0 | loss: 8.1989923Losses:  6.840367317199707 1.1851978302001953 1.061668872833252 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 6.8403673Losses:  9.787223108112812 1.2329158782958984 1.1381995677947998 3.064306505024433
CurrentTrain: epoch  2, batch     2 | loss: 9.7872231Losses:  13.845058426260948 1.0 0.7514181137084961 8.994176849722862
CurrentTrain: epoch  2, batch     3 | loss: 13.8450584Losses:  11.544364491477609 1.1552919149398804 1.2784172296524048 4.994308987632394
CurrentTrain: epoch  3, batch     0 | loss: 11.5443645Losses:  8.05389991030097 1.1893846988677979 1.1298844814300537 1.8332587741315365
CurrentTrain: epoch  3, batch     1 | loss: 8.0538999Losses:  7.983165204524994 1.1416053771972656 1.0136337280273438 1.6886176466941833
CurrentTrain: epoch  3, batch     2 | loss: 7.9831652Losses:  9.16582340747118 1.1774730682373047 1.5716047286987305 1.4169554188847542
CurrentTrain: epoch  3, batch     3 | loss: 9.1658234Losses:  7.520675957202911 1.1883251667022705 1.1421384811401367 1.5323765873908997
CurrentTrain: epoch  4, batch     0 | loss: 7.5206760Losses:  6.722596168518066 1.1356337070465088 1.102250337600708 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 6.7225962Losses:  7.791133433580399 1.1825350522994995 1.0735732316970825 1.5072875320911407
CurrentTrain: epoch  4, batch     2 | loss: 7.7911334Losses:  5.391169458627701 1.2937297821044922 0.6866693496704102 1.4369377195835114
CurrentTrain: epoch  4, batch     3 | loss: 5.3911695Losses:  13.93896235525608 1.2510664463043213 1.0678212642669678 8.715143576264381
CurrentTrain: epoch  5, batch     0 | loss: 13.9389624Losses:  7.175266604870558 1.1052989959716797 0.7994846105575562 1.573608260601759
CurrentTrain: epoch  5, batch     1 | loss: 7.1752666Losses:  13.263587176799774 1.124253273010254 1.2769687175750732 6.451202571392059
CurrentTrain: epoch  5, batch     2 | loss: 13.2635872Losses:  5.692495029419661 1.0290684700012207 0.8945698738098145 1.5369612388312817
CurrentTrain: epoch  5, batch     3 | loss: 5.6924950Losses:  8.602718621492386 1.183310627937317 1.2118446826934814 3.0644629299640656
CurrentTrain: epoch  6, batch     0 | loss: 8.6027186Losses:  9.061241332441568 1.1010303497314453 1.1217557191848755 2.8641192354261875
CurrentTrain: epoch  6, batch     1 | loss: 9.0612413Losses:  7.101558089256287 1.1247079372406006 1.049356460571289 1.4157541990280151
CurrentTrain: epoch  6, batch     2 | loss: 7.1015581Losses:  7.035324245691299 1.1571273803710938 1.0339841842651367 1.4253536760807037
CurrentTrain: epoch  6, batch     3 | loss: 7.0353242Losses:  8.981788992881775 0.9906126260757446 1.042569875717163 2.8831523656845093
CurrentTrain: epoch  7, batch     0 | loss: 8.9817890Losses:  8.54655645787716 1.1864415407180786 1.2772648334503174 2.916795715689659
CurrentTrain: epoch  7, batch     1 | loss: 8.5465565Losses:  11.59962485730648 1.1572866439819336 1.1454493999481201 6.04690383374691
CurrentTrain: epoch  7, batch     2 | loss: 11.5996249Losses:  5.977501817047596 1.1801419258117676 1.1874709129333496 1.4383239224553108
CurrentTrain: epoch  7, batch     3 | loss: 5.9775018Losses:  7.084620773792267 1.031084656715393 1.1389389038085938 1.4051645398139954
CurrentTrain: epoch  8, batch     0 | loss: 7.0846208Losses:  8.469590663909912 1.0760517120361328 1.2329347133636475 2.8918862342834473
CurrentTrain: epoch  8, batch     1 | loss: 8.4695907Losses:  8.061939090490341 1.1363736391067505 0.9674394130706787 2.8731516301631927
CurrentTrain: epoch  8, batch     2 | loss: 8.0619391Losses:  6.227775543928146 1.341700553894043 1.2821788787841797 1.4063901603221893
CurrentTrain: epoch  8, batch     3 | loss: 6.2277755Losses:  8.244455482810736 1.1588134765625 1.2081270217895508 2.882564689964056
CurrentTrain: epoch  9, batch     0 | loss: 8.2444555Losses:  6.670815974473953 1.112335443496704 1.2138242721557617 1.4207015335559845
CurrentTrain: epoch  9, batch     1 | loss: 6.6708160Losses:  5.520472526550293 0.9883013963699341 1.13966965675354 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.5204725Losses:  10.124655932188034 1.0 1.1692914962768555 6.233559340238571
CurrentTrain: epoch  9, batch     3 | loss: 10.1246559
Losses:  3.5540034770965576 1.165166974067688 1.2165281772613525 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.5540035Losses:  2.7289364337921143 0.9579699039459229 1.3025598526000977 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.7289364Losses:  3.32706880569458 0.9367918968200684 1.4707828760147095 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 3.3270688Losses:  3.1226072311401367 1.271710753440857 1.3627656698226929 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 3.1226072Losses:  3.8956990242004395 1.030165433883667 1.4756006002426147 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.8956990Losses:  3.88124418258667 1.0062336921691895 1.3627618551254272 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.8812442Losses:  2.9308111667633057 1.1180540323257446 1.4408986568450928 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.9308112Losses:  3.022984504699707 1.0806915760040283 1.1864137649536133 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 3.0229845Losses:  2.9263741970062256 1.0974689722061157 1.3322029113769531 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.9263742Losses:  3.197077751159668 1.020569086074829 1.3179794549942017 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 3.1970778Losses:  2.8374338150024414 1.036340594291687 1.3272864818572998 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.8374338Losses:  3.155733108520508 1.0663596391677856 1.3418059349060059 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 3.1557331Losses:  2.55029559135437 0.8542472720146179 1.2767586708068848 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.5502956Losses:  2.308098793029785 0.9277015924453735 1.0424726009368896 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.3080988Losses:  3.070455551147461 1.2185969352722168 1.28551185131073 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 3.0704556Losses:  2.9671952724456787 1.2183648347854614 1.548039436340332 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.9671953Losses:  2.552877426147461 0.8987250924110413 1.4079147577285767 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.5528774Losses:  2.673177719116211 1.0618386268615723 1.1907258033752441 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.6731777Losses:  2.488429546356201 1.1274099349975586 1.2313604354858398 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.4884295Losses:  2.544116973876953 1.013643503189087 1.2394157648086548 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.5441170Losses:  2.60263729095459 1.1030337810516357 1.2044005393981934 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.6026373Losses:  2.292964458465576 1.0243585109710693 1.1606887578964233 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.2929645Losses:  2.410599708557129 0.9782117605209351 1.339673638343811 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.4105997Losses:  2.734724760055542 1.0544439554214478 1.4772611856460571 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.7347248Losses:  2.3409667015075684 0.994613528251648 1.1747910976409912 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.3409667Losses:  2.477586030960083 1.08818781375885 1.2763572931289673 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.4775860Losses:  2.537043571472168 0.9817668199539185 1.4579520225524902 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.5370436Losses:  2.8120598793029785 1.168140172958374 1.4909924268722534 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 2.8120599Losses:  2.4112720489501953 0.9519602656364441 1.2176368236541748 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.4112720Losses:  2.507689952850342 1.0134649276733398 1.421118974685669 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.5076900Losses:  2.686882257461548 1.1690913438796997 1.4506510496139526 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.6868823Losses:  2.2795562744140625 1.0011427402496338 1.121424913406372 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 2.2795563Losses:  2.5342512130737305 1.0246102809906006 1.403915524482727 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.5342512Losses:  2.3603992462158203 0.9906513690948486 1.2779295444488525 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.3603992Losses:  2.2987430095672607 1.0245558023452759 1.117255687713623 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.2987430Losses:  2.442312479019165 1.0582338571548462 1.332866907119751 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 2.4423125Losses:  2.647808313369751 1.1163963079452515 1.4113850593566895 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.6478083Losses:  2.194042205810547 1.0416966676712036 1.1040138006210327 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.1940422Losses:  2.4486560821533203 1.024841547012329 1.3547585010528564 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.4486561Losses:  2.1755075454711914 0.89594966173172 1.1467050313949585 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 2.1755075
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 72.03%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 68.87%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 67.21%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 66.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 66.27%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.91%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 81.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.92%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 81.47%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 79.58%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 79.00%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 78.57%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 78.69%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 78.73%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 78.58%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 78.44%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.05%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 79.75%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 79.44%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 78.81%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 78.45%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 78.09%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 77.81%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 77.62%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 77.44%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 77.18%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 76.76%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 76.82%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 76.65%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 76.33%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 75.97%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 76.03%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 75.88%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 75.33%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 74.93%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 74.48%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 74.17%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 74.05%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 73.76%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 73.65%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 73.36%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 73.20%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 73.04%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.94%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 72.61%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 72.05%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 71.56%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 70.97%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 70.44%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.03%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 71.18%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 71.12%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 70.54%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 70.13%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 69.68%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 69.28%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 68.27%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.02%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 68.66%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 68.40%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.22%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.05%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.84%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 68.46%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 68.14%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 67.94%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 67.55%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 67.68%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.40%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 67.43%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 67.10%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 66.74%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 65.64%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 65.87%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.99%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 66.86%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 66.52%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 65.85%   [EVAL] batch:  198 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 65.19%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.34%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 65.22%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 64.99%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 64.77%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 64.49%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 64.34%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 64.09%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.08%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 64.97%   [EVAL] batch:  220 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  222 | acc: 50.00%,  total acc: 64.85%   [EVAL] batch:  223 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 64.67%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.43%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 66.78%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 66.84%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.20%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 67.48%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 67.31%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 67.03%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 66.86%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 66.75%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 66.83%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.84%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 66.43%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 66.35%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 67.10%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 67.04%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 66.78%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 66.57%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 66.40%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.98%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 65.91%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 66.01%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 66.04%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.14%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 66.26%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 66.40%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 67.51%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 67.30%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 67.13%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 67.02%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 67.07%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 67.04%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 66.97%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.78%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 68.30%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 68.13%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 67.97%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 67.80%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 67.61%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 67.45%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 67.89%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 67.76%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 67.65%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 67.44%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 67.43%   
cur_acc:  ['0.9484', '0.7500', '0.7143', '0.7173', '0.7490', '0.6627']
his_acc:  ['0.9484', '0.8420', '0.7816', '0.7492', '0.7310', '0.6743']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  11.519009791314602 1.3302479982376099 1.511376976966858 2.4710208997130394
CurrentTrain: epoch  0, batch     0 | loss: 11.5190098Losses:  11.395896708127111 1.312673568725586 1.3764305114746094 2.2993562570773065
CurrentTrain: epoch  0, batch     1 | loss: 11.3958967Losses:  14.529674045741558 1.2060513496398926 1.1688990592956543 5.760910503566265
CurrentTrain: epoch  0, batch     2 | loss: 14.5296740Losses:  13.835517317056656 1.4601101875305176 1.5430026054382324 1.8059705793857574
CurrentTrain: epoch  0, batch     3 | loss: 13.8355173Losses:  9.690173238515854 1.3174834251403809 1.4013216495513916 1.443753331899643
CurrentTrain: epoch  1, batch     0 | loss: 9.6901732Losses:  8.339982032775879 1.3006970882415771 1.5011188983917236 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 8.3399820Losses:  9.398665834218264 1.186356782913208 1.2008733749389648 1.5738968141376972
CurrentTrain: epoch  1, batch     2 | loss: 9.3986658Losses:  8.717292070388794 1.5454597473144531 1.2835378646850586 1.417661428451538
CurrentTrain: epoch  1, batch     3 | loss: 8.7172921Losses:  10.6637943983078 1.2639994621276855 1.2598395347595215 3.4590953588485718
CurrentTrain: epoch  2, batch     0 | loss: 10.6637944Losses:  9.256184287369251 1.2555922269821167 1.268283486366272 1.4626285508275032
CurrentTrain: epoch  2, batch     1 | loss: 9.2561843Losses:  9.830112274736166 1.2915942668914795 1.3080333471298218 1.5178506933152676
CurrentTrain: epoch  2, batch     2 | loss: 9.8301123Losses:  10.184626936912537 1.3052072525024414 1.1945781707763672 1.4451268911361694
CurrentTrain: epoch  2, batch     3 | loss: 10.1846269Losses:  7.434596538543701 1.2662972211837769 1.4344897270202637 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.4345965Losses:  13.150929301977158 1.2924416065216064 1.3208318948745728 5.012396663427353
CurrentTrain: epoch  3, batch     1 | loss: 13.1509293Losses:  8.94112241640687 1.1843442916870117 1.4865210056304932 1.7070888318121433
CurrentTrain: epoch  3, batch     2 | loss: 8.9411224Losses:  11.589014388620853 1.58111572265625 1.33331298828125 1.5195468440651894
CurrentTrain: epoch  3, batch     3 | loss: 11.5890144Losses:  10.977284148335457 1.3424493074417114 1.5431933403015137 3.0942241698503494
CurrentTrain: epoch  4, batch     0 | loss: 10.9772841Losses:  8.388168081641197 1.1779485940933228 1.2401392459869385 1.6667754501104355
CurrentTrain: epoch  4, batch     1 | loss: 8.3881681Losses:  7.582153797149658 1.2105985879898071 1.3379297256469727 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 7.5821538Losses:  7.396674074232578 1.1776185035705566 1.26804780960083 1.5165757313370705
CurrentTrain: epoch  4, batch     3 | loss: 7.3966741Losses:  7.531911849975586 1.3128366470336914 1.5247161388397217 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.5319118Losses:  11.685138326138258 1.228642463684082 1.3249433040618896 4.656638722866774
CurrentTrain: epoch  5, batch     1 | loss: 11.6851383Losses:  6.643504619598389 1.1845512390136719 1.2022861242294312 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 6.6435046Losses:  8.71704400330782 1.0841145515441895 1.0292201042175293 1.454700119793415
CurrentTrain: epoch  5, batch     3 | loss: 8.7170440Losses:  10.026001781225204 1.1609889268875122 1.3916085958480835 3.1587394177913666
CurrentTrain: epoch  6, batch     0 | loss: 10.0260018Losses:  17.058320991694927 1.3719029426574707 1.4376351833343506 9.540404312312603
CurrentTrain: epoch  6, batch     1 | loss: 17.0583210Losses:  12.6294996291399 1.1770470142364502 1.2736527919769287 6.036636546254158
CurrentTrain: epoch  6, batch     2 | loss: 12.6294996Losses:  7.446648836135864 1.2987637519836426 0.893740177154541 1.412137746810913
CurrentTrain: epoch  6, batch     3 | loss: 7.4466488Losses:  9.394749581813812 1.178863286972046 1.2018545866012573 3.022704064846039
CurrentTrain: epoch  7, batch     0 | loss: 9.3947496Losses:  8.445727787911892 1.230780839920044 1.3747470378875732 1.5281223878264427
CurrentTrain: epoch  7, batch     1 | loss: 8.4457278Losses:  9.781300853937864 1.2811648845672607 1.5209031105041504 3.305022072046995
CurrentTrain: epoch  7, batch     2 | loss: 9.7813009Losses:  7.7113013081252575 0.9916229248046875 1.1700248718261719 1.6329273991286755
CurrentTrain: epoch  7, batch     3 | loss: 7.7113013Losses:  9.683052368462086 1.1222953796386719 1.3512380123138428 2.978857345879078
CurrentTrain: epoch  8, batch     0 | loss: 9.6830524Losses:  8.880537007004023 1.1522841453552246 1.1876819133758545 2.9224796034395695
CurrentTrain: epoch  8, batch     1 | loss: 8.8805370Losses:  12.736253902316093 1.2633448839187622 1.4506760835647583 6.638720199465752
CurrentTrain: epoch  8, batch     2 | loss: 12.7362539Losses:  7.532631304115057 1.5525059700012207 1.0849652290344238 1.555110838264227
CurrentTrain: epoch  8, batch     3 | loss: 7.5326313Losses:  6.166797161102295 1.15305495262146 1.3573966026306152 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 6.1667972Losses:  6.013925552368164 1.3081681728363037 1.434579849243164 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 6.0139256Losses:  7.829570710659027 1.1138354539871216 1.2884567975997925 1.4228295683860779
CurrentTrain: epoch  9, batch     2 | loss: 7.8295707Losses:  6.827449008822441 1.1915860176086426 1.639310359954834 1.4140502661466599
CurrentTrain: epoch  9, batch     3 | loss: 6.8274490
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  26.295831149443984 5.552451133728027 5.577536582946777 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 26.2958311Losses:  27.326138220727444 5.773777008056641 5.988034248352051 5.83479567617178
CurrentTrain: epoch  0, batch     1 | loss: 27.3261382Losses:  25.82458506897092 5.709115028381348 5.717672824859619 4.393385041505098
CurrentTrain: epoch  0, batch     2 | loss: 25.8245851Losses:  26.013113107532263 5.699300765991211 5.934866905212402 5.150165643543005
CurrentTrain: epoch  0, batch     3 | loss: 26.0131131Losses:  24.124083269387484 5.7388081550598145 5.783657550811768 2.500635851174593
CurrentTrain: epoch  0, batch     4 | loss: 24.1240833Losses:  24.461819987744093 5.643383502960205 5.676997184753418 4.399232249706984
CurrentTrain: epoch  0, batch     5 | loss: 24.4618200Losses:  29.427756506949663 5.71573543548584 5.753735542297363 8.278102118521929
CurrentTrain: epoch  0, batch     6 | loss: 29.4277565Losses:  25.41563129425049 5.85491418838501 5.907627582550049 3.8524255752563477
CurrentTrain: epoch  0, batch     7 | loss: 25.4156313Losses:  32.256224416196346 5.787766933441162 5.603268623352051 10.522452138364315
CurrentTrain: epoch  0, batch     8 | loss: 32.2562244Losses:  23.318131923675537 5.682732582092285 5.758840560913086 2.8937735557556152
CurrentTrain: epoch  0, batch     9 | loss: 23.3181319Losses:  24.497735865414143 5.716352462768555 5.765320777893066 3.8615244701504707
CurrentTrain: epoch  0, batch    10 | loss: 24.4977359Losses:  24.042199604213238 5.633352279663086 5.380321025848389 3.06090497225523
CurrentTrain: epoch  0, batch    11 | loss: 24.0421996Losses:  21.656970977783203 5.773733139038086 5.762779235839844 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 21.6569710Losses:  21.430418014526367 5.752556324005127 5.70031213760376 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 21.4304180Losses:  26.74371163547039 5.698381423950195 5.593071937561035 5.328859493136406
CurrentTrain: epoch  0, batch    14 | loss: 26.7437116Losses:  25.12891105748713 5.459867000579834 5.49002742767334 4.961224595084786
CurrentTrain: epoch  0, batch    15 | loss: 25.1289111Losses:  24.189104586839676 5.451009750366211 5.657116413116455 3.9713063538074493
CurrentTrain: epoch  0, batch    16 | loss: 24.1891046Losses:  24.4599085226655 5.690216064453125 5.5334954261779785 3.7726431265473366
CurrentTrain: epoch  0, batch    17 | loss: 24.4599085Losses:  25.999449133872986 5.610940933227539 5.369255065917969 5.050970435142517
CurrentTrain: epoch  0, batch    18 | loss: 25.9994491Losses:  21.983942598104477 5.629571437835693 5.409388065338135 1.433668702840805
CurrentTrain: epoch  0, batch    19 | loss: 21.9839426Losses:  23.945594184100628 5.645238399505615 5.702710151672363 4.266719214618206
CurrentTrain: epoch  0, batch    20 | loss: 23.9455942Losses:  24.598490113392472 5.56035041809082 5.270440101623535 5.049379700794816
CurrentTrain: epoch  0, batch    21 | loss: 24.5984901Losses:  28.968918588012457 5.63496732711792 5.633040428161621 8.693684365600348
CurrentTrain: epoch  0, batch    22 | loss: 28.9689186Losses:  22.22826224565506 5.621831893920898 5.606043815612793 1.4376052021980286
CurrentTrain: epoch  0, batch    23 | loss: 22.2282622Losses:  26.024903163313866 5.59221887588501 5.646692752838135 5.692736491560936
CurrentTrain: epoch  0, batch    24 | loss: 26.0249032Losses:  20.60581398010254 5.732980728149414 5.585779190063477 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 20.6058140Losses:  20.15067481994629 5.732672691345215 5.468155384063721 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 20.1506748Losses:  25.057598128914833 5.559077739715576 5.4994001388549805 4.798282638192177
CurrentTrain: epoch  0, batch    27 | loss: 25.0575981Losses:  24.393986023962498 5.700695037841797 5.427131652832031 4.622017182409763
CurrentTrain: epoch  0, batch    28 | loss: 24.3939860Losses:  20.196792602539062 5.715816974639893 5.499622344970703 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 20.1967926Losses:  18.995174407958984 5.650247573852539 5.46584415435791 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 18.9951744Losses:  23.84303878247738 5.4081854820251465 5.35552978515625 5.2139627784490585
CurrentTrain: epoch  0, batch    31 | loss: 23.8430388Losses:  19.244857788085938 5.551647186279297 5.332098960876465 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 19.2448578Losses:  19.81703081727028 5.5819292068481445 5.405648231506348 1.4027051031589508
CurrentTrain: epoch  0, batch    33 | loss: 19.8170308Losses:  23.2968072257936 5.713911056518555 5.38292121887207 3.294112142175436
CurrentTrain: epoch  0, batch    34 | loss: 23.2968072Losses:  23.417793717235327 5.721911430358887 5.4467644691467285 3.3322434090077877
CurrentTrain: epoch  0, batch    35 | loss: 23.4177937Losses:  21.153520487248898 5.425731182098389 5.427756309509277 1.8153628334403038
CurrentTrain: epoch  0, batch    36 | loss: 21.1535205Losses:  20.861665278673172 5.529860496520996 5.479353904724121 1.4247832596302032
CurrentTrain: epoch  0, batch    37 | loss: 20.8616653Losses:  19.134864807128906 5.780723571777344 5.464845180511475 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 19.1348648Losses:  25.146637150086462 5.711786270141602 5.191556930541992 5.063093372620642
CurrentTrain: epoch  0, batch    39 | loss: 25.1466372Losses:  26.818883776664734 5.651014804840088 5.450840950012207 7.33552348613739
CurrentTrain: epoch  0, batch    40 | loss: 26.8188838Losses:  25.93900203704834 5.540147304534912 5.268302917480469 5.807703971862793
CurrentTrain: epoch  0, batch    41 | loss: 25.9390020Losses:  23.550844732671976 5.616569519042969 5.519941806793213 3.69971901550889
CurrentTrain: epoch  0, batch    42 | loss: 23.5508447Losses:  25.543989837169647 5.53548526763916 5.392948150634766 6.511309325695038
CurrentTrain: epoch  0, batch    43 | loss: 25.5439898Losses:  21.04711152240634 5.618793487548828 5.288727760314941 1.7917289845645428
CurrentTrain: epoch  0, batch    44 | loss: 21.0471115Losses:  18.672290802001953 5.552741050720215 5.305824279785156 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 18.6722908Losses:  24.434392131865025 5.578027248382568 5.269269943237305 5.025927700102329
CurrentTrain: epoch  0, batch    46 | loss: 24.4343921Losses:  24.65628457069397 5.641127586364746 5.331110000610352 5.004886865615845
CurrentTrain: epoch  0, batch    47 | loss: 24.6562846Losses:  21.87220799457282 5.564120769500732 5.536374568939209 2.2399505330249667
CurrentTrain: epoch  0, batch    48 | loss: 21.8722080Losses:  21.22859076038003 5.6110334396362305 5.244853973388672 1.510569367557764
CurrentTrain: epoch  0, batch    49 | loss: 21.2285908Losses:  28.243446815758944 5.756098747253418 5.434864044189453 8.519627083092928
CurrentTrain: epoch  0, batch    50 | loss: 28.2434468Losses:  22.644511938095093 5.5316081047058105 5.330679893493652 4.652648687362671
CurrentTrain: epoch  0, batch    51 | loss: 22.6445119Losses:  20.216079711914062 5.756397247314453 5.410482406616211 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 20.2160797Losses:  24.3778870604001 5.391242980957031 5.277487277984619 5.386105825658888
CurrentTrain: epoch  0, batch    53 | loss: 24.3778871Losses:  25.434512093663216 5.4513702392578125 5.2123122215271 6.966952279210091
CurrentTrain: epoch  0, batch    54 | loss: 25.4345121Losses:  20.017189882695675 5.675829887390137 5.352734565734863 1.544033907353878
CurrentTrain: epoch  0, batch    55 | loss: 20.0171899Losses:  31.201855801045895 5.45127010345459 5.214926719665527 12.071961544454098
CurrentTrain: epoch  0, batch    56 | loss: 31.2018558Losses:  21.368438113480806 5.531442165374756 5.354725360870361 2.9867528565227985
CurrentTrain: epoch  0, batch    57 | loss: 21.3684381Losses:  22.72396158427 5.628513813018799 5.505655288696289 3.696285955607891
CurrentTrain: epoch  0, batch    58 | loss: 22.7239616Losses:  20.641201362013817 5.5186638832092285 5.413496971130371 1.535618171095848
CurrentTrain: epoch  0, batch    59 | loss: 20.6412014Losses:  34.30265659838915 5.5921220779418945 5.388520240783691 15.955671735107899
CurrentTrain: epoch  0, batch    60 | loss: 34.3026566Losses:  22.713333029299974 5.425076484680176 5.382452011108398 4.653701681643724
CurrentTrain: epoch  0, batch    61 | loss: 22.7133330Losses:  20.734137572348118 5.399328231811523 5.530998229980469 1.5951757803559303
CurrentTrain: epoch  0, batch    62 | loss: 20.7341376Losses:  19.046585083007812 5.637326240539551 5.205970287322998 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 19.0465851Losses:  22.51432025898248 5.521227836608887 5.475826263427734 4.10847080219537
CurrentTrain: epoch  1, batch     1 | loss: 22.5143203Losses:  24.761809077113867 5.6101226806640625 5.21013069152832 6.628851618617773
CurrentTrain: epoch  1, batch     2 | loss: 24.7618091Losses:  20.00832089036703 5.575511932373047 5.413800239562988 1.4594813212752342
CurrentTrain: epoch  1, batch     3 | loss: 20.0083209Losses:  21.002893332391977 5.670692443847656 5.5307393074035645 1.8015707768499851
CurrentTrain: epoch  1, batch     4 | loss: 21.0028933Losses:  18.917140305042267 5.427140235900879 5.381628513336182 1.4014561772346497
CurrentTrain: epoch  1, batch     5 | loss: 18.9171403Losses:  18.527057647705078 5.4440178871154785 5.242513179779053 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 18.5270576Losses:  20.22980146482587 5.435579299926758 5.197584629058838 1.7657816894352436
CurrentTrain: epoch  1, batch     7 | loss: 20.2298015Losses:  19.900623643770814 5.607125282287598 5.3390655517578125 1.568571412935853
CurrentTrain: epoch  1, batch     8 | loss: 19.9006236Losses:  24.507733024656773 5.612924098968506 5.332627296447754 6.544907249510288
CurrentTrain: epoch  1, batch     9 | loss: 24.5077330Losses:  24.153099060058594 5.51400089263916 5.119811534881592 5.252107620239258
CurrentTrain: epoch  1, batch    10 | loss: 24.1530991Losses:  21.576326850801706 5.451921463012695 5.495880603790283 3.204044822603464
CurrentTrain: epoch  1, batch    11 | loss: 21.5763269Losses:  20.837097704410553 5.358633995056152 5.222577095031738 2.8499074578285217
CurrentTrain: epoch  1, batch    12 | loss: 20.8370977Losses:  17.96335220336914 5.538346290588379 5.364439010620117 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 17.9633522Losses:  22.55413233116269 5.546532154083252 5.106532573699951 3.6424978859722614
CurrentTrain: epoch  1, batch    14 | loss: 22.5541323Losses:  18.317546844482422 5.490772247314453 5.252695560455322 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 18.3175468Losses:  22.245784640312195 5.437835693359375 5.364781856536865 3.102788805961609
CurrentTrain: epoch  1, batch    16 | loss: 22.2457846Losses:  23.396879702806473 5.46230411529541 5.158082962036133 4.417082339525223
CurrentTrain: epoch  1, batch    17 | loss: 23.3968797Losses:  24.024850700050592 5.347072601318359 5.401890754699707 6.109487388283014
CurrentTrain: epoch  1, batch    18 | loss: 24.0248507Losses:  18.325435638427734 5.471199035644531 5.348661422729492 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 18.3254356Losses:  18.014389038085938 5.303778171539307 5.386845588684082 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 18.0143890Losses:  28.798466600477695 5.503480434417725 5.189013481140137 9.91284266859293
CurrentTrain: epoch  1, batch    21 | loss: 28.7984666Losses:  18.50099754333496 5.42565393447876 5.348735809326172 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 18.5009975Losses:  17.758047103881836 5.479961395263672 5.329448699951172 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 17.7580471Losses:  21.225471686571836 5.369996070861816 5.367660045623779 4.391891669481993
CurrentTrain: epoch  1, batch    24 | loss: 21.2254717Losses:  19.923953734338284 5.384748458862305 5.471780776977539 2.8801610097289085
CurrentTrain: epoch  1, batch    25 | loss: 19.9239537Losses:  18.566055297851562 5.465433120727539 5.235342979431152 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 18.5660553Losses:  17.941146850585938 5.390079975128174 5.247476577758789 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 17.9411469Losses:  18.253952026367188 5.426848411560059 5.263689994812012 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 18.2539520Losses:  22.208878606557846 5.275602340698242 5.426275253295898 4.347575277090073
CurrentTrain: epoch  1, batch    29 | loss: 22.2088786Losses:  22.164458069950342 5.4775071144104 5.181809902191162 3.695832047611475
CurrentTrain: epoch  1, batch    30 | loss: 22.1644581Losses:  22.050805632025003 5.574972629547119 5.34406852722168 3.2748447097837925
CurrentTrain: epoch  1, batch    31 | loss: 22.0508056Losses:  22.223192140460014 5.436967849731445 5.241695404052734 3.962289735674858
CurrentTrain: epoch  1, batch    32 | loss: 22.2231921Losses:  17.402008056640625 5.403816223144531 5.281740188598633 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 17.4020081Losses:  17.36163330078125 5.376807689666748 5.437347888946533 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 17.3616333Losses:  25.169995740056038 5.6436238288879395 5.293937683105469 6.460309460759163
CurrentTrain: epoch  1, batch    35 | loss: 25.1699957Losses:  20.020543217658997 5.440640449523926 5.311237812042236 1.6359511613845825
CurrentTrain: epoch  1, batch    36 | loss: 20.0205432Losses:  18.505042612552643 5.339130878448486 5.209549427032471 1.6209331154823303
CurrentTrain: epoch  1, batch    37 | loss: 18.5050426Losses:  20.18530972674489 5.379394054412842 5.433732032775879 2.9878152199089527
CurrentTrain: epoch  1, batch    38 | loss: 20.1853097Losses:  21.83007974177599 5.385416030883789 5.361302375793457 4.702514357864857
CurrentTrain: epoch  1, batch    39 | loss: 21.8300797Losses:  18.970617651939392 5.356141567230225 5.359844207763672 1.4717353582382202
CurrentTrain: epoch  1, batch    40 | loss: 18.9706177Losses:  20.247880771756172 5.416729927062988 5.195545196533203 2.9651125222444534
CurrentTrain: epoch  1, batch    41 | loss: 20.2478808Losses:  19.549478117376566 5.495996475219727 5.219600677490234 1.8427024520933628
CurrentTrain: epoch  1, batch    42 | loss: 19.5494781Losses:  22.272227846086025 5.457559585571289 5.3804850578308105 4.493407808244228
CurrentTrain: epoch  1, batch    43 | loss: 22.2722278Losses:  22.637161180377007 5.489012718200684 5.320102691650391 3.908130571246147
CurrentTrain: epoch  1, batch    44 | loss: 22.6371612Losses:  21.281683292239904 5.606924533843994 5.25054931640625 3.004484500735998
CurrentTrain: epoch  1, batch    45 | loss: 21.2816833Losses:  18.585603713989258 5.653334617614746 5.266538143157959 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 18.5856037Losses:  17.77825164794922 5.497716903686523 5.153829574584961 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 17.7782516Losses:  19.436943113803864 5.365818977355957 5.224732398986816 1.4080448746681213
CurrentTrain: epoch  1, batch    48 | loss: 19.4369431Losses:  27.575992688536644 5.547589302062988 5.108547210693359 9.38520060479641
CurrentTrain: epoch  1, batch    49 | loss: 27.5759927Losses:  19.3438055254519 5.369233131408691 5.1324615478515625 1.442526075989008
CurrentTrain: epoch  1, batch    50 | loss: 19.3438055Losses:  18.010530471801758 5.4834394454956055 5.322135925292969 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 18.0105305Losses:  24.043180719017982 5.3783979415893555 5.212623596191406 6.484956994652748
CurrentTrain: epoch  1, batch    52 | loss: 24.0431807Losses:  17.070531845092773 5.4771504402160645 5.298563003540039 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 17.0705318Losses:  20.406806737184525 5.233955383300781 5.26004695892334 3.3335645496845245
CurrentTrain: epoch  1, batch    54 | loss: 20.4068067Losses:  19.9874828197062 5.176327705383301 5.371237277984619 2.908327851444483
CurrentTrain: epoch  1, batch    55 | loss: 19.9874828Losses:  17.727352142333984 5.459897994995117 5.1855363845825195 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 17.7273521Losses:  18.014217376708984 5.361732482910156 5.251782417297363 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 18.0142174Losses:  17.093822479248047 5.315698623657227 5.288261890411377 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 17.0938225Losses:  21.47986862435937 5.488048076629639 5.274598121643066 3.261671755462885
CurrentTrain: epoch  1, batch    59 | loss: 21.4798686Losses:  17.34756088256836 5.424371719360352 5.292503356933594 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 17.3475609Losses:  18.89907455444336 5.515612602233887 5.1784257888793945 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 18.8990746Losses:  20.029942777007818 5.392756462097168 5.305724143981934 4.379192616790533
CurrentTrain: epoch  1, batch    62 | loss: 20.0299428Losses:  16.241804122924805 5.307209491729736 5.344945907592773 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 16.2418041Losses:  16.955448150634766 5.374355316162109 5.244407653808594 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 16.9554482Losses:  17.886237233877182 5.409881114959717 5.241757392883301 1.400244802236557
CurrentTrain: epoch  2, batch     2 | loss: 17.8862372Losses:  22.161180272698402 5.347236156463623 5.34572696685791 3.08084274828434
CurrentTrain: epoch  2, batch     3 | loss: 22.1611803Losses:  19.806151539087296 5.512186050415039 5.25461483001709 1.4693023264408112
CurrentTrain: epoch  2, batch     4 | loss: 19.8061515Losses:  19.77899506688118 5.403420448303223 5.141266822814941 2.158908396959305
CurrentTrain: epoch  2, batch     5 | loss: 19.7789951Losses:  17.376996994018555 5.489020347595215 5.297950744628906 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 17.3769970Losses:  22.236913304775953 5.397207260131836 5.183067321777344 4.956483464688063
CurrentTrain: epoch  2, batch     7 | loss: 22.2369133Losses:  20.381366258487105 5.276504039764404 5.204941272735596 3.422822481021285
CurrentTrain: epoch  2, batch     8 | loss: 20.3813663Losses:  21.510475151240826 5.4790358543396 5.138552665710449 3.8233528062701225
CurrentTrain: epoch  2, batch     9 | loss: 21.5104752Losses:  18.5390431471169 5.295722007751465 5.235616683959961 1.4969517774879932
CurrentTrain: epoch  2, batch    10 | loss: 18.5390431Losses:  17.903403278440237 5.2370405197143555 5.195417881011963 1.529025074094534
CurrentTrain: epoch  2, batch    11 | loss: 17.9034033Losses:  23.429868884384632 5.308959484100342 5.391499042510986 5.9255315735936165
CurrentTrain: epoch  2, batch    12 | loss: 23.4298689Losses:  18.778092060238123 5.3561553955078125 5.256069660186768 1.5001761056482792
CurrentTrain: epoch  2, batch    13 | loss: 18.7780921Losses:  20.946319580078125 5.361407279968262 5.357029914855957 2.880115509033203
CurrentTrain: epoch  2, batch    14 | loss: 20.9463196Losses:  19.337325667962432 5.345302104949951 5.089252471923828 2.3420177455991507
CurrentTrain: epoch  2, batch    15 | loss: 19.3373257Losses:  20.185302257537842 5.479521751403809 5.06422233581543 2.8534045219421387
CurrentTrain: epoch  2, batch    16 | loss: 20.1853023Losses:  20.09555545821786 5.361395359039307 5.270051956176758 2.9015628434717655
CurrentTrain: epoch  2, batch    17 | loss: 20.0955555Losses:  27.6643800213933 5.354645729064941 5.365786075592041 11.392472214996815
CurrentTrain: epoch  2, batch    18 | loss: 27.6643800Losses:  19.464409802109003 5.427038669586182 5.1437907218933105 1.860405895859003
CurrentTrain: epoch  2, batch    19 | loss: 19.4644098Losses:  16.45968246459961 5.223680019378662 5.273834705352783 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 16.4596825Losses:  17.212839126586914 5.437440395355225 5.183109283447266 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 17.2128391Losses:  21.265688754618168 5.343988418579102 5.097646713256836 4.359598971903324
CurrentTrain: epoch  2, batch    22 | loss: 21.2656888Losses:  19.120995491743088 5.242597579956055 5.135181427001953 1.6584405601024628
CurrentTrain: epoch  2, batch    23 | loss: 19.1209955Losses:  17.56975032016635 5.207710266113281 5.278299331665039 1.5168080441653728
CurrentTrain: epoch  2, batch    24 | loss: 17.5697503Losses:  22.011198427528143 5.420224189758301 5.332494735717773 4.844955828040838
CurrentTrain: epoch  2, batch    25 | loss: 22.0111984Losses:  18.459655545651913 5.413177490234375 5.054408073425293 1.4194314703345299
CurrentTrain: epoch  2, batch    26 | loss: 18.4596555Losses:  18.54219687730074 5.2913594245910645 5.1681904792785645 1.7939783409237862
CurrentTrain: epoch  2, batch    27 | loss: 18.5421969Losses:  18.08015176653862 5.304201602935791 5.1812744140625 1.4185974299907684
CurrentTrain: epoch  2, batch    28 | loss: 18.0801518Losses:  28.709794644266367 5.323486804962158 5.209987163543701 12.035594586282969
CurrentTrain: epoch  2, batch    29 | loss: 28.7097946Losses:  18.495026651769876 5.346466064453125 5.327976226806641 1.4903250373899937
CurrentTrain: epoch  2, batch    30 | loss: 18.4950267Losses:  17.81776098534465 5.329347610473633 5.149997711181641 1.4861741475760937
CurrentTrain: epoch  2, batch    31 | loss: 17.8177610Losses:  19.28135697171092 5.290614128112793 5.144305229187012 2.8996507339179516
CurrentTrain: epoch  2, batch    32 | loss: 19.2813570Losses:  19.94796834886074 5.347299575805664 5.204357147216797 2.2206772416830063
CurrentTrain: epoch  2, batch    33 | loss: 19.9479683Losses:  22.393118929117918 5.396167755126953 5.054317474365234 5.782282900065184
CurrentTrain: epoch  2, batch    34 | loss: 22.3931189Losses:  23.47112387418747 5.347405433654785 5.007455825805664 5.895875155925751
CurrentTrain: epoch  2, batch    35 | loss: 23.4711239Losses:  19.627717785537243 5.154355525970459 5.09761905670166 2.8933351561427116
CurrentTrain: epoch  2, batch    36 | loss: 19.6277178Losses:  19.23002015426755 5.403660297393799 5.162891387939453 1.8042999394237995
CurrentTrain: epoch  2, batch    37 | loss: 19.2300202Losses:  18.422229766845703 5.494789123535156 5.276238441467285 1.4073829650878906
CurrentTrain: epoch  2, batch    38 | loss: 18.4222298Losses:  20.808769654482603 5.278177261352539 5.2044878005981445 3.3665356151759624
CurrentTrain: epoch  2, batch    39 | loss: 20.8087697Losses:  20.887944754213095 5.2716169357299805 5.23802375793457 4.49887615069747
CurrentTrain: epoch  2, batch    40 | loss: 20.8879448Losses:  19.63236089423299 5.384346961975098 5.211642742156982 1.4929565973579884
CurrentTrain: epoch  2, batch    41 | loss: 19.6323609Losses:  17.848953127861023 5.211207389831543 5.129116535186768 1.4021490812301636
CurrentTrain: epoch  2, batch    42 | loss: 17.8489531Losses:  17.212238311767578 5.276491165161133 5.253623008728027 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 17.2122383Losses:  17.440722469240427 5.1092424392700195 5.136054992675781 1.471047405153513
CurrentTrain: epoch  2, batch    44 | loss: 17.4407225Losses:  20.32085257768631 5.4342498779296875 5.193173885345459 3.2931845784187317
CurrentTrain: epoch  2, batch    45 | loss: 20.3208526Losses:  16.46685218811035 5.2015533447265625 5.264606475830078 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 16.4668522Losses:  17.041641235351562 5.329573631286621 5.232102394104004 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 17.0416412Losses:  16.185672760009766 5.263467788696289 5.216752052307129 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 16.1856728Losses:  18.950448781251907 5.351485252380371 5.283409118652344 1.4285257160663605
CurrentTrain: epoch  2, batch    49 | loss: 18.9504488Losses:  26.260376684367657 5.436637878417969 5.232850074768066 9.147011511027813
CurrentTrain: epoch  2, batch    50 | loss: 26.2603767Losses:  19.89236385934055 5.2863664627075195 5.334325790405273 3.5902074109762907
CurrentTrain: epoch  2, batch    51 | loss: 19.8923639Losses:  17.986616602167487 5.271401405334473 5.208682537078857 1.733860483393073
CurrentTrain: epoch  2, batch    52 | loss: 17.9866166Losses:  19.199774891138077 5.177624225616455 5.241313457489014 2.8429748117923737
CurrentTrain: epoch  2, batch    53 | loss: 19.1997749Losses:  18.175187796354294 5.317695617675781 5.066651344299316 1.40239879488945
CurrentTrain: epoch  2, batch    54 | loss: 18.1751878Losses:  18.935232531279325 5.359659194946289 5.0451459884643555 2.919836413115263
CurrentTrain: epoch  2, batch    55 | loss: 18.9352325Losses:  18.161713430657983 5.205834865570068 5.32223653793335 1.805418798699975
CurrentTrain: epoch  2, batch    56 | loss: 18.1617134Losses:  17.080394744873047 5.35165548324585 5.090386390686035 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 17.0803947Losses:  16.785425186157227 5.373608589172363 5.204540252685547 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 16.7854252Losses:  27.75791859999299 5.466653823852539 5.199738502502441 11.291701558977365
CurrentTrain: epoch  2, batch    59 | loss: 27.7579186Losses:  16.728424072265625 5.3253936767578125 5.158073425292969 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 16.7284241Losses:  19.67084338143468 5.318363189697266 5.240440368652344 2.043302793055773
CurrentTrain: epoch  2, batch    61 | loss: 19.6708434Losses:  16.372486114501953 5.434971809387207 5.179736137390137 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 16.3724861Losses:  17.68932429328561 5.348323822021484 5.265475749969482 1.4281910993158817
CurrentTrain: epoch  3, batch     0 | loss: 17.6893243Losses:  17.57157090306282 5.144384384155273 5.228306293487549 1.422526866197586
CurrentTrain: epoch  3, batch     1 | loss: 17.5715709Losses:  16.757049560546875 5.321795463562012 5.244122505187988 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 16.7570496Losses:  20.88582431524992 5.356788158416748 5.156396389007568 4.3268376514315605
CurrentTrain: epoch  3, batch     3 | loss: 20.8858243Losses:  18.78242701664567 5.162282943725586 5.133498191833496 3.2748691476881504
CurrentTrain: epoch  3, batch     4 | loss: 18.7824270Losses:  21.259453158825636 5.343151569366455 5.199073791503906 4.316330295056105
CurrentTrain: epoch  3, batch     5 | loss: 21.2594532Losses:  17.700324147939682 5.417462348937988 4.957487106323242 1.4045744836330414
CurrentTrain: epoch  3, batch     6 | loss: 17.7003241Losses:  20.63973856717348 5.266105651855469 5.136778831481934 4.252090938389301
CurrentTrain: epoch  3, batch     7 | loss: 20.6397386Losses:  17.49279185011983 5.216857433319092 5.148833274841309 1.4538170881569386
CurrentTrain: epoch  3, batch     8 | loss: 17.4927919Losses:  16.718990325927734 5.256031036376953 5.1182990074157715 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 16.7189903Losses:  16.407615661621094 5.335633277893066 5.126458168029785 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 16.4076157Losses:  19.694011874496937 5.507706642150879 5.235573768615723 3.0316736176609993
CurrentTrain: epoch  3, batch    11 | loss: 19.6940119Losses:  20.435522884130478 5.143451690673828 5.419188499450684 4.269570201635361
CurrentTrain: epoch  3, batch    12 | loss: 20.4355229Losses:  20.556402925401926 5.358684062957764 5.279425144195557 2.850287202745676
CurrentTrain: epoch  3, batch    13 | loss: 20.5564029Losses:  27.21960039064288 5.221513748168945 5.236417293548584 10.377034854143858
CurrentTrain: epoch  3, batch    14 | loss: 27.2196004Losses:  18.08198767527938 5.229564189910889 5.131974220275879 1.4133723340928555
CurrentTrain: epoch  3, batch    15 | loss: 18.0819877Losses:  22.911429814994335 5.362617492675781 5.140401840209961 6.083716802299023
CurrentTrain: epoch  3, batch    16 | loss: 22.9114298Losses:  17.46831253170967 5.126251220703125 5.384149551391602 1.4556668102741241
CurrentTrain: epoch  3, batch    17 | loss: 17.4683125Losses:  20.447808280587196 5.279717445373535 5.1634931564331055 4.387253776192665
CurrentTrain: epoch  3, batch    18 | loss: 20.4478083Losses:  16.81503078341484 5.054156303405762 5.27911901473999 1.4798676669597626
CurrentTrain: epoch  3, batch    19 | loss: 16.8150308Losses:  19.45488800853491 5.08632755279541 5.0798420906066895 4.238758705556393
CurrentTrain: epoch  3, batch    20 | loss: 19.4548880Losses:  19.336156629025936 5.330304145812988 5.11376953125 3.2754228338599205
CurrentTrain: epoch  3, batch    21 | loss: 19.3361566Losses:  23.003574814647436 5.427705764770508 5.0563249588012695 6.219683136790991
CurrentTrain: epoch  3, batch    22 | loss: 23.0035748Losses:  18.13769644498825 5.308843612670898 5.158483028411865 1.4163276553153992
CurrentTrain: epoch  3, batch    23 | loss: 18.1376964Losses:  18.08008385077119 5.268151760101318 5.118577003479004 1.4413356818258762
CurrentTrain: epoch  3, batch    24 | loss: 18.0800839Losses:  19.67174294963479 5.230184555053711 5.143521308898926 2.9342131949961185
CurrentTrain: epoch  3, batch    25 | loss: 19.6717429Losses:  18.59646164253354 5.175312519073486 5.407525062561035 2.9167007096111774
CurrentTrain: epoch  3, batch    26 | loss: 18.5964616Losses:  16.907873202115297 5.264474868774414 5.115185737609863 1.4458513744175434
CurrentTrain: epoch  3, batch    27 | loss: 16.9078732Losses:  21.445478066802025 5.124273300170898 5.079588413238525 4.550422295928001
CurrentTrain: epoch  3, batch    28 | loss: 21.4454781Losses:  21.841496288776398 5.350549221038818 5.214516639709473 5.815712749958038
CurrentTrain: epoch  3, batch    29 | loss: 21.8414963Losses:  15.911096572875977 5.209770679473877 5.21360969543457 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 15.9110966Losses:  15.621390342712402 5.161965370178223 5.06578254699707 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 15.6213903Losses:  21.38573146983981 5.165505409240723 5.0495147705078125 5.745378267019987
CurrentTrain: epoch  3, batch    32 | loss: 21.3857315Losses:  18.986797481775284 5.052318572998047 5.22444486618042 2.902921825647354
CurrentTrain: epoch  3, batch    33 | loss: 18.9867975Losses:  16.34128189086914 5.134944438934326 5.20218563079834 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 16.3412819Losses:  18.179461892694235 5.098067760467529 5.208240509033203 1.5171808563172817
CurrentTrain: epoch  3, batch    35 | loss: 18.1794619Losses:  15.803770065307617 5.338203430175781 5.193192005157471 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 15.8037701Losses:  16.26563262939453 5.224936485290527 5.4340386390686035 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 16.2656326Losses:  18.614092346280813 5.013522148132324 5.300691604614258 2.815258499234915
CurrentTrain: epoch  3, batch    38 | loss: 18.6140923Losses:  20.175398409366608 4.996049880981445 5.353497505187988 4.254505693912506
CurrentTrain: epoch  3, batch    39 | loss: 20.1753984Losses:  25.709304112941027 5.251200199127197 5.178333282470703 7.819018620997667
CurrentTrain: epoch  3, batch    40 | loss: 25.7093041Losses:  16.89303970336914 5.350711822509766 5.189847469329834 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 16.8930397Losses:  16.568936049938202 5.2033209800720215 4.963346004486084 1.3960596919059753
CurrentTrain: epoch  3, batch    42 | loss: 16.5689360Losses:  17.686309278011322 4.893851280212402 5.493156433105469 1.7844824194908142
CurrentTrain: epoch  3, batch    43 | loss: 17.6863093Losses:  19.99223130568862 5.380283355712891 5.334653854370117 3.160398419946432
CurrentTrain: epoch  3, batch    44 | loss: 19.9922313Losses:  18.708364695310593 5.230129241943359 5.074625015258789 2.837172716856003
CurrentTrain: epoch  3, batch    45 | loss: 18.7083647Losses:  20.005107751116157 5.275084018707275 5.133607387542725 3.2910898830741644
CurrentTrain: epoch  3, batch    46 | loss: 20.0051078Losses:  18.644242584705353 5.166856288909912 5.463075637817383 2.8138983845710754
CurrentTrain: epoch  3, batch    47 | loss: 18.6442426Losses:  16.038673400878906 5.264732360839844 5.21788215637207 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 16.0386734Losses:  26.79378865286708 5.194950580596924 5.259383678436279 10.210965853184462
CurrentTrain: epoch  3, batch    49 | loss: 26.7937887Losses:  16.625022888183594 5.274033069610596 5.349295616149902 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 16.6250229Losses:  17.708988659083843 5.318090438842773 5.086065292358398 1.4335865899920464
CurrentTrain: epoch  3, batch    51 | loss: 17.7089887Losses:  19.72334686294198 4.923600673675537 5.2707719802856445 4.340596351772547
CurrentTrain: epoch  3, batch    52 | loss: 19.7233469Losses:  21.17711742967367 5.157378673553467 5.279353618621826 4.330840192735195
CurrentTrain: epoch  3, batch    53 | loss: 21.1771174Losses:  16.732294857501984 5.02169132232666 5.286705017089844 1.3969600796699524
CurrentTrain: epoch  3, batch    54 | loss: 16.7322949Losses:  19.077459007501602 5.428426742553711 5.123072624206543 2.833158165216446
CurrentTrain: epoch  3, batch    55 | loss: 19.0774590Losses:  15.369953155517578 5.005640983581543 5.148845195770264 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 15.3699532Losses:  15.895125389099121 5.044430732727051 5.143926620483398 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 15.8951254Losses:  20.895997114479542 5.174352645874023 5.082764625549316 5.362814016640186
CurrentTrain: epoch  3, batch    58 | loss: 20.8959971Losses:  26.40656642615795 5.257021903991699 5.081465721130371 9.969737812876701
CurrentTrain: epoch  3, batch    59 | loss: 26.4065664Losses:  16.604389190673828 5.176606178283691 5.34153938293457 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 16.6043892Losses:  21.13675481081009 5.362156867980957 5.181612968444824 4.561925709247589
CurrentTrain: epoch  3, batch    61 | loss: 21.1367548Losses:  17.02771684527397 5.260756492614746 4.946637153625488 1.407985895872116
CurrentTrain: epoch  3, batch    62 | loss: 17.0277168Losses:  20.416348554193974 5.193398952484131 5.172407150268555 4.339490033686161
CurrentTrain: epoch  4, batch     0 | loss: 20.4163486Losses:  20.79998893290758 4.833695411682129 5.372928619384766 5.0337163880467415
CurrentTrain: epoch  4, batch     1 | loss: 20.7999889Losses:  15.255331993103027 4.998951435089111 5.292721748352051 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 15.2553320Losses:  19.367483649402857 5.217185974121094 5.171258926391602 4.256465468555689
CurrentTrain: epoch  4, batch     3 | loss: 19.3674836Losses:  24.643646359443665 5.075259208679199 5.380681037902832 8.6731196641922
CurrentTrain: epoch  4, batch     4 | loss: 24.6436464Losses:  17.133054792881012 5.07339334487915 5.338986873626709 1.4093170762062073
CurrentTrain: epoch  4, batch     5 | loss: 17.1330548Losses:  16.98957223445177 4.944158554077148 5.315508842468262 1.5228239968419075
CurrentTrain: epoch  4, batch     6 | loss: 16.9895722Losses:  14.93248176574707 5.068505764007568 5.2097487449646 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 14.9324818Losses:  19.22577514871955 5.1356425285339355 5.2808356285095215 2.858293917030096
CurrentTrain: epoch  4, batch     8 | loss: 19.2257751Losses:  18.003863483667374 5.20367956161499 5.142491340637207 1.5418864786624908
CurrentTrain: epoch  4, batch     9 | loss: 18.0038635Losses:  20.27574697509408 5.031511306762695 5.310123443603516 4.414395961910486
CurrentTrain: epoch  4, batch    10 | loss: 20.2757470Losses:  19.05950590968132 5.016879081726074 5.497463226318359 2.87055441737175
CurrentTrain: epoch  4, batch    11 | loss: 19.0595059Losses:  17.535822987556458 5.248073577880859 5.155983924865723 1.4348212480545044
CurrentTrain: epoch  4, batch    12 | loss: 17.5358230Losses:  15.556550979614258 5.023684501647949 5.216841220855713 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 15.5565510Losses:  17.435262270271778 5.200964450836182 5.016000270843506 1.4735408499836922
CurrentTrain: epoch  4, batch    14 | loss: 17.4352623Losses:  17.04546895623207 5.005002021789551 5.072090148925781 1.3958899080753326
CurrentTrain: epoch  4, batch    15 | loss: 17.0454690Losses:  19.665556339547038 5.089081764221191 5.351334571838379 3.19642105512321
CurrentTrain: epoch  4, batch    16 | loss: 19.6655563Losses:  15.61760139465332 5.148593902587891 5.254355430603027 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 15.6176014Losses:  20.27616073191166 5.373709201812744 5.094297885894775 4.346082225441933
CurrentTrain: epoch  4, batch    18 | loss: 20.2761607Losses:  24.358475849032402 5.2213053703308105 4.923069953918457 8.726192638278008
CurrentTrain: epoch  4, batch    19 | loss: 24.3584758Losses:  19.29777431860566 5.207918167114258 5.266883850097656 2.8516988791525364
CurrentTrain: epoch  4, batch    20 | loss: 19.2977743Losses:  18.7109260186553 4.799086570739746 5.490521430969238 2.8860740289092064
CurrentTrain: epoch  4, batch    21 | loss: 18.7109260Losses:  17.229624327272177 5.320796966552734 5.211569786071777 1.4715314470231533
CurrentTrain: epoch  4, batch    22 | loss: 17.2296243Losses:  22.48609086498618 5.147939682006836 5.045079708099365 7.13273736461997
CurrentTrain: epoch  4, batch    23 | loss: 22.4860909Losses:  16.83821225911379 5.054572105407715 5.159311294555664 1.4821150377392769
CurrentTrain: epoch  4, batch    24 | loss: 16.8382123Losses:  16.38271141052246 5.211368560791016 5.024786472320557 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 16.3827114Losses:  16.569724705070257 4.968991756439209 5.270086288452148 1.4360186122357845
CurrentTrain: epoch  4, batch    26 | loss: 16.5697247Losses:  18.443859020248055 5.137131690979004 5.211900234222412 1.9926280174404383
CurrentTrain: epoch  4, batch    27 | loss: 18.4438590Losses:  17.438171863555908 5.340717315673828 5.105119705200195 1.415478229522705
CurrentTrain: epoch  4, batch    28 | loss: 17.4381719Losses:  21.939254820346832 5.125646591186523 5.313319683074951 6.0215683579444885
CurrentTrain: epoch  4, batch    29 | loss: 21.9392548Losses:  21.004485346376896 5.014305114746094 5.398107528686523 5.7289974465966225
CurrentTrain: epoch  4, batch    30 | loss: 21.0044853Losses:  15.555597305297852 5.152162551879883 5.135074138641357 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 15.5555973Losses:  18.01611914485693 5.210516929626465 4.925047874450684 3.000166080892086
CurrentTrain: epoch  4, batch    32 | loss: 18.0161191Losses:  20.278038827702403 5.086625576019287 5.17996883392334 4.982532350346446
CurrentTrain: epoch  4, batch    33 | loss: 20.2780388Losses:  15.34753131866455 5.239171981811523 5.146969795227051 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 15.3475313Losses:  17.092062443494797 5.184508323669434 4.972064018249512 1.4038195312023163
CurrentTrain: epoch  4, batch    35 | loss: 17.0920624Losses:  20.657896384596825 5.390883445739746 5.243261337280273 4.637834891676903
CurrentTrain: epoch  4, batch    36 | loss: 20.6578964Losses:  20.084863780997694 5.23953104019165 5.3212409019470215 3.7490903129801154
CurrentTrain: epoch  4, batch    37 | loss: 20.0848638Losses:  15.159192085266113 5.102486610412598 5.115670204162598 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 15.1591921Losses:  19.832973793148994 5.108039379119873 5.127116680145264 4.377950981259346
CurrentTrain: epoch  4, batch    39 | loss: 19.8329738Losses:  20.28332808613777 4.999680042266846 5.283656120300293 4.625043898820877
CurrentTrain: epoch  4, batch    40 | loss: 20.2833281Losses:  15.616006851196289 5.216280937194824 5.401739597320557 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 15.6160069Losses:  21.893921479582787 5.13388729095459 5.198240756988525 6.2772127240896225
CurrentTrain: epoch  4, batch    42 | loss: 21.8939215Losses:  18.026463359594345 4.893816947937012 5.284611701965332 2.886490672826767
CurrentTrain: epoch  4, batch    43 | loss: 18.0264634Losses:  15.87899112701416 5.362381935119629 5.185433387756348 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 15.8789911Losses:  16.856531221419573 5.238481521606445 4.901468276977539 1.505769807845354
CurrentTrain: epoch  4, batch    45 | loss: 16.8565312Losses:  18.924947272986174 4.89914608001709 5.16451358795166 2.9991917721927166
CurrentTrain: epoch  4, batch    46 | loss: 18.9249473Losses:  17.097551073879004 5.133331298828125 5.377007007598877 1.5787598751485348
CurrentTrain: epoch  4, batch    47 | loss: 17.0975511Losses:  21.73129542917013 5.329812049865723 5.083072185516357 5.810641132295132
CurrentTrain: epoch  4, batch    48 | loss: 21.7312954Losses:  15.719295501708984 5.074099540710449 5.576232433319092 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 15.7192955Losses:  18.349524673074484 5.188321590423584 5.129895210266113 3.0057594142854214
CurrentTrain: epoch  4, batch    50 | loss: 18.3495247Losses:  14.773934364318848 4.781205654144287 5.197796821594238 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 14.7739344Losses:  19.37747645750642 5.075033664703369 5.236216068267822 4.275797609239817
CurrentTrain: epoch  4, batch    52 | loss: 19.3774765Losses:  18.52570715546608 4.940495491027832 5.127264022827148 1.6172122061252594
CurrentTrain: epoch  4, batch    53 | loss: 18.5257072Losses:  16.147945791482925 4.8340559005737305 5.341560363769531 1.3997720777988434
CurrentTrain: epoch  4, batch    54 | loss: 16.1479458Losses:  15.780048370361328 5.0439229011535645 5.1310529708862305 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 15.7800484Losses:  26.215943451970816 5.33121395111084 5.269203186035156 10.345828171819448
CurrentTrain: epoch  4, batch    56 | loss: 26.2159435Losses:  19.284393273293972 4.690547943115234 5.440846920013428 4.254182778298855
CurrentTrain: epoch  4, batch    57 | loss: 19.2843933Losses:  19.275119807571173 5.35780143737793 5.172998428344727 3.186929728835821
CurrentTrain: epoch  4, batch    58 | loss: 19.2751198Losses:  18.37998666986823 5.046462059020996 5.0656657218933105 3.061686422675848
CurrentTrain: epoch  4, batch    59 | loss: 18.3799867Losses:  16.675000816583633 4.840137004852295 5.29609489440918 1.4092423021793365
CurrentTrain: epoch  4, batch    60 | loss: 16.6750008Losses:  17.32839035987854 5.465690612792969 5.0568695068359375 1.4599478244781494
CurrentTrain: epoch  4, batch    61 | loss: 17.3283904Losses:  21.048961613327265 5.149599075317383 5.241245269775391 6.08079144731164
CurrentTrain: epoch  4, batch    62 | loss: 21.0489616Losses:  15.716732025146484 5.382206916809082 5.006342887878418 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 15.7167320Losses:  18.3136476688087 5.107173919677734 5.238183498382568 2.973128717392683
CurrentTrain: epoch  5, batch     1 | loss: 18.3136477Losses:  18.31460040807724 5.198548793792725 5.172018051147461 2.8136276602745056
CurrentTrain: epoch  5, batch     2 | loss: 18.3146004Losses:  18.53517320379615 5.163261413574219 5.097557067871094 2.9331176541745663
CurrentTrain: epoch  5, batch     3 | loss: 18.5351732Losses:  19.438820615410805 4.944659233093262 5.24203634262085 4.328190580010414
CurrentTrain: epoch  5, batch     4 | loss: 19.4388206Losses:  20.481224551796913 4.694433689117432 5.242762088775635 5.81241275370121
CurrentTrain: epoch  5, batch     5 | loss: 20.4812246Losses:  15.721125602722168 5.129894256591797 5.1750383377075195 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 15.7211256Losses:  16.852162208408117 5.242815017700195 5.061241149902344 1.4260691069066525
CurrentTrain: epoch  5, batch     7 | loss: 16.8521622Losses:  20.021241813898087 5.041408538818359 5.120902061462402 4.403769165277481
CurrentTrain: epoch  5, batch     8 | loss: 20.0212418Losses:  21.340915258973837 5.002081871032715 5.083832263946533 6.056205328553915
CurrentTrain: epoch  5, batch     9 | loss: 21.3409153Losses:  19.018864423036575 5.330231666564941 5.188465595245361 2.8108451664447784
CurrentTrain: epoch  5, batch    10 | loss: 19.0188644Losses:  39.91095149517059 4.949782371520996 5.321340084075928 24.55429255962372
CurrentTrain: epoch  5, batch    11 | loss: 39.9109515Losses:  18.359505645930767 5.168152332305908 5.24245023727417 2.823100082576275
CurrentTrain: epoch  5, batch    12 | loss: 18.3595056Losses:  16.619945466518402 4.817530155181885 5.470494747161865 1.4002627730369568
CurrentTrain: epoch  5, batch    13 | loss: 16.6199455Losses:  17.20370978116989 5.266689300537109 5.0724711418151855 1.7435123324394226
CurrentTrain: epoch  5, batch    14 | loss: 17.2037098Losses:  17.02227707952261 5.180045127868652 5.046662330627441 1.4823381528258324
CurrentTrain: epoch  5, batch    15 | loss: 17.0222771Losses:  16.69935067743063 4.7770538330078125 5.436297416687012 1.4604590758681297
CurrentTrain: epoch  5, batch    16 | loss: 16.6993507Losses:  18.61120617762208 4.913012504577637 5.455522060394287 2.864725235849619
CurrentTrain: epoch  5, batch    17 | loss: 18.6112062Losses:  19.49488901719451 5.138761520385742 5.091011047363281 4.520635362714529
CurrentTrain: epoch  5, batch    18 | loss: 19.4948890Losses:  16.462449342012405 5.1014862060546875 5.135005950927734 1.4077990353107452
CurrentTrain: epoch  5, batch    19 | loss: 16.4624493Losses:  17.382279366254807 4.94892692565918 5.115952491760254 2.807735413312912
CurrentTrain: epoch  5, batch    20 | loss: 17.3822794Losses:  20.318964928388596 4.816778182983398 5.309680938720703 5.73389145731926
CurrentTrain: epoch  5, batch    21 | loss: 20.3189649Losses:  17.756658993661404 4.759302139282227 5.411917209625244 3.05436559766531
CurrentTrain: epoch  5, batch    22 | loss: 17.7566590Losses:  19.23244409635663 5.341256141662598 5.151449203491211 3.4250786788761616
CurrentTrain: epoch  5, batch    23 | loss: 19.2324441Losses:  17.509951137006283 4.7825541496276855 5.2853803634643555 2.895076297223568
CurrentTrain: epoch  5, batch    24 | loss: 17.5099511Losses:  30.301529198884964 4.706365585327148 5.376675605773926 14.387666016817093
CurrentTrain: epoch  5, batch    25 | loss: 30.3015292Losses:  18.270929530262947 5.120270729064941 5.269844055175781 3.056876376271248
CurrentTrain: epoch  5, batch    26 | loss: 18.2709295Losses:  16.8722485601902 5.037023544311523 5.1099700927734375 1.4011925756931305
CurrentTrain: epoch  5, batch    27 | loss: 16.8722486Losses:  16.3102573081851 5.141392230987549 5.220891952514648 1.4758495017886162
CurrentTrain: epoch  5, batch    28 | loss: 16.3102573Losses:  20.040147822350264 5.055641174316406 5.239349842071533 4.392459910362959
CurrentTrain: epoch  5, batch    29 | loss: 20.0401478Losses:  17.5220701508224 5.091063499450684 5.1058735847473145 1.517076712101698
CurrentTrain: epoch  5, batch    30 | loss: 17.5220702Losses:  19.81726646795869 5.410099983215332 5.001833915710449 4.4363946951925755
CurrentTrain: epoch  5, batch    31 | loss: 19.8172665Losses:  16.7194401063025 5.305379867553711 5.10849666595459 1.4240900315344334
CurrentTrain: epoch  5, batch    32 | loss: 16.7194401Losses:  30.461288571357727 4.783389091491699 5.348333358764648 14.484614491462708
CurrentTrain: epoch  5, batch    33 | loss: 30.4612886Losses:  18.59824550524354 5.049206256866455 5.329120635986328 2.887826804071665
CurrentTrain: epoch  5, batch    34 | loss: 18.5982455Losses:  19.379664186388254 4.980844974517822 5.311511516571045 4.293984178453684
CurrentTrain: epoch  5, batch    35 | loss: 19.3796642Losses:  22.477260749787092 5.2737226486206055 5.10365104675293 6.856320541352034
CurrentTrain: epoch  5, batch    36 | loss: 22.4772607Losses:  16.416959173977375 4.726344108581543 5.532379150390625 1.4248622730374336
CurrentTrain: epoch  5, batch    37 | loss: 16.4169592Losses:  16.748796429485083 5.0429792404174805 5.26584529876709 1.4654187820851803
CurrentTrain: epoch  5, batch    38 | loss: 16.7487964Losses:  16.221205830574036 4.838498115539551 5.188376426696777 1.401241421699524
CurrentTrain: epoch  5, batch    39 | loss: 16.2212058Losses:  16.22568565607071 4.656360149383545 5.441246509552002 1.3928361535072327
CurrentTrain: epoch  5, batch    40 | loss: 16.2256857Losses:  15.809053421020508 5.087433815002441 5.334029674530029 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 15.8090534Losses:  16.950251322239637 4.838861465454102 5.176743507385254 1.5006625466048717
CurrentTrain: epoch  5, batch    42 | loss: 16.9502513Losses:  19.60492754727602 4.765766620635986 5.358597755432129 4.2912736013531685
CurrentTrain: epoch  5, batch    43 | loss: 19.6049275Losses:  15.207596778869629 4.702702522277832 5.274020195007324 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 15.2075968Losses:  14.947508811950684 5.258029937744141 5.11293888092041 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 14.9475088Losses:  18.107397954910994 5.051329612731934 5.476552963256836 2.9704465083777905
CurrentTrain: epoch  5, batch    46 | loss: 18.1073980Losses:  17.776843816041946 5.101679801940918 5.199851989746094 1.400729924440384
CurrentTrain: epoch  5, batch    47 | loss: 17.7768438Losses:  18.78671183809638 4.8451151847839355 5.515880584716797 2.8774291537702084
CurrentTrain: epoch  5, batch    48 | loss: 18.7867118Losses:  17.136618733406067 5.059018611907959 5.233214378356934 1.4050198793411255
CurrentTrain: epoch  5, batch    49 | loss: 17.1366187Losses:  16.653852611780167 4.695770263671875 5.34776496887207 1.4080163538455963
CurrentTrain: epoch  5, batch    50 | loss: 16.6538526Losses:  24.803232669830322 4.7573628425598145 5.305694580078125 10.032055377960205
CurrentTrain: epoch  5, batch    51 | loss: 24.8032327Losses:  19.190043188631535 4.888455867767334 5.376862049102783 4.31519577652216
CurrentTrain: epoch  5, batch    52 | loss: 19.1900432Losses:  23.61591400951147 5.01027774810791 5.236500263214111 8.519585274159908
CurrentTrain: epoch  5, batch    53 | loss: 23.6159140Losses:  16.62327527999878 4.908075332641602 5.137408256530762 1.5499157905578613
CurrentTrain: epoch  5, batch    54 | loss: 16.6232753Losses:  17.811405301094055 4.884734153747559 5.386165142059326 2.8674622774124146
CurrentTrain: epoch  5, batch    55 | loss: 17.8114053Losses:  18.26146975159645 4.911599636077881 5.14370059967041 2.8415755331516266
CurrentTrain: epoch  5, batch    56 | loss: 18.2614698Losses:  16.007808357477188 4.967127799987793 5.107308387756348 1.4724718630313873
CurrentTrain: epoch  5, batch    57 | loss: 16.0078084Losses:  20.291568495333195 5.2174577713012695 5.428091049194336 4.5152241960167885
CurrentTrain: epoch  5, batch    58 | loss: 20.2915685Losses:  20.469075098633766 4.841921806335449 5.157986164093018 4.853991404175758
CurrentTrain: epoch  5, batch    59 | loss: 20.4690751Losses:  16.527955312281847 4.826903343200684 5.151263236999512 1.5802490897476673
CurrentTrain: epoch  5, batch    60 | loss: 16.5279553Losses:  16.7182195186615 5.313791275024414 5.096831321716309 1.4468886852264404
CurrentTrain: epoch  5, batch    61 | loss: 16.7182195Losses:  19.82393879443407 5.407801151275635 5.115591526031494 4.306042142212391
CurrentTrain: epoch  5, batch    62 | loss: 19.8239388Losses:  21.723357245326042 5.119518280029297 5.076831817626953 6.671918913722038
CurrentTrain: epoch  6, batch     0 | loss: 21.7233572Losses:  17.875411063432693 5.0228424072265625 5.199240684509277 2.8341017067432404
CurrentTrain: epoch  6, batch     1 | loss: 17.8754111Losses:  16.6574886739254 4.967738628387451 5.2731170654296875 1.4553459584712982
CurrentTrain: epoch  6, batch     2 | loss: 16.6574887Losses:  16.90130754560232 5.197188377380371 5.203061103820801 1.459622822701931
CurrentTrain: epoch  6, batch     3 | loss: 16.9013075Losses:  16.2216553427279 4.993183135986328 5.282628059387207 1.4381136633455753
CurrentTrain: epoch  6, batch     4 | loss: 16.2216553Losses:  17.052935983985662 5.298141002655029 5.0409016609191895 1.5512861274182796
CurrentTrain: epoch  6, batch     5 | loss: 17.0529360Losses:  14.810382843017578 4.792460918426514 5.228220462799072 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 14.8103828Losses:  16.20205792784691 4.933619022369385 5.1896772384643555 1.4648743569850922
CurrentTrain: epoch  6, batch     7 | loss: 16.2020579Losses:  21.101206839084625 4.5490922927856445 5.352370262145996 6.195108473300934
CurrentTrain: epoch  6, batch     8 | loss: 21.1012068Losses:  17.93281914666295 5.183990001678467 5.195137023925781 3.0578916259109974
CurrentTrain: epoch  6, batch     9 | loss: 17.9328191Losses:  19.407946970313787 4.9297308921813965 5.2247490882873535 3.3277563117444515
CurrentTrain: epoch  6, batch    10 | loss: 19.4079470Losses:  18.66755760461092 4.526881217956543 5.7080793380737305 3.0919550731778145
CurrentTrain: epoch  6, batch    11 | loss: 18.6675576Losses:  16.89420308917761 5.15293550491333 5.138344764709473 1.452502153813839
CurrentTrain: epoch  6, batch    12 | loss: 16.8942031Losses:  16.39081820845604 4.79865026473999 5.390621662139893 1.4038263261318207
CurrentTrain: epoch  6, batch    13 | loss: 16.3908182Losses:  16.45349284261465 4.94289493560791 5.489795684814453 1.4624297246336937
CurrentTrain: epoch  6, batch    14 | loss: 16.4534928Losses:  18.32881649583578 5.019182205200195 5.192191123962402 2.964701734483242
CurrentTrain: epoch  6, batch    15 | loss: 18.3288165Losses:  14.477581024169922 4.733353614807129 5.2485032081604 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 14.4775810Losses:  17.655140280723572 4.757447242736816 5.226117134094238 2.9589446783065796
CurrentTrain: epoch  6, batch    17 | loss: 17.6551403Losses:  16.404958315193653 4.932394027709961 5.3419294357299805 1.564693994820118
CurrentTrain: epoch  6, batch    18 | loss: 16.4049583Losses:  14.654777526855469 4.746799945831299 5.159815311431885 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 14.6547775Losses:  19.65834478661418 4.78251838684082 5.268606662750244 4.649038832634687
CurrentTrain: epoch  6, batch    20 | loss: 19.6583448Losses:  16.210374742746353 4.86175537109375 5.137156963348389 1.4213675558567047
CurrentTrain: epoch  6, batch    21 | loss: 16.2103747Losses:  17.41714733839035 5.044965744018555 5.057621479034424 2.8482901453971863
CurrentTrain: epoch  6, batch    22 | loss: 17.4171473Losses:  16.45405986905098 5.02813720703125 5.117156028747559 1.4285118877887726
CurrentTrain: epoch  6, batch    23 | loss: 16.4540599Losses:  19.47576666250825 5.246221542358398 5.078117370605469 4.322906021028757
CurrentTrain: epoch  6, batch    24 | loss: 19.4757667Losses:  17.928967896848917 4.8599958419799805 5.389092445373535 2.8980478681623936
CurrentTrain: epoch  6, batch    25 | loss: 17.9289679Losses:  16.661682173609734 4.993159294128418 5.363247871398926 1.4743547886610031
CurrentTrain: epoch  6, batch    26 | loss: 16.6616822Losses:  16.229525595903397 5.014158725738525 5.129621505737305 1.4491014778614044
CurrentTrain: epoch  6, batch    27 | loss: 16.2295256Losses:  20.68213575333357 5.147783279418945 5.124148368835449 5.854562930762768
CurrentTrain: epoch  6, batch    28 | loss: 20.6821358Losses:  20.725353308022022 4.870823860168457 5.149406909942627 5.788967199623585
CurrentTrain: epoch  6, batch    29 | loss: 20.7253533Losses:  18.785498559474945 4.739180564880371 5.315484046936035 4.2010621428489685
CurrentTrain: epoch  6, batch    30 | loss: 18.7854986Losses:  16.350703805685043 5.125073432922363 5.035712718963623 1.417132943868637
CurrentTrain: epoch  6, batch    31 | loss: 16.3507038Losses:  17.619870472699404 4.516667366027832 5.429786682128906 2.95411329343915
CurrentTrain: epoch  6, batch    32 | loss: 17.6198705Losses:  15.967031121253967 4.984424591064453 5.0884294509887695 1.4105554819107056
CurrentTrain: epoch  6, batch    33 | loss: 15.9670311Losses:  14.966970443725586 4.533890724182129 5.270252227783203 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 14.9669704Losses:  19.374320164322853 5.082485675811768 5.169599533081055 3.348551884293556
CurrentTrain: epoch  6, batch    35 | loss: 19.3743202Losses:  17.876068267971277 4.804351329803467 5.237231254577637 2.9226152040064335
CurrentTrain: epoch  6, batch    36 | loss: 17.8760683Losses:  15.315869331359863 5.148655891418457 5.202004432678223 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 15.3158693Losses:  22.46317619085312 4.634069919586182 5.242420196533203 7.560933530330658
CurrentTrain: epoch  6, batch    38 | loss: 22.4631762Losses:  14.958130836486816 5.136382102966309 4.959902763366699 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 14.9581308Losses:  15.265547752380371 5.071942329406738 5.2275543212890625 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 15.2655478Losses:  16.196801334619522 5.035290718078613 5.179837703704834 1.4030524790287018
CurrentTrain: epoch  6, batch    41 | loss: 16.1968013Losses:  18.88758158683777 5.249567031860352 5.2371320724487305 2.8424346446990967
CurrentTrain: epoch  6, batch    42 | loss: 18.8875816Losses:  18.134699910879135 4.732521057128906 5.454629898071289 3.040578931570053
CurrentTrain: epoch  6, batch    43 | loss: 18.1346999Losses:  16.56697953119874 5.09078311920166 5.430873870849609 1.4684802331030369
CurrentTrain: epoch  6, batch    44 | loss: 16.5669795Losses:  16.263898938894272 4.73093318939209 5.373579978942871 1.4026995599269867
CurrentTrain: epoch  6, batch    45 | loss: 16.2638989Losses:  18.225851237773895 4.651812553405762 5.327027320861816 2.1700193285942078
CurrentTrain: epoch  6, batch    46 | loss: 18.2258512Losses:  17.656219720840454 4.745511531829834 5.2715163230896 2.863556146621704
CurrentTrain: epoch  6, batch    47 | loss: 17.6562197Losses:  16.252879884094 5.016120433807373 5.210731506347656 1.4987190030515194
CurrentTrain: epoch  6, batch    48 | loss: 16.2528799Losses:  16.392364233732224 4.5186638832092285 5.467449188232422 1.4315087497234344
CurrentTrain: epoch  6, batch    49 | loss: 16.3923642Losses:  16.254514515399933 4.778287887573242 5.273514747619629 1.4086950421333313
CurrentTrain: epoch  6, batch    50 | loss: 16.2545145Losses:  14.789780616760254 4.787827491760254 5.233129501342773 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 14.7897806Losses:  14.923300743103027 4.710040092468262 5.228714942932129 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 14.9233007Losses:  19.015301451086998 4.995371341705322 5.179002285003662 4.378399595618248
CurrentTrain: epoch  6, batch    53 | loss: 19.0153015Losses:  14.498127937316895 4.89947509765625 5.117018699645996 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 14.4981279Losses:  19.171179987490177 5.130996227264404 5.282121181488037 4.305197931826115
CurrentTrain: epoch  6, batch    55 | loss: 19.1711800Losses:  16.437321238219738 5.119424343109131 5.196413516998291 1.4666352793574333
CurrentTrain: epoch  6, batch    56 | loss: 16.4373212Losses:  17.80263263732195 5.180096626281738 5.10917329788208 2.9237931445240974
CurrentTrain: epoch  6, batch    57 | loss: 17.8026326Losses:  16.29456463456154 4.961708068847656 5.289066791534424 1.4501985609531403
CurrentTrain: epoch  6, batch    58 | loss: 16.2945646Losses:  16.5201430991292 4.7992706298828125 5.237565040588379 1.4360242560505867
CurrentTrain: epoch  6, batch    59 | loss: 16.5201431Losses:  14.471332550048828 4.912949085235596 5.228437900543213 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 14.4713326Losses:  15.558082610368729 4.405778884887695 5.402549743652344 1.4009628593921661
CurrentTrain: epoch  6, batch    61 | loss: 15.5580826Losses:  19.117964882403612 5.157769680023193 5.1465163230896 4.2365828938782215
CurrentTrain: epoch  6, batch    62 | loss: 19.1179649Losses:  16.12090229988098 4.714681625366211 5.252325057983398 1.4531853199005127
CurrentTrain: epoch  7, batch     0 | loss: 16.1209023Losses:  17.580455780029297 4.945157527923584 5.122857093811035 2.8218088150024414
CurrentTrain: epoch  7, batch     1 | loss: 17.5804558Losses:  14.674912452697754 4.65488338470459 5.346081733703613 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 14.6749125Losses:  22.4887877702713 5.051488876342773 5.2840070724487305 7.639530301094055
CurrentTrain: epoch  7, batch     3 | loss: 22.4887878Losses:  21.858352959156036 4.952177047729492 5.059725284576416 7.545952141284943
CurrentTrain: epoch  7, batch     4 | loss: 21.8583530Losses:  17.173065930604935 4.56569242477417 5.26727819442749 2.805609494447708
CurrentTrain: epoch  7, batch     5 | loss: 17.1730659Losses:  14.753134727478027 5.042224884033203 5.045053482055664 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 14.7531347Losses:  20.24524948000908 4.828630447387695 5.24301815032959 5.669559210538864
CurrentTrain: epoch  7, batch     7 | loss: 20.2452495Losses:  16.362911019474268 4.90952730178833 5.106257438659668 1.4250152446329594
CurrentTrain: epoch  7, batch     8 | loss: 16.3629110Losses:  16.362988471984863 5.368616580963135 5.092749118804932 1.445754051208496
CurrentTrain: epoch  7, batch     9 | loss: 16.3629885Losses:  19.291048139333725 4.871696472167969 5.222433090209961 4.431933492422104
CurrentTrain: epoch  7, batch    10 | loss: 19.2910481Losses:  14.586475372314453 4.780160427093506 5.24310302734375 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 14.5864754Losses:  18.622215777635574 5.252968788146973 5.11916446685791 2.820956736803055
CurrentTrain: epoch  7, batch    12 | loss: 18.6222158Losses:  15.774902522563934 4.845674514770508 5.250068664550781 1.398791491985321
CurrentTrain: epoch  7, batch    13 | loss: 15.7749025Losses:  18.395915500819683 5.262772560119629 5.257650375366211 3.4786848947405815
CurrentTrain: epoch  7, batch    14 | loss: 18.3959155Losses:  17.825313836336136 4.838302135467529 5.322299957275391 2.807247430086136
CurrentTrain: epoch  7, batch    15 | loss: 17.8253138Losses:  21.878396973013878 4.748371124267578 5.333283424377441 7.272124275565147
CurrentTrain: epoch  7, batch    16 | loss: 21.8783970Losses:  15.96952572464943 4.841270446777344 5.3583478927612305 1.3961566984653473
CurrentTrain: epoch  7, batch    17 | loss: 15.9695257Losses:  14.594440460205078 4.799031734466553 5.143917083740234 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 14.5944405Losses:  20.32656353712082 4.993341445922852 5.208558082580566 5.835393607616425
CurrentTrain: epoch  7, batch    19 | loss: 20.3265635Losses:  16.036733210086823 4.79865837097168 5.336674690246582 1.4061999917030334
CurrentTrain: epoch  7, batch    20 | loss: 16.0367332Losses:  18.7321141846478 4.93220329284668 5.091763496398926 4.334021914750338
CurrentTrain: epoch  7, batch    21 | loss: 18.7321142Losses:  17.33068546652794 4.921873569488525 5.271938800811768 2.7982910573482513
CurrentTrain: epoch  7, batch    22 | loss: 17.3306855Losses:  14.573959350585938 4.430002212524414 5.551778793334961 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 14.5739594Losses:  14.962495803833008 5.187043190002441 5.305838584899902 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 14.9624958Losses:  16.43795356154442 4.625335693359375 5.426664352416992 1.4426399171352386
CurrentTrain: epoch  7, batch    25 | loss: 16.4379536Losses:  17.572401646524668 4.915921211242676 5.202778339385986 2.854370716959238
CurrentTrain: epoch  7, batch    26 | loss: 17.5724016Losses:  15.875707387924194 5.0420989990234375 5.042771816253662 1.4123551845550537
CurrentTrain: epoch  7, batch    27 | loss: 15.8757074Losses:  17.06033730506897 4.839232444763184 5.171231269836426 2.793545961380005
CurrentTrain: epoch  7, batch    28 | loss: 17.0603373Losses:  15.928214073181152 5.0859785079956055 5.106083869934082 1.4004249572753906
CurrentTrain: epoch  7, batch    29 | loss: 15.9282141Losses:  14.172913551330566 4.882072448730469 5.007582664489746 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 14.1729136Losses:  21.43186393752694 4.572739601135254 5.540493011474609 7.0684377290308475
CurrentTrain: epoch  7, batch    31 | loss: 21.4318639Losses:  16.985644310712814 4.587636947631836 5.284058570861816 2.8375243842601776
CurrentTrain: epoch  7, batch    32 | loss: 16.9856443Losses:  22.131757080554962 5.058066368103027 5.178737640380859 7.587168991565704
CurrentTrain: epoch  7, batch    33 | loss: 22.1317571Losses:  18.281190268695354 4.214107513427734 5.509230613708496 4.243679396808147
CurrentTrain: epoch  7, batch    34 | loss: 18.2811903Losses:  14.114189147949219 4.4539794921875 5.32654333114624 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 14.1141891Losses:  19.38580232858658 4.904221534729004 5.206364631652832 4.330324232578278
CurrentTrain: epoch  7, batch    36 | loss: 19.3858023Losses:  17.8758732303977 5.063427925109863 5.329867362976074 2.9080721363425255
CurrentTrain: epoch  7, batch    37 | loss: 17.8758732Losses:  16.88080707192421 4.389823913574219 5.359528541564941 2.843300014734268
CurrentTrain: epoch  7, batch    38 | loss: 16.8808071Losses:  14.526966094970703 4.75080680847168 5.231625556945801 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 14.5269661Losses:  17.010607536882162 4.457871437072754 5.393458843231201 2.8410785757005215
CurrentTrain: epoch  7, batch    40 | loss: 17.0106075Losses:  17.233430206775665 4.587041854858398 5.391206741333008 2.8234370350837708
CurrentTrain: epoch  7, batch    41 | loss: 17.2334302Losses:  14.269966125488281 4.689199447631836 5.298471450805664 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 14.2699661Losses:  16.045178025960922 4.875585079193115 5.184407711029053 1.4073511064052582
CurrentTrain: epoch  7, batch    43 | loss: 16.0451780Losses:  18.829373374581337 4.804934501647949 5.144412040710449 4.443626418709755
CurrentTrain: epoch  7, batch    44 | loss: 18.8293734Losses:  17.071840465068817 4.807255744934082 5.115286827087402 2.8333742022514343
CurrentTrain: epoch  7, batch    45 | loss: 17.0718405Losses:  19.827666014432907 4.45009708404541 5.244847297668457 5.653260916471481
CurrentTrain: epoch  7, batch    46 | loss: 19.8276660Losses:  15.79761478304863 4.86390495300293 5.2328972816467285 1.401595801115036
CurrentTrain: epoch  7, batch    47 | loss: 15.7976148Losses:  15.871573831886053 4.8634843826293945 5.128597259521484 1.4417909644544125
CurrentTrain: epoch  7, batch    48 | loss: 15.8715738Losses:  16.391741305589676 4.888713359832764 5.430385112762451 1.4037785828113556
CurrentTrain: epoch  7, batch    49 | loss: 16.3917413Losses:  15.365206718444824 5.0225067138671875 5.223684310913086 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 15.3652067Losses:  16.201244473457336 4.990819931030273 5.219255447387695 1.4261761903762817
CurrentTrain: epoch  7, batch    51 | loss: 16.2012445Losses:  18.476434111595154 4.879143238067627 5.122539043426514 4.231337904930115
CurrentTrain: epoch  7, batch    52 | loss: 18.4764341Losses:  15.429848060011864 4.444766521453857 5.345405578613281 1.4465383142232895
CurrentTrain: epoch  7, batch    53 | loss: 15.4298481Losses:  18.42441099882126 4.539806365966797 5.3750481605529785 4.226761996746063
CurrentTrain: epoch  7, batch    54 | loss: 18.4244110Losses:  14.68445873260498 5.022470951080322 5.250237464904785 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 14.6844587Losses:  14.50787353515625 5.122235298156738 5.174523830413818 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 14.5078735Losses:  14.841288566589355 4.902390480041504 5.177308082580566 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 14.8412886Losses:  20.299257025122643 4.954323768615723 5.1902289390563965 5.870998129248619
CurrentTrain: epoch  7, batch    58 | loss: 20.2992570Losses:  14.057045936584473 4.51593017578125 5.2414703369140625 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 14.0570459Losses:  16.014744579792023 4.8900980949401855 5.423100471496582 1.430873692035675
CurrentTrain: epoch  7, batch    60 | loss: 16.0147446Losses:  15.857648998498917 4.704085350036621 5.442671298980713 1.4132978022098541
CurrentTrain: epoch  7, batch    61 | loss: 15.8576490Losses:  13.854999542236328 4.229562759399414 5.396942138671875 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 13.8549995Losses:  15.885164082050323 4.766664505004883 5.199721336364746 1.4377191662788391
CurrentTrain: epoch  8, batch     0 | loss: 15.8851641Losses:  17.32336443476379 4.617929458618164 5.297601699829102 3.074354348704219
CurrentTrain: epoch  8, batch     1 | loss: 17.3233644Losses:  17.149044420570135 4.757047653198242 5.251256465911865 2.89451065286994
CurrentTrain: epoch  8, batch     2 | loss: 17.1490444Losses:  15.589662425220013 5.010464668273926 4.882346153259277 1.4252241775393486
CurrentTrain: epoch  8, batch     3 | loss: 15.5896624Losses:  15.953800946474075 4.921364784240723 5.136391639709473 1.4056089222431183
CurrentTrain: epoch  8, batch     4 | loss: 15.9538009Losses:  14.43659782409668 4.818020820617676 5.370711326599121 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 14.4365978Losses:  14.232355117797852 4.673141956329346 5.296518325805664 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 14.2323551Losses:  16.9128235578537 4.263087272644043 5.57031774520874 2.8029450178146362
CurrentTrain: epoch  8, batch     7 | loss: 16.9128236Losses:  17.157646246254444 4.707276344299316 5.223297119140625 2.8416023924946785
CurrentTrain: epoch  8, batch     8 | loss: 17.1576462Losses:  18.120509833097458 4.145301818847656 5.642347812652588 4.210532873868942
CurrentTrain: epoch  8, batch     9 | loss: 18.1205098Losses:  14.598882675170898 5.227035999298096 5.1041178703308105 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 14.5988827Losses:  15.616439402103424 4.874412536621094 5.0879082679748535 1.4547134041786194
CurrentTrain: epoch  8, batch    11 | loss: 15.6164394Losses:  16.134093433618546 5.249741554260254 4.975604057312012 1.4018413126468658
CurrentTrain: epoch  8, batch    12 | loss: 16.1340934Losses:  15.845166075974703 5.014928817749023 5.113702774047852 1.4895914681255817
CurrentTrain: epoch  8, batch    13 | loss: 15.8451661Losses:  15.522038578987122 4.6923723220825195 5.1842732429504395 1.3995848894119263
CurrentTrain: epoch  8, batch    14 | loss: 15.5220386Losses:  15.998763650655746 4.852162837982178 5.383047103881836 1.4046855866909027
CurrentTrain: epoch  8, batch    15 | loss: 15.9987637Losses:  20.147700153291225 5.327978610992432 4.967834949493408 4.411669574677944
CurrentTrain: epoch  8, batch    16 | loss: 20.1477002Losses:  14.44626522064209 4.940445899963379 5.117592811584473 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 14.4462652Losses:  16.03225103020668 4.907801628112793 5.275027275085449 1.563676506280899
CurrentTrain: epoch  8, batch    18 | loss: 16.0322510Losses:  18.346062511205673 4.524419784545898 5.226531505584717 4.3340023458004
CurrentTrain: epoch  8, batch    19 | loss: 18.3460625Losses:  14.273557662963867 4.50549840927124 5.474098205566406 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 14.2735577Losses:  14.435370445251465 4.72585916519165 5.386675834655762 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 14.4353704Losses:  20.62220447510481 5.014658451080322 5.236108303070068 5.963602714240551
CurrentTrain: epoch  8, batch    22 | loss: 20.6222045Losses:  24.11697317287326 4.466897487640381 5.395275115966797 9.961224805563688
CurrentTrain: epoch  8, batch    23 | loss: 24.1169732Losses:  14.040369033813477 4.453704833984375 5.303542613983154 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 14.0403690Losses:  27.134366281330585 5.153348445892334 5.0924391746521 12.747735269367695
CurrentTrain: epoch  8, batch    25 | loss: 27.1343663Losses:  15.460423290729523 4.445924758911133 5.338201522827148 1.3957718014717102
CurrentTrain: epoch  8, batch    26 | loss: 15.4604233Losses:  24.765378614887595 4.149897575378418 5.960238933563232 10.07096447609365
CurrentTrain: epoch  8, batch    27 | loss: 24.7653786Losses:  13.907472610473633 4.466746807098389 5.237111568450928 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 13.9074726Losses:  18.37982714176178 4.409950256347656 5.462038040161133 4.255049347877502
CurrentTrain: epoch  8, batch    29 | loss: 18.3798271Losses:  15.733294997364283 5.058239936828613 5.007146835327148 1.4390168525278568
CurrentTrain: epoch  8, batch    30 | loss: 15.7332950Losses:  20.331296112388372 4.350878715515137 5.352974891662598 6.378172066062689
CurrentTrain: epoch  8, batch    31 | loss: 20.3312961Losses:  16.875467091798782 4.425769805908203 5.427376747131348 2.785140782594681
CurrentTrain: epoch  8, batch    32 | loss: 16.8754671Losses:  14.13691234588623 4.866440296173096 5.037237167358398 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 14.1369123Losses:  16.828586250543594 5.075241565704346 5.207382678985596 1.4343296587467194
CurrentTrain: epoch  8, batch    34 | loss: 16.8285863Losses:  15.743436094373465 4.846586227416992 5.22179651260376 1.4570000134408474
CurrentTrain: epoch  8, batch    35 | loss: 15.7434361Losses:  15.62857773900032 4.688196182250977 5.225620269775391 1.4106889069080353
CurrentTrain: epoch  8, batch    36 | loss: 15.6285777Losses:  14.171058654785156 4.772073745727539 5.2061967849731445 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 14.1710587Losses:  23.189260818064213 4.877915382385254 5.376229286193848 8.653289176523685
CurrentTrain: epoch  8, batch    38 | loss: 23.1892608Losses:  18.419163580983877 4.584205627441406 5.464542865753174 4.23009192571044
CurrentTrain: epoch  8, batch    39 | loss: 18.4191636Losses:  15.194198429584503 4.478338718414307 5.076911926269531 1.4131277203559875
CurrentTrain: epoch  8, batch    40 | loss: 15.1941984Losses:  15.997587233781815 5.090571403503418 5.2167582511901855 1.402887374162674
CurrentTrain: epoch  8, batch    41 | loss: 15.9975872Losses:  16.890327364206314 4.656218528747559 5.130070686340332 2.8373116552829742
CurrentTrain: epoch  8, batch    42 | loss: 16.8903274Losses:  23.1371426358819 3.9903554916381836 5.977128028869629 9.0156826749444
CurrentTrain: epoch  8, batch    43 | loss: 23.1371426Losses:  15.520557343959808 4.737675189971924 5.171806812286377 1.3897914290428162
CurrentTrain: epoch  8, batch    44 | loss: 15.5205573Losses:  15.66659900546074 5.035302639007568 4.9583353996276855 1.393071860074997
CurrentTrain: epoch  8, batch    45 | loss: 15.6665990Losses:  18.360611405223608 4.625229835510254 5.190128326416016 4.400567498058081
CurrentTrain: epoch  8, batch    46 | loss: 18.3606114Losses:  15.35252282768488 4.72115421295166 5.040875434875488 1.4269714131951332
CurrentTrain: epoch  8, batch    47 | loss: 15.3525228Losses:  14.963329315185547 5.203770637512207 5.108738899230957 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 14.9633293Losses:  14.06981086730957 4.65347957611084 5.23312520980835 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 14.0698109Losses:  16.878183752298355 4.337601184844971 5.501814365386963 2.8207344114780426
CurrentTrain: epoch  8, batch    50 | loss: 16.8781838Losses:  15.47271541506052 4.5772271156311035 5.226275444030762 1.4376459494233131
CurrentTrain: epoch  8, batch    51 | loss: 15.4727154Losses:  15.34054471552372 4.474039077758789 5.221533298492432 1.4580526500940323
CurrentTrain: epoch  8, batch    52 | loss: 15.3405447Losses:  14.139147758483887 4.657932281494141 5.333264350891113 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 14.1391478Losses:  18.792790353298187 5.183277130126953 4.995265007019043 4.388235032558441
CurrentTrain: epoch  8, batch    54 | loss: 18.7927904Losses:  15.744518429040909 5.024686813354492 5.115579605102539 1.4475614130496979
CurrentTrain: epoch  8, batch    55 | loss: 15.7445184Losses:  18.726610898971558 4.8583455085754395 5.221730709075928 4.4803855419158936
CurrentTrain: epoch  8, batch    56 | loss: 18.7266109Losses:  18.490194216370583 4.995019912719727 5.116825580596924 4.219661608338356
CurrentTrain: epoch  8, batch    57 | loss: 18.4901942Losses:  15.787871807813644 4.885181427001953 5.223588943481445 1.3962015807628632
CurrentTrain: epoch  8, batch    58 | loss: 15.7878718Losses:  15.798376649618149 4.994997978210449 5.197559356689453 1.4017482697963715
CurrentTrain: epoch  8, batch    59 | loss: 15.7983766Losses:  13.686341285705566 4.347042560577393 5.23201847076416 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 13.6863413Losses:  19.879934798926115 4.664276123046875 5.394124984741211 5.616972457617521
CurrentTrain: epoch  8, batch    61 | loss: 19.8799348Losses:  15.276327639818192 3.7582716941833496 5.972621917724609 1.425095111131668
CurrentTrain: epoch  8, batch    62 | loss: 15.2763276Losses:  17.27834991365671 4.632124900817871 5.242471694946289 3.2599993124604225
CurrentTrain: epoch  9, batch     0 | loss: 17.2783499Losses:  14.445898056030273 5.298820495605469 5.031516075134277 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 14.4458981Losses:  14.163738250732422 4.672891616821289 5.296673774719238 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 14.1637383Losses:  17.1152258887887 4.898754596710205 5.2565107345581055 2.821064092218876
CurrentTrain: epoch  9, batch     3 | loss: 17.1152259Losses:  16.851723670959473 4.3687357902526855 5.471640586853027 2.816251754760742
CurrentTrain: epoch  9, batch     4 | loss: 16.8517237Losses:  18.642839647829533 4.9470295906066895 5.189455032348633 4.322731234133244
CurrentTrain: epoch  9, batch     5 | loss: 18.6428396Losses:  15.235747277736664 4.641132354736328 5.060512542724609 1.3984107375144958
CurrentTrain: epoch  9, batch     6 | loss: 15.2357473Losses:  17.056237369775772 4.834409713745117 5.25261926651001 2.8210650980472565
CurrentTrain: epoch  9, batch     7 | loss: 17.0562374Losses:  16.441811561584473 3.899240493774414 5.614999294281006 2.7996912002563477
CurrentTrain: epoch  9, batch     8 | loss: 16.4418116Losses:  14.300260543823242 4.874835968017578 5.303253173828125 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 14.3002605Losses:  16.678274150937796 4.254805088043213 5.376564979553223 2.9263677559792995
CurrentTrain: epoch  9, batch    10 | loss: 16.6782742Losses:  16.576167166233063 5.044392108917236 5.280512809753418 1.4033613801002502
CurrentTrain: epoch  9, batch    11 | loss: 16.5761672Losses:  16.911205478012562 4.453739166259766 5.429876804351807 2.833639331161976
CurrentTrain: epoch  9, batch    12 | loss: 16.9112055Losses:  14.253274917602539 4.826948642730713 5.245555877685547 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 14.2532749Losses:  17.029352255165577 5.02050256729126 4.978192329406738 2.8641358092427254
CurrentTrain: epoch  9, batch    14 | loss: 17.0293523Losses:  17.98202918469906 4.582005500793457 5.1341552734375 4.19270633161068
CurrentTrain: epoch  9, batch    15 | loss: 17.9820292Losses:  16.96646948903799 4.732694149017334 5.153547286987305 2.850384436547756
CurrentTrain: epoch  9, batch    16 | loss: 16.9664695Losses:  14.011856079101562 4.633752822875977 5.26297664642334 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 14.0118561Losses:  15.38128399848938 4.525129318237305 5.263515949249268 1.4291222095489502
CurrentTrain: epoch  9, batch    18 | loss: 15.3812840Losses:  21.183888290077448 4.719316482543945 5.2257232666015625 7.131550643593073
CurrentTrain: epoch  9, batch    19 | loss: 21.1838883Losses:  15.239341013133526 4.426390647888184 5.279850006103516 1.4235880300402641
CurrentTrain: epoch  9, batch    20 | loss: 15.2393410Losses:  15.851524859666824 4.301041126251221 5.472539901733398 1.4022498428821564
CurrentTrain: epoch  9, batch    21 | loss: 15.8515249Losses:  13.736398696899414 4.265880584716797 5.349399089813232 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 13.7363987Losses:  16.542767882347107 3.91454815864563 5.6315016746521 2.8172162771224976
CurrentTrain: epoch  9, batch    23 | loss: 16.5427679Losses:  15.066365242004395 5.127108573913574 5.193118095397949 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 15.0663652Losses:  18.368566658347845 4.624425888061523 5.349663734436035 4.311062958091497
CurrentTrain: epoch  9, batch    25 | loss: 18.3685667Losses:  14.477468490600586 4.846299171447754 5.297885417938232 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 14.4774685Losses:  16.621256321668625 4.532901763916016 5.17833137512207 2.819137066602707
CurrentTrain: epoch  9, batch    27 | loss: 16.6212563Losses:  16.762391328811646 4.601937770843506 5.227653503417969 2.8376543521881104
CurrentTrain: epoch  9, batch    28 | loss: 16.7623913Losses:  16.752985831350088 4.255018711090088 5.46539831161499 2.8621510230004787
CurrentTrain: epoch  9, batch    29 | loss: 16.7529858Losses:  15.598099198192358 4.810939788818359 5.153226852416992 1.4239859245717525
CurrentTrain: epoch  9, batch    30 | loss: 15.5980992Losses:  18.419866278767586 4.725649833679199 5.303083419799805 4.286538794636726
CurrentTrain: epoch  9, batch    31 | loss: 18.4198663Losses:  23.396798759698868 4.923351287841797 5.122450828552246 9.15989175438881
CurrentTrain: epoch  9, batch    32 | loss: 23.3967988Losses:  18.169944614171982 4.443605422973633 5.403124809265137 4.222292751073837
CurrentTrain: epoch  9, batch    33 | loss: 18.1699446Losses:  14.136807441711426 4.914938449859619 5.129778861999512 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 14.1368074Losses:  19.970719151198864 4.836358070373535 5.292697906494141 5.689852528274059
CurrentTrain: epoch  9, batch    35 | loss: 19.9707192Losses:  16.970121026039124 4.376967430114746 5.673340320587158 2.8088566064834595
CurrentTrain: epoch  9, batch    36 | loss: 16.9701210Losses:  14.122673988342285 4.580275058746338 5.148654937744141 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 14.1226740Losses:  15.589462257921696 4.808994770050049 5.121491432189941 1.427989937365055
CurrentTrain: epoch  9, batch    38 | loss: 15.5894623Losses:  13.825647354125977 4.360445499420166 5.288487434387207 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 13.8256474Losses:  15.281544629484415 4.577630996704102 5.132040977478027 1.4450244344770908
CurrentTrain: epoch  9, batch    40 | loss: 15.2815446Losses:  14.007222175598145 4.572845935821533 5.3033952713012695 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 14.0072222Losses:  19.05807274580002 5.044410705566406 5.199705600738525 4.2230793833732605
CurrentTrain: epoch  9, batch    42 | loss: 19.0580727Losses:  16.71031567454338 4.166119575500488 5.62844181060791 2.7896938025951385
CurrentTrain: epoch  9, batch    43 | loss: 16.7103157Losses:  15.172860771417618 4.022900104522705 5.588433265686035 1.4232451021671295
CurrentTrain: epoch  9, batch    44 | loss: 15.1728608Losses:  15.34369444847107 4.187621116638184 5.607944488525391 1.4056575298309326
CurrentTrain: epoch  9, batch    45 | loss: 15.3436944Losses:  17.24191964045167 4.769032001495361 5.284449100494385 2.874566201120615
CurrentTrain: epoch  9, batch    46 | loss: 17.2419196Losses:  17.067045897245407 4.766027450561523 5.240649223327637 2.816545218229294
CurrentTrain: epoch  9, batch    47 | loss: 17.0670459Losses:  13.945015907287598 4.5015177726745605 5.287816047668457 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 13.9450159Losses:  16.97750434279442 4.559509754180908 5.171218395233154 2.8182102143764496
CurrentTrain: epoch  9, batch    49 | loss: 16.9775043Losses:  18.251703284680843 4.607715606689453 5.143780708312988 4.290287993848324
CurrentTrain: epoch  9, batch    50 | loss: 18.2517033Losses:  13.671894073486328 4.006451606750488 5.4994940757751465 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 13.6718941Losses:  17.009679786860943 4.677445411682129 5.275906562805176 2.8820943757891655
CurrentTrain: epoch  9, batch    52 | loss: 17.0096798Losses:  17.07352563738823 4.979794502258301 5.000455856323242 2.8275053203105927
CurrentTrain: epoch  9, batch    53 | loss: 17.0735256Losses:  16.47297528386116 4.188701152801514 5.391597747802734 2.8195729553699493
CurrentTrain: epoch  9, batch    54 | loss: 16.4729753Losses:  18.365979604423046 4.747982025146484 5.0510969161987305 4.318081311881542
CurrentTrain: epoch  9, batch    55 | loss: 18.3659796Losses:  18.62174515426159 4.919771194458008 5.121328353881836 4.327555701136589
CurrentTrain: epoch  9, batch    56 | loss: 18.6217452Losses:  22.54360058903694 4.277897357940674 5.540297985076904 8.64593556523323
CurrentTrain: epoch  9, batch    57 | loss: 22.5436006Losses:  15.374423235654831 4.5822224617004395 5.203059196472168 1.4078828990459442
CurrentTrain: epoch  9, batch    58 | loss: 15.3744232Losses:  18.44727498292923 5.0899658203125 4.905146598815918 4.210932552814484
CurrentTrain: epoch  9, batch    59 | loss: 18.4472750Losses:  15.432675868272781 4.704526901245117 5.050482273101807 1.431458979845047
CurrentTrain: epoch  9, batch    60 | loss: 15.4326759Losses:  19.493788927793503 4.549025058746338 5.515114784240723 4.201156824827194
CurrentTrain: epoch  9, batch    61 | loss: 19.4937889Losses:  20.13308848440647 4.487969398498535 5.555248260498047 5.914340391755104
CurrentTrain: epoch  9, batch    62 | loss: 20.1330885
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  18.82157412916422 5.355140686035156 5.348750114440918 1.758147157728672
CurrentTrain: epoch  0, batch     0 | loss: 18.8215741Losses:  22.689168021082878 5.43370246887207 5.405777931213379 5.907353445887566
CurrentTrain: epoch  0, batch     1 | loss: 22.6891680Losses:  19.75459833815694 5.403264045715332 5.415949821472168 2.9078156538307667
CurrentTrain: epoch  0, batch     2 | loss: 19.7545983Losses:  18.943509474396706 5.301161289215088 5.030275821685791 1.4271806627511978
CurrentTrain: epoch  0, batch     3 | loss: 18.9435095Losses:  17.160476684570312 5.3575592041015625 5.306547164916992 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 17.1604767Losses:  18.164082910865545 5.33901834487915 5.488892078399658 1.4505666755139828
CurrentTrain: epoch  1, batch     1 | loss: 18.1640829Losses:  17.091708686202765 5.448862075805664 5.283529281616211 1.4979024194180965
CurrentTrain: epoch  1, batch     2 | loss: 17.0917087Losses:  18.110291957855225 5.519040107727051 5.360873222351074 1.4667716026306152
CurrentTrain: epoch  1, batch     3 | loss: 18.1102920Losses:  15.304189682006836 5.305813312530518 5.303995609283447 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 15.3041897Losses:  19.00506477802992 5.354274272918701 5.4107255935668945 3.0585191771388054
CurrentTrain: epoch  2, batch     1 | loss: 19.0050648Losses:  15.907562255859375 5.382795333862305 5.343534469604492 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 15.9075623Losses:  15.574484281241894 5.622011661529541 5.571746349334717 1.5523685738444328
CurrentTrain: epoch  2, batch     3 | loss: 15.5744843Losses:  19.002606093883514 5.26113224029541 5.364560127258301 3.2347933650016785
CurrentTrain: epoch  3, batch     0 | loss: 19.0026061Losses:  17.460745859891176 5.27604866027832 5.420488357543945 3.0373955257236958
CurrentTrain: epoch  3, batch     1 | loss: 17.4607459Losses:  16.667243480682373 5.45064640045166 5.474531650543213 1.4043688774108887
CurrentTrain: epoch  3, batch     2 | loss: 16.6672435Losses:  16.361350677907467 5.762065410614014 5.3749213218688965 1.4476696476340294
CurrentTrain: epoch  3, batch     3 | loss: 16.3613507Losses:  14.309342384338379 5.277321815490723 5.567161560058594 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 14.3093424Losses:  20.023950135335326 5.382565021514893 5.382639408111572 5.274795090779662
CurrentTrain: epoch  4, batch     1 | loss: 20.0239501Losses:  14.413650512695312 5.328521728515625 5.284521102905273 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 14.4136505Losses:  16.109810017049313 5.452019214630127 5.245670795440674 1.4753343090415
CurrentTrain: epoch  4, batch     3 | loss: 16.1098100Losses:  14.478782653808594 5.288431167602539 5.468439102172852 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 14.4787827Losses:  13.962441444396973 5.315525054931641 5.459967613220215 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.9624414Losses:  18.162655156105757 5.338173866271973 5.364960670471191 3.938832562416792
CurrentTrain: epoch  5, batch     2 | loss: 18.1626552Losses:  15.527130238711834 5.245199680328369 5.238290309906006 1.4431878253817558
CurrentTrain: epoch  5, batch     3 | loss: 15.5271302Losses:  14.073283195495605 5.395750045776367 5.457691192626953 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 14.0732832Losses:  13.784557342529297 5.29978609085083 5.4170637130737305 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 13.7845573Losses:  13.767155647277832 5.277949333190918 5.385619163513184 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 13.7671556Losses:  14.696596190333366 5.064232349395752 5.736570835113525 1.5356350392103195
CurrentTrain: epoch  6, batch     3 | loss: 14.6965962Losses:  16.256705701351166 5.433751583099365 5.351104736328125 2.475050389766693
CurrentTrain: epoch  7, batch     0 | loss: 16.2567057Losses:  14.87454703822732 5.2895002365112305 5.360607624053955 1.5109720565378666
CurrentTrain: epoch  7, batch     1 | loss: 14.8745470Losses:  14.628645166754723 5.161640167236328 5.429037570953369 1.483377680182457
CurrentTrain: epoch  7, batch     2 | loss: 14.6286452Losses:  14.532042272388935 5.391958236694336 5.242679595947266 1.491352804005146
CurrentTrain: epoch  7, batch     3 | loss: 14.5320423Losses:  17.652164675295353 5.521036148071289 5.287710666656494 4.295553423464298
CurrentTrain: epoch  8, batch     0 | loss: 17.6521647Losses:  16.230037033557892 5.267358303070068 5.475282669067383 2.814453423023224
CurrentTrain: epoch  8, batch     1 | loss: 16.2300370Losses:  12.904542922973633 5.120944976806641 5.449126243591309 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 12.9045429Losses:  13.682149097323418 5.190351963043213 4.956449031829834 1.4412461966276169
CurrentTrain: epoch  8, batch     3 | loss: 13.6821491Losses:  14.444646060466766 5.193931579589844 5.415646553039551 1.4050742983818054
CurrentTrain: epoch  9, batch     0 | loss: 14.4446461Losses:  14.437474362552166 5.357987880706787 5.350745677947998 1.4496728107333183
CurrentTrain: epoch  9, batch     1 | loss: 14.4374744Losses:  14.863245453685522 5.310111999511719 5.1054534912109375 1.9379715584218502
CurrentTrain: epoch  9, batch     2 | loss: 14.8632455Losses:  14.117063201963902 4.934754848480225 5.843867778778076 1.4577805176377296
CurrentTrain: epoch  9, batch     3 | loss: 14.1170632
Losses:  12.547962188720703 4.6992058753967285 5.42645263671875 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 12.5479622Losses:  18.15857118740678 5.504886150360107 5.12044620513916 5.775882665067911
MemoryTrain:  epoch  0, batch     1 | loss: 18.1585712Losses:  12.343437194824219 4.685441970825195 5.255498886108398 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 12.3434372Losses:  17.24921538680792 5.552392959594727 5.376750946044922 5.6585123762488365
MemoryTrain:  epoch  1, batch     1 | loss: 17.2492154Losses:  11.463716506958008 5.11966609954834 5.267976760864258 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 11.4637165Losses:  17.901386324316263 3.80694317817688 5.780972480773926 5.665038172155619
MemoryTrain:  epoch  2, batch     1 | loss: 17.9013863Losses:  11.482601165771484 4.86746072769165 5.323916912078857 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 11.4826012Losses:  16.565074793994427 4.829925537109375 5.354722023010254 5.703973643481731
MemoryTrain:  epoch  3, batch     1 | loss: 16.5650748Losses:  11.523835182189941 5.037459373474121 5.308646202087402 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 11.5238352Losses:  15.851134773343801 4.2929768562316895 5.3926568031311035 5.875022407621145
MemoryTrain:  epoch  4, batch     1 | loss: 15.8511348Losses:  10.77801513671875 4.75076961517334 5.389998912811279 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 10.7780151Losses:  18.14802075177431 5.319998741149902 5.11985969543457 5.741123206913471
MemoryTrain:  epoch  5, batch     1 | loss: 18.1480208Losses:  11.035633087158203 4.731893539428711 5.300682544708252 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 11.0356331Losses:  16.552964091300964 5.327228546142578 5.467287063598633 5.643419146537781
MemoryTrain:  epoch  6, batch     1 | loss: 16.5529641Losses:  11.085800170898438 4.855936050415039 5.298095703125 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 11.0858002Losses:  15.851053088903427 4.820641994476318 5.353972911834717 5.628396838903427
MemoryTrain:  epoch  7, batch     1 | loss: 15.8510531Losses:  11.025026321411133 4.827053546905518 5.2677717208862305 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 11.0250263Losses:  16.080918103456497 4.839179039001465 5.435736179351807 5.74694898724556
MemoryTrain:  epoch  8, batch     1 | loss: 16.0809181Losses:  11.087564468383789 4.773284912109375 5.362103462219238 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 11.0875645Losses:  16.465654704719782 5.250185012817383 5.491645812988281 5.6846784092485905
MemoryTrain:  epoch  9, batch     1 | loss: 16.4656547
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 96.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 96.59%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 96.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 94.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 77.34%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 76.04%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 75.28%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 71.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.99%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 91.73%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 90.97%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.77%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 92.43%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 92.13%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 91.91%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 92.35%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.37%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.48%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.62%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 92.64%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 92.58%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 92.60%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.53%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.39%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.17%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.80%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.82%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.77%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 91.42%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.37%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 91.21%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.88%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.73%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 90.42%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 90.29%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 89.92%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 89.56%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 89.01%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 88.54%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.21%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 87.82%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.12%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 86.70%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 86.40%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 85.98%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 85.70%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 85.30%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 85.20%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 84.81%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 84.38%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 83.89%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 83.52%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 83.16%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.87%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 82.69%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.84%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 83.08%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.78%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 84.25%   
cur_acc:  ['0.9484', '0.7599']
his_acc:  ['0.9484', '0.8425']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  17.69226140715182 5.350956439971924 5.397706508636475 1.5603205654770136
CurrentTrain: epoch  0, batch     0 | loss: 17.6922614Losses:  17.544015884399414 5.271353721618652 5.177203178405762 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 17.5440159Losses:  20.489733912050724 5.129243850708008 5.453774452209473 4.286719538271427
CurrentTrain: epoch  0, batch     2 | loss: 20.4897339Losses:  20.609191492199898 5.213931560516357 5.13241720199585 1.5842185765504837
CurrentTrain: epoch  0, batch     3 | loss: 20.6091915Losses:  17.919424146413803 5.269545555114746 5.2073845863342285 1.4192791879177094
CurrentTrain: epoch  1, batch     0 | loss: 17.9194241Losses:  24.160790357738733 5.188815116882324 5.110915184020996 9.085398588329554
CurrentTrain: epoch  1, batch     1 | loss: 24.1607904Losses:  18.163060694932938 5.188790798187256 5.297353744506836 1.4510531723499298
CurrentTrain: epoch  1, batch     2 | loss: 18.1630607Losses:  18.951006591320038 5.523562431335449 5.173389434814453 1.4547945857048035
CurrentTrain: epoch  1, batch     3 | loss: 18.9510066Losses:  23.87921930849552 5.122759819030762 5.488629341125488 8.19929625093937
CurrentTrain: epoch  2, batch     0 | loss: 23.8792193Losses:  18.444304414093494 5.245481491088867 5.288229942321777 2.3969411328434944
CurrentTrain: epoch  2, batch     1 | loss: 18.4443044Losses:  17.034636195749044 5.323657035827637 5.113740921020508 1.4399678073823452
CurrentTrain: epoch  2, batch     2 | loss: 17.0346362Losses:  17.92716597765684 5.324587345123291 5.194913387298584 1.4820861741900444
CurrentTrain: epoch  2, batch     3 | loss: 17.9271660Losses:  15.351350784301758 5.294166564941406 5.230851650238037 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.3513508Losses:  18.308582831174135 5.21584415435791 5.091864585876465 3.5962348468601704
CurrentTrain: epoch  3, batch     1 | loss: 18.3085828Losses:  18.48841244727373 5.069387435913086 5.378998756408691 2.8823695853352547
CurrentTrain: epoch  3, batch     2 | loss: 18.4884124Losses:  17.73974640108645 5.066770553588867 5.535755157470703 1.898037264123559
CurrentTrain: epoch  3, batch     3 | loss: 17.7397464Losses:  18.76389503479004 5.293166160583496 5.211630821228027 3.281468391418457
CurrentTrain: epoch  4, batch     0 | loss: 18.7638950Losses:  15.894678622484207 5.0884504318237305 5.239117622375488 1.4137339890003204
CurrentTrain: epoch  4, batch     1 | loss: 15.8946786Losses:  18.746140122413635 5.081513404846191 5.411520957946777 3.1903358697891235
CurrentTrain: epoch  4, batch     2 | loss: 18.7461401Losses:  15.299570083618164 5.490979194641113 5.292362213134766 1.5466957092285156
CurrentTrain: epoch  4, batch     3 | loss: 15.2995701Losses:  14.917285919189453 5.070545196533203 5.485973358154297 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 14.9172859Losses:  15.502380218356848 5.248902320861816 5.191938400268555 1.5023563764989376
CurrentTrain: epoch  5, batch     1 | loss: 15.5023802Losses:  16.4739308655262 5.121859550476074 5.152350425720215 1.4246049225330353
CurrentTrain: epoch  5, batch     2 | loss: 16.4739309Losses:  16.8743886500597 5.165766716003418 5.239978790283203 1.4550618678331375
CurrentTrain: epoch  5, batch     3 | loss: 16.8743887Losses:  17.45799219235778 5.240386962890625 5.410667419433594 2.890528317540884
CurrentTrain: epoch  6, batch     0 | loss: 17.4579922Losses:  17.96301269158721 5.05219841003418 5.402214050292969 2.905117031186819
CurrentTrain: epoch  6, batch     1 | loss: 17.9630127Losses:  19.59961897134781 5.125485420227051 5.128558158874512 5.033046782016754
CurrentTrain: epoch  6, batch     2 | loss: 19.5996190Losses:  17.295958176255226 5.463223934173584 5.285701274871826 1.6503034979104996
CurrentTrain: epoch  6, batch     3 | loss: 17.2959582Losses:  13.885074615478516 5.2330780029296875 5.314327716827393 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 13.8850746Losses:  16.40139129757881 5.229840278625488 5.390727519989014 1.4092324078083038
CurrentTrain: epoch  7, batch     1 | loss: 16.4013913Losses:  19.347028642892838 4.987636566162109 5.2835693359375 4.734276682138443
CurrentTrain: epoch  7, batch     2 | loss: 19.3470286Losses:  16.228715466335416 4.60093355178833 5.225616931915283 1.6552558410912752
CurrentTrain: epoch  7, batch     3 | loss: 16.2287155Losses:  19.106840632855892 5.101955413818359 5.322963714599609 4.6974339708685875
CurrentTrain: epoch  8, batch     0 | loss: 19.1068406Losses:  17.695439368486404 5.137929916381836 5.1877360343933105 2.926907569169998
CurrentTrain: epoch  8, batch     1 | loss: 17.6954394Losses:  17.85695481300354 5.071290016174316 5.422096252441406 4.352518320083618
CurrentTrain: epoch  8, batch     2 | loss: 17.8569548Losses:  15.878962069749832 5.403829574584961 4.919787406921387 1.401093989610672
CurrentTrain: epoch  8, batch     3 | loss: 15.8789621Losses:  14.075852394104004 5.003119468688965 5.268670082092285 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 14.0758524Losses:  15.28584298864007 5.149544715881348 5.371068000793457 1.4387217499315739
CurrentTrain: epoch  9, batch     1 | loss: 15.2858430Losses:  16.175597220659256 5.171459197998047 5.0878143310546875 1.4905748665332794
CurrentTrain: epoch  9, batch     2 | loss: 16.1755972Losses:  13.692490793764591 5.204166412353516 4.840292930603027 1.45053980499506
CurrentTrain: epoch  9, batch     3 | loss: 13.6924908
Losses:  11.388274192810059 4.839173316955566 5.424223899841309 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 11.3882742Losses:  11.149591445922852 5.030782222747803 5.357250690460205 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 11.1495914Losses:  11.386268615722656 4.864925384521484 5.391274929046631 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 11.3862686Losses:  12.230701446533203 5.0824971199035645 5.719369411468506 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 12.2307014Losses:  11.798198699951172 4.870234489440918 5.591662406921387 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 11.7981987Losses:  10.796960830688477 5.048308372497559 5.429272651672363 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 10.7969608Losses:  11.078731536865234 4.940533638000488 5.633332252502441 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 11.0787315Losses:  10.894455909729004 4.88635778427124 5.3222856521606445 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 10.8944559Losses:  10.393353462219238 4.869174003601074 5.3720173835754395 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.3933535Losses:  11.337531089782715 5.0061116218566895 5.371429920196533 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 11.3375311Losses:  10.777169227600098 4.854662895202637 5.420418739318848 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 10.7771692Losses:  10.912507057189941 4.99957275390625 5.407111644744873 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 10.9125071Losses:  10.927978515625 4.753942489624023 5.446285247802734 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 10.9279785Losses:  10.46004867553711 5.080920219421387 5.272710800170898 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 10.4600487Losses:  10.485807418823242 5.212576866149902 5.15858268737793 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.4858074Losses:  10.933822631835938 4.568737983703613 5.569390773773193 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 10.9338226Losses:  10.485751152038574 5.00039005279541 5.34890604019165 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 10.4857512Losses:  10.738890647888184 4.776577472686768 5.604641437530518 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 10.7388906Losses:  10.318697929382324 4.708471775054932 5.484401702880859 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 10.3186979Losses:  10.390584945678711 5.07391357421875 5.244586944580078 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 10.3905849
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 76.60%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 76.66%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 77.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.68%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.82%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 76.29%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 76.27%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 75.94%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.01%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 90.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.79%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 90.19%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 89.83%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 89.48%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 89.04%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.91%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 88.87%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.04%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.37%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.34%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.49%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.76%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.92%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.85%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 89.66%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.56%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 89.30%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.35%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 89.25%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 89.01%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 88.84%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 88.37%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 87.86%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 87.71%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 87.15%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 87.23%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 87.02%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 86.69%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 86.24%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 85.86%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 85.48%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 85.18%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 85.08%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 84.91%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 84.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.41%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 84.13%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 83.80%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.59%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.39%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.77%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.18%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 81.65%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 80.97%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 80.52%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 80.19%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.98%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.04%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 80.99%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.05%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 80.85%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 80.71%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 80.47%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 80.18%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 79.58%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.64%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 79.72%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 79.29%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 78.99%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 78.74%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 78.54%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 78.30%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:  150 | acc: 68.75%,  total acc: 78.97%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 78.78%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 78.72%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 78.51%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 78.49%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.84%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 79.82%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 79.80%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.89%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 79.79%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 79.67%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 79.68%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 79.76%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.73%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 79.67%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 79.54%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 79.52%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 79.39%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 79.19%   
cur_acc:  ['0.9484', '0.7599', '0.7550']
his_acc:  ['0.9484', '0.8425', '0.7919']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  16.111572265625 5.524631500244141 5.086111068725586 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 16.1115723Losses:  17.592189498245716 5.42334508895874 5.493271350860596 1.4715821221470833
CurrentTrain: epoch  0, batch     1 | loss: 17.5921895Losses:  23.56706127896905 5.4849066734313965 5.30091667175293 8.243508193641901
CurrentTrain: epoch  0, batch     2 | loss: 23.5670613Losses:  17.274442985653877 5.436333656311035 5.560488700866699 1.5257180482149124
CurrentTrain: epoch  0, batch     3 | loss: 17.2744430Losses:  18.86847371608019 5.444022178649902 5.618772983551025 2.9886156991124153
CurrentTrain: epoch  1, batch     0 | loss: 18.8684737Losses:  14.91411018371582 5.502572059631348 5.454626083374023 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 14.9141102Losses:  15.860365841537714 5.476932525634766 5.4319167137146 1.442601177841425
CurrentTrain: epoch  1, batch     2 | loss: 15.8603658Losses:  15.245170459151268 5.454623699188232 4.975477695465088 1.4693716615438461
CurrentTrain: epoch  1, batch     3 | loss: 15.2451705Losses:  19.09628652781248 5.595540523529053 5.409299373626709 4.2573926374316216
CurrentTrain: epoch  2, batch     0 | loss: 19.0962865Losses:  17.10096860304475 5.355022430419922 5.215951919555664 2.87387490645051
CurrentTrain: epoch  2, batch     1 | loss: 17.1009686Losses:  16.89823803305626 5.391996383666992 5.616643905639648 2.9182794988155365
CurrentTrain: epoch  2, batch     2 | loss: 16.8982380Losses:  18.020761221647263 4.664516448974609 5.0 6.411001890897751
CurrentTrain: epoch  2, batch     3 | loss: 18.0207612Losses:  16.308801777660847 5.344542026519775 5.255643844604492 2.1592646911740303
CurrentTrain: epoch  3, batch     0 | loss: 16.3088018Losses:  18.199261486530304 5.5438408851623535 5.25075626373291 4.278095066547394
CurrentTrain: epoch  3, batch     1 | loss: 18.1992615Losses:  16.37503355741501 5.35739803314209 5.439517974853516 2.906519114971161
CurrentTrain: epoch  3, batch     2 | loss: 16.3750336Losses:  14.216832302510738 4.848289966583252 5.664700031280518 1.5022651180624962
CurrentTrain: epoch  3, batch     3 | loss: 14.2168323Losses:  15.15307179838419 5.372134208679199 5.305891513824463 1.4599870815873146
CurrentTrain: epoch  4, batch     0 | loss: 15.1530718Losses:  13.951904296875 5.335450172424316 5.517801284790039 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 13.9519043Losses:  15.226323012262583 5.428622245788574 5.284121036529541 1.5010517872869968
CurrentTrain: epoch  4, batch     2 | loss: 15.2263230Losses:  13.249071538448334 5.262613296508789 4.701820373535156 1.398920476436615
CurrentTrain: epoch  4, batch     3 | loss: 13.2490715Losses:  16.013909302651882 5.308108329772949 5.296079158782959 2.8945130929350853
CurrentTrain: epoch  5, batch     0 | loss: 16.0139093Losses:  13.503364562988281 5.403205394744873 5.387461185455322 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.5033646Losses:  15.000441431999207 5.333071708679199 5.1838884353637695 1.5269354581832886
CurrentTrain: epoch  5, batch     2 | loss: 15.0004414Losses:  15.065402619540691 5.231115341186523 5.890296936035156 1.550706498324871
CurrentTrain: epoch  5, batch     3 | loss: 15.0654026Losses:  13.605525970458984 5.3596367835998535 5.602553367614746 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 13.6055260Losses:  14.842626482248306 5.348660469055176 5.447057723999023 1.4148615896701813
CurrentTrain: epoch  6, batch     1 | loss: 14.8426265Losses:  19.101859867572784 5.306680679321289 5.080812454223633 6.01298314332962
CurrentTrain: epoch  6, batch     2 | loss: 19.1018599Losses:  14.577285021543503 5.337141036987305 5.873403549194336 1.4362556636333466
CurrentTrain: epoch  6, batch     3 | loss: 14.5772850Losses:  14.486126102507114 5.210565567016602 5.200473308563232 1.438243068754673
CurrentTrain: epoch  7, batch     0 | loss: 14.4861261Losses:  13.225223541259766 5.306691646575928 5.230350494384766 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 13.2252235Losses:  17.588555574417114 5.429523944854736 5.6880645751953125 4.221210718154907
CurrentTrain: epoch  7, batch     2 | loss: 17.5885556Losses:  14.303342074155807 5.247745990753174 5.706120014190674 1.51386758685112
CurrentTrain: epoch  7, batch     3 | loss: 14.3033421Losses:  14.308025866746902 5.310327529907227 5.292879104614258 1.4073567688465118
CurrentTrain: epoch  8, batch     0 | loss: 14.3080259Losses:  14.480504993349314 5.2429986000061035 5.484017372131348 1.4917202033102512
CurrentTrain: epoch  8, batch     1 | loss: 14.4805050Losses:  12.713544845581055 5.2136688232421875 5.3547163009643555 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 12.7135448Losses:  14.172460988163948 5.378577709197998 5.09301233291626 1.4268955737352371
CurrentTrain: epoch  8, batch     3 | loss: 14.1724610Losses:  15.996394611895084 5.282963752746582 5.547027587890625 2.892796017229557
CurrentTrain: epoch  9, batch     0 | loss: 15.9963946Losses:  12.696979522705078 5.262882232666016 5.207716941833496 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 12.6969795Losses:  18.313675455749035 5.244768142700195 5.214611053466797 5.644382052123547
CurrentTrain: epoch  9, batch     2 | loss: 18.3136755Losses:  13.632826946675777 5.226388931274414 5.145973205566406 1.48483195155859
CurrentTrain: epoch  9, batch     3 | loss: 13.6328269
Losses:  10.805408477783203 4.650233745574951 5.260015964508057 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.8054085Losses:  10.79096508026123 5.029764175415039 5.1994452476501465 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 10.7909651Losses:  12.57674789428711 5.204803466796875 5.40946102142334 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 12.5767479Losses:  11.362812042236328 5.162603378295898 5.442882537841797 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 11.3628120Losses:  11.676166534423828 4.7064337730407715 5.327671527862549 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 11.6761665Losses:  10.961099624633789 4.908154010772705 5.106513023376465 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 10.9610996Losses:  10.67220687866211 4.921694755554199 5.263370513916016 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 10.6722069Losses:  10.914324760437012 4.791217803955078 5.375080585479736 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 10.9143248Losses:  11.65512466430664 5.198962211608887 5.319143295288086 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 11.6551247Losses:  10.426291465759277 4.669261932373047 5.345416069030762 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 10.4262915Losses:  10.592710494995117 5.157845973968506 5.211462020874023 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 10.5927105Losses:  11.301860809326172 5.061590194702148 5.243929862976074 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 11.3018608Losses:  10.764633178710938 4.960618019104004 5.467639923095703 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.7646332Losses:  10.302801132202148 4.792680740356445 5.222844123840332 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 10.3028011Losses:  10.694721221923828 5.0014495849609375 5.317037105560303 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 10.6947212Losses:  10.432269096374512 4.822246551513672 5.389841079711914 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 10.4322691Losses:  10.585774421691895 4.875912189483643 5.407799243927002 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 10.5857744Losses:  10.30218505859375 4.917167663574219 5.1619672775268555 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 10.3021851Losses:  10.333820343017578 4.909445285797119 5.266480922698975 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 10.3338203Losses:  10.483333587646484 4.844616889953613 5.454163551330566 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 10.4833336Losses:  10.241275787353516 4.916406631469727 5.232867240905762 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 10.2412758Losses:  10.02810287475586 4.584329605102539 5.345950126647949 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.0281029Losses:  10.380855560302734 4.954349517822266 5.210596084594727 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 10.3808556Losses:  10.959434509277344 5.190101146697998 5.638693809509277 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 10.9594345Losses:  10.071187973022461 4.714577674865723 5.270552635192871 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 10.0711880Losses:  10.583361625671387 4.9703688621521 5.506606101989746 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 10.5833616Losses:  10.411834716796875 4.9745588302612305 5.363227367401123 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 10.4118347Losses:  10.311344146728516 4.848927021026611 5.395403861999512 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 10.3113441Losses:  10.647109985351562 4.851737022399902 5.688626289367676 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 10.6471100Losses:  10.458246231079102 4.943429470062256 5.432661056518555 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 10.4582462
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 54.08%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 52.86%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 51.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 53.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 55.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 58.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 59.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 60.89%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 62.31%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 73.23%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 73.18%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 73.33%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.74%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 87.94%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 87.61%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 87.29%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.99%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.01%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.21%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 87.41%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 87.59%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 87.59%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 88.11%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.18%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 88.26%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 88.40%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 88.30%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 88.21%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.96%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 87.58%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 87.28%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.84%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 86.63%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 85.99%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 85.87%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 85.60%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 85.28%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 85.37%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 85.19%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 85.01%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 84.57%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 84.14%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 83.72%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 83.44%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 83.35%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 83.27%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 83.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.74%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 82.28%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 82.09%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.96%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.78%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 81.37%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 80.67%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 80.10%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.43%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 78.89%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 78.46%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.26%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 79.39%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 79.64%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.70%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.27%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 78.89%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 78.42%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 78.10%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 77.69%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 77.15%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.23%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.52%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.94%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 77.47%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.10%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.77%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.54%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.31%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 76.09%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 76.82%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 76.56%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 76.43%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 76.34%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 76.08%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.52%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 76.93%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 77.02%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 77.13%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 77.16%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 77.18%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 77.16%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 77.19%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 77.11%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 77.09%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.11%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 76.99%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 76.95%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 76.94%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 76.89%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 76.81%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.09%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:  194 | acc: 12.50%,  total acc: 76.89%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 76.66%   [EVAL] batch:  196 | acc: 25.00%,  total acc: 76.40%   [EVAL] batch:  197 | acc: 12.50%,  total acc: 76.07%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 75.75%   [EVAL] batch:  199 | acc: 6.25%,  total acc: 75.41%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 75.43%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 75.43%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 75.37%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.34%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 75.33%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 75.15%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 74.88%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 74.70%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 74.40%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 74.23%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 73.94%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 73.86%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 74.57%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:  222 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  224 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 75.98%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.31%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.48%   
cur_acc:  ['0.9484', '0.7599', '0.7550', '0.7500']
his_acc:  ['0.9484', '0.8425', '0.7919', '0.7648']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  17.908559296280146 5.358243942260742 5.275355339050293 1.6437601782381535
CurrentTrain: epoch  0, batch     0 | loss: 17.9085593Losses:  22.52517756819725 5.278539180755615 5.244973182678223 6.337692826986313
CurrentTrain: epoch  0, batch     1 | loss: 22.5251776Losses:  19.148529712110758 5.362390518188477 5.333852767944336 3.003231707960367
CurrentTrain: epoch  0, batch     2 | loss: 19.1485297Losses:  18.532609526067972 5.201661586761475 5.143178462982178 1.5903468765318394
CurrentTrain: epoch  0, batch     3 | loss: 18.5326095Losses:  22.568739973008633 5.348658561706543 5.426912307739258 7.306738935410976
CurrentTrain: epoch  1, batch     0 | loss: 22.5687400Losses:  15.455697059631348 5.239482879638672 5.209046363830566 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 15.4556971Losses:  16.181114196777344 5.327277183532715 5.316128253936768 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 16.1811142Losses:  16.267992071807384 5.203321933746338 5.475402355194092 1.569643072783947
CurrentTrain: epoch  1, batch     3 | loss: 16.2679921Losses:  17.501653969287872 5.218270301818848 5.074081897735596 1.5237467885017395
CurrentTrain: epoch  2, batch     0 | loss: 17.5016540Losses:  16.439038805663586 5.28917932510376 5.304410934448242 1.5963378474116325
CurrentTrain: epoch  2, batch     1 | loss: 16.4390388Losses:  23.14907729625702 5.279172897338867 5.18471622467041 9.039947390556335
CurrentTrain: epoch  2, batch     2 | loss: 23.1490773Losses:  15.847398214042187 5.365706443786621 4.961688041687012 1.4436802193522453
CurrentTrain: epoch  2, batch     3 | loss: 15.8473982Losses:  15.651996612548828 5.240056991577148 5.434277057647705 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.6519966Losses:  15.714819848537445 5.2428388595581055 5.323606491088867 1.416252076625824
CurrentTrain: epoch  3, batch     1 | loss: 15.7148198Losses:  17.71076664701104 5.27341365814209 5.28749418258667 3.159535262733698
CurrentTrain: epoch  3, batch     2 | loss: 17.7107666Losses:  14.120460335165262 5.219875335693359 5.0131120681762695 1.5620926059782505
CurrentTrain: epoch  3, batch     3 | loss: 14.1204603Losses:  21.449370622634888 5.181682586669922 5.223366737365723 6.577409029006958
CurrentTrain: epoch  4, batch     0 | loss: 21.4493706Losses:  15.887331876903772 5.302350044250488 5.253300189971924 1.5829171277582645
CurrentTrain: epoch  4, batch     1 | loss: 15.8873319Losses:  15.215719722211361 5.222635269165039 5.217983245849609 1.4692826494574547
CurrentTrain: epoch  4, batch     2 | loss: 15.2157197Losses:  15.41242165863514 5.116569519042969 5.133886337280273 1.446973279118538
CurrentTrain: epoch  4, batch     3 | loss: 15.4124217Losses:  17.784713234752417 5.204765796661377 5.2725749015808105 3.157266106456518
CurrentTrain: epoch  5, batch     0 | loss: 17.7847132Losses:  13.477655410766602 5.20308780670166 5.121685028076172 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.4776554Losses:  13.88440990447998 5.226426601409912 5.38704776763916 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 13.8844099Losses:  13.903734922409058 5.259990692138672 5.02775764465332 1.4285075664520264
CurrentTrain: epoch  5, batch     3 | loss: 13.9037349Losses:  15.82207154110074 5.125748634338379 5.11611270904541 1.8921885378658772
CurrentTrain: epoch  6, batch     0 | loss: 15.8220715Losses:  16.569679476320744 5.172212600708008 5.3304338455200195 3.05798552185297
CurrentTrain: epoch  6, batch     1 | loss: 16.5696795Losses:  15.24901608377695 5.251185417175293 5.2325263023376465 1.445120133459568
CurrentTrain: epoch  6, batch     2 | loss: 15.2490161Losses:  14.192144848406315 5.170668125152588 5.22844934463501 1.4820656552910805
CurrentTrain: epoch  6, batch     3 | loss: 14.1921448Losses:  16.141097120940685 5.141506195068359 5.020572662353516 2.8347874209284782
CurrentTrain: epoch  7, batch     0 | loss: 16.1410971Losses:  13.764429092407227 5.252237319946289 5.413553237915039 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 13.7644291Losses:  14.942732594907284 5.21537446975708 5.210513591766357 1.4489238485693932
CurrentTrain: epoch  7, batch     2 | loss: 14.9427326Losses:  16.159646928310394 5.195222854614258 5.748887062072754 1.3990258574485779
CurrentTrain: epoch  7, batch     3 | loss: 16.1596469Losses:  13.404985427856445 5.084697246551514 5.082709789276123 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 13.4049854Losses:  17.603655166924 5.227299213409424 5.138031482696533 4.498020477592945
CurrentTrain: epoch  8, batch     1 | loss: 17.6036552Losses:  16.403055105358362 5.141922950744629 5.224238395690918 3.0987137891352177
CurrentTrain: epoch  8, batch     2 | loss: 16.4030551Losses:  14.388622902333736 5.224798202514648 5.534710884094238 1.4930044636130333
CurrentTrain: epoch  8, batch     3 | loss: 14.3886229Losses:  16.040648464113474 5.179506301879883 5.256802558898926 2.849420551210642
CurrentTrain: epoch  9, batch     0 | loss: 16.0406485Losses:  14.598933793604374 5.119441986083984 5.159986972808838 1.4683013930916786
CurrentTrain: epoch  9, batch     1 | loss: 14.5989338Losses:  13.24804973602295 5.184177398681641 5.218798637390137 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 13.2480497Losses:  13.881546206772327 5.02597713470459 4.739288330078125 1.4632074311375618
CurrentTrain: epoch  9, batch     3 | loss: 13.8815462
Losses:  10.832399368286133 4.873772621154785 5.2941789627075195 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.8323994Losses:  10.316816329956055 4.650940895080566 5.361810684204102 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 10.3168163Losses:  10.637102127075195 5.2129621505737305 5.243427276611328 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 10.6371021Losses:  13.175262462347746 5.340202808380127 5.562886714935303 1.5546941868960857
MemoryTrain:  epoch  0, batch     3 | loss: 13.1752625Losses:  11.198917388916016 4.878369331359863 5.5634989738464355 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 11.1989174Losses:  10.786741256713867 4.845582962036133 5.3187479972839355 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 10.7867413Losses:  10.966513633728027 5.03215217590332 5.324885368347168 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 10.9665136Losses:  13.314317256212234 5.06265115737915 5.965959072113037 1.4402518570423126
MemoryTrain:  epoch  1, batch     3 | loss: 13.3143173Losses:  10.916873931884766 4.845019340515137 5.594061851501465 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 10.9168739Losses:  10.403301239013672 5.025381088256836 5.160052299499512 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 10.4033012Losses:  10.410160064697266 4.833889007568359 5.218115329742432 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 10.4101601Losses:  13.08715184777975 5.287145614624023 6.235732078552246 1.4319642409682274
MemoryTrain:  epoch  2, batch     3 | loss: 13.0871518Losses:  10.691787719726562 4.764925003051758 5.51649284362793 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 10.6917877Losses:  10.211374282836914 4.905064582824707 5.137173652648926 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 10.2113743Losses:  10.620308876037598 5.0293121337890625 5.341562271118164 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 10.6203089Losses:  11.352090135216713 5.038429260253906 4.829000473022461 1.445112481713295
MemoryTrain:  epoch  3, batch     3 | loss: 11.3520901Losses:  10.989964485168457 5.213133811950684 5.4702301025390625 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.9899645Losses:  10.148946762084961 4.505868434906006 5.536655902862549 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 10.1489468Losses:  10.311420440673828 4.937219619750977 5.182771682739258 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 10.3114204Losses:  11.849741131067276 5.223152160644531 5.049045562744141 1.3986074030399323
MemoryTrain:  epoch  4, batch     3 | loss: 11.8497411Losses:  10.523601531982422 5.074294090270996 5.259981155395508 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 10.5236015Losses:  10.489867210388184 5.094897270202637 5.212213516235352 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 10.4898672Losses:  10.226314544677734 4.650571346282959 5.461973190307617 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 10.2263145Losses:  12.67941053211689 4.344322681427002 6.791305065155029 1.452313020825386
MemoryTrain:  epoch  5, batch     3 | loss: 12.6794105Losses:  10.654561996459961 5.001191139221191 5.438348770141602 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 10.6545620Losses:  10.161359786987305 4.644278526306152 5.377781391143799 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 10.1613598Losses:  10.139413833618164 4.947014808654785 5.0808563232421875 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 10.1394138Losses:  11.655202522873878 5.12467622756958 5.006663799285889 1.4744259268045425
MemoryTrain:  epoch  6, batch     3 | loss: 11.6552025Losses:  10.11152458190918 4.637833595275879 5.382941722869873 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.1115246Losses:  10.302271842956543 5.003186225891113 5.197699546813965 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 10.3022718Losses:  10.60101318359375 5.050836563110352 5.415297508239746 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 10.6010132Losses:  12.205745704472065 4.742312908172607 5.8999810218811035 1.4873113706707954
MemoryTrain:  epoch  7, batch     3 | loss: 12.2057457Losses:  10.479033470153809 5.176360130310059 5.21271276473999 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 10.4790335Losses:  10.400850296020508 5.0350022315979 5.235438823699951 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 10.4008503Losses:  10.039697647094727 4.455327033996582 5.5154547691345215 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 10.0396976Losses:  11.51790651679039 4.886016845703125 5.160207748413086 1.4360717236995697
MemoryTrain:  epoch  8, batch     3 | loss: 11.5179065Losses:  10.297396659851074 4.975447654724121 5.2209601402282715 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 10.2973967Losses:  10.443763732910156 5.014779090881348 5.349676609039307 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 10.4437637Losses:  10.15877628326416 4.533689498901367 5.552855491638184 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 10.1587763Losses:  12.065398283302784 5.057653903961182 5.368343830108643 1.4904547408223152
MemoryTrain:  epoch  9, batch     3 | loss: 12.0653983
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 16.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 42.71%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 45.31%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 46.32%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 46.53%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 50.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.40%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 59.70%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 59.58%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 60.08%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 60.16%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 59.01%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 58.57%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 57.47%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 56.08%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 56.41%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 57.05%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 57.34%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 57.77%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 59.16%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 59.52%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 60.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 84.43%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 84.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.71%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 84.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 84.27%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 84.47%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.85%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 85.07%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 85.20%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 85.85%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 86.02%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 85.55%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 85.10%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 84.65%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 84.18%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 83.84%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 83.58%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 83.26%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 83.09%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 82.99%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 82.47%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 82.53%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 82.30%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 82.01%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 82.13%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 81.58%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 81.12%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 80.60%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 80.35%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 80.23%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 80.05%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 79.88%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.64%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 79.47%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 79.25%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 79.09%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.99%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 78.39%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.78%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.24%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 76.59%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 76.07%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 75.73%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 76.81%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 77.07%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 77.10%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 76.64%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 76.28%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 75.83%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 75.48%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 74.48%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.96%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 74.55%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 74.25%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 74.03%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.82%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.57%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 73.71%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 74.10%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 73.94%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 73.70%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 73.55%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 73.40%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 73.74%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.16%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 74.61%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 74.59%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 74.52%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 74.59%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 74.53%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.03%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  194 | acc: 18.75%,  total acc: 74.87%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 74.62%   [EVAL] batch:  196 | acc: 25.00%,  total acc: 74.37%   [EVAL] batch:  197 | acc: 12.50%,  total acc: 74.05%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 73.74%   [EVAL] batch:  199 | acc: 6.25%,  total acc: 73.41%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 73.45%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 73.55%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 73.50%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 73.57%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 73.57%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 73.40%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 72.88%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 72.59%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 72.39%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 72.11%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 72.82%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 72.81%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 72.69%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 74.14%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 74.19%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 74.27%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 74.30%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.90%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 74.60%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 74.38%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 74.09%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 73.82%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 73.60%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 73.32%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 73.35%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 73.41%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 73.42%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 73.43%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 73.12%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 72.99%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 73.22%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 73.51%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 73.42%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 73.26%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 73.27%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 73.19%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 73.00%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 72.89%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 72.71%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 72.47%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 72.48%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 72.49%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:  294 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 72.68%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  297 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 72.89%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.88%   
cur_acc:  ['0.9484', '0.7599', '0.7550', '0.7500', '0.6984']
his_acc:  ['0.9484', '0.8425', '0.7919', '0.7648', '0.7388']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  16.48082733154297 5.232499599456787 5.197600364685059 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 16.4808273Losses:  17.72922259569168 5.250789642333984 5.26780366897583 1.505059540271759
CurrentTrain: epoch  0, batch     1 | loss: 17.7292226Losses:  18.111739287152886 5.344496726989746 5.313359260559082 1.668395170941949
CurrentTrain: epoch  0, batch     2 | loss: 18.1117393Losses:  19.53665117174387 5.180021286010742 5.352160453796387 1.856830157339573
CurrentTrain: epoch  0, batch     3 | loss: 19.5366512Losses:  16.932017114013433 5.218976974487305 5.2152910232543945 1.4723737500607967
CurrentTrain: epoch  1, batch     0 | loss: 16.9320171Losses:  17.422558665275574 5.227502822875977 5.298118591308594 1.55211341381073
CurrentTrain: epoch  1, batch     1 | loss: 17.4225587Losses:  14.779852867126465 5.213979721069336 5.218158721923828 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 14.7798529Losses:  16.29389801993966 5.246485233306885 5.2256646156311035 1.6342338658869267
CurrentTrain: epoch  1, batch     3 | loss: 16.2938980Losses:  16.062253944575787 5.1862406730651855 5.145707607269287 1.535110466182232
CurrentTrain: epoch  2, batch     0 | loss: 16.0622539Losses:  14.763137817382812 5.160938739776611 4.992039203643799 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 14.7631378Losses:  17.62390521541238 5.212315082550049 5.134237766265869 2.9688835479319096
CurrentTrain: epoch  2, batch     2 | loss: 17.6239052Losses:  22.714423574507236 5.0 4.822879791259766 9.74894753843546
CurrentTrain: epoch  2, batch     3 | loss: 22.7144236Losses:  19.65190379321575 5.140465259552002 5.2457404136657715 5.213505432009697
CurrentTrain: epoch  3, batch     0 | loss: 19.6519038Losses:  15.991617925465107 5.182401180267334 5.092179298400879 1.86282230168581
CurrentTrain: epoch  3, batch     1 | loss: 15.9916179Losses:  15.90922912582755 5.110725402832031 5.028853416442871 1.6847151182591915
CurrentTrain: epoch  3, batch     2 | loss: 15.9092291Losses:  17.180159203708172 5.180494785308838 5.577104091644287 1.4181810542941093
CurrentTrain: epoch  3, batch     3 | loss: 17.1801592Losses:  15.48900007456541 5.18248176574707 5.135196685791016 1.5242202207446098
CurrentTrain: epoch  4, batch     0 | loss: 15.4890001Losses:  14.728630065917969 5.115441799163818 5.1478962898254395 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 14.7286301Losses:  15.690708424896002 5.165375709533691 5.0414323806762695 1.513317372649908
CurrentTrain: epoch  4, batch     2 | loss: 15.6907084Losses:  13.491975978016853 5.271377086639404 4.72194242477417 1.4433118849992752
CurrentTrain: epoch  4, batch     3 | loss: 13.4919760Losses:  21.927292317152023 5.237788200378418 5.110221862792969 8.705801457166672
CurrentTrain: epoch  5, batch     0 | loss: 21.9272923Losses:  15.236994471400976 5.0966644287109375 4.925503730773926 1.5122849605977535
CurrentTrain: epoch  5, batch     1 | loss: 15.2369945Losses:  20.949261784553528 5.103476524353027 5.306190013885498 6.123650670051575
CurrentTrain: epoch  5, batch     2 | loss: 20.9492618Losses:  14.260063854977489 5.04731559753418 5.185386657714844 1.7179782073944807
CurrentTrain: epoch  5, batch     3 | loss: 14.2600639Losses:  16.468188129365444 5.1710309982299805 5.064836502075195 3.1118486747145653
CurrentTrain: epoch  6, batch     0 | loss: 16.4681881Losses:  16.976481705904007 5.078216075897217 5.102372169494629 2.896231919527054
CurrentTrain: epoch  6, batch     1 | loss: 16.9764817Losses:  15.09448528289795 5.114651679992676 5.0797295570373535 1.4096879959106445
CurrentTrain: epoch  6, batch     2 | loss: 15.0944853Losses:  15.11950434744358 5.128495216369629 5.030755043029785 1.418465033173561
CurrentTrain: epoch  6, batch     3 | loss: 15.1195043Losses:  16.953891895711422 4.982872009277344 5.044947624206543 2.907420299947262
CurrentTrain: epoch  7, batch     0 | loss: 16.9538919Losses:  16.526320729404688 5.162851333618164 5.300291061401367 2.90379074588418
CurrentTrain: epoch  7, batch     1 | loss: 16.5263207Losses:  18.954218346625566 5.145966053009033 5.1082444190979 5.476433236151934
CurrentTrain: epoch  7, batch     2 | loss: 18.9542183Losses:  13.79459423571825 5.1627607345581055 5.045286178588867 1.4292797520756721
CurrentTrain: epoch  7, batch     3 | loss: 13.7945942Losses:  14.871109396219254 5.0092573165893555 5.070528030395508 1.4049533903598785
CurrentTrain: epoch  8, batch     0 | loss: 14.8711094Losses:  16.390842974185944 5.059352397918701 5.285404682159424 2.8568673729896545
CurrentTrain: epoch  8, batch     1 | loss: 16.3908430Losses:  16.023991011083126 5.1238555908203125 5.041245460510254 2.8543333038687706
CurrentTrain: epoch  8, batch     2 | loss: 16.0239910Losses:  13.84875637292862 5.3529052734375 4.904399871826172 1.398722231388092
CurrentTrain: epoch  8, batch     3 | loss: 13.8487564Losses:  16.205376829952 5.12798547744751 5.143624782562256 2.951382841914892
CurrentTrain: epoch  9, batch     0 | loss: 16.2053768Losses:  14.584809243679047 5.10850715637207 5.190898895263672 1.4310616850852966
CurrentTrain: epoch  9, batch     1 | loss: 14.5848092Losses:  13.392938613891602 4.976869106292725 5.110780715942383 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 13.3929386Losses:  18.325603112578392 5.0 5.264103889465332 6.300927743315697
CurrentTrain: epoch  9, batch     3 | loss: 18.3256031
Losses:  11.572320938110352 5.081535816192627 5.366847991943359 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 11.5723209Losses:  10.623699188232422 4.934049606323242 5.233295440673828 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 10.6236992Losses:  10.957976341247559 4.487780570983887 5.43712043762207 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 10.9579763Losses:  11.269736289978027 5.18597412109375 5.45097541809082 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 11.2697363Losses:  11.738757133483887 4.780107498168945 5.5245232582092285 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 11.7387571Losses:  11.682868957519531 4.783478736877441 5.355088233947754 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 11.6828690Losses:  10.996511459350586 5.099577903747559 5.4431304931640625 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 10.9965115Losses:  11.035524368286133 5.048243999481201 5.235749244689941 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 11.0355244Losses:  10.953453063964844 5.070517539978027 5.399746894836426 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 10.9534531Losses:  10.856666564941406 4.821962356567383 5.3935651779174805 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 10.8566666Losses:  10.563488960266113 4.805660724639893 5.3616251945495605 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 10.5634890Losses:  10.936529159545898 4.834767818450928 5.526311874389648 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 10.9365292Losses:  10.261613845825195 4.460381984710693 5.434617519378662 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 10.2616138Losses:  10.209329605102539 4.764013290405273 5.183647155761719 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 10.2093296Losses:  10.900720596313477 5.215448379516602 5.331514358520508 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 10.9007206Losses:  10.832986831665039 5.142755508422852 5.512472629547119 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 10.8329868Losses:  10.021703720092773 4.361481189727783 5.414942741394043 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.0217037Losses:  10.550146102905273 5.040241241455078 5.179956436157227 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 10.5501461Losses:  10.4701566696167 5.116961479187012 5.169921875 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 10.4701567Losses:  10.361429214477539 4.9193620681762695 5.3160319328308105 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 10.3614292Losses:  10.430242538452148 4.9069671630859375 5.313024520874023 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 10.4302425Losses:  10.312627792358398 5.027358055114746 5.183260917663574 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 10.3126278Losses:  10.018205642700195 4.521036624908447 5.387076377868652 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 10.0182056Losses:  10.71447467803955 5.038990497589111 5.489445686340332 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 10.7144747Losses:  10.087095260620117 4.855470657348633 5.097316741943359 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 10.0870953Losses:  10.364234924316406 4.95847225189209 5.310277462005615 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 10.3642349Losses:  10.038822174072266 4.571921348571777 5.381956100463867 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 10.0388222Losses:  10.814794540405273 5.097884178161621 5.553916931152344 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 10.8147945Losses:  10.105527877807617 4.684879302978516 5.288576126098633 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.1055279Losses:  10.40795612335205 4.767406463623047 5.569454193115234 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 10.4079561Losses:  10.582460403442383 5.0444159507751465 5.4673943519592285 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 10.5824604Losses:  10.164539337158203 4.950321674346924 5.072977066040039 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 10.1645393Losses:  10.334209442138672 4.841065883636475 5.382099151611328 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 10.3342094Losses:  10.313654899597168 4.917489051818848 5.312049388885498 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 10.3136549Losses:  10.434368133544922 4.935238838195801 5.361672878265381 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 10.4343681Losses:  10.010709762573242 4.563482761383057 5.394063472747803 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 10.0107098Losses:  10.365701675415039 4.757575988769531 5.48930025100708 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 10.3657017Losses:  10.280006408691406 4.929520130157471 5.29583740234375 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 10.2800064Losses:  10.54061508178711 4.949216842651367 5.521412372589111 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 10.5406151Losses:  10.020110130310059 4.657423496246338 5.253900527954102 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 10.0201101
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 12.50%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 71.19%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 69.64%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 67.87%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 66.87%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 81.12%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 81.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 82.24%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 81.36%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 80.83%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 80.21%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 79.61%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 79.33%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 79.20%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 79.52%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 79.36%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 79.48%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 79.41%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 80.50%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 80.18%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 79.63%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 79.25%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 78.88%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 78.20%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 77.94%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 77.35%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 77.18%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 76.65%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 76.54%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 76.32%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 76.34%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 76.00%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 75.59%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 75.20%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 75.06%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 74.94%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.63%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 74.21%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.10%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 73.60%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 73.09%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.59%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 72.05%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.51%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 71.15%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 71.02%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 72.31%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 72.23%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 72.10%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 71.73%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 71.36%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 70.95%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 70.64%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 70.24%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 69.80%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.93%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 70.50%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 70.13%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 69.86%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 69.67%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 69.49%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.18%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 69.98%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 70.27%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 70.06%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 69.74%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 69.48%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 69.29%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 69.18%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 69.39%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 70.12%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 69.94%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 69.58%   [EVAL] batch:  196 | acc: 12.50%,  total acc: 69.29%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 68.94%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  199 | acc: 6.25%,  total acc: 68.38%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 68.60%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 68.51%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 68.24%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 68.00%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 67.71%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 67.51%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 67.25%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 68.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 69.83%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 70.04%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.35%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 70.52%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 70.06%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 69.83%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 69.38%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 69.33%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.33%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 69.30%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 69.25%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 69.17%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 68.98%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 68.89%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 68.77%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 69.22%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 69.18%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 69.09%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 68.79%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.57%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 68.44%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.25%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 68.01%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 67.95%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 68.02%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 68.20%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 68.93%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 68.85%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 68.73%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.00%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 69.04%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 68.83%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 68.66%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 69.98%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 69.80%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 69.64%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 69.46%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 69.26%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 69.10%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.19%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 69.42%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 69.12%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 69.00%   
cur_acc:  ['0.9484', '0.7599', '0.7550', '0.7500', '0.6984', '0.6687']
his_acc:  ['0.9484', '0.8425', '0.7919', '0.7648', '0.7388', '0.6900']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  19.129406847059727 5.319397926330566 5.423498153686523 2.2081154957413673
CurrentTrain: epoch  0, batch     0 | loss: 19.1294068Losses:  19.47577200131491 5.29995059967041 5.349035739898682 2.3896895428188145
CurrentTrain: epoch  0, batch     1 | loss: 19.4757720Losses:  22.537867221981287 5.199843883514404 5.278031826019287 5.663546238094568
CurrentTrain: epoch  0, batch     2 | loss: 22.5378672Losses:  21.921110151335597 5.490779876708984 5.558018684387207 1.9286937694996595
CurrentTrain: epoch  0, batch     3 | loss: 21.9211102Losses:  17.557752192020416 5.277666091918945 5.411336898803711 1.4367843270301819
CurrentTrain: epoch  1, batch     0 | loss: 17.5577522Losses:  16.346073150634766 5.303195476531982 5.4975175857543945 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 16.3460732Losses:  17.41753501072526 5.177685737609863 5.22238826751709 1.7123014368116856
CurrentTrain: epoch  1, batch     2 | loss: 17.4175350Losses:  16.75356673449278 5.514255046844482 5.2479424476623535 1.4225540086627007
CurrentTrain: epoch  1, batch     3 | loss: 16.7535667Losses:  18.585321605205536 5.252262115478516 5.266789436340332 3.488752543926239
CurrentTrain: epoch  2, batch     0 | loss: 18.5853216Losses:  17.36353388056159 5.232719421386719 5.460675239562988 1.462311651557684
CurrentTrain: epoch  2, batch     1 | loss: 17.3635339Losses:  17.690193712711334 5.265148162841797 5.319387912750244 1.418692171573639
CurrentTrain: epoch  2, batch     2 | loss: 17.6901937Losses:  18.271898984909058 5.30764102935791 5.205258369445801 1.4195010662078857
CurrentTrain: epoch  2, batch     3 | loss: 18.2718990Losses:  15.34061050415039 5.23829984664917 5.36485481262207 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.3406105Losses:  21.070840049535036 5.24511194229126 5.320091247558594 5.0277549512684345
CurrentTrain: epoch  3, batch     1 | loss: 21.0708400Losses:  16.90797570347786 5.184946060180664 5.457603454589844 1.7630658447742462
CurrentTrain: epoch  3, batch     2 | loss: 16.9079757Losses:  19.373321186751127 5.5693888664245605 5.294356822967529 1.5476127974689007
CurrentTrain: epoch  3, batch     3 | loss: 19.3733212Losses:  19.000403244048357 5.3204345703125 5.531695365905762 3.0808513946831226
CurrentTrain: epoch  4, batch     0 | loss: 19.0004032Losses:  16.24238346517086 5.162322998046875 5.129533767700195 1.6403870433568954
CurrentTrain: epoch  4, batch     1 | loss: 16.2423835Losses:  15.483297348022461 5.16779088973999 5.286006927490234 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 15.4832973Losses:  15.575524173676968 5.173822402954102 5.390005111694336 1.5984285697340965
CurrentTrain: epoch  4, batch     3 | loss: 15.5755242Losses:  15.48058795928955 5.273855209350586 5.519112586975098 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 15.4805880Losses:  19.785925194621086 5.215548992156982 5.331567764282227 4.611080452799797
CurrentTrain: epoch  5, batch     1 | loss: 19.7859252Losses:  14.696123123168945 5.1559648513793945 5.297997951507568 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 14.6961231Losses:  16.595003381371498 5.121969223022461 4.869434356689453 1.4534781128168106
CurrentTrain: epoch  5, batch     3 | loss: 16.5950034Losses:  17.9457632265985 5.147409439086914 5.307980537414551 3.1646314822137356
CurrentTrain: epoch  6, batch     0 | loss: 17.9457632Losses:  25.230872936546803 5.348167419433594 5.421246528625488 9.773661442101002
CurrentTrain: epoch  6, batch     1 | loss: 25.2308729Losses:  20.383233204483986 5.122151851654053 5.255565166473389 5.942845478653908
CurrentTrain: epoch  6, batch     2 | loss: 20.3832332Losses:  15.195279583334923 5.267465114593506 4.830891132354736 1.4314894527196884
CurrentTrain: epoch  6, batch     3 | loss: 15.1952796Losses:  17.347815232351422 5.158237934112549 5.21074104309082 3.062531189993024
CurrentTrain: epoch  7, batch     0 | loss: 17.3478152Losses:  16.333085507154465 5.215168476104736 5.197584629058838 1.5651716887950897
CurrentTrain: epoch  7, batch     1 | loss: 16.3330855Losses:  17.83371089398861 5.224300384521484 5.599304676055908 3.253848299384117
CurrentTrain: epoch  7, batch     2 | loss: 17.8337109Losses:  15.535732440650463 4.999313831329346 5.090264797210693 1.6516172215342522
CurrentTrain: epoch  7, batch     3 | loss: 15.5357324Losses:  17.695694476366043 5.074063777923584 5.414781093597412 2.952196627855301
CurrentTrain: epoch  8, batch     0 | loss: 17.6956945Losses:  16.850379936397076 5.126509666442871 5.22562313079834 2.917789451777935
CurrentTrain: epoch  8, batch     1 | loss: 16.8503799Losses:  22.907398521900177 5.225025653839111 5.385085582733154 8.772276222705841
CurrentTrain: epoch  8, batch     2 | loss: 22.9073985Losses:  15.549781784415245 5.535062789916992 5.1644182205200195 1.5263070911169052
CurrentTrain: epoch  8, batch     3 | loss: 15.5497818Losses:  14.197702407836914 5.136168003082275 5.324570178985596 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 14.1977024Losses:  14.018560409545898 5.255288124084473 5.474802017211914 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 14.0185604Losses:  15.802879147231579 5.093437194824219 5.298847675323486 1.4269559904932976
CurrentTrain: epoch  9, batch     2 | loss: 15.8028791Losses:  14.796358853578568 5.112833023071289 5.658914566040039 1.431008130311966
CurrentTrain: epoch  9, batch     3 | loss: 14.7963589
Losses:  10.59202766418457 4.833757400512695 5.374786376953125 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.5920277Losses:  11.372627258300781 4.999721527099609 5.485789775848389 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 11.3726273Losses:  10.478591918945312 4.7600274085998535 5.515271186828613 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 10.4785919Losses:  10.469134330749512 4.682510852813721 5.482713222503662 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 10.4691343Losses:  15.80442002043128 5.16310453414917 5.495776653289795 4.2594542764127254
MemoryTrain:  epoch  0, batch     4 | loss: 15.8044200Losses:  11.123713493347168 5.031611442565918 5.26639461517334 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 11.1237135Losses:  11.116653442382812 5.036637306213379 5.381298065185547 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 11.1166534Losses:  10.589775085449219 4.5591607093811035 5.5786638259887695 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 10.5897751Losses:  10.937957763671875 4.702145576477051 5.5334014892578125 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 10.9379578Losses:  14.955089963972569 5.126102924346924 5.048975467681885 4.355514921247959
MemoryTrain:  epoch  1, batch     4 | loss: 14.9550900Losses:  10.3228120803833 4.810654640197754 5.290165424346924 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 10.3228121Losses:  10.737142562866211 4.703624725341797 5.496543884277344 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 10.7371426Losses:  10.73182487487793 4.8985595703125 5.460199356079102 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 10.7318249Losses:  10.549826622009277 4.804594993591309 5.198511600494385 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 10.5498266Losses:  15.19846312329173 4.972078323364258 5.7741265296936035 4.273954074829817
MemoryTrain:  epoch  2, batch     4 | loss: 15.1984631Losses:  10.310205459594727 4.6517815589904785 5.355735778808594 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 10.3102055Losses:  10.862163543701172 5.165861129760742 5.415702819824219 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 10.8621635Losses:  10.407574653625488 4.751455783843994 5.191232681274414 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 10.4075747Losses:  10.35932731628418 4.6104888916015625 5.53361701965332 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 10.3593273Losses:  15.274614423513412 5.1725616455078125 5.637850284576416 4.30831441283226
MemoryTrain:  epoch  3, batch     4 | loss: 15.2746144Losses:  10.420658111572266 5.049182415008545 5.168397903442383 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.4206581Losses:  9.998529434204102 4.480198383331299 5.36325740814209 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.9985294Losses:  10.630477905273438 5.018036365509033 5.409553527832031 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 10.6304779Losses:  10.37936782836914 4.75493049621582 5.485039234161377 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 10.3793678Losses:  15.804603420197964 4.865063190460205 5.917366027832031 4.274518810212612
MemoryTrain:  epoch  4, batch     4 | loss: 15.8046034Losses:  9.821825981140137 4.449924945831299 5.275153160095215 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.8218260Losses:  10.518999099731445 4.949041366577148 5.357305526733398 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 10.5189991Losses:  10.468278884887695 4.903106212615967 5.4567646980285645 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 10.4682789Losses:  10.568851470947266 5.047189235687256 5.3088531494140625 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 10.5688515Losses:  14.85726923123002 4.5030317306518555 5.625279426574707 4.4867762960493565
MemoryTrain:  epoch  5, batch     4 | loss: 14.8572692Losses:  10.399850845336914 4.9028425216674805 5.370466232299805 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 10.3998508Losses:  9.998392105102539 4.607180595397949 5.286786079406738 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.9983921Losses:  10.32796573638916 4.897194862365723 5.191712379455566 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 10.3279657Losses:  10.541183471679688 4.85305118560791 5.461665630340576 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 10.5411835Losses:  14.58243602886796 4.717135429382324 5.430049419403076 4.320404473692179
MemoryTrain:  epoch  6, batch     4 | loss: 14.5824360Losses:  10.36440658569336 4.678892135620117 5.475553512573242 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.3644066Losses:  10.32397174835205 4.956066608428955 5.270430564880371 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 10.3239717Losses:  10.49098014831543 4.989108085632324 5.413156986236572 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 10.4909801Losses:  9.812997817993164 4.560577869415283 5.168173313140869 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 9.8129978Losses:  14.718178126960993 5.015785217285156 5.362630367279053 4.303470943123102
MemoryTrain:  epoch  7, batch     4 | loss: 14.7181781Losses:  9.919621467590332 4.642556190490723 5.209908485412598 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.9196215Losses:  9.715641975402832 4.542746543884277 5.0826520919799805 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.7156420Losses:  10.499067306518555 4.977287769317627 5.399262428283691 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 10.4990673Losses:  10.429131507873535 4.897953033447266 5.414549827575684 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 10.4291315Losses:  14.827238343656063 4.881689548492432 5.566760063171387 4.322210572659969
MemoryTrain:  epoch  8, batch     4 | loss: 14.8272383Losses:  10.307256698608398 4.983746528625488 5.2381486892700195 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 10.3072567Losses:  10.029348373413086 4.4601945877075195 5.469099044799805 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 10.0293484Losses:  10.421976089477539 5.012526512145996 5.328385353088379 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 10.4219761Losses:  10.101305961608887 4.589568614959717 5.358560562133789 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 10.1013060Losses:  14.323617726564407 4.952712535858154 5.051424503326416 4.257384091615677
MemoryTrain:  epoch  9, batch     4 | loss: 14.3236177
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 60.05%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 55.05%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 53.24%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 51.56%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 50.22%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 49.17%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 47.78%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 49.81%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 51.10%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 52.50%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 53.47%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 54.22%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 55.43%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 56.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 57.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 61.67%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 63.24%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 62.98%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 62.85%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 62.84%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 62.95%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 62.83%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 62.08%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 62.08%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 61.68%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 61.90%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 61.21%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.32%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 80.57%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 81.37%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 81.48%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 81.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 81.36%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 79.96%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 79.45%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 78.96%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 78.38%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 77.98%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 78.03%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 78.22%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 78.17%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 78.48%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 79.19%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 78.65%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 78.37%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 78.16%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 77.97%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 77.52%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 77.41%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 77.31%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 77.06%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 76.58%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 76.26%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 75.90%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 75.76%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 75.68%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 75.34%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 74.87%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 74.47%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 74.02%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 73.84%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 73.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 72.82%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 72.72%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 72.68%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.58%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 72.20%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 71.70%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 71.16%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 70.62%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 70.16%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 69.81%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 71.00%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 71.04%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 70.92%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 70.85%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 70.39%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 69.88%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 69.38%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 68.99%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 68.56%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 68.18%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.02%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 68.66%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 68.40%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.22%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.05%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.84%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 68.50%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 68.22%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 67.98%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 67.78%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  162 | acc: 18.75%,  total acc: 67.41%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 67.04%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 66.63%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 66.23%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 65.83%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 65.48%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 65.20%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 64.75%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 64.60%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 64.51%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 64.46%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 64.40%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 64.61%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 65.09%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 65.51%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 65.83%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 65.50%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 65.16%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 64.84%   [EVAL] batch:  198 | acc: 0.00%,  total acc: 64.51%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 64.19%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 64.49%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 64.54%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 64.43%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 64.18%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 63.91%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 63.63%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 63.42%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 63.15%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 64.15%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 64.27%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.34%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.65%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.87%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 66.91%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 66.67%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:  254 | acc: 12.50%,  total acc: 66.23%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 65.97%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.93%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 65.87%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 65.72%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 66.18%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 65.88%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 65.72%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 65.39%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 65.21%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 64.98%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 64.80%   [EVAL] batch:  288 | acc: 25.00%,  total acc: 64.66%   [EVAL] batch:  289 | acc: 6.25%,  total acc: 64.46%   [EVAL] batch:  290 | acc: 12.50%,  total acc: 64.28%   [EVAL] batch:  291 | acc: 18.75%,  total acc: 64.13%   [EVAL] batch:  292 | acc: 31.25%,  total acc: 64.01%   [EVAL] batch:  293 | acc: 25.00%,  total acc: 63.88%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 63.95%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 65.07%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 64.94%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 64.95%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 65.11%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 65.36%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 65.19%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 65.13%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 64.97%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 66.47%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 66.30%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 65.98%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 65.81%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 65.66%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 66.13%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.63%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 65.59%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 65.52%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 65.30%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 65.21%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 65.11%   [EVAL] batch:  379 | acc: 31.25%,  total acc: 65.02%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 64.86%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 65.12%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 65.48%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 65.40%   [EVAL] batch:  395 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 65.27%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 65.20%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 65.08%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 64.84%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 64.69%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 64.55%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 64.31%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 64.16%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 65.14%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  422 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  425 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:  426 | acc: 50.00%,  total acc: 65.21%   [EVAL] batch:  427 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 65.18%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:  435 | acc: 37.50%,  total acc: 64.98%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 64.90%   
cur_acc:  ['0.9484', '0.7599', '0.7550', '0.7500', '0.6984', '0.6687', '0.6121']
his_acc:  ['0.9484', '0.8425', '0.7919', '0.7648', '0.7388', '0.6900', '0.6490']
Clustering into  39  clusters
Clusters:  [31  0 23  2  1 35 24 10 14 14  8 38  7  1 25 20 19  0  3  3  9  3 22  3
 26  0  8 29  0  3  3  2 12  6 27  3  7 33 28  0  0 21  3  3 10 37  5  3
 34  5 36  3 13 17  3  3  4 18  6 32  0  3  8  7  9  3  5  3  5  3 11 30
 15  3  3  3 16  4  3  4]
Losses:  20.942151360213757 5.194619655609131 5.341463565826416 4.343735985457897
CurrentTrain: epoch  0, batch     0 | loss: 20.9421514Losses:  21.99317282065749 5.092861175537109 5.3703813552856445 6.438978370279074
CurrentTrain: epoch  0, batch     1 | loss: 21.9931728Losses:  19.645799979567528 5.2599687576293945 5.358617782592773 1.586725577712059
CurrentTrain: epoch  0, batch     2 | loss: 19.6458000Losses:  29.03000319004059 5.0 5.484967231750488 12.740120530128479
CurrentTrain: epoch  0, batch     3 | loss: 29.0300032Losses:  22.986836317926645 5.061493873596191 5.406810760498047 6.802288893610239
CurrentTrain: epoch  1, batch     0 | loss: 22.9868363Losses:  17.58995881676674 5.21872615814209 5.430808067321777 1.411888748407364
CurrentTrain: epoch  1, batch     1 | loss: 17.5899588Losses:  17.19777312874794 5.2300615310668945 5.353392124176025 1.4224988520145416
CurrentTrain: epoch  1, batch     2 | loss: 17.1977731Losses:  18.224043406546116 5.239648818969727 5.546090126037598 1.6989312544465065
CurrentTrain: epoch  1, batch     3 | loss: 18.2240434Losses:  20.810890838503838 5.101431846618652 5.2943315505981445 5.047910377383232
CurrentTrain: epoch  2, batch     0 | loss: 20.8108908Losses:  15.705231614410877 5.065520286560059 5.218832969665527 1.49045080691576
CurrentTrain: epoch  2, batch     1 | loss: 15.7052316Losses:  22.387252755463123 5.221513271331787 5.355077266693115 5.801494546234608
CurrentTrain: epoch  2, batch     2 | loss: 22.3872528Losses:  14.869621902704239 5.214622497558594 5.516287803649902 1.398851066827774
CurrentTrain: epoch  2, batch     3 | loss: 14.8696219Losses:  15.414560317993164 5.152222633361816 5.415166854858398 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.4145603Losses:  16.30927186831832 5.096569061279297 5.250214576721191 1.5336227975785732
CurrentTrain: epoch  3, batch     1 | loss: 16.3092719Losses:  15.080937385559082 5.095548629760742 5.273038864135742 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 15.0809374Losses:  19.391851402819157 5.230185508728027 4.947159767150879 1.5402374044060707
CurrentTrain: epoch  3, batch     3 | loss: 19.3918514Losses:  16.95850858092308 5.087184906005859 5.318402290344238 1.4189835488796234
CurrentTrain: epoch  4, batch     0 | loss: 16.9585086Losses:  14.52419662475586 5.16905403137207 5.260863780975342 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 14.5241966Losses:  20.054211486130953 5.10916805267334 5.254093170166016 5.481578696519136
CurrentTrain: epoch  4, batch     2 | loss: 20.0542115Losses:  18.776304095983505 5.104209899902344 5.274544715881348 1.4493540227413177
CurrentTrain: epoch  4, batch     3 | loss: 18.7763041Losses:  16.400096714496613 5.061695098876953 5.229526519775391 1.4360654950141907
CurrentTrain: epoch  5, batch     0 | loss: 16.4000967Losses:  18.445914562791586 5.090218544006348 5.293353080749512 4.382978733628988
CurrentTrain: epoch  5, batch     1 | loss: 18.4459146Losses:  16.759389106184244 5.174405097961426 5.24030876159668 1.4666540063917637
CurrentTrain: epoch  5, batch     2 | loss: 16.7593891Losses:  16.57981564849615 5.192383766174316 5.044172286987305 1.4940135702490807
CurrentTrain: epoch  5, batch     3 | loss: 16.5798156Losses:  15.218667984008789 5.183182716369629 5.291869640350342 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 15.2186680Losses:  17.456445902585983 5.070010185241699 5.391073703765869 2.936540812253952
CurrentTrain: epoch  6, batch     1 | loss: 17.4564459Losses:  14.344053268432617 5.0915656089782715 5.407229900360107 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 14.3440533Losses:  15.760815612971783 4.978000640869141 5.4954729080200195 1.4738349840044975
CurrentTrain: epoch  6, batch     3 | loss: 15.7608156Losses:  13.891010284423828 4.968319892883301 5.276115417480469 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 13.8910103Losses:  22.623090781271458 5.155582427978516 5.223546028137207 7.213196791708469
CurrentTrain: epoch  7, batch     1 | loss: 22.6230908Losses:  18.778060421347618 5.1060333251953125 5.389123916625977 4.37653873860836
CurrentTrain: epoch  7, batch     2 | loss: 18.7780604Losses:  14.494130402803421 5.13977575302124 5.506035327911377 1.4969303905963898
CurrentTrain: epoch  7, batch     3 | loss: 14.4941304Losses:  22.970012366771698 5.071918487548828 5.360021591186523 8.869880378246307
CurrentTrain: epoch  8, batch     0 | loss: 22.9700124Losses:  20.109888967126608 5.146450042724609 5.280118942260742 5.0747365318238735
CurrentTrain: epoch  8, batch     1 | loss: 20.1098890Losses:  15.484120573848486 5.025588035583496 5.21107816696167 1.6519577167928219
CurrentTrain: epoch  8, batch     2 | loss: 15.4841206Losses:  16.52446758747101 5.2121052742004395 5.464457035064697 1.398422360420227
CurrentTrain: epoch  8, batch     3 | loss: 16.5244676Losses:  17.39175260066986 5.091999053955078 5.312993049621582 2.8567041158676147
CurrentTrain: epoch  9, batch     0 | loss: 17.3917526Losses:  14.985761851072311 5.000173568725586 5.2027907371521 1.4099371135234833
CurrentTrain: epoch  9, batch     1 | loss: 14.9857619Losses:  15.73160159215331 5.099646091461182 5.261982440948486 1.514222975820303
CurrentTrain: epoch  9, batch     2 | loss: 15.7316016Losses:  15.771817103028297 5.139673233032227 5.628979682922363 1.4506415277719498
CurrentTrain: epoch  9, batch     3 | loss: 15.7718171
Losses:  10.470661163330078 4.989051818847656 5.218993186950684 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.4706612Losses:  10.99514102935791 4.893911361694336 5.510732650756836 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 10.9951410Losses:  10.174345016479492 4.52629280090332 5.4579057693481445 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 10.1743450Losses:  10.181781768798828 4.780555725097656 5.2369890213012695 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 10.1817818Losses:  10.688454627990723 4.917380332946777 5.267967700958252 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 10.6884546Losses:  10.99871826171875 5.120820045471191 5.444300651550293 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 10.9987183Losses:  11.09188461303711 4.982453346252441 5.306458473205566 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 11.0918846Losses:  11.027048110961914 4.552922248840332 5.568493843078613 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 11.0270481Losses:  10.490999221801758 4.624166965484619 5.551873207092285 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 10.4909992Losses:  10.436687469482422 4.75748872756958 5.330489635467529 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 10.4366875Losses:  10.655562400817871 4.574688911437988 5.437519073486328 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 10.6555624Losses:  10.470240592956543 4.738120079040527 5.322146415710449 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 10.4702406Losses:  10.70187759399414 4.954366683959961 5.3222832679748535 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 10.7018776Losses:  10.59859848022461 4.964339256286621 5.49165153503418 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 10.5985985Losses:  10.541532516479492 4.8164191246032715 5.5034613609313965 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 10.5415325Losses:  10.419357299804688 4.788697242736816 5.324370384216309 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 10.4193573Losses:  10.173562049865723 4.724033355712891 5.275583267211914 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 10.1735620Losses:  10.527607917785645 4.561913967132568 5.619976043701172 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 10.5276079Losses:  10.762550354003906 5.002336502075195 5.504443168640137 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 10.7625504Losses:  10.091936111450195 4.902244567871094 5.081055164337158 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 10.0919361Losses:  10.130073547363281 4.683110237121582 5.290679931640625 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.1300735Losses:  10.394744873046875 4.866633892059326 5.392983436584473 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 10.3947449Losses:  10.447482109069824 4.800042152404785 5.405981063842773 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 10.4474821Losses:  10.112972259521484 4.600905895233154 5.377040386199951 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 10.1129723Losses:  10.37320327758789 4.894959449768066 5.405898094177246 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 10.3732033Losses:  10.42343521118164 4.898307800292969 5.4588093757629395 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 10.4234352Losses:  10.222295761108398 4.751745700836182 5.375005722045898 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 10.2222958Losses:  10.323460578918457 4.948454856872559 5.221942901611328 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 10.3234606Losses:  10.473182678222656 4.75832462310791 5.558889389038086 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 10.4731827Losses:  9.884184837341309 4.434953212738037 5.344318866729736 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 9.8841848Losses:  10.258613586425781 4.883624076843262 5.2947492599487305 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 10.2586136Losses:  9.917330741882324 4.654729843139648 5.1690778732299805 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.9173307Losses:  10.16019058227539 4.7978196144104 5.22114896774292 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 10.1601906Losses:  10.44373893737793 4.825509071350098 5.441864013671875 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 10.4437389Losses:  9.962112426757812 4.4643049240112305 5.389196395874023 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 9.9621124Losses:  10.418891906738281 4.974945068359375 5.323352813720703 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.4188919Losses:  10.155448913574219 4.702181816101074 5.382813453674316 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 10.1554489Losses:  9.906352996826172 4.398240089416504 5.428290843963623 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 9.9063530Losses:  10.431983947753906 4.900611877441406 5.39450740814209 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 10.4319839Losses:  10.155960083007812 4.649371147155762 5.447117805480957 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 10.1559601Losses:  10.334753036499023 5.080121994018555 5.1964216232299805 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 10.3347530Losses:  10.390403747558594 4.689493179321289 5.62105655670166 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 10.3904037Losses:  9.81634521484375 4.433300971984863 5.3181281089782715 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 9.8163452Losses:  9.991438865661621 4.532118797302246 5.3723859786987305 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 9.9914389Losses:  10.298284530639648 4.854121208190918 5.345847129821777 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 10.2982845Losses:  10.015851020812988 4.568264484405518 5.36567497253418 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 10.0158510Losses:  10.046453475952148 4.596288681030273 5.411186218261719 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 10.0464535Losses:  10.166027069091797 4.725886821746826 5.335278511047363 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 10.1660271Losses:  10.021596908569336 4.587741374969482 5.350384712219238 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 10.0215969Losses:  10.499516487121582 4.966096878051758 5.442867755889893 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 10.4995165
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 39.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 41.35%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 51.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 51.70%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 50.78%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 49.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 51.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 53.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 57.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 59.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 61.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 62.68%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 67.11%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 66.38%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 65.57%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 64.69%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 64.45%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 64.01%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 63.49%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 80.69%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 80.05%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 80.10%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 79.82%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 78.99%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 78.39%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 77.36%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 77.12%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.49%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 75.98%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 74.81%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 74.07%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 73.25%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 73.83%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 73.80%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 73.60%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 73.69%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 73.35%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 73.22%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 72.96%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 72.64%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 71.98%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 71.54%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 70.90%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 70.75%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 70.64%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 70.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 70.22%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 70.02%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 69.57%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 69.04%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 68.58%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 68.07%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 67.57%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 67.24%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 68.70%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 68.06%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 67.57%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 67.14%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 66.72%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 65.89%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 66.77%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 66.47%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 66.13%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 65.85%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 66.35%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 65.81%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 65.54%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 65.36%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 65.25%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  159 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 65.66%   [EVAL] batch:  162 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 65.13%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 64.73%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 64.00%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 63.65%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 63.42%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 63.38%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 63.26%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 63.21%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 62.96%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 62.85%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 62.68%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 62.53%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 62.40%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 62.33%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 62.60%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 62.81%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 62.91%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 63.00%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 63.14%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 64.18%   [EVAL] batch:  194 | acc: 0.00%,  total acc: 63.85%   [EVAL] batch:  195 | acc: 0.00%,  total acc: 63.52%   [EVAL] batch:  196 | acc: 0.00%,  total acc: 63.20%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 62.88%   [EVAL] batch:  198 | acc: 6.25%,  total acc: 62.59%   [EVAL] batch:  199 | acc: 0.00%,  total acc: 62.28%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 62.53%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 62.56%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 62.44%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 62.20%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 61.93%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 61.67%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 61.46%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 61.20%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 61.18%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 62.30%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 62.39%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 62.44%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 62.47%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 62.53%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 62.47%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 63.42%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 64.52%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 64.65%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.01%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 65.16%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 64.98%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 64.77%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 64.57%   [EVAL] batch:  254 | acc: 12.50%,  total acc: 64.36%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 64.14%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 64.07%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 64.09%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 64.11%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 64.08%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 64.09%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 63.92%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 63.75%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 63.65%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 63.50%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 64.10%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 63.95%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 63.88%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 63.83%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 63.74%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 63.56%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 63.29%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 63.07%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 62.87%   [EVAL] batch:  288 | acc: 25.00%,  total acc: 62.74%   [EVAL] batch:  289 | acc: 6.25%,  total acc: 62.54%   [EVAL] batch:  290 | acc: 12.50%,  total acc: 62.37%   [EVAL] batch:  291 | acc: 18.75%,  total acc: 62.22%   [EVAL] batch:  292 | acc: 31.25%,  total acc: 62.12%   [EVAL] batch:  293 | acc: 25.00%,  total acc: 61.99%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 61.99%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 61.97%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 61.97%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 62.02%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 62.04%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 62.04%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 62.29%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 62.54%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 62.66%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 62.97%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 63.20%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 63.34%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 63.15%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 63.05%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 62.95%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 62.89%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 62.87%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 62.97%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 63.06%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 63.14%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.33%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 63.27%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 63.09%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 62.96%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 62.80%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 62.67%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 62.54%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 62.61%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 62.93%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 63.31%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 63.72%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 64.16%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 63.99%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 63.86%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 63.52%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 63.38%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 63.50%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 63.82%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 63.70%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 63.62%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 63.52%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 63.40%   [EVAL] batch:  369 | acc: 31.25%,  total acc: 63.31%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 63.26%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 63.15%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  376 | acc: 0.00%,  total acc: 62.81%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 62.68%   [EVAL] batch:  378 | acc: 0.00%,  total acc: 62.52%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 62.40%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 62.25%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 62.32%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 62.47%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 62.52%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 62.56%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 62.61%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 62.69%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 62.83%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 62.91%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 62.88%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 62.77%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 62.67%   [EVAL] batch:  396 | acc: 12.50%,  total acc: 62.55%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 62.42%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 62.30%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 62.17%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 62.02%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 61.86%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 61.71%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 61.59%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 61.47%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 61.33%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 61.33%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 61.41%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 61.48%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 61.70%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 61.99%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 62.04%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 62.31%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:  422 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 62.51%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 62.57%   [EVAL] batch:  425 | acc: 18.75%,  total acc: 62.47%   [EVAL] batch:  426 | acc: 43.75%,  total acc: 62.43%   [EVAL] batch:  427 | acc: 56.25%,  total acc: 62.41%   [EVAL] batch:  428 | acc: 56.25%,  total acc: 62.40%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 62.40%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 62.38%   [EVAL] batch:  432 | acc: 31.25%,  total acc: 62.31%   [EVAL] batch:  433 | acc: 37.50%,  total acc: 62.26%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 62.26%   [EVAL] batch:  435 | acc: 31.25%,  total acc: 62.18%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 62.14%   [EVAL] batch:  438 | acc: 6.25%,  total acc: 62.02%   [EVAL] batch:  439 | acc: 18.75%,  total acc: 61.92%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 61.83%   [EVAL] batch:  441 | acc: 12.50%,  total acc: 61.72%   [EVAL] batch:  442 | acc: 31.25%,  total acc: 61.65%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 61.60%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 61.64%   [EVAL] batch:  445 | acc: 56.25%,  total acc: 61.63%   [EVAL] batch:  446 | acc: 31.25%,  total acc: 61.56%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 61.57%   [EVAL] batch:  448 | acc: 56.25%,  total acc: 61.55%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 61.58%   [EVAL] batch:  450 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 61.66%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 61.70%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 61.77%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 61.83%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 61.87%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 61.82%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 61.78%   [EVAL] batch:  458 | acc: 43.75%,  total acc: 61.74%   [EVAL] batch:  459 | acc: 31.25%,  total acc: 61.67%   [EVAL] batch:  460 | acc: 31.25%,  total acc: 61.61%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 61.55%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 61.53%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 61.69%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 61.94%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 62.18%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 62.26%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 62.34%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 62.53%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 62.57%   [EVAL] batch:  477 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 62.63%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 62.72%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 62.69%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 62.62%   [EVAL] batch:  483 | acc: 25.00%,  total acc: 62.54%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 62.51%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 62.49%   [EVAL] batch:  486 | acc: 50.00%,  total acc: 62.46%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 62.49%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 62.54%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 62.67%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 62.72%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 62.79%   [EVAL] batch:  494 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:  495 | acc: 12.50%,  total acc: 62.61%   [EVAL] batch:  496 | acc: 25.00%,  total acc: 62.54%   [EVAL] batch:  497 | acc: 31.25%,  total acc: 62.47%   [EVAL] batch:  498 | acc: 37.50%,  total acc: 62.42%   [EVAL] batch:  499 | acc: 56.25%,  total acc: 62.41%   
cur_acc:  ['0.9484', '0.7599', '0.7550', '0.7500', '0.6984', '0.6687', '0.6121', '0.6349']
his_acc:  ['0.9484', '0.8425', '0.7919', '0.7648', '0.7388', '0.6900', '0.6490', '0.6241']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  23.211036294698715 5.628896713256836 5.662745475769043 2.01093253493309
CurrentTrain: epoch  0, batch     0 | loss: 23.2110363Losses:  30.84500989690423 5.678893566131592 5.880683898925781 9.599928949028254
CurrentTrain: epoch  0, batch     1 | loss: 30.8450099Losses:  30.957587033510208 5.644460678100586 5.775139808654785 9.793341428041458
CurrentTrain: epoch  0, batch     2 | loss: 30.9575870Losses:  27.736269734799862 5.606893539428711 5.598197937011719 6.457484029233456
CurrentTrain: epoch  0, batch     3 | loss: 27.7362697Losses:  22.08682333678007 5.57966423034668 5.431665420532227 1.5333717986941338
CurrentTrain: epoch  0, batch     4 | loss: 22.0868233Losses:  26.44071937352419 5.5811686515808105 5.693641662597656 5.724721677601337
CurrentTrain: epoch  0, batch     5 | loss: 26.4407194Losses:  26.953641023486853 5.842528343200684 5.798534393310547 5.781888093799353
CurrentTrain: epoch  0, batch     6 | loss: 26.9536410Losses:  21.35647964477539 5.583728313446045 5.674598693847656 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.3564796Losses:  22.806812692433596 5.719076633453369 5.485213279724121 1.599808145314455
CurrentTrain: epoch  0, batch     8 | loss: 22.8068127Losses:  24.929938536137342 5.589975357055664 5.687215328216553 3.6949817948043346
CurrentTrain: epoch  0, batch     9 | loss: 24.9299385Losses:  26.171562146395445 5.709018230438232 5.707873821258545 5.867256116122007
CurrentTrain: epoch  0, batch    10 | loss: 26.1715621Losses:  24.30414677411318 5.667513847351074 5.655784606933594 2.8689508512616158
CurrentTrain: epoch  0, batch    11 | loss: 24.3041468Losses:  29.48176233842969 5.954438209533691 5.735475540161133 8.721733499318361
CurrentTrain: epoch  0, batch    12 | loss: 29.4817623Losses:  21.023956298828125 5.7255706787109375 5.647498607635498 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 21.0239563Losses:  22.834054473787546 5.663617134094238 5.6878981590271 1.8702960051596165
CurrentTrain: epoch  0, batch    14 | loss: 22.8340545Losses:  25.673665542155504 5.721979141235352 5.7900471687316895 3.71993400529027
CurrentTrain: epoch  0, batch    15 | loss: 25.6736655Losses:  23.994081865996122 5.672430992126465 5.6648945808410645 3.7704261653125286
CurrentTrain: epoch  0, batch    16 | loss: 23.9940819Losses:  23.427525226026773 5.537076950073242 5.377054214477539 3.2020499147474766
CurrentTrain: epoch  0, batch    17 | loss: 23.4275252Losses:  22.11674278974533 5.765744209289551 5.735254287719727 1.5163056254386902
CurrentTrain: epoch  0, batch    18 | loss: 22.1167428Losses:  19.990480422973633 5.674785614013672 5.634363651275635 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.9904804Losses:  30.950670816004276 5.643714904785156 5.5870771408081055 10.693400003015995
CurrentTrain: epoch  0, batch    20 | loss: 30.9506708Losses:  27.35566458106041 5.6540937423706055 5.594053268432617 7.107392638921738
CurrentTrain: epoch  0, batch    21 | loss: 27.3556646Losses:  20.003753662109375 5.608014106750488 5.607047080993652 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 20.0037537Losses:  22.31281555071473 5.685424327850342 5.462690353393555 1.4428108967840672
CurrentTrain: epoch  0, batch    23 | loss: 22.3128156Losses:  21.314102172851562 5.680267810821533 5.3730082511901855 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 21.3141022Losses:  19.72110939025879 5.721857070922852 5.6093597412109375 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 19.7211094Losses:  31.539061587303877 5.849043846130371 5.588066101074219 10.953476946800947
CurrentTrain: epoch  0, batch    26 | loss: 31.5390616Losses:  19.072742462158203 5.578080177307129 5.58509635925293 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 19.0727425Losses:  22.262882810086012 5.7295613288879395 5.562656879425049 1.6526647619903088
CurrentTrain: epoch  0, batch    28 | loss: 22.2628828Losses:  22.484080515801907 5.845640182495117 5.526108264923096 3.068051539361477
CurrentTrain: epoch  0, batch    29 | loss: 22.4840805Losses:  34.587990164756775 5.567665100097656 5.372389316558838 14.788835883140564
CurrentTrain: epoch  0, batch    30 | loss: 34.5879902Losses:  30.63640173524618 5.8183746337890625 5.79283332824707 9.78235586732626
CurrentTrain: epoch  0, batch    31 | loss: 30.6364017Losses:  26.966711830347776 5.713884353637695 5.480216026306152 6.828753303736448
CurrentTrain: epoch  0, batch    32 | loss: 26.9667118Losses:  26.438011087477207 5.548859596252441 5.477012634277344 5.51242820173502
CurrentTrain: epoch  0, batch    33 | loss: 26.4380111Losses:  20.134904861450195 5.734930038452148 5.574141025543213 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 20.1349049Losses:  22.689498603343964 5.715464115142822 5.436886787414551 2.1519543528556824
CurrentTrain: epoch  0, batch    35 | loss: 22.6894986Losses:  19.414554595947266 5.6448211669921875 5.549976348876953 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 19.4145546Losses:  22.30963486433029 5.588559627532959 5.589852809906006 3.0336567759513855
CurrentTrain: epoch  0, batch    37 | loss: 22.3096349Losses:  24.443216256797314 5.510317802429199 5.293537139892578 4.834657602012157
CurrentTrain: epoch  0, batch    38 | loss: 24.4432163Losses:  21.673424504697323 5.500214576721191 5.463418960571289 2.8716495260596275
CurrentTrain: epoch  0, batch    39 | loss: 21.6734245Losses:  24.36620767042041 5.446389198303223 5.202151298522949 5.419014524668455
CurrentTrain: epoch  0, batch    40 | loss: 24.3662077Losses:  24.28004080057144 5.468899726867676 5.255360126495361 6.045009672641754
CurrentTrain: epoch  0, batch    41 | loss: 24.2800408Losses:  21.371128022670746 5.676007270812988 5.393162727355957 2.4320563673973083
CurrentTrain: epoch  0, batch    42 | loss: 21.3711280Losses:  20.63542688637972 5.432588577270508 5.425693988800049 1.4686197116971016
CurrentTrain: epoch  0, batch    43 | loss: 20.6354269Losses:  24.860481005162 5.6388092041015625 5.464890003204346 4.517055254429579
CurrentTrain: epoch  0, batch    44 | loss: 24.8604810Losses:  26.590212505310774 5.658520698547363 5.337881565093994 6.430332820862532
CurrentTrain: epoch  0, batch    45 | loss: 26.5902125Losses:  22.358900655061007 5.531910419464111 5.420835971832275 3.1782652027904987
CurrentTrain: epoch  0, batch    46 | loss: 22.3589007Losses:  23.539551328867674 5.750095367431641 5.392246246337891 4.513628553599119
CurrentTrain: epoch  0, batch    47 | loss: 23.5395513Losses:  24.700194351375103 5.555861949920654 5.285001277923584 4.6056661531329155
CurrentTrain: epoch  0, batch    48 | loss: 24.7001944Losses:  21.035585667937994 5.523679256439209 5.335362911224365 2.952209737151861
CurrentTrain: epoch  0, batch    49 | loss: 21.0355857Losses:  20.47604751586914 5.744485855102539 5.4290876388549805 -0.0
CurrentTrain: epoch  0, batch    50 | loss: 20.4760475Losses:  20.400261979550123 5.4919209480285645 5.3359150886535645 1.5516902022063732
CurrentTrain: epoch  0, batch    51 | loss: 20.4002620Losses:  21.34727507457137 5.575251579284668 5.3044633865356445 3.0579951368272305
CurrentTrain: epoch  0, batch    52 | loss: 21.3472751Losses:  21.572571888566017 5.3831071853637695 5.2655816078186035 3.3830863386392593
CurrentTrain: epoch  0, batch    53 | loss: 21.5725719Losses:  24.636582657694817 5.643779754638672 5.512739181518555 5.3157017678022385
CurrentTrain: epoch  0, batch    54 | loss: 24.6365827Losses:  23.775126107037067 5.420647621154785 5.381396770477295 3.0015169456601143
CurrentTrain: epoch  0, batch    55 | loss: 23.7751261Losses:  20.573753356933594 5.682286739349365 5.361747741699219 -0.0
CurrentTrain: epoch  0, batch    56 | loss: 20.5737534Losses:  19.828486025333405 5.4561591148376465 5.431285858154297 1.4194474816322327
CurrentTrain: epoch  0, batch    57 | loss: 19.8284860Losses:  25.924757957458496 5.73165225982666 5.391275405883789 5.730313301086426
CurrentTrain: epoch  0, batch    58 | loss: 25.9247580Losses:  18.995939254760742 5.4109697341918945 5.298977375030518 -0.0
CurrentTrain: epoch  0, batch    59 | loss: 18.9959393Losses:  28.52473470568657 5.425304889678955 5.405381202697754 9.087566584348679
CurrentTrain: epoch  0, batch    60 | loss: 28.5247347Losses:  28.732301082462072 5.652193069458008 5.318154335021973 9.717645015567541
CurrentTrain: epoch  0, batch    61 | loss: 28.7323011Losses:  20.644448399543762 5.55782413482666 5.336335182189941 1.4995490312576294
CurrentTrain: epoch  0, batch    62 | loss: 20.6444484Losses:  18.79250717163086 5.620651721954346 5.51902961730957 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 18.7925072Losses:  19.383769989013672 5.508216857910156 5.290338039398193 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 19.3837700Losses:  20.65422976296395 5.59880256652832 5.460023880004883 2.0035682944580913
CurrentTrain: epoch  1, batch     2 | loss: 20.6542298Losses:  25.188036248087883 5.767030715942383 5.423647880554199 6.318882271647453
CurrentTrain: epoch  1, batch     3 | loss: 25.1880362Losses:  19.72511339187622 5.573388576507568 5.351455211639404 1.7427353858947754
CurrentTrain: epoch  1, batch     4 | loss: 19.7251134Losses:  19.10573387145996 5.525146484375 5.324804306030273 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.1057339Losses:  26.149551145732403 5.37651252746582 5.418829441070557 7.063949339091778
CurrentTrain: epoch  1, batch     6 | loss: 26.1495511Losses:  21.58964042365551 5.446225166320801 5.242234230041504 4.283983990550041
CurrentTrain: epoch  1, batch     7 | loss: 21.5896404Losses:  20.339754574932158 5.634828567504883 5.236539840698242 1.8790421420708299
CurrentTrain: epoch  1, batch     8 | loss: 20.3397546Losses:  23.857793912291527 5.595004558563232 5.398799419403076 4.436609372496605
CurrentTrain: epoch  1, batch     9 | loss: 23.8577939Losses:  18.0780029296875 5.559260368347168 5.3756608963012695 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 18.0780029Losses:  28.285020161420107 5.723057746887207 5.426088809967041 9.162623692303896
CurrentTrain: epoch  1, batch    11 | loss: 28.2850202Losses:  21.731832776218653 5.470464706420898 5.424461364746094 3.227566037327051
CurrentTrain: epoch  1, batch    12 | loss: 21.7318328Losses:  22.99223840981722 5.3299455642700195 5.35893440246582 4.773181326687336
CurrentTrain: epoch  1, batch    13 | loss: 22.9922384Losses:  18.347942352294922 5.536945343017578 5.418058395385742 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 18.3479424Losses:  24.042354676872492 5.407108306884766 5.239038944244385 6.218706224113703
CurrentTrain: epoch  1, batch    15 | loss: 24.0423547Losses:  20.92330700159073 5.370044231414795 5.358590126037598 2.871261179447174
CurrentTrain: epoch  1, batch    16 | loss: 20.9233070Losses:  20.637488041073084 5.537232398986816 5.2286481857299805 2.9451815225183964
CurrentTrain: epoch  1, batch    17 | loss: 20.6374880Losses:  20.343877404928207 5.600398063659668 5.542913436889648 1.505797952413559
CurrentTrain: epoch  1, batch    18 | loss: 20.3438774Losses:  24.11947689950466 5.60728645324707 5.143840789794922 4.866631135344505
CurrentTrain: epoch  1, batch    19 | loss: 24.1194769Losses:  17.69113540649414 5.4231133460998535 5.254422187805176 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 17.6911354Losses:  24.503706708550453 5.464909553527832 5.4838786125183105 6.495154157280922
CurrentTrain: epoch  1, batch    21 | loss: 24.5037067Losses:  21.24486616253853 5.516265392303467 5.23604679107666 3.0435226261615753
CurrentTrain: epoch  1, batch    22 | loss: 21.2448662Losses:  21.053292635828257 5.519478797912598 5.356880187988281 2.842849139124155
CurrentTrain: epoch  1, batch    23 | loss: 21.0532926Losses:  37.67148053646088 5.620536804199219 5.274635314941406 19.229652762413025
CurrentTrain: epoch  1, batch    24 | loss: 37.6714805Losses:  17.98706817626953 5.382502555847168 5.350170135498047 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.9870682Losses:  17.61501121520996 5.539069175720215 5.317047119140625 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 17.6150112Losses:  18.160175323486328 5.446385383605957 5.396129608154297 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 18.1601753Losses:  23.35114574432373 5.447722434997559 5.1960577964782715 4.619532585144043
CurrentTrain: epoch  1, batch    28 | loss: 23.3511457Losses:  24.27909580245614 5.460758209228516 5.132272243499756 5.560368690639734
CurrentTrain: epoch  1, batch    29 | loss: 24.2790958Losses:  19.019613824784756 5.329610824584961 5.388609886169434 1.4943452700972557
CurrentTrain: epoch  1, batch    30 | loss: 19.0196138Losses:  29.522757686674595 5.586602210998535 5.024750709533691 11.06764904409647
CurrentTrain: epoch  1, batch    31 | loss: 29.5227577Losses:  26.365692373365164 5.620415687561035 5.0294365882873535 7.651384588330984
CurrentTrain: epoch  1, batch    32 | loss: 26.3656924Losses:  21.018670335412025 5.517764091491699 5.275747299194336 2.467149034142494
CurrentTrain: epoch  1, batch    33 | loss: 21.0186703Losses:  19.63067838549614 5.413967132568359 5.4320759773254395 1.4122984111309052
CurrentTrain: epoch  1, batch    34 | loss: 19.6306784Losses:  23.254671808332205 5.431206226348877 5.2978315353393555 4.924467798322439
CurrentTrain: epoch  1, batch    35 | loss: 23.2546718Losses:  17.12872314453125 5.430024147033691 5.170088291168213 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 17.1287231Losses:  24.86659710109234 5.48426628112793 5.170317649841309 6.311966821551323
CurrentTrain: epoch  1, batch    37 | loss: 24.8665971Losses:  20.843830779194832 5.448822975158691 5.228110313415527 2.1701590567827225
CurrentTrain: epoch  1, batch    38 | loss: 20.8438308Losses:  21.631081987172365 5.317045211791992 5.232805252075195 3.222512651234865
CurrentTrain: epoch  1, batch    39 | loss: 21.6310820Losses:  24.364924907684326 5.360126972198486 5.123293876647949 6.558658123016357
CurrentTrain: epoch  1, batch    40 | loss: 24.3649249Losses:  18.201644897460938 5.440896987915039 5.398194313049316 -0.0
CurrentTrain: epoch  1, batch    41 | loss: 18.2016449Losses:  20.886662259697914 5.393265724182129 5.352883815765381 3.155949369072914
CurrentTrain: epoch  1, batch    42 | loss: 20.8866623Losses:  20.110781960189342 5.345112323760986 5.620710372924805 1.47026377171278
CurrentTrain: epoch  1, batch    43 | loss: 20.1107820Losses:  19.773537695407867 5.509323596954346 5.264366626739502 1.4003782868385315
CurrentTrain: epoch  1, batch    44 | loss: 19.7735377Losses:  17.738727569580078 5.364834785461426 5.423868656158447 -0.0
CurrentTrain: epoch  1, batch    45 | loss: 17.7387276Losses:  17.707237243652344 5.446704864501953 5.312664031982422 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 17.7072372Losses:  19.6848229393363 5.644375324249268 5.332023620605469 1.4754494652152061
CurrentTrain: epoch  1, batch    47 | loss: 19.6848229Losses:  18.14630126953125 5.525293827056885 5.208823204040527 -0.0
CurrentTrain: epoch  1, batch    48 | loss: 18.1463013Losses:  33.20036479085684 5.599222183227539 5.2869415283203125 14.528535567224026
CurrentTrain: epoch  1, batch    49 | loss: 33.2003648Losses:  20.01604199409485 5.332798480987549 5.296098709106445 2.841925859451294
CurrentTrain: epoch  1, batch    50 | loss: 20.0160420Losses:  19.757947199046612 5.492419242858887 5.317654609680176 1.9652264043688774
CurrentTrain: epoch  1, batch    51 | loss: 19.7579472Losses:  18.541151497513056 5.212583065032959 5.511093616485596 1.478659126907587
CurrentTrain: epoch  1, batch    52 | loss: 18.5411515Losses:  16.800321578979492 5.483242511749268 5.414676666259766 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 16.8003216Losses:  20.421535901725292 5.243335723876953 5.322619438171387 2.9074215218424797
CurrentTrain: epoch  1, batch    54 | loss: 20.4215359Losses:  17.308792114257812 5.401836395263672 5.277836799621582 -0.0
CurrentTrain: epoch  1, batch    55 | loss: 17.3087921Losses:  18.859697736799717 5.409076690673828 5.402244567871094 1.4721931591629982
CurrentTrain: epoch  1, batch    56 | loss: 18.8596977Losses:  17.621034622192383 5.552005767822266 5.220922470092773 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 17.6210346Losses:  18.412275314331055 5.2769575119018555 5.359719753265381 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 18.4122753Losses:  20.381749898195267 5.473505973815918 5.287300109863281 3.0477655231952667
CurrentTrain: epoch  1, batch    59 | loss: 20.3817499Losses:  19.45764261484146 5.4865922927856445 5.340670585632324 1.4633761048316956
CurrentTrain: epoch  1, batch    60 | loss: 19.4576426Losses:  19.255334934219718 5.452424049377441 5.259018898010254 1.6698361244052649
CurrentTrain: epoch  1, batch    61 | loss: 19.2553349Losses:  19.492137908935547 5.4833173751831055 5.159711837768555 -0.0
CurrentTrain: epoch  1, batch    62 | loss: 19.4921379Losses:  20.957022793591022 5.504241943359375 5.215703964233398 4.351275570690632
CurrentTrain: epoch  2, batch     0 | loss: 20.9570228Losses:  22.387009628117085 5.510913372039795 5.128227233886719 4.673245437443256
CurrentTrain: epoch  2, batch     1 | loss: 22.3870096Losses:  16.791706085205078 5.205826282501221 5.421702861785889 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 16.7917061Losses:  23.22528061643243 5.619237899780273 5.228593826293945 5.984832618385553
CurrentTrain: epoch  2, batch     3 | loss: 23.2252806Losses:  20.59294931590557 5.301613807678223 5.316465377807617 4.2916340082883835
CurrentTrain: epoch  2, batch     4 | loss: 20.5929493Losses:  17.436098515987396 5.229225158691406 5.441164016723633 1.4370169043540955
CurrentTrain: epoch  2, batch     5 | loss: 17.4360985Losses:  19.370418399572372 5.365872383117676 5.135392189025879 1.4267614781856537
CurrentTrain: epoch  2, batch     6 | loss: 19.3704184Losses:  18.47271853685379 5.304142951965332 5.086940765380859 1.4461644291877747
CurrentTrain: epoch  2, batch     7 | loss: 18.4727185Losses:  21.237190023064613 5.4673967361450195 5.235195159912109 4.3291890770196915
CurrentTrain: epoch  2, batch     8 | loss: 21.2371900Losses:  20.05641945451498 5.456640243530273 5.21343994140625 3.000251851975918
CurrentTrain: epoch  2, batch     9 | loss: 20.0564195Losses:  16.877546310424805 5.291598320007324 5.384520053863525 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 16.8775463Losses:  23.571230456233025 5.6569976806640625 5.0941691398620605 4.792973086237907
CurrentTrain: epoch  2, batch    11 | loss: 23.5712305Losses:  21.130873166024685 5.273629188537598 5.328573226928711 4.487856350839138
CurrentTrain: epoch  2, batch    12 | loss: 21.1308732Losses:  21.00056215748191 5.257167816162109 5.144413948059082 4.862397637218237
CurrentTrain: epoch  2, batch    13 | loss: 21.0005622Losses:  18.034440957009792 5.3738789558410645 5.229373931884766 1.4853648766875267
CurrentTrain: epoch  2, batch    14 | loss: 18.0344410Losses:  17.224624633789062 5.311679840087891 5.183337211608887 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 17.2246246Losses:  25.32897500693798 5.409796714782715 5.139861583709717 7.584180161356926
CurrentTrain: epoch  2, batch    16 | loss: 25.3289750Losses:  25.62223494797945 5.1872687339782715 5.262561798095703 9.162453301250935
CurrentTrain: epoch  2, batch    17 | loss: 25.6222349Losses:  17.27033805847168 5.361534118652344 5.2558159828186035 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 17.2703381Losses:  20.814670838415623 5.295079231262207 5.286798000335693 3.073303498327732
CurrentTrain: epoch  2, batch    19 | loss: 20.8146708Losses:  17.521526336669922 5.31879997253418 5.135435104370117 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 17.5215263Losses:  19.734894156455994 5.068267345428467 5.417124271392822 2.8905452489852905
CurrentTrain: epoch  2, batch    21 | loss: 19.7348942Losses:  19.98269248008728 5.3765668869018555 5.221499443054199 2.897186040878296
CurrentTrain: epoch  2, batch    22 | loss: 19.9826925Losses:  18.121191442012787 5.322750091552734 5.258641242980957 1.4189209342002869
CurrentTrain: epoch  2, batch    23 | loss: 18.1211914Losses:  18.704601034522057 5.317086696624756 5.051966667175293 1.887214407324791
CurrentTrain: epoch  2, batch    24 | loss: 18.7046010Losses:  19.744899429380894 5.4740166664123535 5.084352493286133 2.945001281797886
CurrentTrain: epoch  2, batch    25 | loss: 19.7448994Losses:  19.189923375844955 5.463062763214111 5.167042255401611 1.4429217278957367
CurrentTrain: epoch  2, batch    26 | loss: 19.1899234Losses:  21.527864273637533 5.251636505126953 5.3598480224609375 4.409326370805502
CurrentTrain: epoch  2, batch    27 | loss: 21.5278643Losses:  19.64263454079628 5.349483966827393 5.299063682556152 2.991835743188858
CurrentTrain: epoch  2, batch    28 | loss: 19.6426345Losses:  17.689897932112217 5.291232585906982 5.177562713623047 1.426836408674717
CurrentTrain: epoch  2, batch    29 | loss: 17.6898979Losses:  21.046583734452724 5.4515886306762695 5.238157749176025 3.147121988236904
CurrentTrain: epoch  2, batch    30 | loss: 21.0465837Losses:  23.419197149574757 5.172148704528809 5.256429672241211 6.4064217284321785
CurrentTrain: epoch  2, batch    31 | loss: 23.4191971Losses:  20.064191550016403 5.405622959136963 5.178847312927246 2.9243485629558563
CurrentTrain: epoch  2, batch    32 | loss: 20.0641916Losses:  20.56757638603449 5.212302207946777 5.33665132522583 2.9915780797600746
CurrentTrain: epoch  2, batch    33 | loss: 20.5675764Losses:  18.197621822357178 5.277337074279785 5.342955589294434 1.426938533782959
CurrentTrain: epoch  2, batch    34 | loss: 18.1976218Losses:  18.94889533519745 5.326324939727783 5.361565113067627 2.852906107902527
CurrentTrain: epoch  2, batch    35 | loss: 18.9488953Losses:  18.34104347229004 5.3422346115112305 5.2482008934021 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 18.3410435Losses:  21.548040717840195 5.301852226257324 5.13840913772583 4.813402503728867
CurrentTrain: epoch  2, batch    37 | loss: 21.5480407Losses:  18.211413584649563 5.23431396484375 5.15653657913208 1.6838771924376488
CurrentTrain: epoch  2, batch    38 | loss: 18.2114136Losses:  22.144220806658268 5.383388519287109 5.135560989379883 4.819048382341862
CurrentTrain: epoch  2, batch    39 | loss: 22.1442208Losses:  20.53414636105299 5.410712242126465 5.160427093505859 2.9396276995539665
CurrentTrain: epoch  2, batch    40 | loss: 20.5341464Losses:  20.494935680180788 5.372261047363281 5.2717108726501465 2.9325024373829365
CurrentTrain: epoch  2, batch    41 | loss: 20.4949357Losses:  21.31574109196663 5.46076774597168 5.086798667907715 3.193029910326004
CurrentTrain: epoch  2, batch    42 | loss: 21.3157411Losses:  25.556029334664345 5.2636542320251465 5.137977600097656 8.917185798287392
CurrentTrain: epoch  2, batch    43 | loss: 25.5560293Losses:  18.645909786224365 5.315515518188477 5.1788434982299805 1.4127554893493652
CurrentTrain: epoch  2, batch    44 | loss: 18.6459098Losses:  17.99679484963417 5.376721382141113 5.162295341491699 1.4304305613040924
CurrentTrain: epoch  2, batch    45 | loss: 17.9967948Losses:  17.924329850822687 5.267181396484375 5.1699042320251465 1.4783650375902653
CurrentTrain: epoch  2, batch    46 | loss: 17.9243299Losses:  18.037195414304733 5.506239414215088 5.134514331817627 1.3948862254619598
CurrentTrain: epoch  2, batch    47 | loss: 18.0371954Losses:  21.392797224223614 5.375357627868652 5.253448486328125 4.892154447734356
CurrentTrain: epoch  2, batch    48 | loss: 21.3927972Losses:  18.849944561719894 5.332427024841309 5.157491207122803 1.551099270582199
CurrentTrain: epoch  2, batch    49 | loss: 18.8499446Losses:  16.83734893798828 5.387660026550293 5.305993556976318 -0.0
CurrentTrain: epoch  2, batch    50 | loss: 16.8373489Losses:  17.67690282687545 5.250209808349609 5.183380603790283 1.512689646333456
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
torch.Size([16, 768])
torch.Size([16])
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
torch.Size([16, 768])
torch.Size([16, 1, 768])
torch.Size([16, 768])
torch.Size([16, 768])
torch.Size([16])
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
abc
torch.Size([16, 1, 768])
torch.Size([16, 768])
torch.Size([16, 1, 768])
torch.Size([16, 768])
torch.Size([16, 768])
torch.Size([16])
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  17.192419474944472 1.0 1.026576042175293 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 17.1924195Losses:  17.587461162358522 1.0 1.0265520811080933 5.83378379419446
CurrentTrain: epoch  0, batch     1 | loss: 17.5874612Losses:  16.4228732958436 1.0 1.0228309631347656 4.398350514471531
CurrentTrain: epoch  0, batch     2 | loss: 16.4228733Losses:  16.412911385297775 1.0 1.028289794921875 5.156438797712326
CurrentTrain: epoch  0, batch     3 | loss: 16.4129114Losses:  14.650591615587473 1.0 1.0302259922027588 2.517054323107004
CurrentTrain: epoch  0, batch     4 | loss: 14.6505916Losses:  15.163779377937317 1.0 1.0354042053222656 4.397704243659973
CurrentTrain: epoch  0, batch     5 | loss: 15.1637794Losses:  19.99788835272193 1.0 1.0259743928909302 8.293955590575933
CurrentTrain: epoch  0, batch     6 | loss: 19.9978884Losses:  15.680115468800068 1.0 1.0227556228637695 3.8606793954968452
CurrentTrain: epoch  0, batch     7 | loss: 15.6801155Losses:  22.861335568130016 1.0 1.02305269241333 10.490636639297009
CurrentTrain: epoch  0, batch     8 | loss: 22.8613356Losses:  13.89647102355957 1.0 1.0329538583755493 2.890416145324707
CurrentTrain: epoch  0, batch     9 | loss: 13.8964710Losses:  15.011645995080471 1.0 1.030163049697876 3.8475567921996117
CurrentTrain: epoch  0, batch    10 | loss: 15.0116460Losses:  15.04045108705759 1.0 1.0256881713867188 3.060617484152317
CurrentTrain: epoch  0, batch    11 | loss: 15.0404511Losses:  12.13076114654541 1.0 1.0238959789276123 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 12.1307611Losses:  11.999116897583008 1.0 1.0261552333831787 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 11.9991169Losses:  17.52307152748108 1.0 1.030756950378418 5.377593278884888
CurrentTrain: epoch  0, batch    14 | loss: 17.5230715Losses:  16.244352780282497 1.0 1.0289504528045654 5.020184002816677
CurrentTrain: epoch  0, batch    15 | loss: 16.2443528Losses:  15.09966403990984 1.0 1.0233573913574219 3.992292709648609
CurrentTrain: epoch  0, batch    16 | loss: 15.0996640Losses:  15.296422943472862 1.0 1.0286121368408203 3.8241405338048935
CurrentTrain: epoch  0, batch    17 | loss: 15.2964229Losses:  17.049518510699272 1.0 1.0204272270202637 5.07091324031353
CurrentTrain: epoch  0, batch    18 | loss: 17.0495185Losses:  12.982681393623352 1.0 1.0286444425582886 1.434285283088684
CurrentTrain: epoch  0, batch    19 | loss: 12.9826814Losses:  14.579802814871073 1.0 1.0195889472961426 4.266694370657206
CurrentTrain: epoch  0, batch    20 | loss: 14.5798028Losses:  15.74795969761908 1.0 1.0246686935424805 5.039659107103944
CurrentTrain: epoch  0, batch    21 | loss: 15.7479597Losses:  19.692538395524025 1.0 1.0324287414550781 8.697175160050392
CurrentTrain: epoch  0, batch    22 | loss: 19.6925384Losses:  13.002836767584085 1.0 1.0190919637680054 1.4411369003355503
CurrentTrain: epoch  0, batch    23 | loss: 13.0028368Losses:  16.777920093387365 1.0 1.0261831283569336 5.690557803958654
CurrentTrain: epoch  0, batch    24 | loss: 16.7779201Losses:  11.31099796295166 1.0 1.0284337997436523 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 11.3109980Losses:  10.935811042785645 1.0 1.025477647781372 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 10.9358110Losses:  15.902767531573772 1.0 1.018712043762207 4.73857819288969
CurrentTrain: epoch  0, batch    27 | loss: 15.9027675Losses:  15.216676525771618 1.0 1.0185186862945557 4.580381207168102
CurrentTrain: epoch  0, batch    28 | loss: 15.2166765Losses:  10.973305702209473 1.0 1.0212126970291138 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 10.9733057Losses:  9.887908935546875 1.0 1.0277019739151 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 9.8879089Losses:  14.95319802686572 1.0 1.0219380855560303 5.145513128489256
CurrentTrain: epoch  0, batch    31 | loss: 14.9531980Losses:  10.369097709655762 1.0 1.0309569835662842 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 10.3690977Losses:  10.80809274315834 1.0 1.0306172370910645 1.4041925966739655
CurrentTrain: epoch  0, batch    33 | loss: 10.8080927Losses:  14.088652692735195 1.0 1.0218092203140259 3.209978185594082
CurrentTrain: epoch  0, batch    34 | loss: 14.0886527Losses:  14.280831199139357 1.0 1.0240453481674194 3.351459365338087
CurrentTrain: epoch  0, batch    35 | loss: 14.2808312Losses:  12.222927071154118 1.0 1.0253969430923462 1.74605081230402
CurrentTrain: epoch  0, batch    36 | loss: 12.2229271Losses:  11.761690467596054 1.0 1.018921136856079 1.425600379705429
CurrentTrain: epoch  0, batch    37 | loss: 11.7616905Losses:  9.8692626953125 1.0 1.0169897079467773 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 9.8692627Losses:  16.329235532321036 1.0 1.0154472589492798 5.1815647864714265
CurrentTrain: epoch  0, batch    39 | loss: 16.3292355Losses:  17.688692174851894 1.0 1.02268385887146 7.297626577317715
CurrentTrain: epoch  0, batch    40 | loss: 17.6886922Losses:  17.14051102474332 1.0 1.024368166923523 5.880058754235506
CurrentTrain: epoch  0, batch    41 | loss: 17.1405110Losses:  14.500106375664473 1.0 1.0253863334655762 3.772460501641035
CurrentTrain: epoch  0, batch    42 | loss: 14.5001064Losses:  16.698418712243438 1.0 1.0252721309661865 6.588014697656035
CurrentTrain: epoch  0, batch    43 | loss: 16.6984187Losses:  12.122045677155256 1.0 1.0212674140930176 1.8386689834296703
CurrentTrain: epoch  0, batch    44 | loss: 12.1220457Losses:  9.816651344299316 1.0 1.0221519470214844 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 9.8166513Losses:  15.375070713460445 1.0 1.0246086120605469 4.810283802449703
CurrentTrain: epoch  0, batch    46 | loss: 15.3750707Losses:  15.623013563454151 1.0 1.0273334980010986 4.955938406288624
CurrentTrain: epoch  0, batch    47 | loss: 15.6230136Losses:  12.950597295537591 1.0 1.0229451656341553 2.430139074102044
CurrentTrain: epoch  0, batch    48 | loss: 12.9505973Losses:  12.359026070684195 1.0 1.024756908416748 1.5207692347466946
CurrentTrain: epoch  0, batch    49 | loss: 12.3590261Losses:  18.990724105387926 1.0 1.019980549812317 8.494250793009996
CurrentTrain: epoch  0, batch    50 | loss: 18.9907241Losses:  13.75281685963273 1.0 1.0216164588928223 4.650602046400309
CurrentTrain: epoch  0, batch    51 | loss: 13.7528169Losses:  11.040108680725098 1.0 1.0228930711746216 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 11.0401087Losses:  15.613684474490583 1.0 1.0238550901412964 5.366787730716169
CurrentTrain: epoch  0, batch    53 | loss: 15.6136845Losses:  17.002685721963644 1.0 1.0345380306243896 7.209957297891378
CurrentTrain: epoch  0, batch    54 | loss: 17.0026857Losses:  10.967094328254461 1.0 1.0233633518218994 1.5426014922559261
CurrentTrain: epoch  0, batch    55 | loss: 10.9670943Losses:  22.55336942523718 1.0 1.025974154472351 12.12082090228796
CurrentTrain: epoch  0, batch    56 | loss: 22.5533694Losses:  12.45108700543642 1.0 1.0277448892593384 2.980901725590229
CurrentTrain: epoch  0, batch    57 | loss: 12.4510870Losses:  13.534987658262253 1.0 1.0238080024719238 3.7401516139507294
CurrentTrain: epoch  0, batch    58 | loss: 13.5349877Losses:  11.613822173327208 1.0 1.0141708850860596 1.5096532814204693
CurrentTrain: epoch  0, batch    59 | loss: 11.6138222Losses:  25.275659680366516 1.0 1.0290992259979248 15.954377293586731
CurrentTrain: epoch  0, batch    60 | loss: 25.2756597Losses:  13.68936662375927 1.0 1.0138849020004272 4.607813164591789
CurrentTrain: epoch  0, batch    61 | loss: 13.6893666Losses:  11.834227994084358 1.0 1.0327508449554443 1.6166462451219559
CurrentTrain: epoch  0, batch    62 | loss: 11.8342280Losses:  10.194100379943848 1.0 1.0180596113204956 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 10.1941004Losses:  13.528322962112725 1.0 1.0207468271255493 4.155876901932061
CurrentTrain: epoch  1, batch     1 | loss: 13.5283230Losses:  15.897706054151058 1.0 1.021178126335144 6.606991790235043
CurrentTrain: epoch  1, batch     2 | loss: 15.8977061Losses:  11.05692209303379 1.0 1.0288848876953125 1.484260693192482
CurrentTrain: epoch  1, batch     3 | loss: 11.0569221Losses:  11.824328977614641 1.0 1.0199394226074219 1.8521953411400318
CurrentTrain: epoch  1, batch     4 | loss: 11.8243290Losses:  10.067302405834198 1.0 1.0301843881607056 1.4000021815299988
CurrentTrain: epoch  1, batch     5 | loss: 10.0673024Losses:  9.812389373779297 1.0 1.0247055292129517 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 9.8123894Losses:  11.525358449667692 1.0 1.0291873216629028 1.6779911629855633
CurrentTrain: epoch  1, batch     7 | loss: 11.5253584Losses:  10.898007586598396 1.0 1.0212798118591309 1.582804873585701
CurrentTrain: epoch  1, batch     8 | loss: 10.8980076Losses:  15.577697090804577 1.0 1.0238770246505737 6.567123703658581
CurrentTrain: epoch  1, batch     9 | loss: 15.5776971Losses:  15.392481036484241 1.0 1.022007703781128 5.21372527629137
CurrentTrain: epoch  1, batch    10 | loss: 15.3924810Losses:  12.58686127141118 1.0 1.0249366760253906 3.17673172429204
CurrentTrain: epoch  1, batch    11 | loss: 12.5868613Losses:  12.200194276869297 1.0 1.0212056636810303 2.8574284687638283
CurrentTrain: epoch  1, batch    12 | loss: 12.2001943Losses:  9.023887634277344 1.0 1.0275638103485107 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 9.0238876Losses:  13.899133998900652 1.0 1.018017053604126 3.664005596190691
CurrentTrain: epoch  1, batch    14 | loss: 13.8991340Losses:  9.483871459960938 1.0 1.017981767654419 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 9.4838715Losses:  13.402385301887989 1.0 1.023606777191162 3.107472963631153
CurrentTrain: epoch  1, batch    16 | loss: 13.4023853Losses:  14.66988379508257 1.0 1.0320940017700195 4.368630476295948
CurrentTrain: epoch  1, batch    17 | loss: 14.6698838Losses:  15.106907287612557 1.0 1.0278403759002686 5.996664443984628
CurrentTrain: epoch  1, batch    18 | loss: 15.1069073Losses:  9.447656631469727 1.0 1.0205326080322266 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 9.4476566Losses:  9.245767593383789 1.0 1.0270094871520996 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 9.2457676Losses:  19.784995019435883 1.0 1.0208910703659058 9.720771729946136
CurrentTrain: epoch  1, batch    21 | loss: 19.7849950Losses:  9.637836456298828 1.0 1.028249740600586 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 9.6378365Losses:  8.883184432983398 1.0 1.0233416557312012 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 8.8831844Losses:  12.367062546312809 1.0 1.0246226787567139 4.350643135607243
CurrentTrain: epoch  1, batch    24 | loss: 12.3670625Losses:  10.966163970530033 1.0 1.0289232730865479 2.8709710612893105
CurrentTrain: epoch  1, batch    25 | loss: 10.9661640Losses:  9.797602653503418 1.0 1.027661681175232 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 9.7976027Losses:  9.22321891784668 1.0 1.01719331741333 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 9.2232189Losses:  9.507472038269043 1.0 1.0298688411712646 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 9.5074720Losses:  13.363218817859888 1.0 1.0221501588821411 4.319094214588404
CurrentTrain: epoch  1, batch    29 | loss: 13.3632188Losses:  13.335494071245193 1.0 1.0201581716537476 3.5612402260303497
CurrentTrain: epoch  1, batch    30 | loss: 13.3354941Losses:  13.10553688928485 1.0 1.028135061264038 3.27292961999774
CurrentTrain: epoch  1, batch    31 | loss: 13.1055369Losses:  13.62511332333088 1.0 1.024545669555664 4.084431484341621
CurrentTrain: epoch  1, batch    32 | loss: 13.6251133Losses:  8.738666534423828 1.0 1.0207273960113525 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 8.7386665Losses:  8.492913246154785 1.0 1.020240068435669 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 8.4929132Losses:  16.536301635205746 1.0 1.0210912227630615 6.768710158765316
CurrentTrain: epoch  1, batch    35 | loss: 16.5363016Losses:  11.16037368774414 1.0 1.0163562297821045 1.6408329010009766
CurrentTrain: epoch  1, batch    36 | loss: 11.1603737Losses:  9.979327954351902 1.0 1.0319931507110596 1.6637704744935036
CurrentTrain: epoch  1, batch    37 | loss: 9.9793280Losses:  11.320914216339588 1.0 1.0221161842346191 2.9856843426823616
CurrentTrain: epoch  1, batch    38 | loss: 11.3209142Losses:  12.941339030861855 1.0 1.018733263015747 4.641485705971718
CurrentTrain: epoch  1, batch    39 | loss: 12.9413390Losses:  10.199181735515594 1.0 1.0250210762023926 1.4596378207206726
CurrentTrain: epoch  1, batch    40 | loss: 10.1991817Losses:  11.546879667788744 1.0 1.023223876953125 2.969546217471361
CurrentTrain: epoch  1, batch    41 | loss: 11.5468797Losses:  10.842722702771425 1.0 1.0191459655761719 1.8666723258793354
CurrentTrain: epoch  1, batch    42 | loss: 10.8427227Losses:  13.462518185377121 1.0 1.022444486618042 4.580273121595383
CurrentTrain: epoch  1, batch    43 | loss: 13.4625182Losses:  13.768541235476732 1.0 1.030344843864441 3.890700239688158
CurrentTrain: epoch  1, batch    44 | loss: 13.7685412Losses:  12.378891680389643 1.0 1.0305731296539307 2.9730479456484318
CurrentTrain: epoch  1, batch    45 | loss: 12.3788917Losses:  9.629158973693848 1.0 1.0251954793930054 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 9.6291590Losses:  9.099349975585938 1.0 1.0208866596221924 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 9.0993500Losses:  10.781302332878113 1.0 1.0241222381591797 1.4118965864181519
CurrentTrain: epoch  1, batch    48 | loss: 10.7813023Losses:  18.71558027714491 1.0 1.0270774364471436 9.243567757308483
CurrentTrain: epoch  1, batch    49 | loss: 18.7155803Losses:  10.806735694408417 1.0 1.0266690254211426 1.4360730051994324
CurrentTrain: epoch  1, batch    50 | loss: 10.8067357Losses:  9.124698638916016 1.0 1.0193493366241455 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 9.1246986Losses:  15.364828046411276 1.0 1.014343023300171 6.4776038490235806
CurrentTrain: epoch  1, batch    52 | loss: 15.3648280Losses:  8.136736869812012 1.0 1.0196545124053955 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 8.1367369Losses:  11.65313272178173 1.0 1.0237574577331543 3.148274704813957
CurrentTrain: epoch  1, batch    54 | loss: 11.6531327Losses:  11.361692100763321 1.0 1.0195257663726807 2.92185178399086
CurrentTrain: epoch  1, batch    55 | loss: 11.3616921Losses:  9.022684097290039 1.0 1.024322509765625 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 9.0226841Losses:  9.316347122192383 1.0 1.0178416967391968 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 9.3163471Losses:  8.399245262145996 1.0 1.0205657482147217 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 8.3992453Losses:  12.595816820859909 1.0 1.0245317220687866 3.185153216123581
CurrentTrain: epoch  1, batch    59 | loss: 12.5958168Losses:  8.597516059875488 1.0 1.0181949138641357 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 8.5975161Losses:  10.135359764099121 1.0 1.0310618877410889 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 10.1353598Losses:  11.237584315240383 1.0 1.0199097394943237 4.328175745904446
CurrentTrain: epoch  1, batch    62 | loss: 11.2375843Losses:  7.540227890014648 1.0 1.021904468536377 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.5402279Losses:  8.317023277282715 1.0 1.0171730518341064 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 8.3170233Losses:  9.185454934835434 1.0 1.0251810550689697 1.4198146760463715
CurrentTrain: epoch  2, batch     2 | loss: 9.1854549Losses:  13.583022139966488 1.0 1.0173473358154297 3.1917314752936363
CurrentTrain: epoch  2, batch     3 | loss: 13.5830221Losses:  11.018033862113953 1.0 1.0294371843338013 1.4829014539718628
CurrentTrain: epoch  2, batch     4 | loss: 11.0180339Losses:  10.975202269852161 1.0 1.0203040838241577 1.9847771599888802
CurrentTrain: epoch  2, batch     5 | loss: 10.9752023Losses:  8.566804885864258 1.0 1.0205110311508179 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 8.5668049Losses:  13.65746832638979 1.0 1.0262407064437866 4.949513919651508
CurrentTrain: epoch  2, batch     7 | loss: 13.6574683Losses:  11.84274210035801 1.0 1.0224089622497559 3.3969890028238297
CurrentTrain: epoch  2, batch     8 | loss: 11.8427421Losses:  12.813131030648947 1.0 1.0305253267288208 3.77383678779006
CurrentTrain: epoch  2, batch     9 | loss: 12.8131310Losses:  9.870413471013308 1.0 1.0246706008911133 1.4911696203052998
CurrentTrain: epoch  2, batch    10 | loss: 9.8704135Losses:  9.358591429889202 1.0 1.0208954811096191 1.5192836448550224
CurrentTrain: epoch  2, batch    11 | loss: 9.3585914Losses:  14.590181894600391 1.0 1.0166829824447632 5.937580652534962
CurrentTrain: epoch  2, batch    12 | loss: 14.5901819Losses:  10.029669016599655 1.0 1.0211819410324097 1.4908840358257294
CurrentTrain: epoch  2, batch    13 | loss: 10.0296690Losses:  12.201445579528809 1.0 1.0197336673736572 2.912930488586426
CurrentTrain: epoch  2, batch    14 | loss: 12.2014456Losses:  10.70441916026175 1.0 1.025973916053772 2.1640682462602854
CurrentTrain: epoch  2, batch    15 | loss: 10.7044192Losses:  11.586118459701538 1.0 1.0186086893081665 2.831063985824585
CurrentTrain: epoch  2, batch    16 | loss: 11.5861185Losses:  11.422529343515635 1.0 1.016690969467163 2.8876410759985447
CurrentTrain: epoch  2, batch    17 | loss: 11.4225293Losses:  18.424095276743174 1.0 1.021926999092102 10.90688717737794
CurrentTrain: epoch  2, batch    18 | loss: 18.4240953Losses:  11.021277910098433 1.0 1.0228227376937866 1.9692130144685507
CurrentTrain: epoch  2, batch    19 | loss: 11.0212779Losses:  7.913025856018066 1.0 1.02131986618042 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 7.9130259Losses:  8.560256958007812 1.0 1.017960786819458 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 8.5602570Losses:  12.775058254599571 1.0 1.0124902725219727 4.358819469809532
CurrentTrain: epoch  2, batch    22 | loss: 12.7750583Losses:  10.563113279640675 1.0 1.028607964515686 1.572539396584034
CurrentTrain: epoch  2, batch    23 | loss: 10.5631133Losses:  9.016771350055933 1.0 1.022982120513916 1.4742942191660404
CurrentTrain: epoch  2, batch    24 | loss: 9.0167714Losses:  13.016773723065853 1.0 1.0181171894073486 4.705404780805111
CurrentTrain: epoch  2, batch    25 | loss: 13.0167737Losses:  10.031442046165466 1.0 1.023086428642273 1.4137300252914429
CurrentTrain: epoch  2, batch    26 | loss: 10.0314420Losses:  10.04725094884634 1.0 1.0280365943908691 1.8140880689024925
CurrentTrain: epoch  2, batch    27 | loss: 10.0472509Losses:  9.530955165624619 1.0 1.0216176509857178 1.408860057592392
CurrentTrain: epoch  2, batch    28 | loss: 9.5309552Losses:  20.03409617394209 1.0 1.0165799856185913 11.90030425041914
CurrentTrain: epoch  2, batch    29 | loss: 20.0340962Losses:  9.741717603057623 1.0 1.0202786922454834 1.4731638692319393
CurrentTrain: epoch  2, batch    30 | loss: 9.7417176Losses:  9.293687101453543 1.0 1.0200875997543335 1.4906127415597439
CurrentTrain: epoch  2, batch    31 | loss: 9.2936871Losses:  10.860797498375177 1.0 1.0265295505523682 2.935414884239435
CurrentTrain: epoch  2, batch    32 | loss: 10.8607975Losses:  11.154927775263786 1.0 1.022009015083313 2.0545869320631027
CurrentTrain: epoch  2, batch    33 | loss: 11.1549278Losses:  13.761894039809704 1.0 1.0168516635894775 5.570316128432751
CurrentTrain: epoch  2, batch    34 | loss: 13.7618940Losses:  15.065021462738514 1.0 1.0224307775497437 5.8950890973210335
CurrentTrain: epoch  2, batch    35 | loss: 15.0650215Losses:  11.263948999345303 1.0 1.0237187147140503 2.855101190507412
CurrentTrain: epoch  2, batch    36 | loss: 11.2639490Losses:  10.638483576476574 1.0 1.0197075605392456 1.807363085448742
CurrentTrain: epoch  2, batch    37 | loss: 10.6384836Losses:  9.617757797241211 1.0 1.0283492803573608 1.405177116394043
CurrentTrain: epoch  2, batch    38 | loss: 9.6177578Losses:  12.10347456485033 1.0 1.0218768119812012 3.206700272858143
CurrentTrain: epoch  2, batch    39 | loss: 12.1034746Losses:  12.280106574296951 1.0 1.0275599956512451 4.438365966081619
CurrentTrain: epoch  2, batch    40 | loss: 12.2801066Losses:  10.978596076369286 1.0 1.0248324871063232 1.4605191797018051
CurrentTrain: epoch  2, batch    41 | loss: 10.9785961Losses:  9.440753072500229 1.0 1.0323166847229004 1.409160703420639
CurrentTrain: epoch  2, batch    42 | loss: 9.4407531Losses:  8.618497848510742 1.0 1.0160887241363525 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 8.6184978Losses:  9.165143731981516 1.0 1.021389365196228 1.4523694552481174
CurrentTrain: epoch  2, batch    44 | loss: 9.1651437Losses:  11.465488566085696 1.0 1.0250802040100098 3.1217185389250517
CurrentTrain: epoch  2, batch    45 | loss: 11.4654886Losses:  7.903749465942383 1.0 1.0234169960021973 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 7.9037495Losses:  8.462395668029785 1.0 1.022078514099121 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 8.4623957Losses:  7.6710710525512695 1.0 1.0231781005859375 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 7.6710711Losses:  10.2232461348176 1.0 1.0214040279388428 1.446387805044651
CurrentTrain: epoch  2, batch    49 | loss: 10.2232461Losses:  17.572768766433 1.0 1.0222172737121582 9.10416467860341
CurrentTrain: epoch  2, batch    50 | loss: 17.5727688Losses:  11.098155211657286 1.0 1.0186851024627686 3.481118392199278
CurrentTrain: epoch  2, batch    51 | loss: 11.0981552Losses:  9.39918446354568 1.0 1.027055025100708 1.678741691634059
CurrentTrain: epoch  2, batch    52 | loss: 9.3991845Losses:  10.725070774555206 1.0 1.0225553512573242 2.824357330799103
CurrentTrain: epoch  2, batch    53 | loss: 10.7250708Losses:  9.814778830856085 1.0 1.0233123302459717 1.4225058816373348
CurrentTrain: epoch  2, batch    54 | loss: 9.8147788Losses:  10.526795536279678 1.0 1.0236274003982544 2.9685846865177155
CurrentTrain: epoch  2, batch    55 | loss: 10.5267955Losses:  9.541590290144086 1.0 1.020153522491455 1.7570734787732363
CurrentTrain: epoch  2, batch    56 | loss: 9.5415903Losses:  8.60908317565918 1.0 1.0276379585266113 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 8.6090832Losses:  8.176712036132812 1.0 1.0227044820785522 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 8.1767120Losses:  19.295646730810404 1.0 1.0233197212219238 11.523498121649027
CurrentTrain: epoch  2, batch    59 | loss: 19.2956467Losses:  8.13142204284668 1.0 1.0249581336975098 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 8.1314220Losses:  11.03575261309743 1.0 1.0241217613220215 1.9554913826286793
CurrentTrain: epoch  2, batch    61 | loss: 11.0357526Losses:  7.7729291915893555 1.0 1.0321810245513916 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 7.7729292Losses:  9.059494227170944 1.0 1.0177949666976929 1.4068810641765594
CurrentTrain: epoch  3, batch     0 | loss: 9.0594942Losses:  9.229522138834 1.0 1.020922064781189 1.4093312323093414
CurrentTrain: epoch  3, batch     1 | loss: 9.2295221Losses:  8.166722297668457 1.0 1.0265982151031494 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 8.1667223Losses:  12.415309194475412 1.0 1.0185219049453735 4.323708776384592
CurrentTrain: epoch  3, batch     3 | loss: 12.4153092Losses:  10.672210838645697 1.0 1.0239317417144775 3.462008621543646
CurrentTrain: epoch  3, batch     4 | loss: 10.6722108Losses:  12.606319759041071 1.0 1.0269217491149902 4.282301280647516
CurrentTrain: epoch  3, batch     5 | loss: 12.6063198Losses:  9.302973061800003 1.0 1.024454951286316 1.4022133648395538
CurrentTrain: epoch  3, batch     6 | loss: 9.3029731Losses:  12.129683449864388 1.0 1.0191659927368164 4.251656487584114
CurrentTrain: epoch  3, batch     7 | loss: 12.1296834Losses:  9.105388164520264 1.0 1.0151842832565308 1.4865574836730957
CurrentTrain: epoch  3, batch     8 | loss: 9.1053882Losses:  8.33393669128418 1.0 1.025620460510254 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 8.3339367Losses:  7.900583744049072 1.0 1.0242382287979126 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 7.9005837Losses:  10.743711113929749 1.0 1.0194008350372314 2.9319454431533813
CurrentTrain: epoch  3, batch    11 | loss: 10.7437111Losses:  11.766540348529816 1.0 1.0130712985992432 4.2373932003974915
CurrentTrain: epoch  3, batch    12 | loss: 11.7665403Losses:  11.937416646629572 1.0 1.0187687873840332 2.886023137718439
CurrentTrain: epoch  3, batch    13 | loss: 11.9374166Losses:  18.80357774347067 1.0 1.0173022747039795 10.483873687684536
CurrentTrain: epoch  3, batch    14 | loss: 18.8035777Losses:  9.615921713411808 1.0 1.0184216499328613 1.4160983338952065
CurrentTrain: epoch  3, batch    15 | loss: 9.6159217Losses:  14.285036414861679 1.0 1.027223825454712 6.0447162091732025
CurrentTrain: epoch  3, batch    16 | loss: 14.2850364Losses:  8.922242853790522 1.0 1.0128719806671143 1.447613451629877
CurrentTrain: epoch  3, batch    17 | loss: 8.9222429Losses:  11.988811254501343 1.0 1.0277941226959229 4.372397184371948
CurrentTrain: epoch  3, batch    18 | loss: 11.9888113Losses:  8.517575941979885 1.0 1.0184502601623535 1.5375673398375511
CurrentTrain: epoch  3, batch    19 | loss: 8.5175759Losses:  11.29248109459877 1.0 1.0134522914886475 4.235197693109512
CurrentTrain: epoch  3, batch    20 | loss: 11.2924811Losses:  11.017162047326565 1.0 1.0254244804382324 3.404724322259426
CurrentTrain: epoch  3, batch    21 | loss: 11.0171620Losses:  14.387521415948868 1.0 1.0220820903778076 6.114326149225235
CurrentTrain: epoch  3, batch    22 | loss: 14.3875214Losses:  9.737920969724655 1.0 1.0199525356292725 1.4289667308330536
CurrentTrain: epoch  3, batch    23 | loss: 9.7379210Losses:  9.728706482797861 1.0 1.0263123512268066 1.4573327340185642
CurrentTrain: epoch  3, batch    24 | loss: 9.7287065Losses:  11.269379641860723 1.0 1.017221450805664 2.938921954482794
CurrentTrain: epoch  3, batch    25 | loss: 11.2693796Losses:  9.958675809204578 1.0 1.0202040672302246 2.8954171612858772
CurrentTrain: epoch  3, batch    26 | loss: 9.9586758Losses:  8.539076652377844 1.0 1.0266008377075195 1.4459927938878536
CurrentTrain: epoch  3, batch    27 | loss: 8.5390767Losses:  13.298481818288565 1.0 1.0221121311187744 4.564499732106924
CurrentTrain: epoch  3, batch    28 | loss: 13.2984818Losses:  13.260121949017048 1.0 1.0228137969970703 5.740845330059528
CurrentTrain: epoch  3, batch    29 | loss: 13.2601219Losses:  7.4956464767456055 1.0 1.020401120185852 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 7.4956465Losses:  7.370552062988281 1.0 1.0191807746887207 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 7.3705521Losses:  13.204334024339914 1.0 1.023776650428772 5.767213586717844
CurrentTrain: epoch  3, batch    32 | loss: 13.2043340Losses:  10.695125576108694 1.0 1.015478491783142 2.8916382752358913
CurrentTrain: epoch  3, batch    33 | loss: 10.6951256Losses:  7.970277786254883 1.0 1.0197434425354004 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 7.9702778Losses:  9.91058780066669 1.0 1.0201141834259033 1.5607075821608305
CurrentTrain: epoch  3, batch    35 | loss: 9.9105878Losses:  7.240386486053467 1.0 1.0220131874084473 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 7.2403865Losses:  7.475803852081299 1.0 1.0195517539978027 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 7.4758039Losses:  10.267112493515015 1.0 1.0205883979797363 2.8195202350616455
CurrentTrain: epoch  3, batch    38 | loss: 10.2671125Losses:  11.815368440002203 1.0 1.0147274732589722 4.2582595609128475
CurrentTrain: epoch  3, batch    39 | loss: 11.8153684Losses:  17.07062078639865 1.0 1.0229370594024658 7.648805867880583
CurrentTrain: epoch  3, batch    40 | loss: 17.0706208Losses:  8.32742691040039 1.0 1.0282001495361328 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 8.3274269Losses:  8.401040524244308 1.0 1.0225021839141846 1.407448261976242
CurrentTrain: epoch  3, batch    42 | loss: 8.4010405Losses:  9.312297411262989 1.0 1.012882947921753 1.8188787177205086
CurrentTrain: epoch  3, batch    43 | loss: 9.3122974Losses:  11.366848580539227 1.0 1.0241475105285645 3.2264085933566093
CurrentTrain: epoch  3, batch    44 | loss: 11.3668486Losses:  10.349610205739737 1.0 1.0224645137786865 2.846665259450674
CurrentTrain: epoch  3, batch    45 | loss: 10.3496102Losses:  11.581202998757362 1.0 1.0276148319244385 3.227448955178261
CurrentTrain: epoch  3, batch    46 | loss: 11.5812030Losses:  9.984305739402771 1.0 1.0133750438690186 2.817952513694763
CurrentTrain: epoch  3, batch    47 | loss: 9.9843057Losses:  7.544322967529297 1.0 1.0136382579803467 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 7.5443230Losses:  18.842194076627493 1.0 1.0289366245269775 10.69802426919341
CurrentTrain: epoch  3, batch    49 | loss: 18.8421941Losses:  7.991816997528076 1.0 1.018566608428955 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 7.9918170Losses:  9.234688192605972 1.0 1.0221567153930664 1.42347851395607
CurrentTrain: epoch  3, batch    51 | loss: 9.2346882Losses:  11.54975701496005 1.0 1.0187876224517822 4.341816913336515
CurrentTrain: epoch  3, batch    52 | loss: 11.5497570Losses:  12.668000373989344 1.0 1.0178751945495605 4.302338752895594
CurrentTrain: epoch  3, batch    53 | loss: 12.6680004Losses:  8.440728396177292 1.0 1.020182490348816 1.395412653684616
CurrentTrain: epoch  3, batch    54 | loss: 8.4407284Losses:  10.525533735752106 1.0 1.0225648880004883 2.820975363254547
CurrentTrain: epoch  3, batch    55 | loss: 10.5255337Losses:  7.209141254425049 1.0 1.025132656097412 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 7.2091413Losses:  7.682734489440918 1.0 1.0256987810134888 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.6827345Losses:  12.619387343525887 1.0 1.022315263748169 5.367107108235359
CurrentTrain: epoch  3, batch    58 | loss: 12.6193873Losses:  17.382805943489075 1.0 1.0274171829223633 9.328478932380676
CurrentTrain: epoch  3, batch    59 | loss: 17.3828059Losses:  7.972629070281982 1.0 1.0223826169967651 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 7.9726291Losses:  13.001165475696325 1.0 1.0257980823516846 4.94868191704154
CurrentTrain: epoch  3, batch    61 | loss: 13.0011655Losses:  8.8252454996109 1.0 1.0321805477142334 1.4020906686782837
CurrentTrain: epoch  3, batch    62 | loss: 8.8252455Losses:  12.029147420078516 1.0 1.0198957920074463 4.323998723179102
CurrentTrain: epoch  4, batch     0 | loss: 12.0291474Losses:  12.505874060094357 1.0 1.0157606601715088 4.935983084142208
CurrentTrain: epoch  4, batch     1 | loss: 12.5058741Losses:  6.9825053215026855 1.0 1.0179351568222046 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.9825053Losses:  10.950441863387823 1.0 1.0248141288757324 4.237955119460821
CurrentTrain: epoch  4, batch     3 | loss: 10.9504419Losses:  16.007682599127293 1.0 1.0189297199249268 8.578890599310398
CurrentTrain: epoch  4, batch     4 | loss: 16.0076826Losses:  8.69005760550499 1.0 1.0229047536849976 1.4016116559505463
CurrentTrain: epoch  4, batch     5 | loss: 8.6900576Losses:  8.666965059936047 1.0 1.0126831531524658 1.4976268336176872
CurrentTrain: epoch  4, batch     6 | loss: 8.6669651Losses:  6.673126220703125 1.0 1.015409231185913 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 6.6731262Losses:  10.84393659606576 1.0 1.0217561721801758 2.8606945611536503
CurrentTrain: epoch  4, batch     8 | loss: 10.8439366Losses:  9.58786054700613 1.0 1.0216716527938843 1.4972614869475365
CurrentTrain: epoch  4, batch     9 | loss: 9.5878605Losses:  11.83006390556693 1.0 1.0213572978973389 4.298471536487341
CurrentTrain: epoch  4, batch    10 | loss: 11.8300639Losses:  10.618626285344362 1.0 1.0117319822311401 2.9069706685841084
CurrentTrain: epoch  4, batch    11 | loss: 10.6186263Losses:  9.087577283382416 1.0 1.0216408967971802 1.4199523329734802
CurrentTrain: epoch  4, batch    12 | loss: 9.0875773Losses:  7.344919204711914 1.0 1.0133739709854126 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 7.3449192Losses:  9.285814259201288 1.0 1.0265299081802368 1.4410514570772648
CurrentTrain: epoch  4, batch    14 | loss: 9.2858143Losses:  9.00100263953209 1.0 1.0155889987945557 1.410291999578476
CurrentTrain: epoch  4, batch    15 | loss: 9.0010026Losses:  11.220200400799513 1.0 1.0221307277679443 3.1982058100402355
CurrentTrain: epoch  4, batch    16 | loss: 11.2202004Losses:  7.182552337646484 1.0 1.0274758338928223 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 7.1825523Losses:  11.794060438871384 1.0 1.0235708951950073 4.3341319262981415
CurrentTrain: epoch  4, batch    18 | loss: 11.7940604Losses:  16.35468389838934 1.0 1.0264736413955688 8.77209236472845
CurrentTrain: epoch  4, batch    19 | loss: 16.3546839Losses:  10.802204124629498 1.0 1.0176472663879395 2.8542246744036674
CurrentTrain: epoch  4, batch    20 | loss: 10.8022041Losses:  10.314535945653915 1.0 1.014080286026001 2.8401316106319427
CurrentTrain: epoch  4, batch    21 | loss: 10.3145359Losses:  8.702998083084822 1.0 1.0272585153579712 1.5201629810035229
CurrentTrain: epoch  4, batch    22 | loss: 8.7029981Losses:  14.161356814205647 1.0 1.0175089836120605 7.06375540047884
CurrentTrain: epoch  4, batch    23 | loss: 14.1613568Losses:  8.586889654397964 1.0 1.0223678350448608 1.4659365713596344
CurrentTrain: epoch  4, batch    24 | loss: 8.5868897Losses:  8.104022979736328 1.0 1.0217647552490234 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 8.1040230Losses:  8.351788580417633 1.0 1.0188148021697998 1.4455023407936096
CurrentTrain: epoch  4, batch    26 | loss: 8.3517886Losses:  10.220530081540346 1.0 1.0238474607467651 2.1307054050266743
CurrentTrain: epoch  4, batch    27 | loss: 10.2205301Losses:  9.012310117483139 1.0 1.0228244066238403 1.4169856011867523
CurrentTrain: epoch  4, batch    28 | loss: 9.0123101Losses:  13.667098715901375 1.0 1.0192437171936035 6.19157238304615
CurrentTrain: epoch  4, batch    29 | loss: 13.6670987Losses:  12.64732412993908 1.0 1.0123982429504395 5.732025191187859
CurrentTrain: epoch  4, batch    30 | loss: 12.6473241Losses:  7.296093940734863 1.0 1.0209304094314575 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 7.2960939Losses:  9.908276572823524 1.0 1.022479772567749 3.0031633526086807
CurrentTrain: epoch  4, batch    32 | loss: 9.9082766Losses:  12.039269503206015 1.0 1.016499638557434 5.0200901590287685
CurrentTrain: epoch  4, batch    33 | loss: 12.0392695Losses:  7.002608299255371 1.0 1.022585153579712 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 7.0026083Losses:  8.925504237413406 1.0 1.0262236595153809 1.4078779518604279
CurrentTrain: epoch  4, batch    35 | loss: 8.9255042Losses:  12.05472368746996 1.0 1.0194835662841797 4.6897339299321175
CurrentTrain: epoch  4, batch    36 | loss: 12.0547237Losses:  11.40973673760891 1.0 1.0193686485290527 3.683200463652611
CurrentTrain: epoch  4, batch    37 | loss: 11.4097367Losses:  6.982888698577881 1.0 1.0181392431259155 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 6.9828887Losses:  11.569551669061184 1.0 1.0233274698257446 4.3588807210326195
CurrentTrain: epoch  4, batch    39 | loss: 11.5695517Losses:  12.077342599630356 1.0 1.0168224573135376 4.698918431997299
CurrentTrain: epoch  4, batch    40 | loss: 12.0773426Losses:  6.993273735046387 1.0 1.0145959854125977 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 6.9932737Losses:  13.513719607144594 1.0 1.0319637060165405 6.211805868893862
CurrentTrain: epoch  4, batch    42 | loss: 13.5137196Losses:  9.763899847865105 1.0 1.0161995887756348 2.860350176692009
CurrentTrain: epoch  4, batch    43 | loss: 9.7638998Losses:  7.36370849609375 1.0 1.020498514175415 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 7.3637085Losses:  8.78427853435278 1.0 1.0311373472213745 1.536550186574459
CurrentTrain: epoch  4, batch    45 | loss: 8.7842785Losses:  10.88699271902442 1.0 1.0200121402740479 2.9949448369443417
CurrentTrain: epoch  4, batch    46 | loss: 10.8869927Losses:  8.624477997422218 1.0 1.0208446979522705 1.6080356985330582
CurrentTrain: epoch  4, batch    47 | loss: 8.6244780Losses:  13.41267692297697 1.0 1.0214958190917969 5.834426037967205
CurrentTrain: epoch  4, batch    48 | loss: 13.4126769Losses:  7.032196998596191 1.0 1.0119609832763672 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 7.0321970Losses:  10.018019441515207 1.0 1.0192439556121826 2.980494264513254
CurrentTrain: epoch  4, batch    50 | loss: 10.0180194Losses:  6.803108215332031 1.0 1.0220885276794434 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 6.8031082Losses:  11.066550374031067 1.0 1.0213086605072021 4.256228566169739
CurrentTrain: epoch  4, batch    52 | loss: 11.0665504Losses:  10.357367008924484 1.0 1.0131676197052002 1.4967856109142303
CurrentTrain: epoch  4, batch    53 | loss: 10.3573670Losses:  7.96012082695961 1.0 1.0202713012695312 1.3919841349124908
CurrentTrain: epoch  4, batch    54 | loss: 7.9601208Losses:  7.571674346923828 1.0 1.0218623876571655 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 7.5716743Losses:  17.409933745861053 1.0 1.0185644626617432 10.160794913768768
CurrentTrain: epoch  4, batch    56 | loss: 17.4099337Losses:  11.248199678957462 1.0 1.0161490440368652 4.3179256692528725
CurrentTrain: epoch  4, batch    57 | loss: 11.2481997Losses:  10.570427853614092 1.0 1.0168052911758423 3.058998543769121
CurrentTrain: epoch  4, batch    58 | loss: 10.5704279Losses:  10.260448575019836 1.0 1.028056263923645 3.078811287879944
CurrentTrain: epoch  4, batch    59 | loss: 10.2604486Losses:  8.544527024030685 1.0 1.015818476676941 1.4142121970653534
CurrentTrain: epoch  4, batch    60 | loss: 8.5445270Losses:  8.800465162843466 1.0 1.0253655910491943 1.4432921968400478
CurrentTrain: epoch  4, batch    61 | loss: 8.8004652Losses:  12.681000649929047 1.0 1.0144615173339844 6.033338487148285
CurrentTrain: epoch  4, batch    62 | loss: 12.6810006Losses:  7.44179630279541 1.0 1.0250093936920166 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.4417963Losses:  9.895099733024836 1.0 1.0216681957244873 2.8931857086718082
CurrentTrain: epoch  5, batch     1 | loss: 9.8950997Losses:  9.934609144926071 1.0 1.0258476734161377 2.7994610965251923
CurrentTrain: epoch  5, batch     2 | loss: 9.9346091Losses:  10.217152886092663 1.0 1.0225517749786377 2.919707588851452
CurrentTrain: epoch  5, batch     3 | loss: 10.2171529Losses:  11.213188748806715 1.0 1.015293836593628 4.301915746182203
CurrentTrain: epoch  5, batch     4 | loss: 11.2131887Losses:  12.558419063687325 1.0 1.0167783498764038 5.807593658566475
CurrentTrain: epoch  5, batch     5 | loss: 12.5584191Losses:  7.432561874389648 1.0 1.0202654600143433 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 7.4325619Losses:  8.549851447343826 1.0 1.0284758806228638 1.4046225845813751
CurrentTrain: epoch  5, batch     7 | loss: 8.5498514Losses:  11.769006796181202 1.0 1.0185613632202148 4.3537860587239265
CurrentTrain: epoch  5, batch     8 | loss: 11.7690068Losses:  13.39504823088646 1.0 1.0199795961380005 6.210922330617905
CurrentTrain: epoch  5, batch     9 | loss: 13.3950482Losses:  10.451579749584198 1.0 1.0156049728393555 2.7986246943473816
CurrentTrain: epoch  5, batch    10 | loss: 10.4515797Losses:  29.190224200487137 1.0 1.0226311683654785 22.07850220799446
CurrentTrain: epoch  5, batch    11 | loss: 29.1902242Losses:  9.891313910484314 1.0 1.0207018852233887 2.8063710927963257
CurrentTrain: epoch  5, batch    12 | loss: 9.8913139Losses:  8.341959059238434 1.0 1.0170986652374268 1.4003086686134338
CurrentTrain: epoch  5, batch    13 | loss: 8.3419591Losses:  8.830483354628086 1.0 1.0234856605529785 1.6975192204117775
CurrentTrain: epoch  5, batch    14 | loss: 8.8304834Losses:  8.826049648225307 1.0 1.02402663230896 1.4932039603590965
CurrentTrain: epoch  5, batch    15 | loss: 8.8260496Losses:  8.457789331674576 1.0 1.0169273614883423 1.4369224607944489
CurrentTrain: epoch  5, batch    16 | loss: 8.4577893Losses:  10.422561168670654 1.0 1.0125669240951538 2.8580188751220703
CurrentTrain: epoch  5, batch    17 | loss: 10.4225612Losses:  11.334151268005371 1.0 1.0224816799163818 4.592679977416992
CurrentTrain: epoch  5, batch    18 | loss: 11.3341513Losses:  8.244347959756851 1.0 1.0208343267440796 1.416665941476822
CurrentTrain: epoch  5, batch    19 | loss: 8.2443480Losses:  9.354668080806732 1.0 1.0187411308288574 2.806759774684906
CurrentTrain: epoch  5, batch    20 | loss: 9.3546681Losses:  12.296474613249302 1.0 1.0209380388259888 5.8057896271348
CurrentTrain: epoch  5, batch    21 | loss: 12.2964746Losses:  9.600358668714762 1.0 1.0148444175720215 3.0266091264784336
CurrentTrain: epoch  5, batch    22 | loss: 9.6003587Losses:  11.010102912783623 1.0 1.017401933670044 3.655271217226982
CurrentTrain: epoch  5, batch    23 | loss: 11.0101029Losses:  9.433951172977686 1.0 1.0170737504959106 2.916823659092188
CurrentTrain: epoch  5, batch    24 | loss: 9.4339512Losses:  22.79933089017868 1.0 1.0175600051879883 15.135828197002411
CurrentTrain: epoch  5, batch    25 | loss: 22.7993309Losses:  9.755162876099348 1.0 1.0154011249542236 2.9860970191657543
CurrentTrain: epoch  5, batch    26 | loss: 9.7551629Losses:  8.692373037338257 1.0 1.0201025009155273 1.4070923328399658
CurrentTrain: epoch  5, batch    27 | loss: 8.6923730Losses:  7.995108045637608 1.0 1.0188583135604858 1.454844392836094
CurrentTrain: epoch  5, batch    28 | loss: 7.9951080Losses:  11.732010334730148 1.0 1.0194910764694214 4.388996571302414
CurrentTrain: epoch  5, batch    29 | loss: 11.7320103Losses:  9.163414165377617 1.0 1.019119381904602 1.5591623038053513
CurrentTrain: epoch  5, batch    30 | loss: 9.1634142Losses:  11.36673179268837 1.0 1.0216583013534546 4.372678428888321
CurrentTrain: epoch  5, batch    31 | loss: 11.3667318Losses:  8.262600004673004 1.0 1.024566411972046 1.4097910523414612
CurrentTrain: epoch  5, batch    32 | loss: 8.2626000Losses:  22.302746891975403 1.0 1.0160081386566162 14.760950207710266
CurrentTrain: epoch  5, batch    33 | loss: 22.3027469Losses:  10.121551744639874 1.0 1.0186691284179688 2.903072588145733
CurrentTrain: epoch  5, batch    34 | loss: 10.1215517Losses:  11.0285902954638 1.0 1.0168858766555786 4.260548684746027
CurrentTrain: epoch  5, batch    35 | loss: 11.0285903Losses:  13.810662318021059 1.0 1.0191596746444702 6.613981295377016
CurrentTrain: epoch  5, batch    36 | loss: 13.8106623Losses:  8.138051453977823 1.0 1.0141856670379639 1.4478363431990147
CurrentTrain: epoch  5, batch    37 | loss: 8.1380515Losses:  8.430704858154058 1.0 1.0239148139953613 1.491492535918951
CurrentTrain: epoch  5, batch    38 | loss: 8.4307049Losses:  8.153077125549316 1.0 1.0166974067687988 1.4124512672424316
CurrentTrain: epoch  5, batch    39 | loss: 8.1530771Losses:  8.179643332958221 1.0 1.016422986984253 1.3978211283683777
CurrentTrain: epoch  5, batch    40 | loss: 8.1796433Losses:  7.36562967300415 1.0 1.0212340354919434 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 7.3656297Losses:  8.931138344109058 1.0 1.02244234085083 1.4669340327382088
CurrentTrain: epoch  5, batch    42 | loss: 8.9311383Losses:  11.577316105365753 1.0 1.0235512256622314 4.275777637958527
CurrentTrain: epoch  5, batch    43 | loss: 11.5773161Losses:  7.097283363342285 1.0 1.016218900680542 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 7.0972834Losses:  6.608120441436768 1.0 1.0158361196517944 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 6.6081204Losses:  9.598753347992897 1.0 1.013033390045166 3.025301828980446
CurrentTrain: epoch  5, batch    46 | loss: 9.5987533Losses:  9.383073896169662 1.0 1.0149832963943481 1.4012499749660492
CurrentTrain: epoch  5, batch    47 | loss: 9.3830739Losses:  10.40300938487053 1.0 1.0144580602645874 2.854776829481125
CurrentTrain: epoch  5, batch    48 | loss: 10.4030094Losses:  8.715959280729294 1.0 1.0227582454681396 1.4001118838787079
CurrentTrain: epoch  5, batch    49 | loss: 8.7159593Losses:  8.685465395450592 1.0 1.0173993110656738 1.3966065049171448
CurrentTrain: epoch  5, batch    50 | loss: 8.6854654Losses:  16.90405859053135 1.0 1.0147082805633545 10.109418049454689
CurrentTrain: epoch  5, batch    51 | loss: 16.9040586Losses:  10.977622613310814 1.0 1.0157698392868042 4.296562775969505
CurrentTrain: epoch  5, batch    52 | loss: 10.9776226Losses:  15.410887777805328 1.0 1.0169477462768555 8.526189386844635
CurrentTrain: epoch  5, batch    53 | loss: 15.4108878Losses:  8.65933645516634 1.0 1.017407774925232 1.6400750949978828
CurrentTrain: epoch  5, batch    54 | loss: 8.6593365Losses:  9.526926845312119 1.0 1.0190154314041138 2.8254941403865814
CurrentTrain: epoch  5, batch    55 | loss: 9.5269268Losses:  10.181654542684555 1.0 1.0144321918487549 2.801254838705063
CurrentTrain: epoch  5, batch    56 | loss: 10.1816545Losses:  7.9340976513922215 1.0 1.0197901725769043 1.4592331685125828
CurrentTrain: epoch  5, batch    57 | loss: 7.9340977Losses:  11.89375240728259 1.0 1.0169665813446045 4.711327861994505
CurrentTrain: epoch  5, batch    58 | loss: 11.8937524Losses:  12.728951156139374 1.0 1.0196539163589478 5.027290046215057
CurrentTrain: epoch  5, batch    59 | loss: 12.7289512Losses:  8.489791169762611 1.0 1.016014575958252 1.5233943611383438
CurrentTrain: epoch  5, batch    60 | loss: 8.4897912Losses:  8.333907838910818 1.0 1.020723819732666 1.4358861409127712
CurrentTrain: epoch  5, batch    61 | loss: 8.3339078Losses:  11.340867165476084 1.0 1.0180740356445312 4.300225380808115
CurrentTrain: epoch  5, batch    62 | loss: 11.3408672Losses:  13.782093621790409 1.0 1.0134562253952026 6.892128564417362
CurrentTrain: epoch  6, batch     0 | loss: 13.7820936Losses:  9.626973450183868 1.0 1.0163769721984863 2.825010120868683
CurrentTrain: epoch  6, batch     1 | loss: 9.6269735Losses:  8.482945214956999 1.0 1.0167689323425293 1.4609835259616375
CurrentTrain: epoch  6, batch     2 | loss: 8.4829452Losses:  8.522916287183762 1.0 1.0159374475479126 1.4665436446666718
CurrentTrain: epoch  6, batch     3 | loss: 8.5229163Losses:  7.971498876810074 1.0 1.0165166854858398 1.42350235581398
CurrentTrain: epoch  6, batch     4 | loss: 7.9714989Losses:  8.76302494853735 1.0 1.0270947217941284 1.557485245168209
CurrentTrain: epoch  6, batch     5 | loss: 8.7630249Losses:  6.805870056152344 1.0 1.0164726972579956 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 6.8058701Losses:  8.136500842869282 1.0 1.0176734924316406 1.4653287008404732
CurrentTrain: epoch  6, batch     7 | loss: 8.1365008Losses:  13.283233866095543 1.0 1.0206282138824463 6.179948076605797
CurrentTrain: epoch  6, batch     8 | loss: 13.2832339Losses:  9.612398106604815 1.0 1.0151453018188477 3.044898945838213
CurrentTrain: epoch  6, batch     9 | loss: 9.6123981Losses:  11.187562223523855 1.0 1.0169289112091064 3.2411539517343044
CurrentTrain: epoch  6, batch    10 | loss: 11.1875622Losses:  10.542004100978374 1.0 1.0118353366851807 3.099986545741558
CurrentTrain: epoch  6, batch    11 | loss: 10.5420041Losses:  8.69459518790245 1.0 1.0144619941711426 1.4567826688289642
CurrentTrain: epoch  6, batch    12 | loss: 8.6945952Losses:  8.291479587554932 1.0 1.0151805877685547 1.3945317268371582
CurrentTrain: epoch  6, batch    13 | loss: 8.2914796Losses:  8.056210912764072 1.0 1.0146058797836304 1.4275859966874123
CurrentTrain: epoch  6, batch    14 | loss: 8.0562109Losses:  10.14885849878192 1.0 1.0187480449676514 2.882276963442564
CurrentTrain: epoch  6, batch    15 | loss: 10.1488585Losses:  6.631568908691406 1.0 1.0153024196624756 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 6.6315689Losses:  9.680339753627777 1.0 1.0165095329284668 2.834774434566498
CurrentTrain: epoch  6, batch    17 | loss: 9.6803398Losses:  8.30787144601345 1.0 1.0179951190948486 1.5156379789113998
CurrentTrain: epoch  6, batch    18 | loss: 8.3078714Losses:  6.777105331420898 1.0 1.0176959037780762 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 6.7771053Losses:  11.673372726887465 1.0 1.0112113952636719 4.54054019972682
CurrentTrain: epoch  6, batch    20 | loss: 11.6733727Losses:  8.401221007108688 1.0 1.0177022218704224 1.4363944232463837
CurrentTrain: epoch  6, batch    21 | loss: 8.4012210Losses:  9.3734231851995 1.0 1.0171432495117188 2.853168573230505
CurrentTrain: epoch  6, batch    22 | loss: 9.3734232Losses:  8.461779713630676 1.0 1.017122745513916 1.4242836236953735
CurrentTrain: epoch  6, batch    23 | loss: 8.4617797Losses:  11.231098525226116 1.0 1.021040678024292 4.322949759662151
CurrentTrain: epoch  6, batch    24 | loss: 11.2310985Losses:  9.771180100739002 1.0 1.0112223625183105 2.8961109593510628
CurrentTrain: epoch  6, batch    25 | loss: 9.7711801Losses:  8.318142421543598 1.0 1.0151941776275635 1.4422764852643013
CurrentTrain: epoch  6, batch    26 | loss: 8.3181424Losses:  8.15310350060463 1.0 1.019132375717163 1.449165016412735
CurrentTrain: epoch  6, batch    27 | loss: 8.1531035Losses:  12.379533302038908 1.0 1.0159313678741455 5.778595458716154
CurrentTrain: epoch  6, batch    28 | loss: 12.3795333Losses:  12.697453953325748 1.0 1.0154383182525635 5.767860867083073
CurrentTrain: epoch  6, batch    29 | loss: 12.6974540Losses:  10.81320270895958 1.0 1.0115454196929932 4.214644283056259
CurrentTrain: epoch  6, batch    30 | loss: 10.8132027Losses:  8.230591177940369 1.0 1.0210515260696411 1.431683897972107
CurrentTrain: epoch  6, batch    31 | loss: 8.2305912Losses:  9.719362866133451 1.0 1.0114949941635132 2.944603096693754
CurrentTrain: epoch  6, batch    32 | loss: 9.7193629Losses:  7.947341233491898 1.0 1.0201480388641357 1.407007485628128
CurrentTrain: epoch  6, batch    33 | loss: 7.9473412Losses:  6.920029163360596 1.0 1.0129915475845337 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 6.9200292Losses:  10.689904361963272 1.0 1.0167597532272339 3.2445951998233795
CurrentTrain: epoch  6, batch    35 | loss: 10.6899044Losses:  9.817646883428097 1.0 1.0142500400543213 2.9344653114676476
CurrentTrain: epoch  6, batch    36 | loss: 9.8176469Losses:  6.973871231079102 1.0 1.0161898136138916 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.9738712Losses:  14.242454938590527 1.0 1.0144236087799072 7.6143326088786125
CurrentTrain: epoch  6, batch    38 | loss: 14.2424549Losses:  6.829075813293457 1.0 1.0235825777053833 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 6.8290758Losses:  6.813699722290039 1.0 1.0166547298431396 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 6.8136997Losses:  7.9437850415706635 1.0 1.0177032947540283 1.4051822125911713
CurrentTrain: epoch  6, batch    41 | loss: 7.9437850Losses:  10.205766826868057 1.0 1.0193027257919312 2.8221289217472076
CurrentTrain: epoch  6, batch    42 | loss: 10.2057668Losses:  9.898693725466728 1.0 1.014267086982727 3.132527992129326
CurrentTrain: epoch  6, batch    43 | loss: 9.8986937Losses:  8.040557112544775 1.0 1.0162975788116455 1.5154916904866695
CurrentTrain: epoch  6, batch    44 | loss: 8.0405571Losses:  8.101817071437836 1.0 1.0117387771606445 1.4130772948265076
CurrentTrain: epoch  6, batch    45 | loss: 8.1018171Losses:  10.21886295080185 1.0 1.0174916982650757 2.3641509413719177
CurrentTrain: epoch  6, batch    46 | loss: 10.2188630Losses:  9.611369617283344 1.0 1.0219659805297852 2.964429385960102
CurrentTrain: epoch  6, batch    47 | loss: 9.6113696Losses:  7.991083238273859 1.0 1.0157054662704468 1.463274572044611
CurrentTrain: epoch  6, batch    48 | loss: 7.9910832Losses:  8.282921969890594 1.0 1.0126739740371704 1.4470974802970886
CurrentTrain: epoch  6, batch    49 | loss: 8.2829220Losses:  8.09299311041832 1.0 1.0140156745910645 1.3972900807857513
CurrentTrain: epoch  6, batch    50 | loss: 8.0929931Losses:  6.690149784088135 1.0 1.020174503326416 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 6.6901498Losses:  6.86164665222168 1.0 1.0159026384353638 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 6.8616467Losses:  10.809059001505375 1.0 1.0166548490524292 4.349583961069584
CurrentTrain: epoch  6, batch    53 | loss: 10.8090590Losses:  6.454522609710693 1.0 1.0205011367797852 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 6.4545226Losses:  10.827508486807346 1.0 1.0106220245361328 4.319321192800999
CurrentTrain: epoch  6, batch    55 | loss: 10.8275085Losses:  8.112960308790207 1.0 1.014890193939209 1.4683828055858612
CurrentTrain: epoch  6, batch    56 | loss: 8.1129603Losses:  9.501185167580843 1.0 1.0172189474105835 2.886932600289583
CurrentTrain: epoch  6, batch    57 | loss: 9.5011852Losses:  8.054936319589615 1.0 1.0147407054901123 1.4594239294528961
CurrentTrain: epoch  6, batch    58 | loss: 8.0549363Losses:  8.399928718805313 1.0 1.0151149034500122 1.4180485308170319
CurrentTrain: epoch  6, batch    59 | loss: 8.3999287Losses:  6.341628551483154 1.0 1.0123707056045532 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 6.3416286Losses:  7.73055362328887 1.0 1.0155937671661377 1.4115657769143581
CurrentTrain: epoch  6, batch    61 | loss: 7.7305536Losses:  10.91153559088707 1.0 1.0104594230651855 4.25509437918663
CurrentTrain: epoch  6, batch    62 | loss: 10.9115356Losses:  8.026132881641388 1.0 1.0148392915725708 1.4546368718147278
CurrentTrain: epoch  7, batch     0 | loss: 8.0261329Losses:  9.451194494962692 1.0 1.0233476161956787 2.8212811648845673
CurrentTrain: epoch  7, batch     1 | loss: 9.4511945Losses:  6.681809425354004 1.0 1.0134823322296143 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 6.6818094Losses:  14.239940829575062 1.0 1.0146235227584839 7.684673495590687
CurrentTrain: epoch  7, batch     3 | loss: 14.2399408Losses:  13.939066097140312 1.0 1.007765769958496 7.575284168124199
CurrentTrain: epoch  7, batch     4 | loss: 13.9390661Losses:  9.315723657608032 1.0 1.0160068273544312 2.829058885574341
CurrentTrain: epoch  7, batch     5 | loss: 9.3157237Losses:  6.660593509674072 1.0 1.0193408727645874 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 6.6605935Losses:  12.181270126253366 1.0 1.0152208805084229 5.690745834261179
CurrentTrain: epoch  7, batch     7 | loss: 12.1812701Losses:  7.915537483990192 1.0 1.0133907794952393 1.4330002143979073
CurrentTrain: epoch  7, batch     8 | loss: 7.9155375Losses:  7.943954937160015 1.0 1.0193480253219604 1.4545760080218315
CurrentTrain: epoch  7, batch     9 | loss: 7.9439549Losses:  11.135435499250889 1.0 1.0137238502502441 4.5475606098771095
CurrentTrain: epoch  7, batch    10 | loss: 11.1354355Losses:  6.686725616455078 1.0 1.0145905017852783 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 6.6867256Losses:  10.245314359664917 1.0 1.015593409538269 2.809197187423706
CurrentTrain: epoch  7, batch    12 | loss: 10.2453144Losses:  7.684660851955414 1.0 1.0164762735366821 1.391539990901947
CurrentTrain: epoch  7, batch    13 | loss: 7.6846609Losses:  9.770319748669863 1.0 1.0155434608459473 3.3395617492496967
CurrentTrain: epoch  7, batch    14 | loss: 9.7703197Losses:  9.501225769519806 1.0 1.0127092599868774 2.810632050037384
CurrentTrain: epoch  7, batch    15 | loss: 9.5012258Losses:  13.873035840690136 1.0 1.0155339241027832 7.234095506370068
CurrentTrain: epoch  7, batch    16 | loss: 13.8730358Losses:  7.849491357803345 1.0 1.0136817693710327 1.3903348445892334
CurrentTrain: epoch  7, batch    17 | loss: 7.8494914Losses:  6.631694793701172 1.0 1.0159919261932373 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 6.6316948Losses:  12.331031389534473 1.0 1.0156564712524414 5.965986795723438
CurrentTrain: epoch  7, batch    19 | loss: 12.3310314Losses:  7.834353297948837 1.0 1.0137569904327393 1.405859798192978
CurrentTrain: epoch  7, batch    20 | loss: 7.8343533Losses:  10.785725202411413 1.0 1.0155861377716064 4.362351980060339
CurrentTrain: epoch  7, batch    21 | loss: 10.7857252Losses:  9.168640702962875 1.0 1.0208899974822998 2.7902704179286957
CurrentTrain: epoch  7, batch    22 | loss: 9.1686407Losses:  6.498188495635986 1.0 1.009206771850586 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 6.4981885Losses:  6.51684045791626 1.0 1.0105279684066772 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 6.5168405Losses:  8.382565136998892 1.0 1.008715033531189 1.4404050074517727
CurrentTrain: epoch  7, batch    25 | loss: 8.3825651Losses:  9.440230250358582 1.0 1.012634515762329 2.84359347820282
CurrentTrain: epoch  7, batch    26 | loss: 9.4402303Losses:  7.819004267454147 1.0 1.0200743675231934 1.4230878055095673
CurrentTrain: epoch  7, batch    27 | loss: 7.8190043Losses:  9.068585485219955 1.0 1.0149167776107788 2.7907415330410004
CurrentTrain: epoch  7, batch    28 | loss: 9.0685855Losses:  7.7562036216259 1.0 1.0163731575012207 1.395119160413742
CurrentTrain: epoch  7, batch    29 | loss: 7.7562036Losses:  6.312626838684082 1.0 1.0158236026763916 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 6.3126268Losses:  13.287674747407436 1.0 1.0109858512878418 7.064167343080044
CurrentTrain: epoch  7, batch    31 | loss: 13.2876747Losses:  9.138798385858536 1.0 1.0130348205566406 2.867602974176407
CurrentTrain: epoch  7, batch    32 | loss: 9.1387984Losses:  14.055606313049793 1.0 1.0134706497192383 7.72626156359911
CurrentTrain: epoch  7, batch    33 | loss: 14.0556063Losses:  10.552034214138985 1.0 1.0089263916015625 4.216013267636299
CurrentTrain: epoch  7, batch    34 | loss: 10.5520342Losses:  6.361244201660156 1.0 1.0128498077392578 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 6.3612442Losses:  11.560285206884146 1.0 1.014976978302002 4.458496209233999
CurrentTrain: epoch  7, batch    36 | loss: 11.5602852Losses:  9.56394013389945 1.0 1.0150189399719238 2.9054938219487667
CurrentTrain: epoch  7, batch    37 | loss: 9.5639401Losses:  9.14195603132248 1.0 1.0110567808151245 2.825813949108124
CurrentTrain: epoch  7, batch    38 | loss: 9.1419560Losses:  6.71689510345459 1.0 1.0173499584197998 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 6.7168951Losses:  9.209433998912573 1.0 1.014646291732788 2.83593412861228
CurrentTrain: epoch  7, batch    40 | loss: 9.2094340Losses:  9.295131146907806 1.0 1.0173561573028564 2.807891309261322
CurrentTrain: epoch  7, batch    41 | loss: 9.2951311Losses:  6.344298839569092 1.0 1.0146757364273071 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 6.3442988Losses:  7.95352840423584 1.0 1.00917649269104 1.4111747741699219
CurrentTrain: epoch  7, batch    43 | loss: 7.9535284Losses:  10.98434142023325 1.0 1.0180656909942627 4.484917439520359
CurrentTrain: epoch  7, batch    44 | loss: 10.9843414Losses:  9.191004008054733 1.0 1.011094093322754 2.8413183391094208
CurrentTrain: epoch  7, batch    45 | loss: 9.1910040Losses:  12.103406250476837 1.0 1.013981580734253 5.6196282505989075
CurrentTrain: epoch  7, batch    46 | loss: 12.1034063Losses:  7.722322434186935 1.0 1.0153231620788574 1.3946108520030975
CurrentTrain: epoch  7, batch    47 | loss: 7.7223224Losses:  7.9312359392642975 1.0 1.0199875831604004 1.4573957026004791
CurrentTrain: epoch  7, batch    48 | loss: 7.9312359Losses:  8.071719884872437 1.0 1.0109524726867676 1.3929235935211182
CurrentTrain: epoch  7, batch    49 | loss: 8.0717199Losses:  7.200922012329102 1.0 1.0179911851882935 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 7.2009220Losses:  8.018409878015518 1.0 1.0163441896438599 1.4387275278568268
CurrentTrain: epoch  7, batch    51 | loss: 8.0184099Losses:  10.534767717123032 1.0 1.0110058784484863 4.2688256204128265
CurrentTrain: epoch  7, batch    52 | loss: 10.5347677Losses:  7.628804415464401 1.0 1.0084013938903809 1.413553923368454
CurrentTrain: epoch  7, batch    53 | loss: 7.6288044Losses:  10.559673607349396 1.0 1.0134546756744385 4.254472076892853
CurrentTrain: epoch  7, batch    54 | loss: 10.5596736Losses:  6.490310192108154 1.0 1.0180152654647827 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 6.4903102Losses:  6.22951078414917 1.0 1.0152192115783691 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 6.2295108Losses:  6.904299736022949 1.0 1.0147507190704346 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 6.9042997Losses:  12.172428097575903 1.0 1.0118211507797241 5.874992337077856
CurrentTrain: epoch  7, batch    58 | loss: 12.1724281Losses:  6.323544502258301 1.0 1.0139343738555908 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 6.3235445Losses:  7.703436344861984 1.0 1.0129629373550415 1.411469429731369
CurrentTrain: epoch  7, batch    60 | loss: 7.7034363Losses:  7.750575929880142 1.0 1.014095425605774 1.4211463034152985
CurrentTrain: epoch  7, batch    61 | loss: 7.7505759Losses:  6.271193981170654 1.0 1.0057674646377563 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 6.2711940Losses:  7.9777427315711975 1.0 1.0217198133468628 1.468739092350006
CurrentTrain: epoch  8, batch     0 | loss: 7.9777427Losses:  9.491216126829386 1.0 1.0143927335739136 3.0673565305769444
CurrentTrain: epoch  8, batch     1 | loss: 9.4912161Losses:  9.169235613197088 1.0 1.0148112773895264 2.8777774833142757
CurrentTrain: epoch  8, batch     2 | loss: 9.1692356Losses:  7.722315702587366 1.0 1.0191013813018799 1.4242161847651005
CurrentTrain: epoch  8, batch     3 | loss: 7.7223157Losses:  7.9667649269104 1.0 1.0186792612075806 1.42041015625
CurrentTrain: epoch  8, batch     4 | loss: 7.9667649Losses:  6.286672592163086 1.0 1.0096715688705444 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 6.2866726Losses:  6.299201965332031 1.0 1.0089484453201294 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 6.2992020Losses:  9.139246821403503 1.0 1.0110061168670654 2.835823893547058
CurrentTrain: epoch  8, batch     7 | loss: 9.1392468Losses:  9.240357611328363 1.0 1.018885850906372 2.887213919311762
CurrentTrain: epoch  8, batch     8 | loss: 9.2403576Losses:  10.35001065582037 1.0 1.0100352764129639 4.20727326720953
CurrentTrain: epoch  8, batch     9 | loss: 10.3500107Losses:  6.308392524719238 1.0 1.0155481100082397 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 6.3083925Losses:  7.687483847141266 1.0 1.0185418128967285 1.454740583896637
CurrentTrain: epoch  8, batch    11 | loss: 7.6874838Losses:  7.9350487887859344 1.0 1.0181653499603271 1.403691977262497
CurrentTrain: epoch  8, batch    12 | loss: 7.9350488Losses:  7.745677649974823 1.0 1.0112334489822388 1.486447513103485
CurrentTrain: epoch  8, batch    13 | loss: 7.7456776Losses:  7.722313046455383 1.0 1.0120011568069458 1.4208933115005493
CurrentTrain: epoch  8, batch    14 | loss: 7.7223130Losses:  7.807620614767075 1.0 1.0146112442016602 1.4011045396327972
CurrentTrain: epoch  8, batch    15 | loss: 7.8076206Losses:  11.788057282567024 1.0 1.0203665494918823 4.349269345402718
CurrentTrain: epoch  8, batch    16 | loss: 11.7880573Losses:  6.286718368530273 1.0 1.0136477947235107 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 6.2867184Losses:  7.820422250777483 1.0 1.0170023441314697 1.5091038532555103
CurrentTrain: epoch  8, batch    18 | loss: 7.8204223Losses:  10.64743173122406 1.0 1.0129916667938232 4.317848563194275
CurrentTrain: epoch  8, batch    19 | loss: 10.6474317Losses:  6.31437873840332 1.0 1.0147860050201416 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 6.3143787Losses:  6.348357200622559 1.0 1.0106334686279297 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 6.3483572Losses:  12.386681392788887 1.0 1.0112707614898682 5.90910704433918
CurrentTrain: epoch  8, batch    22 | loss: 12.3866814Losses:  16.26219215616584 1.0 1.012247085571289 9.930457975715399
CurrentTrain: epoch  8, batch    23 | loss: 16.2621922Losses:  6.287288188934326 1.0 1.0126241445541382 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 6.2872882Losses:  18.952081210911274 1.0 1.013113260269165 12.770960338413715
CurrentTrain: epoch  8, batch    25 | loss: 18.9520812Losses:  7.7114574909210205 1.0 1.013432502746582 1.3949110507965088
CurrentTrain: epoch  8, batch    26 | loss: 7.7114575Losses:  16.687916414812207 1.0 1.0082874298095703 10.072637217119336
CurrentTrain: epoch  8, batch    27 | loss: 16.6879164Losses:  6.249187469482422 1.0 1.0091149806976318 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 6.2491875Losses:  10.563619613647461 1.0 1.010683536529541 4.230762958526611
CurrentTrain: epoch  8, batch    29 | loss: 10.5636196Losses:  7.685877561569214 1.0 1.0160765647888184 1.4184610843658447
CurrentTrain: epoch  8, batch    30 | loss: 7.6858776Losses:  12.931649535894394 1.0 1.01284921169281 6.629018157720566
CurrentTrain: epoch  8, batch    31 | loss: 12.9316495Losses:  9.057024419307709 1.0 1.0115939378738403 2.787188470363617
CurrentTrain: epoch  8, batch    32 | loss: 9.0570244Losses:  6.295590877532959 1.0 1.014168381690979 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 6.2955909Losses:  8.513350307941437 1.0 1.0136024951934814 1.4342401623725891
CurrentTrain: epoch  8, batch    34 | loss: 8.5133503Losses:  7.714096192270517 1.0 1.0143914222717285 1.4417654313147068
CurrentTrain: epoch  8, batch    35 | loss: 7.7140962Losses:  7.696332693099976 1.0 1.012558937072754 1.4003689289093018
CurrentTrain: epoch  8, batch    36 | loss: 7.6963327Losses:  6.241546630859375 1.0 1.0123319625854492 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.2415466Losses:  14.96579047292471 1.0 1.013962745666504 8.64372082799673
CurrentTrain: epoch  8, batch    38 | loss: 14.9657905Losses:  10.414528727531433 1.0 1.0087743997573853 4.2389243841171265
CurrentTrain: epoch  8, batch    39 | loss: 10.4145287Losses:  7.694636225700378 1.0 1.0138496160507202 1.3977302312850952
CurrentTrain: epoch  8, batch    40 | loss: 7.6946362Losses:  7.742981463670731 1.0 1.0161399841308594 1.3960638344287872
CurrentTrain: epoch  8, batch    41 | loss: 7.7429815Losses:  9.173810716718435 1.0 1.010871171951294 2.8654077015817165
CurrentTrain: epoch  8, batch    42 | loss: 9.1738107Losses:  15.161568503826857 1.0 1.0048789978027344 8.971701484173536
CurrentTrain: epoch  8, batch    43 | loss: 15.1615685Losses:  7.65660360455513 1.0 1.0122029781341553 1.3968265354633331
CurrentTrain: epoch  8, batch    44 | loss: 7.6566036Losses:  7.737641543149948 1.0 1.0156481266021729 1.390815943479538
CurrentTrain: epoch  8, batch    45 | loss: 7.7376415Losses:  10.575860023498535 1.0 1.0083626508712769 4.353416442871094
CurrentTrain: epoch  8, batch    46 | loss: 10.5758600Losses:  7.6504680551588535 1.0 1.0106439590454102 1.4307491220533848
CurrentTrain: epoch  8, batch    47 | loss: 7.6504681Losses:  6.843441486358643 1.0 1.0157837867736816 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 6.8434415Losses:  6.228389739990234 1.0 1.0161926746368408 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 6.2283897Losses:  9.103856801986694 1.0 1.0097277164459229 2.8119332790374756
CurrentTrain: epoch  8, batch    50 | loss: 9.1038568Losses:  7.728576481342316 1.0 1.0102784633636475 1.440970242023468
CurrentTrain: epoch  8, batch    51 | loss: 7.7285765Losses:  7.682511001825333 1.0 1.0124385356903076 1.3982254564762115
CurrentTrain: epoch  8, batch    52 | loss: 7.6825110Losses:  6.193229675292969 1.0 1.006824254989624 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 6.1932297Losses:  10.726623140275478 1.0 1.020708680152893 4.424006067216396
CurrentTrain: epoch  8, batch    54 | loss: 10.7266231Losses:  7.680003762245178 1.0 1.0139575004577637 1.4267245531082153
CurrentTrain: epoch  8, batch    55 | loss: 7.6800038Losses:  10.713569305837154 1.0 1.0091354846954346 4.4717718586325645
CurrentTrain: epoch  8, batch    56 | loss: 10.7135693Losses:  10.443842083215714 1.0 1.011049747467041 4.222113281488419
CurrentTrain: epoch  8, batch    57 | loss: 10.4438421Losses:  7.7032531797885895 1.0 1.0094107389450073 1.3922270834445953
CurrentTrain: epoch  8, batch    58 | loss: 7.7032532Losses:  7.685328271239996 1.0 1.0130858421325684 1.419205453246832
CurrentTrain: epoch  8, batch    59 | loss: 7.6853283Losses:  6.169322967529297 1.0 1.0114842653274536 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 6.1693230Losses:  11.82191789150238 1.0 1.0142498016357422 5.603686690330505
CurrentTrain: epoch  8, batch    61 | loss: 11.8219179Losses:  7.593615472316742 1.0 1.0050098896026611 1.4191264510154724
CurrentTrain: epoch  8, batch    62 | loss: 7.5936155Losses:  9.273667756468058 1.0 1.0123478174209595 3.073764745146036
CurrentTrain: epoch  9, batch     0 | loss: 9.2736678Losses:  6.14990234375 1.0 1.0097638368606567 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 6.1499023Losses:  6.23263692855835 1.0 1.0115461349487305 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 6.2326369Losses:  9.016895979642868 1.0 1.0134732723236084 2.825205534696579
CurrentTrain: epoch  9, batch     3 | loss: 9.0168960Losses:  9.00448814034462 1.0 1.0140759944915771 2.7932702600955963
CurrentTrain: epoch  9, batch     4 | loss: 9.0044881Losses:  10.479876287281513 1.0 1.017359733581543 4.251231916248798
CurrentTrain: epoch  9, batch     5 | loss: 10.4798763Losses:  7.5890392661094666 1.0 1.0115667581558228 1.392518937587738
CurrentTrain: epoch  9, batch     6 | loss: 7.5890393Losses:  9.0358367562294 1.0 1.0122504234313965 2.8370261788368225
CurrentTrain: epoch  9, batch     7 | loss: 9.0358368Losses:  8.947589576244354 1.0 1.008217453956604 2.7968804240226746
CurrentTrain: epoch  9, batch     8 | loss: 8.9475896Losses:  6.162022590637207 1.0 1.0130455493927002 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 6.1620226Losses:  9.06395548954606 1.0 1.0099101066589355 2.9128276742994785
CurrentTrain: epoch  9, batch    10 | loss: 9.0639555Losses:  8.218870908021927 1.0 1.007926106452942 1.4068305790424347
CurrentTrain: epoch  9, batch    11 | loss: 8.2188709Losses:  9.065934270620346 1.0 1.0093050003051758 2.8670330941677094
CurrentTrain: epoch  9, batch    12 | loss: 9.0659343Losses:  6.207878589630127 1.0 1.0111985206604004 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 6.2078786Losses:  9.06006833910942 1.0 1.0153422355651855 2.847340315580368
CurrentTrain: epoch  9, batch    14 | loss: 9.0600683Losses:  10.3033182695508 1.0 1.0093483924865723 4.186330564320087
CurrentTrain: epoch  9, batch    15 | loss: 10.3033183Losses:  9.163029313087463 1.0 1.0168427228927612 2.8743778467178345
CurrentTrain: epoch  9, batch    16 | loss: 9.1630293Losses:  6.1454386711120605 1.0 1.0130829811096191 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 6.1454387Losses:  7.616591960191727 1.0 1.011765480041504 1.4226613342761993
CurrentTrain: epoch  9, batch    18 | loss: 7.6165920Losses:  13.245304226875305 1.0 1.0130691528320312 7.092134118080139
CurrentTrain: epoch  9, batch    19 | loss: 13.2453042Losses:  7.533307433128357 1.0 1.0090391635894775 1.3924630880355835
CurrentTrain: epoch  9, batch    20 | loss: 7.5333074Losses:  8.093772292137146 1.0 1.011272668838501 1.3945516347885132
CurrentTrain: epoch  9, batch    21 | loss: 8.0937723Losses:  6.141897201538086 1.0 1.0109622478485107 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 6.1418972Losses:  9.115774720907211 1.0 1.0073107481002808 2.90639266371727
CurrentTrain: epoch  9, batch    23 | loss: 9.1157747Losses:  6.84128999710083 1.0 1.0119272470474243 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 6.8412900Losses:  10.487308204174042 1.0 1.0143382549285889 4.371666610240936
CurrentTrain: epoch  9, batch    25 | loss: 10.4873082Losses:  6.323129653930664 1.0 1.0148155689239502 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 6.3231297Losses:  8.953671306371689 1.0 1.007542610168457 2.836951106786728
CurrentTrain: epoch  9, batch    27 | loss: 8.9536713Losses:  8.948339819908142 1.0 1.00911545753479 2.826924681663513
CurrentTrain: epoch  9, batch    28 | loss: 8.9483398Losses:  8.999175876379013 1.0 1.0082662105560303 2.821126788854599
CurrentTrain: epoch  9, batch    29 | loss: 8.9991759Losses:  7.636648267507553 1.0 1.0157594680786133 1.405925840139389
CurrentTrain: epoch  9, batch    30 | loss: 7.6366483Losses:  10.42431215196848 1.0 1.0093505382537842 4.306001223623753
CurrentTrain: epoch  9, batch    31 | loss: 10.4243122Losses:  14.411671116948128 1.0 1.0128135681152344 8.207004025578499
CurrentTrain: epoch  9, batch    32 | loss: 14.4116711Losses:  10.406535029411316 1.0 1.0081572532653809 4.272570013999939
CurrentTrain: epoch  9, batch    33 | loss: 10.4065350Losses:  6.12065315246582 1.0 1.0121333599090576 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 6.1206532Losses:  11.842365503311157 1.0 1.0072804689407349 5.671358823776245
CurrentTrain: epoch  9, batch    35 | loss: 11.8423655Losses:  8.92635378241539 1.0 1.0058355331420898 2.7941335141658783
CurrentTrain: epoch  9, batch    36 | loss: 8.9263538Losses:  6.334576606750488 1.0 1.0119082927703857 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.3345766Losses:  7.832466538995504 1.0 1.01829195022583 1.4345769248902798
CurrentTrain: epoch  9, batch    38 | loss: 7.8324665Losses:  6.212496280670166 1.0 1.0120242834091187 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 6.2124963Losses:  7.55858388543129 1.0 1.0090967416763306 1.414605289697647
CurrentTrain: epoch  9, batch    40 | loss: 7.5585839Losses:  6.113699913024902 1.0 1.0073497295379639 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 6.1136999Losses:  10.588564038276672 1.0 1.0159683227539062 4.2270671129226685
CurrentTrain: epoch  9, batch    42 | loss: 10.5885640Losses:  8.962583482265472 1.0 1.0065147876739502 2.8249377608299255
CurrentTrain: epoch  9, batch    43 | loss: 8.9625835Losses:  7.567047744989395 1.0 1.0084137916564941 1.394383579492569
CurrentTrain: epoch  9, batch    44 | loss: 7.5670477Losses:  7.617032669484615 1.0 1.01090407371521 1.4463106617331505
CurrentTrain: epoch  9, batch    45 | loss: 7.6170327Losses:  9.240833077579737 1.0 1.010589361190796 2.8983896113932133
CurrentTrain: epoch  9, batch    46 | loss: 9.2408331Losses:  9.004236876964569 1.0 1.009671926498413 2.814078986644745
CurrentTrain: epoch  9, batch    47 | loss: 9.0042369Losses:  6.179908275604248 1.0 1.0137019157409668 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 6.1799083Losses:  9.022371172904968 1.0 1.013529896736145 2.809898257255554
CurrentTrain: epoch  9, batch    49 | loss: 9.0223712Losses:  10.491626381874084 1.0 1.012352705001831 4.302522301673889
CurrentTrain: epoch  9, batch    50 | loss: 10.4916264Losses:  6.16579008102417 1.0 1.0091400146484375 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 6.1657901Losses:  9.00206783041358 1.0 1.0120398998260498 2.8381493352353573
CurrentTrain: epoch  9, batch    52 | loss: 9.0020678Losses:  9.081458751112223 1.0 1.0189943313598633 2.850125018507242
CurrentTrain: epoch  9, batch    53 | loss: 9.0814588Losses:  8.948104530572891 1.0 1.0051994323730469 2.8437825739383698
CurrentTrain: epoch  9, batch    54 | loss: 8.9481045Losses:  10.540516134351492 1.0 1.0152883529663086 4.241531129926443
CurrentTrain: epoch  9, batch    55 | loss: 10.5405161Losses:  10.544757585972548 1.0 1.0114632844924927 4.359503488987684
CurrentTrain: epoch  9, batch    56 | loss: 10.5447576Losses:  14.92161599546671 1.0 1.007971167564392 8.813932336866856
CurrentTrain: epoch  9, batch    57 | loss: 14.9216160Losses:  7.538300126791 1.0 1.0111191272735596 1.4062267243862152
CurrentTrain: epoch  9, batch    58 | loss: 7.5383001Losses:  10.51796779036522 1.0 1.0148394107818604 4.202880471944809
CurrentTrain: epoch  9, batch    59 | loss: 10.5179678Losses:  7.647881805896759 1.0 1.0190763473510742 1.4280231595039368
CurrentTrain: epoch  9, batch    60 | loss: 7.6478818Losses:  11.46269142255187 1.0 1.0082695484161377 4.208655472844839
CurrentTrain: epoch  9, batch    61 | loss: 11.4626914Losses:  12.108669206500053 1.0 1.013129711151123 5.907802984118462
CurrentTrain: epoch  9, batch    62 | loss: 12.1086692
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  9.916821662336588 1.0 1.0376152992248535 1.561351004987955
CurrentTrain: epoch  0, batch     0 | loss: 9.9168217Losses:  13.833861351013184 1.0 1.0403714179992676 5.926114082336426
CurrentTrain: epoch  0, batch     1 | loss: 13.8338614Losses:  10.970226287841797 1.0 1.0381808280944824 2.913487434387207
CurrentTrain: epoch  0, batch     2 | loss: 10.9702263Losses:  11.088140949606895 1.0 1.0405309200286865 1.5545430034399033
CurrentTrain: epoch  0, batch     3 | loss: 11.0881409Losses:  8.556920051574707 1.0 1.0343654155731201 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 8.5569201Losses:  9.507687747478485 1.0 1.0320398807525635 1.6296655535697937
CurrentTrain: epoch  1, batch     1 | loss: 9.5076877Losses:  8.525489337742329 1.0 1.0397950410842896 1.509658344089985
CurrentTrain: epoch  1, batch     2 | loss: 8.5254893Losses:  9.106425791978836 1.0 1.0254257917404175 1.4297018349170685
CurrentTrain: epoch  1, batch     3 | loss: 9.1064258Losses:  6.928281307220459 1.0 1.0410881042480469 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 6.9282813Losses:  10.373105615377426 1.0 1.0315347909927368 2.963298410177231
CurrentTrain: epoch  2, batch     1 | loss: 10.3731056Losses:  7.3189311027526855 1.0 1.0293736457824707 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 7.3189311Losses:  6.434682808816433 1.0 1.0371603965759277 1.4879426583647728
CurrentTrain: epoch  2, batch     3 | loss: 6.4346828Losses:  10.586531728506088 1.0 1.025341510772705 3.191476434469223
CurrentTrain: epoch  3, batch     0 | loss: 10.5865317Losses:  9.196651205420494 1.0 1.0461342334747314 3.129352316260338
CurrentTrain: epoch  3, batch     1 | loss: 9.1966512Losses:  7.927356746047735 1.0 1.0323193073272705 1.4017648957669735
CurrentTrain: epoch  3, batch     2 | loss: 7.9273567Losses:  7.324280045926571 1.0 1.0143195390701294 1.4419434294104576
CurrentTrain: epoch  3, batch     3 | loss: 7.3242800Losses:  5.812534809112549 1.0 1.035438060760498 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 5.8125348Losses:  11.85851850733161 1.0 1.0283699035644531 5.553103353828192
CurrentTrain: epoch  4, batch     1 | loss: 11.8585185Losses:  6.011237621307373 1.0 1.030863642692566 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.0112376Losses:  8.515620008111 1.0 1.0498361587524414 1.4261334091424942
CurrentTrain: epoch  4, batch     3 | loss: 8.5156200Losses:  6.240117073059082 1.0 1.0353379249572754 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.2401171Losses:  5.735040664672852 1.0 1.0341458320617676 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.7350407Losses:  9.48968531191349 1.0 1.0292437076568604 3.7751524597406387
CurrentTrain: epoch  5, batch     2 | loss: 9.4896853Losses:  6.990355663001537 1.0 1.025848627090454 1.436078242957592
CurrentTrain: epoch  5, batch     3 | loss: 6.9903557Losses:  5.343208312988281 1.0 1.0298526287078857 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.3432083Losses:  5.528726577758789 1.0 1.0291166305541992 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 5.5287266Losses:  5.452882766723633 1.0 1.0364844799041748 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 5.4528828Losses:  6.439060136675835 1.0 1.0321012735366821 1.4724911898374557
CurrentTrain: epoch  6, batch     3 | loss: 6.4390601Losses:  7.747055046260357 1.0 1.0251208543777466 2.347162239253521
CurrentTrain: epoch  7, batch     0 | loss: 7.7470550Losses:  6.65161619707942 1.0 1.0315415859222412 1.5755911879241467
CurrentTrain: epoch  7, batch     1 | loss: 6.6516162Losses:  6.229950584471226 1.0 1.0365638732910156 1.4714466705918312
CurrentTrain: epoch  7, batch     2 | loss: 6.2299506Losses:  6.181728392839432 1.0 1.0408344268798828 1.4696545898914337
CurrentTrain: epoch  7, batch     3 | loss: 6.1817284Losses:  9.079441573470831 1.0 1.0274837017059326 4.344906356185675
CurrentTrain: epoch  8, batch     0 | loss: 9.0794416Losses:  7.665342029184103 1.0 1.0347435474395752 2.84068888053298
CurrentTrain: epoch  8, batch     1 | loss: 7.6653420Losses:  4.835264205932617 1.0 1.03244948387146 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 4.8352642Losses:  5.99272371083498 1.0 1.0488855838775635 1.414004571735859
CurrentTrain: epoch  8, batch     3 | loss: 5.9927237Losses:  6.135447591543198 1.0 1.039502739906311 1.4002529084682465
CurrentTrain: epoch  9, batch     0 | loss: 6.1354476Losses:  5.865687727928162 1.0 1.0256415605545044 1.4248613119125366
CurrentTrain: epoch  9, batch     1 | loss: 5.8656877Losses:  6.621658645570278 1.0 1.0315008163452148 1.910697303712368
CurrentTrain: epoch  9, batch     2 | loss: 6.6216586Losses:  5.799494929611683 1.0 1.0301082134246826 1.4926626160740852
CurrentTrain: epoch  9, batch     3 | loss: 5.7994949
Losses:  4.286159515380859 1.0 1.0206973552703857 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 4.2861595Losses:  8.756972074508667 1.0 1.013628363609314 5.767139196395874
MemoryTrain:  epoch  0, batch     1 | loss: 8.7569721Losses:  4.088936805725098 1.0 1.0237133502960205 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 4.0889368Losses:  8.065879076719284 1.0 1.025453805923462 5.6563365161418915
MemoryTrain:  epoch  1, batch     1 | loss: 8.0658791Losses:  3.0632541179656982 1.0 1.0254309177398682 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 3.0632541Losses:  10.317797962576151 1.0 1.010652780532837 5.636880699545145
MemoryTrain:  epoch  2, batch     1 | loss: 10.3177980Losses:  3.2676515579223633 1.0 1.021347165107727 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 3.2676516Losses:  8.708072312176228 1.0 1.017697811126709 5.720163471996784
MemoryTrain:  epoch  3, batch     1 | loss: 8.7080723Losses:  3.238656520843506 1.0 1.0255115032196045 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 3.2386565Losses:  8.61915197223425 1.0 1.009087324142456 5.732550047338009
MemoryTrain:  epoch  4, batch     1 | loss: 8.6191520Losses:  2.669839859008789 1.0 1.0180118083953857 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.6698399Losses:  9.146446377038956 1.0 1.0378880500793457 5.6935862600803375
MemoryTrain:  epoch  5, batch     1 | loss: 9.1464464Losses:  2.8561201095581055 1.0 1.0181437730789185 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.8561201Losses:  7.77627694606781 1.0 1.0185291767120361 5.631374716758728
MemoryTrain:  epoch  6, batch     1 | loss: 7.7762769Losses:  2.7736573219299316 1.0 1.0197875499725342 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.7736573Losses:  7.813275873661041 1.0 1.0264947414398193 5.656463921070099
MemoryTrain:  epoch  7, batch     1 | loss: 7.8132759Losses:  2.7386507987976074 1.0 1.0188136100769043 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.7386508Losses:  7.937360182404518 1.0 1.0404950380325317 5.769148722290993
MemoryTrain:  epoch  8, batch     1 | loss: 7.9373602Losses:  2.63802433013916 1.0 1.0229220390319824 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.6380243Losses:  7.740599013864994 1.0 1.0235788822174072 5.683831073343754
MemoryTrain:  epoch  9, batch     1 | loss: 7.7405990
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 95.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 77.90%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 76.85%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 75.69%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 73.94%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 73.18%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 71.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.73%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.19%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.12%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.83%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.74%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 93.00%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.23%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 93.24%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 93.24%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 93.17%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 93.10%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.95%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 92.58%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.53%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 92.28%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 92.15%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.88%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 91.64%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 91.25%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 91.21%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 91.03%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 90.52%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 90.16%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 89.67%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 89.26%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.92%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 88.65%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 88.38%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 88.00%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 87.56%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 87.38%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 86.95%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 86.72%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 86.49%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 86.38%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 85.92%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 85.42%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 84.92%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 84.49%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 84.01%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 83.71%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 83.52%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 84.45%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 84.78%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.80%   
cur_acc:  ['0.9484', '0.7619']
his_acc:  ['0.9484', '0.8480']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  8.62835967168212 1.0 1.0324296951293945 1.499778624624014
CurrentTrain: epoch  0, batch     0 | loss: 8.6283597Losses:  9.0333833694458 1.0 1.0311228036880493 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 9.0333834Losses:  11.79700879752636 1.0 1.0369055271148682 4.187520310282707
CurrentTrain: epoch  0, batch     2 | loss: 11.7970088Losses:  12.196778617799282 1.0 1.0099380016326904 1.5499003753066063
CurrentTrain: epoch  0, batch     3 | loss: 12.1967786Losses:  9.170723136514425 1.0 1.037562608718872 1.4217187725007534
CurrentTrain: epoch  1, batch     0 | loss: 9.1707231Losses:  14.945319041609764 1.0 1.028655767440796 8.338024958968163
CurrentTrain: epoch  1, batch     1 | loss: 14.9453190Losses:  9.65340256690979 1.0 1.034621238708496 1.4350807666778564
CurrentTrain: epoch  1, batch     2 | loss: 9.6534026Losses:  10.366646118462086 1.0 1.022362232208252 1.4144881442189217
CurrentTrain: epoch  1, batch     3 | loss: 10.3666461Losses:  15.400233570486307 1.0 1.041481852531433 8.341862980276346
CurrentTrain: epoch  2, batch     0 | loss: 15.4002336Losses:  9.321422293782234 1.0 1.0296001434326172 1.9675089865922928
CurrentTrain: epoch  2, batch     1 | loss: 9.3214223Losses:  8.571394801139832 1.0 1.0313694477081299 1.450539469718933
CurrentTrain: epoch  2, batch     2 | loss: 8.5713948Losses:  9.184004753828049 1.0 1.012026071548462 1.5235986411571503
CurrentTrain: epoch  2, batch     3 | loss: 9.1840048Losses:  6.790570259094238 1.0 1.031440019607544 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.7905703Losses:  9.751689553260803 1.0 1.0321686267852783 3.305036187171936
CurrentTrain: epoch  3, batch     1 | loss: 9.7516896Losses:  10.204566325992346 1.0 1.044921875 2.9346655271947384
CurrentTrain: epoch  3, batch     2 | loss: 10.2045663Losses:  9.106207884848118 1.0 1.0492480993270874 1.7368131056427956
CurrentTrain: epoch  3, batch     3 | loss: 9.1062079Losses:  9.80839829146862 1.0 1.0271570682525635 2.9085102528333664
CurrentTrain: epoch  4, batch     0 | loss: 9.8083983Losses:  7.596452087163925 1.0 1.0375038385391235 1.4376257359981537
CurrentTrain: epoch  4, batch     1 | loss: 7.5964521Losses:  10.288271605968475 1.0 1.044405460357666 3.1280705332756042
CurrentTrain: epoch  4, batch     2 | loss: 10.2882716Losses:  6.3234568759799 1.0 1.0420503616333008 1.4713803455233574
CurrentTrain: epoch  4, batch     3 | loss: 6.3234569Losses:  6.373193264007568 1.0 1.0444060564041138 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.3731933Losses:  7.048852734267712 1.0 1.0296740531921387 1.4783137366175652
CurrentTrain: epoch  5, batch     1 | loss: 7.0488527Losses:  8.244699507951736 1.0 1.0343239307403564 1.437055617570877
CurrentTrain: epoch  5, batch     2 | loss: 8.2446995Losses:  8.230378955602646 1.0 1.0167174339294434 1.4325397908687592
CurrentTrain: epoch  5, batch     3 | loss: 8.2303790Losses:  8.750376466661692 1.0 1.0344173908233643 2.895861390978098
CurrentTrain: epoch  6, batch     0 | loss: 8.7503765Losses:  9.576579786837101 1.0 1.0484838485717773 2.9092504754662514
CurrentTrain: epoch  6, batch     1 | loss: 9.5765798Losses:  11.491496741771698 1.0 1.0304381847381592 5.244113624095917
CurrentTrain: epoch  6, batch     2 | loss: 11.4914967Losses:  8.399834666401148 1.0 1.034214735031128 1.5970592834055424
CurrentTrain: epoch  6, batch     3 | loss: 8.3998347Losses:  5.213137149810791 1.0 1.0351271629333496 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.2131371Losses:  7.686725348234177 1.0 1.0402064323425293 1.406293123960495
CurrentTrain: epoch  7, batch     1 | loss: 7.6867253Losses:  11.053081214427948 1.0 1.0394022464752197 4.813675582408905
CurrentTrain: epoch  7, batch     2 | loss: 11.0530812Losses:  8.54659315571189 1.0 1.0110504627227783 1.5108418129384518
CurrentTrain: epoch  7, batch     3 | loss: 8.5465932Losses:  10.324387419968843 1.0 1.0307819843292236 4.541316855698824
CurrentTrain: epoch  8, batch     0 | loss: 10.3243874Losses:  9.226870447397232 1.0 1.0393962860107422 2.851376920938492
CurrentTrain: epoch  8, batch     1 | loss: 9.2268704Losses:  9.245106436312199 1.0 1.041346549987793 4.311444498598576
CurrentTrain: epoch  8, batch     2 | loss: 9.2451064Losses:  7.497213959693909 1.0 1.027733564376831 1.3960033655166626
CurrentTrain: epoch  8, batch     3 | loss: 7.4972140Losses:  5.813300132751465 1.0 1.0416896343231201 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 5.8133001Losses:  6.744673639535904 1.0 1.0414903163909912 1.427727609872818
CurrentTrain: epoch  9, batch     1 | loss: 6.7446736Losses:  7.744014985859394 1.0 1.030583381652832 1.4874360635876656
CurrentTrain: epoch  9, batch     2 | loss: 7.7440150Losses:  5.602267742156982 1.0 1.0316842794418335 1.4506540298461914
CurrentTrain: epoch  9, batch     3 | loss: 5.6022677
Losses:  2.8097171783447266 1.0 1.0296766757965088 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.8097172Losses:  2.6460745334625244 1.0 1.0298810005187988 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.6460745Losses:  2.8770782947540283 1.0 1.0251219272613525 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.8770783Losses:  2.8538031578063965 1.0 1.02763032913208 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.8538032Losses:  2.663464307785034 1.0 1.0270133018493652 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.6634643Losses:  2.2895455360412598 1.0 1.02519690990448 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.2895455Losses:  2.1807641983032227 1.0 1.031976580619812 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.1807642Losses:  2.291947841644287 1.0 1.0204678773880005 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2919478Losses:  2.195026397705078 1.0 1.0309383869171143 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.1950264Losses:  2.144145965576172 1.0 1.0217493772506714 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.1441460Losses:  2.154714584350586 1.0 1.0237544775009155 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.1547146Losses:  2.1051926612854004 1.0 1.0267729759216309 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.1051927Losses:  2.095643997192383 1.0 1.0276401042938232 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.0956440Losses:  2.070537567138672 1.0 1.0233770608901978 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.0705376Losses:  2.0691399574279785 1.0 1.0279017686843872 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.0691400Losses:  2.095737934112549 1.0 1.0222525596618652 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.0957379Losses:  2.0678091049194336 1.0 1.0207762718200684 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.0678091Losses:  2.075094223022461 1.0 1.0327951908111572 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.0750942Losses:  2.0725021362304688 1.0 1.0278536081314087 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.0725021Losses:  2.0638699531555176 1.0 1.0232127904891968 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.0638700
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 63.26%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 71.44%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 71.50%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.93%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 86.65%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.26%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.23%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 90.78%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 90.53%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 90.67%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 90.72%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.98%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.06%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.95%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 90.82%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 90.62%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.66%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 90.55%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 90.21%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 90.03%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 89.85%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 89.75%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 89.51%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 89.28%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 88.90%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 88.54%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 88.11%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 86.78%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 86.33%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 86.02%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 85.91%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 85.73%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 85.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.15%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.93%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.59%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.17%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.96%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.53%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 82.99%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.40%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 81.76%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.64%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.81%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 81.09%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 81.40%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 81.40%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 81.35%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 81.10%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 80.71%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 80.37%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 80.04%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 79.66%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 79.25%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 79.57%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 79.63%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 79.80%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 79.32%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 78.93%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 78.59%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 78.26%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 77.65%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.17%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 78.27%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 78.08%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 77.94%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 77.72%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 77.58%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 77.48%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 77.51%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.57%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.67%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.14%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 78.82%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.84%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 78.77%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.86%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 78.73%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 78.51%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 78.46%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 78.40%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 78.26%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 78.19%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 78.06%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 77.86%   
cur_acc:  ['0.9484', '0.7619', '0.7093']
his_acc:  ['0.9484', '0.8480', '0.7786']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  7.280276298522949 1.0 1.0314981937408447 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 7.2802763Losses:  8.137510068714619 1.0 1.0382927656173706 1.4309551790356636
CurrentTrain: epoch  0, batch     1 | loss: 8.1375101Losses:  14.27360736578703 1.0 1.0377490520477295 7.844244115054607
CurrentTrain: epoch  0, batch     2 | loss: 14.2736074Losses:  7.968920528888702 1.0 1.0255217552185059 1.4397619366645813
CurrentTrain: epoch  0, batch     3 | loss: 7.9689205Losses:  9.290580779314041 1.0 1.0338528156280518 3.008855849504471
CurrentTrain: epoch  1, batch     0 | loss: 9.2905808Losses:  5.7678093910217285 1.0 1.028082013130188 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 5.7678094Losses:  6.611572802066803 1.0 1.0335123538970947 1.4397874474525452
CurrentTrain: epoch  1, batch     2 | loss: 6.6115728Losses:  6.894907668232918 1.0 1.0241806507110596 1.4409997016191483
CurrentTrain: epoch  1, batch     3 | loss: 6.8949077Losses:  9.86270072311163 1.0 1.03377366065979 4.223271630704403
CurrentTrain: epoch  2, batch     0 | loss: 9.8627007Losses:  8.286417447030544 1.0 1.0327866077423096 2.8829683884978294
CurrentTrain: epoch  2, batch     1 | loss: 8.2864174Losses:  7.7193752601742744 1.0 1.031576156616211 2.8741799667477608
CurrentTrain: epoch  2, batch     2 | loss: 7.7193753Losses:  10.523154020309448 1.0 1.0350745916366577 6.711104154586792
CurrentTrain: epoch  2, batch     3 | loss: 10.5231540Losses:  6.957036547362804 1.0 1.0297002792358398 1.6671214625239372
CurrentTrain: epoch  3, batch     0 | loss: 6.9570365Losses:  9.26592481136322 1.0 1.03403902053833 4.291382193565369
CurrentTrain: epoch  3, batch     1 | loss: 9.2659248Losses:  7.617920123040676 1.0 1.0318275690078735 2.875277243554592
CurrentTrain: epoch  3, batch     2 | loss: 7.6179201Losses:  5.55964495241642 1.0 1.0235403776168823 1.4587261825799942
CurrentTrain: epoch  3, batch     3 | loss: 5.5596450Losses:  6.29227277636528 1.0 1.029348611831665 1.4003393352031708
CurrentTrain: epoch  4, batch     0 | loss: 6.2922728Losses:  4.8794097900390625 1.0 1.0292572975158691 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 4.8794098Losses:  6.477093875408173 1.0 1.0364913940429688 1.4386698603630066
CurrentTrain: epoch  4, batch     2 | loss: 6.4770939Losses:  5.31205128878355 1.0 1.0392142534255981 1.4230027124285698
CurrentTrain: epoch  4, batch     3 | loss: 5.3120513Losses:  7.3041782677173615 1.0 1.0345790386199951 2.864400416612625
CurrentTrain: epoch  5, batch     0 | loss: 7.3041783Losses:  4.62840461730957 1.0 1.0297659635543823 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 4.6284046Losses:  6.559387777000666 1.0 1.032004952430725 1.481329057365656
CurrentTrain: epoch  5, batch     2 | loss: 6.5593878Losses:  5.804198458790779 1.0 1.0247995853424072 1.568863108754158
CurrentTrain: epoch  5, batch     3 | loss: 5.8041985Losses:  4.436586856842041 1.0 1.0303126573562622 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 4.4365869Losses:  6.039678454399109 1.0 1.033806562423706 1.428878664970398
CurrentTrain: epoch  6, batch     1 | loss: 6.0396785Losses:  10.502278354018927 1.0 1.031040906906128 5.807908084243536
CurrentTrain: epoch  6, batch     2 | loss: 10.5022784Losses:  5.4193314872682095 1.0 1.0261822938919067 1.5842503868043423
CurrentTrain: epoch  6, batch     3 | loss: 5.4193315Losses:  6.020961634814739 1.0 1.031555414199829 1.469210498034954
CurrentTrain: epoch  7, batch     0 | loss: 6.0209616Losses:  4.690013885498047 1.0 1.0291187763214111 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 4.6900139Losses:  8.414172306656837 1.0 1.031613826751709 4.20468582212925
CurrentTrain: epoch  7, batch     2 | loss: 8.4141723Losses:  5.316218852996826 1.0 1.0285685062408447 1.5033321380615234
CurrentTrain: epoch  7, batch     3 | loss: 5.3162189Losses:  5.848067969083786 1.0 1.0335261821746826 1.396468847990036
CurrentTrain: epoch  8, batch     0 | loss: 5.8480680Losses:  5.797096140682697 1.0 1.029942512512207 1.4806698635220528
CurrentTrain: epoch  8, batch     1 | loss: 5.7970961Losses:  4.113520622253418 1.0 1.0280861854553223 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 4.1135206Losses:  5.768829181790352 1.0 1.0373611450195312 1.4453833848237991
CurrentTrain: epoch  8, batch     3 | loss: 5.7688292Losses:  7.064600761979818 1.0 1.0282707214355469 2.8148096166551113
CurrentTrain: epoch  9, batch     0 | loss: 7.0646008Losses:  4.300868988037109 1.0 1.0237557888031006 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.3008690Losses:  9.80882704257965 1.0 1.0346534252166748 5.615118622779846
CurrentTrain: epoch  9, batch     2 | loss: 9.8088270Losses:  5.297517254948616 1.0 1.0471652746200562 1.5389518290758133
CurrentTrain: epoch  9, batch     3 | loss: 5.2975173
Losses:  2.931612968444824 1.0 1.0209423303604126 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.9316130Losses:  2.6227355003356934 1.0 1.026473045349121 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.6227355Losses:  3.411433219909668 1.0 1.0381098985671997 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 3.4114332Losses:  2.5821688175201416 1.0 1.0264617204666138 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.5821688Losses:  3.1795809268951416 1.0 1.0302116870880127 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.1795809Losses:  3.030031204223633 1.0 1.0231738090515137 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 3.0300312Losses:  2.590195655822754 1.0 1.039154291152954 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.5901957Losses:  2.5296692848205566 1.0 1.0246975421905518 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.5296693Losses:  2.686447858810425 1.0 1.0143883228302002 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.6864479Losses:  2.2332754135131836 1.0 1.02112877368927 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.2332754Losses:  2.1931941509246826 1.0 1.0336718559265137 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.1931942Losses:  3.1476058959960938 1.0 1.0256717205047607 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 3.1476059Losses:  2.3346691131591797 1.0 1.030578374862671 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.3346691Losses:  2.2899904251098633 1.0 1.0255050659179688 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.2899904Losses:  2.143989086151123 1.0 1.0276859998703003 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.1439891Losses:  2.153139591217041 1.0 1.0277256965637207 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.1531396Losses:  2.245239734649658 1.0 1.0311038494110107 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.2452397Losses:  2.126720428466797 1.0 1.025317668914795 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.1267204Losses:  2.1189162731170654 1.0 1.031794548034668 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.1189163Losses:  2.1244454383850098 1.0 1.0186412334442139 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.1244454Losses:  2.068974494934082 1.0 1.0311875343322754 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.0689745Losses:  2.089934825897217 1.0 1.0315310955047607 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.0899348Losses:  2.0740537643432617 1.0 1.0236326456069946 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.0740538Losses:  2.064542055130005 1.0 1.0270464420318604 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.0645421Losses:  2.0582327842712402 1.0 1.0198421478271484 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.0582328Losses:  2.0783166885375977 1.0 1.0312143564224243 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.0783167Losses:  2.071958065032959 1.0 1.0286214351654053 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.0719581Losses:  2.078162670135498 1.0 1.0270531177520752 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.0781627Losses:  2.061842918395996 1.0 1.0233768224716187 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.0618429Losses:  2.0421645641326904 1.0 1.0197128057479858 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.0421646
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 76.35%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 76.44%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 76.53%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.80%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.82%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.38%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 85.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 85.51%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.84%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 86.40%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 86.10%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 86.02%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 85.58%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.27%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 86.47%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 86.58%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 87.15%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.41%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 87.58%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.58%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 87.66%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 87.58%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 87.58%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 87.42%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 87.35%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 87.27%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 86.90%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.26%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 86.06%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 85.72%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 85.32%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 85.07%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 84.89%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.58%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 84.14%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 83.64%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 83.16%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 82.75%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 82.47%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 82.40%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 82.26%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 81.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.68%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.50%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 81.07%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.95%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.78%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.37%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 79.80%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.24%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.58%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.10%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.73%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 78.62%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 78.83%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 78.62%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 78.25%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 77.98%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 77.62%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 77.16%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 76.81%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 76.85%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.51%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 77.45%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 76.98%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 76.61%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.29%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.06%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.83%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 75.52%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 76.24%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 75.90%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 75.74%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 75.53%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 75.32%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 75.16%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.95%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 75.90%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 75.81%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 75.96%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 75.78%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 75.67%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 75.56%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 75.49%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.41%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 75.54%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 75.51%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 75.47%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.50%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 75.69%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 76.13%   [EVAL] batch:  194 | acc: 37.50%,  total acc: 75.93%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 75.77%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 75.70%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 75.51%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 75.31%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 75.16%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 75.19%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.94%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 74.79%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 74.64%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 74.46%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 74.23%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 74.05%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.85%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 73.80%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 74.55%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 74.58%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 74.53%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 75.67%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.17%   
cur_acc:  ['0.9484', '0.7619', '0.7093', '0.7738']
his_acc:  ['0.9484', '0.8480', '0.7786', '0.7618']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  9.91507113352418 1.0 1.0267367362976074 1.9802595414221287
CurrentTrain: epoch  0, batch     0 | loss: 9.9150711Losses:  14.493956476449966 1.0 1.0298610925674438 6.944021612405777
CurrentTrain: epoch  0, batch     1 | loss: 14.4939565Losses:  10.765208527445793 1.0 1.0241031646728516 3.003494545817375
CurrentTrain: epoch  0, batch     2 | loss: 10.7652085Losses:  10.233189690858126 1.0 1.0131762027740479 1.5341302044689655
CurrentTrain: epoch  0, batch     3 | loss: 10.2331897Losses:  14.445487201213837 1.0 1.029249906539917 7.8142072558403015
CurrentTrain: epoch  1, batch     0 | loss: 14.4454872Losses:  7.229552745819092 1.0 1.0266079902648926 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 7.2295527Losses:  7.447856426239014 1.0 1.024186611175537 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 7.4478564Losses:  7.514818042516708 1.0 1.0288374423980713 1.4715355336666107
CurrentTrain: epoch  1, batch     3 | loss: 7.5148180Losses:  9.34205612167716 1.0 1.0307822227478027 1.5220626257359982
CurrentTrain: epoch  2, batch     0 | loss: 9.3420561Losses:  8.03504889830947 1.0 1.0246577262878418 1.5768660865724087
CurrentTrain: epoch  2, batch     1 | loss: 8.0350489Losses:  14.576946128159761 1.0 1.0214295387268066 8.964518416672945
CurrentTrain: epoch  2, batch     2 | loss: 14.5769461Losses:  7.635941430926323 1.0 1.0167551040649414 1.4599403589963913
CurrentTrain: epoch  2, batch     3 | loss: 7.6359414Losses:  6.926594257354736 1.0 1.024813175201416 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.9265943Losses:  7.323149126023054 1.0 1.0211308002471924 1.469203393906355
CurrentTrain: epoch  3, batch     1 | loss: 7.3231491Losses:  9.513181291520596 1.0 1.028648853302002 3.2727123126387596
CurrentTrain: epoch  3, batch     2 | loss: 9.5131813Losses:  6.662376603111625 1.0 1.0390912294387817 1.650454243645072
CurrentTrain: epoch  3, batch     3 | loss: 6.6623766Losses:  12.900534216314554 1.0 1.0216586589813232 6.476189199835062
CurrentTrain: epoch  4, batch     0 | loss: 12.9005342Losses:  7.433472435921431 1.0 1.0274088382720947 1.6533105783164501
CurrentTrain: epoch  4, batch     1 | loss: 7.4334724Losses:  6.76971622928977 1.0 1.0251123905181885 1.456403698772192
CurrentTrain: epoch  4, batch     2 | loss: 6.7697162Losses:  7.0857290625572205 1.0 1.0415618419647217 1.4291109442710876
CurrentTrain: epoch  4, batch     3 | loss: 7.0857291Losses:  9.321666710078716 1.0 1.0275248289108276 3.019607536494732
CurrentTrain: epoch  5, batch     0 | loss: 9.3216667Losses:  5.098285675048828 1.0 1.022599458694458 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.0982857Losses:  5.318739414215088 1.0 1.024096965789795 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 5.3187394Losses:  5.585451811552048 1.0 1.0234148502349854 1.4437319934368134
CurrentTrain: epoch  5, batch     3 | loss: 5.5854518Losses:  8.097982935607433 1.0 1.0269265174865723 2.149923376739025
CurrentTrain: epoch  6, batch     0 | loss: 8.0979829Losses:  7.902524262666702 1.0 1.0233359336853027 2.873426228761673
CurrentTrain: epoch  6, batch     1 | loss: 7.9025243Losses:  6.950608521699905 1.0 1.0309659242630005 1.4157922565937042
CurrentTrain: epoch  6, batch     2 | loss: 6.9506085Losses:  5.339028835296631 1.0 1.008681058883667 1.41227126121521
CurrentTrain: epoch  6, batch     3 | loss: 5.3390288Losses:  8.108255419880152 1.0 1.0317296981811523 2.8512521125376225
CurrentTrain: epoch  7, batch     0 | loss: 8.1082554Losses:  5.20960807800293 1.0 1.0185658931732178 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.2096081Losses:  6.584449887275696 1.0 1.0335965156555176 1.4191781282424927
CurrentTrain: epoch  7, batch     2 | loss: 6.5844499Losses:  7.332777142524719 1.0 1.0388861894607544 1.3975602388381958
CurrentTrain: epoch  7, batch     3 | loss: 7.3327771Losses:  5.412899971008301 1.0 1.0271823406219482 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 5.4129000Losses:  9.77013448625803 1.0 1.0291829109191895 4.923034228384495
CurrentTrain: epoch  8, batch     1 | loss: 9.7701345Losses:  8.092116802930832 1.0 1.0308340787887573 3.03909632563591
CurrentTrain: epoch  8, batch     2 | loss: 8.0921168Losses:  5.416247308254242 1.0 1.0206462144851685 1.3912519812583923
CurrentTrain: epoch  8, batch     3 | loss: 5.4162473Losses:  7.701271768659353 1.0 1.0320100784301758 2.9024131260812283
CurrentTrain: epoch  9, batch     0 | loss: 7.7012718Losses:  6.399017807096243 1.0 1.0253082513809204 1.4427032433450222
CurrentTrain: epoch  9, batch     1 | loss: 6.3990178Losses:  4.977235794067383 1.0 1.024465799331665 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 4.9772358Losses:  6.174623034894466 1.0 1.0380427837371826 1.478988192975521
CurrentTrain: epoch  9, batch     3 | loss: 6.1746230
Losses:  2.6337060928344727 1.0 1.0374422073364258 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.6337061Losses:  2.3459370136260986 1.0 1.0235073566436768 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.3459370Losses:  2.086136817932129 1.0 1.028860330581665 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.0861368Losses:  4.7418827610090375 1.0 1.0446152687072754 1.8900289135053754
MemoryTrain:  epoch  0, batch     3 | loss: 4.7418828Losses:  2.6732490062713623 1.0 1.0245275497436523 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.6732490Losses:  2.4247541427612305 1.0 1.0296108722686768 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.4247541Losses:  2.4619297981262207 1.0 1.0329537391662598 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.4619298Losses:  4.02522050589323 1.0 1.026275396347046 1.4814394637942314
MemoryTrain:  epoch  1, batch     3 | loss: 4.0252205Losses:  2.3766961097717285 1.0 1.0302213430404663 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.3766961Losses:  2.3044180870056152 1.0 1.0268261432647705 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.3044181Losses:  2.195230484008789 1.0 1.0276403427124023 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.1952305Losses:  3.571290224790573 1.0 1.0309770107269287 1.4719931781291962
MemoryTrain:  epoch  2, batch     3 | loss: 3.5712902Losses:  2.3184332847595215 1.0 1.0262389183044434 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.3184333Losses:  2.2140908241271973 1.0 1.0215541124343872 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2140908Losses:  2.126289129257202 1.0 1.025864839553833 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.1262891Losses:  3.5454509779810905 1.0 1.0249910354614258 1.4648097082972527
MemoryTrain:  epoch  3, batch     3 | loss: 3.5454510Losses:  2.232187032699585 1.0 1.0241174697875977 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.2321870Losses:  2.1064679622650146 1.0 1.0218091011047363 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.1064680Losses:  2.1417617797851562 1.0 1.0284790992736816 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.1417618Losses:  3.5897207632660866 1.0 1.0114097595214844 1.447735346853733
MemoryTrain:  epoch  4, batch     3 | loss: 3.5897208Losses:  2.124783992767334 1.0 1.0246226787567139 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.1247840Losses:  2.170764446258545 1.0 1.026505947113037 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.1707644Losses:  2.1164114475250244 1.0 1.0293099880218506 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.1164114Losses:  3.6012411899864674 1.0 1.016337275505066 1.527904111891985
MemoryTrain:  epoch  5, batch     3 | loss: 3.6012412Losses:  2.1377158164978027 1.0 1.0309184789657593 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.1377158Losses:  2.1415493488311768 1.0 1.0180091857910156 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.1415493Losses:  2.09096622467041 1.0 1.0270695686340332 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.0909662Losses:  3.5747068151831627 1.0 1.0277959108352661 1.5105741247534752
MemoryTrain:  epoch  6, batch     3 | loss: 3.5747068Losses:  2.079220771789551 1.0 1.0242574214935303 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.0792208Losses:  2.077247142791748 1.0 1.0215017795562744 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.0772471Losses:  2.0838558673858643 1.0 1.029733657836914 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.0838559Losses:  3.6774140521883965 1.0 1.035808801651001 1.528097741305828
MemoryTrain:  epoch  7, batch     3 | loss: 3.6774141Losses:  2.0748815536499023 1.0 1.0217645168304443 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.0748816Losses:  2.076688289642334 1.0 1.0243958234786987 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.0766883Losses:  2.0774993896484375 1.0 1.0282437801361084 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.0774994Losses:  3.4612401574850082 1.0 1.0342246294021606 1.4075912088155746
MemoryTrain:  epoch  8, batch     3 | loss: 3.4612402Losses:  2.0879080295562744 1.0 1.0239067077636719 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.0879080Losses:  2.091489553451538 1.0 1.0227413177490234 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.0914896Losses:  2.073141574859619 1.0 1.0310437679290771 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.0731416Losses:  3.5375450029969215 1.0 1.0269858837127686 1.457031525671482
MemoryTrain:  epoch  9, batch     3 | loss: 3.5375450
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 72.80%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 74.29%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 75.13%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 75.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.66%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 79.17%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 78.53%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 77.66%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 78.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 78.46%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 78.18%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.07%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 79.07%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.60%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 80.46%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 80.64%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 81.17%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 81.33%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 81.33%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 81.01%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 80.77%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 80.70%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 80.34%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 80.05%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 79.63%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 79.53%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 79.47%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 79.24%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 78.98%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 78.80%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 78.76%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 78.39%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 77.96%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 77.60%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 77.23%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 77.15%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 76.88%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.67%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.53%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 76.27%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 76.07%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.94%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.58%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 75.12%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.60%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.03%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 73.54%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 74.60%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 74.26%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 73.88%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 73.55%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 72.98%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 72.76%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.82%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 73.29%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 72.99%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 72.70%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 72.49%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.29%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.05%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 72.81%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 72.53%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 72.30%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 72.08%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 71.90%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 72.27%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.15%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 72.30%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 72.61%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 72.49%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 72.40%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 72.38%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 73.11%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.05%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 72.89%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 72.77%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 72.62%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.69%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 72.58%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.54%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 72.43%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 72.24%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 71.85%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.68%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 71.46%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 71.42%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 72.13%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 72.17%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 72.18%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 72.17%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.45%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 73.73%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 73.79%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.09%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 74.45%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 74.71%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 74.66%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 74.66%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 74.59%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 74.59%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 74.55%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 74.48%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 74.39%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 74.25%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 74.25%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 74.66%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  275 | acc: 87.50%,  total acc: 74.80%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 74.77%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 74.78%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 74.73%   [EVAL] batch:  279 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 74.82%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 74.80%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 74.67%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 74.56%   [EVAL] batch:  285 | acc: 31.25%,  total acc: 74.41%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 74.24%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 74.20%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 74.27%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 74.40%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:  296 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  297 | acc: 87.50%,  total acc: 74.52%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.56%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 74.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.50%   
cur_acc:  ['0.9484', '0.7619', '0.7093', '0.7738', '0.7966']
his_acc:  ['0.9484', '0.8480', '0.7786', '0.7618', '0.7550']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  8.450044631958008 1.0 1.0264222621917725 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 8.4500446Losses:  9.74374132975936 1.0 1.0231986045837402 1.6344426237046719
CurrentTrain: epoch  0, batch     1 | loss: 9.7437413Losses:  9.805532440543175 1.0 1.0226441621780396 1.5019941180944443
CurrentTrain: epoch  0, batch     2 | loss: 9.8055324Losses:  11.485099965706468 1.0 1.047407865524292 1.812838727608323
CurrentTrain: epoch  0, batch     3 | loss: 11.4851000Losses:  8.734380394220352 1.0 1.0174369812011719 1.4120375216007233
CurrentTrain: epoch  1, batch     0 | loss: 8.7343804Losses:  9.229456715285778 1.0 1.0204463005065918 1.6228578612208366
CurrentTrain: epoch  1, batch     1 | loss: 9.2294567Losses:  6.583669662475586 1.0 1.0283650159835815 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 6.5836697Losses:  7.660703934729099 1.0 1.0202525854110718 1.5968482866883278
CurrentTrain: epoch  1, batch     3 | loss: 7.6607039Losses:  8.058775618672371 1.0 1.020331621170044 1.6906535178422928
CurrentTrain: epoch  2, batch     0 | loss: 8.0587756Losses:  6.873225688934326 1.0 1.0218806266784668 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 6.8732257Losses:  9.523999825119972 1.0 1.0269360542297363 2.9477235227823257
CurrentTrain: epoch  2, batch     2 | loss: 9.5239998Losses:  14.9088653922081 1.0 1.0 9.23725837469101
CurrentTrain: epoch  2, batch     3 | loss: 14.9088654Losses:  10.898599043488503 1.0 1.0228594541549683 4.640866652131081
CurrentTrain: epoch  3, batch     0 | loss: 10.8985990Losses:  8.202655618079007 1.0 1.0231144428253174 2.0503843473270535
CurrentTrain: epoch  3, batch     1 | loss: 8.2026556Losses:  7.8617226742208 1.0 1.0196393728256226 1.551160540431738
CurrentTrain: epoch  3, batch     2 | loss: 7.8617227Losses:  8.298271238803864 1.0 1.043806552886963 1.4508867859840393
CurrentTrain: epoch  3, batch     3 | loss: 8.2982712Losses:  7.115834958851337 1.0 1.027418851852417 1.477497823536396
CurrentTrain: epoch  4, batch     0 | loss: 7.1158350Losses:  6.597618103027344 1.0 1.019033432006836 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 6.5976181Losses:  7.5438224002718925 1.0 1.0179126262664795 1.4959641620516777
CurrentTrain: epoch  4, batch     2 | loss: 7.5438224Losses:  5.461479492485523 1.0 1.01944899559021 1.4223826602101326
CurrentTrain: epoch  4, batch     3 | loss: 5.4614795Losses:  13.658181264996529 1.0 1.0227349996566772 8.670631006360054
CurrentTrain: epoch  5, batch     0 | loss: 13.6581813Losses:  7.210692103952169 1.0 1.019656777381897 1.4483835063874722
CurrentTrain: epoch  5, batch     1 | loss: 7.2106921Losses:  12.75904893875122 1.0 1.0218305587768555 6.132864475250244
CurrentTrain: epoch  5, batch     2 | loss: 12.7590489Losses:  5.80362980812788 1.0 1.002148151397705 1.4557956978678703
CurrentTrain: epoch  5, batch     3 | loss: 5.8036298Losses:  8.215754937380552 1.0 1.0198150873184204 2.9700088016688824
CurrentTrain: epoch  6, batch     0 | loss: 8.2157549Losses:  8.981171812862158 1.0 1.0189862251281738 2.8556716106832027
CurrentTrain: epoch  6, batch     1 | loss: 8.9811718Losses:  6.9624481201171875 1.0 1.0221424102783203 1.4103069305419922
CurrentTrain: epoch  6, batch     2 | loss: 6.9624481Losses:  6.672601990401745 1.0 1.0215833187103271 1.4601033255457878
CurrentTrain: epoch  6, batch     3 | loss: 6.6726020Losses:  9.049640975892544 1.0 1.0100109577178955 2.9162438735365868
CurrentTrain: epoch  7, batch     0 | loss: 9.0496410Losses:  8.128644917160273 1.0 1.0251026153564453 2.946084950119257
CurrentTrain: epoch  7, batch     1 | loss: 8.1286449Losses:  11.439498648047447 1.0 1.0264619588851929 6.077912554144859
CurrentTrain: epoch  7, batch     2 | loss: 11.4394986Losses:  5.737162567675114 1.0 1.0112813711166382 1.4680175557732582
CurrentTrain: epoch  7, batch     3 | loss: 5.7371626Losses:  7.160265684127808 1.0 1.0188462734222412 1.421142339706421
CurrentTrain: epoch  8, batch     0 | loss: 7.1602657Losses:  8.06286135315895 1.0 1.0217015743255615 2.8563412725925446
CurrentTrain: epoch  8, batch     1 | loss: 8.0628614Losses:  8.040305566042662 1.0 1.0217489004135132 2.8817362301051617
CurrentTrain: epoch  8, batch     2 | loss: 8.0403056Losses:  5.539087526500225 1.0 1.031665325164795 1.4489014074206352
CurrentTrain: epoch  8, batch     3 | loss: 5.5390875Losses:  7.94060617312789 1.0 1.0179829597473145 2.849965628236532
CurrentTrain: epoch  9, batch     0 | loss: 7.9406062Losses:  6.426303535699844 1.0 1.0278140306472778 1.405203491449356
CurrentTrain: epoch  9, batch     1 | loss: 6.4263035Losses:  5.542417526245117 1.0 1.0199565887451172 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.5424175Losses:  10.223769560456276 1.0 1.0 6.3878733068704605
CurrentTrain: epoch  9, batch     3 | loss: 10.2237696
Losses:  2.8593904972076416 1.0 1.027834415435791 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.8593905Losses:  2.4384498596191406 1.0 1.018249273300171 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.4384499Losses:  2.7178919315338135 1.0 1.0212407112121582 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.7178919Losses:  2.441267490386963 1.0 1.0258015394210815 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 2.4412675Losses:  3.069997787475586 1.0 1.0203123092651367 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.0699978Losses:  3.098723888397217 1.0 1.0227210521697998 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 3.0987239Losses:  2.2108280658721924 1.0 1.0239982604980469 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.2108281Losses:  2.789266586303711 1.0 1.0341358184814453 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 2.7892666Losses:  2.4181032180786133 1.0 1.0290546417236328 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.4181032Losses:  2.5186920166015625 1.0 1.0316200256347656 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.5186920Losses:  2.2072982788085938 1.0 1.0181862115859985 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.2072983Losses:  2.445950984954834 1.0 1.021582007408142 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 2.4459510Losses:  2.2497475147247314 1.0 1.0198934078216553 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.2497475Losses:  2.2594869136810303 1.0 1.0260509252548218 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2594869Losses:  2.197159767150879 1.0 1.0254764556884766 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.1971598Losses:  2.2262344360351562 1.0 1.0257574319839478 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.2262344Losses:  2.1065375804901123 1.0 1.0181114673614502 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.1065376Losses:  2.200545310974121 1.0 1.0250248908996582 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.2005453Losses:  2.228123664855957 1.0 1.0283639430999756 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.2281237Losses:  2.0959088802337646 1.0 1.0267012119293213 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.0959089Losses:  2.126041889190674 1.0 1.024857759475708 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.1260419Losses:  2.094119071960449 1.0 1.0236365795135498 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.0941191Losses:  2.1320762634277344 1.0 1.0254542827606201 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.1320763Losses:  2.3827810287475586 1.0 1.0196173191070557 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.3827810Losses:  2.0875847339630127 1.0 1.0196099281311035 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.0875847Losses:  2.1321897506713867 1.0 1.0249260663986206 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.1321898Losses:  2.0664873123168945 1.0 1.0202295780181885 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.0664873Losses:  2.174924612045288 1.0 1.024301528930664 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 2.1749246Losses:  2.0991287231445312 1.0 1.0133880376815796 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.0991287Losses:  2.0815353393554688 1.0 1.0280396938323975 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.0815353Losses:  2.065073013305664 1.0 1.025988221168518 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.0650730Losses:  2.0949153900146484 1.0 1.026955485343933 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 2.0949154Losses:  2.0843372344970703 1.0 1.0271474123001099 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.0843372Losses:  2.066041946411133 1.0 1.0225433111190796 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.0660419Losses:  2.0625064373016357 1.0 1.0192302465438843 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.0625064Losses:  2.06398868560791 1.0 1.0246243476867676 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 2.0639887Losses:  2.059105396270752 1.0 1.0200666189193726 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.0591054Losses:  2.066775321960449 1.0 1.0303452014923096 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.0667753Losses:  2.0670461654663086 1.0 1.0193169116973877 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.0670462Losses:  2.088757038116455 1.0 1.0198804140090942 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 2.0887570
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 45.70%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 46.32%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 48.36%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 70.89%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 67.23%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 63.92%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 68.15%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 68.04%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 68.08%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 66.46%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 65.67%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 59.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 69.85%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 75.28%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 74.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 74.39%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 75.55%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 74.78%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 74.08%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 73.81%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 73.93%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 74.54%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 76.01%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 75.23%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 74.92%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 74.93%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 74.78%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 74.72%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 74.44%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 74.17%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 73.90%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 73.71%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 73.34%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 72.66%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 72.55%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 72.29%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 71.84%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 71.48%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 71.31%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 70.91%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 70.01%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 69.55%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 69.20%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 68.86%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 70.42%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 70.25%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 70.15%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 69.69%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 69.43%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 68.70%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 68.42%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.52%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 68.88%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 68.62%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.44%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.27%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 68.55%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 68.38%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 68.07%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.87%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 68.45%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 68.11%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 67.70%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 67.29%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 66.89%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 66.61%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 66.40%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 66.08%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 65.82%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 66.10%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:  194 | acc: 6.25%,  total acc: 66.83%   [EVAL] batch:  195 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:  196 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  197 | acc: 0.00%,  total acc: 65.88%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 65.61%   [EVAL] batch:  199 | acc: 6.25%,  total acc: 65.31%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 65.41%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 65.44%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 65.37%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 65.23%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 65.13%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 64.94%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 64.81%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 64.65%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.58%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 65.64%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 65.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 67.49%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 68.85%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 68.19%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.31%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 68.89%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 68.84%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 68.84%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 68.88%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 68.64%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 68.49%   [EVAL] batch:  286 | acc: 37.50%,  total acc: 68.38%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 68.32%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 69.81%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 69.65%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 69.30%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 69.10%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 68.97%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 68.87%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  320 | acc: 62.50%,  total acc: 68.89%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:  323 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 69.02%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 68.96%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 68.73%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 68.69%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 68.62%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 69.94%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 69.80%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 69.64%   [EVAL] batch:  353 | acc: 12.50%,  total acc: 69.47%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 69.31%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 69.15%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 69.61%   [EVAL] batch:  363 | acc: 87.50%,  total acc: 69.66%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 69.56%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 69.49%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 69.42%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 69.25%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.27%   
cur_acc:  ['0.9484', '0.7619', '0.7093', '0.7738', '0.7966', '0.6567']
his_acc:  ['0.9484', '0.8480', '0.7786', '0.7618', '0.7550', '0.6927']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  10.969551421701908 1.0 1.0352789163589478 2.6481269374489784
CurrentTrain: epoch  0, batch     0 | loss: 10.9695514Losses:  10.531942195259035 1.0 1.0294698476791382 1.8507326310500503
CurrentTrain: epoch  0, batch     1 | loss: 10.5319422Losses:  14.433187380433083 1.0 1.032155156135559 5.881300821900368
CurrentTrain: epoch  0, batch     2 | loss: 14.4331874Losses:  12.80405898578465 1.0 1.0307163000106812 1.8951654005795717
CurrentTrain: epoch  0, batch     3 | loss: 12.8040590Losses:  9.153813123703003 1.0 1.0312765836715698 1.4210765361785889
CurrentTrain: epoch  1, batch     0 | loss: 9.1538131Losses:  7.754037380218506 1.0 1.0308746099472046 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 7.7540374Losses:  9.422874361276627 1.0 1.0304629802703857 1.593783289194107
CurrentTrain: epoch  1, batch     2 | loss: 9.4228744Losses:  8.17201990634203 1.0 1.0198044776916504 1.4342083409428596
CurrentTrain: epoch  1, batch     3 | loss: 8.1720199Losses:  10.298313651233912 1.0 1.0298528671264648 3.3380117751657963
CurrentTrain: epoch  2, batch     0 | loss: 10.2983137Losses:  9.090471535921097 1.0 1.036299467086792 1.4095480740070343
CurrentTrain: epoch  2, batch     1 | loss: 9.0904715Losses:  9.66256470233202 1.0 1.025925874710083 1.6263002827763557
CurrentTrain: epoch  2, batch     2 | loss: 9.6625647Losses:  9.525220394134521 1.0 1.0298762321472168 1.41054105758667
CurrentTrain: epoch  2, batch     3 | loss: 9.5252204Losses:  6.952968597412109 1.0 1.034098505973816 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.9529686Losses:  12.901401676237583 1.0 1.0282988548278809 4.943496860563755
CurrentTrain: epoch  3, batch     1 | loss: 12.9014017Losses:  8.473484091460705 1.0 1.0324240922927856 1.5648723170161247
CurrentTrain: epoch  3, batch     2 | loss: 8.4734841Losses:  10.564276471734047 1.0 1.0401889085769653 1.5072305351495743
CurrentTrain: epoch  3, batch     3 | loss: 10.5642765Losses:  10.49312711134553 1.0 1.0342177152633667 3.026551488786936
CurrentTrain: epoch  4, batch     0 | loss: 10.4931271Losses:  9.297241285443306 1.0 1.0294203758239746 2.509223535656929
CurrentTrain: epoch  4, batch     1 | loss: 9.2972413Losses:  7.317123889923096 1.0 1.029919147491455 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 7.3171239Losses:  7.047417916357517 1.0 1.0253078937530518 1.5062611475586891
CurrentTrain: epoch  4, batch     3 | loss: 7.0474179Losses:  7.075099945068359 1.0 1.0338013172149658 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.0750999Losses:  12.033862747251987 1.0 1.0319437980651855 4.9726540222764015
CurrentTrain: epoch  5, batch     1 | loss: 12.0338627Losses:  6.6293044090271 1.0 1.0271716117858887 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 6.6293044Losses:  9.023683957755566 1.0 1.0370070934295654 1.5327591225504875
CurrentTrain: epoch  5, batch     3 | loss: 9.0236840Losses:  10.172069054096937 1.0 1.0326855182647705 3.3515362553298473
CurrentTrain: epoch  6, batch     0 | loss: 10.1720691Losses:  16.99450496584177 1.0 1.0373752117156982 10.075068511068821
CurrentTrain: epoch  6, batch     1 | loss: 16.9945050Losses:  12.508969657123089 1.0 1.0260136127471924 6.051510207355022
CurrentTrain: epoch  6, batch     2 | loss: 12.5089697Losses:  7.8012271746993065 1.0 1.029670238494873 1.4323368892073631
CurrentTrain: epoch  6, batch     3 | loss: 7.8012272Losses:  9.08625191822648 1.0 1.0300296545028687 2.8739110864698887
CurrentTrain: epoch  7, batch     0 | loss: 9.0862519Losses:  8.291310034692287 1.0 1.0280559062957764 1.5119483098387718
CurrentTrain: epoch  7, batch     1 | loss: 8.2913100Losses:  9.82807620614767 1.0 1.0316308736801147 3.830224357545376
CurrentTrain: epoch  7, batch     2 | loss: 9.8280762Losses:  8.401148531585932 1.0 1.0242111682891846 1.617901537567377
CurrentTrain: epoch  7, batch     3 | loss: 8.4011485Losses:  9.318961657583714 1.0 1.0341739654541016 2.558807887136936
CurrentTrain: epoch  8, batch     0 | loss: 9.3189617Losses:  8.892950240522623 1.0 1.0293362140655518 2.8717042841017246
CurrentTrain: epoch  8, batch     1 | loss: 8.8929502Losses:  14.837881565093994 1.0 1.0307035446166992 8.982675552368164
CurrentTrain: epoch  8, batch     2 | loss: 14.8378816Losses:  6.515731953084469 1.0 1.0275506973266602 1.4736663326621056
CurrentTrain: epoch  8, batch     3 | loss: 6.5157320Losses:  6.1360883712768555 1.0 1.0358004570007324 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 6.1360884Losses:  5.608476638793945 1.0 1.0273407697677612 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 5.6084766Losses:  7.88659480214119 1.0 1.0323410034179688 1.4166536629199982
CurrentTrain: epoch  9, batch     2 | loss: 7.8865948Losses:  6.50341834872961 1.0 1.0284783840179443 1.4238866791129112
CurrentTrain: epoch  9, batch     3 | loss: 6.5034183
Losses:  2.317840576171875 1.0 1.0213598012924194 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.3178406Losses:  2.6400275230407715 1.0 1.0275344848632812 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.6400275Losses:  2.2736244201660156 1.0 1.0222866535186768 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.2736244Losses:  2.3548359870910645 1.0 1.019777536392212 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 2.3548360Losses:  7.093761242926121 1.0 1.0192538499832153 4.385446585714817
MemoryTrain:  epoch  0, batch     4 | loss: 7.0937612Losses:  2.6100072860717773 1.0 1.0244240760803223 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.6100073Losses:  2.4573140144348145 1.0 1.0246257781982422 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.4573140Losses:  2.5561447143554688 1.0 1.0189625024795532 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.5561447Losses:  2.383345365524292 1.0 1.0202138423919678 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 2.3833454Losses:  6.591670334339142 1.0 1.0215213298797607 4.260419189929962
MemoryTrain:  epoch  1, batch     4 | loss: 6.5916703Losses:  2.2046971321105957 1.0 1.0156770944595337 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.2046971Losses:  2.208371162414551 1.0 1.0314332246780396 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.2083712Losses:  2.198024272918701 1.0 1.0173840522766113 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.1980243Losses:  2.3997111320495605 1.0 1.02353835105896 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 2.3997111Losses:  6.5828524343669415 1.0 1.0242383480072021 4.505757641047239
MemoryTrain:  epoch  2, batch     4 | loss: 6.5828524Losses:  2.1336629390716553 1.0 1.0268840789794922 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.1336629Losses:  2.2113876342773438 1.0 1.0220940113067627 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.2113876Losses:  2.209855079650879 1.0 1.020906686782837 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.2098551Losses:  2.101235866546631 1.0 1.020092487335205 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.1012359Losses:  6.36164128780365 1.0 1.015454888343811 4.278890490531921
MemoryTrain:  epoch  3, batch     4 | loss: 6.3616413Losses:  2.1188597679138184 1.0 1.019068717956543 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.1188598Losses:  2.0943312644958496 1.0 1.0210092067718506 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.0943313Losses:  2.1222667694091797 1.0 1.0231162309646606 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.1222668Losses:  2.071030616760254 1.0 1.0231267213821411 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.0710306Losses:  6.360333748161793 1.0 1.0239108800888062 4.254653282463551
MemoryTrain:  epoch  4, batch     4 | loss: 6.3603337Losses:  2.0804481506347656 1.0 1.0143898725509644 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.0804482Losses:  2.0886645317077637 1.0 1.0216801166534424 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.0886645Losses:  2.0555269718170166 1.0 1.022276759147644 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.0555270Losses:  2.1299896240234375 1.0 1.0292952060699463 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.1299896Losses:  6.435295224189758 1.0 1.0168280601501465 4.345556139945984
MemoryTrain:  epoch  5, batch     4 | loss: 6.4352952Losses:  2.0787510871887207 1.0 1.0211507081985474 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.0787511Losses:  2.0630974769592285 1.0 1.015771746635437 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.0630975Losses:  2.0704169273376465 1.0 1.0241893529891968 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.0704169Losses:  2.0600743293762207 1.0 1.0192523002624512 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 2.0600743Losses:  6.339561901986599 1.0 1.0246608257293701 4.2371132001280785
MemoryTrain:  epoch  6, batch     4 | loss: 6.3395619Losses:  2.0751428604125977 1.0 1.0229594707489014 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.0751429Losses:  2.0776116847991943 1.0 1.024430751800537 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.0776117Losses:  2.064960479736328 1.0 1.0210617780685425 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.0649605Losses:  2.0607857704162598 1.0 1.0156586170196533 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 2.0607858Losses:  6.488259710371494 1.0 1.0249041318893433 4.441002763807774
MemoryTrain:  epoch  7, batch     4 | loss: 6.4882597Losses:  2.0687761306762695 1.0 1.0239593982696533 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.0687761Losses:  2.0653862953186035 1.0 1.0205023288726807 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.0653863Losses:  2.0744762420654297 1.0 1.022555947303772 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.0744762Losses:  2.0654430389404297 1.0 1.0168676376342773 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 2.0654430Losses:  6.4128610119223595 1.0 1.015377402305603 4.3684040531516075
MemoryTrain:  epoch  8, batch     4 | loss: 6.4128610Losses:  2.0828042030334473 1.0 1.0185219049453735 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.0828042Losses:  2.0609469413757324 1.0 1.0175893306732178 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.0609469Losses:  2.0509793758392334 1.0 1.0191428661346436 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.0509794Losses:  2.0728540420532227 1.0 1.0244340896606445 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 2.0728540Losses:  6.351895451545715 1.0 1.024250864982605 4.286341309547424
MemoryTrain:  epoch  9, batch     4 | loss: 6.3518955
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 61.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 56.94%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 54.91%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 53.23%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 51.46%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 49.80%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 50.20%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 51.33%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 52.39%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 55.57%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 56.41%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 57.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 60.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 62.22%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 65.02%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 63.90%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 62.82%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 61.77%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 60.76%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 59.78%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 58.83%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 63.75%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 58.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 74.18%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 73.70%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 74.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 74.45%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 73.81%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 73.52%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 73.16%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 72.98%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 72.82%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 73.62%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 73.55%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 74.84%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 74.60%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 74.45%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 74.46%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 74.24%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 74.02%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 74.03%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 73.90%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 73.65%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 73.24%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 72.50%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 71.91%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 71.47%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 70.97%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 70.55%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 70.20%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 69.86%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 69.72%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 68.81%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 68.57%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 68.51%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 67.20%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 66.65%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 66.27%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 65.96%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 67.63%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 67.13%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 66.94%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 66.76%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 67.36%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 67.05%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 66.64%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 66.28%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 67.30%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 66.40%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 66.03%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 65.84%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 65.74%   [EVAL] batch:  160 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 65.55%   [EVAL] batch:  162 | acc: 18.75%,  total acc: 65.26%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 64.86%   [EVAL] batch:  164 | acc: 6.25%,  total acc: 64.51%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 64.12%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 63.74%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 63.36%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 63.09%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 62.94%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 62.90%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 62.75%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 62.72%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 62.75%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 62.68%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 62.57%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 62.46%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 62.33%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 62.26%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 62.22%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 62.23%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 62.47%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 62.57%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 62.63%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 62.77%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 62.93%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 63.57%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 63.69%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 63.52%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 63.26%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 63.10%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 63.00%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 63.06%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 62.99%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 62.99%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 62.99%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 62.89%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 62.62%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 62.38%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 62.23%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 62.03%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 62.03%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.21%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 63.04%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 63.09%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 63.23%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 65.19%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 66.41%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 66.47%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 66.29%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 66.02%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 66.45%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 66.38%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 66.34%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 66.13%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 66.05%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  286 | acc: 37.50%,  total acc: 65.81%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  288 | acc: 25.00%,  total acc: 65.46%   [EVAL] batch:  289 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  290 | acc: 25.00%,  total acc: 65.18%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 65.05%   [EVAL] batch:  292 | acc: 18.75%,  total acc: 64.89%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 64.80%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 64.80%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 64.85%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 66.04%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 65.87%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 65.72%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 65.54%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.39%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 65.32%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.58%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.63%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 65.39%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 65.31%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.82%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 66.77%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 66.62%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 66.50%   [EVAL] batch:  353 | acc: 12.50%,  total acc: 66.35%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 66.21%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 66.08%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 66.61%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 66.60%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 66.58%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  369 | acc: 18.75%,  total acc: 66.42%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 66.34%   [EVAL] batch:  371 | acc: 18.75%,  total acc: 66.21%   [EVAL] batch:  372 | acc: 25.00%,  total acc: 66.10%   [EVAL] batch:  373 | acc: 31.25%,  total acc: 66.01%   [EVAL] batch:  374 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 65.87%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 65.80%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 65.72%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 65.63%   [EVAL] batch:  379 | acc: 50.00%,  total acc: 65.59%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 65.47%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 65.45%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.59%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  394 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:  395 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 65.88%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 65.84%   [EVAL] batch:  398 | acc: 18.75%,  total acc: 65.73%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 65.64%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.49%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 65.35%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.18%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 65.04%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 64.88%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.72%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 65.02%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  422 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 65.80%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  431 | acc: 18.75%,  total acc: 65.83%   [EVAL] batch:  432 | acc: 0.00%,  total acc: 65.68%   [EVAL] batch:  433 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  434 | acc: 0.00%,  total acc: 65.37%   [EVAL] batch:  435 | acc: 0.00%,  total acc: 65.22%   [EVAL] batch:  436 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:  437 | acc: 0.00%,  total acc: 64.93%   
cur_acc:  ['0.9484', '0.7619', '0.7093', '0.7738', '0.7966', '0.6567', '0.5883']
his_acc:  ['0.9484', '0.8480', '0.7786', '0.7618', '0.7550', '0.6927', '0.6493']
Clustering into  39  clusters
Clusters:  [31  0 23  2  1 35 24 10 14 14  8 38  7  1 25 20 19  0  3  3  9  3 22  3
 26  0  8 29  0  3  3  2 12  6 27  3  7 33 28  0  0 21  3  3 10 37  5  3
 34  5 36  3 13 17  3  3  4 18  6 32  0  3  8  7  9  3  5  3  5  3 11 30
 15  3  3  3 16  4  3  4]
Losses:  12.237550981342793 1.0 1.0296995639801025 4.271560914814472
CurrentTrain: epoch  0, batch     0 | loss: 12.2375510Losses:  13.527271144092083 1.0 1.0320515632629395 6.413839690387249
CurrentTrain: epoch  0, batch     1 | loss: 13.5272711Losses:  11.143075168132782 1.0 1.028889536857605 1.7735339999198914
CurrentTrain: epoch  0, batch     2 | loss: 11.1430752Losses:  21.482620544731617 1.0 1.0222530364990234 13.53555042296648
CurrentTrain: epoch  0, batch     3 | loss: 21.4826205Losses:  14.479226116091013 1.0 1.0226893424987793 6.59048605337739
CurrentTrain: epoch  1, batch     0 | loss: 14.4792261Losses:  8.968485951423645 1.0 1.0222222805023193 1.4090214967727661
CurrentTrain: epoch  1, batch     1 | loss: 8.9684860Losses:  8.576111927628517 1.0 1.034135103225708 1.4384242445230484
CurrentTrain: epoch  1, batch     2 | loss: 8.5761119Losses:  8.923068884760141 1.0 1.0138063430786133 1.5441402234137058
CurrentTrain: epoch  1, batch     3 | loss: 8.9230689Losses:  11.762843824923038 1.0 1.0333524942398071 4.116359926760197
CurrentTrain: epoch  2, batch     0 | loss: 11.7628438Losses:  7.546780198812485 1.0 1.028324007987976 1.422027200460434
CurrentTrain: epoch  2, batch     1 | loss: 7.5467802Losses:  13.750397380441427 1.0 1.025469422340393 5.559318240731955
CurrentTrain: epoch  2, batch     2 | loss: 13.7503974Losses:  6.04942923784256 1.0 1.0372240543365479 1.3986003994941711
CurrentTrain: epoch  2, batch     3 | loss: 6.0494292Losses:  7.031702041625977 1.0 1.0308761596679688 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.0317020Losses:  7.936911631375551 1.0 1.0284247398376465 1.4518580920994282
CurrentTrain: epoch  3, batch     1 | loss: 7.9369116Losses:  6.9637227058410645 1.0 1.0304902791976929 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 6.9637227Losses:  10.663104832172394 1.0 1.0258764028549194 1.4263504147529602
CurrentTrain: epoch  3, batch     3 | loss: 10.6631048Losses:  8.808893736451864 1.0 1.0240418910980225 1.431271132081747
CurrentTrain: epoch  4, batch     0 | loss: 8.8088937Losses:  6.277535438537598 1.0 1.0384409427642822 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 6.2775354Losses:  10.902259968221188 1.0 1.0253310203552246 4.8269659504294395
CurrentTrain: epoch  4, batch     2 | loss: 10.9022600Losses:  10.789163917303085 1.0 1.0262787342071533 1.5407460629940033
CurrentTrain: epoch  4, batch     3 | loss: 10.7891639Losses:  7.847242563962936 1.0 1.0214558839797974 1.4026019275188446
CurrentTrain: epoch  5, batch     0 | loss: 7.8472426Losses:  10.20067037269473 1.0 1.0319023132324219 4.315966736525297
CurrentTrain: epoch  5, batch     1 | loss: 10.2006704Losses:  8.378109008073807 1.0 1.0290336608886719 1.4150238335132599
CurrentTrain: epoch  5, batch     2 | loss: 8.3781090Losses:  8.135458327829838 1.0 1.0313823223114014 1.475438453257084
CurrentTrain: epoch  5, batch     3 | loss: 8.1354583Losses:  6.394710063934326 1.0 1.0277776718139648 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 6.3947101Losses:  8.903224896639585 1.0 1.0267269611358643 2.8897332660853863
CurrentTrain: epoch  6, batch     1 | loss: 8.9032249Losses:  6.129305362701416 1.0 1.036476492881775 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 6.1293054Losses:  7.961262218654156 1.0 1.0160771608352661 1.5137214586138725
CurrentTrain: epoch  6, batch     3 | loss: 7.9612622Losses:  5.490489482879639 1.0 1.0288128852844238 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.4904895Losses:  12.400417935103178 1.0 1.0228872299194336 5.866123329848051
CurrentTrain: epoch  7, batch     1 | loss: 12.4004179Losses:  10.63491078838706 1.0 1.0270566940307617 4.339966978877783
CurrentTrain: epoch  7, batch     2 | loss: 10.6349108Losses:  5.787975467741489 1.0 1.0344526767730713 1.457391895353794
CurrentTrain: epoch  7, batch     3 | loss: 5.7879755Losses:  15.356690034270287 1.0 1.0237221717834473 9.567885503172874
CurrentTrain: epoch  8, batch     0 | loss: 15.3566900Losses:  11.586704693734646 1.0 1.0274524688720703 5.424383603036404
CurrentTrain: epoch  8, batch     1 | loss: 11.5867047Losses:  7.029809225350618 1.0 1.0301964282989502 1.5024797804653645
CurrentTrain: epoch  8, batch     2 | loss: 7.0298092Losses:  8.343588203191757 1.0 1.0137282609939575 1.3923448026180267
CurrentTrain: epoch  8, batch     3 | loss: 8.3435882Losses:  8.883139483630657 1.0 1.0279855728149414 2.835299365222454
CurrentTrain: epoch  9, batch     0 | loss: 8.8831395Losses:  6.588739547878504 1.0 1.0272055864334106 1.4116078950464725
CurrentTrain: epoch  9, batch     1 | loss: 6.5887395Losses:  7.192124132066965 1.0 1.025970697402954 1.465378526598215
CurrentTrain: epoch  9, batch     2 | loss: 7.1921241Losses:  7.627379387617111 1.0 1.0311250686645508 1.4518947303295135
CurrentTrain: epoch  9, batch     3 | loss: 7.6273794
Losses:  2.1680169105529785 1.0 1.0200425386428833 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.1680169Losses:  2.5490245819091797 1.0 1.0175514221191406 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.5490246Losses:  2.208491802215576 1.0 1.0236726999282837 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.2084918Losses:  2.1503491401672363 1.0 1.0261436700820923 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 2.1503491Losses:  2.513261079788208 1.0 1.0241639614105225 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 2.5132611Losses:  2.4795613288879395 1.0 1.034783124923706 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.4795613Losses:  2.46677303314209 1.0 1.023508071899414 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.4667730Losses:  2.397599458694458 1.0 1.0185022354125977 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.3975995Losses:  2.2751998901367188 1.0 1.0202569961547852 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 2.2751999Losses:  2.2179007530212402 1.0 1.0171781778335571 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 2.2179008Losses:  2.307326078414917 1.0 1.0216598510742188 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.3073261Losses:  2.2285187244415283 1.0 1.025054931640625 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.2285187Losses:  2.143744468688965 1.0 1.0290663242340088 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.1437445Losses:  2.189183235168457 1.0 1.0239354372024536 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 2.1891832Losses:  2.1226601600646973 1.0 1.0184807777404785 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 2.1226602Losses:  2.1363096237182617 1.0 1.0218477249145508 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.1363096Losses:  2.084977626800537 1.0 1.0226852893829346 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 2.0849776Losses:  2.1060168743133545 1.0 1.0231109857559204 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.1060169Losses:  2.1482396125793457 1.0 1.0208568572998047 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 2.1482396Losses:  2.0702929496765137 1.0 1.020405888557434 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 2.0702929Losses:  2.089954376220703 1.0 1.0254775285720825 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.0899544Losses:  2.10483980178833 1.0 1.0199540853500366 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 2.1048398Losses:  2.083333969116211 1.0 1.0278549194335938 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 2.0833340Losses:  2.075471878051758 1.0 1.0165339708328247 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 2.0754719Losses:  2.0594513416290283 1.0 1.0181835889816284 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 2.0594513Losses:  2.0651636123657227 1.0 1.0224246978759766 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.0651636Losses:  2.0802111625671387 1.0 1.0275709629058838 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 2.0802112Losses:  2.0684118270874023 1.0 1.0218762159347534 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 2.0684118Losses:  2.0788187980651855 1.0 1.021864652633667 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 2.0788188Losses:  2.075939655303955 1.0 1.0156692266464233 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 2.0759397Losses:  2.073103666305542 1.0 1.0282286405563354 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.0731037Losses:  2.0641069412231445 1.0 1.0197741985321045 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 2.0641069Losses:  2.066983938217163 1.0 1.0258386135101318 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 2.0669839Losses:  2.0847668647766113 1.0 1.0234110355377197 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 2.0847669Losses:  2.043891668319702 1.0 1.0113917589187622 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 2.0438917Losses:  2.065938949584961 1.0 1.0196561813354492 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.0659389Losses:  2.0571372509002686 1.0 1.0266995429992676 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 2.0571373Losses:  2.0559499263763428 1.0 1.020853042602539 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 2.0559499Losses:  2.0880327224731445 1.0 1.0186092853546143 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 2.0880327Losses:  2.056931257247925 1.0 1.0188074111938477 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 2.0569313Losses:  2.056403875350952 1.0 1.0224494934082031 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.0564039Losses:  2.0490081310272217 1.0 1.0172176361083984 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 2.0490081Losses:  2.0562491416931152 1.0 1.0186026096343994 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 2.0562491Losses:  2.0784912109375 1.0 1.023053765296936 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 2.0784912Losses:  2.0660858154296875 1.0 1.0245270729064941 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 2.0660858Losses:  2.0617237091064453 1.0 1.0193994045257568 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.0617237Losses:  2.03532338142395 1.0 1.0147745609283447 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 2.0353234Losses:  2.0612239837646484 1.0 1.0179234743118286 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 2.0612240Losses:  2.0583384037017822 1.0 1.0266742706298828 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 2.0583384Losses:  2.050921678543091 1.0 1.0202654600143433 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 2.0509217
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 42.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 58.55%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 54.62%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 53.91%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 54.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 55.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 60.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 70.65%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 69.74%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 68.97%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 67.40%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 66.73%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 66.07%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 58.07%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 56.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 73.30%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 66.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 68.86%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 67.92%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.06%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 66.60%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 65.34%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 64.74%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 64.34%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 64.13%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 66.40%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 66.46%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 66.34%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 66.59%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 66.55%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 66.29%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 65.76%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 65.08%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 64.65%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 64.36%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 64.14%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 63.87%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 63.72%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 63.58%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 63.11%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 62.92%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 62.92%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 62.97%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 62.73%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 62.27%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 61.87%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 61.42%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 61.15%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 60.83%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 60.79%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 61.13%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 61.47%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 61.80%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 62.86%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 62.76%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 62.86%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 62.80%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 62.80%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 62.80%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 62.40%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 62.31%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 62.16%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 62.12%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 62.31%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 62.78%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 62.96%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 63.17%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 62.86%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 62.63%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 62.46%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 62.33%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 62.15%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 62.84%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 63.16%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 62.79%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 62.18%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 61.85%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 61.54%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 61.51%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 61.48%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 61.41%   [EVAL] batch:  160 | acc: 43.75%,  total acc: 61.30%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 61.34%   [EVAL] batch:  162 | acc: 18.75%,  total acc: 61.08%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 60.71%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 60.05%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 59.69%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 59.34%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 59.10%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 59.12%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 59.25%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 59.27%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 59.25%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 59.27%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 59.39%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 59.20%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 59.00%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 58.92%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 58.80%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 58.65%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 58.63%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 58.72%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 58.85%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 59.04%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 59.16%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 59.27%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 59.43%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 59.61%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 59.79%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 60.21%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 60.32%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 60.52%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 60.70%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 60.51%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 60.36%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 60.28%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 60.20%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 60.05%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 59.97%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 59.98%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 60.09%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 60.16%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 60.08%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 60.12%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 60.11%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 59.95%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 59.81%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 59.64%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 59.54%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 59.35%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 59.36%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 59.55%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 59.74%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 59.92%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 60.29%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 60.45%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 60.43%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 60.52%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 60.59%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 60.69%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 60.64%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 61.16%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 61.49%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 61.63%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 61.80%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 62.29%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 62.58%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 62.55%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 62.68%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 62.78%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 63.30%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 63.32%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 63.27%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 63.26%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 63.33%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 63.40%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 63.42%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 63.31%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 63.11%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 62.95%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 62.87%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 62.76%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 62.78%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 63.14%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 63.26%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 63.50%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 63.47%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 63.44%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 63.48%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 63.45%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 63.43%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 63.29%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 63.14%   [EVAL] batch:  285 | acc: 31.25%,  total acc: 63.02%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 62.91%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 62.74%   [EVAL] batch:  288 | acc: 25.00%,  total acc: 62.61%   [EVAL] batch:  289 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:  290 | acc: 25.00%,  total acc: 62.37%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 62.24%   [EVAL] batch:  292 | acc: 31.25%,  total acc: 62.14%   [EVAL] batch:  293 | acc: 43.75%,  total acc: 62.07%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 62.08%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 62.08%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 62.14%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 62.14%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 62.21%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 62.46%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 62.58%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 62.71%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.15%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 63.23%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 63.38%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 63.19%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 63.01%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 62.84%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 62.66%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 62.58%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 62.73%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 62.79%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 62.94%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 62.86%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 62.69%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 62.54%   [EVAL] batch:  328 | acc: 6.25%,  total acc: 62.37%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 62.18%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 62.03%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 62.09%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.31%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 62.72%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 62.99%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 63.45%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 63.71%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 63.55%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 63.42%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 63.33%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 63.21%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 63.06%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 62.94%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 63.01%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 63.09%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 63.55%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 63.58%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 63.53%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 63.52%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 63.45%   [EVAL] batch:  369 | acc: 18.75%,  total acc: 63.33%   [EVAL] batch:  370 | acc: 25.00%,  total acc: 63.22%   [EVAL] batch:  371 | acc: 18.75%,  total acc: 63.10%   [EVAL] batch:  372 | acc: 18.75%,  total acc: 62.99%   [EVAL] batch:  373 | acc: 37.50%,  total acc: 62.92%   [EVAL] batch:  374 | acc: 25.00%,  total acc: 62.82%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 62.73%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 62.63%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 62.55%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 62.43%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 62.38%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 62.27%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 62.25%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 62.34%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 62.40%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 62.45%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 62.48%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 62.52%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 62.58%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 62.63%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 62.68%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:  391 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 62.93%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 62.80%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 62.56%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 62.47%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 62.34%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 62.23%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 62.09%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 61.94%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 61.79%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 61.65%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 61.50%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 61.35%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 61.35%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 61.41%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 61.48%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 61.70%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 61.74%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 61.79%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 61.87%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 61.94%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 61.96%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 62.17%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 62.20%   [EVAL] batch:  421 | acc: 87.50%,  total acc: 62.26%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 62.29%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 62.35%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 62.31%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 62.30%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 62.32%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 62.35%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  431 | acc: 6.25%,  total acc: 62.25%   [EVAL] batch:  432 | acc: 0.00%,  total acc: 62.11%   [EVAL] batch:  433 | acc: 0.00%,  total acc: 61.97%   [EVAL] batch:  434 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:  435 | acc: 0.00%,  total acc: 61.68%   [EVAL] batch:  436 | acc: 0.00%,  total acc: 61.54%   [EVAL] batch:  437 | acc: 0.00%,  total acc: 61.40%   [EVAL] batch:  438 | acc: 6.25%,  total acc: 61.28%   [EVAL] batch:  439 | acc: 6.25%,  total acc: 61.15%   [EVAL] batch:  440 | acc: 12.50%,  total acc: 61.04%   [EVAL] batch:  441 | acc: 6.25%,  total acc: 60.92%   [EVAL] batch:  442 | acc: 25.00%,  total acc: 60.84%   [EVAL] batch:  443 | acc: 18.75%,  total acc: 60.74%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 60.85%   [EVAL] batch:  446 | acc: 56.25%,  total acc: 60.84%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 60.90%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 60.95%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 61.01%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 61.12%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 61.16%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 61.22%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 61.29%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:  457 | acc: 31.25%,  total acc: 61.30%   [EVAL] batch:  458 | acc: 25.00%,  total acc: 61.22%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 61.18%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 61.10%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 61.07%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 61.19%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 61.28%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 61.35%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 61.43%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 61.51%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 62.30%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 62.51%   [EVAL] batch:  481 | acc: 43.75%,  total acc: 62.47%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 62.40%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 62.33%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 62.29%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 62.27%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 62.23%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 62.24%   [EVAL] batch:  488 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:  490 | acc: 68.75%,  total acc: 62.37%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 62.41%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 62.46%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 62.46%   [EVAL] batch:  494 | acc: 25.00%,  total acc: 62.39%   [EVAL] batch:  495 | acc: 18.75%,  total acc: 62.30%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 62.25%   [EVAL] batch:  497 | acc: 25.00%,  total acc: 62.17%   [EVAL] batch:  498 | acc: 43.75%,  total acc: 62.14%   [EVAL] batch:  499 | acc: 50.00%,  total acc: 62.11%   
cur_acc:  ['0.9484', '0.7619', '0.7093', '0.7738', '0.7966', '0.6567', '0.5883', '0.6607']
his_acc:  ['0.9484', '0.8480', '0.7786', '0.7618', '0.7550', '0.6927', '0.6493', '0.6211']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  13.956730455160141 1.0 1.0373375415802002 2.01093253493309
CurrentTrain: epoch  0, batch     0 | loss: 13.9567305Losses:  21.323113191872835 1.0 1.0269912481307983 9.612203348428011
CurrentTrain: epoch  0, batch     1 | loss: 21.3231132Losses:  21.569124698638916 1.0 1.0219587087631226 9.802808284759521
CurrentTrain: epoch  0, batch     2 | loss: 21.5691247Losses:  18.56618256494403 1.0 1.0222547054290771 6.472939919680357
CurrentTrain: epoch  0, batch     3 | loss: 18.5661826Losses:  13.096308536827564 1.0 1.026660680770874 1.5340088084340096
CurrentTrain: epoch  0, batch     4 | loss: 13.0963085Losses:  17.218542974442244 1.0 1.0292470455169678 5.752493780106306
CurrentTrain: epoch  0, batch     5 | loss: 17.2185430Losses:  17.37195574119687 1.0 1.0295512676239014 5.822752822190523
CurrentTrain: epoch  0, batch     6 | loss: 17.3719557Losses:  12.120676040649414 1.0 1.0313267707824707 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 12.1206760Losses:  13.622089989483356 1.0 1.0232479572296143 1.5999523475766182
CurrentTrain: epoch  0, batch     8 | loss: 13.6220900Losses:  15.661752317100763 1.0 1.025040626525879 3.6914583183825016
CurrentTrain: epoch  0, batch     9 | loss: 15.6617523Losses:  16.756335359066725 1.0 1.0308812856674194 5.858198266476393
CurrentTrain: epoch  0, batch    10 | loss: 16.7563354Losses:  15.027277171611786 1.0 1.021837592124939 2.907982051372528
CurrentTrain: epoch  0, batch    11 | loss: 15.0272772Losses:  19.837870873510838 1.0 1.0251007080078125 8.74661187082529
CurrentTrain: epoch  0, batch    12 | loss: 19.8378709Losses:  11.662822723388672 1.0 1.029205083847046 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 11.6628227Losses:  13.527719113975763 1.0 1.027571439743042 1.9158960320055485
CurrentTrain: epoch  0, batch    14 | loss: 13.5277191Losses:  16.098542973399162 1.0 1.0285850763320923 3.668828770518303
CurrentTrain: epoch  0, batch    15 | loss: 16.0985430Losses:  14.684267729520798 1.0 1.0230944156646729 3.798887938261032
CurrentTrain: epoch  0, batch    16 | loss: 14.6842677Losses:  14.54346565157175 1.0 1.0253450870513916 3.2266207113862038
CurrentTrain: epoch  0, batch    17 | loss: 14.5434657Losses:  12.61482948064804 1.0 1.027348279953003 1.5132727026939392
CurrentTrain: epoch  0, batch    18 | loss: 12.6148295Losses:  10.675618171691895 1.0 1.0224858522415161 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 10.6756182Losses:  21.844389863312244 1.0 1.029215693473816 10.826594300568104
CurrentTrain: epoch  0, batch    20 | loss: 21.8443899Losses:  18.105515018105507 1.0 1.0179641246795654 7.1225123554468155
CurrentTrain: epoch  0, batch    21 | loss: 18.1055150Losses:  10.768960952758789 1.0 1.0276070833206177 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 10.7689610Losses:  13.167390022426844 1.0 1.0283724069595337 1.4440347291529179
CurrentTrain: epoch  0, batch    23 | loss: 13.1673900Losses:  12.285377502441406 1.0 1.0246310234069824 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 12.2853775Losses:  10.40722942352295 1.0 1.0272397994995117 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 10.4072294Losses:  22.308638345450163 1.0 1.0211459398269653 11.144416581839323
CurrentTrain: epoch  0, batch    26 | loss: 22.3086383Losses:  9.933527946472168 1.0 1.0203993320465088 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 9.9335279Losses:  12.98951057344675 1.0 1.0214242935180664 1.6640282049775124
CurrentTrain: epoch  0, batch    28 | loss: 12.9895106Losses:  13.105582162737846 1.0 1.0259935855865479 3.065622255206108
CurrentTrain: epoch  0, batch    29 | loss: 13.1055822Losses:  25.61930286884308 1.0 1.030004858970642 14.78330910205841
CurrentTrain: epoch  0, batch    30 | loss: 25.6193029Losses:  21.072259955108166 1.0 1.0231153964996338 9.826880507171154
CurrentTrain: epoch  0, batch    31 | loss: 21.0722600Losses:  17.71601739525795 1.0 1.0253796577453613 6.762186676263809
CurrentTrain: epoch  0, batch    32 | loss: 17.7160174Losses:  17.440934844315052 1.0 1.0232254266738892 5.549921698868275
CurrentTrain: epoch  0, batch    33 | loss: 17.4409348Losses:  10.82626724243164 1.0 1.0258313417434692 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 10.8262672Losses:  13.546708568930626 1.0 1.025172233581543 2.118962749838829
CurrentTrain: epoch  0, batch    35 | loss: 13.5467086Losses:  10.225821495056152 1.0 1.025160789489746 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 10.2258215Losses:  13.134246081113815 1.0 1.0269498825073242 3.0426075160503387
CurrentTrain: epoch  0, batch    37 | loss: 13.1342461Losses:  15.423441544175148 1.0 1.0204007625579834 4.5986486822366714
CurrentTrain: epoch  0, batch    38 | loss: 15.4234415Losses:  12.701305244117975 1.0 1.0184961557388306 2.873015258461237
CurrentTrain: epoch  0, batch    39 | loss: 12.7013052Losses:  15.700953248888254 1.0 1.0329580307006836 5.412532571703196
CurrentTrain: epoch  0, batch    40 | loss: 15.7009532Losses:  15.787290485575795 1.0 1.0154972076416016 6.337039859965444
CurrentTrain: epoch  0, batch    41 | loss: 15.7872905Losses:  12.228690266609192 1.0 1.0262317657470703 2.40308678150177
CurrentTrain: epoch  0, batch    42 | loss: 12.2286903Losses:  11.744800630956888 1.0 1.0287079811096191 1.4738503135740757
CurrentTrain: epoch  0, batch    43 | loss: 11.7448006Losses:  15.528588090091944 1.0 1.0216773748397827 4.294211182743311
CurrentTrain: epoch  0, batch    44 | loss: 15.5285881Losses:  17.30452185869217 1.0 1.024745225906372 6.181390106678009
CurrentTrain: epoch  0, batch    45 | loss: 17.3045219Losses:  13.283276289701462 1.0 1.0282647609710693 3.083172529935837
CurrentTrain: epoch  0, batch    46 | loss: 13.2832763Losses:  14.36301502212882 1.0 1.0261842012405396 4.530065383762121
CurrentTrain: epoch  0, batch    47 | loss: 14.3630150Losses:  15.633274242281914 1.0 1.0153367519378662 4.463144466280937
CurrentTrain: epoch  0, batch    48 | loss: 15.6332742Losses:  12.188842263072729 1.0 1.0261809825897217 2.9714331291615963
CurrentTrain: epoch  0, batch    49 | loss: 12.1888423Losses:  11.272475242614746 1.0 1.0149527788162231 -0.0
CurrentTrain: epoch  0, batch    50 | loss: 11.2724752Losses:  11.50648520886898 1.0 1.0205724239349365 1.5285026878118515
CurrentTrain: epoch  0, batch    51 | loss: 11.5064852Losses:  12.394642811268568 1.0 1.0261986255645752 3.0291013531386852
CurrentTrain: epoch  0, batch    52 | loss: 12.3946428Losses:  12.802516397088766 1.0 1.0210795402526855 3.3404440246522427
CurrentTrain: epoch  0, batch    53 | loss: 12.8025164Losses:  15.24072390422225 1.0 1.0210013389587402 5.080815609544516
CurrentTrain: epoch  0, batch    54 | loss: 15.2407239Losses:  14.914723347872496 1.0 1.0191104412078857 3.0097245685756207
CurrentTrain: epoch  0, batch    55 | loss: 14.9147233Losses:  11.495110511779785 1.0 1.0213481187820435 -0.0
CurrentTrain: epoch  0, batch    56 | loss: 11.4951105Losses:  10.84017500281334 1.0 1.0229830741882324 1.418771117925644
CurrentTrain: epoch  0, batch    57 | loss: 10.8401750Losses:  16.78521517664194 1.0 1.019179344177246 5.6933982744812965
CurrentTrain: epoch  0, batch    58 | loss: 16.7852152Losses:  10.25722885131836 1.0 1.0297026634216309 -0.0
CurrentTrain: epoch  0, batch    59 | loss: 10.2572289Losses:  19.79263147711754 1.0 1.0195013284683228 9.27841505408287
CurrentTrain: epoch  0, batch    60 | loss: 19.7926315Losses:  20.078995883464813 1.0 1.0234382152557373 10.054758250713348
CurrentTrain: epoch  0, batch    61 | loss: 20.0789959Losses:  11.781017988920212 1.0 1.0238285064697266 1.513975828886032
CurrentTrain: epoch  0, batch    62 | loss: 11.7810180Losses:  9.598448753356934 1.0 1.0321929454803467 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.5984488Losses:  10.555906295776367 1.0 1.0232748985290527 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 10.5559063Losses:  11.516011237166822 1.0 1.0218451023101807 1.944060324691236
CurrentTrain: epoch  1, batch     2 | loss: 11.5160112Losses:  16.007619857788086 1.0 1.029362440109253 6.306938171386719
CurrentTrain: epoch  1, batch     3 | loss: 16.0076199Losses:  10.746323473751545 1.0 1.0214555263519287 1.7531956508755684
CurrentTrain: epoch  1, batch     4 | loss: 10.7463235Losses:  10.220171928405762 1.0 1.0262556076049805 -0.0
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  17.19092888198793 1.0 1.0250855684280396 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 17.1909289Losses:  17.776122633367777 1.0 1.2115342617034912 5.836158338934183
CurrentTrain: epoch  0, batch     1 | loss: 17.7761226Losses:  16.400510739535093 1.0 1.0061943531036377 4.393155049532652
CurrentTrain: epoch  0, batch     2 | loss: 16.4005107Losses:  16.611083336174488 1.0 1.2297340631484985 5.154274292290211
CurrentTrain: epoch  0, batch     3 | loss: 16.6110833Losses:  14.660130862146616 1.0 1.0420200824737549 2.5182374753057957
CurrentTrain: epoch  0, batch     4 | loss: 14.6601309Losses:  15.164468865841627 1.0 1.0324199199676514 4.3953867964446545
CurrentTrain: epoch  0, batch     5 | loss: 15.1644689Losses:  19.929780710488558 1.0 0.9626924395561218 8.289335954934359
CurrentTrain: epoch  0, batch     6 | loss: 19.9297807Losses:  15.698720447719097 1.0 1.0462214946746826 3.856923572719097
CurrentTrain: epoch  0, batch     7 | loss: 15.6987204Losses:  22.66250116378069 1.0 0.8255126476287842 10.493204899132252
CurrentTrain: epoch  0, batch     8 | loss: 22.6625012Losses:  13.946750454604626 1.0 1.0875823497772217 2.8914239928126335
CurrentTrain: epoch  0, batch     9 | loss: 13.9467505Losses:  15.038194462656975 1.0 1.0404760837554932 3.8491609543561935
CurrentTrain: epoch  0, batch    10 | loss: 15.0381945Losses:  14.838631391525269 1.0 0.8111696839332581 3.0636746883392334
CurrentTrain: epoch  0, batch    11 | loss: 14.8386314Losses:  12.097480773925781 1.0 0.9873930215835571 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 12.0974808Losses:  11.915424346923828 1.0 0.9474907517433167 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 11.9154243Losses:  17.344141840934753 1.0 0.8898371458053589 5.339642405509949
CurrentTrain: epoch  0, batch    14 | loss: 17.3441418Losses:  16.198741786181927 1.0 0.9706019759178162 5.030529849231243
CurrentTrain: epoch  0, batch    15 | loss: 16.1987418Losses:  15.215901844203472 1.0 1.1418434381484985 4.00516652315855
CurrentTrain: epoch  0, batch    16 | loss: 15.2159018Losses:  15.09877823293209 1.0 0.8378108739852905 3.7943978160619736
CurrentTrain: epoch  0, batch    17 | loss: 15.0987782Losses:  16.727753028273582 1.0 0.7063845992088318 5.077537879347801
CurrentTrain: epoch  0, batch    18 | loss: 16.7277530Losses:  12.686230450868607 1.0 0.7257097959518433 1.4326379597187042
CurrentTrain: epoch  0, batch    19 | loss: 12.6862305Losses:  14.38270240649581 1.0 0.8398876190185547 4.26747852191329
CurrentTrain: epoch  0, batch    20 | loss: 14.3827024Losses:  15.438594482839108 1.0 0.6985852718353271 5.034246109426022
CurrentTrain: epoch  0, batch    21 | loss: 15.4385945Losses:  19.63425025716424 1.0 0.9831947088241577 8.662816617637873
CurrentTrain: epoch  0, batch    22 | loss: 19.6342503Losses:  12.895092636346817 1.0 0.9222081303596497 1.4382759630680084
CurrentTrain: epoch  0, batch    23 | loss: 12.8950926Losses:  16.817677818238735 1.0 1.039725422859192 5.698330245912075
CurrentTrain: epoch  0, batch    24 | loss: 16.8176778Losses:  11.151025772094727 1.0 0.8558220863342285 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 11.1510258Losses:  10.671306610107422 1.0 0.7182021141052246 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 10.6713066Losses:  15.734589539468288 1.0 0.8203450441360474 4.758479081094265
CurrentTrain: epoch  0, batch    27 | loss: 15.7345895Losses:  14.885970771312714 1.0 0.6799771785736084 4.56301087141037
CurrentTrain: epoch  0, batch    28 | loss: 14.8859708Losses:  10.76115608215332 1.0 0.7942832112312317 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 10.7611561Losses:  9.668974876403809 1.0 0.7760932445526123 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 9.6689749Losses:  14.835297510027885 1.0 0.8982730507850647 5.140039369463921
CurrentTrain: epoch  0, batch    31 | loss: 14.8352975Losses:  10.163945198059082 1.0 0.7845355272293091 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 10.1639452Losses:  10.633231937885284 1.0 0.8200997114181519 1.4032695889472961
CurrentTrain: epoch  0, batch    33 | loss: 10.6332319Losses:  13.711737107485533 1.0 0.565471887588501 3.2337955944240093
CurrentTrain: epoch  0, batch    34 | loss: 13.7117371Losses:  13.993158370256424 1.0 0.7166365385055542 3.3377504646778107
CurrentTrain: epoch  0, batch    35 | loss: 13.9931584Losses:  12.114247746765614 1.0 0.8526241779327393 1.778642125427723
CurrentTrain: epoch  0, batch    36 | loss: 12.1142477Losses:  11.556834936141968 1.0 0.7864603996276855 1.4236867427825928
CurrentTrain: epoch  0, batch    37 | loss: 11.5568349Losses:  9.563852310180664 1.0 0.6638073921203613 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 9.5638523Losses:  15.759041396901011 1.0 0.474790096282959 5.086605636402965
CurrentTrain: epoch  0, batch    39 | loss: 15.7590414Losses:  17.453247778117657 1.0 0.7399405241012573 7.290780775249004
CurrentTrain: epoch  0, batch    40 | loss: 17.4532478Losses:  16.784338921308517 1.0 0.7120108604431152 5.788746803998947
CurrentTrain: epoch  0, batch    41 | loss: 16.7843389Losses:  14.352558020502329 1.0 0.8418364524841309 3.7645796574652195
CurrentTrain: epoch  0, batch    42 | loss: 14.3525580Losses:  16.333626240491867 1.0 0.6857556104660034 6.511062115430832
CurrentTrain: epoch  0, batch    43 | loss: 16.3336262Losses:  11.804166566580534 1.0 0.6407393217086792 1.83189083263278
CurrentTrain: epoch  0, batch    44 | loss: 11.8041666Losses:  9.591232299804688 1.0 0.7503372430801392 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 9.5912323Losses:  14.999192871153355 1.0 0.5747659802436829 4.8299748077988625
CurrentTrain: epoch  0, batch    46 | loss: 14.9991929Losses:  15.331865936517715 1.0 0.7017168998718262 4.913065582513809
CurrentTrain: epoch  0, batch    47 | loss: 15.3318659Losses:  12.87187214102596 1.0 0.9304563999176025 2.380268289707601
CurrentTrain: epoch  0, batch    48 | loss: 12.8718721Losses:  12.07006024941802 1.0 0.6522409915924072 1.5183939896523952
CurrentTrain: epoch  0, batch    49 | loss: 12.0700602Losses:  18.716935597360134 1.0 0.7154830694198608 8.451247654855251
CurrentTrain: epoch  0, batch    50 | loss: 18.7169356Losses:  13.45713061466813 1.0 0.656700611114502 4.667137328535318
CurrentTrain: epoch  0, batch    51 | loss: 13.4571306Losses:  10.788411140441895 1.0 0.706734299659729 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 10.7884111Losses:  15.442901186645031 1.0 0.8103162050247192 5.312033228576183
CurrentTrain: epoch  0, batch    53 | loss: 15.4429012Losses:  16.658569436520338 1.0 0.7473769187927246 7.127317529171705
CurrentTrain: epoch  0, batch    54 | loss: 16.6585694Losses:  10.614011503756046 1.0 0.6394611597061157 1.5330674424767494
CurrentTrain: epoch  0, batch    55 | loss: 10.6140115Losses:  22.12703911960125 1.0 0.736427903175354 11.912769481539726
CurrentTrain: epoch  0, batch    56 | loss: 22.1270391Losses:  12.25825548171997 1.0 0.7625428438186646 2.9744067192077637
CurrentTrain: epoch  0, batch    57 | loss: 12.2582555Losses:  13.3108448125422 1.0 0.7771201133728027 3.691469583660364
CurrentTrain: epoch  0, batch    58 | loss: 13.3108448Losses:  11.430152170360088 1.0 0.777213454246521 1.514124147593975
CurrentTrain: epoch  0, batch    59 | loss: 11.4301522Losses:  25.027139581739902 1.0 0.7003209590911865 15.957418359816074
CurrentTrain: epoch  0, batch    60 | loss: 25.0271396Losses:  13.323717627674341 1.0 0.6878143548965454 4.562143836170435
CurrentTrain: epoch  0, batch    61 | loss: 13.3237176Losses:  11.864003956317902 1.0 0.9836053848266602 1.5986040234565735
CurrentTrain: epoch  0, batch    62 | loss: 11.8640040Losses:  9.77999496459961 1.0 0.5177245140075684 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.7799950Losses:  13.283170074690133 1.0 0.8055438995361328 4.093625396955758
CurrentTrain: epoch  1, batch     1 | loss: 13.2831701Losses:  15.504002530127764 1.0 0.6634035110473633 6.507232625037432
CurrentTrain: epoch  1, batch     2 | loss: 15.5040025Losses:  10.816647253930569 1.0 0.7122092247009277 1.4835421666502953
CurrentTrain: epoch  1, batch     3 | loss: 10.8166473Losses:  11.55927125364542 1.0 0.6796398162841797 1.8209041729569435
CurrentTrain: epoch  1, batch     4 | loss: 11.5592713Losses:  9.949140667915344 1.0 0.8812152147293091 1.3985234498977661
CurrentTrain: epoch  1, batch     5 | loss: 9.9491407Losses:  9.526554107666016 1.0 0.7099740505218506 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 9.5265541Losses:  11.358795322477818 1.0 0.7058086395263672 1.7385465279221535
CurrentTrain: epoch  1, batch     7 | loss: 11.3587953Losses:  10.584225695580244 1.0 0.6489237546920776 1.5842647962272167
CurrentTrain: epoch  1, batch     8 | loss: 10.5842257Losses:  15.090233735740185 1.0 0.5999633073806763 6.413012437522411
CurrentTrain: epoch  1, batch     9 | loss: 15.0902337Losses:  15.096191581338644 1.0 0.5427430868148804 5.322728332132101
CurrentTrain: epoch  1, batch    10 | loss: 15.0961916Losses:  12.332891523838043 1.0 0.7880949974060059 3.13691908121109
CurrentTrain: epoch  1, batch    11 | loss: 12.3328915Losses:  11.95838651433587 1.0 0.7211227416992188 2.8590155579149723
CurrentTrain: epoch  1, batch    12 | loss: 11.9583865Losses:  8.796363830566406 1.0 0.7627807855606079 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 8.7963638Losses:  13.304464977234602 1.0 0.4903069734573364 3.532238643616438
CurrentTrain: epoch  1, batch    14 | loss: 13.3044650Losses:  9.161422729492188 1.0 0.6446698904037476 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 9.1614227Losses:  13.22866751998663 1.0 0.758760929107666 3.142075799405575
CurrentTrain: epoch  1, batch    16 | loss: 13.2286675Losses:  14.374024137854576 1.0 0.6231625080108643 4.391256079077721
CurrentTrain: epoch  1, batch    17 | loss: 14.3740241Losses:  14.860393170267344 1.0 0.7712215185165405 5.95162833109498
CurrentTrain: epoch  1, batch    18 | loss: 14.8603932Losses:  9.196571350097656 1.0 0.6830059289932251 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 9.1965714Losses:  9.177289009094238 1.0 0.8662959337234497 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 9.1772890Losses:  19.35926342755556 1.0 0.5555206537246704 9.67859173566103
CurrentTrain: epoch  1, batch    21 | loss: 19.3592634Losses:  9.503043174743652 1.0 0.829293966293335 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 9.5030432Losses:  8.615675926208496 1.0 0.7318563461303711 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 8.6156759Losses:  12.179979644715786 1.0 0.7921081781387329 4.356302581727505
CurrentTrain: epoch  1, batch    24 | loss: 12.1799796Losses:  10.801511846482754 1.0 0.8408843278884888 2.8680201396346092
CurrentTrain: epoch  1, batch    25 | loss: 10.8015118Losses:  9.50003719329834 1.0 0.6544009447097778 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 9.5000372Losses:  8.906925201416016 1.0 0.650175929069519 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 8.9069252Losses:  9.216659545898438 1.0 0.6844427585601807 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 9.2166595Losses:  13.174665674567223 1.0 0.7815093994140625 4.330047830939293
CurrentTrain: epoch  1, batch    29 | loss: 13.1746657Losses:  12.933804847300053 1.0 0.5291124582290649 3.610403396189213
CurrentTrain: epoch  1, batch    30 | loss: 12.9338048Losses:  12.839653827250004 1.0 0.6851035356521606 3.272844173014164
CurrentTrain: epoch  1, batch    31 | loss: 12.8396538Losses:  13.178788051009178 1.0 0.6712722778320312 3.9666899293661118
CurrentTrain: epoch  1, batch    32 | loss: 13.1787881Losses:  8.368766784667969 1.0 0.6502509117126465 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 8.3687668Losses:  8.217529296875 1.0 0.7090644836425781 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 8.2175293Losses:  15.944244887679815 1.0 0.6032885313034058 6.57070591673255
CurrentTrain: epoch  1, batch    35 | loss: 15.9442449Losses:  10.727945663034916 1.0 0.5992478132247925 1.5943568721413612
CurrentTrain: epoch  1, batch    36 | loss: 10.7279457Losses:  9.65585120767355 1.0 0.7075234651565552 1.63240035623312
CurrentTrain: epoch  1, batch    37 | loss: 9.6558512Losses:  11.080685071647167 1.0 0.7550961971282959 2.9950775429606438
CurrentTrain: epoch  1, batch    38 | loss: 11.0806851Losses:  12.626246355473995 1.0 0.6889065504074097 4.634103201329708
CurrentTrain: epoch  1, batch    39 | loss: 12.6262464Losses:  9.911718398332596 1.0 0.7040070295333862 1.454812079668045
CurrentTrain: epoch  1, batch    40 | loss: 9.9117184Losses:  11.141167365014553 1.0 0.6111994981765747 2.941770277917385
CurrentTrain: epoch  1, batch    41 | loss: 11.1411674Losses:  10.367580141872168 1.0 0.5448638200759888 1.8325488232076168
CurrentTrain: epoch  1, batch    42 | loss: 10.3675801Losses:  13.140382952988148 1.0 0.7040139436721802 4.541003413498402
CurrentTrain: epoch  1, batch    43 | loss: 13.1403830Losses:  13.490448016673326 1.0 0.7366278171539307 3.841417331248522
CurrentTrain: epoch  1, batch    44 | loss: 13.4904480Losses:  12.107525177299976 1.0 0.6957404613494873 2.995240516960621
CurrentTrain: epoch  1, batch    45 | loss: 12.1075252Losses:  9.155356407165527 1.0 0.5335420370101929 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 9.1553564Losses:  8.543793678283691 1.0 0.42686927318573 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 8.5437937Losses:  10.437563091516495 1.0 0.6472650766372681 1.4097091257572174
CurrentTrain: epoch  1, batch    48 | loss: 10.4375631Losses:  18.289562441408634 1.0 0.5914840698242188 9.150099016726017
CurrentTrain: epoch  1, batch    49 | loss: 18.2895624Losses:  10.388704095035791 1.0 0.5685489177703857 1.4381940700113773
CurrentTrain: epoch  1, batch    50 | loss: 10.3887041Losses:  8.775640487670898 1.0 0.6259171962738037 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 8.7756405Losses:  14.935263559222221 1.0 0.5685433149337769 6.46087734401226
CurrentTrain: epoch  1, batch    52 | loss: 14.9352636Losses:  7.7893571853637695 1.0 0.6266934871673584 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 7.7893572Losses:  11.390807375311852 1.0 0.6912145614624023 3.1882393211126328
CurrentTrain: epoch  1, batch    54 | loss: 11.3908074Losses:  11.109534073621035 1.0 0.771631121635437 2.909863281995058
CurrentTrain: epoch  1, batch    55 | loss: 11.1095341Losses:  8.651382446289062 1.0 0.5814801454544067 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 8.6513824Losses:  8.928457260131836 1.0 0.6164032220840454 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 8.9284573Losses:  8.129436492919922 1.0 0.7170910835266113 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 8.1294365Losses:  12.280457828193903 1.0 0.7093961238861084 3.1605962254107
CurrentTrain: epoch  1, batch    59 | loss: 12.2804578Losses:  8.290067672729492 1.0 0.6636258363723755 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 8.2900677Losses:  9.735893249511719 1.0 0.5965676307678223 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 9.7358932Losses:  10.644790031015873 1.0 0.43808436393737793 4.311021186411381
CurrentTrain: epoch  1, batch    62 | loss: 10.6447900Losses:  7.2540283203125 1.0 0.6929711103439331 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.2540283Losses:  7.93369197845459 1.0 0.6327066421508789 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 7.9336920Losses:  8.742295391857624 1.0 0.5671783685684204 1.416696198284626
CurrentTrain: epoch  2, batch     2 | loss: 8.7422954Losses:  13.298927642405033 1.0 0.7413845062255859 3.1391871944069862
CurrentTrain: epoch  2, batch     3 | loss: 13.2989276Losses:  10.669636815786362 1.0 0.6211909055709839 1.4684239327907562
CurrentTrain: epoch  2, batch     4 | loss: 10.6696368Losses:  10.600826531648636 1.0 0.6443672180175781 1.9722779095172882
CurrentTrain: epoch  2, batch     5 | loss: 10.6008265Losses:  8.163270950317383 1.0 0.5762450695037842 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 8.1632710Losses:  13.361776761710644 1.0 0.6032174825668335 5.002005986869335
CurrentTrain: epoch  2, batch     7 | loss: 13.3617768Losses:  11.418777577579021 1.0 0.6214333772659302 3.3380585834383965
CurrentTrain: epoch  2, batch     8 | loss: 11.4187776Losses:  12.584037601947784 1.0 0.6601883172988892 3.850377857685089
CurrentTrain: epoch  2, batch     9 | loss: 12.5840376Losses:  9.614397458732128 1.0 0.6963351964950562 1.4951681420207024
CurrentTrain: epoch  2, batch    10 | loss: 9.6143975Losses:  9.083954613655806 1.0 0.6817563772201538 1.5311181955039501
CurrentTrain: epoch  2, batch    11 | loss: 9.0839546Losses:  14.233574770390987 1.0 0.6605685949325562 5.8978299126029015
CurrentTrain: epoch  2, batch    12 | loss: 14.2335748Losses:  9.88031829148531 1.0 0.8001331090927124 1.5081087425351143
CurrentTrain: epoch  2, batch    13 | loss: 9.8803183Losses:  12.006830669939518 1.0 0.7554857730865479 2.9047112241387367
CurrentTrain: epoch  2, batch    14 | loss: 12.0068307Losses:  10.182064812630415 1.0 0.518051266670227 2.133438866585493
CurrentTrain: epoch  2, batch    15 | loss: 10.1820648Losses:  11.137015402317047 1.0 0.5143001079559326 2.8516274094581604
CurrentTrain: epoch  2, batch    16 | loss: 11.1370154Losses:  11.101957470178604 1.0 0.6572167873382568 2.8727571070194244
CurrentTrain: epoch  2, batch    17 | loss: 11.1019575Losses:  18.13100892305374 1.0 0.6318032741546631 10.967881977558136
CurrentTrain: epoch  2, batch    18 | loss: 18.1310089Losses:  10.453981164842844 1.0 0.5323493480682373 1.8551805056631565
CurrentTrain: epoch  2, batch    19 | loss: 10.4539812Losses:  7.674796104431152 1.0 0.7437410354614258 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 7.6747961Losses:  8.199448585510254 1.0 0.6024025678634644 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 8.1994486Losses:  12.380298115313053 1.0 0.5934910774230957 4.332677341997623
CurrentTrain: epoch  2, batch    22 | loss: 12.3802981Losses:  10.318262606859207 1.0 0.6524515151977539 1.6049380600452423
CurrentTrain: epoch  2, batch    23 | loss: 10.3182626Losses:  8.704342983663082 1.0 0.7002819776535034 1.4816566929221153
CurrentTrain: epoch  2, batch    24 | loss: 8.7043430Losses:  12.679256338626146 1.0 0.5483946800231934 4.743051905184984
CurrentTrain: epoch  2, batch    25 | loss: 12.6792563Losses:  9.556025177240372 1.0 0.47814881801605225 1.4105173647403717
CurrentTrain: epoch  2, batch    26 | loss: 9.5560252Losses:  9.805898882448673 1.0 0.6903566122055054 1.8279959931969643
CurrentTrain: epoch  2, batch    27 | loss: 9.8058989Losses:  9.274840265512466 1.0 0.727603554725647 1.4102891981601715
CurrentTrain: epoch  2, batch    28 | loss: 9.2748403Losses:  19.653718940913677 1.0 0.5632761716842651 11.947843544185162
CurrentTrain: epoch  2, batch    29 | loss: 19.6537189Losses:  9.554839346557856 1.0 0.7783401012420654 1.4914219118654728
CurrentTrain: epoch  2, batch    30 | loss: 9.5548393Losses:  8.789155270904303 1.0 0.5265610218048096 1.4786598943173885
CurrentTrain: epoch  2, batch    31 | loss: 8.7891553Losses:  10.471133291721344 1.0 0.6504584550857544 2.9035821557044983
CurrentTrain: epoch  2, batch    32 | loss: 10.4711333Losses:  10.875467993319035 1.0 0.6658082008361816 2.125279165804386
CurrentTrain: epoch  2, batch    33 | loss: 10.8754680Losses:  13.237109545618296 1.0 0.45603692531585693 5.603955153375864
CurrentTrain: epoch  2, batch    34 | loss: 13.2371095Losses:  14.58901422470808 1.0 0.4636944532394409 5.944911174476147
CurrentTrain: epoch  2, batch    35 | loss: 14.5890142Losses:  10.9944341853261 1.0 0.7260823249816895 2.8624323084950447
CurrentTrain: epoch  2, batch    36 | loss: 10.9944342Losses:  10.179851088672876 1.0 0.5581299066543579 1.79816010966897
CurrentTrain: epoch  2, batch    37 | loss: 10.1798511Losses:  9.135339677333832 1.0 0.5149139165878296 1.403237760066986
CurrentTrain: epoch  2, batch    38 | loss: 9.1353397Losses:  11.719429939985275 1.0 0.661020040512085 3.1301755607128143
CurrentTrain: epoch  2, batch    39 | loss: 11.7194299Losses:  11.89053663983941 1.0 0.6451836824417114 4.43108019605279
CurrentTrain: epoch  2, batch    40 | loss: 11.8905366Losses:  10.759599186480045 1.0 0.7755991220474243 1.4642367139458656
CurrentTrain: epoch  2, batch    41 | loss: 10.7595992Losses:  9.188019901514053 1.0 0.7322090864181519 1.4059111177921295
CurrentTrain: epoch  2, batch    42 | loss: 9.1880199Losses:  8.17371654510498 1.0 0.5825824737548828 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 8.1737165Losses:  8.826452374458313 1.0 0.6735230684280396 1.4414793252944946
CurrentTrain: epoch  2, batch    44 | loss: 8.8264524Losses:  11.045162815600634 1.0 0.5125904083251953 3.1820708699524403
CurrentTrain: epoch  2, batch    45 | loss: 11.0451628Losses:  7.5799689292907715 1.0 0.670896053314209 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 7.5799689Losses:  8.009140968322754 1.0 0.5987398624420166 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 8.0091410Losses:  7.298438549041748 1.0 0.6126502752304077 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 7.2984385Losses:  9.698980338871479 1.0 0.4469170570373535 1.4442977979779243
CurrentTrain: epoch  2, batch    49 | loss: 9.6989803Losses:  16.973503470420837 1.0 0.4748234748840332 9.045849680900574
CurrentTrain: epoch  2, batch    50 | loss: 16.9735035Losses:  10.712645910680294 1.0 0.5857222080230713 3.509941481053829
CurrentTrain: epoch  2, batch    51 | loss: 10.7126459Losses:  9.112182522192597 1.0 0.6529921293258667 1.7280587200075388
CurrentTrain: epoch  2, batch    52 | loss: 9.1121825Losses:  10.402964264154434 1.0 0.6830133199691772 2.815275341272354
CurrentTrain: epoch  2, batch    53 | loss: 10.4029643Losses:  9.440465807914734 1.0 0.5576090812683105 1.4196242094039917
CurrentTrain: epoch  2, batch    54 | loss: 9.4404658Losses:  10.035334940999746 1.0 0.5447870492935181 2.9595831595361233
CurrentTrain: epoch  2, batch    55 | loss: 10.0353349Losses:  9.210308954119682 1.0 0.7147465944290161 1.7126569002866745
CurrentTrain: epoch  2, batch    56 | loss: 9.2103090Losses:  8.080214500427246 1.0 0.45260655879974365 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 8.0802145Losses:  7.823737144470215 1.0 0.6347744464874268 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 7.8237371Losses:  18.57753748074174 1.0 0.4168297052383423 11.343569222837687
CurrentTrain: epoch  2, batch    59 | loss: 18.5775375Losses:  7.65313196182251 1.0 0.5237679481506348 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 7.6531320Losses:  10.758592769503593 1.0 0.6301771402359009 2.0141498297452927
CurrentTrain: epoch  2, batch    61 | loss: 10.7585928Losses:  7.341465473175049 1.0 0.5891890525817871 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 7.3414655Losses:  8.682716932147741 1.0 0.6151348352432251 1.4093571566045284
CurrentTrain: epoch  3, batch     0 | loss: 8.6827169Losses:  8.914597600698471 1.0 0.6952298879623413 1.4027596414089203
CurrentTrain: epoch  3, batch     1 | loss: 8.9145976Losses:  7.740577697753906 1.0 0.6069583892822266 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 7.7405777Losses:  11.898612432181835 1.0 0.5487205982208252 4.303656987845898
CurrentTrain: epoch  3, batch     3 | loss: 11.8986124Losses:  10.277646634727716 1.0 0.6012139320373535 3.496665094047785
CurrentTrain: epoch  3, batch     4 | loss: 10.2776466Losses:  12.111024796962738 1.0 0.5161293745040894 4.253702104091644
CurrentTrain: epoch  3, batch     5 | loss: 12.1110248Losses:  8.848103791475296 1.0 0.4919337034225464 1.40420463681221
CurrentTrain: epoch  3, batch     6 | loss: 8.8481038Losses:  11.848605513572693 1.0 0.6726070642471313 4.2504624128341675
CurrentTrain: epoch  3, batch     7 | loss: 11.8486055Losses:  8.702526826411486 1.0 0.580836296081543 1.4677784629166126
CurrentTrain: epoch  3, batch     8 | loss: 8.7025268Losses:  7.922595977783203 1.0 0.6067202091217041 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 7.9225960Losses:  7.426532745361328 1.0 0.5225914716720581 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 7.4265327Losses:  10.329565789550543 1.0 0.527068018913269 2.949862267822027
CurrentTrain: epoch  3, batch    11 | loss: 10.3295658Losses:  11.490785956382751 1.0 0.7038723230361938 4.248714327812195
CurrentTrain: epoch  3, batch    12 | loss: 11.4907860Losses:  11.402624487876892 1.0 0.5321779251098633 2.880438208580017
CurrentTrain: epoch  3, batch    13 | loss: 11.4026245Losses:  18.374027110636234 1.0 0.658227801322937 10.344227649271488
CurrentTrain: epoch  3, batch    14 | loss: 18.3740271Losses:  9.266308542340994 1.0 0.6366949081420898 1.4105050526559353
CurrentTrain: epoch  3, batch    15 | loss: 9.2663085Losses:  13.798771277070045 1.0 0.5404144525527954 5.96795167028904
CurrentTrain: epoch  3, batch    16 | loss: 13.7987713Losses:  8.630578577518463 1.0 0.7079592943191528 1.435312807559967
CurrentTrain: epoch  3, batch    17 | loss: 8.6305786Losses:  11.55245877802372 1.0 0.623813271522522 4.3609767109155655
CurrentTrain: epoch  3, batch    18 | loss: 11.5524588Losses:  8.185971148312092 1.0 0.6973503828048706 1.5115617588162422
CurrentTrain: epoch  3, batch    19 | loss: 8.1859711Losses:  10.921113461256027 1.0 0.6514530181884766 4.228653401136398
CurrentTrain: epoch  3, batch    20 | loss: 10.9211135Losses:  10.716026484966278 1.0 0.5349792242050171 3.560377776622772
CurrentTrain: epoch  3, batch    21 | loss: 10.7160265Losses:  14.008388638496399 1.0 0.6307110786437988 6.085218548774719
CurrentTrain: epoch  3, batch    22 | loss: 14.0083886Losses:  9.319047309458256 1.0 0.6004984378814697 1.4347489848732948
CurrentTrain: epoch  3, batch    23 | loss: 9.3190473Losses:  9.40996453166008 1.0 0.662492036819458 1.4460906684398651
CurrentTrain: epoch  3, batch    24 | loss: 9.4099645Losses:  10.916594676673412 1.0 0.6398011445999146 2.9307710453867912
CurrentTrain: epoch  3, batch    25 | loss: 10.9165947Losses:  9.63775136321783 1.0 0.7048366069793701 2.8864534124732018
CurrentTrain: epoch  3, batch    26 | loss: 9.6377514Losses:  8.192224316298962 1.0 0.6615756750106812 1.4470675513148308
CurrentTrain: epoch  3, batch    27 | loss: 8.1922243Losses:  13.01203340291977 1.0 0.6134170293807983 4.652470529079437
CurrentTrain: epoch  3, batch    28 | loss: 13.0120334Losses:  12.83467223495245 1.0 0.6019245386123657 5.7501847967505455
CurrentTrain: epoch  3, batch    29 | loss: 12.8346722Losses:  7.169624328613281 1.0 0.6433466672897339 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 7.1696243Losses:  6.940086364746094 1.0 0.5804417133331299 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 6.9400864Losses:  12.801249373704195 1.0 0.6065566539764404 5.752935279160738
CurrentTrain: epoch  3, batch    32 | loss: 12.8012494Losses:  10.403274524956942 1.0 0.724502444267273 2.869067180901766
CurrentTrain: epoch  3, batch    33 | loss: 10.4032745Losses:  7.630087852478027 1.0 0.6503894329071045 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 7.6300879Losses:  9.57163892686367 1.0 0.7213718891143799 1.5317638963460922
CurrentTrain: epoch  3, batch    35 | loss: 9.5716389Losses:  6.785369396209717 1.0 0.5403615236282349 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 6.7853694Losses:  7.141741752624512 1.0 0.6495288610458374 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 7.1417418Losses:  9.897134631872177 1.0 0.659351110458374 2.814253658056259
CurrentTrain: epoch  3, batch    38 | loss: 9.8971346Losses:  11.563094437122345 1.0 0.7031618356704712 4.243521511554718
CurrentTrain: epoch  3, batch    39 | loss: 11.5630944Losses:  16.6904707737267 1.0 0.6778732538223267 7.591057855635881
CurrentTrain: epoch  3, batch    40 | loss: 16.6904708Losses:  7.808933734893799 1.0 0.48109376430511475 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 7.8089337Losses:  7.94194695353508 1.0 0.5395327806472778 1.3983354270458221
CurrentTrain: epoch  3, batch    42 | loss: 7.9419470Losses:  8.882014006376266 1.0 0.7571291923522949 1.6880352199077606
CurrentTrain: epoch  3, batch    43 | loss: 8.8820140Losses:  11.030570857226849 1.0 0.6187863349914551 3.2700251266360283
CurrentTrain: epoch  3, batch    44 | loss: 11.0305709Losses:  9.86505201458931 1.0 0.5365399122238159 2.8280084431171417
CurrentTrain: epoch  3, batch    45 | loss: 9.8650520Losses:  11.106671571731567 1.0 0.5514897108078003 3.1946370601654053
CurrentTrain: epoch  3, batch    46 | loss: 11.1066716Losses:  9.465147346258163 1.0 0.5256661176681519 2.8115857541561127
CurrentTrain: epoch  3, batch    47 | loss: 9.4651473Losses:  7.188787460327148 1.0 0.6459529399871826 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 7.1887875Losses:  18.46596894785762 1.0 0.6896883249282837 10.606643062084913
CurrentTrain: epoch  3, batch    49 | loss: 18.4659689Losses:  7.599618911743164 1.0 0.5758739709854126 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 7.5996189Losses:  8.689414143562317 1.0 0.4653433561325073 1.4136906862258911
CurrentTrain: epoch  3, batch    51 | loss: 8.6894141Losses:  11.209058590233326 1.0 0.6663261651992798 4.3265808299183846
CurrentTrain: epoch  3, batch    52 | loss: 11.2090586Losses:  12.465132426470518 1.0 0.7352579832077026 4.312602709978819
CurrentTrain: epoch  3, batch    53 | loss: 12.4651324Losses:  8.029256016016006 1.0 0.6220841407775879 1.3952838480472565
CurrentTrain: epoch  3, batch    54 | loss: 8.0292560Losses:  10.03946179151535 1.0 0.5310008525848389 2.819490134716034
CurrentTrain: epoch  3, batch    55 | loss: 10.0394618Losses:  6.827098369598389 1.0 0.6334109306335449 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 6.8270984Losses:  7.3939924240112305 1.0 0.7317192554473877 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.3939924Losses:  12.16958962380886 1.0 0.5304505825042725 5.377331361174583
CurrentTrain: epoch  3, batch    58 | loss: 12.1695896Losses:  17.040215462446213 1.0 0.5233194828033447 9.478926628828049
CurrentTrain: epoch  3, batch    59 | loss: 17.0402155Losses:  7.500660419464111 1.0 0.5438121557235718 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 7.5006604Losses:  12.583270519971848 1.0 0.5521223545074463 5.00859209895134
CurrentTrain: epoch  3, batch    61 | loss: 12.5832705Losses:  8.433775037527084 1.0 0.6331136226654053 1.4016476571559906
CurrentTrain: epoch  3, batch    62 | loss: 8.4337750Losses:  11.591201238334179 1.0 0.5763546228408813 4.291671685874462
CurrentTrain: epoch  4, batch     0 | loss: 11.5912012Losses:  12.15606091171503 1.0 0.749606728553772 4.8478657975792885
CurrentTrain: epoch  4, batch     1 | loss: 12.1560609Losses:  6.623431205749512 1.0 0.6874867677688599 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.6234312Losses:  10.476346548646688 1.0 0.5397087335586548 4.251825388520956
CurrentTrain: epoch  4, batch     3 | loss: 10.4763465Losses:  15.513642944395542 1.0 0.5504647493362427 8.552166141569614
CurrentTrain: epoch  4, batch     4 | loss: 15.5136429Losses:  8.269483536481857 1.0 0.5840152502059937 1.3970060050487518
CurrentTrain: epoch  4, batch     5 | loss: 8.2694835Losses:  8.312528904527426 1.0 0.6664038896560669 1.4784973226487637
CurrentTrain: epoch  4, batch     6 | loss: 8.3125289Losses:  6.220984935760498 1.0 0.5914925336837769 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 6.2209849Losses:  10.53105565160513 1.0 0.6552482843399048 2.878253661096096
CurrentTrain: epoch  4, batch     8 | loss: 10.5310557Losses:  9.188061833381653 1.0 0.6128913164138794 1.4608608484268188
CurrentTrain: epoch  4, batch     9 | loss: 9.1880618Losses:  11.440921809524298 1.0 0.675887942314148 4.272664096206427
CurrentTrain: epoch  4, batch    10 | loss: 11.4409218Losses:  10.353744007647038 1.0 0.750820517539978 2.911568619310856
CurrentTrain: epoch  4, batch    11 | loss: 10.3537440Losses:  8.563201487064362 1.0 0.4748753309249878 1.4192286133766174
CurrentTrain: epoch  4, batch    12 | loss: 8.5632015Losses:  6.867187023162842 1.0 0.5351086854934692 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 6.8671870Losses:  8.772907108068466 1.0 0.5209147930145264 1.4637678563594818
CurrentTrain: epoch  4, batch    14 | loss: 8.7729071Losses:  8.607290536165237 1.0 0.6152493953704834 1.4084990322589874
CurrentTrain: epoch  4, batch    15 | loss: 8.6072905Losses:  10.795982163399458 1.0 0.6487168073654175 3.1739290170371532
CurrentTrain: epoch  4, batch    16 | loss: 10.7959822Losses:  6.752862930297852 1.0 0.6171857118606567 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 6.7528629Losses:  11.213149935007095 1.0 0.4628866910934448 4.296626001596451
CurrentTrain: epoch  4, batch    18 | loss: 11.2131499Losses:  15.798594377934933 1.0 0.4977431297302246 8.72235145419836
CurrentTrain: epoch  4, batch    19 | loss: 15.7985944Losses:  10.484318971633911 1.0 0.6597671508789062 2.8399107456207275
CurrentTrain: epoch  4, batch    20 | loss: 10.4843190Losses:  10.204720947891474 1.0 0.803343653678894 2.8619451262056828
CurrentTrain: epoch  4, batch    21 | loss: 10.2047209Losses:  8.095548570156097 1.0 0.4915050268173218 1.4738697409629822
CurrentTrain: epoch  4, batch    22 | loss: 8.0955486Losses:  13.72811334580183 1.0 0.6010082960128784 7.061763934791088
CurrentTrain: epoch  4, batch    23 | loss: 13.7281133Losses:  8.109723567962646 1.0 0.5593987703323364 1.4628486633300781
CurrentTrain: epoch  4, batch    24 | loss: 8.1097236Losses:  7.758432388305664 1.0 0.6404848098754883 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 7.7584324Losses:  7.976845890283585 1.0 0.6486866474151611 1.4257546961307526
CurrentTrain: epoch  4, batch    26 | loss: 7.9768459Losses:  9.586681641638279 1.0 0.6322528123855591 1.9403870478272438
CurrentTrain: epoch  4, batch    27 | loss: 9.5866816Losses:  8.500092148780823 1.0 0.48872506618499756 1.4180361032485962
CurrentTrain: epoch  4, batch    28 | loss: 8.5000921Losses:  13.055613748729229 1.0 0.6887867450714111 5.925098650157452
CurrentTrain: epoch  4, batch    29 | loss: 13.0556137Losses:  12.38400986790657 1.0 0.7345186471939087 5.734243422746658
CurrentTrain: epoch  4, batch    30 | loss: 12.3840099Losses:  6.861022472381592 1.0 0.5473847389221191 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 6.8610225Losses:  9.38279826194048 1.0 0.49764585494995117 2.9733901694417
CurrentTrain: epoch  4, batch    32 | loss: 9.3827983Losses:  11.868005748838186 1.0 0.5816724300384521 5.261137004941702
CurrentTrain: epoch  4, batch    33 | loss: 11.8680057Losses:  6.544528484344482 1.0 0.5766967535018921 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 6.5445285Losses:  8.445311099290848 1.0 0.45818209648132324 1.3967290222644806
CurrentTrain: epoch  4, batch    35 | loss: 8.4453111Losses:  11.503093354403973 1.0 0.5201277732849121 4.654627911746502
CurrentTrain: epoch  4, batch    36 | loss: 11.5030934Losses:  10.95682131499052 1.0 0.5691624879837036 3.6906961128115654
CurrentTrain: epoch  4, batch    37 | loss: 10.9568213Losses:  6.535782337188721 1.0 0.6002364158630371 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 6.5357823Losses:  11.25605583190918 1.0 0.6718885898590088 4.323336601257324
CurrentTrain: epoch  4, batch    39 | loss: 11.2560558Losses:  11.65233363211155 1.0 0.5821012258529663 4.664746180176735
CurrentTrain: epoch  4, batch    40 | loss: 11.6523336Losses:  6.624716758728027 1.0 0.6451681852340698 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 6.6247168Losses:  13.040671743452549 1.0 0.5013951063156128 6.19884005934
CurrentTrain: epoch  4, batch    42 | loss: 13.0406717Losses:  9.4514135196805 1.0 0.6979705095291138 2.843421347439289
CurrentTrain: epoch  4, batch    43 | loss: 9.4514135Losses:  6.892375946044922 1.0 0.5557359457015991 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 6.8923759Losses:  8.283339817076921 1.0 0.5096818208694458 1.503579456359148
CurrentTrain: epoch  4, batch    45 | loss: 8.2833398Losses:  10.628152914345264 1.0 0.6869624853134155 2.9831152632832527
CurrentTrain: epoch  4, batch    46 | loss: 10.6281529Losses:  8.238563902676105 1.0 0.6120295524597168 1.5755008533596992
CurrentTrain: epoch  4, batch    47 | loss: 8.2385639Losses:  12.865786403417587 1.0 0.45984482765197754 5.818681567907333
CurrentTrain: epoch  4, batch    48 | loss: 12.8657864Losses:  6.832025527954102 1.0 0.7605322599411011 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 6.8320255Losses:  9.632121302187443 1.0 0.6489828824996948 2.971616007387638
CurrentTrain: epoch  4, batch    50 | loss: 9.6321213Losses:  6.46411657333374 1.0 0.6577029228210449 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 6.4641166Losses:  10.749830808490515 1.0 0.7083560228347778 4.248537626117468
CurrentTrain: epoch  4, batch    52 | loss: 10.7498308Losses:  10.046528607606888 1.0 0.5985672473907471 1.5102660953998566
CurrentTrain: epoch  4, batch    53 | loss: 10.0465286Losses:  7.717240422964096 1.0 0.7572085857391357 1.3926087319850922
CurrentTrain: epoch  4, batch    54 | loss: 7.7172404Losses:  7.102445602416992 1.0 0.5994869470596313 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 7.1024456Losses:  16.457706034183502 1.0 0.5042209625244141 9.678126871585846
CurrentTrain: epoch  4, batch    56 | loss: 16.4577060Losses:  10.890122178941965 1.0 0.6785316467285156 4.300418619066477
CurrentTrain: epoch  4, batch    57 | loss: 10.8901222Losses:  10.15813708677888 1.0 0.581811785697937 3.0385158099234104
CurrentTrain: epoch  4, batch    58 | loss: 10.1581371Losses:  9.963210340589285 1.0 0.6479365825653076 3.0636541806161404
CurrentTrain: epoch  4, batch    59 | loss: 9.9632103Losses:  8.217064708471298 1.0 0.6487776041030884 1.4203761518001556
CurrentTrain: epoch  4, batch    60 | loss: 8.2170647Losses:  8.414798732846975 1.0 0.510391116142273 1.4460401497781277
CurrentTrain: epoch  4, batch    61 | loss: 8.4147987Losses:  12.154918260872364 1.0 0.5743212699890137 6.016138143837452
CurrentTrain: epoch  4, batch    62 | loss: 12.1549183Losses:  7.017908573150635 1.0 0.5741947889328003 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 7.0179086Losses:  9.514547936618328 1.0 0.5603088140487671 2.9132443591952324
CurrentTrain: epoch  5, batch     1 | loss: 9.5145479Losses:  9.491154313087463 1.0 0.5546951293945312 2.798521637916565
CurrentTrain: epoch  5, batch     2 | loss: 9.4911543Losses:  9.855181124061346 1.0 0.5824713706970215 2.9262579940259457
CurrentTrain: epoch  5, batch     3 | loss: 9.8551811Losses:  10.824591185897589 1.0 0.6277273893356323 4.297575976699591
CurrentTrain: epoch  5, batch     4 | loss: 10.8245912Losses:  12.139670543372631 1.0 0.648017168045044 5.773313693702221
CurrentTrain: epoch  5, batch     5 | loss: 12.1396705Losses:  7.03469181060791 1.0 0.5755692720413208 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 7.0346918Losses:  8.159503493458033 1.0 0.578884482383728 1.4171247817575932
CurrentTrain: epoch  5, batch     7 | loss: 8.1595035Losses:  11.250604841858149 1.0 0.5350273847579956 4.330222342163324
CurrentTrain: epoch  5, batch     8 | loss: 11.2506048Losses:  13.041038028895855 1.0 0.66860032081604 6.1645111963152885
CurrentTrain: epoch  5, batch     9 | loss: 13.0410380Losses:  10.020775526762009 1.0 0.5502214431762695 2.8015439212322235
CurrentTrain: epoch  5, batch    10 | loss: 10.0207755Losses:  28.353612542152405 1.0 0.5929611921310425 21.65332281589508
CurrentTrain: epoch  5, batch    11 | loss: 28.3536125Losses:  9.377470225095749 1.0 0.48556768894195557 2.8018233478069305
CurrentTrain: epoch  5, batch    12 | loss: 9.3774702Losses:  8.015767127275467 1.0 0.7352515459060669 1.3987665474414825
CurrentTrain: epoch  5, batch    13 | loss: 8.0157671Losses:  8.322632767260075 1.0 0.4406473636627197 1.744020439684391
CurrentTrain: epoch  5, batch    14 | loss: 8.3226328Losses:  8.392680700868368 1.0 0.5898318290710449 1.4639092050492764
CurrentTrain: epoch  5, batch    15 | loss: 8.3926807Losses:  8.135537602007389 1.0 0.6482508182525635 1.4531178250908852
CurrentTrain: epoch  5, batch    16 | loss: 8.1355376Losses:  9.941317766904831 1.0 0.567758321762085 2.837389200925827
CurrentTrain: epoch  5, batch    17 | loss: 9.9413178Losses:  10.83371440321207 1.0 0.456156849861145 4.647968210279942
CurrentTrain: epoch  5, batch    18 | loss: 10.8337144Losses:  7.750188112258911 1.0 0.5257927179336548 1.4046132564544678
CurrentTrain: epoch  5, batch    19 | loss: 7.7501881Losses:  8.860384941101074 1.0 0.5576357841491699 2.802281379699707
CurrentTrain: epoch  5, batch    20 | loss: 8.8603849Losses:  12.022364653646946 1.0 0.7173869609832764 5.828719176352024
CurrentTrain: epoch  5, batch    21 | loss: 12.0223647Losses:  9.187192996963859 1.0 0.5787098407745361 3.0284233894199133
CurrentTrain: epoch  5, batch    22 | loss: 9.1871930Losses:  10.052581369876862 1.0 0.4644489288330078 3.2088399529457092
CurrentTrain: epoch  5, batch    23 | loss: 10.0525814Losses:  8.870099939405918 1.0 0.5509207248687744 2.8662232533097267
CurrentTrain: epoch  5, batch    24 | loss: 8.8700999Losses:  21.32756668329239 1.0 0.7477163076400757 13.914213240146637
CurrentTrain: epoch  5, batch    25 | loss: 21.3275667Losses:  9.176490016281605 1.0 0.4674053192138672 2.9272949174046516
CurrentTrain: epoch  5, batch    26 | loss: 9.1764900Losses:  8.241848438978195 1.0 0.5588158369064331 1.3970389068126678
CurrentTrain: epoch  5, batch    27 | loss: 8.2418484Losses:  7.531192176043987 1.0 0.5608365535736084 1.4417918846011162
CurrentTrain: epoch  5, batch    28 | loss: 7.5311922Losses:  11.475324507802725 1.0 0.588443398475647 4.494171496480703
CurrentTrain: epoch  5, batch    29 | loss: 11.4753245Losses:  8.566399216651917 1.0 0.5482006072998047 1.504024624824524
CurrentTrain: epoch  5, batch    30 | loss: 8.5663992Losses:  10.77371459454298 1.0 0.3894602060317993 4.43008141964674
CurrentTrain: epoch  5, batch    31 | loss: 10.7737146Losses:  7.717751204967499 1.0 0.454964280128479 1.4274703860282898
CurrentTrain: epoch  5, batch    32 | loss: 7.7177512Losses:  21.900904554873705 1.0 0.7496403455734253 14.56017293408513
CurrentTrain: epoch  5, batch    33 | loss: 21.9009046Losses:  9.804251946508884 1.0 0.605739951133728 2.9333513155579567
CurrentTrain: epoch  5, batch    34 | loss: 9.8042519Losses:  10.617149583995342 1.0 0.5974816083908081 4.241443865001202
CurrentTrain: epoch  5, batch    35 | loss: 10.6171496Losses:  13.018831092864275 1.0 0.4786834716796875 6.355112869292498
CurrentTrain: epoch  5, batch    36 | loss: 13.0188311Losses:  7.831290785223246 1.0 0.7080557346343994 1.461306158453226
CurrentTrain: epoch  5, batch    37 | loss: 7.8312908Losses:  8.139572620391846 1.0 0.6723196506500244 1.4859824180603027
CurrentTrain: epoch  5, batch    38 | loss: 8.1395726Losses:  7.740378230810165 1.0 0.6150356531143188 1.401639312505722
CurrentTrain: epoch  5, batch    39 | loss: 7.7403782Losses:  7.815379232168198 1.0 0.6618564128875732 1.3955021798610687
CurrentTrain: epoch  5, batch    40 | loss: 7.8153792Losses:  6.922767639160156 1.0 0.5943734645843506 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 6.9227676Losses:  8.532478168606758 1.0 0.6343604326248169 1.4286702424287796
CurrentTrain: epoch  5, batch    42 | loss: 8.5324782Losses:  11.208926685154438 1.0 0.6634029150009155 4.262078292667866
CurrentTrain: epoch  5, batch    43 | loss: 11.2089267Losses:  6.731444835662842 1.0 0.6099386215209961 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 6.7314448Losses:  5.975322723388672 1.0 0.3904761075973511 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 5.9753227Losses:  9.143401324748993 1.0 0.6349871158599854 2.985266864299774
CurrentTrain: epoch  5, batch    46 | loss: 9.1434013Losses:  9.040932953357697 1.0 0.5777715444564819 1.395327866077423
CurrentTrain: epoch  5, batch    47 | loss: 9.0409330Losses:  10.006098568439484 1.0 0.5949957370758057 2.8802841305732727
CurrentTrain: epoch  5, batch    48 | loss: 10.0060986Losses:  8.2516408264637 1.0 0.5299216508865356 1.4004798233509064
CurrentTrain: epoch  5, batch    49 | loss: 8.2516408Losses:  8.382934629917145 1.0 0.5931291580200195 1.3959155678749084
CurrentTrain: epoch  5, batch    50 | loss: 8.3829346Losses:  16.544794149696827 1.0 0.5796835422515869 10.129286833107471
CurrentTrain: epoch  5, batch    51 | loss: 16.5447941Losses:  10.6338524594903 1.0 0.6364669799804688 4.310732342302799
CurrentTrain: epoch  5, batch    52 | loss: 10.6338525Losses:  14.97474217414856 1.0 0.5582654476165771 8.504482507705688
CurrentTrain: epoch  5, batch    53 | loss: 14.9747422Losses:  8.150751430541277 1.0 0.5944104194641113 1.5255130119621754
CurrentTrain: epoch  5, batch    54 | loss: 8.1507514Losses:  9.223055005073547 1.0 0.6518151760101318 2.82371723651886
CurrentTrain: epoch  5, batch    55 | loss: 9.2230550Losses:  9.794934779405594 1.0 0.5615216493606567 2.801264315843582
CurrentTrain: epoch  5, batch    56 | loss: 9.7949348Losses:  7.517603628337383 1.0 0.623454213142395 1.4394395276904106
CurrentTrain: epoch  5, batch    57 | loss: 7.5176036Losses:  11.541410088539124 1.0 0.565284013748169 4.842841744422913
CurrentTrain: epoch  5, batch    58 | loss: 11.5414101Losses:  12.46958577632904 1.0 0.6962881088256836 5.073170065879822
CurrentTrain: epoch  5, batch    59 | loss: 12.4695858Losses:  7.945113696157932 1.0 0.4904510974884033 1.5107531920075417
CurrentTrain: epoch  5, batch    60 | loss: 7.9451137Losses:  7.742193847894669 1.0 0.43487393856048584 1.4399005472660065
CurrentTrain: epoch  5, batch    61 | loss: 7.7421938Losses:  10.729488220065832 1.0 0.44454216957092285 4.2658565901219845
CurrentTrain: epoch  5, batch    62 | loss: 10.7294882Losses:  13.556765235960484 1.0 0.5755212306976318 7.103005088865757
CurrentTrain: epoch  6, batch     0 | loss: 13.5567652Losses:  9.096397880464792 1.0 0.48045289516448975 2.8275623358786106
CurrentTrain: epoch  6, batch     1 | loss: 9.0963979Losses:  8.166737258434296 1.0 0.6364405155181885 1.4557186961174011
CurrentTrain: epoch  6, batch     2 | loss: 8.1667373Losses:  8.016206711530685 1.0 0.49335598945617676 1.4463796317577362
CurrentTrain: epoch  6, batch     3 | loss: 8.0162067Losses:  7.53093147277832 1.0 0.558753490447998 1.4374351501464844
CurrentTrain: epoch  6, batch     4 | loss: 7.5309315Losses:  8.094499059021473 1.0 0.3799196481704712 1.5507621243596077
CurrentTrain: epoch  6, batch     5 | loss: 8.0944991Losses:  6.34751033782959 1.0 0.5508451461791992 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 6.3475103Losses:  7.718523148447275 1.0 0.6309696435928345 1.4372783936560154
CurrentTrain: epoch  6, batch     7 | loss: 7.7185231Losses:  12.87435945123434 1.0 0.6997699737548828 6.069864593446255
CurrentTrain: epoch  6, batch     8 | loss: 12.8743595Losses:  9.09421007707715 1.0 0.5475268363952637 3.048958707600832
CurrentTrain: epoch  6, batch     9 | loss: 9.0942101Losses:  10.73547112569213 1.0 0.5388972759246826 3.2663320265710354
CurrentTrain: epoch  6, batch    10 | loss: 10.7354711Losses:  10.157815292477608 1.0 0.599943995475769 3.071151092648506
CurrentTrain: epoch  6, batch    11 | loss: 10.1578153Losses:  8.246627449989319 1.0 0.5256234407424927 1.463227391242981
CurrentTrain: epoch  6, batch    12 | loss: 8.2466274Losses:  7.891270279884338 1.0 0.6108933687210083 1.4007335901260376
CurrentTrain: epoch  6, batch    13 | loss: 7.8912703Losses:  7.682477854192257 1.0 0.6521159410476685 1.4208048805594444
CurrentTrain: epoch  6, batch    14 | loss: 7.6824779Losses:  9.606741074472666 1.0 0.4745056629180908 2.891916397958994
CurrentTrain: epoch  6, batch    15 | loss: 9.6067411Losses:  6.177006721496582 1.0 0.5797415971755981 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 6.1770067Losses:  9.307602345943451 1.0 0.5984761714935303 2.851334035396576
CurrentTrain: epoch  6, batch    17 | loss: 9.3076023Losses:  7.895980216562748 1.0 0.611473560333252 1.466025210916996
CurrentTrain: epoch  6, batch    18 | loss: 7.8959802Losses:  6.416018962860107 1.0 0.6344105005264282 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 6.4160190Losses:  11.206535328179598 1.0 0.6200083494186401 4.490281570702791
CurrentTrain: epoch  6, batch    20 | loss: 11.2065353Losses:  7.9736513793468475 1.0 0.5568665266036987 1.4530710875988007
CurrentTrain: epoch  6, batch    21 | loss: 7.9736514Losses:  8.874455247074366 1.0 0.5522629022598267 2.8520768024027348
CurrentTrain: epoch  6, batch    22 | loss: 8.8744552Losses:  7.974471896886826 1.0 0.5535273551940918 1.4172318875789642
CurrentTrain: epoch  6, batch    23 | loss: 7.9744719Losses:  10.600140511989594 1.0 0.42179834842681885 4.283806264400482
CurrentTrain: epoch  6, batch    24 | loss: 10.6001405Losses:  9.331696063280106 1.0 0.6060329675674438 2.869681864976883
CurrentTrain: epoch  6, batch    25 | loss: 9.3316961Losses:  8.028655022382736 1.0 0.7017747163772583 1.429732769727707
CurrentTrain: epoch  6, batch    26 | loss: 8.0286550Losses:  7.597788035869598 1.0 0.49277591705322266 1.428752601146698
CurrentTrain: epoch  6, batch    27 | loss: 7.5977880Losses:  11.860248167067766 1.0 0.48644769191741943 5.763543207198381
CurrentTrain: epoch  6, batch    28 | loss: 11.8602482Losses:  12.199658423662186 1.0 0.5687106847763062 5.665928393602371
CurrentTrain: epoch  6, batch    29 | loss: 12.1996584Losses:  10.383079618215561 1.0 0.5607340335845947 4.213535398244858
CurrentTrain: epoch  6, batch    30 | loss: 10.3830796Losses:  7.658045530319214 1.0 0.44377601146698 1.4218738079071045
CurrentTrain: epoch  6, batch    31 | loss: 7.6580455Losses:  9.327326156198978 1.0 0.6616833209991455 2.9015401378273964
CurrentTrain: epoch  6, batch    32 | loss: 9.3273262Losses:  7.472251832485199 1.0 0.5358090400695801 1.419585645198822
CurrentTrain: epoch  6, batch    33 | loss: 7.4722518Losses:  6.622854709625244 1.0 0.6824620962142944 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 6.6228547Losses:  10.528513759374619 1.0 0.533270001411438 3.281878799200058
CurrentTrain: epoch  6, batch    35 | loss: 10.5285138Losses:  9.505284927785397 1.0 0.6230320930480957 2.886852405965328
CurrentTrain: epoch  6, batch    36 | loss: 9.5052849Losses:  6.68098783493042 1.0 0.558413028717041 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.6809878Losses:  14.381071574985981 1.0 0.7609468698501587 7.547472007572651
CurrentTrain: epoch  6, batch    38 | loss: 14.3810716Losses:  6.479794979095459 1.0 0.5449919700622559 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 6.4797950Losses:  6.517389297485352 1.0 0.560497522354126 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 6.5173893Losses:  7.65287059545517 1.0 0.6408488750457764 1.3954338431358337
CurrentTrain: epoch  6, batch    41 | loss: 7.6528706Losses:  9.986214309930801 1.0 0.5845282077789307 2.823981910943985
CurrentTrain: epoch  6, batch    42 | loss: 9.9862143Losses:  9.835789188742638 1.0 0.6159908771514893 3.296767696738243
CurrentTrain: epoch  6, batch    43 | loss: 9.8357892Losses:  7.727877356112003 1.0 0.6310653686523438 1.5141179338097572
CurrentTrain: epoch  6, batch    44 | loss: 7.7278774Losses:  7.862216591835022 1.0 0.6137984991073608 1.4241811037063599
CurrentTrain: epoch  6, batch    45 | loss: 7.8622166Losses:  9.997960925102234 1.0 0.6117258071899414 2.359272837638855
CurrentTrain: epoch  6, batch    46 | loss: 9.9979609Losses:  9.22340802103281 1.0 0.6012395620346069 2.8756349459290504
CurrentTrain: epoch  6, batch    47 | loss: 9.2234080Losses:  7.704666670411825 1.0 0.7217668294906616 1.4326687417924404
CurrentTrain: epoch  6, batch    48 | loss: 7.7046667Losses:  8.154530547559261 1.0 0.7404477596282959 1.4284353479743004
CurrentTrain: epoch  6, batch    49 | loss: 8.1545305Losses:  7.781630575656891 1.0 0.5458885431289673 1.3991547226905823
CurrentTrain: epoch  6, batch    50 | loss: 7.7816306Losses:  6.463119983673096 1.0 0.7220159769058228 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 6.4631200Losses:  6.653408050537109 1.0 0.5998268127441406 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 6.6534081Losses:  10.451980877667665 1.0 0.5707880258560181 4.388937283307314
CurrentTrain: epoch  6, batch    53 | loss: 10.4519809Losses:  5.972182750701904 1.0 0.48672711849212646 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 5.9721828Losses:  10.352901950478554 1.0 0.5725123882293701 4.265087619423866
CurrentTrain: epoch  6, batch    55 | loss: 10.3529020Losses:  7.750634998083115 1.0 0.5656932592391968 1.4747045934200287
CurrentTrain: epoch  6, batch    56 | loss: 7.7506350Losses:  8.957339022308588 1.0 0.4639855623245239 2.872157786041498
CurrentTrain: epoch  6, batch    57 | loss: 8.9573390Losses:  7.645769275724888 1.0 0.558686375617981 1.4455658569931984
CurrentTrain: epoch  6, batch    58 | loss: 7.6457693Losses:  8.17782124876976 1.0 0.6561282873153687 1.4139562547206879
CurrentTrain: epoch  6, batch    59 | loss: 8.1778212Losses:  5.942028045654297 1.0 0.5761940479278564 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 5.9420280Losses:  7.445194337517023 1.0 0.6846499443054199 1.4056840874254704
CurrentTrain: epoch  6, batch    61 | loss: 7.4451943Losses:  10.470614701509476 1.0 0.5107924938201904 4.263173371553421
CurrentTrain: epoch  6, batch    62 | loss: 10.4706147Losses:  7.800003942102194 1.0 0.6168922185897827 1.46438592299819
CurrentTrain: epoch  7, batch     0 | loss: 7.8000039Losses:  9.155622124671936 1.0 0.5774151086807251 2.8090802431106567
CurrentTrain: epoch  7, batch     1 | loss: 9.1556221Losses:  6.496626853942871 1.0 0.6623440980911255 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 6.4966269Losses:  13.834943950176239 1.0 0.573332667350769 7.6849557757377625
CurrentTrain: epoch  7, batch     3 | loss: 13.8349440Losses:  13.414744257926941 1.0 0.49637341499328613 7.5513142347335815
CurrentTrain: epoch  7, batch     4 | loss: 13.4147443Losses:  9.001473128795624 1.0 0.6193581819534302 2.837563216686249
CurrentTrain: epoch  7, batch     5 | loss: 9.0014731Losses:  6.121483325958252 1.0 0.3779940605163574 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 6.1214833Losses:  11.70402455329895 1.0 0.4858812093734741 5.676339864730835
CurrentTrain: epoch  7, batch     7 | loss: 11.7040246Losses:  7.429674685001373 1.0 0.45598626136779785 1.4195733666419983
CurrentTrain: epoch  7, batch     8 | loss: 7.4296747Losses:  7.355607450008392 1.0 0.44793379306793213 1.4377326369285583
CurrentTrain: epoch  7, batch     9 | loss: 7.3556075Losses:  10.692926362156868 1.0 0.5581860542297363 4.481628373265266
CurrentTrain: epoch  7, batch    10 | loss: 10.6929264Losses:  6.17210578918457 1.0 0.48722684383392334 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 6.1721058Losses:  9.739477962255478 1.0 0.5204311609268188 2.791776031255722
CurrentTrain: epoch  7, batch    12 | loss: 9.7394780Losses:  7.243168115615845 1.0 0.5679425001144409 1.3971703052520752
CurrentTrain: epoch  7, batch    13 | loss: 7.2431681Losses:  9.657410833984613 1.0 0.5267164707183838 3.6950294710695744
CurrentTrain: epoch  7, batch    14 | loss: 9.6574108Losses:  9.299857147037983 1.0 0.6725336313247681 2.826905734837055
CurrentTrain: epoch  7, batch    15 | loss: 9.2998571Losses:  13.571699440479279 1.0 0.6105914115905762 7.183522999286652
CurrentTrain: epoch  7, batch    16 | loss: 13.5716994Losses:  7.4305256605148315 1.0 0.5722843408584595 1.392015814781189
CurrentTrain: epoch  7, batch    17 | loss: 7.4305257Losses:  6.3161444664001465 1.0 0.6631150841712952 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 6.3161445Losses:  11.709441311657429 1.0 0.487412691116333 5.8634301498532295
CurrentTrain: epoch  7, batch    19 | loss: 11.7094413Losses:  7.516159296035767 1.0 0.672871470451355 1.399423360824585
CurrentTrain: epoch  7, batch    20 | loss: 7.5161593Losses:  10.283868573606014 1.0 0.5619345903396606 4.300706647336483
CurrentTrain: epoch  7, batch    21 | loss: 10.2838686Losses:  8.791866958141327 1.0 0.6436702013015747 2.7908007502555847
CurrentTrain: epoch  7, batch    22 | loss: 8.7918670Losses:  6.174709796905518 1.0 0.657685399055481 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 6.1747098Losses:  6.137814998626709 1.0 0.615923285484314 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 6.1378150Losses:  8.094756536185741 1.0 0.646367073059082 1.4723085686564445
CurrentTrain: epoch  7, batch    25 | loss: 8.0947565Losses:  8.890017479658127 1.0 0.4987049102783203 2.7945131957530975
CurrentTrain: epoch  7, batch    26 | loss: 8.8900175Losses:  7.309954699128866 1.0 0.5039654970169067 1.4314337335526943
CurrentTrain: epoch  7, batch    27 | loss: 7.3099547Losses:  8.529236733913422 1.0 0.4765204191207886 2.791036546230316
CurrentTrain: epoch  7, batch    28 | loss: 8.5292367Losses:  7.12659627199173 1.0 0.39002811908721924 1.388315498828888
CurrentTrain: epoch  7, batch    29 | loss: 7.1265963Losses:  5.804116725921631 1.0 0.4941765069961548 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 5.8041167Losses:  12.817262977361679 1.0 0.5529071092605591 7.05178102850914
CurrentTrain: epoch  7, batch    31 | loss: 12.8172630Losses:  8.68502277508378 1.0 0.55976402759552 2.8508719839155674
CurrentTrain: epoch  7, batch    32 | loss: 8.6850228Losses:  13.464791528880596 1.0 0.500656008720398 7.663556806743145
CurrentTrain: epoch  7, batch    33 | loss: 13.4647915Losses:  10.232413150370121 1.0 0.699042797088623 4.195632316172123
CurrentTrain: epoch  7, batch    34 | loss: 10.2324132Losses:  6.009602069854736 1.0 0.6341234445571899 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 6.0096021Losses:  11.143119554966688 1.0 0.5069499015808105 4.426897268742323
CurrentTrain: epoch  7, batch    36 | loss: 11.1431196Losses:  9.07745161280036 1.0 0.45104289054870605 2.8528412841260433
CurrentTrain: epoch  7, batch    37 | loss: 9.0774516Losses:  8.7991341650486 1.0 0.6565005779266357 2.832606226205826
CurrentTrain: epoch  7, batch    38 | loss: 8.7991342Losses:  6.410508155822754 1.0 0.6004304885864258 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 6.4105082Losses:  8.815500020980835 1.0 0.6015582084655762 2.8328592777252197
CurrentTrain: epoch  7, batch    40 | loss: 8.8155000Losses:  8.949005514383316 1.0 0.5938577651977539 2.8032130300998688
CurrentTrain: epoch  7, batch    41 | loss: 8.9490055Losses:  5.902322769165039 1.0 0.5323299169540405 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 5.9023228Losses:  7.553891241550446 1.0 0.553292989730835 1.43428236246109
CurrentTrain: epoch  7, batch    43 | loss: 7.5538912Losses:  10.477499790489674 1.0 0.521630048751831 4.386748142540455
CurrentTrain: epoch  7, batch    44 | loss: 10.4774998Losses:  8.753196235746145 1.0 0.5376944541931152 2.8413810692727566
CurrentTrain: epoch  7, batch    45 | loss: 8.7531962Losses:  11.734571672976017 1.0 0.6000771522521973 5.621802546083927
CurrentTrain: epoch  7, batch    46 | loss: 11.7345717Losses:  7.324542671442032 1.0 0.5782285928726196 1.3984462320804596
CurrentTrain: epoch  7, batch    47 | loss: 7.3245427Losses:  7.544132381677628 1.0 0.5918005704879761 1.4442888796329498
CurrentTrain: epoch  7, batch    48 | loss: 7.5441324Losses:  7.723686099052429 1.0 0.5912864208221436 1.3925760984420776
CurrentTrain: epoch  7, batch    49 | loss: 7.7236861Losses:  6.739196300506592 1.0 0.48181021213531494 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 6.7391963Losses:  7.570051547139883 1.0 0.5246028900146484 1.434497233480215
CurrentTrain: epoch  7, batch    51 | loss: 7.5700515Losses:  10.03051683306694 1.0 0.47545742988586426 4.286616533994675
CurrentTrain: epoch  7, batch    52 | loss: 10.0305168Losses:  7.247317347675562 1.0 0.6311463117599487 1.4113531447947025
CurrentTrain: epoch  7, batch    53 | loss: 7.2473173Losses:  10.188016027212143 1.0 0.6551711559295654 4.212488263845444
CurrentTrain: epoch  7, batch    54 | loss: 10.1880160Losses:  6.110447883605957 1.0 0.5427359342575073 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 6.1104479Losses:  5.7145161628723145 1.0 0.4963115453720093 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 5.7145162Losses:  6.4786763191223145 1.0 0.4844614267349243 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 6.4786763Losses:  11.592394437640905 1.0 0.42652320861816406 5.849144544452429
CurrentTrain: epoch  7, batch    58 | loss: 11.5923944Losses:  6.0021772384643555 1.0 0.676647424697876 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 6.0021772Losses:  7.268304020166397 1.0 0.5657840967178345 1.410325676202774
CurrentTrain: epoch  7, batch    60 | loss: 7.2683040Losses:  7.274498343467712 1.0 0.5638924837112427 1.4029372930526733
CurrentTrain: epoch  7, batch    61 | loss: 7.2744983Losses:  5.928731918334961 1.0 0.6455724239349365 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 5.9287319Losses:  7.586816053837538 1.0 0.5817446708679199 1.4620039276778698
CurrentTrain: epoch  8, batch     0 | loss: 7.5868161Losses:  9.078856885433197 1.0 0.5932084321975708 3.0952557921409607
CurrentTrain: epoch  8, batch     1 | loss: 9.0788569Losses:  8.729657545685768 1.0 0.5725303888320923 2.8732174783945084
CurrentTrain: epoch  8, batch     2 | loss: 8.7296575Losses:  7.077106326818466 1.0 0.36208510398864746 1.4065330922603607
CurrentTrain: epoch  8, batch     3 | loss: 7.0771063Losses:  7.45607915148139 1.0 0.48726212978363037 1.4388329051434994
CurrentTrain: epoch  8, batch     4 | loss: 7.4560792Losses:  5.877041816711426 1.0 0.5860764980316162 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 5.8770418Losses:  5.847306251525879 1.0 0.5572326183319092 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 5.8473063Losses:  8.779477804899216 1.0 0.6477447748184204 2.820937365293503
CurrentTrain: epoch  8, batch     7 | loss: 8.7794778Losses:  8.95536095276475 1.0 0.6688987016677856 2.9068284668028355
CurrentTrain: epoch  8, batch     8 | loss: 8.9553610Losses:  10.060251235961914 1.0 0.7339701652526855 4.199382305145264
CurrentTrain: epoch  8, batch     9 | loss: 10.0602512Losses:  5.615628242492676 1.0 0.3199855089187622 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 5.6156282Losses:  7.292974159121513 1.0 0.5461186170578003 1.5015989989042282
CurrentTrain: epoch  8, batch    11 | loss: 7.2929742Losses:  7.3898747861385345 1.0 0.3972853422164917 1.4072526395320892
CurrentTrain: epoch  8, batch    12 | loss: 7.3898748Losses:  7.177022960036993 1.0 0.46281909942626953 1.4766502641141415
CurrentTrain: epoch  8, batch    13 | loss: 7.1770230Losses:  7.214945614337921 1.0 0.5159337520599365 1.4142473340034485
CurrentTrain: epoch  8, batch    14 | loss: 7.2149456Losses:  7.439571231603622 1.0 0.6319447755813599 1.3942430913448334
CurrentTrain: epoch  8, batch    15 | loss: 7.4395712Losses:  11.193319525569677 1.0 0.4444674253463745 4.317893233150244
CurrentTrain: epoch  8, batch    16 | loss: 11.1933195Losses:  5.743035316467285 1.0 0.4779684543609619 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 5.7430353Losses:  7.2895392924547195 1.0 0.4179112911224365 1.5389718562364578
CurrentTrain: epoch  8, batch    18 | loss: 7.2895393Losses:  10.289970695972443 1.0 0.691215991973877 4.251048386096954
CurrentTrain: epoch  8, batch    19 | loss: 10.2899707Losses:  5.949968338012695 1.0 0.646630048751831 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 5.9499683Losses:  5.991000175476074 1.0 0.6251413822174072 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 5.9910002Losses:  11.748374193906784 1.0 0.46894001960754395 5.841511934995651
CurrentTrain: epoch  8, batch    22 | loss: 11.7483742Losses:  15.907546363770962 1.0 0.633544921875 9.939577423036098
CurrentTrain: epoch  8, batch    23 | loss: 15.9075464Losses:  5.911561012268066 1.0 0.6285747289657593 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 5.9115610Losses:  18.250936530530453 1.0 0.3941025733947754 12.682735942304134
CurrentTrain: epoch  8, batch    25 | loss: 18.2509365Losses:  7.291920363903046 1.0 0.5809844732284546 1.3926421999931335
CurrentTrain: epoch  8, batch    26 | loss: 7.2919204Losses:  16.38594664633274 1.0 0.6929678916931152 10.063621893525124
CurrentTrain: epoch  8, batch    27 | loss: 16.3859466Losses:  5.90447473526001 1.0 0.6489291191101074 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 5.9044747Losses:  10.235012352466583 1.0 0.6191091537475586 4.23786860704422
CurrentTrain: epoch  8, batch    29 | loss: 10.2350124Losses:  7.13539931550622 1.0 0.43626153469085693 1.4305791594088078
CurrentTrain: epoch  8, batch    30 | loss: 7.1353993Losses:  12.433083645999432 1.0 0.6039643287658691 6.52500306814909
CurrentTrain: epoch  8, batch    31 | loss: 12.4330836Losses:  8.687451809644699 1.0 0.6390827894210815 2.7884120643138885
CurrentTrain: epoch  8, batch    32 | loss: 8.6874518Losses:  5.752582550048828 1.0 0.4421340227127075 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 5.7525826Losses:  7.945701897144318 1.0 0.4550679922103882 1.4297626614570618
CurrentTrain: epoch  8, batch    34 | loss: 7.9457019Losses:  7.2623299062252045 1.0 0.574393630027771 1.4260872304439545
CurrentTrain: epoch  8, batch    35 | loss: 7.2623299Losses:  7.217078894376755 1.0 0.553680419921875 1.399499624967575
CurrentTrain: epoch  8, batch    36 | loss: 7.2170789Losses:  5.806556224822998 1.0 0.5677648782730103 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 5.8065562Losses:  14.5091244392097 1.0 0.6254305839538574 8.577293079346418
CurrentTrain: epoch  8, batch    38 | loss: 14.5091244Losses:  9.928304314613342 1.0 0.5345351696014404 4.235981583595276
CurrentTrain: epoch  8, batch    39 | loss: 9.9283043Losses:  7.143333047628403 1.0 0.447523832321167 1.40696582198143
CurrentTrain: epoch  8, batch    40 | loss: 7.1433330Losses:  7.303668767213821 1.0 0.5552258491516113 1.4008953869342804
CurrentTrain: epoch  8, batch    41 | loss: 7.3036688Losses:  8.717782437801361 1.0 0.56963050365448 2.854740560054779
CurrentTrain: epoch  8, batch    42 | loss: 8.7177824Losses:  14.925824575126171 1.0 0.7777278423309326 8.974261693656445
CurrentTrain: epoch  8, batch    43 | loss: 14.9258246Losses:  7.189055532217026 1.0 0.5528843402862549 1.3935566842556
CurrentTrain: epoch  8, batch    44 | loss: 7.1890555Losses:  7.240836083889008 1.0 0.48369544744491577 1.3936085104942322
CurrentTrain: epoch  8, batch    45 | loss: 7.2408361Losses:  10.103287279605865 1.0 0.5816532373428345 4.3098631501197815
CurrentTrain: epoch  8, batch    46 | loss: 10.1032873Losses:  6.983311355113983 1.0 0.3571498394012451 1.4182259440422058
CurrentTrain: epoch  8, batch    47 | loss: 6.9833114Losses:  6.288020133972168 1.0 0.45800280570983887 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 6.2880201Losses:  5.758968830108643 1.0 0.5464454889297485 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 5.7589688Losses:  8.755931913852692 1.0 0.6820812225341797 2.813542902469635
CurrentTrain: epoch  8, batch    50 | loss: 8.7559319Losses:  7.119786292314529 1.0 0.44931113719940186 1.4193992912769318
CurrentTrain: epoch  8, batch    51 | loss: 7.1197863Losses:  7.221179187297821 1.0 0.5563925504684448 1.396594226360321
CurrentTrain: epoch  8, batch    52 | loss: 7.2211792Losses:  5.711472034454346 1.0 0.5366882085800171 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 5.7114720Losses:  10.115159094333649 1.0 0.39496755599975586 4.355624735355377
CurrentTrain: epoch  8, batch    54 | loss: 10.1151591Losses:  7.018071204423904 1.0 0.4035417437553406 1.4169578850269318
CurrentTrain: epoch  8, batch    55 | loss: 7.0180712Losses:  10.099567472934723 1.0 0.5050137042999268 4.420731604099274
CurrentTrain: epoch  8, batch    56 | loss: 10.0995675Losses:  9.775570452213287 1.0 0.3581550121307373 4.216118395328522
CurrentTrain: epoch  8, batch    57 | loss: 9.7755705Losses:  7.189443469047546 1.0 0.5209734439849854 1.3923243284225464
CurrentTrain: epoch  8, batch    58 | loss: 7.1894435Losses:  7.23174124956131 1.0 0.5552147626876831 1.4174063801765442
CurrentTrain: epoch  8, batch    59 | loss: 7.2317412Losses:  5.661227703094482 1.0 0.5070748329162598 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 5.6612277Losses:  11.402216583490372 1.0 0.6028386354446411 5.607554107904434
CurrentTrain: epoch  8, batch    61 | loss: 11.4022166Losses:  7.330630298703909 1.0 0.7564859390258789 1.4161810837686062
CurrentTrain: epoch  8, batch    62 | loss: 7.3306303Losses:  8.96246488019824 1.0 0.5497872829437256 3.2290740720927715
CurrentTrain: epoch  9, batch     0 | loss: 8.9624649Losses:  5.458037376403809 1.0 0.3203018307685852 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 5.4580374Losses:  5.806125164031982 1.0 0.5850375890731812 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.8061252Losses:  8.46943411231041 1.0 0.4930418133735657 2.801691383123398
CurrentTrain: epoch  9, batch     3 | loss: 8.4694341Losses:  8.634008049964905 1.0 0.6460638046264648 2.7996212244033813
CurrentTrain: epoch  9, batch     4 | loss: 8.6340080Losses:  9.929224133491516 1.0 0.47703123092651367 4.250564694404602
CurrentTrain: epoch  9, batch     5 | loss: 9.9292241Losses:  7.122527301311493 1.0 0.5726399421691895 1.3897854685783386
CurrentTrain: epoch  9, batch     6 | loss: 7.1225273Losses:  8.508129745721817 1.0 0.49341094493865967 2.8337556421756744
CurrentTrain: epoch  9, batch     7 | loss: 8.5081297Losses:  8.605028212070465 1.0 0.675234317779541 2.7974587082862854
CurrentTrain: epoch  9, batch     8 | loss: 8.6050282Losses:  5.596811771392822 1.0 0.4445004463195801 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 5.5968118Losses:  8.660689502954483 1.0 0.6192814111709595 2.9036861956119537
CurrentTrain: epoch  9, batch    10 | loss: 8.6606895Losses:  7.778931438922882 1.0 0.5745759010314941 1.3925850987434387
CurrentTrain: epoch  9, batch    11 | loss: 7.7789314Losses:  8.667645633220673 1.0 0.6376471519470215 2.8535863757133484
CurrentTrain: epoch  9, batch    12 | loss: 8.6676456Losses:  5.683860778808594 1.0 0.5045220851898193 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 5.6838608Losses:  8.447755485773087 1.0 0.41907572746276855 2.8336922228336334
CurrentTrain: epoch  9, batch    14 | loss: 8.4477555Losses:  9.810008428990841 1.0 0.5056416392326355 4.195086382329464
CurrentTrain: epoch  9, batch    15 | loss: 9.8100084Losses:  8.75486808642745 1.0 0.6233326196670532 2.858930166810751
CurrentTrain: epoch  9, batch    16 | loss: 8.7548681Losses:  5.5947065353393555 1.0 0.465157687664032 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 5.5947065Losses:  7.118962105363607 1.0 0.521248459815979 1.4107697568833828
CurrentTrain: epoch  9, batch    18 | loss: 7.1189621Losses:  12.678549230098724 1.0 0.48286664485931396 7.055173814296722
CurrentTrain: epoch  9, batch    19 | loss: 12.6785492Losses:  7.088356226682663 1.0 0.5676544308662415 1.391545981168747
CurrentTrain: epoch  9, batch    20 | loss: 7.0883562Losses:  7.645456075668335 1.0 0.5929838418960571 1.3918201923370361
CurrentTrain: epoch  9, batch    21 | loss: 7.6454561Losses:  5.809929847717285 1.0 0.6796127557754517 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 5.8099298Losses:  8.758776817470789 1.0 0.6780662536621094 2.912560138851404
CurrentTrain: epoch  9, batch    23 | loss: 8.7587768Losses:  6.343787670135498 1.0 0.4250950813293457 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 6.3437877Losses:  10.016954377293587 1.0 0.4984992742538452 4.379142716526985
CurrentTrain: epoch  9, batch    25 | loss: 10.0169544Losses:  5.83768367767334 1.0 0.4748266935348511 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 5.8376837Losses:  8.32920616865158 1.0 0.4095577001571655 2.81101530790329
CurrentTrain: epoch  9, batch    27 | loss: 8.3292062Losses:  8.289649784564972 1.0 0.3675355315208435 2.819920837879181
CurrentTrain: epoch  9, batch    28 | loss: 8.2896498Losses:  8.593893319368362 1.0 0.5915740728378296 2.834594041109085
CurrentTrain: epoch  9, batch    29 | loss: 8.5938933Losses:  7.116011917591095 1.0 0.4613865613937378 1.417002022266388
CurrentTrain: epoch  9, batch    30 | loss: 7.1160119Losses:  10.001348495483398 1.0 0.5418040752410889 4.310009002685547
CurrentTrain: epoch  9, batch    31 | loss: 10.0013485Losses:  14.530231140553951 1.0 0.5312519669532776 8.774451397359371
CurrentTrain: epoch  9, batch    32 | loss: 14.5302311Losses:  9.883717235177755 1.0 0.527815580368042 4.2380854450166225
CurrentTrain: epoch  9, batch    33 | loss: 9.8837172Losses:  5.559957504272461 1.0 0.4504373073577881 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 5.5599575Losses:  11.457495033740997 1.0 0.6087188720703125 5.694869816303253
CurrentTrain: epoch  9, batch    35 | loss: 11.4574950Losses:  8.519102573394775 1.0 0.5972883701324463 2.793483257293701
CurrentTrain: epoch  9, batch    36 | loss: 8.5191026Losses:  5.878209590911865 1.0 0.502836287021637 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 5.8782096Losses:  7.267914921045303 1.0 0.5178513526916504 1.427767425775528
CurrentTrain: epoch  9, batch    38 | loss: 7.2679149Losses:  5.774223804473877 1.0 0.5686537027359009 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 5.7742238Losses:  7.018537610769272 1.0 0.4822506904602051 1.4009624421596527
CurrentTrain: epoch  9, batch    40 | loss: 7.0185376Losses:  5.671089172363281 1.0 0.5737651586532593 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 5.6710892Losses:  10.052736103534698 1.0 0.5484105348587036 4.21486359834671
CurrentTrain: epoch  9, batch    42 | loss: 10.0527361Losses:  8.585653722286224 1.0 0.6358975172042847 2.8159098029136658
CurrentTrain: epoch  9, batch    43 | loss: 8.5856537Losses:  7.180336982011795 1.0 0.6242509484291077 1.3953442871570587
CurrentTrain: epoch  9, batch    44 | loss: 7.1803370Losses:  7.15228733420372 1.0 0.5763353109359741 1.4251960217952728
CurrentTrain: epoch  9, batch    45 | loss: 7.1522873Losses:  8.777041733264923 1.0 0.5192001461982727 2.933333694934845
CurrentTrain: epoch  9, batch    46 | loss: 8.7770417Losses:  8.497593998908997 1.0 0.5001317262649536 2.7950717210769653
CurrentTrain: epoch  9, batch    47 | loss: 8.4975940Losses:  5.693262100219727 1.0 0.5127612352371216 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 5.6932621Losses:  8.52937576174736 1.0 0.5032961368560791 2.786326140165329
CurrentTrain: epoch  9, batch    49 | loss: 8.5293758Losses:  10.078893549740314 1.0 0.5441254377365112 4.308375246822834
CurrentTrain: epoch  9, batch    50 | loss: 10.0788935Losses:  5.797873020172119 1.0 0.6497950553894043 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 5.7978730Losses:  8.483522534370422 1.0 0.4909248352050781 2.8229094743728638
CurrentTrain: epoch  9, batch    52 | loss: 8.4835225Losses:  8.580024417489767 1.0 0.4340304136276245 2.8397766910493374
CurrentTrain: epoch  9, batch    53 | loss: 8.5800244Losses:  8.51851013302803 1.0 0.6114276647567749 2.825109750032425
CurrentTrain: epoch  9, batch    54 | loss: 8.5185101Losses:  10.029211401939392 1.0 0.4691445827484131 4.224225401878357
CurrentTrain: epoch  9, batch    55 | loss: 10.0292114Losses:  9.948055438697338 1.0 0.44339871406555176 4.3203918263316154
CurrentTrain: epoch  9, batch    56 | loss: 9.9480554Losses:  14.414508439600468 1.0 0.5598247647285461 8.768709756433964
CurrentTrain: epoch  9, batch    57 | loss: 14.4145084Losses:  7.026056617498398 1.0 0.5024201273918152 1.3973077237606049
CurrentTrain: epoch  9, batch    58 | loss: 7.0260566Losses:  9.857351392507553 1.0 0.3518873453140259 4.213862508535385
CurrentTrain: epoch  9, batch    59 | loss: 9.8573514Losses:  7.102946788072586 1.0 0.45394444465637207 1.4006247818470001
CurrentTrain: epoch  9, batch    60 | loss: 7.1029468Losses:  11.019503980875015 1.0 0.5757192373275757 4.213751703500748
CurrentTrain: epoch  9, batch    61 | loss: 11.0195040Losses:  11.587137185037136 1.0 0.6107659339904785 5.800497971475124
CurrentTrain: epoch  9, batch    62 | loss: 11.5871372
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  9.904056906700134 1.0 1.0508699417114258 1.514391303062439
CurrentTrain: epoch  0, batch     0 | loss: 9.9040569Losses:  13.838125418871641 1.0 1.0782581567764282 5.8456570617854595
CurrentTrain: epoch  0, batch     1 | loss: 13.8381254Losses:  10.741448044776917 1.0 0.8232537508010864 2.890713334083557
CurrentTrain: epoch  0, batch     2 | loss: 10.7414480Losses:  10.987726256251335 1.0 0.7440481185913086 1.5279837101697922
CurrentTrain: epoch  0, batch     3 | loss: 10.9877263Losses:  8.655202865600586 1.0 1.022925853729248 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 8.6552029Losses:  9.558909349143505 1.0 1.0473219156265259 1.606124334037304
CurrentTrain: epoch  1, batch     1 | loss: 9.5589093Losses:  8.441028762608767 1.0 0.8629624843597412 1.47994439676404
CurrentTrain: epoch  1, batch     2 | loss: 8.4410288Losses:  8.689824789762497 1.0 0.6293783187866211 1.4334475696086884
CurrentTrain: epoch  1, batch     3 | loss: 8.6898248Losses:  6.6923723220825195 1.0 0.856529712677002 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 6.6923723Losses:  10.360011160373688 1.0 0.9127578735351562 2.9870510697364807
CurrentTrain: epoch  2, batch     1 | loss: 10.3600112Losses:  7.332004547119141 1.0 0.9530684947967529 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 7.3320045Losses:  6.347963623702526 1.0 0.8111429214477539 1.4781950041651726
CurrentTrain: epoch  2, batch     3 | loss: 6.3479636Losses:  10.333268914371729 1.0 0.9978176355361938 3.0448716022074223
CurrentTrain: epoch  3, batch     0 | loss: 10.3332689Losses:  9.077840980142355 1.0 0.9520115852355957 3.0799261890351772
CurrentTrain: epoch  3, batch     1 | loss: 9.0778410Losses:  7.867472469806671 1.0 0.7933216094970703 1.4045866131782532
CurrentTrain: epoch  3, batch     2 | loss: 7.8674725Losses:  7.14378148317337 1.0 0.5397977828979492 1.471837341785431
CurrentTrain: epoch  3, batch     3 | loss: 7.1437815Losses:  5.5127716064453125 1.0 0.7922971248626709 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 5.5127716Losses:  12.068213116377592 1.0 0.9122732877731323 5.783253323286772
CurrentTrain: epoch  4, batch     1 | loss: 12.0682131Losses:  5.933533668518066 1.0 0.8093876838684082 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 5.9335337Losses:  8.256862588226795 1.0 0.46706390380859375 1.448103852570057
CurrentTrain: epoch  4, batch     3 | loss: 8.2568626Losses:  6.038910865783691 1.0 0.7688174247741699 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.0389109Losses:  5.482919216156006 1.0 0.7426058053970337 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.4829192Losses:  9.621437177062035 1.0 0.9311716556549072 3.8390179723501205
CurrentTrain: epoch  5, batch     2 | loss: 9.6214372Losses:  6.682090520858765 1.0 0.6698703765869141 1.4053900241851807
CurrentTrain: epoch  5, batch     3 | loss: 6.6820905Losses:  5.205078125 1.0 0.8225482702255249 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.2050781Losses:  5.351171016693115 1.0 0.8265122175216675 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 5.3511710Losses:  5.350940704345703 1.0 0.8514851331710815 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 5.3509407Losses:  6.085141249001026 1.0 0.8722381591796875 1.4528947547078133
CurrentTrain: epoch  6, batch     3 | loss: 6.0851412Losses:  7.281799055635929 1.0 0.6183915138244629 2.212671972811222
CurrentTrain: epoch  7, batch     0 | loss: 7.2817991Losses:  6.382107675075531 1.0 0.8342704772949219 1.5494484305381775
CurrentTrain: epoch  7, batch     1 | loss: 6.3821077Losses:  5.887366980314255 1.0 0.7496837377548218 1.4542801082134247
CurrentTrain: epoch  7, batch     2 | loss: 5.8873670Losses:  5.77931022644043 1.0 0.4782094955444336 1.4745702743530273
CurrentTrain: epoch  7, batch     3 | loss: 5.7793102Losses:  8.728803686797619 1.0 0.6497845649719238 4.343305639922619
CurrentTrain: epoch  8, batch     0 | loss: 8.7288037Losses:  7.458840046077967 1.0 0.8910583257675171 2.8248254396021366
CurrentTrain: epoch  8, batch     1 | loss: 7.4588400Losses:  4.802502632141113 1.0 0.8731874227523804 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 4.8025026Losses:  6.078661918640137 1.0 0.7753438949584961 1.4246621131896973
CurrentTrain: epoch  8, batch     3 | loss: 6.0786619Losses:  6.261378526687622 1.0 0.9681791067123413 1.4016549587249756
CurrentTrain: epoch  9, batch     0 | loss: 6.2613785Losses:  5.623405694961548 1.0 0.6940178871154785 1.4174487590789795
CurrentTrain: epoch  9, batch     1 | loss: 5.6234057Losses:  6.098505295813084 1.0 0.6989237070083618 1.7973835840821266
CurrentTrain: epoch  9, batch     2 | loss: 6.0985053Losses:  5.617634430527687 1.0 0.9680099487304688 1.4658056646585464
CurrentTrain: epoch  9, batch     3 | loss: 5.6176344
Losses:  3.927285671234131 1.0 0.7266056537628174 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.9272857Losses:  8.691405065357685 1.0 0.45468950271606445 5.723398931324482
MemoryTrain:  epoch  0, batch     1 | loss: 8.6914051Losses:  3.9493343830108643 1.0 0.6973345279693604 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.9493344Losses:  7.910297226160765 1.0 0.6178169250488281 5.622445415705442
MemoryTrain:  epoch  1, batch     1 | loss: 7.9102972Losses:  2.7348456382751465 1.0 0.7017027139663696 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.7348456Losses:  9.67276131361723 1.0 0.857454776763916 5.639846198260784
MemoryTrain:  epoch  2, batch     1 | loss: 9.6727613Losses:  2.952237367630005 1.0 0.7660472393035889 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.9522374Losses:  7.574940986931324 1.0 0.6426587104797363 5.679394789040089
MemoryTrain:  epoch  3, batch     1 | loss: 7.5749410Losses:  3.1697468757629395 1.0 0.831608772277832 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 3.1697469Losses:  8.0537189245224 1.0 0.9075779914855957 5.618803858757019
MemoryTrain:  epoch  4, batch     1 | loss: 8.0537189Losses:  2.666593551635742 1.0 0.805420994758606 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.6665936Losses:  9.222642865031958 1.0 0.8041558265686035 5.681201662868261
MemoryTrain:  epoch  5, batch     1 | loss: 9.2226429Losses:  2.6702933311462402 1.0 0.7818018198013306 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.6702933Losses:  7.396284133195877 1.0 0.6273674964904785 5.623865634202957
MemoryTrain:  epoch  6, batch     1 | loss: 7.3962841Losses:  2.5655925273895264 1.0 0.6929517984390259 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.5655925Losses:  8.031502779573202 1.0 1.025594711303711 5.674939449876547
MemoryTrain:  epoch  7, batch     1 | loss: 8.0315028Losses:  2.635359287261963 1.0 0.7559493780136108 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.6353593Losses:  7.994867701083422 1.0 0.9046115875244141 5.909290689975023
MemoryTrain:  epoch  8, batch     1 | loss: 7.9948677Losses:  2.4732890129089355 1.0 0.6432480812072754 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 2.4732890Losses:  7.574753791093826 1.0 0.8812465667724609 5.651694804430008
MemoryTrain:  epoch  9, batch     1 | loss: 7.5747538
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 88.50%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 73.67%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 72.19%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 76.41%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 75.69%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 93.07%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.86%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.65%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.65%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 93.83%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 93.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.59%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.43%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 93.19%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 93.04%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.73%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.75%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.68%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 92.47%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 92.44%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 92.17%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 92.12%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 91.99%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 91.58%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 91.13%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 90.69%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 90.07%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 89.52%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 89.18%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 88.78%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 88.45%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 88.06%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.69%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 87.56%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 87.26%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 87.02%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 86.79%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 86.73%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 86.27%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 85.82%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 85.26%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 84.83%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 84.40%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 84.10%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 83.90%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 84.40%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 84.61%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 84.76%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 84.84%   #############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  18.295831149443984 1.5524510145187378 1.5775365829467773 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 18.2958311torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  19.329860411584377 1.7774994373321533 1.9880342483520508 5.83479567617178
CurrentTrain: epoch  0, batch     1 | loss: 19.3298604torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  17.82791530713439 1.712453842163086 1.717653751373291 4.393430825322866
CurrentTrain: epoch  0, batch     2 | loss: 17.8279153torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  18.01407589018345 1.7004165649414062 1.9347906112670898 5.150220528244972
CurrentTrain: epoch  0, batch     3 | loss: 18.0140759torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.125554379075766 1.7402275800704956 1.7835633754730225 2.500932987779379
CurrentTrain: epoch  0, batch     4 | loss: 16.1255544torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.479349620640278 1.6612745523452759 1.676901936531067 4.399185664951801
CurrentTrain: epoch  0, batch     5 | loss: 16.4793496torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  21.420855797827244 1.7184674739837646 1.7442914247512817 8.278360642492771
CurrentTrain: epoch  0, batch     6 | loss: 21.4208558torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  17.415121980011463 1.8553433418273926 1.9073048830032349 3.8523644879460335
CurrentTrain: epoch  0, batch     7 | loss: 17.4151220torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  24.254942439496517 1.7878482341766357 1.6029136180877686 10.522154353559017
CurrentTrain: epoch  0, batch     8 | loss: 24.2549424torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  15.318220175802708 1.6844685077667236 1.7583390474319458 2.8937578573822975
CurrentTrain: epoch  0, batch     9 | loss: 15.3182202torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.5000927336514 1.7198998928070068 1.7648414373397827 3.8615848906338215
CurrentTrain: epoch  0, batch    10 | loss: 16.5000927torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.042084857821465 1.6333351135253906 1.3800948858261108 3.0612374991178513
CurrentTrain: epoch  0, batch    11 | loss: 16.0420849torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  13.65581226348877 1.7740113735198975 1.762507677078247 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 13.6558123torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  13.430277824401855 1.7550334930419922 1.699263572692871 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 13.4302778torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  18.74189656227827 1.6987485885620117 1.5929794311523438 5.328183107078075
CurrentTrain: epoch  0, batch    14 | loss: 18.7418966torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  17.14369836449623 1.4759492874145508 1.4891884326934814 4.963307052850723
CurrentTrain: epoch  0, batch    15 | loss: 17.1436984torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.19427127018571 1.4594173431396484 1.6570197343826294 3.9717828668653965
CurrentTrain: epoch  0, batch    16 | loss: 16.1942713torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.459271132946014 1.6903033256530762 1.5330939292907715 3.772894561290741
CurrentTrain: epoch  0, batch    17 | loss: 16.4592711torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  17.998283490538597 1.6119736433029175 1.3695379495620728 5.051900014281273
CurrentTrain: epoch  0, batch    18 | loss: 17.9982835torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  13.982996344566345 1.6295504570007324 1.4092522859573364 1.4333728551864624
CurrentTrain: epoch  0, batch    19 | loss: 13.9829963torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  15.940610405057669 1.6465280055999756 1.7016754150390625 4.266829963773489
CurrentTrain: epoch  0, batch    20 | loss: 15.9406104torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.59988392703235 1.5615501403808594 1.2718346118927002 5.049848450347781
CurrentTrain: epoch  0, batch    21 | loss: 16.5998839torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  20.975914299488068 1.6412009000778198 1.6325050592422485 8.69694834947586
CurrentTrain: epoch  0, batch    22 | loss: 20.9759143torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  14.228049516677856 1.625218391418457 1.6052501201629639 1.4379818439483643
CurrentTrain: epoch  0, batch    23 | loss: 14.2280495torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  18.024254232645035 1.5927014350891113 1.6466702222824097 5.693576246500015
CurrentTrain: epoch  0, batch    24 | loss: 18.0242542torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  12.60379409790039 1.7335578203201294 1.5859875679016113 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 12.6037941torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  12.096501350402832 1.732974886894226 1.4160926342010498 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 12.0965014torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  17.054589487612247 1.5616824626922607 1.4994038343429565 4.797547556459904
CurrentTrain: epoch  0, batch    27 | loss: 17.0545895torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  16.390363551676273 1.70139479637146 1.427811622619629 4.619444705545902
CurrentTrain: epoch  0, batch    28 | loss: 16.3903636torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  12.18915843963623 1.7167983055114746 1.494136095046997 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 12.1891584torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  10.99421501159668 1.6499271392822266 1.4652628898620605 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 10.9942150torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  15.835686855018139 1.4102174043655396 1.3574326038360596 5.206738643348217
CurrentTrain: epoch  0, batch    31 | loss: 15.8356869torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  11.24498462677002 1.5517385005950928 1.332444190979004 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 11.2449846torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  11.816437512636185 1.5825302600860596 1.4061193466186523 1.4026439487934113
CurrentTrain: epoch  0, batch    33 | loss: 11.8164375torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  15.296016965061426 1.7143665552139282 1.3828608989715576 3.294786725193262
CurrentTrain: epoch  0, batch    34 | loss: 15.2960170torch.Size([16, 1, 768])
torch.Size([16, 768])
Losses:  15.39129101857543 1.7215101718902588 1.4211487770080566 3.332077380269766
CurrentTrain: epoch  0, batch    35 | loss: 15.3912910#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  17.8325113710016 1.3207916021347046 1.3458771705627441 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 17.8325114Losses:  18.842761002480984 1.5324923992156982 1.7468078136444092 5.8394975289702415
CurrentTrain: epoch  0, batch     1 | loss: 18.8427610Losses:  17.383552636951208 1.485356092453003 1.493894100189209 4.408569421619177
CurrentTrain: epoch  0, batch     2 | loss: 17.3835526Losses:  17.43883127346635 1.412987232208252 1.6456342935562134 5.16335004940629
CurrentTrain: epoch  0, batch     3 | loss: 17.4388313Losses:  15.718206461519003 1.5302681922912598 1.5684990882873535 2.533542688935995
CurrentTrain: epoch  0, batch     4 | loss: 15.7182065Losses:  15.986056350171566 1.4117004871368408 1.4590086936950684 4.398505233228207
CurrentTrain: epoch  0, batch     5 | loss: 15.9860564Losses:  20.847005769610405 1.4647552967071533 1.4214534759521484 8.307585641741753
CurrentTrain: epoch  0, batch     6 | loss: 20.8470058Losses:  16.93151082843542 1.6323354244232178 1.6805472373962402 3.8532599434256554
CurrentTrain: epoch  0, batch     7 | loss: 16.9315108Losses:  23.7795290350914 1.5684888362884521 1.388635516166687 10.504817426204681
CurrentTrain: epoch  0, batch     8 | loss: 23.7795290Losses:  14.76447919011116 1.4234799146652222 1.5253527164459229 2.8934417068958282
CurrentTrain: epoch  0, batch     9 | loss: 14.7644792Losses:  15.991824455559254 1.493248701095581 1.5466408729553223 3.8598912432789803
CurrentTrain: epoch  0, batch    10 | loss: 15.9918245Losses:  15.670805051922798 1.4432134628295898 1.2373114824295044 3.0652085095643997
CurrentTrain: epoch  0, batch    11 | loss: 15.6708051Losses:  13.105938911437988 1.526972770690918 1.524843454360962 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 13.1059389Losses:  13.007719039916992 1.5933303833007812 1.4920493364334106 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 13.0077190Losses:  18.258839163929224 1.4949532747268677 1.353956699371338 5.3638253547251225
CurrentTrain: epoch  0, batch    14 | loss: 18.2588392Losses:  16.6366720572114 1.2535631656646729 1.174203634262085 5.073753394186497
CurrentTrain: epoch  0, batch    15 | loss: 16.6366721Losses:  15.455941177904606 1.1306062936782837 1.2822798490524292 4.031375862658024
CurrentTrain: epoch  0, batch    16 | loss: 15.4559412Losses:  15.81019613519311 1.398192286491394 1.1927976608276367 3.8310701586306095
CurrentTrain: epoch  0, batch    17 | loss: 15.8101961Losses:  17.429217256605625 1.365299940109253 1.1031441688537598 5.076182283461094
CurrentTrain: epoch  0, batch    18 | loss: 17.4292173Losses:  13.414463698863983 1.3998513221740723 1.1207114458084106 1.4305989146232605
CurrentTrain: epoch  0, batch    19 | loss: 13.4144637Losses:  14.973900705575943 1.3211811780929565 1.2052100896835327 4.267893701791763
CurrentTrain: epoch  0, batch    20 | loss: 14.9739007Losses:  15.911356084048748 1.2588847875595093 0.9893219470977783 5.037974469363689
CurrentTrain: epoch  0, batch    21 | loss: 15.9113561Losses:  20.53553406894207 1.3873181343078613 1.395268201828003 8.857686206698418
CurrentTrain: epoch  0, batch    22 | loss: 20.5355341Losses:  13.603842109441757 1.3914077281951904 1.3051576614379883 1.4495843350887299
CurrentTrain: epoch  0, batch    23 | loss: 13.6038421Losses:  17.40914604254067 1.2684026956558228 1.3548033237457275 5.791622849181294
CurrentTrain: epoch  0, batch    24 | loss: 17.4091460Losses:  11.936044692993164 1.4387047290802002 1.3207290172576904 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 11.9360447Losses:  11.52356243133545 1.4761807918548584 1.220576524734497 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 11.5235624Losses:  16.069647945463657 1.2620047330856323 1.0538842678070068 4.731902278959751
CurrentTrain: epoch  0, batch    27 | loss: 16.0696479Losses:  15.454413639381528 1.365644097328186 1.0892164707183838 4.468331562355161
CurrentTrain: epoch  0, batch    28 | loss: 15.4544136Losses:  11.683222770690918 1.5182031393051147 1.320703387260437 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 11.6832228Losses:  10.37234878540039 1.3453110456466675 1.1892269849777222 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 10.3723488Losses:  14.928508408367634 1.0682648420333862 1.0659109354019165 5.063400872051716
CurrentTrain: epoch  0, batch    31 | loss: 14.9285084Losses:  10.664624214172363 1.2431036233901978 1.1476078033447266 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 10.6646242Losses:  11.09028074145317 1.2217845916748047 1.164310097694397 1.405880182981491
CurrentTrain: epoch  0, batch    33 | loss: 11.0902807Losses:  14.414051294326782 1.401850700378418 1.068601369857788 3.2143776416778564
CurrentTrain: epoch  0, batch    34 | loss: 14.4140513Losses:  14.748584270477295 1.4128026962280273 1.1466326713562012 3.3187174797058105
CurrentTrain: epoch  0, batch    35 | loss: 14.7485843Losses:  12.241449810564518 1.0970999002456665 1.0274301767349243 1.7403597608208656
CurrentTrain: epoch  0, batch    36 | loss: 12.2414498Losses:  11.912926524877548 1.2196439504623413 1.0809884071350098 1.423468440771103
CurrentTrain: epoch  0, batch    37 | loss: 11.9129265Losses:  10.349813461303711 1.4340180158615112 1.1619422435760498 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 10.3498135Losses:  16.716614097356796 1.4588384628295898 1.028402328491211 5.216134399175644
CurrentTrain: epoch  0, batch    39 | loss: 16.7166141Losses:  18.23864035308361 1.4374427795410156 1.229217290878296 7.268166109919548
CurrentTrain: epoch  0, batch    40 | loss: 18.2386404Losses:  17.333687514066696 1.2492848634719849 1.0390292406082153 5.94742938876152
CurrentTrain: epoch  0, batch    41 | loss: 17.3336875Losses:  15.058479938656092 1.368996262550354 1.248591661453247 3.7951456643640995
CurrentTrain: epoch  0, batch    42 | loss: 15.0584799Losses:  17.056481957435608 1.2146509885787964 1.0231869220733643 6.798030495643616
CurrentTrain: epoch  0, batch    43 | loss: 17.0564820Losses:  12.393061455339193 1.308825969696045 1.048115611076355 1.8163202367722988
CurrentTrain: epoch  0, batch    44 | loss: 12.3930615Losses:  10.168886184692383 1.2702751159667969 1.1183390617370605 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 10.1688862Losses:  15.723511397838593 1.294669508934021 1.081739902496338 4.873753249645233
CurrentTrain: epoch  0, batch    46 | loss: 15.7235114Losses:  15.704581387341022 1.277652621269226 0.9812020063400269 4.902549870312214
CurrentTrain: epoch  0, batch    47 | loss: 15.7045814Losses:  13.607742315158248 1.3400249481201172 1.2831863164901733 2.5779628809541464
CurrentTrain: epoch  0, batch    48 | loss: 13.6077423Losses:  12.844781659543514 1.4167895317077637 1.132738471031189 1.5373742803931236
CurrentTrain: epoch  0, batch    49 | loss: 12.8447817Losses:  19.568169061094522 1.3392276763916016 1.1274030208587646 8.678995553404093
CurrentTrain: epoch  0, batch    50 | loss: 19.5681691Losses:  14.097479093819857 1.1885350942611694 1.1505558490753174 4.6763784773647785
CurrentTrain: epoch  0, batch    51 | loss: 14.0974791Losses:  11.676262855529785 1.5036617517471313 1.1749531030654907 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 11.6762629Losses:  15.574866376817226 1.016784429550171 0.9509761333465576 5.4145670756697655
CurrentTrain: epoch  0, batch    53 | loss: 15.5748664Losses:  17.336606830358505 1.0403969287872314 0.9773552417755127 7.512617915868759
CurrentTrain: epoch  0, batch    54 | loss: 17.3366068Losses:  11.32258528843522 1.2733551263809204 1.0741169452667236 1.5446187891066074
CurrentTrain: epoch  0, batch    55 | loss: 11.3225853Losses:  22.767075553536415 1.118213176727295 0.9966580867767334 12.257248893380165
CurrentTrain: epoch  0, batch    56 | loss: 22.7670756Losses:  12.56157997623086 1.1013156175613403 1.0759776830673218 2.990099225193262
CurrentTrain: epoch  0, batch    57 | loss: 12.5615800Losses:  14.003480765968561 1.2887749671936035 1.2228482961654663 3.7965343929827213
CurrentTrain: epoch  0, batch    58 | loss: 14.0034808Losses:  11.946462858468294 1.2450077533721924 1.159077525138855 1.5222217924892902
CurrentTrain: epoch  0, batch    59 | loss: 11.9464629Losses:  25.353260099887848 1.0560684204101562 1.0038998126983643 16.04349046945572
CurrentTrain: epoch  0, batch    60 | loss: 25.3532601Losses:  13.83283857256174 1.0912607908248901 0.9856101274490356 4.628367938101292
CurrentTrain: epoch  0, batch    61 | loss: 13.8328386Losses:  12.360199194401503 1.2084486484527588 1.3477346897125244 1.6455499939620495
CurrentTrain: epoch  0, batch    62 | loss: 12.3601992Losses:  10.545089721679688 1.403262972831726 1.047142505645752 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 10.5450897Losses:  13.906329836230725 1.2112445831298828 1.1848955154418945 4.146977152209729
CurrentTrain: epoch  1, batch     1 | loss: 13.9063298Losses:  16.33608578518033 1.3565174341201782 1.100113034248352 6.620824325829744
CurrentTrain: epoch  1, batch     2 | loss: 16.3360858Losses:  11.200151801109314 1.174576997756958 1.0945974588394165 1.4741042852401733
CurrentTrain: epoch  1, batch     3 | loss: 11.2001518Losses:  12.252843793481588 1.3717350959777832 1.1656708717346191 1.868469174951315
CurrentTrain: epoch  1, batch     4 | loss: 12.2528438Losses:  10.118658304214478 1.0406205654144287 1.0717893838882446 1.403026819229126
CurrentTrain: epoch  1, batch     5 | loss: 10.1186583Losses:  9.952276229858398 1.1296029090881348 1.0471423864364624 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 9.9522762Losses:  11.465929355472326 1.077313780784607 0.9541640281677246 1.739490833133459
CurrentTrain: epoch  1, batch     7 | loss: 11.4659294Losses:  10.950625700876117 1.1430140733718872 0.9778729677200317 1.5570500325411558
CurrentTrain: epoch  1, batch     8 | loss: 10.9506257Losses:  15.779109951108694 1.215461015701294 1.0126842260360718 6.614405628293753
CurrentTrain: epoch  1, batch     9 | loss: 15.7791100Losses:  15.306032057851553 1.1589207649230957 0.9203194379806519 5.09097945317626
CurrentTrain: epoch  1, batch    10 | loss: 15.3060321Losses:  12.810432516038418 1.1699131727218628 1.0899454355239868 3.1835614070296288
CurrentTrain: epoch  1, batch    11 | loss: 12.8104325Losses:  12.195070177316666 1.0501145124435425 1.0132615566253662 2.837837129831314
CurrentTrain: epoch  1, batch    12 | loss: 12.1950702Losses:  9.07127571105957 1.0312196016311646 1.0529659986495972 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 9.0712757Losses:  13.870894342660904 1.2073392868041992 0.921332836151123 3.571444422006607
CurrentTrain: epoch  1, batch    14 | loss: 13.8708943Losses:  9.467132568359375 1.1042629480361938 0.940834641456604 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 9.4671326Losses:  13.450125221163034 1.0779069662094116 1.005784273147583 3.09850550070405
CurrentTrain: epoch  1, batch    16 | loss: 13.4501252Losses:  14.66779688745737 1.0669347047805786 0.9763401746749878 4.396702565252781
CurrentTrain: epoch  1, batch    17 | loss: 14.6677969Losses:  15.113317741081119 0.9344007968902588 1.1355998516082764 6.01237322203815
CurrentTrain: epoch  1, batch    18 | loss: 15.1133177Losses:  9.61666488647461 1.1701737642288208 1.0409339666366577 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 9.6166649Losses:  9.263530731201172 0.9930236339569092 1.141779899597168 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 9.2635307Losses:  19.9199930280447 1.2362761497497559 1.0054465532302856 9.666219338774681
CurrentTrain: epoch  1, batch    21 | loss: 19.9199930Losses:  9.842580795288086 1.1274073123931885 1.1266652345657349 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 9.8425808Losses:  9.03990364074707 1.0970005989074707 1.0715396404266357 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 9.0399036Losses:  12.32873672991991 1.0036041736602783 1.0195286273956299 4.347731061279774
CurrentTrain: epoch  1, batch    24 | loss: 12.3287367Losses:  11.081105563789606 1.039196491241455 1.113779067993164 2.867839191108942
CurrentTrain: epoch  1, batch    25 | loss: 11.0811056Losses:  9.841293334960938 1.0789381265640259 0.9997711181640625 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 9.8412933Losses:  9.307010650634766 1.144188642501831 0.9762663841247559 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 9.3070107Losses:  9.676140785217285 1.1395138502120972 1.0690728425979614 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 9.6761408Losses:  13.366287045180798 1.033976674079895 1.0263679027557373 4.327914051711559
CurrentTrain: epoch  1, batch    29 | loss: 13.3662870Losses:  13.542207967489958 1.2281626462936401 1.1087491512298584 3.43483854457736
CurrentTrain: epoch  1, batch    30 | loss: 13.5422080Losses:  13.169384509325027 1.0953611135482788 1.1087857484817505 3.2140832245349884
CurrentTrain: epoch  1, batch    31 | loss: 13.1693845Losses:  13.727126881480217 1.0652521848678589 1.128026008605957 4.035565182566643
CurrentTrain: epoch  1, batch    32 | loss: 13.7271269Losses:  8.648966789245605 0.9770762920379639 0.9429951906204224 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 8.6489668Losses:  8.489923477172852 1.021682620048523 1.0393590927124023 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 8.4899235Losses:  16.975258223712444 1.3640074729919434 1.0957540273666382 6.707654349505901
CurrentTrain: epoch  1, batch    35 | loss: 16.9752582Losses:  11.218011498451233 1.1358802318572998 0.9691530466079712 1.6479755640029907
CurrentTrain: epoch  1, batch    36 | loss: 11.2180115Losses:  9.823503412306309 0.8332159519195557 1.0200570821762085 1.7059077396988869
CurrentTrain: epoch  1, batch    37 | loss: 9.8235034Losses:  11.296367079019547 0.9725161790847778 1.036231279373169 2.9918245375156403
CurrentTrain: epoch  1, batch    38 | loss: 11.2963671Losses:  12.959395870566368 1.068570613861084 1.0 4.628081783652306
CurrentTrain: epoch  1, batch    39 | loss: 12.9593959Losses:  10.285419825464487 1.0142645835876465 1.0777117013931274 1.449634913355112
CurrentTrain: epoch  1, batch    40 | loss: 10.2854198Losses:  11.440013494342566 0.9772491455078125 1.0 2.9590211771428585
CurrentTrain: epoch  1, batch    41 | loss: 11.4400135Losses:  10.881284713745117 1.1006906032562256 1.0 1.833073616027832
CurrentTrain: epoch  1, batch    42 | loss: 10.8812847Losses:  13.564829960465431 1.0421712398529053 1.0658801794052124 4.62349046766758
CurrentTrain: epoch  1, batch    43 | loss: 13.5648300Losses:  13.828250268474221 1.0919573307037354 1.136006236076355 3.844177583232522
CurrentTrain: epoch  1, batch    44 | loss: 13.8282503Losses:  12.493675448000431 1.1443307399749756 1.0396013259887695 2.9989750161767006
CurrentTrain: epoch  1, batch    45 | loss: 12.4936754Losses:  9.908737182617188 1.219794511795044 1.0826233625411987 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 9.9087372Losses:  9.17241096496582 1.140626072883606 0.9633421897888184 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 9.1724110Losses:  10.6330127120018 0.9973587989807129 0.9660223722457886 1.4185819029808044
CurrentTrain: epoch  1, batch    48 | loss: 10.6330127Losses:  18.765890523791313 1.0295319557189941 1.0104420185089111 9.343123838305473
CurrentTrain: epoch  1, batch    49 | loss: 18.7658905Losses:  10.690869335085154 0.9305433034896851 0.9918156862258911 1.430158618837595
CurrentTrain: epoch  1, batch    50 | loss: 10.6908693Losses:  9.285523414611816 1.233931541442871 1.0244745016098022 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 9.2855234Losses:  15.43646178767085 1.1350133419036865 0.9848265647888184 6.524646144360304
CurrentTrain: epoch  1, batch    52 | loss: 15.4364618Losses:  8.084447860717773 1.0172942876815796 0.9819166660308838 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 8.0844479Losses:  11.418784201145172 0.8681888580322266 0.990607500076294 3.123801290988922
CurrentTrain: epoch  1, batch    54 | loss: 11.4187842Losses:  11.43918962776661 1.002936840057373 1.026058316230774 2.932257369160652
CurrentTrain: epoch  1, batch    55 | loss: 11.4391896Losses:  9.022956848144531 1.0849578380584717 1.0330332517623901 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 9.0229568Losses:  9.401742935180664 1.11001455783844 1.0168499946594238 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 9.4017429Losses:  8.539938926696777 1.0016465187072754 1.139478325843811 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 8.5399389Losses:  12.656333547085524 1.0748146772384644 1.0469156503677368 3.209568601101637
CurrentTrain: epoch  1, batch    59 | loss: 12.6563335Losses:  8.62523078918457 1.0831578969955444 1.0407272577285767 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 8.6252308Losses:  10.372823715209961 1.1874488592147827 1.091187596321106 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 10.3728237Losses:  11.089780628681183 0.8987717628479004 1.0 4.315230667591095
CurrentTrain: epoch  1, batch    62 | loss: 11.0897806Losses:  7.484826564788818 0.9160323143005371 1.0620193481445312 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.4848266Losses:  8.305792808532715 1.0112534761428833 1.0 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 8.3057928Losses:  8.92714824527502 0.7934435606002808 1.0051336288452148 1.4156102165579796
CurrentTrain: epoch  2, batch     2 | loss: 8.9271482Losses:  13.993455938994884 1.2405846118927002 1.1665118932724 3.2259293124079704
CurrentTrain: epoch  2, batch     3 | loss: 13.9934559Losses:  10.923126317560673 1.016181468963623 1.0011835098266602 1.5455828681588173
CurrentTrain: epoch  2, batch     4 | loss: 10.9231263Losses:  10.958402000367641 0.9383786916732788 1.0 2.0971234664320946
CurrentTrain: epoch  2, batch     5 | loss: 10.9584020Losses:  8.655322074890137 1.1261693239212036 1.033748984336853 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 8.6553221Losses:  13.429755859076977 0.988735556602478 1.0037890672683716 4.876285247504711
CurrentTrain: epoch  2, batch     7 | loss: 13.4297559Losses:  11.748767878860235 0.9672360420227051 1.0010950565338135 3.358104731887579
CurrentTrain: epoch  2, batch     8 | loss: 11.7487679Losses:  12.74734714999795 1.090450406074524 1.0134177207946777 3.718593869358301
CurrentTrain: epoch  2, batch     9 | loss: 12.7473471Losses:  9.650530245155096 0.9008744955062866 0.9702951908111572 1.5089429877698421
CurrentTrain: epoch  2, batch    10 | loss: 9.6505302Losses:  9.021763622760773 0.7485121488571167 1.0 1.4900014996528625
CurrentTrain: epoch  2, batch    11 | loss: 9.0217636Losses:  14.731789126992226 1.12270188331604 0.9537137746810913 5.986845508217812
CurrentTrain: epoch  2, batch    12 | loss: 14.7317891Losses:  9.932957917451859 0.9112981557846069 1.0906051397323608 1.47006157040596
CurrentTrain: epoch  2, batch    13 | loss: 9.9329579Losses:  12.40489823371172 1.2079936265945435 1.092924952507019 2.9226356223225594
CurrentTrain: epoch  2, batch    14 | loss: 12.4048982Losses:  10.599495559930801 0.9625004529953003 0.9670994281768799 2.177713066339493
CurrentTrain: epoch  2, batch    15 | loss: 10.5994956Losses:  11.465044386684895 0.8768055438995361 1.0143331289291382 2.8769935443997383
CurrentTrain: epoch  2, batch    16 | loss: 11.4650444Losses:  11.358898546546698 0.9704685211181641 1.0 2.9241050742566586
CurrentTrain: epoch  2, batch    17 | loss: 11.3588985Losses:  18.60611417889595 0.8216552734375 1.0 11.32872560620308
CurrentTrain: epoch  2, batch    18 | loss: 18.6061142Losses:  10.6662238240242 1.0059031248092651 0.9739353656768799 1.7739785313606262
CurrentTrain: epoch  2, batch    19 | loss: 10.6662238Losses:  7.820032596588135 0.9762841463088989 1.0 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 7.8200326Losses:  8.439369201660156 0.9651286602020264 1.0 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 8.4393692Losses:  12.71405615657568 1.0567363500595093 0.9896376132965088 4.341150425374508
CurrentTrain: epoch  2, batch    22 | loss: 12.7140562Losses:  10.25797601789236 0.7689367532730103 0.9767225980758667 1.5631575211882591
CurrentTrain: epoch  2, batch    23 | loss: 10.2579760Losses:  8.822470810264349 0.8567248582839966 0.9914648532867432 1.45127597078681
CurrentTrain: epoch  2, batch    24 | loss: 8.8224708Losses:  12.880481973290443 1.0450297594070435 1.0 4.687831178307533
CurrentTrain: epoch  2, batch    25 | loss: 12.8804820Losses:  9.728429913520813 0.8505204916000366 0.9830322265625 1.4099293947219849
CurrentTrain: epoch  2, batch    26 | loss: 9.7284299Losses:  9.75727954506874 0.7938010692596436 1.0198873281478882 1.7900034487247467
CurrentTrain: epoch  2, batch    27 | loss: 9.7572795Losses:  9.442066013813019 0.9212771654129028 1.048945665359497 1.4179142117500305
CurrentTrain: epoch  2, batch    28 | loss: 9.4420660Losses:  19.66032215207815 1.0519338846206665 0.9624931812286377 11.569475136697292
CurrentTrain: epoch  2, batch    29 | loss: 19.6603222Losses:  9.643437471240759 0.9765689373016357 1.0607025623321533 1.439498033374548
CurrentTrain: epoch  2, batch    30 | loss: 9.6434375Losses:  9.15456473827362 0.8028278350830078 1.0 1.507865309715271
CurrentTrain: epoch  2, batch    31 | loss: 9.1545647Losses:  10.534806698560715 0.7434694766998291 0.9745770692825317 2.9224028289318085
CurrentTrain: epoch  2, batch    32 | loss: 10.5348067Losses:  11.257690124213696 0.9251282215118408 1.0428180694580078 2.109483413398266
CurrentTrain: epoch  2, batch    33 | loss: 11.2576901Losses:  13.806197013705969 0.9836560487747192 1.0610226392745972 5.629828300327063
CurrentTrain: epoch  2, batch    34 | loss: 13.8061970Losses:  15.023574769496918 0.9417617321014404 0.9819614887237549 5.891352593898773
CurrentTrain: epoch  2, batch    35 | loss: 15.0235748Losses:  11.054701298475266 0.7733134031295776 1.0461384057998657 2.8671946227550507
CurrentTrain: epoch  2, batch    36 | loss: 11.0547013Losses:  10.591145411133766 0.8580619096755981 1.0154075622558594 1.8351162821054459
CurrentTrain: epoch  2, batch    37 | loss: 10.5911454Losses:  9.429381366819143 0.8643231391906738 0.998078465461731 1.406370159238577
CurrentTrain: epoch  2, batch    38 | loss: 9.4293814Losses:  12.114940010011196 0.8887325525283813 1.0 3.3705533370375633
CurrentTrain: epoch  2, batch    39 | loss: 12.1149400Losses:  11.92723709717393 0.7627105712890625 1.0 4.369772497564554
CurrentTrain: epoch  2, batch    40 | loss: 11.9272371Losses:  11.047069795429707 0.9337631464004517 1.1017533540725708 1.463624246418476
CurrentTrain: epoch  2, batch    41 | loss: 11.0470698Losses:  9.051938503980637 0.7053548097610474 0.9824361801147461 1.408750981092453
CurrentTrain: epoch  2, batch    42 | loss: 9.0519385Losses:  8.567676544189453 1.0126339197158813 1.0 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 8.5676765Losses:  8.87219724059105 0.7382389307022095 0.9621397256851196 1.4451814591884613
CurrentTrain: epoch  2, batch    44 | loss: 8.8721972Losses:  11.245016857981682 0.9338955879211426 1.0234285593032837 2.959852024912834
CurrentTrain: epoch  2, batch    45 | loss: 11.2450169Losses:  7.571946144104004 0.7275701761245728 0.986986517906189 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 7.5719461Losses:  8.384698867797852 0.9517315626144409 1.0477759838104248 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 8.3846989Losses:  7.613092422485352 0.9533481597900391 1.0232044458389282 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 7.6130924Losses:  10.054198358207941 0.9698523283004761 1.0023616552352905 1.4502383209764957
CurrentTrain: epoch  2, batch    49 | loss: 10.0541984Losses:  16.738931145519018 0.7914291620254517 1.0 8.546283211559057
CurrentTrain: epoch  2, batch    50 | loss: 16.7389311Losses:  10.795476127415895 0.8370708227157593 1.0 3.4054896123707294
CurrentTrain: epoch  2, batch    51 | loss: 10.7954761Losses:  8.927562413737178 0.6320041418075562 0.9881258010864258 1.6504527907818556
CurrentTrain: epoch  2, batch    52 | loss: 8.9275624Losses:  10.43856155499816 0.7750673294067383 1.0 2.852442856878042
CurrentTrain: epoch  2, batch    53 | loss: 10.4385616Losses:  9.404038667678833 0.7378495931625366 0.9901183843612671 1.4345438480377197
CurrentTrain: epoch  2, batch    54 | loss: 9.4040387Losses:  10.320078115910292 0.8569501638412476 1.0 2.97140858694911
CurrentTrain: epoch  2, batch    55 | loss: 10.3200781Losses:  9.399507220834494 0.8090180158615112 1.0426503419876099 1.8212029300630093
CurrentTrain: epoch  2, batch    56 | loss: 9.3995072Losses:  8.37314224243164 0.8237501382827759 1.0102837085723877 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 8.3731422Losses:  7.816232204437256 0.7255932092666626 1.000923991203308 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 7.8162322Losses:  19.46790698543191 0.8410415649414062 1.0 11.937477145344019
CurrentTrain: epoch  2, batch    59 | loss: 19.4679070Losses:  7.886335372924805 0.8105117082595825 1.0171400308609009 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 7.8863354Losses:  10.984786685556173 0.9588445425033569 1.0231034755706787 1.981245692819357
CurrentTrain: epoch  2, batch    61 | loss: 10.9847867Losses:  7.394787788391113 0.7258801460266113 0.9727737903594971 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 7.3947878Losses:  8.883134216070175 0.9058786630630493 1.0 1.414179652929306
CurrentTrain: epoch  3, batch     0 | loss: 8.8831342Losses:  8.932454258203506 0.8260918855667114 0.9705548286437988 1.4199415743350983
CurrentTrain: epoch  3, batch     1 | loss: 8.9324543Losses:  7.844407558441162 0.7598142623901367 0.9959157705307007 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 7.8444076Losses:  12.242364462465048 0.9231851100921631 0.9879205226898193 4.354378279298544
CurrentTrain: epoch  3, batch     3 | loss: 12.2423645Losses:  10.192504435777664 0.653457522392273 1.0 3.384415179491043
CurrentTrain: epoch  3, batch     4 | loss: 10.1925044Losses:  12.281582955271006 0.6713587045669556 1.0 4.365874890238047
CurrentTrain: epoch  3, batch     5 | loss: 12.2815830Losses:  8.838524848222733 0.678071141242981 1.0040467977523804 1.3972916901111603
CurrentTrain: epoch  3, batch     6 | loss: 8.8385248Losses:  11.788382887840271 0.7400399446487427 1.0272488594055176 4.264087557792664
CurrentTrain: epoch  3, batch     7 | loss: 11.7883829Losses:  8.792822480201721 0.7392896413803101 1.0045061111450195 1.4459019899368286
CurrentTrain: epoch  3, batch     8 | loss: 8.7928225Losses:  8.162877082824707 0.8311588764190674 1.0661643743515015 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 8.1628771Losses:  7.623223781585693 0.837553858757019 1.0 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 7.6232238Losses:  10.277345038950443 0.6927216053009033 1.0 2.9485724940896034
CurrentTrain: epoch  3, batch    11 | loss: 10.2773450Losses:  11.697729527950287 0.9779258966445923 1.0351476669311523 4.220159471035004
CurrentTrain: epoch  3, batch    12 | loss: 11.6977295Losses:  11.896435435861349 1.069736361503601 0.9956371784210205 2.9388672672212124
CurrentTrain: epoch  3, batch    13 | loss: 11.8964354Losses:  18.840993385761976 0.8358346223831177 1.0 10.756441574543715
CurrentTrain: epoch  3, batch    14 | loss: 18.8409934Losses:  9.46481093019247 0.9026061296463013 1.0448379516601562 1.415352426469326
CurrentTrain: epoch  3, batch    15 | loss: 9.4648109Losses:  13.950598865747452 0.7540839910507202 1.0286918878555298 6.073151737451553
CurrentTrain: epoch  3, batch    16 | loss: 13.9505989Losses:  8.743507534265518 0.8366907835006714 1.0535774230957031 1.45180144906044
CurrentTrain: epoch  3, batch    17 | loss: 8.7435075Losses:  11.441434517502785 0.5409098863601685 1.0 4.4069444090127945
CurrentTrain: epoch  3, batch    18 | loss: 11.4414345Losses:  8.183289088308811 0.6866663694381714 1.0 1.5597577467560768
CurrentTrain: epoch  3, batch    19 | loss: 8.1832891Losses:  11.092736460268497 0.7844356298446655 0.9918403625488281 4.252940870821476
CurrentTrain: epoch  3, batch    20 | loss: 11.0927365Losses:  10.431606225669384 0.5161492824554443 1.0 3.4051551148295403
CurrentTrain: epoch  3, batch    21 | loss: 10.4316062Losses:  14.306440830230713 0.8750113248825073 1.0258595943450928 6.237823963165283
CurrentTrain: epoch  3, batch    22 | loss: 14.3064408Losses:  9.33639955893159 0.647952675819397 1.0480763912200928 1.4459099806845188
CurrentTrain: epoch  3, batch    23 | loss: 9.3363996Losses:  9.418081901967525 0.772720456123352 1.0452135801315308 1.4761678203940392
CurrentTrain: epoch  3, batch    24 | loss: 9.4180819Losses:  10.979671649634838 0.817778468132019 0.9750922918319702 2.910349063575268
CurrentTrain: epoch  3, batch    25 | loss: 10.9796716Losses:  9.609769735485315 0.6839019060134888 1.0 2.9132222272455692
CurrentTrain: epoch  3, batch    26 | loss: 9.6097697Losses:  8.034473817795515 0.5110349655151367 1.0 1.4668792895972729
CurrentTrain: epoch  3, batch    27 | loss: 8.0344738Losses:  12.796133164316416 0.8124746084213257 0.9896187782287598 4.349238518625498
CurrentTrain: epoch  3, batch    28 | loss: 12.7961332Losses:  12.833309821784496 0.6142818927764893 1.0 5.727892570197582
CurrentTrain: epoch  3, batch    29 | loss: 12.8333098Losses:  7.112881660461426 0.681676983833313 1.0359896421432495 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 7.1128817Losses:  6.947584629058838 0.6641851663589478 0.9896092414855957 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 6.9475846Losses:  12.703346882015467 0.6030620336532593 0.9932290315628052 5.772420559078455
CurrentTrain: epoch  3, batch    32 | loss: 12.7033469Losses:  10.552086561918259 0.8087440729141235 1.0 2.9758341014385223
CurrentTrain: epoch  3, batch    33 | loss: 10.5520866Losses:  7.6484856605529785 0.6953874826431274 1.0514640808105469 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 7.6484857Losses:  9.659937622025609 0.7808898687362671 0.98046875 1.5601470489054918
CurrentTrain: epoch  3, batch    35 | loss: 9.6599376Losses:  6.689492225646973 0.5794379711151123 1.0 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 6.6894922Losses:  7.0402703285217285 0.6875902414321899 0.9821395874023438 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 7.0402703Losses:  9.964625924825668 0.7612059116363525 1.0 2.8103462159633636
CurrentTrain: epoch  3, batch    38 | loss: 9.9646259Losses:  11.622552588582039 0.812049388885498 1.0 4.312750533223152
CurrentTrain: epoch  3, batch    39 | loss: 11.6225526Losses:  16.780799135565758 0.8053761720657349 1.0619704723358154 7.51924155652523
CurrentTrain: epoch  3, batch    40 | loss: 16.7807991Losses:  7.911394119262695 0.6519243717193604 1.0 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 7.9113941Losses:  7.893013149499893 0.5655580759048462 0.9830127954483032 1.4057389795780182
CurrentTrain: epoch  3, batch    42 | loss: 7.8930131Losses:  9.089087277650833 0.7850545644760132 1.0173665285110474 1.8000815212726593
CurrentTrain: epoch  3, batch    43 | loss: 9.0890873Losses:  10.804319508373737 0.6502977609634399 0.9861457347869873 3.0810586288571358
CurrentTrain: epoch  3, batch    44 | loss: 10.8043195Losses:  10.07402490451932 0.5425605773925781 1.0 3.101083029061556
CurrentTrain: epoch  3, batch    45 | loss: 10.0740249Losses:  11.372145073488355 0.7340459823608398 1.0 3.422403233125806
CurrentTrain: epoch  3, batch    46 | loss: 11.3721451Losses:  9.596339233219624 0.6272920370101929 1.0 2.8475527837872505
CurrentTrain: epoch  3, batch    47 | loss: 9.5963392Losses:  7.270019054412842 0.7873539924621582 1.0 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 7.2700191Losses:  17.999495442956686 0.5819125175476074 0.968165397644043 10.605173524469137
CurrentTrain: epoch  3, batch    49 | loss: 17.9994954Losses:  7.690356731414795 0.6451481580734253 1.0394474267959595 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 7.6903567Losses:  8.668329272419214 0.5187691450119019 1.0479133129119873 1.4229221679270267
CurrentTrain: epoch  3, batch    51 | loss: 8.6683293Losses:  11.16165678948164 0.6465442180633545 1.0 4.346312932670116
CurrentTrain: epoch  3, batch    52 | loss: 11.1616568Losses:  12.781441677361727 0.9196832180023193 1.1154881715774536 4.357589710503817
CurrentTrain: epoch  3, batch    53 | loss: 12.7814417Losses:  7.974111020565033 0.5751974582672119 1.0 1.3994478583335876
CurrentTrain: epoch  3, batch    54 | loss: 7.9741110Losses:  9.68320643901825 0.43821394443511963 1.0 2.8351224660873413
CurrentTrain: epoch  3, batch    55 | loss: 9.6832064Losses:  6.6622843742370605 0.5317672491073608 1.0 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 6.6622844Losses:  7.30702018737793 0.594936728477478 1.0554338693618774 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.3070202Losses:  11.768674179911613 0.42176806926727295 1.0 5.247259423136711
CurrentTrain: epoch  3, batch    58 | loss: 11.7686742Losses:  17.040567688643932 0.5353959798812866 0.9627028703689575 9.593599133193493
CurrentTrain: epoch  3, batch    59 | loss: 17.0405677Losses:  7.569631576538086 0.6127727031707764 1.0117281675338745 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 7.5696316Losses:  12.069843456149101 0.5859503746032715 1.0 4.663798972964287
CurrentTrain: epoch  3, batch    61 | loss: 12.0698435Losses:  8.106061905622482 0.47778773307800293 1.0 1.4008483588695526
CurrentTrain: epoch  3, batch    62 | loss: 8.1060619Losses:  11.537259306758642 0.6229444742202759 1.0353280305862427 4.320158209651709
CurrentTrain: epoch  4, batch     0 | loss: 11.5372593Losses:  12.263626620173454 0.6624211072921753 1.0786116123199463 4.9754519909620285
CurrentTrain: epoch  4, batch     1 | loss: 12.2636266Losses:  6.600530624389648 0.7075693607330322 1.0 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.6005306Losses:  10.290954675525427 0.4589552879333496 1.0 4.204690542072058
CurrentTrain: epoch  4, batch     3 | loss: 10.2909547Losses:  15.652776472270489 0.7298523187637329 1.0175130367279053 8.583539716899395
CurrentTrain: epoch  4, batch     4 | loss: 15.6527765Losses:  8.076201826334 0.4489384889602661 1.0335572957992554 1.408330351114273
CurrentTrain: epoch  4, batch     5 | loss: 8.0762018Losses:  8.18589449673891 0.6634944677352905 1.0 1.4831790998578072
CurrentTrain: epoch  4, batch     6 | loss: 8.1858945Losses:  6.133800506591797 0.49069392681121826 1.0 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 6.1338005Losses:  10.332814134657383 0.5626019239425659 1.0 2.9794692173600197
CurrentTrain: epoch  4, batch     8 | loss: 10.3328141Losses:  9.093009192496538 0.5722088813781738 1.0353871583938599 1.5385520048439503
CurrentTrain: epoch  4, batch     9 | loss: 9.0930092Losses:  11.477993037551641 0.6955417394638062 1.0 4.406186606734991
CurrentTrain: epoch  4, batch    10 | loss: 11.4779930Losses:  10.336503203958273 0.7082756757736206 1.0 2.9508611522614956
CurrentTrain: epoch  4, batch    11 | loss: 10.3365032Losses:  8.494309812784195 0.5667030811309814 1.0 1.4387811720371246
CurrentTrain: epoch  4, batch    12 | loss: 8.4943098Losses:  6.9368896484375 0.6965041160583496 1.0106837749481201 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 6.9368896Losses:  8.699909150600433 0.5122683048248291 1.0429353713989258 1.4330605864524841
CurrentTrain: epoch  4, batch    14 | loss: 8.6999092Losses:  8.582188934087753 0.6266353130340576 1.0017281770706177 1.4195683896541595
CurrentTrain: epoch  4, batch    15 | loss: 8.5821889Losses:  10.92423607595265 0.7864353656768799 1.00711190700531 3.2447865176945925
CurrentTrain: epoch  4, batch    16 | loss: 10.9242361Losses:  6.534473419189453 0.4715355634689331 1.0 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 6.5344734Losses:  11.180211581289768 0.46279847621917725 1.0 4.445531405508518
CurrentTrain: epoch  4, batch    18 | loss: 11.1802116Losses:  15.673016458749771 0.5716333389282227 1.0 8.711440473794937
CurrentTrain: epoch  4, batch    19 | loss: 15.6730165Losses:  10.246868193149567 0.5135335922241211 1.0832806825637817 2.890649378299713
CurrentTrain: epoch  4, batch    20 | loss: 10.2468682Losses:  10.04528247565031 0.7471579313278198 1.0653342008590698 2.8635717555880547
CurrentTrain: epoch  4, batch    21 | loss: 10.0452825Losses:  7.924599319696426 0.42421507835388184 1.0 1.4531284868717194
CurrentTrain: epoch  4, batch    22 | loss: 7.9245993Losses:  13.611510463058949 0.5688686370849609 0.9843035936355591 7.033663935959339
CurrentTrain: epoch  4, batch    23 | loss: 13.6115105Losses:  8.03020754083991 0.5260523557662964 1.0511435270309448 1.462809469550848
CurrentTrain: epoch  4, batch    24 | loss: 8.0302075Losses:  7.589760780334473 0.5942095518112183 1.0378226041793823 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 7.5897608Losses:  7.823324620723724 0.5851427316665649 1.0 1.437558114528656
CurrentTrain: epoch  4, batch    26 | loss: 7.8233246Losses:  10.091401066631079 0.7210708856582642 1.068937063217163 2.2389716766774654
CurrentTrain: epoch  4, batch    27 | loss: 10.0914011Losses:  8.422573540359735 0.5302228927612305 1.0 1.40676686540246
CurrentTrain: epoch  4, batch    28 | loss: 8.4225735Losses:  13.010805495083332 0.6312602758407593 1.010524868965149 6.012068159878254
CurrentTrain: epoch  4, batch    29 | loss: 13.0108055Losses:  12.441761277616024 0.5990856885910034 1.0 5.881338857114315
CurrentTrain: epoch  4, batch    30 | loss: 12.4417613Losses:  6.831514358520508 0.665805459022522 1.0 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 6.8315144Losses:  9.02298429608345 0.375529408454895 1.0 2.887748032808304
CurrentTrain: epoch  4, batch    32 | loss: 9.0229843Losses:  11.404254171997309 0.4690014123916626 1.0 4.981507990509272
CurrentTrain: epoch  4, batch    33 | loss: 11.4042542Losses:  6.34515905380249 0.49685919284820557 1.0 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 6.3451591Losses:  8.214152187108994 0.4427133798599243 0.9891380071640015 1.4086798131465912
CurrentTrain: epoch  4, batch    35 | loss: 8.2141522Losses:  11.695355668663979 0.6122889518737793 1.0 4.849768415093422
CurrentTrain: epoch  4, batch    36 | loss: 11.6953557Losses:  10.669807113707066 0.5522918701171875 1.0245213508605957 3.464263118803501
CurrentTrain: epoch  4, batch    37 | loss: 10.6698071Losses:  6.468983173370361 0.5768016576766968 1.0 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 6.4689832Losses:  10.879982523620129 0.44318485260009766 1.0 4.393127970397472
CurrentTrain: epoch  4, batch    39 | loss: 10.8799825Losses:  11.343587759882212 0.54203200340271 1.0 4.499304655939341
CurrentTrain: epoch  4, batch    40 | loss: 11.3435878Losses:  6.609918594360352 0.6827793121337891 1.0 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 6.6099186Losses:  12.574970062822104 0.3462296724319458 1.0 6.11518793925643
CurrentTrain: epoch  4, batch    42 | loss: 12.5749701Losses:  9.454387459903955 0.7252352237701416 1.0 2.8833406306803226
CurrentTrain: epoch  4, batch    43 | loss: 9.4543875Losses:  6.723787307739258 0.4738982915878296 1.0 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 6.7237873Losses:  8.127652402967215 0.45905137062072754 1.0317847728729248 1.5087196789681911
CurrentTrain: epoch  4, batch    45 | loss: 8.1276524Losses:  10.46974367275834 0.6974911689758301 1.0150012969970703 2.954710427671671
CurrentTrain: epoch  4, batch    46 | loss: 10.4697437Losses:  8.03170907497406 0.4313161373138428 1.0 1.6912187337875366
CurrentTrain: epoch  4, batch    47 | loss: 8.0317091Losses:  12.889189548790455 0.6117303371429443 0.9914917945861816 5.7931874468922615
CurrentTrain: epoch  4, batch    48 | loss: 12.8891895Losses:  6.563195705413818 0.6421151161193848 1.0 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 6.5631957Losses:  9.401811055839062 0.4852113723754883 1.0 2.994169645011425
CurrentTrain: epoch  4, batch    50 | loss: 9.4018111Losses:  6.314465045928955 0.5782769918441772 1.0 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 6.3144650Losses:  10.544649537652731 0.5262497663497925 1.0 4.286074575036764
CurrentTrain: epoch  4, batch    52 | loss: 10.5446495Losses:  10.094391226768494 0.72429358959198 1.051634430885315 1.4980272054672241
CurrentTrain: epoch  4, batch    53 | loss: 10.0943912Losses:  7.371621072292328 0.4692937135696411 1.0 1.4002341628074646
CurrentTrain: epoch  4, batch    54 | loss: 7.3716211Losses:  7.191684722900391 0.5702688694000244 1.0293320417404175 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 7.1916847Losses:  17.527028378099203 0.43986809253692627 1.0 10.979256447404623
CurrentTrain: epoch  4, batch    56 | loss: 17.5270284Losses:  11.06240464746952 0.6548205614089966 1.045784592628479 4.389252200722694
CurrentTrain: epoch  4, batch    57 | loss: 11.0624046Losses:  9.98339918255806 0.4992004632949829 1.017892837524414 3.0167544186115265
CurrentTrain: epoch  4, batch    58 | loss: 9.9833992Losses:  9.39640323445201 0.44311976432800293 1.0 2.9606980495154858
CurrentTrain: epoch  4, batch    59 | loss: 9.3964032Losses:  8.010761141777039 0.5985845327377319 1.0 1.4123448133468628
CurrentTrain: epoch  4, batch    60 | loss: 8.0107611Losses:  7.924876868724823 0.32248592376708984 1.0 1.445375144481659
CurrentTrain: epoch  4, batch    61 | loss: 7.9248769Losses:  11.946704164147377 0.38044118881225586 1.0 5.962667241692543
CurrentTrain: epoch  4, batch    62 | loss: 11.9467042Losses:  6.7020134925842285 0.41921281814575195 1.024558186531067 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.7020135Losses:  9.2845678627491 0.4504890441894531 1.0435936450958252 2.87553408741951
CurrentTrain: epoch  5, batch     1 | loss: 9.2845679Losses:  9.47829395532608 0.5645111799240112 1.0 2.8748775124549866
CurrentTrain: epoch  5, batch     2 | loss: 9.4782940Losses:  9.66018071398139 0.46190524101257324 1.0 2.9559771083295345
CurrentTrain: epoch  5, batch     3 | loss: 9.6601807Losses:  10.594233095645905 0.43973982334136963 1.0 4.26104074716568
CurrentTrain: epoch  5, batch     4 | loss: 10.5942331Losses:  12.253243405371904 0.6634950637817383 1.0 5.855051476508379
CurrentTrain: epoch  5, batch     5 | loss: 12.2532434Losses:  6.7064008712768555 0.47071373462677 1.0 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 6.7064009Losses:  7.7174845933914185 0.30720996856689453 1.0 1.4091349840164185
CurrentTrain: epoch  5, batch     7 | loss: 7.7174846Losses:  11.470777951180935 0.6628164052963257 1.041549801826477 4.3740119561553
CurrentTrain: epoch  5, batch     8 | loss: 11.4707780Losses:  12.649049490690231 0.41488468647003174 1.1052312850952148 6.079659193754196
CurrentTrain: epoch  5, batch     9 | loss: 12.6490495Losses:  9.670379608869553 0.47943997383117676 1.0049951076507568 2.803507775068283
CurrentTrain: epoch  5, batch    10 | loss: 9.6703796Losses:  29.00545635819435 0.48449814319610596 1.0361855030059814 22.51688614487648
CurrentTrain: epoch  5, batch    11 | loss: 29.0054564Losses:  9.262721240520477 0.4616464376449585 1.0 2.815229117870331
CurrentTrain: epoch  5, batch    12 | loss: 9.2627212Losses:  7.7586773335933685 0.5252115726470947 1.0627784729003906 1.3971575200557709
CurrentTrain: epoch  5, batch    13 | loss: 7.7586773Losses:  8.106055498123169 0.3981287479400635 1.0 1.6776926517486572
CurrentTrain: epoch  5, batch    14 | loss: 8.1060555Losses:  8.225862681865692 0.504644513130188 1.0 1.4407750964164734
CurrentTrain: epoch  5, batch    15 | loss: 8.2258627Losses:  7.8345871567726135 0.5384746789932251 1.0035849809646606 1.417030394077301
CurrentTrain: epoch  5, batch    16 | loss: 7.8345872Losses:  10.00807048752904 0.5377026796340942 1.0441129207611084 2.8836369700729847
CurrentTrain: epoch  5, batch    17 | loss: 10.0080705Losses:  10.582326006144285 0.3557165861129761 1.042775273323059 4.5160189382731915
CurrentTrain: epoch  5, batch    18 | loss: 10.5823260Losses:  7.654174327850342 0.4876137971878052 1.0240929126739502 1.421919345855713
CurrentTrain: epoch  5, batch    19 | loss: 7.6541743Losses:  8.63054445385933 0.3327676057815552 1.0 2.7985847294330597
CurrentTrain: epoch  5, batch    20 | loss: 8.6305445Losses:  11.699236460030079 0.4485490322113037 1.0 5.850605554878712
CurrentTrain: epoch  5, batch    21 | loss: 11.6992365Losses:  8.943782612681389 0.45528995990753174 1.0 3.0197871178388596
CurrentTrain: epoch  5, batch    22 | loss: 8.9437826Losses:  10.543577194213867 0.43939316272735596 1.0 3.8818273544311523
CurrentTrain: epoch  5, batch    23 | loss: 10.5435772Losses:  8.874591741710901 0.48184990882873535 1.0 2.9413909055292606
CurrentTrain: epoch  5, batch    24 | loss: 8.8745917Losses:  23.367302805185318 0.6377686262130737 1.0188066959381104 16.10313406586647
CurrentTrain: epoch  5, batch    25 | loss: 23.3673028Losses:  9.077742092311382 0.49493205547332764 1.0 2.9199805185198784
CurrentTrain: epoch  5, batch    26 | loss: 9.0777421Losses:  8.048707753419876 0.47869837284088135 1.0 1.4161751568317413
CurrentTrain: epoch  5, batch    27 | loss: 8.0487078Losses:  7.193849202245474 0.21873760223388672 1.0 1.5150066576898098
CurrentTrain: epoch  5, batch    28 | loss: 7.1938492Losses:  10.943757981061935 0.4668961763381958 1.0 4.376551598310471
CurrentTrain: epoch  5, batch    29 | loss: 10.9437580Losses:  8.555776784196496 0.44061267375946045 1.0079436302185059 1.5656744930893183
CurrentTrain: epoch  5, batch    30 | loss: 8.5557768Losses:  10.492433868348598 0.29938364028930664 1.0 4.2914121970534325
CurrentTrain: epoch  5, batch    31 | loss: 10.4924339Losses:  7.373186498880386 0.2628796100616455 1.0 1.4144119322299957
CurrentTrain: epoch  5, batch    32 | loss: 7.3731865Losses:  22.527433916926384 0.6409399509429932 1.101164698600769 15.27193121612072
CurrentTrain: epoch  5, batch    33 | loss: 22.5274339Losses:  9.458822194486856 0.5913122892379761 1.0 2.859043065458536
CurrentTrain: epoch  5, batch    34 | loss: 9.4588222Losses:  10.439043141901493 0.47376716136932373 1.0 4.258824922144413
CurrentTrain: epoch  5, batch    35 | loss: 10.4390431Losses:  13.212832134217024 0.49255526065826416 1.0 6.60084168240428
CurrentTrain: epoch  5, batch    36 | loss: 13.2128321Losses:  7.770988084375858 0.5942457914352417 1.0431294441223145 1.4568577781319618
CurrentTrain: epoch  5, batch    37 | loss: 7.7709881Losses:  7.731977496296167 0.4062420129776001 1.0365220308303833 1.4627356864511967
CurrentTrain: epoch  5, batch    38 | loss: 7.7319775Losses:  7.6411444544792175 0.4947237968444824 1.0032100677490234 1.408876121044159
CurrentTrain: epoch  5, batch    39 | loss: 7.6411445Losses:  7.714248716831207 0.5872198343276978 1.0 1.4154148697853088
CurrentTrain: epoch  5, batch    40 | loss: 7.7142487Losses:  6.738007545471191 0.4687366485595703 1.0 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 6.7380075Losses:  8.337172903120518 0.49499571323394775 1.0 1.5364765301346779
CurrentTrain: epoch  5, batch    42 | loss: 8.3371729Losses:  11.027750313282013 0.5544638633728027 1.0467082262039185 4.281982719898224
CurrentTrain: epoch  5, batch    43 | loss: 11.0277503Losses:  6.538168430328369 0.5386326313018799 1.0459944009780884 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 6.5381684Losses:  5.810340881347656 0.28402721881866455 1.0 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 5.8103409Losses:  8.975224521011114 0.4356786012649536 1.0 3.029250171035528
CurrentTrain: epoch  5, batch    46 | loss: 8.9752245Losses:  8.676273345947266 0.37633955478668213 0.9877040386199951 1.4092216491699219
CurrentTrain: epoch  5, batch    47 | loss: 8.6762733Losses:  10.103499460965395 0.6023252010345459 1.0866776704788208 2.8726635463535786
CurrentTrain: epoch  5, batch    48 | loss: 10.1034995Losses:  7.967202961444855 0.39732515811920166 0.9957473278045654 1.3957880139350891
CurrentTrain: epoch  5, batch    49 | loss: 7.9672030Losses:  8.017982065677643 0.47435879707336426 1.0 1.4034247994422913
CurrentTrain: epoch  5, batch    50 | loss: 8.0179821Losses:  16.44988589733839 0.457855224609375 1.0283660888671875 10.289781145751476
CurrentTrain: epoch  5, batch    51 | loss: 16.4498859Losses:  10.795256778597832 0.47429800033569336 1.0445568561553955 4.598889514803886
CurrentTrain: epoch  5, batch    52 | loss: 10.7952568Losses:  14.740609414875507 0.4091736078262329 1.0 8.638911969959736
CurrentTrain: epoch  5, batch    53 | loss: 14.7406094Losses:  8.227671600878239 0.41564464569091797 1.0407453775405884 1.7731556668877602
CurrentTrain: epoch  5, batch    54 | loss: 8.2276716Losses:  8.877503216266632 0.42756569385528564 1.0 2.816958725452423
CurrentTrain: epoch  5, batch    55 | loss: 8.8775032Losses:  9.633151981979609 0.5541188716888428 0.9787344932556152 2.8329062201082706
CurrentTrain: epoch  5, batch    56 | loss: 9.6331520Losses:  7.217678755521774 0.35567259788513184 1.0 1.4425866305828094
CurrentTrain: epoch  5, batch    57 | loss: 7.2176788Losses:  11.181971848011017 0.4242140054702759 1.0 4.7466400265693665
CurrentTrain: epoch  5, batch    58 | loss: 11.1819718Losses:  12.1750378459692 0.4444371461868286 1.02934730052948 5.022776111960411
CurrentTrain: epoch  5, batch    59 | loss: 12.1750378Losses:  7.88006928563118 0.5228047370910645 1.0 1.504426509141922
CurrentTrain: epoch  5, batch    60 | loss: 7.8800693Losses:  7.520301625132561 0.3074948787689209 1.0 1.4969661682844162
CurrentTrain: epoch  5, batch    61 | loss: 7.5203016Losses:  10.47383514419198 0.35521507263183594 1.0 4.292721901088953
CurrentTrain: epoch  5, batch    62 | loss: 10.4738351Losses:  14.06237031519413 0.4506411552429199 1.0 7.653535857796669
CurrentTrain: epoch  6, batch     0 | loss: 14.0623703Losses:  8.874632835388184 0.338104248046875 1.0 2.812976360321045
CurrentTrain: epoch  6, batch     1 | loss: 8.8746328Losses:  7.811933007091284 0.4047166109085083 1.0 1.528631653636694
CurrentTrain: epoch  6, batch     2 | loss: 7.8119330Losses:  7.788748320192099 0.3692086935043335 1.0 1.4771957956254482
CurrentTrain: epoch  6, batch     3 | loss: 7.7887483Losses:  7.178065836429596 0.276340126991272 1.0 1.4026327729225159
CurrentTrain: epoch  6, batch     4 | loss: 7.1780658Losses:  8.021424353122711 0.3163151741027832 0.9922308921813965 1.541241705417633
CurrentTrain: epoch  6, batch     5 | loss: 8.0214244Losses:  6.117572784423828 0.3979470729827881 1.0 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 6.1175728Losses:  7.520599961280823 0.3778914213180542 1.0311521291732788 1.4955636262893677
CurrentTrain: epoch  6, batch     7 | loss: 7.5206000Losses:  12.669407226145267 0.542709469795227 1.0 6.115943767130375
CurrentTrain: epoch  6, batch     8 | loss: 12.6694072Losses:  8.83070619404316 0.3242323398590088 1.0 2.9696059972047806
CurrentTrain: epoch  6, batch     9 | loss: 8.8307062Losses:  10.670746341347694 0.6228559017181396 1.1213836669921875 3.080866351723671
CurrentTrain: epoch  6, batch    10 | loss: 10.6707463Losses:  10.654805336147547 0.6342763900756836 1.0427273511886597 3.4785420037806034
CurrentTrain: epoch  6, batch    11 | loss: 10.6548053Losses:  8.1217842400074 0.34809863567352295 1.0 1.4495773613452911
CurrentTrain: epoch  6, batch    12 | loss: 8.1217842Losses:  7.808428019285202 0.4139772653579712 1.0 1.407762736082077
CurrentTrain: epoch  6, batch    13 | loss: 7.8084280Losses:  7.322565674781799 0.3257502317428589 1.0 1.4283372163772583
CurrentTrain: epoch  6, batch    14 | loss: 7.3225657Losses:  9.627836734056473 0.37058722972869873 1.0 2.872892886400223
CurrentTrain: epoch  6, batch    15 | loss: 9.6278367Losses:  6.081624984741211 0.3679828643798828 1.0 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 6.0816250Losses:  9.042186558246613 0.36532342433929443 1.0 2.838343918323517
CurrentTrain: epoch  6, batch    17 | loss: 9.0421866Losses:  7.7507911287248135 0.3506131172180176 1.0 1.5773344598710537
CurrentTrain: epoch  6, batch    18 | loss: 7.7507911Losses:  6.043327808380127 0.39035189151763916 1.0 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 6.0433278Losses:  11.374751642346382 0.4459412097930908 1.0143218040466309 4.601240232586861
CurrentTrain: epoch  6, batch    20 | loss: 11.3747516Losses:  8.137161705642939 0.3850964307785034 1.0 1.7819599844515324
CurrentTrain: epoch  6, batch    21 | loss: 8.1371617Losses:  8.657373640686274 0.31836116313934326 1.0 2.841215346008539
CurrentTrain: epoch  6, batch    22 | loss: 8.6573736Losses:  7.9015435837209225 0.3409748077392578 1.0 1.4867629669606686
CurrentTrain: epoch  6, batch    23 | loss: 7.9015436Losses:  10.447590246796608 0.2855421304702759 1.0 4.3603252321481705
CurrentTrain: epoch  6, batch    24 | loss: 10.4475902Losses:  9.279225029051304 0.39746975898742676 1.0 2.901025451719761
CurrentTrain: epoch  6, batch    25 | loss: 9.2792250Losses:  7.745253413915634 0.44260311126708984 1.0 1.415259212255478
CurrentTrain: epoch  6, batch    26 | loss: 7.7452534Losses:  7.496142536401749 0.29571378231048584 1.0 1.5042006075382233
CurrentTrain: epoch  6, batch    27 | loss: 7.4961425Losses:  11.618706554174423 0.23072779178619385 1.0 5.7872041165828705
CurrentTrain: epoch  6, batch    28 | loss: 11.6187066Losses:  11.985262159258127 0.4109593629837036 1.0 5.733806852251291
CurrentTrain: epoch  6, batch    29 | loss: 11.9852622Losses:  10.266204684972763 0.4195441007614136 1.0 4.230922073125839
CurrentTrain: epoch  6, batch    30 | loss: 10.2662047Losses:  7.584708496928215 0.34758639335632324 1.0 1.487678810954094
CurrentTrain: epoch  6, batch    31 | loss: 7.5847085Losses:  9.30834523588419 0.5937449932098389 1.0 2.9559864178299904
CurrentTrain: epoch  6, batch    32 | loss: 9.3083452Losses:  7.1615777015686035 0.2331019639968872 1.0443177223205566 1.400345802307129
CurrentTrain: epoch  6, batch    33 | loss: 7.1615777Losses:  6.126199722290039 0.46215105056762695 1.0 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 6.1261997Losses:  9.709716118872166 0.2506105899810791 1.0 3.403826989233494
CurrentTrain: epoch  6, batch    35 | loss: 9.7097161Losses:  9.285788394510746 0.3543328046798706 1.065395474433899 2.9356101527810097
CurrentTrain: epoch  6, batch    36 | loss: 9.2857884Losses:  6.185103893280029 0.3316408395767212 1.0 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.1851039Losses:  13.486182861030102 0.40142738819122314 1.0 7.604472808539867
CurrentTrain: epoch  6, batch    38 | loss: 13.4861829Losses:  5.864850997924805 0.23647785186767578 1.0 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 5.8648510Losses:  5.996498107910156 0.3523608446121216 1.0 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 5.9964981Losses:  7.211142390966415 0.3058873414993286 1.0 1.4194444119930267
CurrentTrain: epoch  6, batch    41 | loss: 7.2111424Losses:  9.222294189035892 0.2581603527069092 1.0486125946044922 2.836120940744877
CurrentTrain: epoch  6, batch    42 | loss: 9.2222942Losses:  9.466823726892471 0.4774538278579712 1.0 3.2416235506534576
CurrentTrain: epoch  6, batch    43 | loss: 9.4668237Losses:  7.232814289629459 0.2805147171020508 1.0 1.506743885576725
CurrentTrain: epoch  6, batch    44 | loss: 7.2328143Losses:  7.387139055877924 0.4228442907333374 1.0 1.4459764696657658
CurrentTrain: epoch  6, batch    45 | loss: 7.3871391Losses:  9.56273877620697 0.48752474784851074 0.9923964738845825 2.4685500860214233
CurrentTrain: epoch  6, batch    46 | loss: 9.5627388Losses:  8.851117506623268 0.3062732219696045 1.0 2.9850438982248306
CurrentTrain: epoch  6, batch    47 | loss: 8.8511175Losses:  7.304611790925264 0.2643481492996216 1.0469988584518433 1.5385380871593952
CurrentTrain: epoch  6, batch    48 | loss: 7.3046118Losses:  7.60348754376173 0.4872027635574341 1.0 1.4366722628474236
CurrentTrain: epoch  6, batch    49 | loss: 7.6034875Losses:  7.403496116399765 0.38701045513153076 1.048231601715088 1.3990601003170013
CurrentTrain: epoch  6, batch    50 | loss: 7.4034961Losses:  5.833420276641846 0.36648309230804443 1.0339957475662231 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 5.8334203Losses:  6.041244983673096 0.40660130977630615 1.0 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 6.0412450Losses:  9.926245294511318 0.2817758321762085 1.0 4.268961034715176
CurrentTrain: epoch  6, batch    53 | loss: 9.9262453Losses:  5.5957560539245605 0.24577605724334717 1.0 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 5.5957561Losses:  10.242557652294636 0.31878089904785156 1.0 4.475937969982624
CurrentTrain: epoch  6, batch    55 | loss: 10.2425577Losses:  7.223138391971588 0.25218045711517334 1.0 1.4857025742530823
CurrentTrain: epoch  6, batch    56 | loss: 7.2231384Losses:  8.594235718250275 0.20933866500854492 1.0 2.8542683720588684
CurrentTrain: epoch  6, batch    57 | loss: 8.5942357Losses:  7.405695848166943 0.3735443353652954 1.0 1.449736051261425
CurrentTrain: epoch  6, batch    58 | loss: 7.4056958Losses:  7.500997394323349 0.34757447242736816 1.0 1.4507664144039154
CurrentTrain: epoch  6, batch    59 | loss: 7.5009974Losses:  5.582694053649902 0.2643444538116455 1.0 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 5.5826941Losses:  7.074613988399506 0.40799617767333984 1.0 1.422855794429779
CurrentTrain: epoch  6, batch    61 | loss: 7.0746140Losses:  9.782727807760239 0.07757043838500977 1.0 4.229347318410873
CurrentTrain: epoch  6, batch    62 | loss: 9.7827278Losses:  7.457468271255493 0.46493446826934814 1.060826301574707 1.4131953716278076
CurrentTrain: epoch  7, batch     0 | loss: 7.4574683Losses:  8.648753553628922 0.29451310634613037 1.0554499626159668 2.8175043165683746
CurrentTrain: epoch  7, batch     1 | loss: 8.6487536Losses:  6.030600547790527 0.39257514476776123 1.0953689813613892 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 6.0306005Losses:  13.422606706619263 0.2532527446746826 1.0507076978683472 7.667118310928345
CurrentTrain: epoch  7, batch     3 | loss: 13.4226067Losses:  13.009328722953796 0.12334811687469482 1.0 7.615960955619812
CurrentTrain: epoch  7, batch     4 | loss: 13.0093287Losses:  8.509577985852957 0.317238450050354 1.0 2.8393475972115993
CurrentTrain: epoch  7, batch     5 | loss: 8.5095780Losses:  5.774496078491211 0.21694862842559814 1.0 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 5.7744961Losses:  11.570991206914186 0.30245375633239746 1.0 5.8741318471729755
CurrentTrain: epoch  7, batch     7 | loss: 11.5709912Losses:  7.079708427190781 0.2813187837600708 1.0 1.3951042592525482
CurrentTrain: epoch  7, batch     8 | loss: 7.0797084Losses:  7.027696218341589 0.14304065704345703 1.0 1.4726091288030148
CurrentTrain: epoch  7, batch     9 | loss: 7.0276962Losses:  10.790997199714184 0.3642444610595703 1.0 4.851401977241039
CurrentTrain: epoch  7, batch    10 | loss: 10.7909972Losses:  5.936591625213623 0.3085836172103882 1.0 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 5.9365916Losses:  9.387650966644287 0.19199848175048828 0.9919952154159546 2.839935302734375
CurrentTrain: epoch  7, batch    12 | loss: 9.3876510Losses:  6.877422004938126 0.23873615264892578 1.0 1.398367553949356
CurrentTrain: epoch  7, batch    13 | loss: 6.8774220Losses:  8.925615340471268 0.17025268077850342 1.0 3.389116793870926
CurrentTrain: epoch  7, batch    14 | loss: 8.9256153Losses:  8.548464093357325 0.3174666166305542 1.0 2.828285489231348
CurrentTrain: epoch  7, batch    15 | loss: 8.5484641Losses:  13.045210003852844 0.30929577350616455 1.0 7.368795037269592
CurrentTrain: epoch  7, batch    16 | loss: 13.0452100Losses:  7.047476083040237 0.2857217788696289 1.0 1.3989594280719757
CurrentTrain: epoch  7, batch    17 | loss: 7.0474761Losses:  5.842316627502441 0.36472785472869873 1.0 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 5.8423166Losses:  11.181451506912708 0.09901976585388184 1.0 5.771785445511341
CurrentTrain: epoch  7, batch    19 | loss: 11.1814515Losses:  7.110772132873535 0.3428504467010498 1.0 1.4083433151245117
CurrentTrain: epoch  7, batch    20 | loss: 7.1107721Losses:  9.90796672180295 0.23594272136688232 1.0 4.334623921662569
CurrentTrain: epoch  7, batch    21 | loss: 9.9079667Losses:  8.312732696533203 0.1886441707611084 1.0 2.8185462951660156
CurrentTrain: epoch  7, batch    22 | loss: 8.3127327Losses:  6.062329292297363 0.5527325868606567 1.0 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 6.0623293Losses:  5.648204326629639 0.21614611148834229 1.0 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 5.6482043Losses:  7.69286111369729 0.4532301425933838 1.0 1.4491858817636967
CurrentTrain: epoch  7, batch    25 | loss: 7.6928611Losses:  8.641172468662262 0.29406630992889404 1.0 2.827353060245514
CurrentTrain: epoch  7, batch    26 | loss: 8.6411725Losses:  6.90725764632225 0.15309357643127441 1.0 1.4029527604579926
CurrentTrain: epoch  7, batch    27 | loss: 6.9072576Losses:  8.160978764295578 0.13915526866912842 1.0 2.8080248534679413
CurrentTrain: epoch  7, batch    28 | loss: 8.1609788Losses:  6.851784110069275 0.16980981826782227 1.0 1.3950575590133667
CurrentTrain: epoch  7, batch    29 | loss: 6.8517841Losses:  5.38875675201416 0.1646578311920166 1.0 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 5.3887568Losses:  12.724042303860188 0.3611011505126953 1.0 7.173769362270832
CurrentTrain: epoch  7, batch    31 | loss: 12.7240423Losses:  8.34896570444107 0.3082106113433838 1.0 2.83461195230484
CurrentTrain: epoch  7, batch    32 | loss: 8.3489657Losses:  13.401665546000004 0.2415938377380371 1.0 7.8724101558327675
CurrentTrain: epoch  7, batch    33 | loss: 13.4016655Losses:  10.0081577450037 0.4871026277542114 1.0 4.20953942835331
CurrentTrain: epoch  7, batch    34 | loss: 10.0081577Losses:  5.587777137756348 0.29376280307769775 1.0 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 5.5877771Losses:  10.74243301153183 0.36861956119537354 1.0 4.366927087306976
CurrentTrain: epoch  7, batch    36 | loss: 10.7424330Losses:  9.01371537335217 0.24340546131134033 1.0 3.220056163147092
CurrentTrain: epoch  7, batch    37 | loss: 9.0137154Losses:  8.512613445520401 0.42599189281463623 1.0 2.799039512872696
CurrentTrain: epoch  7, batch    38 | loss: 8.5126134Losses:  5.750268459320068 0.22814548015594482 1.0 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 5.7502685Losses:  8.39358201622963 0.2896796464920044 1.0 2.811225563287735
CurrentTrain: epoch  7, batch    40 | loss: 8.3935820Losses:  8.606505781412125 0.4015394449234009 1.0 2.807622343301773
CurrentTrain: epoch  7, batch    41 | loss: 8.6065058Losses:  5.597078800201416 0.31873559951782227 1.0 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 5.5970788Losses:  7.256198972463608 0.22505605220794678 1.0565110445022583 1.411310762166977
CurrentTrain: epoch  7, batch    43 | loss: 7.2561990Losses:  10.338274903595448 0.22967588901519775 1.0 4.762033887207508
CurrentTrain: epoch  7, batch    44 | loss: 10.3382749Losses:  8.343284636735916 0.23684024810791016 1.0 2.8301210701465607
CurrentTrain: epoch  7, batch    45 | loss: 8.3432846Losses:  11.598027229309082 0.4344482421875 1.0 5.621152400970459
CurrentTrain: epoch  7, batch    46 | loss: 11.5980272Losses:  6.9546394646167755 0.26305854320526123 1.0 1.3975944817066193
CurrentTrain: epoch  7, batch    47 | loss: 6.9546395Losses:  7.008590370416641 0.2190457582473755 1.0 1.401320606470108
CurrentTrain: epoch  7, batch    48 | loss: 7.0085904Losses:  7.198900401592255 0.3187063932418823 1.0 1.3935415148735046
CurrentTrain: epoch  7, batch    49 | loss: 7.1989004Losses:  6.33745002746582 0.2296961545944214 1.0487396717071533 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 6.3374500Losses:  7.170297048985958 0.25020861625671387 1.0 1.455295942723751
CurrentTrain: epoch  7, batch    51 | loss: 7.1702970Losses:  9.73421511799097 0.22002816200256348 1.0 4.298472739756107
CurrentTrain: epoch  7, batch    52 | loss: 9.7342151Losses:  6.893076453357935 0.26833784580230713 1.0 1.4176636077463627
CurrentTrain: epoch  7, batch    53 | loss: 6.8930765Losses:  9.821961283683777 0.33012306690216064 1.0 4.248462557792664
CurrentTrain: epoch  7, batch    54 | loss: 9.8219613Losses:  5.49137020111084 0.18118071556091309 1.0 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 5.4913702Losses:  5.348920822143555 0.14048278331756592 1.0 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 5.3489208Losses:  5.802553176879883 0.23936057090759277 1.0 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 5.8025532Losses:  11.060482699424028 0.09111642837524414 1.0 5.750169474631548
CurrentTrain: epoch  7, batch    58 | loss: 11.0604827Losses:  5.5960469245910645 0.33810555934906006 1.0 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 5.5960469Losses:  6.886917859315872 0.2060948610305786 1.0 1.442623883485794
CurrentTrain: epoch  7, batch    60 | loss: 6.8869179Losses:  6.95543522387743 0.2818028926849365 1.0 1.433899350464344
CurrentTrain: epoch  7, batch    61 | loss: 6.9554352Losses:  5.69658088684082 0.4453563690185547 1.0 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 5.6965809Losses:  7.209542728960514 0.24659991264343262 1.0 1.4926438108086586
CurrentTrain: epoch  8, batch     0 | loss: 7.2095427Losses:  8.545858763158321 0.218481183052063 1.0 2.95350494235754
CurrentTrain: epoch  8, batch     1 | loss: 8.5458588Losses:  8.188695907592773 0.14760398864746094 1.0 2.8287339210510254
CurrentTrain: epoch  8, batch     2 | loss: 8.1886959Losses:  6.7654361091554165 0.13397693634033203 1.0 1.4218997322022915
CurrentTrain: epoch  8, batch     3 | loss: 6.7654361Losses:  6.987682104110718 0.1768122911453247 1.0 1.4145448207855225
CurrentTrain: epoch  8, batch     4 | loss: 6.9876821Losses:  5.462671756744385 0.25442028045654297 1.0 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 5.4626718Losses:  5.516388416290283 0.27383196353912354 1.0 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 5.5163884Losses:  8.48649537563324 0.4201256036758423 1.0 2.821782946586609
CurrentTrain: epoch  8, batch     7 | loss: 8.4864954Losses:  8.531820304691792 0.31316280364990234 1.0455485582351685 2.892668731510639
CurrentTrain: epoch  8, batch     8 | loss: 8.5318203Losses:  9.769286148250103 0.43836843967437744 1.0 4.2165975496172905
CurrentTrain: epoch  8, batch     9 | loss: 9.7692861Losses:  5.304060935974121 0.09174513816833496 1.0 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 5.3040609Losses:  6.771654397249222 0.14950799942016602 1.0 1.475354939699173
CurrentTrain: epoch  8, batch    11 | loss: 6.7716544Losses:  7.012766174972057 0.12133431434631348 1.0 1.493410401046276
CurrentTrain: epoch  8, batch    12 | loss: 7.0127662Losses:  6.814303159713745 0.16405224800109863 1.0 1.466764211654663
CurrentTrain: epoch  8, batch    13 | loss: 6.8143032Losses:  6.867098420858383 0.24479711055755615 1.0 1.4068571031093597
CurrentTrain: epoch  8, batch    14 | loss: 6.8670984Losses:  6.8367869555950165 0.15617609024047852 1.0 1.390621393918991
CurrentTrain: epoch  8, batch    15 | loss: 6.8367870Losses:  10.793250497430563 0.1367630958557129 0.9826130867004395 4.324437078088522
CurrentTrain: epoch  8, batch    16 | loss: 10.7932505Losses:  5.4312872886657715 0.20590901374816895 1.0383275747299194 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 5.4312873Losses:  6.806815147399902 0.13597416877746582 1.0 1.446852684020996
CurrentTrain: epoch  8, batch    18 | loss: 6.8068151Losses:  9.880578577518463 0.31720101833343506 1.0 4.324650824069977
CurrentTrain: epoch  8, batch    19 | loss: 9.8805786Losses:  5.565008163452148 0.32272613048553467 1.0 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 5.5650082Losses:  5.532074928283691 0.26818346977233887 1.0 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 5.5320749Losses:  11.511281475424767 0.2406766414642334 1.0 5.911014065146446
CurrentTrain: epoch  8, batch    22 | loss: 11.5112815Losses:  15.441700905561447 0.25635969638824463 1.0 9.935392826795578
CurrentTrain: epoch  8, batch    23 | loss: 15.4417009Losses:  5.542164325714111 0.3538227081298828 1.0 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 5.5421643Losses:  17.959512144327164 0.015321850776672363 1.0 12.853545099496841
CurrentTrain: epoch  8, batch    25 | loss: 17.9595121Losses:  6.979152858257294 0.34185922145843506 1.0 1.4149815440177917
CurrentTrain: epoch  8, batch    26 | loss: 6.9791529Losses:  16.08611636608839 0.5285296440124512 1.0 10.036406569182873
CurrentTrain: epoch  8, batch    27 | loss: 16.0861164Losses:  5.4280195236206055 0.25026369094848633 1.0 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 5.4280195Losses:  9.888720244169235 0.38756024837493896 1.0 4.213549345731735
CurrentTrain: epoch  8, batch    29 | loss: 9.8887202Losses:  6.7052284479141235 0.06809675693511963 1.0 1.4480534791946411
CurrentTrain: epoch  8, batch    30 | loss: 6.7052284Losses:  12.067999817430973 0.26646506786346436 1.0 6.58116290718317
CurrentTrain: epoch  8, batch    31 | loss: 12.0679998Losses:  8.383223712444305 0.3883894681930542 1.0 2.799235999584198
CurrentTrain: epoch  8, batch    32 | loss: 8.3832237Losses:  5.39788818359375 0.19699275493621826 1.0 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 5.3978882Losses:  7.686479210853577 0.24397575855255127 1.0468997955322266 1.416303277015686
CurrentTrain: epoch  8, batch    34 | loss: 7.6864792Losses:  6.871195696294308 0.19996154308319092 1.0 1.4742783531546593
CurrentTrain: epoch  8, batch    35 | loss: 6.8711957Losses:  6.833847016096115 0.20893454551696777 1.0 1.402445763349533
CurrentTrain: epoch  8, batch    36 | loss: 6.8338470Losses:  5.325002193450928 0.15519845485687256 1.0 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 5.3250022Losses:  14.214400839060545 0.26132142543792725 1.0 8.740863870829344
CurrentTrain: epoch  8, batch    38 | loss: 14.2144008Losses:  9.5583236515522 0.2209179401397705 1.0 4.215045720338821
CurrentTrain: epoch  8, batch    39 | loss: 9.5583237Losses:  6.863747477531433 0.27407073974609375 1.0 1.398086428642273
CurrentTrain: epoch  8, batch    40 | loss: 6.8637475Losses:  6.714533269405365 0.07580375671386719 1.0 1.3938717246055603
CurrentTrain: epoch  8, batch    41 | loss: 6.7145333Losses:  8.358560804277658 0.266140341758728 1.0 2.8451659716665745
CurrentTrain: epoch  8, batch    42 | loss: 8.3585608Losses:  14.800257880240679 0.5678131580352783 1.0 9.056946951895952
CurrentTrain: epoch  8, batch    43 | loss: 14.8002579Losses:  6.797736942768097 0.20156943798065186 1.0 1.397839367389679
CurrentTrain: epoch  8, batch    44 | loss: 6.7977369Losses:  6.762137472629547 0.13214373588562012 1.0 1.3975706696510315
CurrentTrain: epoch  8, batch    45 | loss: 6.7621375Losses:  9.67269393056631 0.15517139434814453 1.0 4.336130820214748
CurrentTrain: epoch  8, batch    46 | loss: 9.6726939Losses:  6.777857236564159 0.22018182277679443 1.0 1.4264106079936028
CurrentTrain: epoch  8, batch    47 | loss: 6.7778572Losses:  5.61828088760376 0.13079583644866943 1.0 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 5.6182809Losses:  5.311755180358887 0.13872504234313965 1.0 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 5.3117552Losses:  8.491111222654581 0.45561885833740234 1.0 2.8029264844954014
CurrentTrain: epoch  8, batch    50 | loss: 8.4911112Losses:  6.818866580724716 0.21667742729187012 1.0 1.4066637456417084
CurrentTrain: epoch  8, batch    51 | loss: 6.8188666Losses:  6.882853150367737 0.2937697172164917 1.0 1.4020315408706665
CurrentTrain: epoch  8, batch    52 | loss: 6.8828532Losses:  5.334020137786865 0.20073175430297852 1.0 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 5.3340201Losses:  9.618604511022568 0.01943814754486084 1.0 4.450790256261826
CurrentTrain: epoch  8, batch    54 | loss: 9.6186045Losses:  6.630663096904755 0.0763401985168457 1.0 1.4143597483634949
CurrentTrain: epoch  8, batch    55 | loss: 6.6306631Losses:  9.604547716677189 0.13220715522766113 1.0 4.344184137880802
CurrentTrain: epoch  8, batch    56 | loss: 9.6045477Losses:  9.460603639483452 0.07363772392272949 1.0 4.263475343585014
CurrentTrain: epoch  8, batch    57 | loss: 9.4606036Losses:  6.865719974040985 0.15274131298065186 1.0 1.396803081035614
CurrentTrain: epoch  8, batch    58 | loss: 6.8657200Losses:  6.681109935045242 0.10247015953063965 1.0 1.4065385162830353
CurrentTrain: epoch  8, batch    59 | loss: 6.6811099Losses:  5.360214710235596 0.25 1.0 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 5.3602147Losses:  11.068940855562687 0.2865898609161377 1.0 5.61679432541132
CurrentTrain: epoch  8, batch    61 | loss: 11.0689409Losses:  7.110611978918314 0.5037391185760498 1.0 1.4454308189451694
CurrentTrain: epoch  8, batch    62 | loss: 7.1106120Losses:  8.554594315588474 0.2503054141998291 1.0 3.161814011633396
CurrentTrain: epoch  9, batch     0 | loss: 8.5545943Losses:  5.076481342315674 0.00862133502960205 1.0 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 5.0764813Losses:  5.3714919090271 0.19376790523529053 1.0 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 5.3714919Losses:  8.101257301867008 0.15200412273406982 1.0 2.8357944265007973
CurrentTrain: epoch  9, batch     3 | loss: 8.1012573Losses:  8.26703029870987 0.3125 1.0 2.787656843662262
CurrentTrain: epoch  9, batch     4 | loss: 8.2670303Losses:  9.56828024238348 0.0625 1.0 4.356204055249691
CurrentTrain: epoch  9, batch     5 | loss: 9.5682802Losses:  6.698965400457382 0.1875 1.0 1.39302858710289
CurrentTrain: epoch  9, batch     6 | loss: 6.6989654Losses:  8.13958552479744 0.1973477602005005 1.0 2.8073244392871857
CurrentTrain: epoch  9, batch     7 | loss: 8.1395855Losses:  8.345496267080307 0.4403390884399414 1.0 2.788118928670883
CurrentTrain: epoch  9, batch     8 | loss: 8.3454963Losses:  5.177928924560547 0.06864809989929199 1.0 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 5.1779289Losses:  8.371262513101101 0.375 1.0 2.8827271088957787
CurrentTrain: epoch  9, batch    10 | loss: 8.3712625Losses:  7.305207073688507 0.2124490737915039 1.0222458839416504 1.3966339230537415
CurrentTrain: epoch  9, batch    11 | loss: 7.3052071Losses:  8.344187200069427 0.3856024742126465 1.0 2.81746906042099
CurrentTrain: epoch  9, batch    12 | loss: 8.3441872Losses:  5.263134002685547 0.125 1.0 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 5.2631340Losses:  8.039894729852676 0.06967055797576904 1.0 2.8396879732608795
CurrentTrain: epoch  9, batch    14 | loss: 8.0398947Losses:  9.443318106234074 0.18998098373413086 1.0 4.185948111116886
CurrentTrain: epoch  9, batch    15 | loss: 9.4433181Losses:  8.272601697593927 0.21342873573303223 1.0 2.86166724935174
CurrentTrain: epoch  9, batch    16 | loss: 8.2726017Losses:  5.282662391662598 0.1875 1.0 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 5.2826624Losses:  6.77065571770072 0.20421528816223145 1.0 1.4622326754033566
CurrentTrain: epoch  9, batch    18 | loss: 6.7706557Losses:  12.352312970906496 0.19310665130615234 1.0 7.076633382588625
CurrentTrain: epoch  9, batch    19 | loss: 12.3523130Losses:  6.7608387768268585 0.25149214267730713 1.0 1.4161899387836456
CurrentTrain: epoch  9, batch    20 | loss: 6.7608388Losses:  7.079922467470169 0.3018765449523926 1.0079891681671143 1.400692731142044
CurrentTrain: epoch  9, batch    21 | loss: 7.0799225Losses:  5.356611251831055 0.2611438035964966 1.0 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 5.3566113Losses:  8.518979962915182 0.452250599861145 1.0 2.8866662345826626
CurrentTrain: epoch  9, batch    23 | loss: 8.5189800Losses:  5.753495216369629 0.13231348991394043 1.0439711809158325 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 5.7534952Losses:  9.746217750012875 0.1875 1.0 4.501252196729183
CurrentTrain: epoch  9, batch    25 | loss: 9.7462178Losses:  5.382820129394531 0.07899022102355957 1.0 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 5.3828201Losses:  8.12379202246666 0.19702303409576416 1.0 2.8581198155879974
CurrentTrain: epoch  9, batch    27 | loss: 8.1237920Losses:  8.12238010764122 0.2567211389541626 1.0 2.8008244931697845
CurrentTrain: epoch  9, batch    28 | loss: 8.1223801Losses:  8.226559937000275 0.2636542320251465 1.0 2.827771008014679
CurrentTrain: epoch  9, batch    29 | loss: 8.2265599Losses:  6.67716309428215 0.12910187244415283 1.0 1.3969387710094452
CurrentTrain: epoch  9, batch    30 | loss: 6.6771631Losses:  9.552942425012589 0.2055819034576416 1.0 4.261082798242569
CurrentTrain: epoch  9, batch    31 | loss: 9.5529424Losses:  12.91498601436615 0.125 1.0 7.648851275444031
CurrentTrain: epoch  9, batch    32 | loss: 12.9149860Losses:  9.730716805905104 0.3212672472000122 1.0 4.311046700924635
CurrentTrain: epoch  9, batch    33 | loss: 9.7307168Losses:  5.202604293823242 0.12542569637298584 1.0 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 5.2026043Losses:  11.016077384352684 0.125 1.0 5.7608741372823715
CurrentTrain: epoch  9, batch    35 | loss: 11.0160774Losses:  8.072265207767487 0.20292437076568604 1.0 2.7944136261940002
CurrentTrain: epoch  9, batch    36 | loss: 8.0722652Losses:  5.415230751037598 0.20119595527648926 1.0 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 5.4152308Losses:  6.742599066346884 0.125 1.0 1.4263864122331142
CurrentTrain: epoch  9, batch    38 | loss: 6.7425991Losses:  5.409143447875977 0.2514735460281372 1.0 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 5.4091434Losses:  6.716544598340988 0.1875 1.0 1.4255761802196503
CurrentTrain: epoch  9, batch    40 | loss: 6.7165446Losses:  5.317540168762207 0.21104025840759277 1.0 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 5.3175402Losses:  9.649358712136745 0.14430129528045654 1.0 4.306360207498074
CurrentTrain: epoch  9, batch    42 | loss: 9.6493587Losses:  8.183992356061935 0.25869274139404297 1.0 2.8204412162303925
CurrentTrain: epoch  9, batch    43 | loss: 8.1839924Losses:  6.896649211645126 0.37940526008605957 1.0 1.392878383398056
CurrentTrain: epoch  9, batch    44 | loss: 6.8966492Losses:  6.900862157344818 0.3820056915283203 1.0 1.4170584082603455
CurrentTrain: epoch  9, batch    45 | loss: 6.9008622Losses:  8.32864710316062 0.21377229690551758 1.0 2.887468781322241
CurrentTrain: epoch  9, batch    46 | loss: 8.3286471Losses:  8.11615002155304 0.13270962238311768 1.0 2.8520394563674927
CurrentTrain: epoch  9, batch    47 | loss: 8.1161500Losses:  5.324332237243652 0.2078385353088379 1.0 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 5.3243322Losses:  8.450483500957489 0.21849024295806885 1.0 2.8134509921073914
CurrentTrain: epoch  9, batch    49 | loss: 8.4504835Losses:  9.664888676255941 0.1875 1.0 4.326727207750082
CurrentTrain: epoch  9, batch    50 | loss: 9.6648887Losses:  5.49540901184082 0.38068127632141113 1.0 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 5.4954090Losses:  8.09413030743599 0.1407254934310913 1.0 2.830805093050003
CurrentTrain: epoch  9, batch    52 | loss: 8.0941303Losses:  8.068027466535568 0.08848035335540771 1.0 2.8229269683361053
CurrentTrain: epoch  9, batch    53 | loss: 8.0680275Losses:  8.30193105340004 0.375 1.0 2.835557132959366
CurrentTrain: epoch  9, batch    54 | loss: 8.3019311Losses:  9.738824721425772 0.22104954719543457 1.0 4.234442587941885
CurrentTrain: epoch  9, batch    55 | loss: 9.7388247Losses:  9.609291136264801 0.13995158672332764 1.0 4.265568792819977
CurrentTrain: epoch  9, batch    56 | loss: 9.6092911Losses:  14.396823607385159 0.33659398555755615 1.0 8.987453661859035
CurrentTrain: epoch  9, batch    57 | loss: 14.3968236Losses:  6.7901400327682495 0.19345343112945557 1.0 1.4411348104476929
CurrentTrain: epoch  9, batch    58 | loss: 6.7901400Losses:  9.458296924829483 0.08939683437347412 1.0 4.196223884820938
CurrentTrain: epoch  9, batch    59 | loss: 9.4582969Losses:  6.715087626129389 0.13875067234039307 1.0 1.456716749817133
CurrentTrain: epoch  9, batch    60 | loss: 6.7150876Losses:  10.78570756316185 0.3727225065231323 1.0272390842437744 4.203948110342026
CurrentTrain: epoch  9, batch    61 | loss: 10.7857076Losses:  11.229711577296257 0.26409220695495605 1.0 5.833165213465691
CurrentTrain: epoch  9, batch    62 | loss: 11.2297116
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  11.385864216834307 1.3052382469177246 2.060197353363037 1.5265616960823536
CurrentTrain: epoch  0, batch     0 | loss: 11.3858642Losses:  15.032516211271286 1.266708493232727 1.9748328924179077 5.674887388944626
CurrentTrain: epoch  0, batch     1 | loss: 15.0325162Losses:  11.83656907081604 1.2713638544082642 1.4867278337478638 2.845543146133423
CurrentTrain: epoch  0, batch     2 | loss: 11.8365691Losses:  11.530984412878752 0.8286848068237305 1.7538938522338867 1.6054272763431072
CurrentTrain: epoch  0, batch     3 | loss: 11.5309844Losses:  9.568236351013184 1.2325503826141357 1.921532392501831 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.5682364Losses:  10.504688188433647 1.2111973762512207 1.4994094371795654 1.8205384463071823
CurrentTrain: epoch  1, batch     1 | loss: 10.5046882Losses:  9.393011059612036 1.2606017589569092 1.5502285957336426 1.4951719902455807
CurrentTrain: epoch  1, batch     2 | loss: 9.3930111Losses:  9.20531540364027 1.1685113906860352 0.8016252517700195 1.4834869429469109
CurrentTrain: epoch  1, batch     3 | loss: 9.2053154Losses:  7.480209827423096 1.1261557340621948 1.324400544166565 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 7.4802098Losses:  11.146112263202667 1.2279870510101318 1.5856348276138306 2.971847355365753
CurrentTrain: epoch  2, batch     1 | loss: 11.1461123Losses:  7.630690574645996 1.1009739637374878 1.2846360206604004 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 7.6306906Losses:  6.6323248744010925 1.2087316513061523 1.1862354278564453 1.4103171229362488
CurrentTrain: epoch  2, batch     3 | loss: 6.6323249Losses:  11.218563683331013 1.157740592956543 1.4912645816802979 3.1240078285336494
CurrentTrain: epoch  3, batch     0 | loss: 11.2185637Losses:  9.815914765000343 1.123881459236145 1.369537591934204 3.3950687795877457
CurrentTrain: epoch  3, batch     1 | loss: 9.8159148Losses:  7.9905112981796265 1.0391616821289062 1.246963620185852 1.403910517692566
CurrentTrain: epoch  3, batch     2 | loss: 7.9905113Losses:  7.170839756727219 1.2251319885253906 1.0 1.4733738601207733
CurrentTrain: epoch  3, batch     3 | loss: 7.1708398Losses:  5.936352252960205 1.0938835144042969 1.2545610666275024 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 5.9363523Losses:  11.691490581259131 1.176461100578308 1.3246020078659058 5.013171603903174
CurrentTrain: epoch  4, batch     1 | loss: 11.6914906Losses:  5.977513790130615 0.9785110950469971 1.1191946268081665 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 5.9775138Losses:  7.20367618650198 1.0926227569580078 0.8507680892944336 1.4987959489226341
CurrentTrain: epoch  4, batch     3 | loss: 7.2036762Losses:  6.234238624572754 1.0817426443099976 1.1422308683395386 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.2342386Losses:  5.666812896728516 1.1174272298812866 1.131309151649475 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.6668129Losses:  9.929403100162745 0.9833019971847534 1.2717478275299072 3.9396236278116703
CurrentTrain: epoch  5, batch     2 | loss: 9.9294031Losses:  6.5988979041576385 0.5522098541259766 1.0 1.5724229514598846
CurrentTrain: epoch  5, batch     3 | loss: 6.5988979Losses:  5.338126182556152 0.9738887548446655 1.0568758249282837 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.3381262Losses:  5.4084649085998535 1.0700594186782837 1.1543259620666504 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 5.4084649Losses:  5.489532470703125 0.9401073455810547 1.1958200931549072 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 5.4895325Losses:  6.878453433513641 1.1567020416259766 1.1110353469848633 1.455542266368866
CurrentTrain: epoch  6, batch     3 | loss: 6.8784534Losses:  7.625986400991678 1.025474190711975 1.0732110738754272 2.3482163585722446
CurrentTrain: epoch  7, batch     0 | loss: 7.6259864Losses:  6.931692432612181 0.9746502637863159 1.126893401145935 1.554064106196165
CurrentTrain: epoch  7, batch     1 | loss: 6.9316924Losses:  6.483573533594608 0.9287083148956299 1.0873643159866333 1.488873578608036
CurrentTrain: epoch  7, batch     2 | loss: 6.4835735Losses:  5.776380572468042 1.1758708953857422 0.6241350173950195 1.504881415516138
CurrentTrain: epoch  7, batch     3 | loss: 5.7763806Losses:  9.522393368184566 1.0440536737442017 1.1473981142044067 4.355445526540279
CurrentTrain: epoch  8, batch     0 | loss: 9.5223934Losses:  8.048155967146158 0.8828794956207275 1.107269287109375 2.9428874887526035
CurrentTrain: epoch  8, batch     1 | loss: 8.0481560Losses:  4.8451642990112305 0.9607008695602417 1.1117265224456787 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 4.8451643Losses:  5.6144062876701355 0.8014297485351562 1.0 1.4127551913261414
CurrentTrain: epoch  8, batch     3 | loss: 5.6144063Losses:  6.110100209712982 0.8929258584976196 1.064619541168213 1.4254454970359802
CurrentTrain: epoch  9, batch     0 | loss: 6.1101002Losses:  6.195075307041407 1.046236276626587 1.0759440660476685 1.4745028354227543
CurrentTrain: epoch  9, batch     1 | loss: 6.1950753Losses:  6.858142003417015 0.8525142669677734 1.1364270448684692 1.8775974363088608
CurrentTrain: epoch  9, batch     2 | loss: 6.8581420Losses:  6.568619590252638 1.169133186340332 1.2755975723266602 1.5452002100646496
CurrentTrain: epoch  9, batch     3 | loss: 6.5686196
Losses:  3.5640597343444824 0.5792189836502075 1.0896574258804321 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 3.5640597Losses:  7.869585677981377 1.0373239517211914 1.0 5.776579305529594
MemoryTrain:  epoch  0, batch     1 | loss: 7.8695857Losses:  3.435605049133301 0.6681149005889893 1.0838690996170044 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 3.4356050Losses:  7.238565772771835 0.47244787216186523 1.0 5.625730603933334
MemoryTrain:  epoch  1, batch     1 | loss: 7.2385658Losses:  2.515096664428711 0.6275094747543335 1.0283560752868652 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.5150967Losses:  9.505253575742245 0.5562896728515625 1.0 5.682035468518734
MemoryTrain:  epoch  2, batch     1 | loss: 9.5052536Losses:  2.5770111083984375 0.5072833299636841 1.0533033609390259 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 2.5770111Losses:  7.795511603355408 0.9358634948730469 1.0 5.63278591632843
MemoryTrain:  epoch  3, batch     1 | loss: 7.7955116Losses:  2.569415330886841 0.5240081548690796 1.1017491817474365 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 2.5694153Losses:  7.412214323878288 0.6449375152587891 1.0 5.673004671931267
MemoryTrain:  epoch  4, batch     1 | loss: 7.4122143Losses:  2.1017777919769287 0.59688401222229 1.0690933465957642 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 2.1017778Losses:  8.351651579141617 0.3939032554626465 1.0 5.668369680643082
MemoryTrain:  epoch  5, batch     1 | loss: 8.3516516Losses:  2.200009822845459 0.5734777450561523 0.9546618461608887 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 2.2000098Losses:  7.257236387580633 0.4891242980957031 1.0 5.679337527602911
MemoryTrain:  epoch  6, batch     1 | loss: 7.2572364Losses:  2.239738941192627 0.56529700756073 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 2.2397389Losses:  7.116714775562286 0.4195585250854492 1.0188603401184082 5.65446549654007
MemoryTrain:  epoch  7, batch     1 | loss: 7.1167148Losses:  2.104743003845215 0.5611588954925537 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 2.1047430Losses:  7.059429503977299 0.3457822799682617 1.0 5.672429896891117
MemoryTrain:  epoch  8, batch     1 | loss: 7.0594295Losses:  1.899389386177063 0.4928133487701416 1.0492132902145386 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 1.8993894Losses:  7.297156095504761 0.5922517776489258 1.0 5.667295813560486
MemoryTrain:  epoch  9, batch     1 | loss: 7.2971561
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.35%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 79.92%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 74.24%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 71.94%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 68.37%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 67.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 73.31%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.53%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 93.23%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.04%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 93.16%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.57%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.66%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.49%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.41%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.33%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 93.26%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.10%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 92.87%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.64%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 92.42%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 92.23%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.94%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.89%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 91.84%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.72%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.45%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 91.15%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 90.69%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 90.66%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 90.42%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 89.92%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 89.36%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 88.75%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 88.28%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 88.02%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 87.63%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 87.31%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 86.88%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 86.39%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 86.21%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 85.68%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.46%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.24%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 84.96%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.52%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 84.03%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.49%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 82.95%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.49%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 82.14%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.91%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.48%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.50%   
cur_acc:  ['0.9464', '0.7331']
his_acc:  ['0.9464', '0.8350']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  8.805315848439932 1.0119731426239014 1.269179344177246 1.4795578680932522
CurrentTrain: epoch  0, batch     0 | loss: 8.8053158Losses:  9.282073020935059 0.9791377782821655 1.278874397277832 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 9.2820730Losses:  11.852523356676102 0.9362776279449463 1.412930965423584 3.907265692949295
CurrentTrain: epoch  0, batch     2 | loss: 11.8525234Losses:  11.881117887794971 1.0616979598999023 0.8636560440063477 1.5123511031270027
CurrentTrain: epoch  0, batch     3 | loss: 11.8811179Losses:  9.342238962650299 0.930876612663269 1.3696447610855103 1.4059324860572815
CurrentTrain: epoch  1, batch     0 | loss: 9.3422390Losses:  14.661681350320578 0.878899097442627 1.1743770837783813 8.14483517035842
CurrentTrain: epoch  1, batch     1 | loss: 14.6616814Losses:  10.089113757014275 0.970569372177124 1.3465546369552612 1.5208507031202316
CurrentTrain: epoch  1, batch     2 | loss: 10.0891138Losses:  9.825356997549534 0.9798879623413086 1.1298751831054688 1.5021710768342018
CurrentTrain: epoch  1, batch     3 | loss: 9.8253570Losses:  16.519722491502762 0.9246137142181396 1.3685821294784546 9.253834754228592
CurrentTrain: epoch  2, batch     0 | loss: 16.5197225Losses:  9.420853033661842 0.8907486200332642 1.2337710857391357 2.107845678925514
CurrentTrain: epoch  2, batch     1 | loss: 9.4208530Losses:  8.661173108965158 0.9135570526123047 1.2248623371124268 1.437341932207346
CurrentTrain: epoch  2, batch     2 | loss: 8.6611731Losses:  9.19799755513668 0.9786996841430664 1.3736295700073242 1.5333971828222275
CurrentTrain: epoch  2, batch     3 | loss: 9.1979976Losses:  6.466343402862549 0.9050666093826294 1.065590262413025 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.4663434Losses:  9.732751324772835 0.7803975343704224 1.14574134349823 3.3229918032884598
CurrentTrain: epoch  3, batch     1 | loss: 9.7327513Losses:  10.681631237268448 0.9169241189956665 1.4770489931106567 2.997323662042618
CurrentTrain: epoch  3, batch     2 | loss: 10.6816312Losses:  9.594370108097792 0.920924186706543 1.6834726333618164 1.7995912842452526
CurrentTrain: epoch  3, batch     3 | loss: 9.5943701Losses:  9.858949333429337 0.9635062217712402 1.018773078918457 3.1803199350833893
CurrentTrain: epoch  4, batch     0 | loss: 9.8589493Losses:  7.797992676496506 0.846939206123352 1.2637138366699219 1.4188413321971893
CurrentTrain: epoch  4, batch     1 | loss: 7.7979927Losses:  10.32921864837408 0.8189588785171509 1.3481780290603638 3.0725844129920006
CurrentTrain: epoch  4, batch     2 | loss: 10.3292186Losses:  5.958199672400951 0.7958221435546875 1.0 1.5108920857310295
CurrentTrain: epoch  4, batch     3 | loss: 5.9581997Losses:  6.458786964416504 0.824755072593689 1.3129684925079346 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.4587870Losses:  7.002712361514568 0.8241230249404907 1.037856936454773 1.4568205997347832
CurrentTrain: epoch  5, batch     1 | loss: 7.0027124Losses:  8.239887654781342 0.9036790132522583 1.2109107971191406 1.4180778861045837
CurrentTrain: epoch  5, batch     2 | loss: 8.2398877Losses:  8.378541313111782 0.8996486663818359 1.0 1.4542239531874657
CurrentTrain: epoch  5, batch     3 | loss: 8.3785413Losses:  8.70688732713461 0.8232072591781616 1.091583490371704 2.9083977565169334
CurrentTrain: epoch  6, batch     0 | loss: 8.7068873Losses:  9.768103897571564 0.7718400955200195 1.4412496089935303 2.9581664204597473
CurrentTrain: epoch  6, batch     1 | loss: 9.7681039Losses:  11.437191307544708 0.8903853893280029 1.0681673288345337 5.229859173297882
CurrentTrain: epoch  6, batch     2 | loss: 11.4371913Losses:  7.910570457577705 0.6523027420043945 1.0 1.4482529908418655
CurrentTrain: epoch  6, batch     3 | loss: 7.9105705Losses:  5.142101287841797 0.7998096942901611 1.0346269607543945 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 5.1421013Losses:  7.7041038274765015 0.8393486738204956 1.222128987312317 1.4193485975265503
CurrentTrain: epoch  7, batch     1 | loss: 7.7041038Losses:  10.625614136457443 0.7882909774780273 1.2348988056182861 4.594818085432053
CurrentTrain: epoch  7, batch     2 | loss: 10.6256141Losses:  8.368275806307793 0.9273014068603516 1.0 1.4754935950040817
CurrentTrain: epoch  7, batch     3 | loss: 8.3682758Losses:  10.13890678063035 0.9339079856872559 1.0624732971191406 4.454423252493143
CurrentTrain: epoch  8, batch     0 | loss: 10.1389068Losses:  9.02878612279892 0.6850379705429077 1.1009055376052856 2.8100447058677673
CurrentTrain: epoch  8, batch     1 | loss: 9.0287861Losses:  9.211508713662624 0.7470524311065674 1.2035589218139648 4.280210457742214
CurrentTrain: epoch  8, batch     2 | loss: 9.2115087Losses:  6.984735697507858 0.8124771118164062 1.0 1.3951976001262665
CurrentTrain: epoch  8, batch     3 | loss: 6.9847357Losses:  5.727149486541748 0.7664638757705688 1.161098837852478 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 5.7271495Losses:  6.687907636165619 0.7676465511322021 1.2132989168167114 1.4359182715415955
CurrentTrain: epoch  9, batch     1 | loss: 6.6879076Losses:  7.389669083058834 0.7829830646514893 1.0727229118347168 1.456420086324215
CurrentTrain: epoch  9, batch     2 | loss: 7.3896691Losses:  5.182303890585899 0.7481822967529297 0.7960453033447266 1.427666649222374
CurrentTrain: epoch  9, batch     3 | loss: 5.1823039
Losses:  2.343935012817383 0.9123013019561768 1.1549036502838135 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.3439350Losses:  2.0850911140441895 0.634627640247345 1.0 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.0850911Losses:  2.1040215492248535 0.6726117134094238 1.0 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.1040215Losses:  2.655705213546753 0.8019158840179443 1.0187537670135498 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.6557052Losses:  2.1936235427856445 0.714737057685852 1.0679986476898193 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 2.1936235Losses:  2.0147786140441895 0.7574652433395386 1.0101597309112549 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 2.0147786Losses:  1.801788330078125 0.6630024909973145 1.0049386024475098 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.8017883Losses:  1.9750676155090332 0.7792004346847534 1.0435961484909058 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 1.9750676Losses:  1.8785890340805054 0.7086179256439209 0.9852370023727417 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 1.8785890Losses:  1.8295142650604248 0.7208884358406067 1.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 1.8295143Losses:  1.9240909814834595 0.7719594240188599 1.0395907163619995 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 1.9240910Losses:  1.6806952953338623 0.6043056845664978 1.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 1.6806953Losses:  1.790745735168457 0.7217633724212646 1.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 1.7907457Losses:  1.6652979850769043 0.6286719441413879 1.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 1.6652980Losses:  1.5963510274887085 0.5528279542922974 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 1.5963510Losses:  1.8398253917694092 0.7833883166313171 1.0153241157531738 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 1.8398254Losses:  1.7204288244247437 0.6925811767578125 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 1.7204288Losses:  1.606263279914856 0.5750591158866882 1.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 1.6062633Losses:  1.6145961284637451 0.5879249572753906 1.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 1.6145961Losses:  1.6722724437713623 0.6300203204154968 1.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 1.6722724
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 60.04%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 58.19%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 56.46%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 55.04%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 55.66%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 56.82%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 58.09%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 59.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 60.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 61.32%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 62.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.79%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 66.63%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.55%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 87.23%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 91.49%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 91.42%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.09%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.17%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 91.21%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 91.54%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.79%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 91.67%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.70%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.69%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.48%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 91.19%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.98%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 90.86%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 90.82%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 90.55%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 90.29%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 89.96%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 89.78%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 89.61%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 89.15%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 88.99%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 88.83%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 88.47%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 88.18%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 87.77%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 87.23%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 86.64%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 86.20%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.95%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 85.52%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 85.23%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 84.81%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.47%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.25%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 83.86%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 83.71%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.51%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 83.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.83%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 82.35%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 81.82%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 81.19%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 80.80%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.41%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.25%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 81.50%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 81.30%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 80.86%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 80.52%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 80.19%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 79.87%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 79.88%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 80.13%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 80.04%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 79.60%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 79.26%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 78.96%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 78.76%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 78.52%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.05%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.19%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 78.85%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 78.33%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.86%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 77.39%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 76.94%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 76.52%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.03%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 77.90%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 77.85%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 77.51%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 77.57%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 77.49%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 77.44%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 77.32%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 77.30%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 77.33%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 77.28%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.29%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 77.49%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 77.39%   
cur_acc:  ['0.9464', '0.7331', '0.6855']
his_acc:  ['0.9464', '0.8350', '0.7739']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  7.709105491638184 1.0926251411437988 1.4409443140029907 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 7.7091055Losses:  8.516267389059067 0.9048798084259033 1.592644214630127 1.425473302602768
CurrentTrain: epoch  0, batch     1 | loss: 8.5162674Losses:  15.34474989399314 0.9136636257171631 1.4273223876953125 8.654770340770483
CurrentTrain: epoch  0, batch     2 | loss: 15.3447499Losses:  7.934771053493023 1.0054740905761719 1.217778205871582 1.4556202813982964
CurrentTrain: epoch  0, batch     3 | loss: 7.9347711Losses:  9.773497246205807 0.8767815828323364 1.3176769018173218 3.176466129720211
CurrentTrain: epoch  1, batch     0 | loss: 9.7734972Losses:  6.185452461242676 0.9542635679244995 1.3698890209197998 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 6.1854525Losses:  6.63935923576355 0.8453788757324219 1.2354059219360352 1.4461424350738525
CurrentTrain: epoch  1, batch     2 | loss: 6.6393592Losses:  6.902472570538521 0.9933061599731445 1.0 1.4366231709718704
CurrentTrain: epoch  1, batch     3 | loss: 6.9024726Losses:  10.206471756100655 0.9913780689239502 1.3716965913772583 4.273849800229073
CurrentTrain: epoch  2, batch     0 | loss: 10.2064718Losses:  8.599930074065924 0.8589218854904175 1.2263668775558472 3.0042293332517147
CurrentTrain: epoch  2, batch     1 | loss: 8.5999301Losses:  7.718782372772694 0.8179632425308228 1.1404039859771729 2.850086160004139
CurrentTrain: epoch  2, batch     2 | loss: 7.7187824Losses:  9.811327576637268 0.2543306350708008 1.0 6.744279503822327
CurrentTrain: epoch  2, batch     3 | loss: 9.8113276Losses:  7.647719666361809 0.8671261072158813 1.0414535999298096 2.2721341103315353
CurrentTrain: epoch  3, batch     0 | loss: 7.6477197Losses:  9.599903590977192 0.8693846464157104 1.277109980583191 4.341018207371235
CurrentTrain: epoch  3, batch     1 | loss: 9.5999036Losses:  7.7070262134075165 0.7540289163589478 1.2178709506988525 2.8556258380413055
CurrentTrain: epoch  3, batch     2 | loss: 7.7070262Losses:  5.195435106754303 0.6135225296020508 1.0 1.4105625748634338
CurrentTrain: epoch  3, batch     3 | loss: 5.1954351Losses:  6.445178635418415 0.8539289236068726 1.1177794933319092 1.4435268715023994
CurrentTrain: epoch  4, batch     0 | loss: 6.4451786Losses:  4.859769821166992 0.8302686214447021 1.061896562576294 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 4.8597698Losses:  6.3905477821826935 0.762321949005127 0.981979489326477 1.4736585915088654
CurrentTrain: epoch  4, batch     2 | loss: 6.3905478Losses:  5.000826932489872 0.6655483245849609 1.0451440811157227 1.4413901343941689
CurrentTrain: epoch  4, batch     3 | loss: 5.0008269Losses:  7.1107339560985565 0.7119961977005005 1.049870252609253 2.9172153174877167
CurrentTrain: epoch  5, batch     0 | loss: 7.1107340Losses:  4.748982906341553 0.8752363920211792 1.0604134798049927 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 4.7489829Losses:  6.576670527458191 0.7446346282958984 1.0642096996307373 1.5239218473434448
CurrentTrain: epoch  5, batch     2 | loss: 6.5766705Losses:  5.9333005249500275 0.8599824905395508 1.0 1.6646681129932404
CurrentTrain: epoch  5, batch     3 | loss: 5.9333005Losses:  4.385550498962402 0.7706105709075928 0.9804390668869019 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 4.3855505Losses:  5.992036044597626 0.6871429681777954 1.039588212966919 1.448581874370575
CurrentTrain: epoch  6, batch     1 | loss: 5.9920360Losses:  10.681195169687271 0.7242240905761719 1.0142295360565186 6.050740152597427
CurrentTrain: epoch  6, batch     2 | loss: 10.6811952Losses:  5.040829963982105 0.6601438522338867 1.0 1.4381077960133553
CurrentTrain: epoch  6, batch     3 | loss: 5.0408300Losses:  5.762116696685553 0.6382811069488525 0.9665426015853882 1.4565437100827694
CurrentTrain: epoch  7, batch     0 | loss: 5.7621167Losses:  4.858369827270508 0.7270808219909668 0.9931654930114746 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 4.8583698Losses:  8.195222742855549 0.7330172061920166 1.010644793510437 4.190322287380695
CurrentTrain: epoch  7, batch     2 | loss: 8.1952227Losses:  5.039730086922646 0.6897287368774414 1.0 1.442441001534462
CurrentTrain: epoch  7, batch     3 | loss: 5.0397301Losses:  5.747244682163 0.6369317770004272 0.9983073472976685 1.4153946302831173
CurrentTrain: epoch  8, batch     0 | loss: 5.7472447Losses:  5.76178402453661 0.6648964881896973 0.9615046977996826 1.513184018433094
CurrentTrain: epoch  8, batch     1 | loss: 5.7617840Losses:  3.980966567993164 0.6954485177993774 0.9693585634231567 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 3.9809666Losses:  5.447961397469044 0.39502811431884766 1.0 1.4576378539204597
CurrentTrain: epoch  8, batch     3 | loss: 5.4479614Losses:  7.036010563373566 0.7794004678726196 0.969626784324646 2.809983551502228
CurrentTrain: epoch  9, batch     0 | loss: 7.0360106Losses:  4.326467037200928 0.6402698755264282 0.9126764535903931 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.3264670Losses:  9.568588577210903 0.5429815053939819 1.0040392875671387 5.666488014161587
CurrentTrain: epoch  9, batch     2 | loss: 9.5685886Losses:  4.2104069367051125 0.029577255249023438 1.0 1.454966701567173
CurrentTrain: epoch  9, batch     3 | loss: 4.2104069
Losses:  2.1816561222076416 0.7623533010482788 0.9968526363372803 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.1816561Losses:  2.039626121520996 0.6969922780990601 1.0262092351913452 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.0396261Losses:  2.736710548400879 0.37715649604797363 1.0518183708190918 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 2.7367105Losses:  1.78794264793396 0.5994745492935181 1.0 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.7879426Losses:  2.8224425315856934 0.6881011724472046 1.0596314668655396 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 2.8224425Losses:  2.244880199432373 0.592503547668457 1.0336778163909912 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 2.2448802Losses:  1.9274593591690063 0.6189515590667725 1.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.9274594Losses:  1.8549015522003174 0.501685380935669 1.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.8549016Losses:  2.1522903442382812 0.7833366394042969 1.0017554759979248 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 2.1522903Losses:  1.6511781215667725 0.589666485786438 1.0 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.6511781Losses:  1.6370171308517456 0.5634496212005615 1.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 1.6370171Losses:  2.037621259689331 0.7370069026947021 1.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 2.0376213Losses:  1.6879103183746338 0.6029719114303589 1.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 1.6879103Losses:  1.5678530931472778 0.5087134838104248 1.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 1.5678531Losses:  1.8107054233551025 0.7320222854614258 1.0 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 1.8107054Losses:  1.6256639957427979 0.5426609516143799 1.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 1.6256640Losses:  1.5853984355926514 0.5112941265106201 1.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 1.5853984Losses:  1.752706527709961 0.7033650875091553 1.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 1.7527065Losses:  1.522324800491333 0.48341989517211914 1.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 1.5223248Losses:  1.7304065227508545 0.6481244564056396 1.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 1.7304065Losses:  1.6085433959960938 0.5895566940307617 1.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 1.6085434Losses:  1.615797996520996 0.5717016458511353 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 1.6157980Losses:  1.565138816833496 0.5268452167510986 1.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 1.5651388Losses:  1.528848648071289 0.5022916793823242 0.9852039813995361 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 1.5288486Losses:  1.6705310344696045 0.6460542678833008 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 1.6705310Losses:  1.518505334854126 0.48925817012786865 1.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 1.5185053Losses:  1.3994044065475464 0.36850857734680176 1.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 1.3994044Losses:  1.5880134105682373 0.5632781982421875 1.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 1.5880134Losses:  1.498013973236084 0.47630631923675537 1.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 1.4980140Losses:  1.467617392539978 0.44562411308288574 1.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 1.4676174
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 72.13%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 78.42%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 78.36%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 78.18%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 78.01%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 78.56%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.27%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.70%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 87.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 87.72%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 87.39%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.29%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.09%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 87.10%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 87.40%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.60%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 87.78%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.56%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.61%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 88.68%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.65%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 88.22%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.05%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 87.96%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 87.58%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 87.05%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 86.10%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 85.68%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 84.91%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 84.73%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 84.48%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 84.10%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 83.79%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 83.42%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 82.93%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.45%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 81.84%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 81.32%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 81.06%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 80.61%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 80.30%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 79.75%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.52%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 79.35%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 79.07%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 78.97%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.87%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.71%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 78.27%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 77.78%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.24%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 76.59%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.18%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 75.84%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.66%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.03%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 77.28%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 77.02%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 76.56%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 76.11%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.62%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 75.24%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 75.24%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.09%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 75.72%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 75.40%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 75.09%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 74.78%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 74.61%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 74.39%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.04%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 74.92%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 74.42%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.94%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 73.50%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 73.06%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.60%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 73.27%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 73.14%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 73.00%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 72.87%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.78%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 72.68%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 72.53%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 72.19%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 71.84%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 71.72%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 72.95%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 72.99%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.07%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 72.91%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 72.82%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 72.84%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 72.70%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 72.65%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.63%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 72.36%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 71.96%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 71.55%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 71.51%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 72.31%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 72.35%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 72.28%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.43%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 73.66%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 73.64%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 73.67%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 73.71%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 73.66%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 73.69%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.25%   
cur_acc:  ['0.9464', '0.7331', '0.6855', '0.7927']
his_acc:  ['0.9464', '0.8350', '0.7739', '0.7425']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  11.248855076730251 1.1384389400482178 1.5378029346466064 2.220581494271755
CurrentTrain: epoch  0, batch     0 | loss: 11.2488551Losses:  15.135538805276155 0.9985730648040771 1.4310016632080078 6.909714449197054
CurrentTrain: epoch  0, batch     1 | loss: 15.1355388Losses:  11.622100614011288 1.0653752088546753 1.4149407148361206 3.049736760556698
CurrentTrain: epoch  0, batch     2 | loss: 11.6221006Losses:  10.291157107800245 1.0688209533691406 1.1458616256713867 1.6666453890502453
CurrentTrain: epoch  0, batch     3 | loss: 10.2911571Losses:  16.07347873598337 1.0273884534835815 1.5690563917160034 8.535514868795872
CurrentTrain: epoch  1, batch     0 | loss: 16.0734787Losses:  7.946073532104492 1.0234336853027344 1.323101282119751 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 7.9460735Losses:  7.937042236328125 1.0798540115356445 1.3044557571411133 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 7.9370422Losses:  7.8872977793216705 0.970881462097168 1.0785627365112305 1.4726702272891998
CurrentTrain: epoch  1, batch     3 | loss: 7.8872978Losses:  9.989946380257607 0.9649524688720703 1.4350780248641968 1.4972572475671768
CurrentTrain: epoch  2, batch     0 | loss: 9.9899464Losses:  8.154016200453043 0.9649158716201782 1.2191263437271118 1.4282829202711582
CurrentTrain: epoch  2, batch     1 | loss: 8.1540162Losses:  15.101305019110441 0.9773857593536377 1.2143112421035767 9.0668177716434
CurrentTrain: epoch  2, batch     2 | loss: 15.1013050Losses:  7.633679158985615 1.024186134338379 0.738795280456543 1.4428055360913277
CurrentTrain: epoch  2, batch     3 | loss: 7.6336792Losses:  7.035834789276123 0.9581013917922974 1.3093124628067017 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 7.0358348Losses:  7.691122051328421 0.9393222332000732 1.2162799835205078 1.426521297544241
CurrentTrain: epoch  3, batch     1 | loss: 7.6911221Losses:  9.720446109771729 0.9554578065872192 1.1325117349624634 3.3392534255981445
CurrentTrain: epoch  3, batch     2 | loss: 9.7204461Losses:  7.393975049257278 0.854914665222168 1.315323829650879 1.5157335102558136
CurrentTrain: epoch  3, batch     3 | loss: 7.3939750Losses:  13.031683839857578 0.9563647508621216 1.1428344249725342 6.499866403639317
CurrentTrain: epoch  4, batch     0 | loss: 13.0316838Losses:  7.686088062822819 0.951251745223999 1.218786358833313 1.6018662229180336
CurrentTrain: epoch  4, batch     1 | loss: 7.6860881Losses:  7.152942642569542 0.8748269081115723 1.1826341152191162 1.578590378165245
CurrentTrain: epoch  4, batch     2 | loss: 7.1529426Losses:  6.902539953589439 0.688105583190918 0.9709224700927734 1.4804422706365585
CurrentTrain: epoch  4, batch     3 | loss: 6.9025400Losses:  9.97605611011386 0.913780689239502 1.1210641860961914 3.3342175595462322
CurrentTrain: epoch  5, batch     0 | loss: 9.9760561Losses:  5.149901390075684 0.8802947998046875 1.082888126373291 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 5.1499014Losses:  5.698622226715088 0.8821995258331299 1.1811357736587524 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 5.6986222Losses:  5.6025290712714195 0.9277429580688477 1.0 1.422359012067318
CurrentTrain: epoch  5, batch     3 | loss: 5.6025291Losses:  10.23782593011856 0.8023511171340942 1.0314514636993408 4.295157968997955
CurrentTrain: epoch  6, batch     0 | loss: 10.2378259Losses:  8.052549052983522 0.8701868057250977 1.0961589813232422 2.9944206960499287
CurrentTrain: epoch  6, batch     1 | loss: 8.0525491Losses:  7.3630509078502655 0.9434545040130615 1.2032577991485596 1.4295763671398163
CurrentTrain: epoch  6, batch     2 | loss: 7.3630509Losses:  5.369552373886108 0.9916772842407227 1.0 1.4373252391815186
CurrentTrain: epoch  6, batch     3 | loss: 5.3695524Losses:  8.009058587253094 0.7848390340805054 1.0737335681915283 2.881136529147625
CurrentTrain: epoch  7, batch     0 | loss: 8.0090586Losses:  5.495730876922607 0.9422668218612671 1.0479148626327515 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.4957309Losses:  6.693401999771595 0.7887287139892578 1.1041502952575684 1.4398843720555305
CurrentTrain: epoch  7, batch     2 | loss: 6.6934020Losses:  7.446307480335236 0.7078266143798828 1.0 1.4059885144233704
CurrentTrain: epoch  7, batch     3 | loss: 7.4463075Losses:  5.438119888305664 0.7759432792663574 1.0464441776275635 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 5.4381199Losses:  10.31062538176775 0.7821518182754517 1.1578446626663208 5.277620621025562
CurrentTrain: epoch  8, batch     1 | loss: 10.3106254Losses:  8.323867991566658 0.8141930103302002 1.0262420177459717 3.1677972823381424
CurrentTrain: epoch  8, batch     2 | loss: 8.3238680Losses:  5.3990209102630615 0.6762104034423828 1.0 1.393916368484497
CurrentTrain: epoch  8, batch     3 | loss: 5.3990209Losses:  7.904274068772793 0.7424472570419312 1.1162645816802979 2.9583388194441795
CurrentTrain: epoch  9, batch     0 | loss: 7.9042741Losses:  6.450463980436325 0.8326265811920166 1.0150103569030762 1.4681431949138641
CurrentTrain: epoch  9, batch     1 | loss: 6.4504640Losses:  4.997766971588135 0.8030427694320679 1.0655560493469238 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 4.9977670Losses:  5.6417189463973045 0.5627241134643555 0.9663276672363281 1.4066439494490623
CurrentTrain: epoch  9, batch     3 | loss: 5.6417189
Losses:  2.345461845397949 0.7689428329467773 1.0390156507492065 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.3454618Losses:  2.044860363006592 0.702080249786377 0.9624588489532471 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 2.0448604Losses:  1.62588369846344 0.5630154609680176 0.974698543548584 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 1.6258837Losses:  3.722309283912182 0.2682781219482422 1.0 1.5804421231150627
MemoryTrain:  epoch  0, batch     3 | loss: 3.7223093Losses:  2.2422268390655518 0.739572286605835 1.0 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.2422268Losses:  1.9867010116577148 0.5634663105010986 1.0114825963974 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.9867010Losses:  1.905916452407837 0.5833460092544556 0.9452527761459351 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 1.9059165Losses:  3.319917030632496 0.6384086608886719 1.0 1.455868549644947
MemoryTrain:  epoch  1, batch     3 | loss: 3.3199170Losses:  1.869223952293396 0.6061291694641113 1.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.8692240Losses:  1.886678695678711 0.6708060503005981 1.059138536453247 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.8866787Losses:  1.743906021118164 0.5789048671722412 1.017566204071045 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 1.7439060Losses:  3.2748186588287354 0.7429647445678711 1.0 1.4295607805252075
MemoryTrain:  epoch  2, batch     3 | loss: 3.2748187Losses:  1.9571574926376343 0.7293181419372559 1.0 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.9571575Losses:  1.6966615915298462 0.6172417402267456 1.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 1.6966616Losses:  1.5944304466247559 0.5196154117584229 1.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 1.5944304Losses:  2.7237573340535164 0.21590232849121094 1.0 1.4710397198796272
MemoryTrain:  epoch  3, batch     3 | loss: 2.7237573Losses:  1.64484441280365 0.5518088340759277 1.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 1.6448444Losses:  1.8162786960601807 0.7222061157226562 1.0431022644042969 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 1.8162787Losses:  1.6239047050476074 0.4958994388580322 1.0 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 1.6239047Losses:  3.27017193287611 0.698390007019043 1.0 1.459407977759838
MemoryTrain:  epoch  4, batch     3 | loss: 3.2701719Losses:  1.5533995628356934 0.47972071170806885 1.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 1.5533996Losses:  1.6472647190093994 0.5367462635040283 1.0110993385314941 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 1.6472647Losses:  1.7736573219299316 0.6798914670944214 1.0393012762069702 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 1.7736573#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  25.8325113710016 5.320791244506836 5.345877170562744 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 25.8325114Losses:  26.842761002480984 5.532492637634277 5.746808052062988 5.8394975289702415
CurrentTrain: epoch  0, batch     1 | loss: 26.8427610Losses:  25.383552636951208 5.485356330871582 5.493894100189209 4.408569421619177
CurrentTrain: epoch  0, batch     2 | loss: 25.3835526Losses:  25.438830319792032 5.412986755371094 5.645634174346924 5.16335004940629
CurrentTrain: epoch  0, batch     3 | loss: 25.4388303Losses:  23.718206461519003 5.53026819229126 5.568498611450195 2.533542688935995
CurrentTrain: epoch  0, batch     4 | loss: 23.7182065Losses:  23.986056350171566 5.411700248718262 5.459008693695068 4.398505233228207
CurrentTrain: epoch  0, batch     5 | loss: 23.9860564Losses:  28.84700672328472 5.464755058288574 5.421453475952148 8.307585641741753
CurrentTrain: epoch  0, batch     6 | loss: 28.8470067Losses:  24.93151082843542 5.632335186004639 5.68054723739624 3.8532599434256554
CurrentTrain: epoch  0, batch     7 | loss: 24.9315108Losses:  31.7795290350914 5.568489074707031 5.388635635375977 10.504817426204681
CurrentTrain: epoch  0, batch     8 | loss: 31.7795290Losses:  22.764478236436844 5.423480033874512 5.525352478027344 2.8934417068958282
CurrentTrain: epoch  0, batch     9 | loss: 22.7644782Losses:  23.991824455559254 5.49324893951416 5.546640872955322 3.8598912432789803
CurrentTrain: epoch  0, batch    10 | loss: 23.9918245Losses:  23.670805051922798 5.44321346282959 5.237311363220215 3.0652085095643997
CurrentTrain: epoch  0, batch    11 | loss: 23.6708051Losses:  21.105939865112305 5.526972770690918 5.524843692779541 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 21.1059399Losses:  21.007719039916992 5.593330383300781 5.492049217224121 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 21.0077190Losses:  26.258839163929224 5.494953155517578 5.353956699371338 5.3638253547251225
CurrentTrain: epoch  0, batch    14 | loss: 26.2588392Losses:  24.636673010885715 5.253562927246094 5.174203872680664 5.073753394186497
CurrentTrain: epoch  0, batch    15 | loss: 24.6366730Losses:  23.45594022423029 5.130606651306152 5.282279968261719 4.031375862658024
CurrentTrain: epoch  0, batch    16 | loss: 23.4559402Losses:  23.81019613519311 5.398192405700684 5.192797660827637 3.8310701586306095
CurrentTrain: epoch  0, batch    17 | loss: 23.8101961Losses:  25.429215349256992 5.365299701690674 5.10314416885376 5.076182283461094
CurrentTrain: epoch  0, batch    18 | loss: 25.4292153Losses:  21.414462745189667 5.399851322174072 5.120711326599121 1.4305989146232605
CurrentTrain: epoch  0, batch    19 | loss: 21.4144627Losses:  22.973900705575943 5.321181297302246 5.205210208892822 4.267893701791763
CurrentTrain: epoch  0, batch    20 | loss: 22.9739007Losses:  23.911357037723064 5.258884429931641 4.989321708679199 5.037974469363689
CurrentTrain: epoch  0, batch    21 | loss: 23.9113570Losses:  28.535535022616386 5.387318134307861 5.395268440246582 8.857686206698418
CurrentTrain: epoch  0, batch    22 | loss: 28.5355350Losses:  21.603843063116074 5.3914079666137695 5.305157661437988 1.4495843350887299
CurrentTrain: epoch  0, batch    23 | loss: 21.6038431Losses:  25.40914604254067 5.268403053283691 5.354803562164307 5.791622849181294
CurrentTrain: epoch  0, batch    24 | loss: 25.4091460Losses:  19.936044692993164 5.438704490661621 5.3207292556762695 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 19.9360447Losses:  19.523563385009766 5.4761810302734375 5.220576763153076 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 19.5235634Losses:  24.069647945463657 5.262004852294922 5.053884506225586 4.731902278959751
CurrentTrain: epoch  0, batch    27 | loss: 24.0696479Losses:  23.454413639381528 5.365644454956055 5.089216709136963 4.468331562355161
CurrentTrain: epoch  0, batch    28 | loss: 23.4544136Losses:  19.683223724365234 5.518202781677246 5.320703506469727 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 19.6832237Losses:  18.37234878540039 5.345311164855957 5.189227104187012 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 18.3723488Losses:  22.928507454693317 5.068264961242676 5.065910816192627 5.063400872051716
CurrentTrain: epoch  0, batch    31 | loss: 22.9285075Losses:  18.664623260498047 5.243103504180908 5.147607803344727 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 18.6646233Losses:  19.09028074145317 5.221784591674805 5.164310455322266 1.405880182981491
CurrentTrain: epoch  0, batch    33 | loss: 19.0902807Losses:  22.414050340652466 5.401850700378418 5.068601131439209 3.2143776416778564
CurrentTrain: epoch  0, batch    34 | loss: 22.4140503Losses:  22.748584270477295 5.412802696228027 5.146632671356201 3.3187174797058105
CurrentTrain: epoch  0, batch    35 | loss: 22.7485843Losses:  20.2414488568902 5.097099781036377 5.027430057525635 1.7403597608208656
CurrentTrain: epoch  0, batch    36 | loss: 20.2414489Losses:  19.912926524877548 5.219644069671631 5.08098840713501 1.423468440771103
CurrentTrain: epoch  0, batch    37 | loss: 19.9129265Losses:  18.349815368652344 5.434018135070801 5.161942481994629 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 18.3498154Losses:  24.716615051031113 5.45883846282959 5.028402328491211 5.216134399175644
CurrentTrain: epoch  0, batch    39 | loss: 24.7166151Losses:  26.23864035308361 5.437442779541016 5.229217052459717 7.268166109919548
CurrentTrain: epoch  0, batch    40 | loss: 26.2386404Losses:  25.33368656039238 5.249284744262695 5.039029121398926 5.94742938876152
CurrentTrain: epoch  0, batch    41 | loss: 25.3336866Losses:  23.05847993865609 5.368996620178223 5.248591423034668 3.7951456643640995
CurrentTrain: epoch  0, batch    42 | loss: 23.0584799Losses:  25.056481957435608 5.214651107788086 5.023186683654785 6.798030495643616
CurrentTrain: epoch  0, batch    43 | loss: 25.0564820Losses:  20.393060501664877 5.308825969696045 5.0481157302856445 1.8163202367722988
CurrentTrain: epoch  0, batch    44 | loss: 20.3930605Losses:  18.168886184692383 5.270275115966797 5.1183390617370605 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 18.1688862Losses:  23.723511397838593 5.294669151306152 5.081739902496338 4.873753249645233
CurrentTrain: epoch  0, batch    46 | loss: 23.7235114Losses:  23.70458234101534 5.277652740478516 4.981202125549316 4.902549870312214
CurrentTrain: epoch  0, batch    47 | loss: 23.7045823Losses:  21.607742315158248 5.340024948120117 5.283186435699463 2.5779628809541464
CurrentTrain: epoch  0, batch    48 | loss: 21.6077423Losses:  20.844781659543514 5.416789531707764 5.1327385902404785 1.5373742803931236
CurrentTrain: epoch  0, batch    49 | loss: 20.8447817Losses:  27.568169061094522 5.339227676391602 5.1274027824401855 8.678995553404093
CurrentTrain: epoch  0, batch    50 | loss: 27.5681691Losses:  22.09748100116849 5.188535213470459 5.1505560874938965 4.6763784773647785
CurrentTrain: epoch  0, batch    51 | loss: 22.0974810Losses:  19.67626190185547 5.503662109375 5.174952983856201 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 19.6762619Losses:  23.574864469468594 5.016784191131592 4.950976371765137 5.4145670756697655
CurrentTrain: epoch  0, batch    53 | loss: 23.5748645Losses:  25.33660778403282 5.040396690368652 4.977355003356934 7.512617915868759
CurrentTrain: epoch  0, batch    54 | loss: 25.3366078Losses:  19.322586242109537 5.273355484008789 5.074117183685303 1.5446187891066074
CurrentTrain: epoch  0, batch    55 | loss: 19.3225862Losses:  30.767075553536415 5.118213176727295 4.9966583251953125 12.257248893380165
CurrentTrain: epoch  0, batch    56 | loss: 30.7670756Losses:  20.561579022556543 5.101315498352051 5.075977802276611 2.990099225193262
CurrentTrain: epoch  0, batch    57 | loss: 20.5615790Losses:  22.003481719642878 5.2887749671936035 5.222848415374756 3.7965343929827213
CurrentTrain: epoch  0, batch    58 | loss: 22.0034817Losses:  19.946461904793978 5.2450079917907715 5.1590776443481445 1.5222217924892902
CurrentTrain: epoch  0, batch    59 | loss: 19.9464619Losses:  33.35325914621353 5.056068420410156 5.003899574279785 16.04349046945572
CurrentTrain: epoch  0, batch    60 | loss: 33.3532591Losses:  21.832839526236057 5.09126091003418 4.985610008239746 4.628367938101292
CurrentTrain: epoch  0, batch    61 | loss: 21.8328395Losses:  20.360199194401503 5.20844841003418 5.347734451293945 1.6455499939620495
CurrentTrain: epoch  0, batch    62 | loss: 20.3601992Losses:  18.545089721679688 5.403263092041016 5.047142505645752 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 18.5450897Losses:  21.90633078990504 5.211244583129883 5.1848955154418945 4.146977152209729
CurrentTrain: epoch  1, batch     1 | loss: 21.9063308Losses:  24.33608578518033 5.356517314910889 5.1001129150390625 6.620824325829744
CurrentTrain: epoch  1, batch     2 | loss: 24.3360858Losses:  19.200151801109314 5.174576759338379 5.094597816467285 1.4741042852401733
CurrentTrain: epoch  1, batch     3 | loss: 19.2001518Losses:  20.25284379348159 5.371735095977783 5.165670871734619 1.868469174951315
CurrentTrain: epoch  1, batch     4 | loss: 20.2528438Losses:  18.118658304214478 5.040620803833008 5.071789741516113 1.403026819229126
CurrentTrain: epoch  1, batch     5 | loss: 18.1186583Losses:  17.9522762298584 5.129602909088135 5.047142505645752 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 17.9522762Losses:  19.465929355472326 5.077313423156738 4.954164028167725 1.739490833133459
CurrentTrain: epoch  1, batch     7 | loss: 19.4659294Losses:  18.950625700876117 5.143013954162598 4.977872848510742 1.5570500325411558
CurrentTrain: epoch  1, batch     8 | loss: 18.9506257Losses:  23.779108997434378 5.215460777282715 5.012684345245361 6.614405628293753
CurrentTrain: epoch  1, batch     9 | loss: 23.7791090Losses:  23.30603301152587 5.158920764923096 4.920319557189941 5.09097945317626
CurrentTrain: epoch  1, batch    10 | loss: 23.3060330Losses:  20.810432516038418 5.169913291931152 5.0899457931518555 3.1835614070296288
CurrentTrain: epoch  1, batch    11 | loss: 20.8104325Losses:  20.195070177316666 5.050114631652832 5.013261795043945 2.837837129831314
CurrentTrain: epoch  1, batch    12 | loss: 20.1950702Losses:  17.07127571105957 5.031219482421875 5.052966117858887 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 17.0712757Losses:  21.870894342660904 5.207339286804199 4.921332836151123 3.571444422006607
CurrentTrain: epoch  1, batch    14 | loss: 21.8708943Losses:  17.467132568359375 5.104262828826904 4.9408345222473145 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 17.4671326Losses:  21.450125221163034 5.077906608581543 5.005784034729004 3.09850550070405
CurrentTrain: epoch  1, batch    16 | loss: 21.4501252Losses:  22.667797841131687 5.066934585571289 4.976340293884277 4.396702565252781
CurrentTrain: epoch  1, batch    17 | loss: 22.6677978Losses:  23.11331774108112 4.93440055847168 5.135599613189697 6.01237322203815
CurrentTrain: epoch  1, batch    18 | loss: 23.1133177Losses:  17.61666488647461 5.170173645019531 5.040934085845947 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 17.6166649Losses:  17.263530731201172 4.993023872375488 5.141779899597168 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 17.2635307Losses:  27.919992074370384 5.236276149749756 5.005446434020996 9.666219338774681
CurrentTrain: epoch  1, batch    21 | loss: 27.9199921Losses:  17.842580795288086 5.127407550811768 5.126665115356445 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 17.8425808Losses:  17.039905548095703 5.097000598907471 5.071539878845215 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 17.0399055Losses:  20.32873672991991 5.003603935241699 5.019528388977051 4.347731061279774
CurrentTrain: epoch  1, batch    24 | loss: 20.3287367Losses:  19.081105563789606 5.039196491241455 5.113779067993164 2.867839191108942
CurrentTrain: epoch  1, batch    25 | loss: 19.0811056Losses:  17.841293334960938 5.078938007354736 4.9997711181640625 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 17.8412933Losses:  17.307010650634766 5.144188404083252 4.976266384124756 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 17.3070107Losses:  17.67613983154297 5.139513969421387 5.069072723388672 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 17.6761398Losses:  21.36628609150648 5.0339765548706055 5.026368141174316 4.327914051711559
CurrentTrain: epoch  1, batch    29 | loss: 21.3662861Losses:  21.54220701381564 5.22816276550293 5.108748912811279 3.43483854457736
CurrentTrain: epoch  1, batch    30 | loss: 21.5422070Losses:  21.169384509325027 5.09536075592041 5.108785629272461 3.2140832245349884
CurrentTrain: epoch  1, batch    31 | loss: 21.1693845Losses:  21.727127835154533 5.065252304077148 5.128026008605957 4.035565182566643
CurrentTrain: epoch  1, batch    32 | loss: 21.7271278Losses:  16.648967742919922 4.977076530456543 4.942995071411133 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 16.6489677Losses:  16.48992347717285 5.0216827392578125 5.039359092712402 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 16.4899235Losses:  24.97525917738676 5.364007472991943 5.0957536697387695 6.707654349505901
CurrentTrain: epoch  1, batch    35 | loss: 24.9752592Losses:  19.218011498451233 5.135880470275879 4.969152927398682 1.6479755640029907
CurrentTrain: epoch  1, batch    36 | loss: 19.2180115Losses:  17.82350341230631 4.833215713500977 5.02005672454834 1.7059077396988869
CurrentTrain: epoch  1, batch    37 | loss: 17.8235034Losses:  19.296367079019547 4.972516059875488 5.03623104095459 2.9918245375156403
CurrentTrain: epoch  1, batch    38 | loss: 19.2963671Losses:  20.959395870566368 5.068570613861084 5.0 4.628081783652306
CurrentTrain: epoch  1, batch    39 | loss: 20.9593959Losses:  18.285419825464487 5.0142645835876465 5.077711582183838 1.449634913355112
CurrentTrain: epoch  1, batch    40 | loss: 18.2854198Losses:  19.44001254066825 4.9772491455078125 5.0 2.9590211771428585
CurrentTrain: epoch  1, batch    41 | loss: 19.4400125Losses:  18.8812837600708 5.1006903648376465 5.0 1.833073616027832
CurrentTrain: epoch  1, batch    42 | loss: 18.8812838Losses:  21.56482996046543 5.042171478271484 5.065880298614502 4.62349046766758
CurrentTrain: epoch  1, batch    43 | loss: 21.5648300Losses:  21.828251222148538 5.091957092285156 5.1360063552856445 3.844177583232522
CurrentTrain: epoch  1, batch    44 | loss: 21.8282512Losses:  20.493676401674747 5.1443305015563965 5.0396013259887695 2.9989750161767006
CurrentTrain: epoch  1, batch    45 | loss: 20.4936764Losses:  17.908737182617188 5.219794273376465 5.082623481750488 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 17.9087372Losses:  17.17241096496582 5.140625953674316 4.963342189788818 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 17.1724110Losses:  18.633013665676117 4.997358798980713 4.966022491455078 1.4185819029808044
CurrentTrain: epoch  1, batch    48 | loss: 18.6330137Losses:  26.76589147746563 5.029531955718994 5.010441780090332 9.343123838305473
CurrentTrain: epoch  1, batch    49 | loss: 26.7658915Losses:  18.69087028875947 4.930543422698975 4.991815567016602 1.430158618837595
CurrentTrain: epoch  1, batch    50 | loss: 18.6908703Losses:  17.285524368286133 5.233931541442871 5.024474620819092 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 17.2855244Losses:  23.43646178767085 5.135013580322266 4.984826564788818 6.524646144360304
CurrentTrain: epoch  1, batch    52 | loss: 23.4364618Losses:  16.084447860717773 5.017294406890869 4.981916427612305 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 16.0844479Losses:  19.418784201145172 4.868188858032227 4.990607261657715 3.123801290988922
CurrentTrain: epoch  1, batch    54 | loss: 19.4187842Losses:  19.439188674092293 5.002936840057373 5.026058197021484 2.932257369160652
CurrentTrain: epoch  1, batch    55 | loss: 19.4391887Losses:  17.02295684814453 5.084958076477051 5.03303337097168 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 17.0229568Losses:  17.401742935180664 5.110014915466309 5.016849994659424 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 17.4017429Losses:  16.539939880371094 5.001646518707275 5.1394782066345215 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 16.5399399Losses:  20.656332593411207 5.074814796447754 5.046915531158447 3.209568601101637
CurrentTrain: epoch  1, batch    59 | loss: 20.6563326Losses:  16.62523078918457 5.083158016204834 5.040727615356445 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 16.6252308Losses:  18.372825622558594 5.187448978424072 5.091187477111816 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 18.3728256Losses:  19.089780151844025 4.8987717628479 5.0 4.315230667591095
CurrentTrain: epoch  1, batch    62 | loss: 19.0897802Losses:  15.484827041625977 4.916032314300537 5.062019348144531 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 15.4848270Losses:  16.30294418334961 5.008403778076172 5.0 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 16.3029442Losses:  16.92696237564087 4.7932233810424805 5.005133152008057 1.4156136512756348
CurrentTrain: epoch  2, batch     2 | loss: 16.9269624Losses:  21.99360677599907 5.240591526031494 5.166565895080566 3.225971430540085
CurrentTrain: epoch  2, batch     3 | loss: 21.9936068Losses:  18.9230813421309 5.016181945800781 5.0011796951293945 1.545582715421915
CurrentTrain: epoch  2, batch     4 | loss: 18.9230813Losses:  18.957852691411972 4.937981605529785 5.0 2.0969689786434174
CurrentTrain: epoch  2, batch     5 | loss: 18.9578527Losses:  16.655132293701172 5.1259918212890625 5.033751487731934 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 16.6551323Losses:  21.423510689288378 4.982285022735596 5.003789901733398 4.876456398516893
CurrentTrain: epoch  2, batch     7 | loss: 21.4235107Losses:  19.74840433895588 4.966702461242676 5.0011396408081055 3.358208492398262
CurrentTrain: epoch  2, batch     8 | loss: 19.7484043Losses:  20.74688769876957 5.090003490447998 5.01346492767334 3.7186703830957413
CurrentTrain: epoch  2, batch     9 | loss: 20.7468877Losses:  17.65064386278391 4.900888919830322 4.970332145690918 1.5089774504303932
CurrentTrain: epoch  2, batch    10 | loss: 17.6506439Losses:  17.01303905621171 4.739812850952148 5.0 1.4897808469831944
CurrentTrain: epoch  2, batch    11 | loss: 17.0130391Losses:  22.73152704536915 5.1227335929870605 4.953803062438965 5.986976340413094
CurrentTrain: epoch  2, batch    12 | loss: 22.7315270Losses:  17.929164975881577 4.908224105834961 5.090531349182129 1.4698868691921234
CurrentTrain: epoch  2, batch    13 | loss: 17.9291650Losses:  20.404548563063145 5.207995891571045 5.0928754806518555 2.9226149693131447
CurrentTrain: epoch  2, batch    14 | loss: 20.4045486Losses:  18.60005458816886 4.9615159034729 4.967347145080566 2.178835716098547
CurrentTrain: epoch  2, batch    15 | loss: 18.6000546Losses:  19.454686038196087 4.866390228271484 5.014351844787598 2.877056948840618
CurrentTrain: epoch  2, batch    16 | loss: 19.4546860Losses:  19.356591023504734 4.968192100524902 5.0 2.9242542162537575
CurrentTrain: epoch  2, batch    17 | loss: 19.3565910Losses:  26.6049745939672 4.820087432861328 5.0 11.328918304294348
CurrentTrain: epoch  2, batch    18 | loss: 26.6049746Losses:  18.665078116580844 5.004427909851074 4.974085807800293 1.7743043433874846
CurrentTrain: epoch  2, batch    19 | loss: 18.6650781Losses:  15.819799423217773 4.976189136505127 5.0 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 15.8197994Losses:  16.435321807861328 4.961406707763672 5.0 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 16.4353218Losses:  20.713164933025837 5.056975364685059 4.989612102508545 4.340863831341267
CurrentTrain: epoch  2, batch    22 | loss: 20.7131649Losses:  18.23478451371193 4.743566036224365 4.977025508880615 1.563329130411148
CurrentTrain: epoch  2, batch    23 | loss: 18.2347845Losses:  16.813008908182383 4.846792697906494 4.99161434173584 1.451315525919199
CurrentTrain: epoch  2, batch    24 | loss: 16.8130089Losses:  20.869941782206297 5.033814430236816 5.0 4.6886006109416485
CurrentTrain: epoch  2, batch    25 | loss: 20.8699418Losses:  17.7132206261158 4.835834503173828 4.983065605163574 1.4097214043140411
CurrentTrain: epoch  2, batch    26 | loss: 17.7132206Losses:  17.718657791614532 4.790127754211426 4.984061241149902 1.7903397679328918
CurrentTrain: epoch  2, batch    27 | loss: 17.7186578Losses:  17.439423352479935 4.9190287590026855 5.048794746398926 1.417996197938919
CurrentTrain: epoch  2, batch    28 | loss: 17.4394234Losses:  27.657358463853598 5.048551559448242 4.962334156036377 11.570135410875082
CurrentTrain: epoch  2, batch    29 | loss: 27.6573585Losses:  17.64725958928466 4.975712776184082 5.063995838165283 1.4396198950707912
CurrentTrain: epoch  2, batch    30 | loss: 17.6472596Losses:  17.144255578517914 4.79097843170166 5.0 1.5100345015525818
CurrentTrain: epoch  2, batch    31 | loss: 17.1442556Losses:  18.52186368778348 4.729841709136963 4.974812984466553 2.9227769263088703
CurrentTrain: epoch  2, batch    32 | loss: 18.5218637Losses:  19.24814524501562 4.915646553039551 5.042438507080078 2.110158108174801
CurrentTrain: epoch  2, batch    33 | loss: 19.2481452Losses:  21.78597391024232 4.9659247398376465 5.061058044433594 5.625939730554819
CurrentTrain: epoch  2, batch    34 | loss: 21.7859739Losses:  23.011627197265625 4.926837921142578 4.98154878616333 5.893512725830078
CurrentTrain: epoch  2, batch    35 | loss: 23.0116272Losses:  19.036578871309757 4.754983425140381 5.045929908752441 2.8665501847863197
CurrentTrain: epoch  2, batch    36 | loss: 19.0365789Losses:  18.574707988649607 4.84006404876709 5.01556396484375 1.8360395468771458
CurrentTrain: epoch  2, batch    37 | loss: 18.5747080Losses:  17.407569855451584 4.84166955947876 4.9989237785339355 1.4063205420970917
CurrentTrain: epoch  2, batch    38 | loss: 17.4075699Losses:  20.113699197769165 4.889056205749512 5.0 3.371704339981079
CurrentTrain: epoch  2, batch    39 | loss: 20.1136992Losses:  19.911510217934847 4.7454833984375 5.0 4.371316660195589
CurrentTrain: epoch  2, batch    40 | loss: 19.9115102Losses:  19.036417059600353 4.927907466888428 5.099995136260986 1.463289313018322
CurrentTrain: epoch  2, batch    41 | loss: 19.0364171Losses:  17.02936327457428 4.6833038330078125 4.982507705688477 1.4086605310440063
CurrentTrain: epoch  2, batch    42 | loss: 17.0293633Losses:  16.56048583984375 5.005277156829834 5.0 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 16.5604858Losses:  16.83015087619424 4.690910339355469 4.965238094329834 1.4455206729471684
CurrentTrain: epoch  2, batch    44 | loss: 16.8301509Losses:  19.21178227290511 4.903203010559082 5.023744106292725 2.958070572465658
CurrentTrain: epoch  2, batch    45 | loss: 19.2117823Losses:  15.547916412353516 4.70745849609375 4.986048221588135 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 15.5479164Losses:  16.35865592956543 4.9222283363342285 5.048149108886719 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 16.3586559Losses:  15.605256080627441 4.945541858673096 5.023371696472168 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 15.6052561Losses:  18.04194601997733 4.960695266723633 5.0024566650390625 1.4499755762517452
CurrentTrain: epoch  2, batch    49 | loss: 18.0419460Losses:  24.692646920681 4.735721588134766 5.0 8.550511300563812
CurrentTrain: epoch  2, batch    50 | loss: 24.6926469Losses:  18.78460130468011 4.81660270690918 5.0 3.4152642227709293
CurrentTrain: epoch  2, batch    51 | loss: 18.7846013Losses:  16.870969094336033 4.573395729064941 4.988374710083008 1.6529496088624
CurrentTrain: epoch  2, batch    52 | loss: 16.8709691Losses:  18.39089436084032 4.713819980621338 5.014702796936035 2.8561529591679573
CurrentTrain: epoch  2, batch    53 | loss: 18.3908944Losses:  17.33342045918107 4.665681838989258 4.99046516418457 1.4336850084364414
CurrentTrain: epoch  2, batch    54 | loss: 17.3334205Losses:  18.292756844311953 4.833400726318359 5.0 2.96742420271039
CurrentTrain: epoch  2, batch    55 | loss: 18.2927568Losses:  17.365302857011557 4.772651672363281 5.041875839233398 1.8234975896775723
CurrentTrain: epoch  2, batch    56 | loss: 17.3653029Losses:  16.330318450927734 4.77938175201416 5.011282444000244 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 16.3303185Losses:  15.763190269470215 4.670558452606201 5.000622749328613 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 15.7631903Losses:  27.408379092812538 4.788219451904297 5.0 11.932096973061562
CurrentTrain: epoch  2, batch    59 | loss: 27.4083791Losses:  15.804361343383789 4.726454734802246 5.017362594604492 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 15.8043613Losses:  18.95927844569087 4.961822986602783 4.98289680480957 1.9906924776732922
CurrentTrain: epoch  2, batch    61 | loss: 18.9592784Losses:  15.354649543762207 4.6935343742370605 4.97176456451416 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 15.3546495Losses:  16.885144982486963 4.90774393081665 5.0 1.4139478541910648
CurrentTrain: epoch  3, batch     0 | loss: 16.8851450Losses:  16.93640375137329 4.827713966369629 4.971144199371338 1.418907642364502
CurrentTrain: epoch  3, batch     1 | loss: 16.9364038Losses:  15.807738304138184 4.730414390563965 4.996442794799805 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 15.8077383Losses:  20.246263455599546 4.92117166519165 4.98762845993042 4.355642270296812
CurrentTrain: epoch  3, batch     3 | loss: 20.2462635Losses:  18.12044843658805 4.580336570739746 5.0 3.383263912051916
CurrentTrain: epoch  3, batch     4 | loss: 18.1204484Losses:  20.282558351755142 4.6736297607421875 5.0 4.365170389413834
CurrentTrain: epoch  3, batch     5 | loss: 20.2825584Losses:  16.752566426992416 4.605600357055664 4.985900402069092 1.3972826898097992
CurrentTrain: epoch  3, batch     6 | loss: 16.7525664Losses:  19.748242519795895 4.70130729675293 5.025391578674316 4.263918064534664
CurrentTrain: epoch  3, batch     7 | loss: 19.7482425Losses:  16.734544210135937 4.680830001831055 5.0041985511779785 1.4468607231974602
CurrentTrain: epoch  3, batch     8 | loss: 16.7345442Losses:  16.13422203063965 4.79887580871582 5.066177845001221 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 16.1342220Losses:  15.56191635131836 4.774000644683838 5.0 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 15.5619164Losses:  18.191373225301504 4.608556747436523 5.0 2.946274157613516
CurrentTrain: epoch  3, batch    11 | loss: 18.1913732Losses:  19.701790004968643 4.980791091918945 5.035227298736572 4.222137600183487
CurrentTrain: epoch  3, batch    12 | loss: 19.7017900Losses:  19.85490758344531 5.031163215637207 4.996549606323242 2.9379344694316387
CurrentTrain: epoch  3, batch    13 | loss: 19.8549076Losses:  26.817046836018562 4.83656644821167 5.0 10.741660788655281
CurrentTrain: epoch  3, batch    14 | loss: 26.8170468Losses:  17.56091007962823 4.906765937805176 5.094938278198242 1.415364120155573
CurrentTrain: epoch  3, batch    15 | loss: 17.5609101Losses:  21.913110122084618 4.72514533996582 5.028571128845215 6.066587790846825
CurrentTrain: epoch  3, batch    16 | loss: 21.9131101Losses:  16.702770050615072 4.798713684082031 5.052980422973633 1.452381905168295
CurrentTrain: epoch  3, batch    17 | loss: 16.7027701Losses:  19.358287632465363 4.448884010314941 5.0 4.413417637348175
CurrentTrain: epoch  3, batch    18 | loss: 19.3582876Losses:  16.100896384567022 4.601320743560791 5.0 1.5592608712613583
CurrentTrain: epoch  3, batch    19 | loss: 16.1008964Losses:  18.97754391655326 4.668447971343994 4.991520404815674 4.255482759326696
CurrentTrain: epoch  3, batch    20 | loss: 18.9775439Losses:  18.227495670318604 4.318210124969482 5.0 3.396214008331299
CurrentTrain: epoch  3, batch    21 | loss: 18.2274957Losses:  22.205529987812042 4.824771881103516 5.0254597663879395 6.189182102680206
CurrentTrain: epoch  3, batch    22 | loss: 22.2055300Losses:  17.261359065771103 4.5218825340271 5.071530342102051 1.4367121160030365
CurrentTrain: epoch  3, batch    23 | loss: 17.2613591Losses:  17.412542201578617 4.7713751792907715 5.0451555252075195 1.4767635837197304
CurrentTrain: epoch  3, batch    24 | loss: 17.4125422Losses:  18.950775928795338 4.796711444854736 4.975644111633301 2.9126404002308846
CurrentTrain: epoch  3, batch    25 | loss: 18.9507759Losses:  17.559977643191814 4.635931015014648 5.0 2.9126206561923027
CurrentTrain: epoch  3, batch    26 | loss: 17.5599776Losses:  15.942048661410809 4.410091400146484 5.0 1.4664932414889336
CurrentTrain: epoch  3, batch    27 | loss: 15.9420487Losses:  20.7623790204525 4.770504951477051 4.990069389343262 4.353029578924179
CurrentTrain: epoch  3, batch    28 | loss: 20.7623790Losses:  20.782527022063732 4.557271480560303 5.0 5.727292113006115
CurrentTrain: epoch  3, batch    29 | loss: 20.7825270Losses:  15.015044212341309 4.587571620941162 5.035611152648926 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 15.0150442Losses:  14.862100601196289 4.577284812927246 4.989171504974365 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 14.8621006Losses:  20.557529516518116 4.457070827484131 4.993622779846191 5.767985410988331
CurrentTrain: epoch  3, batch    32 | loss: 20.5575295Losses:  18.549252476543188 4.804917812347412 5.0 2.976911511272192
CurrentTrain: epoch  3, batch    33 | loss: 18.5492525Losses:  15.580057144165039 4.635298728942871 5.051052093505859 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 15.5800571Losses:  17.635219553485513 4.7528228759765625 4.980896949768066 1.5664253029972315
CurrentTrain: epoch  3, batch    35 | loss: 17.6352196Losses:  14.44744873046875 4.3558149337768555 5.0 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 14.4474487Losses:  14.933014869689941 4.5822906494140625 4.982447624206543 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 14.9330149Losses:  17.903863489627838 4.700931072235107 5.0 2.8077083230018616
CurrentTrain: epoch  3, batch    38 | loss: 17.9038635Losses:  19.459666214883327 4.657258033752441 5.0 4.313788376748562
CurrentTrain: epoch  3, batch    39 | loss: 19.4596662Losses:  24.65558474883437 4.696678161621094 5.0616559982299805 7.510139878839254
CurrentTrain: epoch  3, batch    40 | loss: 24.6555847Losses:  15.74816608428955 4.489788055419922 5.000850677490234 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 15.7481661Losses:  15.831031829118729 4.482661724090576 5.0 1.4062252342700958
CurrentTrain: epoch  3, batch    42 | loss: 15.8310318Losses:  16.99811651557684 4.673635482788086 5.017735481262207 1.8204088434576988
CurrentTrain: epoch  3, batch    43 | loss: 16.9981165Losses:  18.696562759578228 4.547834396362305 4.983580589294434 3.080385200679302
CurrentTrain: epoch  3, batch    44 | loss: 18.6965628Losses:  17.95116137713194 4.39753532409668 5.0 3.1188268586993217
CurrentTrain: epoch  3, batch    45 | loss: 17.9511614Losses:  19.322905886918306 4.668245792388916 5.0 3.438144076615572
CurrentTrain: epoch  3, batch    46 | loss: 19.3229059Losses:  17.372330520302057 4.414565086364746 5.0 2.848246429115534
CurrentTrain: epoch  3, batch    47 | loss: 17.3723305Losses:  15.213172912597656 4.732791900634766 5.0 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 15.2131729Losses:  25.99303689226508 4.555877208709717 4.9692182540893555 10.63084664568305
CurrentTrain: epoch  3, batch    49 | loss: 25.9930369Losses:  15.590054512023926 4.529431343078613 5.036850929260254 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 15.5900545Losses:  16.393168717622757 4.261676788330078 5.04686164855957 1.4257910549640656
CurrentTrain: epoch  3, batch    51 | loss: 16.3931687Losses:  19.0517956353724 4.5261335372924805 5.0 4.335636768490076
CurrentTrain: epoch  3, batch    52 | loss: 19.0517956Losses:  20.67600914463401 4.836336612701416 5.100305557250977 4.354773487895727
CurrentTrain: epoch  3, batch    53 | loss: 20.6760091Losses:  15.849756896495819 4.454334259033203 5.0 1.400711715221405
CurrentTrain: epoch  3, batch    54 | loss: 15.8497569Losses:  17.44483022764325 4.195706844329834 5.0 2.839334774762392
CurrentTrain: epoch  3, batch    55 | loss: 17.4448302Losses:  14.525710105895996 4.396032333374023 5.0 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 14.5257101Losses:  15.163570404052734 4.461833953857422 5.05401086807251 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 15.1635704Losses:  19.6950740814209 4.3007307052612305 5.0 5.281388282775879
CurrentTrain: epoch  3, batch    58 | loss: 19.6950741Losses:  24.96619851142168 4.431931495666504 4.964158058166504 9.619696207344532
CurrentTrain: epoch  3, batch    59 | loss: 24.9661985Losses:  15.457515716552734 4.484357833862305 5.02009391784668 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 15.4575157Losses:  19.914378434419632 4.446913719177246 4.971090316772461 4.675668030977249
CurrentTrain: epoch  3, batch    61 | loss: 19.9143784Losses:  15.89764142036438 4.280518531799316 5.0 1.4006292819976807
CurrentTrain: epoch  3, batch    62 | loss: 15.8976414Losses:  19.529769711196423 4.609789848327637 5.034233093261719 4.32313709706068
CurrentTrain: epoch  4, batch     0 | loss: 19.5297697Losses:  20.02909417450428 4.452376365661621 5.075325012207031 4.95867295563221
CurrentTrain: epoch  4, batch     1 | loss: 20.0290942Losses:  14.50692367553711 4.617103576660156 5.0 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 14.5069237Losses:  18.09099105000496 4.249475479125977 5.0 4.20755198597908
CurrentTrain: epoch  4, batch     3 | loss: 18.0909911Losses:  23.46956780552864 4.546141147613525 5.018852233886719 8.587522059679031
CurrentTrain: epoch  4, batch     4 | loss: 23.4695678Losses:  15.883685022592545 4.256904125213623 5.0375823974609375 1.4086121618747711
CurrentTrain: epoch  4, batch     5 | loss: 15.8836850Losses:  16.085339173674583 4.569828033447266 5.0 1.4789777845144272
CurrentTrain: epoch  4, batch     6 | loss: 16.0853392Losses:  13.811777114868164 4.160109996795654 5.0 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 13.8117771Losses:  18.33436619862914 4.505777359008789 5.0 3.014288302510977
CurrentTrain: epoch  4, batch     8 | loss: 18.3343662Losses:  16.925025925040245 4.400729656219482 5.036648273468018 1.5354270786046982
CurrentTrain: epoch  4, batch     9 | loss: 16.9250259Losses:  19.46939269825816 4.676158428192139 5.0 4.415004651993513
CurrentTrain: epoch  4, batch    10 | loss: 19.4693927Losses:  18.12740632519126 4.508353233337402 5.0 2.9682924412190914
CurrentTrain: epoch  4, batch    11 | loss: 18.1274063Losses:  16.28422823548317 4.370974540710449 5.0 1.4399975836277008
CurrentTrain: epoch  4, batch    12 | loss: 16.2842282Losses:  14.896273612976074 4.481988906860352 5.083291053771973 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 14.8962736Losses:  16.401477478444576 4.221589088439941 5.0098114013671875 1.4414650425314903
CurrentTrain: epoch  4, batch    14 | loss: 16.4014775Losses:  16.4803124666214 4.523093223571777 5.002652168273926 1.421071171760559
CurrentTrain: epoch  4, batch    15 | loss: 16.4803125Losses:  18.790207328274846 4.617554664611816 4.999540328979492 3.295956077054143
CurrentTrain: epoch  4, batch    16 | loss: 18.7902073Losses:  14.319520950317383 4.248505592346191 5.0 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 14.3195210Losses:  19.0845964923501 4.321318626403809 5.0 4.485366679728031
CurrentTrain: epoch  4, batch    18 | loss: 19.0845965Losses:  23.557411156594753 4.486525058746338 5.0 8.682953797280788
CurrentTrain: epoch  4, batch    19 | loss: 23.5574112Losses:  18.068967550992966 4.351286888122559 5.07999324798584 2.8907229602336884
CurrentTrain: epoch  4, batch    20 | loss: 18.0689676Losses:  18.015813436359167 4.724633693695068 5.061691761016846 2.8644634149968624
CurrentTrain: epoch  4, batch    21 | loss: 18.0158134Losses:  15.465442895889282 3.9949769973754883 5.0 1.449749231338501
CurrentTrain: epoch  4, batch    22 | loss: 15.4654429Losses:  21.409335277974606 4.362989902496338 4.989234447479248 7.040419720113277
CurrentTrain: epoch  4, batch    23 | loss: 21.4093353Losses:  15.773340411484241 4.282735824584961 5.048794746398926 1.4633991196751595
CurrentTrain: epoch  4, batch    24 | loss: 15.7733404Losses:  15.403997421264648 4.45648193359375 4.99166202545166 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 15.4039974Losses:  15.684583872556686 4.445377349853516 5.0 1.4367572963237762
CurrentTrain: epoch  4, batch    26 | loss: 15.6845839Losses:  17.963065534830093 4.616742134094238 5.064089775085449 2.2383998930454254
CurrentTrain: epoch  4, batch    27 | loss: 17.9630655Losses:  16.24718627333641 4.358663082122803 5.0 1.4074264466762543
CurrentTrain: epoch  4, batch    28 | loss: 16.2471863Losses:  20.844202991575003 4.569145202636719 5.010021209716797 5.922381397336721
CurrentTrain: epoch  4, batch    29 | loss: 20.8442030Losses:  20.237811967730522 4.375516891479492 5.0 5.884940072894096
CurrentTrain: epoch  4, batch    30 | loss: 20.2378120Losses:  14.739559173583984 4.582282066345215 5.0 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 14.7395592Losses:  16.673609402030706 4.056264877319336 5.0 2.859580662101507
CurrentTrain: epoch  4, batch    32 | loss: 16.6736094Losses:  19.269034504890442 4.3108720779418945 5.0 4.999861836433411
CurrentTrain: epoch  4, batch    33 | loss: 19.2690345Losses:  13.932022094726562 4.068026542663574 5.0 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 13.9320221Losses:  15.974064469337463 4.224101543426514 4.988979339599609 1.4108186960220337
CurrentTrain: epoch  4, batch    35 | loss: 15.9740645Losses:  19.51628290861845 4.3568267822265625 5.0 4.928146235644817
CurrentTrain: epoch  4, batch    36 | loss: 19.5162829Losses:  18.299248283728957 4.191391944885254 5.0262451171875 3.472326820716262
CurrentTrain: epoch  4, batch    37 | loss: 18.2992483Losses:  14.212884902954102 4.317955017089844 5.0 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 14.2128849Losses:  18.709412336349487 4.269885063171387 5.0 4.406174421310425
CurrentTrain: epoch  4, batch    39 | loss: 18.7094123Losses:  19.10856582596898 4.345067024230957 5.0 4.469254035502672
CurrentTrain: epoch  4, batch    40 | loss: 19.1085658Losses:  14.433268547058105 4.504182815551758 5.0 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 14.4332685Losses:  20.31911999359727 4.0943684577941895 5.0 6.119805876165628
CurrentTrain: epoch  4, batch    42 | loss: 20.3191200Losses:  17.378959827125072 4.659674644470215 5.0 2.883154086768627
CurrentTrain: epoch  4, batch    43 | loss: 17.3789598Losses:  14.39914321899414 4.133978843688965 5.0 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 14.3991432Losses:  16.02672066912055 4.354737758636475 5.033116817474365 1.5064264796674252
CurrentTrain: epoch  4, batch    45 | loss: 16.0267207Losses:  18.447752613574266 4.676150321960449 5.020030975341797 2.958097118884325
CurrentTrain: epoch  4, batch    46 | loss: 18.4477526Losses:  15.82023087516427 4.167264938354492 5.041058540344238 1.7019380666315556
CurrentTrain: epoch  4, batch    47 | loss: 15.8202309Losses:  20.79298498481512 4.538601875305176 4.989869117736816 5.785216353833675
CurrentTrain: epoch  4, batch    48 | loss: 20.7929850Losses:  14.211955070495605 4.2991437911987305 5.0 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 14.2119551Losses:  17.247223272919655 4.2878570556640625 5.0 3.022091284394264
CurrentTrain: epoch  4, batch    50 | loss: 17.2472233Losses:  14.165440559387207 4.431004524230957 5.0 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 14.1654406Losses:  18.253379222005606 4.223851203918457 5.0 4.284292574971914
CurrentTrain: epoch  4, batch    52 | loss: 18.2533792Losses:  18.007454752922058 4.6363325119018555 5.054197311401367 1.4895447492599487
CurrentTrain: epoch  4, batch    53 | loss: 18.0074548Losses:  15.16364672780037 4.25710391998291 5.0 1.40247443318367
CurrentTrain: epoch  4, batch    54 | loss: 15.1636467Losses:  14.999217987060547 4.38201904296875 5.031311511993408 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 14.9992180Losses:  25.4069798104465 4.153295516967773 5.0 11.100280057638884
CurrentTrain: epoch  4, batch    56 | loss: 25.4069798Losses:  18.940025582909584 4.5345258712768555 5.0414228439331055 4.397353425621986
CurrentTrain: epoch  4, batch    57 | loss: 18.9400256Losses:  17.667656883597374 4.137385368347168 5.01772403717041 3.03361414372921
CurrentTrain: epoch  4, batch    58 | loss: 17.6676569Losses:  17.249760277569294 4.332194805145264 5.0 2.951103813946247
CurrentTrain: epoch  4, batch    59 | loss: 17.2497603Losses:  15.781398266553879 4.36494255065918 5.0 1.4133648574352264
CurrentTrain: epoch  4, batch    60 | loss: 15.7813983Losses:  15.55285394191742 3.957023859024048 5.0 1.4411910772323608
CurrentTrain: epoch  4, batch    61 | loss: 15.5528539Losses:  19.259948045015335 3.8147356510162354 5.0 5.917324334383011
CurrentTrain: epoch  4, batch    62 | loss: 19.2599480Losses:  14.216777801513672 3.9440393447875977 5.0197882652282715 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 14.2167778Losses:  16.954000025987625 4.143383979797363 5.0 2.8725724518299103
CurrentTrain: epoch  5, batch     1 | loss: 16.9540000Losses:  17.361262563616037 4.449833869934082 5.0 2.8775122202932835
CurrentTrain: epoch  5, batch     2 | loss: 17.3612626Losses:  17.430562790483236 4.223964691162109 5.0 2.9462965093553066
CurrentTrain: epoch  5, batch     3 | loss: 17.4305628Losses:  18.31030422076583 4.121337890625 5.0 4.273161467164755
CurrentTrain: epoch  5, batch     4 | loss: 18.3103042Losses:  20.11606014892459 4.5338134765625 5.0 5.855647932738066
CurrentTrain: epoch  5, batch     5 | loss: 20.1160601Losses:  14.193498611450195 4.036562919616699 5.0 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 14.1934986Losses:  15.510668992996216 4.08450174331665 5.0 1.4141342639923096
CurrentTrain: epoch  5, batch     7 | loss: 15.5106690Losses:  19.252053756266832 4.503552436828613 5.002856731414795 4.369466323405504
CurrentTrain: epoch  5, batch     8 | loss: 19.2520538Losses:  20.37030439451337 4.164534091949463 5.098939895629883 6.067589092999697
CurrentTrain: epoch  5, batch     9 | loss: 20.3703044Losses:  17.385812252759933 4.1576247215271 5.014936923980713 2.8007712066173553
CurrentTrain: epoch  5, batch    10 | loss: 17.3858123Losses:  37.01515507698059 4.351936340332031 5.033450126647949 22.70037007331848
CurrentTrain: epoch  5, batch    11 | loss: 37.0151551Losses:  16.865398675203323 4.043719291687012 5.0 2.8143952190876007
CurrentTrain: epoch  5, batch    12 | loss: 16.8653987Losses:  15.369148701429367 4.138330459594727 5.059396743774414 1.3973660171031952
CurrentTrain: epoch  5, batch    13 | loss: 15.3691487Losses:  15.803404852747917 4.102910995483398 5.0 1.662227675318718
CurrentTrain: epoch  5, batch    14 | loss: 15.8034049Losses:  16.064742114394903 4.352968215942383 5.0 1.4380073808133602
CurrentTrain: epoch  5, batch    15 | loss: 16.0647421Losses:  15.58543148636818 4.255426406860352 5.036218643188477 1.4207738935947418
CurrentTrain: epoch  5, batch    16 | loss: 15.5854315Losses:  17.536194801330566 4.145444869995117 5.0 2.876734733581543
CurrentTrain: epoch  5, batch    17 | loss: 17.5361948Losses:  18.016888339072466 3.785327196121216 5.046557426452637 4.504794795066118
CurrentTrain: epoch  5, batch    18 | loss: 18.0168883Losses:  15.375755041837692 4.2014360427856445 5.023553848266602 1.4215295016765594
CurrentTrain: epoch  5, batch    19 | loss: 15.3757550Losses:  15.970512211322784 3.6715707778930664 5.0 2.7930648922920227
CurrentTrain: epoch  5, batch    20 | loss: 15.9705122Losses:  19.415640376508236 4.145105361938477 5.0 5.864375613629818
CurrentTrain: epoch  5, batch    21 | loss: 19.4156404Losses:  16.61809479817748 4.095528602600098 5.0 3.050883647054434
CurrentTrain: epoch  5, batch    22 | loss: 16.6180948Losses:  18.05157580971718 3.945584535598755 5.0 3.88799586892128
CurrentTrain: epoch  5, batch    23 | loss: 18.0515758Losses:  16.406447548419237 4.070725440979004 5.0 2.9025699086487293
CurrentTrain: epoch  5, batch    24 | loss: 16.4064475Losses:  31.03340220451355 4.438996315002441 5.020138740539551 15.980972051620483
CurrentTrain: epoch  5, batch    25 | loss: 31.0334022Losses:  16.583304569125175 4.002408027648926 5.0 2.920588657259941
CurrentTrain: epoch  5, batch    26 | loss: 16.5833046Losses:  15.921793162822723 4.36328649520874 5.0 1.419223964214325
CurrentTrain: epoch  5, batch    27 | loss: 15.9217932Losses:  14.57633425667882 3.555828094482422 5.0 1.5277607627213001
CurrentTrain: epoch  5, batch    28 | loss: 14.5763343Losses:  18.714552983641624 4.240970611572266 5.0 4.380076512694359
CurrentTrain: epoch  5, batch    29 | loss: 18.7145530Losses:  16.309945100918412 4.118256568908691 5.008018970489502 1.64652537740767
CurrentTrain: epoch  5, batch    30 | loss: 16.3099451Losses:  17.962569564580917 3.793910503387451 5.0 4.27565798163414
CurrentTrain: epoch  5, batch    31 | loss: 17.9625696Losses:  14.785242583602667 3.69191312789917 5.0 1.419731643050909
CurrentTrain: epoch  5, batch    32 | loss: 14.7852426Losses:  30.262475863099098 4.380710601806641 5.097896099090576 15.319568529725075
CurrentTrain: epoch  5, batch    33 | loss: 30.2624759Losses:  17.191709872335196 4.347105026245117 5.0 2.861487742513418
CurrentTrain: epoch  5, batch    34 | loss: 17.1917099Losses:  18.138803593814373 4.188625335693359 5.0 4.263956181704998
CurrentTrain: epoch  5, batch    35 | loss: 18.1388036Losses:  21.06053714081645 4.21461820602417 5.0 6.743015091866255
CurrentTrain: epoch  5, batch    36 | loss: 21.0605371Losses:  15.422665566205978 4.269942283630371 5.0415472984313965 1.4548024833202362
CurrentTrain: epoch  5, batch    37 | loss: 15.4226656Losses:  15.281172692775726 3.9741828441619873 5.052670478820801 1.4482430815696716
CurrentTrain: epoch  5, batch    38 | loss: 15.2811727Losses:  15.436995208263397 4.260953426361084 5.034968376159668 1.4120575785636902
CurrentTrain: epoch  5, batch    39 | loss: 15.4369952Losses:  15.577295571565628 4.426346778869629 5.032445907592773 1.4413111507892609
CurrentTrain: epoch  5, batch    40 | loss: 15.5772956Losses:  14.529969215393066 4.2626471519470215 5.0117998123168945 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 14.5299692Losses:  16.182329773902893 4.329139709472656 5.0 1.5522962808609009
CurrentTrain: epoch  5, batch    42 | loss: 16.1823298Losses:  18.92324509844184 4.4454240798950195 5.05439567565918 4.28574338182807
CurrentTrain: epoch  5, batch    43 | loss: 18.9232451Losses:  14.396242141723633 4.351442813873291 5.08359956741333 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 14.3962421Losses:  13.165191650390625 3.6213583946228027 5.0 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 13.1651917Losses:  16.520607948303223 3.9557478427886963 5.0 3.0467357635498047
CurrentTrain: epoch  5, batch    46 | loss: 16.5206079Losses:  16.078660666942596 3.790998697280884 4.990901947021484 1.4177090525627136
CurrentTrain: epoch  5, batch    47 | loss: 16.0786607Losses:  17.98058757930994 4.463940143585205 5.086635112762451 2.8810258880257607
CurrentTrain: epoch  5, batch    48 | loss: 17.9805876Losses:  15.64539623260498 4.078005790710449 4.996935844421387 1.3955488204956055
CurrentTrain: epoch  5, batch    49 | loss: 15.6453962Losses:  15.80196213722229 4.2946600914001465 5.0 1.408632516860962
CurrentTrain: epoch  5, batch    50 | loss: 15.8019621Losses:  24.47345244139433 3.912808656692505 5.033264636993408 10.851336352527142
CurrentTrain: epoch  5, batch    51 | loss: 24.4734524Losses:  18.521607100963593 4.112556457519531 5.0422492027282715 4.681957900524139
CurrentTrain: epoch  5, batch    52 | loss: 18.5216071Losses:  22.37661723792553 4.046961784362793 5.0 8.643270298838615
CurrentTrain: epoch  5, batch    53 | loss: 22.3766172Losses:  15.831851419061422 4.023096084594727 5.033273696899414 1.7859739623963833
CurrentTrain: epoch  5, batch    54 | loss: 15.8318514Losses:  16.635886162519455 4.1541547775268555 5.0274271965026855 2.818257302045822
CurrentTrain: epoch  5, batch    55 | loss: 16.6358862Losses:  17.2919604703784 4.234668731689453 4.982532024383545 2.838091604411602
CurrentTrain: epoch  5, batch    56 | loss: 17.2919605Losses:  14.737788572907448 3.8735313415527344 5.0 1.444278135895729
CurrentTrain: epoch  5, batch    57 | loss: 14.7377886Losses:  18.860715989023447 4.025469779968262 5.0463690757751465 4.793291214853525
CurrentTrain: epoch  5, batch    58 | loss: 18.8607160Losses:  19.926660031080246 4.117663860321045 5.023620128631592 5.131164044141769
CurrentTrain: epoch  5, batch    59 | loss: 19.9266600Losses:  15.747350338846445 4.411528587341309 5.0 1.5117317475378513
CurrentTrain: epoch  5, batch    60 | loss: 15.7473503Losses:  15.111812498420477 3.938297748565674 5.0 1.4810303710401058
CurrentTrain: epoch  5, batch    61 | loss: 15.1118125Losses:  17.772740185260773 3.708130121231079 5.0 4.248948872089386
CurrentTrain: epoch  5, batch    62 | loss: 17.7727402Losses:  21.84635327383876 4.069114685058594 5.0 7.831016283482313
CurrentTrain: epoch  6, batch     0 | loss: 21.8463533Losses:  16.3474635258317 3.74037504196167 5.0 2.8375453129410744
CurrentTrain: epoch  6, batch     1 | loss: 16.3474635Losses:  15.267629534006119 3.8875246047973633 5.0 1.554060846567154
CurrentTrain: epoch  6, batch     2 | loss: 15.2676295Losses:  15.480605240911245 4.065324306488037 5.0 1.471606370061636
CurrentTrain: epoch  6, batch     3 | loss: 15.4806052Losses:  14.386685401201248 3.488776445388794 5.0 1.402699500322342
CurrentTrain: epoch  6, batch     4 | loss: 14.3866854Losses:  15.630115807056427 3.906301975250244 4.995408058166504 1.5402310490608215
CurrentTrain: epoch  6, batch     5 | loss: 15.6301158Losses:  13.799773216247559 4.031580924987793 5.051481246948242 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 13.7997732Losses:  15.088751997798681 3.9333605766296387 5.035399436950684 1.5152895115315914
CurrentTrain: epoch  6, batch     7 | loss: 15.0887520Losses:  20.491279780864716 4.390078544616699 5.0 6.122474849224091
CurrentTrain: epoch  6, batch     8 | loss: 20.4912798Losses:  16.216114226728678 3.7310640811920166 4.969450950622559 2.950228873640299
CurrentTrain: epoch  6, batch     9 | loss: 16.2161142Losses:  18.69110418856144 4.4891486167907715 5.170049667358398 3.057328477501869
CurrentTrain: epoch  6, batch    10 | loss: 18.6911042Losses:  18.590380812063813 4.394296646118164 5.046408653259277 3.6614991668611765
CurrentTrain: epoch  6, batch    11 | loss: 18.5903808Losses:  15.51774325966835 3.7534613609313965 5.0 1.4489404261112213
CurrentTrain: epoch  6, batch    12 | loss: 15.5177433Losses:  15.773958623409271 4.360254287719727 5.0 1.42092365026474
CurrentTrain: epoch  6, batch    13 | loss: 15.7739586Losses:  14.850496254861355 3.8506264686584473 5.0 1.4290656670928001
CurrentTrain: epoch  6, batch    14 | loss: 14.8504963Losses:  17.49840474128723 4.2196760177612305 5.0 2.88561749458313
CurrentTrain: epoch  6, batch    15 | loss: 17.4984047Losses:  13.573991775512695 3.8550338745117188 5.0 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 13.5739918Losses:  16.74878916144371 4.0878376960754395 5.0 2.8347648084163666
CurrentTrain: epoch  6, batch    17 | loss: 16.7487892Losses:  15.475497480481863 4.018562316894531 5.0 1.5706398449838161
CurrentTrain: epoch  6, batch    18 | loss: 15.4754975Losses:  13.511104583740234 3.8661818504333496 5.0 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 13.5111046Losses:  18.992514688521624 4.049997329711914 5.010128498077393 4.634911615401506
CurrentTrain: epoch  6, batch    20 | loss: 18.9925147Losses:  15.645770732313395 3.9454996585845947 5.0 1.715462390333414
CurrentTrain: epoch  6, batch    21 | loss: 15.6457707Losses:  16.219528287649155 3.890484094619751 5.0 2.8344708383083344
CurrentTrain: epoch  6, batch    22 | loss: 16.2195283Losses:  15.347358524799347 3.7930519580841064 5.0 1.5046480298042297
CurrentTrain: epoch  6, batch    23 | loss: 15.3473585Losses:  18.1285828538239 3.959381103515625 5.0 4.385448355227709
CurrentTrain: epoch  6, batch    24 | loss: 18.1285829Losses:  17.16717354208231 4.285283088684082 5.0 2.9103023186326027
CurrentTrain: epoch  6, batch    25 | loss: 17.1671735Losses:  15.354050278663635 4.062601089477539 5.0 1.41525137424469
CurrentTrain: epoch  6, batch    26 | loss: 15.3540503Losses:  15.021652072668076 3.806365728378296 5.0 1.524099200963974
CurrentTrain: epoch  6, batch    27 | loss: 15.0216521Losses:  18.84007327258587 3.412128448486328 5.0 5.84147422015667
CurrentTrain: epoch  6, batch    28 | loss: 18.8400733Losses:  19.578217651695013 4.045795440673828 5.0 5.711690094321966
CurrentTrain: epoch  6, batch    29 | loss: 19.5782177Losses:  17.521979838609695 3.7183151245117188 5.0 4.213799029588699
CurrentTrain: epoch  6, batch    30 | loss: 17.5219798Losses:  15.107820678502321 3.830817461013794 5.0 1.5406514890491962
CurrentTrain: epoch  6, batch    31 | loss: 15.1078207Losses:  17.02785623818636 4.293676376342773 5.0 2.981378920376301
CurrentTrain: epoch  6, batch    32 | loss: 17.0278562Losses:  14.35471487045288 3.4351234436035156 5.042149543762207 1.3972582817077637
CurrentTrain: epoch  6, batch    33 | loss: 14.3547149Losses:  13.771024703979492 4.1163330078125 5.0 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 13.7710247Losses:  16.910641949623823 3.601861000061035 5.0 3.3878176622092724
CurrentTrain: epoch  6, batch    35 | loss: 16.9106419Losses:  16.772755786776543 3.7752199172973633 5.068356990814209 2.9910003393888474
CurrentTrain: epoch  6, batch    36 | loss: 16.7727558Losses:  13.726217269897461 3.859206199645996 5.0 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 13.7262173Losses:  21.094494879245758 4.058027267456055 5.0 7.568974554538727
CurrentTrain: epoch  6, batch    38 | loss: 21.0944949Losses:  13.464344024658203 3.832977294921875 5.0 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 13.4643440Losses:  13.365228652954102 3.7322661876678467 5.0 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 13.3652287Losses:  14.626855820417404 3.7244906425476074 5.0 1.413600891828537
CurrentTrain: epoch  6, batch    41 | loss: 14.6268558Losses:  16.55559006333351 3.5241024494171143 5.0600199699401855 2.8656906187534332
CurrentTrain: epoch  6, batch    42 | loss: 16.5555901Losses:  17.281190909445286 3.9225380420684814 5.0 3.6360721960663795
CurrentTrain: epoch  6, batch    43 | loss: 17.2811909Losses:  14.735759235918522 3.7350120544433594 5.0 1.5251164212822914
CurrentTrain: epoch  6, batch    44 | loss: 14.7357592Losses:  14.838351160287857 3.9214725494384766 5.0 1.4342192709445953
CurrentTrain: epoch  6, batch    45 | loss: 14.8383512Losses:  17.201716125011444 4.129443168640137 5.031184673309326 2.442867934703827
CurrentTrain: epoch  6, batch    46 | loss: 17.2017161Losses:  16.358345065265894 3.813852310180664 5.0 3.008362803608179
CurrentTrain: epoch  6, batch    47 | loss: 16.3583451Losses:  14.783140279352665 3.781278133392334 5.033855438232422 1.5072556510567665
CurrentTrain: epoch  6, batch    48 | loss: 14.7831403Losses:  15.304854266345501 4.186412811279297 5.0 1.4403294250369072
CurrentTrain: epoch  6, batch    49 | loss: 15.3048543Losses:  15.017586052417755 4.039653778076172 5.050396919250488 1.3986943364143372
CurrentTrain: epoch  6, batch    50 | loss: 15.0175861Losses:  13.382118225097656 3.8884212970733643 5.041717529296875 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 13.3821182Losses:  13.531829833984375 3.970608949661255 5.0 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 13.5318298Losses:  17.259392708539963 3.6168274879455566 5.0 4.264140099287033
CurrentTrain: epoch  6, batch    53 | loss: 17.2593927Losses:  12.886077880859375 3.544116497039795 5.0 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 12.8860779Losses:  17.89102863520384 3.9389824867248535 5.0 4.497353784739971
CurrentTrain: epoch  6, batch    55 | loss: 17.8910286Losses:  14.803299024701118 3.832489252090454 5.0 1.4848032742738724
CurrentTrain: epoch  6, batch    56 | loss: 14.8032990Losses:  16.12887677550316 3.702772617340088 5.0 2.86855611205101
CurrentTrain: epoch  6, batch    57 | loss: 16.1288768Losses:  15.179844830185175 4.11568546295166 5.0 1.44714829698205
CurrentTrain: epoch  6, batch    58 | loss: 15.1798448Losses:  14.806766957044601 3.841301441192627 5.0 1.4567227065563202
CurrentTrain: epoch  6, batch    59 | loss: 14.8067670Losses:  13.122993469238281 3.8010125160217285 5.0 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 13.1229935Losses:  14.796703040599823 4.14258337020874 5.0 1.415660560131073
CurrentTrain: epoch  6, batch    61 | loss: 14.7967030Losses:  17.16604119166732 3.456638813018799 5.0 4.236723717302084
CurrentTrain: epoch  6, batch    62 | loss: 17.1660412Losses:  15.243056148290634 4.226874828338623 5.041977405548096 1.4219597280025482
CurrentTrain: epoch  7, batch     0 | loss: 15.2430561Losses:  16.139961749315262 3.8011937141418457 5.050235748291016 2.817722827196121
CurrentTrain: epoch  7, batch     1 | loss: 16.1399617Losses:  13.403331756591797 3.7706384658813477 5.098355770111084 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 13.4033318Losses:  20.468337304890156 3.3474347591400146 5.047557353973389 7.661122567951679
CurrentTrain: epoch  7, batch     3 | loss: 20.4683373Losses:  20.17024089396 3.3050193786621094 5.0 7.581528201699257
CurrentTrain: epoch  7, batch     4 | loss: 20.1702409Losses:  16.104773968458176 3.9380738735198975 5.0 2.8322968184947968
CurrentTrain: epoch  7, batch     5 | loss: 16.1047740Losses:  13.117586135864258 3.575779438018799 5.040588855743408 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 13.1175861Losses:  19.012282833456993 3.7408154010772705 5.0 5.880171284079552
CurrentTrain: epoch  7, batch     7 | loss: 19.0122828Losses:  14.49464637041092 3.693040370941162 5.0 1.3973696827888489
CurrentTrain: epoch  7, batch     8 | loss: 14.4946464Losses:  14.247441560029984 3.434685468673706 5.0 1.4538901150226593
CurrentTrain: epoch  7, batch     9 | loss: 14.2474416Losses:  18.433151185512543 3.9084010124206543 5.003045082092285 4.935521066188812
CurrentTrain: epoch  7, batch    10 | loss: 18.4331512Losses:  13.318168640136719 3.736633777618408 5.0 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 13.3181686Losses:  16.711852192878723 3.525280475616455 4.987949848175049 2.8491564989089966
CurrentTrain: epoch  7, batch    12 | loss: 16.7118522Losses:  14.16088181734085 3.5269272327423096 5.0 1.3976743817329407
CurrentTrain: epoch  7, batch    13 | loss: 14.1608818Losses:  15.96973191946745 3.3430800437927246 5.0 3.264228455722332
CurrentTrain: epoch  7, batch    14 | loss: 15.9697319Losses:  15.852026972919703 3.5739736557006836 5.0 2.8242483474314213
CurrentTrain: epoch  7, batch    15 | loss: 15.8520270Losses:  20.39374977350235 3.7423079013824463 5.0 7.312985956668854
CurrentTrain: epoch  7, batch    16 | loss: 20.3937498Losses:  14.714119762182236 3.939141273498535 5.0 1.3994129598140717
CurrentTrain: epoch  7, batch    17 | loss: 14.7141198Losses:  13.504343032836914 3.9779412746429443 5.041987419128418 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 13.5043430Losses:  18.43244679272175 3.2773494720458984 5.0 5.8143628388643265
CurrentTrain: epoch  7, batch    19 | loss: 18.4324468Losses:  14.49475234374404 3.6561272144317627 5.03407096862793 1.4720043502748013
CurrentTrain: epoch  7, batch    20 | loss: 14.4947523Losses:  17.35769858956337 3.6544694900512695 5.0 4.349197536706924
CurrentTrain: epoch  7, batch    21 | loss: 17.3576986Losses:  15.720138859003782 3.595669746398926 5.0 2.8216946832835674
CurrentTrain: epoch  7, batch    22 | loss: 15.7201389Losses:  13.648764610290527 4.168004989624023 5.0 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 13.6487646Losses:  13.181323051452637 3.7464346885681152 5.0 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 13.1813231Losses:  15.209119021892548 3.9612159729003906 5.0 1.4692890048027039
CurrentTrain: epoch  7, batch    25 | loss: 15.2091190Losses:  15.957367837429047 3.6483778953552246 5.0 2.825314462184906
CurrentTrain: epoch  7, batch    26 | loss: 15.9573678Losses:  14.33156967163086 3.5597991943359375 5.0 1.4040851593017578
CurrentTrain: epoch  7, batch    27 | loss: 14.3315697Losses:  15.348046869039536 3.3175034523010254 5.0 2.805701822042465
CurrentTrain: epoch  7, batch    28 | loss: 15.3480469Losses:  13.93131086230278 3.2445809841156006 5.0 1.3944751918315887
CurrentTrain: epoch  7, batch    29 | loss: 13.9313109Losses:  12.6207857131958 3.3826982975006104 5.0 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 12.6207857Losses:  20.04805190116167 3.6755709648132324 5.0 7.185113973915577
CurrentTrain: epoch  7, batch    31 | loss: 20.0480519Losses:  15.532497074455023 3.5296778678894043 5.0 2.809714939445257
CurrentTrain: epoch  7, batch    32 | loss: 15.5324971Losses:  20.243775479495525 3.151644706726074 5.0 7.803195111453533
CurrentTrain: epoch  7, batch    33 | loss: 20.2437755Losses:  17.694650642573833 4.1416015625 5.0 4.225730888545513
CurrentTrain: epoch  7, batch    34 | loss: 17.6946506Losses:  13.380783081054688 4.057398796081543 5.0 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 13.3807831Losses:  18.73037549480796 4.291522979736328 4.98597526550293 4.403209891170263
CurrentTrain: epoch  7, batch    36 | loss: 18.7303755Losses:  16.934146008454263 3.6549997329711914 5.0 3.7746458863839507
CurrentTrain: epoch  7, batch    37 | loss: 16.9341460Losses:  16.054617434740067 3.967080593109131 5.0 2.7949223816394806
CurrentTrain: epoch  7, batch    38 | loss: 16.0546174Losses:  13.495023727416992 3.9463884830474854 5.0 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 13.4950237Losses:  15.95731696486473 3.863356113433838 5.0 2.8094821870326996
CurrentTrain: epoch  7, batch    40 | loss: 15.9573170Losses:  16.217425227165222 4.016489505767822 5.0 2.8125160932540894
CurrentTrain: epoch  7, batch    41 | loss: 16.2174252Losses:  13.140504837036133 3.8544678688049316 5.0 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 13.1405048Losses:  14.579271584749222 3.5563788414001465 5.0648698806762695 1.4092018902301788
CurrentTrain: epoch  7, batch    43 | loss: 14.5792716Losses:  17.75665257871151 3.527294158935547 5.0283613204956055 4.845782026648521
CurrentTrain: epoch  7, batch    44 | loss: 17.7566526Losses:  15.606640040874481 3.448254346847534 5.028412342071533 2.8495466113090515
CurrentTrain: epoch  7, batch    45 | loss: 15.6066400Losses:  19.17947717756033 4.0080413818359375 4.989417552947998 5.619128666818142
CurrentTrain: epoch  7, batch    46 | loss: 19.1794772Losses:  14.270437330007553 3.5795340538024902 5.0 1.39862260222435
CurrentTrain: epoch  7, batch    47 | loss: 14.2704373Losses:  14.461189240217209 3.670125722885132 5.0 1.4104747474193573
CurrentTrain: epoch  7, batch    48 | loss: 14.4611892Losses:  14.523077756166458 3.6396989822387695 5.0 1.3941247761249542
CurrentTrain: epoch  7, batch    49 | loss: 14.5230778Losses:  13.978474617004395 3.804867744445801 5.07718563079834 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 13.9784746Losses:  14.801309376955032 3.876049280166626 5.0 1.4675185978412628
CurrentTrain: epoch  7, batch    51 | loss: 14.8013094Losses:  16.85475528240204 3.3389840126037598 5.0 4.304366946220398
CurrentTrain: epoch  7, batch    52 | loss: 16.8547553Losses:  14.349093466997147 3.727262496948242 5.0 1.4098844826221466
CurrentTrain: epoch  7, batch    53 | loss: 14.3490935Losses:  17.2913272716105 3.7988967895507812 5.0 4.243696961551905
CurrentTrain: epoch  7, batch    54 | loss: 17.2913273Losses:  12.837654113769531 3.5234596729278564 5.0 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 12.8376541Losses:  12.064017295837402 2.902070999145508 5.0 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 12.0640173Losses:  12.733796119689941 3.314493179321289 5.0 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 12.7337961Losses:  18.445559419691563 3.405237913131714 5.0 5.808556474745274
CurrentTrain: epoch  7, batch    58 | loss: 18.4455594Losses:  13.016195297241211 3.7562453746795654 5.0 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 13.0161953Losses:  14.169437535107136 3.4558820724487305 5.0 1.477444775402546
CurrentTrain: epoch  7, batch    60 | loss: 14.1694375Losses:  14.401220463216305 3.7430551052093506 5.0 1.4361411556601524
CurrentTrain: epoch  7, batch    61 | loss: 14.4012205Losses:  12.9287109375 3.697554588317871 5.0 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 12.9287109Losses:  14.502729628235102 3.5444562435150146 5.0 1.5260536409914494
CurrentTrain: epoch  8, batch     0 | loss: 14.5027296Losses:  16.12440225854516 3.7940304279327393 5.0 2.9789230562746525
CurrentTrain: epoch  8, batch     1 | loss: 16.1244023Losses:  15.745027959346771 3.7239692211151123 5.0 2.8087610602378845
CurrentTrain: epoch  8, batch     2 | loss: 15.7450280Losses:  13.655604343861341 2.9898431301116943 5.0 1.4688796810805798
CurrentTrain: epoch  8, batch     3 | loss: 13.6556043Losses:  14.457212805747986 3.6318366527557373 5.0 1.4155954122543335
CurrentTrain: epoch  8, batch     4 | loss: 14.4572128Losses:  12.871915817260742 3.6649889945983887 5.0 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 12.8719158Losses:  12.78700065612793 3.5560922622680664 5.0 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 12.7870007Losses:  16.098496731370687 4.027043342590332 5.0 2.819156941026449
CurrentTrain: epoch  8, batch     7 | loss: 16.0984967Losses:  15.782601360231638 3.595386505126953 5.036344528198242 2.8813476599752903
CurrentTrain: epoch  8, batch     8 | loss: 15.7826014Losses:  17.17856728285551 3.808436870574951 5.02744197845459 4.227654807269573
CurrentTrain: epoch  8, batch     9 | loss: 17.1785673Losses:  12.51114559173584 3.293381690979004 5.0 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 12.5111456Losses:  13.777375668287277 3.161156415939331 5.0 1.4591536223888397
CurrentTrain: epoch  8, batch    11 | loss: 13.7773757Losses:  14.321935132145882 3.420712471008301 5.0 1.5086397677659988
CurrentTrain: epoch  8, batch    12 | loss: 14.3219351Losses:  13.964082099497318 3.292248249053955 5.0 1.4905551448464394
CurrentTrain: epoch  8, batch    13 | loss: 13.9640821Losses:  13.855784893035889 3.209650993347168 5.046205520629883 1.4119906425476074
CurrentTrain: epoch  8, batch    14 | loss: 13.8557849Losses:  13.990961819887161 3.331465244293213 5.0 1.3909805119037628
CurrentTrain: epoch  8, batch    15 | loss: 13.9909618Losses:  17.83813828229904 3.25203800201416 4.974567413330078 4.305596053600311
CurrentTrain: epoch  8, batch    16 | loss: 17.8381383Losses:  12.029719352722168 2.814105987548828 5.047731399536133 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 12.0297194Losses:  14.045588038861752 3.1948137283325195 5.027863502502441 1.5860219225287437
CurrentTrain: epoch  8, batch    18 | loss: 14.0455880Losses:  17.295555010437965 3.6901955604553223 5.032876968383789 4.27959717810154
CurrentTrain: epoch  8, batch    19 | loss: 17.2955550Losses:  13.171424865722656 3.919966697692871 5.0 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 13.1714249Losses:  12.960829734802246 3.684553384780884 5.0 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 12.9608297Losses:  19.47555483877659 3.543325424194336 5.0 6.562480345368385
CurrentTrain: epoch  8, batch    22 | loss: 19.4755548Losses:  22.990576338022947 3.7817773818969727 5.0 9.946172308176756
CurrentTrain: epoch  8, batch    23 | loss: 22.9905763Losses:  12.896252632141113 3.721510887145996 5.0 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 12.8962526Losses:  24.324512630701065 2.5663669109344482 5.0 12.686655193567276
CurrentTrain: epoch  8, batch    25 | loss: 24.3245126Losses:  14.310951799154282 3.6547393798828125 5.0 1.4336181581020355
CurrentTrain: epoch  8, batch    26 | loss: 14.3109518Losses:  24.045005839318037 4.3703508377075195 5.048198699951172 10.036280672997236
CurrentTrain: epoch  8, batch    27 | loss: 24.0450058Losses:  12.568244934082031 3.3821141719818115 5.0 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 12.5682449Losses:  17.127531945705414 3.6503071784973145 5.0 4.204394280910492
CurrentTrain: epoch  8, batch    29 | loss: 17.1275319Losses:  13.934975408017635 3.262155294418335 5.0 1.4760472998023033
CurrentTrain: epoch  8, batch    30 | loss: 13.9349754Losses:  19.391129724681377 3.665229320526123 5.0 6.46876835078001
CurrentTrain: epoch  8, batch    31 | loss: 19.3911297Losses:  15.701561868190765 3.661015510559082 5.043148040771484 2.803539216518402
CurrentTrain: epoch  8, batch    32 | loss: 15.7015619Losses:  12.243477821350098 3.034271001815796 5.0 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 12.2434778Losses:  15.096904337406158 3.546557903289795 5.096277713775635 1.4112945199012756
CurrentTrain: epoch  8, batch    34 | loss: 15.0969043Losses:  13.927176158875227 3.226592779159546 5.0 1.4994646720588207
CurrentTrain: epoch  8, batch    35 | loss: 13.9271762Losses:  14.124802708625793 3.427353620529175 5.0 1.3996602296829224
CurrentTrain: epoch  8, batch    36 | loss: 14.1248027Losses:  12.4125394821167 3.244959831237793 5.0 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 12.4125395Losses:  21.518244579434395 3.473247766494751 5.0 8.852992847561836
CurrentTrain: epoch  8, batch    38 | loss: 21.5182446Losses:  16.922003120183945 3.580294609069824 5.0 4.2127202451229095
CurrentTrain: epoch  8, batch    39 | loss: 16.9220031Losses:  13.942597210407257 3.350947856903076 5.0 1.4015796780586243
CurrentTrain: epoch  8, batch    40 | loss: 13.9425972Losses:  13.835037410259247 3.1731984615325928 5.0 1.3953658938407898
CurrentTrain: epoch  8, batch    41 | loss: 13.8350374Losses:  15.80503186583519 3.732344627380371 5.0 2.8240958154201508
CurrentTrain: epoch  8, batch    42 | loss: 15.8050319Losses:  22.442298904061317 4.116162300109863 5.047903537750244 9.074018493294716
CurrentTrain: epoch  8, batch    43 | loss: 22.4422989Losses:  13.85735484957695 3.2704098224639893 5.0 1.3944089114665985
CurrentTrain: epoch  8, batch    44 | loss: 13.8573548Losses:  13.734189301729202 3.0811281204223633 5.0 1.3926480114459991
CurrentTrain: epoch  8, batch    45 | loss: 13.7341893Losses:  16.720036827027798 3.1899263858795166 5.017468452453613 4.342518173158169
CurrentTrain: epoch  8, batch    46 | loss: 16.7200368Losses:  13.614246305078268 3.0550217628479004 5.0 1.441690381616354
CurrentTrain: epoch  8, batch    47 | loss: 13.6142463Losses:  12.221720695495605 2.964474678039551 5.0 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 12.2217207Losses:  12.478645324707031 3.286154270172119 5.0 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 12.4786453Losses:  15.732159405946732 3.6648240089416504 5.037680625915527 2.8097961246967316
CurrentTrain: epoch  8, batch    50 | loss: 15.7321594Losses:  14.190338402986526 3.560688018798828 5.0 1.4100086987018585
CurrentTrain: epoch  8, batch    51 | loss: 14.1903384Losses:  14.013471722602844 3.394742488861084 5.016395092010498 1.400635838508606
CurrentTrain: epoch  8, batch    52 | loss: 14.0134717Losses:  12.547117233276367 3.4151439666748047 5.0 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 12.5471172Losses:  16.43585703521967 2.918715000152588 5.0 4.375422693789005
CurrentTrain: epoch  8, batch    54 | loss: 16.4358570Losses:  13.319456428289413 2.7901511192321777 5.0 1.4193385541439056
CurrentTrain: epoch  8, batch    55 | loss: 13.3194564Losses:  16.657780960202217 3.1378345489501953 5.0 4.40715916454792
CurrentTrain: epoch  8, batch    56 | loss: 16.6577810Losses:  16.296272344887257 2.9052438735961914 5.0 4.2627802565693855
CurrentTrain: epoch  8, batch    57 | loss: 16.2962723Losses:  13.839107364416122 3.1976945400238037 5.0 1.3975351750850677
CurrentTrain: epoch  8, batch    58 | loss: 13.8391074Losses:  13.944523006677628 3.3116323947906494 5.0 1.4072162210941315
CurrentTrain: epoch  8, batch    59 | loss: 13.9445230Losses:  12.500327110290527 3.3931541442871094 5.0 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 12.5003271Losses:  18.159259736537933 3.4053735733032227 5.0 5.632205903530121
CurrentTrain: epoch  8, batch    61 | loss: 18.1592597Losses:  14.971656177192926 4.294516563415527 5.0 1.477031085640192
CurrentTrain: epoch  8, batch    62 | loss: 14.9716562Losses:  15.104255486279726 2.992778778076172 5.0 2.9745309837162495
CurrentTrain: epoch  9, batch     0 | loss: 15.1042555Losses:  11.293757438659668 2.237396001815796 5.0 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 11.2937574Losses:  12.642496109008789 3.442986011505127 5.0 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 12.6424961Losses:  15.174489997327328 3.2208542823791504 5.0 2.853903792798519
CurrentTrain: epoch  9, batch     3 | loss: 15.1744900Losses:  15.647096931934357 3.703552722930908 5.0 2.781785309314728
CurrentTrain: epoch  9, batch     4 | loss: 15.6470969Losses:  16.63051663339138 3.1884067058563232 5.0 4.299362763762474
CurrentTrain: epoch  9, batch     5 | loss: 16.6305166Losses:  13.640860825777054 3.135441303253174 5.0 1.3900082409381866
CurrentTrain: epoch  9, batch     6 | loss: 13.6408608Losses:  14.906049460172653 2.9581503868103027 5.039237022399902 2.797467917203903
CurrentTrain: epoch  9, batch     7 | loss: 14.9060495Losses:  15.772330582141876 3.8610875606536865 5.0 2.788988411426544
CurrentTrain: epoch  9, batch     8 | loss: 15.7723306Losses:  11.957864761352539 2.8344483375549316 5.0 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 11.9578648Losses:  15.527532067149878 3.559671640396118 5.0 2.858001198619604
CurrentTrain: epoch  9, batch    10 | loss: 15.5275321Losses:  13.95072728395462 2.9415721893310547 5.0 1.3988235592842102
CurrentTrain: epoch  9, batch    11 | loss: 13.9507273Losses:  15.383891493082047 3.3808093070983887 5.045402526855469 2.826707273721695
CurrentTrain: epoch  9, batch    12 | loss: 15.3838915Losses:  12.140852928161621 2.9963998794555664 5.0 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 12.1408529Losses:  14.825520157814026 2.8593108654022217 5.0 2.827206254005432
CurrentTrain: epoch  9, batch    14 | loss: 14.8255202Losses:  16.207246258854866 2.9380455017089844 5.0 4.192406132817268
CurrentTrain: epoch  9, batch    15 | loss: 16.2072463Losses:  15.565722815692425 3.4731478691101074 5.0 2.8978704139590263
CurrentTrain: epoch  9, batch    16 | loss: 15.5657228Losses:  12.077278137207031 2.976682662963867 5.0 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 12.0772781Losses:  14.014965027570724 3.467395782470703 5.0 1.4433498084545135
CurrentTrain: epoch  9, batch    18 | loss: 14.0149650Losses:  18.900778144598007 2.7979674339294434 5.0 7.034201949834824
CurrentTrain: epoch  9, batch    19 | loss: 18.9007781Losses:  13.534775916486979 3.0149288177490234 5.0 1.4321662820875645
CurrentTrain: epoch  9, batch    20 | loss: 13.5347759Losses:  14.133374456316233 3.486828327178955 5.027421474456787 1.4100058116018772
CurrentTrain: epoch  9, batch    21 | loss: 14.1333745Losses:  12.642965316772461 3.552730083465576 5.0 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 12.6429653Losses:  16.042893152683973 3.9319708347320557 5.0 2.925810556858778
CurrentTrain: epoch  9, batch    23 | loss: 16.0428932Losses:  12.766228675842285 3.286133050918579 5.051684379577637 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 12.7662287Losses:  16.536517925560474 2.952864646911621 5.0 4.525411434471607
CurrentTrain: epoch  9, batch    25 | loss: 16.5365179Losses:  12.490845680236816 3.161961793899536 5.0 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 12.4908457Losses:  15.005371659994125 3.0869102478027344 5.0 2.832878679037094
CurrentTrain: epoch  9, batch    27 | loss: 15.0053717Losses:  14.862477570772171 2.999868631362915 5.0 2.8035891354084015
CurrentTrain: epoch  9, batch    28 | loss: 14.8624776Losses:  15.693146582692862 3.7206380367279053 5.0 2.826509352773428
CurrentTrain: epoch  9, batch    29 | loss: 15.6931466Losses:  13.371605962514877 2.7965166568756104 5.0 1.3986588418483734
CurrentTrain: epoch  9, batch    30 | loss: 13.3716060Losses:  16.445578403770924 3.1139731407165527 5.0 4.228255100548267
CurrentTrain: epoch  9, batch    31 | loss: 16.4455784Losses:  19.902211517095566 3.1686458587646484 5.0 7.5811494290828705
CurrentTrain: epoch  9, batch    32 | loss: 19.9022115Losses:  16.68383425846696 3.257668972015381 5.0 4.329107467085123
CurrentTrain: epoch  9, batch    33 | loss: 16.6838343Losses:  11.721613883972168 2.651905059814453 5.0 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 11.7216139Losses:  18.052606850862503 3.1173501014709473 5.0 5.7640478909015656
CurrentTrain: epoch  9, batch    35 | loss: 18.0526069Losses:  15.305149883031845 3.419325113296509 5.0 2.801147311925888
CurrentTrain: epoch  9, batch    36 | loss: 15.3051499Losses:  12.409958839416504 3.1322860717773438 5.0 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 12.4099588Losses:  13.466996520757675 2.877424955368042 5.0 1.4084418714046478
CurrentTrain: epoch  9, batch    38 | loss: 13.4669965Losses:  12.464862823486328 3.3028507232666016 5.0 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 12.4648628Losses:  13.765120178461075 3.2085630893707275 5.0 1.4289690554141998
CurrentTrain: epoch  9, batch    40 | loss: 13.7651202Losses:  12.14001178741455 2.979116439819336 5.0 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 12.1400118Losses:  16.16242315992713 2.741530179977417 5.0 4.2485666535794735
CurrentTrain: epoch  9, batch    42 | loss: 16.1624232Losses:  15.596023797988892 3.6726815700531006 5.0 2.8178045749664307
CurrentTrain: epoch  9, batch    43 | loss: 15.5960238Losses:  13.990210354328156 3.4817099571228027 5.0 1.3900240063667297
CurrentTrain: epoch  9, batch    44 | loss: 13.9902104Losses:  14.175485044717789 3.684166431427002 5.0 1.4013752043247223
CurrentTrain: epoch  9, batch    45 | loss: 14.1754850Losses:  15.402556624263525 3.331958293914795 5.0 2.8705188892781734
CurrentTrain: epoch  9, batch    46 | loss: 15.4025566Losses:  14.906703922897577 2.9243264198303223 5.0 2.8451947905123234
CurrentTrain: epoch  9, batch    47 | loss: 14.9067039Losses:  12.242569923400879 3.1187903881073 5.0 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 12.2425699Losses:  15.2653504088521 3.1762006282806396 5.0 2.810947485268116
CurrentTrain: epoch  9, batch    49 | loss: 15.2653504Losses:  16.721655648201704 3.1633524894714355 5.059320449829102 4.346882622689009
CurrentTrain: epoch  9, batch    50 | loss: 16.7216556Losses:  12.719200134277344 3.604525327682495 5.0 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 12.7192001Losses:  15.039732687175274 3.061338186264038 5.0 2.8625819608569145
CurrentTrain: epoch  9, batch    52 | loss: 15.0397327Losses:  14.728010714054108 2.7868595123291016 5.0 2.83601051568985
CurrentTrain: epoch  9, batch    53 | loss: 14.7280107Losses:  15.261455774307251 3.3783860206604004 5.0 2.799318552017212
CurrentTrain: epoch  9, batch    54 | loss: 15.2614558Losses:  16.428476750850677 2.989560604095459 5.0 4.230276525020599
CurrentTrain: epoch  9, batch    55 | loss: 16.4284768Losses:  16.59452000260353 3.224754810333252 5.0 4.227887541055679
CurrentTrain: epoch  9, batch    56 | loss: 16.5945200Losses:  21.355819270014763 3.175894021987915 5.0 9.158381029963493
CurrentTrain: epoch  9, batch    57 | loss: 21.3558193Losses:  13.455536782741547 2.843517780303955 5.0 1.4634751677513123
CurrentTrain: epoch  9, batch    58 | loss: 13.4555368Losses:  16.080630600452423 2.6951422691345215 5.0 4.192758858203888
CurrentTrain: epoch  9, batch    59 | loss: 16.0806306Losses:  13.32214954867959 2.776963233947754 5.0 1.4603941775858402
CurrentTrain: epoch  9, batch    60 | loss: 13.3221495Losses:  17.58228424191475 3.2015085220336914 5.027775287628174 4.20045593380928
CurrentTrain: epoch  9, batch    61 | loss: 17.5822842Losses:  17.792428504675627 2.9382245540618896 5.0 5.704885017126799
CurrentTrain: epoch  9, batch    62 | loss: 17.7924285
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  20.544996235519648 5.342237949371338 6.806306838989258 1.5041141249239445
CurrentTrain: epoch  0, batch     0 | loss: 20.5449962Losses:  25.159752637147903 5.254000186920166 6.933973789215088 6.420157223939896
CurrentTrain: epoch  0, batch     1 | loss: 25.1597526Losses:  21.320419192314148 5.391643524169922 6.198869705200195 2.8401068449020386
CurrentTrain: epoch  0, batch     2 | loss: 21.3204192Losses:  20.034388780593872 4.64533805847168 6.084353446960449 1.6831886768341064
CurrentTrain: epoch  0, batch     3 | loss: 20.0343888Losses:  18.469146728515625 5.220048904418945 6.69254207611084 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 18.4691467Losses:  20.531934954226017 5.266913414001465 5.961428165435791 2.934826113283634
CurrentTrain: epoch  1, batch     1 | loss: 20.5319350Losses:  18.36132648587227 5.292963027954102 6.082327842712402 1.5107329189777374
CurrentTrain: epoch  1, batch     2 | loss: 18.3613265Losses:  17.581493496894836 5.130560874938965 5.171106338500977 1.5522385835647583
CurrentTrain: epoch  1, batch     3 | loss: 17.5814935Losses:  16.28897476196289 5.157231330871582 5.513915538787842 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 16.2889748Losses:  19.60766550898552 5.2256317138671875 6.080718040466309 2.952937573194504
CurrentTrain: epoch  2, batch     1 | loss: 19.6076655Losses:  16.61996078491211 5.125423431396484 5.650540351867676 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 16.6199608Losses:  16.00361794233322 5.395428657531738 6.358719825744629 1.397746741771698
CurrentTrain: epoch  2, batch     3 | loss: 16.0036179Losses:  19.846257466822863 5.190859794616699 5.865720748901367 3.1895210929214954
CurrentTrain: epoch  3, batch     0 | loss: 19.8462575Losses:  19.07757433131337 5.177092552185059 5.601519584655762 3.9510455913841724
CurrentTrain: epoch  3, batch     1 | loss: 19.0775743Losses:  16.379494253546 5.003609657287598 5.401457786560059 1.4293285049498081
CurrentTrain: epoch  3, batch     2 | loss: 16.3794943Losses:  15.662943072617054 5.085786819458008 5.620983123779297 1.5077287629246712
CurrentTrain: epoch  3, batch     3 | loss: 15.6629431Losses:  14.323431968688965 5.0916619300842285 5.333061218261719 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 14.3234320Losses:  20.207581508904696 5.172333717346191 5.502745628356934 4.976835239678621
CurrentTrain: epoch  4, batch     1 | loss: 20.2075815Losses:  14.638655662536621 4.981407165527344 5.333332061767578 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 14.6386557Losses:  16.099658392369747 5.271065711975098 4.944194793701172 1.4800580963492393
CurrentTrain: epoch  4, batch     3 | loss: 16.0996584Losses:  15.029613494873047 5.0936079025268555 5.372479438781738 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 15.0296135Losses:  14.349508285522461 5.155490875244141 5.2685770988464355 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 14.3495083Losses:  18.283968802541494 4.963352203369141 5.373907089233398 3.8461502753198147
CurrentTrain: epoch  5, batch     2 | loss: 18.2839688Losses:  14.792522616684437 4.49574089050293 4.943340301513672 1.5397664979100227
CurrentTrain: epoch  5, batch     3 | loss: 14.7925226Losses:  13.605413436889648 4.920941352844238 5.160211563110352 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 13.6054134Losses:  14.136188507080078 5.064543724060059 5.275117874145508 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 14.1361885Losses:  13.898906707763672 4.981101989746094 5.224498271942139 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 13.8989067Losses:  15.564181305468082 5.275575637817383 5.4178314208984375 1.4560384526848793
CurrentTrain: epoch  6, batch     3 | loss: 15.5641813Losses:  16.47164036333561 5.024477005004883 5.134527683258057 2.5838019996881485
CurrentTrain: epoch  7, batch     0 | loss: 16.4716404Losses:  15.312385022640228 4.922263145446777 5.156160354614258 1.567123830318451
CurrentTrain: epoch  7, batch     1 | loss: 15.3123850Losses:  14.927318930625916 4.931510925292969 5.1122236251831055 1.5261377096176147
CurrentTrain: epoch  7, batch     2 | loss: 14.9273189Losses:  14.951443489640951 5.453157424926758 5.0829668045043945 1.5103128515183926
CurrentTrain: epoch  7, batch     3 | loss: 14.9514435Losses:  18.31410961970687 5.038351535797119 5.244413375854492 4.438526924699545
CurrentTrain: epoch  8, batch     0 | loss: 18.3141096Losses:  16.313018962740898 4.862757682800293 5.134495735168457 2.941432163119316
CurrentTrain: epoch  8, batch     1 | loss: 16.3130190Losses:  13.14291000366211 4.954807758331299 5.134103775024414 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 13.1429100Losses:  14.348031774163246 4.851541519165039 5.0 1.438213124871254
CurrentTrain: epoch  8, batch     3 | loss: 14.3480318Losses:  14.42761754989624 4.897062301635742 5.075621128082275 1.4302573204040527
CurrentTrain: epoch  9, batch     0 | loss: 14.4276175Losses:  14.37468072026968 5.041485786437988 5.157855033874512 1.4756738767027855
CurrentTrain: epoch  9, batch     1 | loss: 14.3746807Losses:  15.230611335486174 4.793354034423828 5.145197868347168 2.033813964575529
CurrentTrain: epoch  9, batch     2 | loss: 15.2306113Losses:  15.048773970454931 5.2869157791137695 5.37637996673584 1.5615894459187984
CurrentTrain: epoch  9, batch     3 | loss: 15.0487740
Losses:  10.875215530395508 4.220091819763184 5.068787097930908 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.8752155Losses:  15.827158153057098 4.748248100280762 5.0 5.8739511370658875
MemoryTrain:  epoch  0, batch     1 | loss: 15.8271582Losses:  10.999368667602539 4.506130218505859 5.04727840423584 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 10.9993687Losses:  13.919277723878622 3.24479079246521 5.0 5.656179007142782
MemoryTrain:  epoch  1, batch     1 | loss: 13.9192777Losses:  9.975675582885742 4.293663024902344 5.0465006828308105 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.9756756Losses:  16.55568565800786 4.017550468444824 5.0 5.693451542407274
MemoryTrain:  epoch  2, batch     1 | loss: 16.5556857Losses:  10.09185791015625 4.02478551864624 5.060744285583496 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 10.0918579Losses:  15.756424810737371 4.921274662017822 5.0 5.718413259834051
MemoryTrain:  epoch  3, batch     1 | loss: 15.7564248Losses:  10.081925392150879 4.0818071365356445 5.002580642700195 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 10.0819254Losses:  15.097397923469543 4.382009029388428 5.0 5.658053517341614
MemoryTrain:  epoch  4, batch     1 | loss: 15.0973979Losses:  9.71292495727539 4.125986576080322 5.095456600189209 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.7129250Losses:  16.541154731065035 4.265871524810791 5.0 5.680106032639742
MemoryTrain:  epoch  5, batch     1 | loss: 16.5411547Losses:  9.93420124053955 4.119462966918945 4.962573528289795 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.9342012Losses:  15.002286322414875 4.231368064880371 5.0 5.690483458340168
MemoryTrain:  epoch  6, batch     1 | loss: 15.0022863Losses:  10.062755584716797 4.222890853881836 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 10.0627556Losses:  14.432590696960688 3.7191152572631836 5.0 5.662213537842035
MemoryTrain:  epoch  7, batch     1 | loss: 14.4325907Losses:  9.931018829345703 4.133546829223633 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.9310188Losses:  14.945265293121338 4.139109134674072 5.0 5.701853275299072
MemoryTrain:  epoch  8, batch     1 | loss: 14.9452653Losses:  9.82276725769043 3.994109869003296 5.045435905456543 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.8227673Losses:  15.226561218500137 4.391977310180664 5.0 5.794650703668594
MemoryTrain:  epoch  9, batch     1 | loss: 15.2265612
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 87.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 74.27%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 72.15%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 69.77%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 69.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.01%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.32%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 91.22%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.14%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.03%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 92.95%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.87%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.38%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.75%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 92.77%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.87%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 92.62%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 92.55%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 92.48%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.43%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.37%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.23%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 91.93%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.56%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.46%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.19%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 91.10%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 91.06%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 90.95%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 90.87%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 90.56%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 90.66%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 90.42%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 90.12%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 89.56%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 89.01%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 88.41%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.08%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 87.69%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 87.44%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 87.00%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 86.51%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 86.21%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 85.68%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.46%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.24%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 84.85%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.40%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 83.97%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 83.54%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 83.12%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.66%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.37%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 82.19%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.49%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.73%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.83%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 82.86%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.50%   
cur_acc:  ['0.9435', '0.7401']
his_acc:  ['0.9435', '0.8350']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  17.14485639333725 5.013064384460449 5.535978317260742 1.449760377407074
CurrentTrain: epoch  0, batch     0 | loss: 17.1448564Losses:  17.72911262512207 4.991678714752197 5.5639801025390625 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 17.7291126Losses:  20.158529542386532 4.916748046875 5.864513397216797 3.676258347928524
CurrentTrain: epoch  0, batch     2 | loss: 20.1585295Losses:  20.100939579308033 5.08393669128418 4.89027214050293 1.6297920420765877
CurrentTrain: epoch  0, batch     3 | loss: 20.1009396Losses:  17.696040093898773 4.919621467590332 5.676150798797607 1.4068192839622498
CurrentTrain: epoch  1, batch     0 | loss: 17.6960401Losses:  23.113170497119427 4.847934722900391 5.259346961975098 8.391401164233685
CurrentTrain: epoch  1, batch     1 | loss: 23.1131705Losses:  18.435265008360147 4.988359451293945 5.6625776290893555 1.4843868650496006
CurrentTrain: epoch  1, batch     2 | loss: 18.4352650Losses:  17.714004956185818 4.948030471801758 5.327101707458496 1.4354519471526146
CurrentTrain: epoch  1, batch     3 | loss: 17.7140050Losses:  25.921509474515915 4.920533180236816 5.547320365905762 10.314164847135544
CurrentTrain: epoch  2, batch     0 | loss: 25.9215095Losses:  17.617231480777264 4.829517364501953 5.370405197143555 2.1075059100985527
CurrentTrain: epoch  2, batch     1 | loss: 17.6172315Losses:  17.094537395983934 4.928105354309082 5.47935676574707 1.4381138272583485
CurrentTrain: epoch  2, batch     2 | loss: 17.0945374Losses:  17.401972945779562 5.021233558654785 5.301259994506836 1.5624496303498745
CurrentTrain: epoch  2, batch     3 | loss: 17.4019729Losses:  14.571849822998047 4.876452445983887 5.147160053253174 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 14.5718498Losses:  18.096925284713507 4.756614685058594 5.345589637756348 3.270308043807745
CurrentTrain: epoch  3, batch     1 | loss: 18.0969253Losses:  18.919378697872162 4.90479850769043 5.610076427459717 3.0256733298301697
CurrentTrain: epoch  3, batch     2 | loss: 18.9193787Losses:  17.92478644475341 4.9431657791137695 5.768662452697754 1.785488959401846
CurrentTrain: epoch  3, batch     3 | loss: 17.9247864Losses:  18.388990223407745 4.928500175476074 5.106664657592773 3.602165997028351
CurrentTrain: epoch  4, batch     0 | loss: 18.3889902Losses:  16.06876429915428 4.839084148406982 5.40085506439209 1.4433093965053558
CurrentTrain: epoch  4, batch     1 | loss: 16.0687643Losses:  18.536694832146168 4.797244071960449 5.493175983428955 3.1037343218922615
CurrentTrain: epoch  4, batch     2 | loss: 18.5366948Losses:  13.880462422966957 4.749456405639648 5.0 1.5555875450372696
CurrentTrain: epoch  4, batch     3 | loss: 13.8804624Losses:  14.754029273986816 4.797290802001953 5.494178771972656 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 14.7540293Losses:  15.271427005529404 4.798164367675781 5.150130271911621 1.4475887715816498
CurrentTrain: epoch  5, batch     1 | loss: 15.2714270Losses:  16.493708550930023 4.886707782745361 5.335423469543457 1.409873902797699
CurrentTrain: epoch  5, batch     2 | loss: 16.4937086Losses:  16.575880780816078 4.896876335144043 5.0 1.4505832344293594
CurrentTrain: epoch  5, batch     3 | loss: 16.5758808Losses:  17.066840559244156 4.797482490539551 5.213717937469482 2.9919828474521637
CurrentTrain: epoch  6, batch     0 | loss: 17.0668406Losses:  18.033490724861622 4.705996513366699 5.52348518371582 3.074025698006153
CurrentTrain: epoch  6, batch     1 | loss: 18.0334907Losses:  21.03210738301277 4.899139404296875 5.150632381439209 6.516593009233475
CurrentTrain: epoch  6, batch     2 | loss: 21.0321074Losses:  15.993317753076553 4.629804611206055 5.0 1.394789844751358
CurrentTrain: epoch  6, batch     3 | loss: 15.9933178Losses:  13.346181869506836 4.76839017868042 5.039659023284912 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 13.3461819Losses:  15.912701934576035 4.826481819152832 5.264506816864014 1.4590562283992767
CurrentTrain: epoch  7, batch     1 | loss: 15.9127019Losses:  18.905740156769753 4.751470565795898 5.363885879516602 4.681937590241432
CurrentTrain: epoch  7, batch     2 | loss: 18.9057402Losses:  16.459411963820457 4.960350036621094 5.0 1.4507459253072739
CurrentTrain: epoch  7, batch     3 | loss: 16.4594120Losses:  18.447840601205826 4.916808605194092 5.172031879425049 4.493042856454849
CurrentTrain: epoch  8, batch     0 | loss: 18.4478406Losses:  17.286157488822937 4.643138885498047 5.2916059494018555 2.811585307121277
CurrentTrain: epoch  8, batch     1 | loss: 17.2861575Losses:  17.320036754012108 4.734378337860107 5.269624710083008 4.260550364851952
CurrentTrain: epoch  8, batch     2 | loss: 17.3200368Losses:  15.023794502019882 4.823596954345703 4.988680839538574 1.3967278897762299
CurrentTrain: epoch  8, batch     3 | loss: 15.0237945Losses:  13.967670440673828 4.724216461181641 5.2577972412109375 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 13.9676704Losses:  14.921066850423813 4.737656593322754 5.289146423339844 1.4413667619228363
CurrentTrain: epoch  9, batch     1 | loss: 14.9210669Losses:  15.565457787364721 4.752007484436035 5.103245735168457 1.4481033943593502
CurrentTrain: epoch  9, batch     2 | loss: 15.5654578Losses:  13.493415787816048 4.715534210205078 4.964969635009766 1.4314174205064774
CurrentTrain: epoch  9, batch     3 | loss: 13.4934158
Losses:  10.521772384643555 4.779051780700684 5.172624111175537 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.5217724Losses:  10.102563858032227 4.401609420776367 5.009986400604248 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 10.1025639Losses:  9.795001983642578 4.459534168243408 4.965268611907959 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 9.7950020Losses:  11.138324737548828 4.689700603485107 5.032937526702881 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 11.1383247Losses:  10.382039070129395 4.484264373779297 5.074697971343994 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 10.3820391Losses:  9.847209930419922 4.60479211807251 4.9989166259765625 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 9.8472099Losses:  9.77351188659668 4.463252544403076 5.017578125 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.7735119Losses:  9.917401313781738 4.5846171379089355 5.0287370681762695 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 9.9174013Losses:  9.678881645202637 4.476506233215332 5.077807903289795 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.6788816Losses:  9.931081771850586 4.535465240478516 5.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.9310818Losses:  9.909053802490234 4.663445472717285 5.056557655334473 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.9090538Losses:  9.419208526611328 4.30186128616333 5.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 9.4192085Losses:  9.762775421142578 4.597436904907227 5.006551742553711 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.7627754Losses:  9.424342155456543 4.383065700531006 5.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.4243422Losses:  9.408974647521973 4.360454559326172 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.4089746Losses:  9.769147872924805 4.588560104370117 5.0728440284729 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 9.7691479Losses:  9.445837020874023 4.414585590362549 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.4458370Losses:  9.525811195373535 4.446425437927246 5.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.5258112Losses:  9.441883087158203 4.405466079711914 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.4418831Losses:  9.444442749023438 4.390951156616211 5.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 9.4444427
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 18.75%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 52.63%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 57.37%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 55.60%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 53.96%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 52.62%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 53.32%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 54.55%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 57.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 59.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 60.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 61.22%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 62.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 65.14%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 65.29%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 67.56%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 89.44%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.72%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 90.26%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.07%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 91.06%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 90.78%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.23%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 90.67%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 90.89%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 90.89%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.92%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 90.95%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 90.75%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.54%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.35%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.00%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.97%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 89.94%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 89.68%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 89.71%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 89.68%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 89.37%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 89.19%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 89.10%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 89.01%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 88.44%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 88.03%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 86.91%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 86.60%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 86.22%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 85.98%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 85.50%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 85.02%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 84.74%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 84.28%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.07%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.87%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 83.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.12%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 82.64%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.11%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 81.65%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.97%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.81%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 81.66%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.96%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 82.15%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 81.75%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 81.45%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 80.91%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 80.52%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 80.19%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 79.77%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 80.30%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 79.90%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 79.55%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 79.21%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 78.79%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 78.58%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 78.26%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 78.56%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 78.04%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 77.61%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 77.15%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 76.69%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 76.28%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 76.27%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.41%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 77.51%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 77.65%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 77.49%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 77.41%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 77.46%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 77.41%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 77.30%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 77.16%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 77.11%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 77.09%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 77.35%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 77.41%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 77.26%   
cur_acc:  ['0.9435', '0.7401', '0.6756']
his_acc:  ['0.9435', '0.8350', '0.7726']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  16.076908111572266 5.101912021636963 5.615328788757324 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 16.0769081Losses:  17.086586356163025 4.859072685241699 5.902652263641357 1.4328559637069702
CurrentTrain: epoch  0, batch     1 | loss: 17.0865864Losses:  23.565995898097754 4.932771682739258 5.769815444946289 8.427036967128515
CurrentTrain: epoch  0, batch     2 | loss: 23.5659959Losses:  16.122265815734863 5.010000228881836 5.213054656982422 1.4363584518432617
CurrentTrain: epoch  0, batch     3 | loss: 16.1222658Losses:  17.82088479027152 4.860494613647461 5.514049530029297 3.0414229296147823
CurrentTrain: epoch  1, batch     0 | loss: 17.8208848Losses:  14.586142539978027 4.941205978393555 5.614974021911621 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 14.5861425Losses:  14.863426387310028 4.807625770568848 5.5118255615234375 1.4331294894218445
CurrentTrain: epoch  1, batch     2 | loss: 14.8634264Losses:  15.454477421939373 5.0777482986450195 5.380465507507324 1.494530789554119
CurrentTrain: epoch  1, batch     3 | loss: 15.4544774Losses:  18.544826984405518 4.956073760986328 5.653346061706543 4.304580211639404
CurrentTrain: epoch  2, batch     0 | loss: 18.5448270Losses:  16.88345991075039 4.849037170410156 5.414074897766113 2.9782484620809555
CurrentTrain: epoch  2, batch     1 | loss: 16.8834599Losses:  15.785330891609192 4.789303779602051 5.243927001953125 2.8316651582717896
CurrentTrain: epoch  2, batch     2 | loss: 15.7853309Losses:  17.716758459806442 4.220664024353027 5.0 6.746092528104782
CurrentTrain: epoch  2, batch     3 | loss: 17.7167585Losses:  15.511335216462612 4.831953048706055 5.115868091583252 2.077693782746792
CurrentTrain: epoch  3, batch     0 | loss: 15.5113352Losses:  17.781776577234268 4.791481018066406 5.448502063751221 4.348794132471085
CurrentTrain: epoch  3, batch     1 | loss: 17.7817766Losses:  15.821185532957315 4.719794273376465 5.384544849395752 2.860904160887003
CurrentTrain: epoch  3, batch     2 | loss: 15.8211855Losses:  13.17975628376007 4.610103607177734 5.0 1.4065533876419067
CurrentTrain: epoch  3, batch     3 | loss: 13.1797563Losses:  14.563258945941925 4.806208610534668 5.3299970626831055 1.4446962475776672
CurrentTrain: epoch  4, batch     0 | loss: 14.5632589Losses:  12.892648696899414 4.801787853240967 5.1042022705078125 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.8926487Losses:  14.477421969175339 4.669007778167725 5.17515754699707 1.462027758359909
CurrentTrain: epoch  4, batch     2 | loss: 14.4774220Losses:  12.896686784923077 4.567375183105469 5.037734031677246 1.4287111684679985
CurrentTrain: epoch  4, batch     3 | loss: 12.8966868Losses:  15.164986666291952 4.635043144226074 5.186753749847412 2.9045439325273037
CurrentTrain: epoch  5, batch     0 | loss: 15.1649867Losses:  12.7327880859375 4.81417179107666 5.083685398101807 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 12.7327881Losses:  14.630437523126602 4.659821033477783 5.202024936676025 1.5042206346988678
CurrentTrain: epoch  5, batch     2 | loss: 14.6304375Losses:  13.784372836351395 4.841838836669922 5.0 1.563294917345047
CurrentTrain: epoch  5, batch     3 | loss: 13.7843728Losses:  12.436774253845215 4.679580211639404 5.089909076690674 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 12.4367743Losses:  13.993295017629862 4.59713888168335 5.080877304077148 1.460620228201151
CurrentTrain: epoch  6, batch     1 | loss: 13.9932950Losses:  18.469379149377346 4.636528968811035 4.979522705078125 5.945650778710842
CurrentTrain: epoch  6, batch     2 | loss: 18.4693791Losses:  13.040086597204208 4.537093162536621 5.0 1.46000275015831
CurrentTrain: epoch  6, batch     3 | loss: 13.0400866Losses:  13.642889380455017 4.544904708862305 4.948805332183838 1.4762195348739624
CurrentTrain: epoch  7, batch     0 | loss: 13.6428894Losses:  12.801338195800781 4.645081520080566 5.0028533935546875 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 12.8013382Losses:  16.185399428009987 4.620673179626465 5.02549934387207 4.205127134919167
CurrentTrain: epoch  7, batch     2 | loss: 16.1853994Losses:  12.92148070782423 4.568269729614258 5.0 1.452833704650402
CurrentTrain: epoch  7, batch     3 | loss: 12.9214807Losses:  13.688442286103964 4.528911590576172 5.007081508636475 1.4401207529008389
CurrentTrain: epoch  8, batch     0 | loss: 13.6884423Losses:  13.898318473249674 4.550552845001221 4.99137020111084 1.603852454572916
CurrentTrain: epoch  8, batch     1 | loss: 13.8983185Losses:  12.008865356445312 4.603652000427246 5.011027812957764 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 12.0088654Losses:  13.226638484746218 4.22136116027832 5.0 1.5196844823658466
CurrentTrain: epoch  8, batch     3 | loss: 13.2266385Losses:  15.014169812202454 4.71235466003418 4.970553398132324 2.8239375352859497
CurrentTrain: epoch  9, batch     0 | loss: 15.0141698Losses:  12.353748321533203 4.505847454071045 4.964988708496094 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 12.3537483Losses:  17.535358674824238 4.3980607986450195 5.02630615234375 5.711587198078632
CurrentTrain: epoch  9, batch     2 | loss: 17.5353587Losses:  12.019717074930668 3.77359676361084 5.0 1.4857052341103554
CurrentTrain: epoch  9, batch     3 | loss: 12.0197171
Losses:  10.132091522216797 4.688562393188477 4.97580623626709 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.1320915Losses:  9.942065238952637 4.586926460266113 5.022435188293457 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 9.9420652Losses:  10.309417724609375 4.15787935256958 5.052968502044678 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 10.3094177Losses:  9.667732238769531 4.482836723327637 4.9954915046691895 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 9.6677322Losses:  10.533597946166992 4.539008617401123 5.03497838973999 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 10.5335979Losses:  10.379636764526367 4.459178447723389 5.0288543701171875 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 10.3796368Losses:  9.97229290008545 4.555788993835449 5.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.9722929Losses:  9.54631233215332 4.31292200088501 5.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 9.5463123Losses:  10.275928497314453 4.674768447875977 4.998560905456543 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 10.2759285Losses:  9.466909408569336 4.399474143981934 5.013278484344482 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.4669094Losses:  9.505940437316895 4.443081378936768 4.961540222167969 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 9.5059404Losses:  10.227575302124023 4.664849281311035 5.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 10.2275753Losses:  9.576993942260742 4.458895206451416 5.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.5769939Losses:  9.359489440917969 4.285676002502441 5.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.3594894Losses:  9.823145866394043 4.748406410217285 5.0 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 9.8231459Losses:  9.485883712768555 4.393372058868408 5.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.4858837Losses:  9.423847198486328 4.30543327331543 5.025354862213135 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 9.4238472Losses:  9.727235794067383 4.667231559753418 5.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 9.7272358Losses:  9.328768730163574 4.2654290199279785 5.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.3287687Losses:  9.691015243530273 4.609335899353027 5.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.6910152Losses:  9.443410873413086 4.426383018493652 5.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 9.4434109Losses:  9.396889686584473 4.347893714904785 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.3968897Losses:  9.42141342163086 4.3714985847473145 5.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 9.4214134Losses:  9.460744857788086 4.424683570861816 5.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 9.4607449Losses:  9.556941986083984 4.531513214111328 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.5569420Losses:  9.33057689666748 4.30328893661499 5.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.3305769Losses:  9.211528778076172 4.163984775543213 5.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 9.2115288Losses:  9.441778182983398 4.40800142288208 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.4417782Losses:  9.375938415527344 4.349390983581543 5.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 9.3759384Losses:  9.222030639648438 4.202470302581787 5.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 9.2220306
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 71.31%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 73.57%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 79.43%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 79.13%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 79.63%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 79.77%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.16%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.69%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.39%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.23%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.29%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 89.91%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.98%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.52%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.41%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.64%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.45%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 89.26%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 89.08%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.04%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 89.02%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 88.70%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 88.53%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 88.37%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 87.93%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 87.71%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 87.29%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 87.01%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 86.54%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 86.21%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 85.55%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.04%   [EVAL] batch:   94 | acc: 25.00%,  total acc: 84.41%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 83.72%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 83.25%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 82.84%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 82.58%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 81.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.68%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 81.31%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 80.95%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.71%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 80.42%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.02%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 79.57%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 79.07%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 78.47%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 78.04%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 77.73%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 78.82%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.45%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.02%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.69%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 78.17%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 77.76%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 77.31%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 76.91%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.81%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.38%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.01%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 76.60%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 76.19%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.96%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.69%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.15%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.31%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.12%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 75.62%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.12%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 74.68%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 74.19%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 73.72%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 74.42%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.32%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 74.13%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 74.14%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 74.08%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 73.98%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 73.88%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 73.86%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 73.76%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 73.42%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 73.36%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 73.25%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 73.80%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 74.01%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 74.32%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 74.46%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 74.49%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 74.47%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 74.44%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 74.41%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.42%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 74.26%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 74.21%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 74.06%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 73.86%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 73.71%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 73.45%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 73.28%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 73.02%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.98%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 73.76%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 73.79%   [EVAL] batch:  222 | acc: 75.00%,  total acc: 73.79%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 75.08%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 75.08%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 75.10%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 75.08%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 75.10%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.60%   
cur_acc:  ['0.9435', '0.7401', '0.6756', '0.8016']
his_acc:  ['0.9435', '0.8350', '0.7726', '0.7560']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  20.075994204729795 5.101298809051514 5.743157863616943 2.7192017547786236
CurrentTrain: epoch  0, batch     0 | loss: 20.0759942Losses:  23.393933836370707 5.037829399108887 5.631215572357178 6.919108930975199
CurrentTrain: epoch  0, batch     1 | loss: 23.3939338Losses:  19.754413217306137 5.045446395874023 5.625394344329834 2.9862609803676605
CurrentTrain: epoch  0, batch     2 | loss: 19.7544132Losses:  17.912394385784864 5.023552894592285 5.206229209899902 1.6032475046813488
CurrentTrain: epoch  0, batch     3 | loss: 17.9123944Losses:  25.054935809224844 4.9964447021484375 5.845167636871338 9.057318087667227
CurrentTrain: epoch  1, batch     0 | loss: 25.0549358Losses:  16.336883544921875 5.014861106872559 5.548994064331055 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 16.3368835Losses:  16.037826538085938 5.094890594482422 5.39163064956665 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 16.0378265Losses:  16.191088028252125 4.967796325683594 5.162167549133301 1.4345172122120857
CurrentTrain: epoch  1, batch     3 | loss: 16.1910880Losses:  18.46439803019166 4.955746173858643 5.74570369720459 1.5389486588537693
CurrentTrain: epoch  2, batch     0 | loss: 18.4643980Losses:  16.411489844322205 4.945493698120117 5.435751438140869 1.4451392889022827
CurrentTrain: epoch  2, batch     1 | loss: 16.4114898Losses:  23.527726102620363 4.966744422912598 5.325953483581543 9.224701810628176
CurrentTrain: epoch  2, batch     2 | loss: 23.5277261Losses:  15.606779165565968 5.013944625854492 4.745401382446289 1.4642859175801277
CurrentTrain: epoch  2, batch     3 | loss: 15.6067792Losses:  15.345932006835938 4.961485862731934 5.491142749786377 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.3459320Losses:  15.97303169965744 4.922722339630127 5.410671710968018 1.4358097910881042
CurrentTrain: epoch  3, batch     1 | loss: 15.9730317Losses:  17.868438996374607 4.92834997177124 5.261028289794922 3.2546160593628883
CurrentTrain: epoch  3, batch     2 | loss: 17.8684390Losses:  15.644095174968243 4.719808578491211 5.55350399017334 1.4629104062914848
CurrentTrain: epoch  3, batch     3 | loss: 15.6440952Losses:  21.27088961750269 4.944488525390625 5.335782051086426 6.426164008677006
CurrentTrain: epoch  4, batch     0 | loss: 21.2708896Losses:  15.991981863975525 4.914352893829346 5.369620323181152 1.628576636314392
CurrentTrain: epoch  4, batch     1 | loss: 15.9919819Losses:  15.372693471610546 4.851720809936523 5.331090927124023 1.588687352836132
CurrentTrain: epoch  4, batch     2 | loss: 15.3726935Losses:  14.879235669970512 4.678102493286133 5.01108455657959 1.4656032770872116
CurrentTrain: epoch  4, batch     3 | loss: 14.8792357Losses:  18.48343200236559 4.895510673522949 5.341877460479736 3.475854106247425
CurrentTrain: epoch  5, batch     0 | loss: 18.4834320Losses:  13.38092041015625 4.863271713256836 5.227594375610352 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.3809204Losses:  13.959076881408691 4.862971305847168 5.268424034118652 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 13.9590769Losses:  13.621934235095978 4.884034156799316 5.070430755615234 1.4292929768562317
CurrentTrain: epoch  5, batch     3 | loss: 13.6219342Losses:  18.893126875162125 4.778528213500977 5.087303638458252 4.798358350992203
CurrentTrain: epoch  6, batch     0 | loss: 18.8931269Losses:  16.236298348754644 4.842390537261963 5.205751419067383 2.9974353574216366
CurrentTrain: epoch  6, batch     1 | loss: 16.2362983Losses:  15.572396129369736 4.9128193855285645 5.292981147766113 1.444941371679306
CurrentTrain: epoch  6, batch     2 | loss: 15.5723961Losses:  13.377746194601059 4.989902496337891 5.0 1.4544330537319183
CurrentTrain: epoch  6, batch     3 | loss: 13.3777462Losses:  16.157304767519236 4.759576320648193 5.145528793334961 2.88231373205781
CurrentTrain: epoch  7, batch     0 | loss: 16.1573048Losses:  13.613502502441406 4.903419494628906 5.119116306304932 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 13.6135025Losses:  14.871043600142002 4.745037078857422 5.193024635314941 1.4460300579667091
CurrentTrain: epoch  7, batch     2 | loss: 14.8710436Losses:  15.490054070949554 4.732134819030762 5.0 1.4091710448265076
CurrentTrain: epoch  7, batch     3 | loss: 15.4900541Losses:  13.54373550415039 4.735713958740234 5.1138224601745605 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 13.5437355Losses:  19.037717174738646 4.744397163391113 5.215115547180176 5.860802959650755
CurrentTrain: epoch  8, batch     1 | loss: 19.0377172Losses:  16.403920639306307 4.7778520584106445 5.0594964027404785 3.1601195223629475
CurrentTrain: epoch  8, batch     2 | loss: 16.4039206Losses:  13.424651354551315 4.644927978515625 5.0 1.3878738582134247
CurrentTrain: epoch  8, batch     3 | loss: 13.4246514Losses:  16.062311805784702 4.721138000488281 5.17714786529541 2.9519507065415382
CurrentTrain: epoch  9, batch     0 | loss: 16.0623118Losses:  14.532721247524023 4.769977569580078 5.062277793884277 1.47096511349082
CurrentTrain: epoch  9, batch     1 | loss: 14.5327212Losses:  13.095170974731445 4.753087520599365 5.130459308624268 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 13.0951710Losses:  13.747427180409431 4.496491432189941 5.0 1.4183647185564041
CurrentTrain: epoch  9, batch     3 | loss: 13.7474272
Losses:  10.379225730895996 4.697421073913574 5.035897731781006 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.3792257Losses:  10.014625549316406 4.699193000793457 5.0 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 10.0146255Losses:  9.378037452697754 4.352818012237549 4.960099220275879 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 9.3780375Losses:  11.472169037908316 4.200543403625488 4.899888038635254 1.5371524058282375
MemoryTrain:  epoch  0, batch     3 | loss: 11.4721690Losses:  10.235820770263672 4.706735610961914 5.0493927001953125 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 10.2358208Losses:  9.818241119384766 4.395576477050781 5.021500110626221 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 9.8182411Losses:  9.87244987487793 4.493978977203369 5.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 9.8724499Losses:  11.1429413408041 4.494866371154785 5.0 1.4331577867269516
MemoryTrain:  epoch  1, batch     3 | loss: 11.1429413Losses:  9.737198829650879 4.4731526374816895 5.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.7371988Losses:  9.736692428588867 4.58284854888916 5.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 9.7366924Losses:  9.665966033935547 4.488946437835693 5.01971960067749 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 9.6659660Losses:  11.240325413644314 4.721968650817871 5.0 1.422778569161892
MemoryTrain:  epoch  2, batch     3 | loss: 11.2403254Losses:  9.938385963439941 4.689418792724609 4.998737335205078 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.9383860Losses:  9.499110221862793 4.40090799331665 5.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 9.4991102Losses:  9.542989730834961 4.475263595581055 5.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 9.5429897Losses:  10.475748158991337 3.9420127868652344 5.0 1.5062876716256142
MemoryTrain:  epoch  3, batch     3 | loss: 10.4757482Losses:  9.587194442749023 4.476040840148926 5.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.5871944Losses:  9.709468841552734 4.619268417358398 5.036068916320801 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.7094688Losses:  9.479994773864746 4.3640289306640625 5.0 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 9.4799948Losses:  11.316657401621342 4.659339904785156 5.0 1.5011466518044472
MemoryTrain:  epoch  4, batch     3 | loss: 11.3166574Losses:  9.421220779418945 4.342814922332764 5.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.4212208Losses:  9.458808898925781 4.352147102355957 5.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 9.4588089Losses:  9.723394393920898 4.636302947998047 5.033576011657715 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 9.7233944Losses:  11.5466826595366 4.996650695800781 5.0 1.5244687236845493
MemoryTrain:  epoch  5, batch     3 | loss: 11.5466827Losses:  9.570257186889648 4.4966206550598145 5.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.5702572Losses:  9.589138984680176 4.501626968383789 5.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.5891390Losses:  9.422993659973145 4.3826799392700195 5.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 9.4229937Losses:  11.033104561269283 4.409771919250488 5.0 1.538672111928463
MemoryTrain:  epoch  6, batch     3 | loss: 11.0331046Losses:  9.527262687683105 4.4854888916015625 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.5272627Losses:  9.365537643432617 4.315065383911133 5.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 9.3655376Losses:  9.510229110717773 4.469352722167969 5.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 9.5102291Losses:  11.228673316538334 4.598388671875 5.0 1.549960471689701
MemoryTrain:  epoch  7, batch     3 | loss: 11.2286733Losses:  9.310745239257812 4.250126838684082 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.3107452Losses:  9.548399925231934 4.488760948181152 5.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.5483999Losses:  9.481298446655273 4.4378814697265625 5.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 9.4812984Losses:  11.0527039244771 4.54404354095459 5.0 1.4430008605122566
MemoryTrain:  epoch  8, batch     3 | loss: 11.0527039Losses:  9.481472969055176 4.410262107849121 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.4814730Losses:  9.320547103881836 4.2771806716918945 5.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 9.3205471Losses:  9.532602310180664 4.4767584800720215 5.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 9.5326023Losses:  11.027009040117264 4.5238237380981445 5.0 1.4353895485401154
MemoryTrain:  epoch  9, batch     3 | loss: 11.0270090
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 39.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 41.35%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 41.52%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 42.08%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 42.58%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.89%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 57.69%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 56.48%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 55.36%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 55.60%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 55.00%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 54.64%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 54.30%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 52.76%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 52.32%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 51.39%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 50.33%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 50.64%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 51.25%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 51.83%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 52.83%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 53.34%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 53.84%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 53.61%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 53.67%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 53.99%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 54.30%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 54.85%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 55.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 56.13%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 57.78%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 58.56%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 59.32%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 60.04%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 62.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.69%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.36%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 89.36%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.38%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.56%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.95%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.15%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.21%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.33%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 88.98%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 88.56%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 88.30%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.13%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.97%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 87.73%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 87.65%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 87.27%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 87.13%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 86.91%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 86.77%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 86.28%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 86.15%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 85.81%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 85.49%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 85.03%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 84.65%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 84.14%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 83.64%   [EVAL] batch:   94 | acc: 12.50%,  total acc: 82.89%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 82.23%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 81.57%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 81.06%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 79.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.70%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 79.41%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 79.07%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.81%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 78.66%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.27%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 77.84%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 77.35%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.76%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.35%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 76.06%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.88%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.51%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 77.82%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 77.58%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 77.17%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 76.61%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 76.31%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.82%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 75.43%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.40%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 75.94%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 75.54%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 75.13%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 74.78%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 74.65%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 74.39%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.83%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 74.34%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.86%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 73.38%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 72.90%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.44%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 73.16%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.09%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 72.85%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 72.75%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 72.71%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 72.61%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 72.51%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 72.42%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 72.33%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 72.18%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 72.05%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.88%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 73.15%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 73.19%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 73.17%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 73.07%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 73.11%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 73.12%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 72.95%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.91%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 72.80%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 72.60%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 72.37%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.98%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 71.76%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 71.71%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 72.47%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 72.52%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 72.49%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 72.39%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.51%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.79%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 73.80%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 73.82%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 74.10%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 73.88%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 73.64%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 73.43%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 73.16%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 72.81%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 72.87%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.85%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 72.86%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 72.77%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 72.76%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 72.63%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 72.55%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 72.43%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 72.34%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 72.80%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 72.63%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 72.46%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 72.30%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 72.20%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 72.02%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.79%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 71.48%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 71.23%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 71.17%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 71.29%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 71.21%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 71.16%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 71.17%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 72.22%   
cur_acc:  ['0.9435', '0.7401', '0.6756', '0.8016', '0.6369']
his_acc:  ['0.9435', '0.8350', '0.7726', '0.7560', '0.7222']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  16.704801559448242 5.020587921142578 5.280664920806885 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 16.7048016Losses:  18.358649615198374 4.9615702629089355 5.410938262939453 1.5278505124151707
CurrentTrain: epoch  0, batch     1 | loss: 18.3586496Losses:  18.183540374040604 5.021295070648193 5.166847229003906 1.500696212053299
CurrentTrain: epoch  0, batch     2 | loss: 18.1835404Losses:  20.740549210458994 5.134799003601074 5.86042594909668 1.6638623513281345
CurrentTrain: epoch  0, batch     3 | loss: 20.7405492Losses:  17.040415346622467 4.985410690307617 5.011639595031738 1.439819872379303
CurrentTrain: epoch  1, batch     0 | loss: 17.0404153Losses:  18.170266538858414 4.966889381408691 5.255064487457275 1.8821863234043121
CurrentTrain: epoch  1, batch     1 | loss: 18.1702665Losses:  15.126492500305176 4.948915481567383 5.275904655456543 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 15.1264925Losses:  16.241910114884377 4.873665809631348 4.854935646057129 1.5259648710489273
CurrentTrain: epoch  1, batch     3 | loss: 16.2419101Losses:  17.428273737430573 4.957084655761719 5.057375907897949 2.388616144657135
CurrentTrain: epoch  2, batch     0 | loss: 17.4282737Losses:  15.391031265258789 4.894012928009033 5.193605422973633 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.3910313Losses:  17.72653701901436 4.91176700592041 5.167623996734619 2.8613560497760773
CurrentTrain: epoch  2, batch     2 | loss: 17.7265370Losses:  24.588575467467308 5.0 5.0 10.673729047179222
CurrentTrain: epoch  2, batch     3 | loss: 24.5885755Losses:  19.014177948236465 4.9033708572387695 5.088137149810791 4.497336059808731
CurrentTrain: epoch  3, batch     0 | loss: 19.0141779Losses:  16.557495220564306 4.924269676208496 5.12838077545166 2.2161952098831534
CurrentTrain: epoch  3, batch     1 | loss: 16.5574952Losses:  16.11687684059143 4.873419284820557 4.983458518981934 1.5678808689117432
CurrentTrain: epoch  3, batch     2 | loss: 16.1168768Losses:  18.378790698945522 4.890478134155273 5.386137962341309 1.5503319129347801
CurrentTrain: epoch  3, batch     3 | loss: 18.3787907Losses:  15.38766872882843 4.881603240966797 5.0619635581970215 1.4694920778274536
CurrentTrain: epoch  4, batch     0 | loss: 15.3876687Losses:  14.659521102905273 4.86711311340332 5.007299423217773 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 14.6595211Losses:  15.53033658489585 4.875368118286133 5.024318695068359 1.4341594837605953
CurrentTrain: epoch  4, batch     2 | loss: 15.5303366Losses:  14.348127849400043 4.9493513107299805 5.0 1.5750346258282661
CurrentTrain: epoch  4, batch     3 | loss: 14.3481278Losses:  22.20696174353361 4.784999847412109 5.0434746742248535 9.012484662234783
CurrentTrain: epoch  5, batch     0 | loss: 22.2069617Losses:  15.445165753364563 4.831035137176514 5.026013374328613 1.4100791215896606
CurrentTrain: epoch  5, batch     1 | loss: 15.4451658Losses:  20.389813795685768 4.934520244598389 5.05078125 5.703244581818581
CurrentTrain: epoch  5, batch     2 | loss: 20.3898138Losses:  14.155162837356329 5.0 4.941441535949707 1.549560572952032
CurrentTrain: epoch  5, batch     3 | loss: 14.1551628Losses:  16.464248552918434 4.883815288543701 5.035414218902588 3.026637926697731
CurrentTrain: epoch  6, batch     0 | loss: 16.4642486Losses:  16.780681282281876 4.876640796661377 5.04019021987915 2.829351097345352
CurrentTrain: epoch  6, batch     1 | loss: 16.7806813Losses:  15.185989171266556 4.778362274169922 5.032689094543457 1.4225452244281769
CurrentTrain: epoch  6, batch     2 | loss: 15.1859892Losses:  15.159944225102663 4.798733711242676 5.0 1.6638066060841084
CurrentTrain: epoch  6, batch     3 | loss: 15.1599442Losses:  17.47519312053919 4.923961639404297 4.970694541931152 3.0324440971016884
CurrentTrain: epoch  7, batch     0 | loss: 17.4751931Losses:  16.05005905032158 4.798211097717285 5.038090705871582 2.876674383878708
CurrentTrain: epoch  7, batch     1 | loss: 16.0500591Losses:  17.488863561302423 4.754693031311035 4.986428260803223 4.59854469075799
CurrentTrain: epoch  7, batch     2 | loss: 17.4888636Losses:  13.379573732614517 4.708310127258301 4.729928970336914 1.419060617685318
CurrentTrain: epoch  7, batch     3 | loss: 13.3795737Losses:  14.941033594310284 4.784419059753418 4.95552921295166 1.4367468282580376
CurrentTrain: epoch  8, batch     0 | loss: 14.9410336Losses:  15.971502862870693 4.851741313934326 5.0019941329956055 2.8783832415938377
CurrentTrain: epoch  8, batch     1 | loss: 15.9715029Losses:  16.203709304332733 4.8409271240234375 5.015847206115723 2.882340133190155
CurrentTrain: epoch  8, batch     2 | loss: 16.2037093Losses:  13.001019045710564 4.534523963928223 5.0 1.435951754450798
CurrentTrain: epoch  8, batch     3 | loss: 13.0010190Losses:  15.977681934833527 4.841743469238281 5.0079498291015625 2.8857467770576477
CurrentTrain: epoch  9, batch     0 | loss: 15.9776819Losses:  14.180323749780655 4.720729827880859 4.973381996154785 1.4036056101322174
CurrentTrain: epoch  9, batch     1 | loss: 14.1803237Losses:  13.626416206359863 4.7592878341674805 5.005505561828613 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 13.6264162Losses:  18.521445028483868 5.0 5.0 6.543047659099102
CurrentTrain: epoch  9, batch     3 | loss: 18.5214450
Losses:  9.983266830444336 4.479335308074951 5.0242133140563965 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 9.9832668Losses:  9.661571502685547 4.458108901977539 5.0 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 9.6615715Losses:  9.884387016296387 4.474301815032959 5.0 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 9.8843870Losses:  9.927033424377441 4.546533107757568 5.041745662689209 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 9.9270334Losses:  10.199909210205078 4.433445930480957 5.021501064300537 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 10.1999092Losses:  10.49521255493164 4.635060787200928 5.021592617034912 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 10.4952126Losses:  9.655729293823242 4.420526504516602 5.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 9.6557293Losses:  10.105087280273438 4.282657146453857 5.026850700378418 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 10.1050873Losses:  9.769613265991211 4.328557968139648 5.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.7696133Losses:  9.949586868286133 4.637693405151367 5.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 9.9495869Losses:  9.590768814086914 4.327670097351074 5.0 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 9.5907688Losses:  9.955812454223633 4.449871063232422 5.0 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 9.9558125Losses:  9.567626953125 4.353606224060059 5.031003952026367 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.5676270Losses:  9.613784790039062 4.507635593414307 5.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 9.6137848Losses:  9.644634246826172 4.411598205566406 5.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 9.6446342Losses:  9.458620071411133 4.349293231964111 5.0 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 9.4586201Losses:  9.727564811706543 4.62237548828125 5.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.7275648Losses:  9.439886093139648 4.324586391448975 5.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.4398861Losses:  9.294690132141113 4.185397624969482 5.0 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 9.2946901Losses:  9.552719116210938 4.466283798217773 5.0 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 9.5527191Losses:  9.402791976928711 4.317378997802734 5.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.4027920Losses:  9.334125518798828 4.267917633056641 5.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 9.3341255Losses:  9.548470497131348 4.471121788024902 5.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 9.5484705Losses:  9.77093505859375 4.458133697509766 5.033934593200684 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 9.7709351Losses:  9.51541805267334 4.416491508483887 5.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.5154181Losses:  9.472454071044922 4.401730060577393 5.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.4724541Losses:  9.278829574584961 4.227414131164551 5.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 9.2788296Losses:  9.732616424560547 4.339567184448242 5.0375871658325195 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 9.7326164Losses:  9.577428817749023 4.507626533508301 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.5774288Losses:  9.362066268920898 4.297006607055664 5.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 9.3620663Losses:  9.245738983154297 4.2167840003967285 5.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 9.2457390Losses:  9.318102836608887 4.25644588470459 5.0 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 9.3181028Losses:  9.324705123901367 4.265896320343018 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.3247051Losses:  9.43513298034668 4.378889560699463 5.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.4351330Losses:  9.378267288208008 4.340251922607422 5.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 9.3782673Losses:  9.368417739868164 4.326179027557373 5.0 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 9.3684177Losses:  9.542610168457031 4.497044563293457 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.5426102Losses:  8.908829689025879 3.8758649826049805 5.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 8.9088297Losses:  9.476234436035156 4.436972141265869 5.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 9.4762344Losses:  9.509109497070312 4.39564323425293 5.033987045288086 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 9.5091095
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 74.51%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 73.08%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 12.50%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 67.73%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 69.93%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 68.19%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 68.02%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.25%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 85.29%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.04%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.34%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 87.61%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 87.39%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.99%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 86.79%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 86.13%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 85.73%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 85.48%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 85.24%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.45%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.79%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 85.77%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 85.23%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 85.02%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 84.89%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 84.57%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 84.53%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 84.19%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 83.60%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 83.36%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 82.97%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 82.65%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 82.43%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 82.07%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 81.86%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 81.38%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.85%   [EVAL] batch:   94 | acc: 12.50%,  total acc: 80.13%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 79.43%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 78.74%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 78.19%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 77.71%   [EVAL] batch:   99 | acc: 0.00%,  total acc: 76.94%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 76.41%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 75.91%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 75.54%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 75.24%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.88%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 74.48%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 74.03%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 73.52%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 73.14%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 72.94%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 74.01%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 73.81%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 73.63%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 72.87%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 72.07%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 71.39%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 71.04%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 71.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 71.54%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 71.16%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 70.79%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 70.47%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.28%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.05%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.82%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 70.74%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 70.27%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.81%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 69.36%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.91%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 69.25%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 69.02%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 68.71%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 68.37%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 68.04%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 67.78%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 67.64%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 67.43%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 67.10%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 66.98%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 66.98%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 67.02%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 68.40%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 68.18%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 68.02%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 67.80%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 67.53%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 67.31%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 67.26%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 67.25%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.26%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 67.26%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 67.18%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 66.98%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 66.78%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 66.58%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 66.24%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 67.07%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 68.80%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 69.37%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 69.22%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 69.00%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 68.63%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 68.41%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 68.32%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 68.14%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 68.05%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 68.39%   [EVAL] batch:  277 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 68.02%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 67.95%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 67.87%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 67.67%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 67.52%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.12%   [EVAL] batch:  287 | acc: 25.00%,  total acc: 66.97%   [EVAL] batch:  288 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 66.92%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 66.90%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 66.82%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 66.74%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 67.56%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 67.36%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 67.19%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 67.01%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 66.88%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 66.75%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 66.82%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 67.18%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 67.13%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 67.23%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 68.45%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 68.17%   [EVAL] batch:  353 | acc: 12.50%,  total acc: 68.01%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 67.85%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 67.73%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 68.13%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 68.05%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 68.00%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 67.90%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 67.82%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 67.84%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 67.95%   
cur_acc:  ['0.9435', '0.7401', '0.6756', '0.8016', '0.6369', '0.6825']
his_acc:  ['0.9435', '0.8350', '0.7726', '0.7560', '0.7222', '0.6795']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  19.176603827625513 5.109432220458984 5.591414928436279 1.9322896338999271
CurrentTrain: epoch  0, batch     0 | loss: 19.1766038Losses:  19.631232022773474 5.051898002624512 5.315934181213379 2.594250440131873
CurrentTrain: epoch  0, batch     1 | loss: 19.6312320Losses:  21.809191770851612 4.897563457489014 5.317712783813477 5.351052351295948
CurrentTrain: epoch  0, batch     2 | loss: 21.8091918Losses:  22.062817831523716 5.15655517578125 5.755058288574219 2.166072149761021
CurrentTrain: epoch  0, batch     3 | loss: 22.0628178Losses:  17.545727223157883 4.989076614379883 5.4733805656433105 1.4198422133922577
CurrentTrain: epoch  1, batch     0 | loss: 17.5457272Losses:  16.213939666748047 5.018764495849609 5.304383277893066 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 16.2139397Losses:  18.368931099772453 4.879268169403076 5.335536479949951 2.443682000041008
CurrentTrain: epoch  1, batch     2 | loss: 18.3689311Losses:  17.93107657134533 5.39365291595459 5.497010231018066 1.4203114956617355
CurrentTrain: epoch  1, batch     3 | loss: 17.9310766Losses:  18.97343248873949 4.989119529724121 5.248615741729736 3.548522897064686
CurrentTrain: epoch  2, batch     0 | loss: 18.9734325Losses:  17.31957044824958 4.90064811706543 5.295942306518555 1.4126070998609066
CurrentTrain: epoch  2, batch     1 | loss: 17.3195704Losses:  19.240482062101364 4.972198963165283 5.475887298583984 2.807908743619919
CurrentTrain: epoch  2, batch     2 | loss: 19.2404821Losses:  18.264032624661922 4.931553840637207 5.475953102111816 1.596586488187313
CurrentTrain: epoch  2, batch     3 | loss: 18.2640326Losses:  15.474905967712402 4.912217140197754 5.344813346862793 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.4749060Losses:  21.097437597811222 4.951572895050049 5.445404529571533 4.813713766634464
CurrentTrain: epoch  3, batch     1 | loss: 21.0974376Losses:  16.416439071297646 4.8064284324646 5.206035137176514 1.5239906460046768
CurrentTrain: epoch  3, batch     2 | loss: 16.4164391Losses:  19.069526862353086 5.082512855529785 5.298183441162109 1.62327403947711
CurrentTrain: epoch  3, batch     3 | loss: 19.0695269Losses:  18.918577376753092 4.993268013000488 5.361311435699463 2.975296203047037
CurrentTrain: epoch  4, batch     0 | loss: 18.9185774Losses:  16.53823632001877 4.759957313537598 5.270969390869141 1.7759272456169128
CurrentTrain: epoch  4, batch     1 | loss: 16.5382363Losses:  15.662530899047852 4.879205703735352 5.330550670623779 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 15.6625309Losses:  15.595252335071564 4.861212730407715 5.345216751098633 1.4820178151130676
CurrentTrain: epoch  4, batch     3 | loss: 15.5952523Losses:  15.60423469543457 4.931530952453613 5.318070888519287 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 15.6042347Losses:  20.147374102845788 4.818750858306885 5.271746635437012 4.889211604371667
CurrentTrain: epoch  5, batch     1 | loss: 20.1473741Losses:  14.67888069152832 4.78724479675293 5.327848434448242 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 14.6788807Losses:  16.994775392115116 4.625779151916504 5.093000411987305 1.5691734328866005
CurrentTrain: epoch  5, batch     3 | loss: 16.9947754Losses:  17.70157226547599 4.649533271789551 5.193689346313477 2.9832027815282345
CurrentTrain: epoch  6, batch     0 | loss: 17.7015723Losses:  24.857010208070278 4.903643608093262 5.289860248565674 9.370621047914028
CurrentTrain: epoch  6, batch     1 | loss: 24.8570102Losses:  20.38519547879696 4.764564514160156 5.225742340087891 5.983345732092857
CurrentTrain: epoch  6, batch     2 | loss: 20.3851955Losses:  15.040651924908161 4.911714553833008 5.0 1.4923502281308174
CurrentTrain: epoch  6, batch     3 | loss: 15.0406519Losses:  17.050508815795183 4.705548286437988 5.109246253967285 2.9525836296379566
CurrentTrain: epoch  7, batch     0 | loss: 17.0505088Losses:  16.267495460808277 4.818721771240234 5.28978157043457 1.4355643466114998
CurrentTrain: epoch  7, batch     1 | loss: 16.2674955Losses:  17.955859940499067 4.813578128814697 5.274229526519775 3.7894075326621532
CurrentTrain: epoch  7, batch     2 | loss: 17.9558599Losses:  16.116226747632027 4.686121940612793 5.044098854064941 1.7283950597047806
CurrentTrain: epoch  7, batch     3 | loss: 16.1162267Losses:  17.306860700249672 4.698640823364258 5.108099937438965 2.7624375969171524
CurrentTrain: epoch  8, batch     0 | loss: 17.3068607Losses:  16.689075499773026 4.736424446105957 5.105330467224121 2.8302421867847443
CurrentTrain: epoch  8, batch     1 | loss: 16.6890755Losses:  20.010999858379364 4.749345779418945 5.253491401672363 6.184216678142548
CurrentTrain: epoch  8, batch     2 | loss: 20.0109999Losses:  15.604762777686119 5.122443199157715 5.409733772277832 1.4679496139287949
CurrentTrain: epoch  8, batch     3 | loss: 15.6047628Losses:  14.012739181518555 4.610514163970947 5.171670436859131 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 14.0127392Losses:  13.8062744140625 4.872551918029785 5.238151550292969 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 13.8062744Losses:  15.66406387463212 4.682853698730469 5.148167610168457 1.4818825162947178
CurrentTrain: epoch  9, batch     2 | loss: 15.6640639Losses:  14.477008514106274 4.633730888366699 4.968700408935547 1.4390847012400627
CurrentTrain: epoch  9, batch     3 | loss: 14.4770085
Losses:  9.618148803710938 4.511693954467773 4.964186668395996 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 9.6181488Losses:  9.865946769714355 4.413995742797852 5.008248805999756 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 9.8659468Losses:  9.647109985351562 4.522852420806885 5.040377616882324 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 9.6471100Losses:  9.95146656036377 4.563592910766602 5.034656524658203 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 9.9514666Losses:  14.318663775920868 4.364781379699707 5.0 4.31478613615036
MemoryTrain:  epoch  0, batch     4 | loss: 14.3186638Losses:  10.14980697631836 4.509434700012207 5.033628940582275 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 10.1498070Losses:  9.891707420349121 4.403685569763184 5.0 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 9.8917074Losses:  9.676189422607422 4.464169025421143 5.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 9.6761894Losses:  9.572484016418457 4.436525344848633 5.0 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 9.5724840Losses:  14.130526047199965 4.515995979309082 5.0 4.377201538532972
MemoryTrain:  epoch  1, batch     4 | loss: 14.1305260Losses:  9.925040245056152 4.666449546813965 5.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.9250402Losses:  9.279842376708984 4.16930627822876 5.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 9.2798424Losses:  9.730171203613281 4.592453956604004 5.020903587341309 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 9.7301712Losses:  9.878524780273438 4.429934501647949 5.041512489318848 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 9.8785248Losses:  13.515729147940874 4.101527214050293 5.0 4.331127364188433
MemoryTrain:  epoch  2, batch     4 | loss: 13.5157291Losses:  9.685004234313965 4.51474666595459 5.028505802154541 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.6850042Losses:  9.236343383789062 4.131799221038818 5.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 9.2363434Losses:  9.71092414855957 4.543275833129883 5.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 9.7109241Losses:  9.736848831176758 4.637198448181152 5.0 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 9.7368488Losses:  13.306174244731665 3.9734535217285156 5.0 4.243005719035864
MemoryTrain:  epoch  3, batch     4 | loss: 13.3061742Losses:  9.420482635498047 4.357822895050049 5.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.4204826Losses:  9.547017097473145 4.477892875671387 5.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.5470171Losses:  9.538206100463867 4.444986820220947 5.028614521026611 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 9.5382061Losses:  9.454479217529297 4.355156898498535 5.010425090789795 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 9.4544792Losses:  13.616540014743805 4.319386959075928 5.0 4.2373228669166565
MemoryTrain:  epoch  4, batch     4 | loss: 13.6165400Losses:  9.622503280639648 4.569093227386475 5.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.6225033Losses:  9.499433517456055 4.421319961547852 5.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 9.4994335Losses:  9.318016052246094 4.256453514099121 5.0062150955200195 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 9.3180161Losses:  9.318305969238281 4.233550071716309 5.0 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 9.3183060Losses:  14.069754879921675 4.581021308898926 5.0 4.44225625321269
MemoryTrain:  epoch  5, batch     4 | loss: 14.0697549Losses:  9.47262191772461 4.400325298309326 5.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.4726219Losses:  9.554777145385742 4.498634338378906 5.005156993865967 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.5547771Losses:  9.383495330810547 4.342158317565918 5.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 9.3834953Losses:  9.37801456451416 4.30788516998291 5.024982929229736 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 9.3780146Losses:  13.380322881042957 4.081082820892334 5.0 4.233193822205067
MemoryTrain:  epoch  6, batch     4 | loss: 13.3803229Losses:  9.315147399902344 4.2686381340026855 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.3151474Losses:  9.259169578552246 4.2043561935424805 5.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 9.2591696Losses:  9.455472946166992 4.3747358322143555 5.031080722808838 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 9.4554729Losses:  9.676589965820312 4.622045516967773 5.0 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 9.6765900Losses:  13.189474072307348 3.876598358154297 5.0 4.279621090739965
MemoryTrain:  epoch  7, batch     4 | loss: 13.1894741Losses:  9.250927925109863 4.207762718200684 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.2509279Losses:  9.45633602142334 4.406838417053223 5.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.4563360Losses:  9.232250213623047 4.172006607055664 5.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 9.2322502Losses:  9.583555221557617 4.536463737487793 5.0 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 9.5835552Losses:  13.53187034651637 4.185748100280762 5.0 4.304502945393324
MemoryTrain:  epoch  8, batch     4 | loss: 13.5318703Losses:  9.145121574401855 4.101391315460205 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.1451216Losses:  9.48155403137207 4.434384346008301 5.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 9.4815540Losses:  9.34530258178711 4.2777252197265625 5.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 9.3453026Losses:  9.429211616516113 4.388890266418457 5.0 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 9.4292116Losses:  13.562304578721523 4.201196670532227 5.0 4.323950849473476
MemoryTrain:  epoch  9, batch     4 | loss: 13.5623046
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 52.56%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 52.17%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 50.52%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 49.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 47.84%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.06%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 44.42%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 42.89%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 41.67%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 40.32%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 42.80%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 44.30%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 45.89%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 47.80%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 49.01%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 49.84%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 51.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 52.29%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 53.27%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 54.36%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.26%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 55.83%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 56.65%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 57.72%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.29%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 58.61%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 59.43%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 59.93%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 59.65%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 59.16%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 58.90%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 58.85%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 58.61%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 58.77%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 58.13%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.02%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 85.07%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 84.54%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 84.05%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 83.90%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 83.61%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.47%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 83.20%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 83.27%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 82.95%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 82.56%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 82.44%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 83.06%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 82.55%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.37%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 82.28%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 82.19%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 82.02%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 81.55%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 81.40%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.18%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 81.03%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 80.60%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 80.40%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 79.99%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 79.58%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 79.19%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 78.74%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 78.23%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 77.66%   [EVAL] batch:   94 | acc: 12.50%,  total acc: 76.97%   [EVAL] batch:   95 | acc: 6.25%,  total acc: 76.24%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 75.52%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 74.94%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 74.31%   [EVAL] batch:   99 | acc: 0.00%,  total acc: 73.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 73.39%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 73.10%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 72.36%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 72.20%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 71.93%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 71.18%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 70.81%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 70.40%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 70.05%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 69.87%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 69.80%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 71.07%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 71.06%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 70.87%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 70.39%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 69.98%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 69.58%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 69.33%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 68.85%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 68.61%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.66%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 69.11%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 68.71%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 68.35%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 68.04%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 67.74%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 68.13%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 67.68%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 67.24%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 66.80%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 66.37%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 65.95%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 66.56%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 66.23%   [EVAL] batch:  164 | acc: 6.25%,  total acc: 65.87%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 65.47%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 65.08%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 64.69%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 64.42%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 64.34%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 64.29%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 64.14%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 63.90%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 63.85%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 63.81%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 63.69%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 63.74%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 63.77%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 64.57%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.30%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 65.51%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.50%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 65.42%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 65.33%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 65.39%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 65.29%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 65.24%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 65.04%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 64.78%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 64.53%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 64.32%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 64.10%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 63.88%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.94%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 66.85%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 67.36%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 66.92%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 66.76%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 66.57%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 66.27%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 66.30%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.27%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 66.30%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.18%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 66.68%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 66.47%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 66.37%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 66.35%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 66.27%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 66.19%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.13%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 66.10%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 65.81%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 65.65%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.42%   [EVAL] batch:  287 | acc: 18.75%,  total acc: 65.26%   [EVAL] batch:  288 | acc: 25.00%,  total acc: 65.12%   [EVAL] batch:  289 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:  290 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:  291 | acc: 37.50%,  total acc: 64.81%   [EVAL] batch:  292 | acc: 37.50%,  total acc: 64.72%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 64.60%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 64.49%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 64.40%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 64.35%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 64.28%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 64.21%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 65.09%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 65.25%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 64.89%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 64.71%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 64.46%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 64.51%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 64.97%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 64.91%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 64.89%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 66.31%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 66.18%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 66.06%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 65.76%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.13%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 65.88%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 65.63%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 65.66%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 65.63%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 65.59%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 65.39%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 65.24%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 65.08%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 64.94%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 64.77%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 64.62%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 64.61%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:  395 | acc: 43.75%,  total acc: 64.95%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 64.83%   [EVAL] batch:  397 | acc: 43.75%,  total acc: 64.78%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 64.65%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 64.55%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 64.24%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 64.08%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 63.92%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 63.78%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  407 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 64.47%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 64.51%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  425 | acc: 50.00%,  total acc: 64.61%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 64.74%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:  432 | acc: 31.25%,  total acc: 64.69%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 64.65%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 64.63%   [EVAL] batch:  435 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 64.59%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 64.48%   
cur_acc:  ['0.9435', '0.7401', '0.6756', '0.8016', '0.6369', '0.6825', '0.5813']
his_acc:  ['0.9435', '0.8350', '0.7726', '0.7560', '0.7222', '0.6795', '0.6448']
Clustering into  39  clusters
Clusters:  [31  0 23  2  1 35 24 10 14 14  8 38  7  1 25 20 19  0  3  3  9  3 22  3
 26  0  8 29  0  3  3  2 12  6 27  3  7 33 28  0  0 21  3  3 10 37  5  3
 34  5 36  3 13 17  3  3  4 18  6 32  0  3  8  7  9  3  5  3  5  3 11 30
 15  3  3  3 16  4  3  4]
Losses:  20.72036001831293 4.886815547943115 5.453125 4.283125139772892
CurrentTrain: epoch  0, batch     0 | loss: 20.7203600Losses:  21.246184021234512 4.648257255554199 5.798459053039551 6.119990974664688
CurrentTrain: epoch  0, batch     1 | loss: 21.2461840Losses:  19.986300088465214 5.167716979980469 5.615750312805176 1.8788381591439247
CurrentTrain: epoch  0, batch     2 | loss: 19.9863001Losses:  30.33119635283947 5.0 5.628866195678711 13.94474844634533
CurrentTrain: epoch  0, batch     3 | loss: 30.3311964Losses:  21.737175188958645 4.945908546447754 5.642194747924805 5.819233141839504
CurrentTrain: epoch  1, batch     0 | loss: 21.7371752Losses:  17.252106219530106 4.883984565734863 5.224301815032959 1.4105668365955353
CurrentTrain: epoch  1, batch     1 | loss: 17.2521062Losses:  16.621302515268326 4.741151809692383 5.317960739135742 1.4143618643283844
CurrentTrain: epoch  1, batch     2 | loss: 16.6213025Losses:  17.417501971125603 5.107269287109375 5.051671981811523 1.6273923367261887
CurrentTrain: epoch  1, batch     3 | loss: 17.4175020Losses:  19.817102283239365 4.856461524963379 5.378940582275391 4.044017642736435
CurrentTrain: epoch  2, batch     0 | loss: 19.8171023Losses:  15.339984267950058 4.639681339263916 5.156742095947266 1.4136994779109955
CurrentTrain: epoch  2, batch     1 | loss: 15.3399843Losses:  21.64134954288602 5.130877494812012 5.422567367553711 4.993108499795198
CurrentTrain: epoch  2, batch     2 | loss: 21.6413495Losses:  13.681662261486053 4.264281272888184 5.335155487060547 1.4624068140983582
CurrentTrain: epoch  2, batch     3 | loss: 13.6816623Losses:  15.456019401550293 4.892984390258789 5.292464256286621 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.4560194Losses:  15.848686963319778 4.760191917419434 5.079505920410156 1.410068303346634
CurrentTrain: epoch  3, batch     1 | loss: 15.8486870Losses:  15.24763011932373 4.753779411315918 5.501917839050293 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 15.2476301Losses:  19.419635951519012 5.309781074523926 5.299901008605957 1.4847108721733093
CurrentTrain: epoch  3, batch     3 | loss: 19.4196360Losses:  17.14025140553713 4.95759916305542 5.270634651184082 1.4378502443432808
CurrentTrain: epoch  4, batch     0 | loss: 17.1402514Losses:  14.004150390625 4.628759860992432 5.119960784912109 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 14.0041504Losses:  19.066952135413885 4.738749980926514 5.086068630218506 4.9317955039441586
CurrentTrain: epoch  4, batch     2 | loss: 19.0669521Losses:  20.01640012860298 5.278725624084473 5.858238220214844 1.5213249027729034
CurrentTrain: epoch  4, batch     3 | loss: 20.0164001Losses:  15.947901453822851 4.847604751586914 5.285715103149414 1.4218070171773434
CurrentTrain: epoch  5, batch     0 | loss: 15.9479015Losses:  18.131895672529936 4.719743728637695 5.090003490447998 4.44324554130435
CurrentTrain: epoch  5, batch     1 | loss: 18.1318957Losses:  16.525015026330948 4.781118392944336 5.220999240875244 1.4224111139774323
CurrentTrain: epoch  5, batch     2 | loss: 16.5250150Losses:  16.612803146243095 5.089471817016602 5.088712692260742 1.4558226317167282
CurrentTrain: epoch  5, batch     3 | loss: 16.6128031Losses:  14.233909606933594 4.805545806884766 5.069391250610352 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 14.2339096Losses:  16.91828801855445 4.836024284362793 5.092771530151367 2.908090379089117
CurrentTrain: epoch  6, batch     1 | loss: 16.9182880Losses:  14.054627418518066 4.65223503112793 5.258798599243164 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 14.0546274Losses:  16.538560815155506 5.0 5.290271759033203 1.4277381375432014
CurrentTrain: epoch  6, batch     3 | loss: 16.5385608Losses:  13.226954460144043 4.674692153930664 5.089755058288574 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 13.2269545Losses:  20.78823770582676 4.968259811401367 5.145864963531494 6.281439915299416
CurrentTrain: epoch  7, batch     1 | loss: 20.7882377Losses:  18.266565296798944 4.6647491455078125 5.146096706390381 4.2475528456270695
CurrentTrain: epoch  7, batch     2 | loss: 18.2665653Losses:  12.56930536776781 3.9776382446289062 5.0 1.436389870941639
CurrentTrain: epoch  7, batch     3 | loss: 12.5693054Losses:  23.224442914128304 4.743456840515137 5.098176002502441 9.578109219670296
CurrentTrain: epoch  8, batch     0 | loss: 23.2244429Losses:  19.276232976466417 4.784318923950195 5.231880187988281 5.138283986598253
CurrentTrain: epoch  8, batch     1 | loss: 19.2762330Losses:  14.70044320821762 4.6423563957214355 5.158195495605469 1.5370778441429138
CurrentTrain: epoch  8, batch     2 | loss: 14.7004432Losses:  16.35791240632534 4.991580009460449 5.0 1.406805381178856
CurrentTrain: epoch  8, batch     3 | loss: 16.3579124Losses:  16.770720720291138 4.684807777404785 5.052547931671143 2.885607957839966
CurrentTrain: epoch  9, batch     0 | loss: 16.7707207Losses:  14.293473720550537 4.629512786865234 5.122651100158691 1.416923999786377
CurrentTrain: epoch  9, batch     1 | loss: 14.2934737Losses:  15.207162916660309 4.834814548492432 5.1702728271484375 1.4518919587135315
CurrentTrain: epoch  9, batch     2 | loss: 15.2071629Losses:  15.622407227754593 4.985573768615723 5.0 1.5255482494831085
CurrentTrain: epoch  9, batch     3 | loss: 15.6224072
Losses:  9.654380798339844 4.272570610046387 4.972740650177002 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 9.6543808Losses:  9.8137845993042 4.526076316833496 5.0 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 9.8137846Losses:  9.72671127319336 4.53792667388916 5.0 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 9.7267113Losses:  9.691526412963867 4.475574016571045 5.06088399887085 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 9.6915264Losses:  9.348998069763184 4.229099750518799 5.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 9.3489981Losses:  9.417997360229492 3.977773666381836 5.088876724243164 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 9.4179974Losses:  9.926013946533203 4.333296775817871 5.0 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 9.9260139Losses:  9.91976547241211 4.513788223266602 5.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 9.9197655Losses:  9.802705764770508 4.541497707366943 5.026106357574463 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 9.8027058Losses:  9.642183303833008 4.525905609130859 5.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 9.6421833Losses:  9.70439338684082 4.531933784484863 5.0 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.7043934Losses:  9.649187088012695 4.422286510467529 5.069027900695801 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 9.6491871Losses:  9.330385208129883 4.200238227844238 5.003752708435059 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 9.3303852Losses:  9.45916748046875 4.313187599182129 5.0 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 9.4591675Losses:  9.403831481933594 4.28987979888916 5.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 9.4038315Losses:  9.400823593139648 4.283388614654541 5.011168479919434 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.4008236Losses:  9.354681015014648 4.289920806884766 5.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 9.3546810Losses:  9.382993698120117 4.312533855438232 5.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 9.3829937Losses:  9.703943252563477 4.563070297241211 5.054045677185059 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 9.7039433Losses:  9.23771858215332 4.199231147766113 5.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 9.2377186Losses:  9.540447235107422 4.445334434509277 5.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.5404472Losses:  9.202022552490234 4.140201568603516 5.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 9.2020226Losses:  9.412757873535156 4.321703910827637 5.039477348327637 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 9.4127579Losses:  9.257068634033203 4.187532424926758 5.0 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 9.2570686Losses:  9.451447486877441 4.398393630981445 5.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 9.4514475Losses:  9.25914192199707 4.213339328765869 5.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.2591419Losses:  9.280206680297852 4.21489953994751 5.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 9.2802067Losses:  9.175024032592773 4.1099419593811035 5.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 9.1750240Losses:  9.474359512329102 4.405880928039551 5.0 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 9.4743595Losses:  9.568599700927734 4.495974063873291 5.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 9.5685997Losses:  8.842338562011719 3.78507137298584 5.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 8.8423386Losses:  9.357927322387695 4.318259239196777 5.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 9.3579273Losses:  9.448661804199219 4.386805534362793 5.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 9.4486618Losses:  9.401607513427734 4.337564945220947 5.010426044464111 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 9.4016075Losses:  9.524505615234375 4.478419303894043 5.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 9.5245056Losses:  9.298868179321289 4.258562088012695 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.2988682Losses:  9.172236442565918 4.131521224975586 5.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 9.1722364Losses:  9.336935043334961 4.289928913116455 5.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 9.3369350Losses:  9.190059661865234 4.124420166015625 5.0 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 9.1900597Losses:  9.36514949798584 4.323952674865723 5.0027384757995605 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 9.3651495Losses:  9.041945457458496 3.989748001098633 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.0419455Losses:  9.233445167541504 4.191669464111328 5.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 9.2334452Losses:  9.359687805175781 4.329165458679199 4.993138790130615 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 9.3596878Losses:  9.362325668334961 4.304850101470947 5.0 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 9.3623257Losses:  9.267248153686523 4.213706970214844 5.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 9.2672482Losses:  9.06456470489502 4.0123491287231445 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.0645647Losses:  9.32754135131836 4.289307594299316 5.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 9.3275414Losses:  9.155696868896484 4.111952781677246 5.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 9.1556969Losses:  9.519676208496094 4.452376842498779 5.0 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 9.5196762Losses:  9.086522102355957 4.038252830505371 5.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 9.0865221
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 60.66%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 62.19%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 73.40%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.06%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 70.94%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 69.83%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 68.64%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 67.81%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 64.98%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.07%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 79.82%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 79.42%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.13%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 79.00%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 79.13%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 79.07%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 78.86%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 78.99%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 79.79%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 79.98%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 80.02%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 79.55%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.41%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 79.35%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 79.45%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 79.40%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 79.42%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 78.79%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 78.53%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 78.42%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 77.70%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 77.46%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 77.15%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 76.92%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 76.49%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 76.01%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 75.47%   [EVAL] batch:   94 | acc: 12.50%,  total acc: 74.80%   [EVAL] batch:   95 | acc: 0.00%,  total acc: 74.02%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 73.32%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 72.77%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 72.10%   [EVAL] batch:   99 | acc: 0.00%,  total acc: 71.38%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 70.71%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 70.15%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 69.76%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 69.52%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 69.10%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 68.41%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 67.90%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 67.57%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 67.35%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 67.31%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 68.06%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 67.62%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 67.24%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 66.49%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 66.27%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 66.51%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 66.23%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 65.85%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 65.51%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 65.18%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 64.90%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 65.13%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 64.71%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 63.87%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 63.46%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 63.88%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 64.02%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 64.23%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 63.95%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 63.64%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 63.25%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 62.87%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 62.57%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 62.21%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 62.13%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 61.95%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 61.89%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 61.78%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 61.79%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 61.54%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 61.44%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 61.27%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 61.17%   [EVAL] batch:  179 | acc: 18.75%,  total acc: 60.94%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 60.91%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 60.95%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 61.13%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 61.35%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 61.52%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 61.86%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 62.03%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 62.53%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 62.63%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  193 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 62.95%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 63.01%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 63.00%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 63.06%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 63.06%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 63.18%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 63.08%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 63.02%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 62.99%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 62.83%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 62.59%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 62.29%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 62.05%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 61.85%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 61.62%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 61.59%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.47%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 62.64%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 62.73%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 62.81%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 62.86%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 62.81%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 63.74%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 64.63%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 64.57%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 64.61%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 64.70%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 64.67%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 64.91%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 65.21%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 64.77%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 64.64%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 64.41%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 64.21%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 64.13%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 64.19%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 64.10%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 64.15%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 64.12%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 63.99%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 63.62%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 64.15%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 64.01%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 63.92%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 63.87%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 63.82%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 63.72%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 63.69%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 63.56%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 63.51%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 63.37%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 63.18%   [EVAL] batch:  287 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:  288 | acc: 37.50%,  total acc: 62.98%   [EVAL] batch:  289 | acc: 37.50%,  total acc: 62.89%   [EVAL] batch:  290 | acc: 43.75%,  total acc: 62.82%   [EVAL] batch:  291 | acc: 68.75%,  total acc: 62.84%   [EVAL] batch:  292 | acc: 43.75%,  total acc: 62.78%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 62.67%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 62.56%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 62.44%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 62.42%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 62.33%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 62.29%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 62.27%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.40%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 63.46%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 63.27%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 63.09%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 62.91%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 62.72%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 62.60%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.00%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 63.04%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 62.90%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 62.71%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 62.54%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 62.35%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 62.20%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 62.05%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 62.10%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 62.43%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.54%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 62.74%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 62.81%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 62.89%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 63.05%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 63.60%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 63.64%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 63.48%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 63.33%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 63.15%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 62.99%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 62.85%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 62.90%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 62.99%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:  363 | acc: 12.50%,  total acc: 63.27%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 63.18%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 63.13%   [EVAL] batch:  366 | acc: 12.50%,  total acc: 62.99%   [EVAL] batch:  367 | acc: 18.75%,  total acc: 62.87%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 62.77%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 62.79%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 62.79%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 62.74%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 62.73%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 62.77%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 62.62%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 62.47%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 62.32%   [EVAL] batch:  378 | acc: 6.25%,  total acc: 62.17%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 62.02%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 61.88%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 61.86%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 61.91%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 61.95%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 62.00%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 62.01%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 62.03%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 62.10%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 62.23%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 62.26%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 62.14%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 62.01%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 61.87%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 61.78%   [EVAL] batch:  398 | acc: 6.25%,  total acc: 61.64%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 61.55%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 61.41%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 61.26%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 61.10%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 60.95%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 60.82%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 60.67%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 60.67%   [EVAL] batch:  407 | acc: 100.00%,  total acc: 60.77%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 60.82%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 60.91%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 60.96%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 61.01%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 61.09%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 61.16%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 61.33%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 61.41%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 61.59%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 61.61%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 61.63%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 61.62%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 61.59%   [EVAL] batch:  425 | acc: 25.00%,  total acc: 61.50%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 61.50%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 61.57%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 61.57%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 61.62%   [EVAL] batch:  431 | acc: 31.25%,  total acc: 61.55%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 61.49%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 61.45%   [EVAL] batch:  434 | acc: 50.00%,  total acc: 61.42%   [EVAL] batch:  435 | acc: 31.25%,  total acc: 61.35%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 61.37%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 61.33%   [EVAL] batch:  438 | acc: 43.75%,  total acc: 61.29%   [EVAL] batch:  439 | acc: 50.00%,  total acc: 61.26%   [EVAL] batch:  440 | acc: 50.00%,  total acc: 61.24%   [EVAL] batch:  441 | acc: 37.50%,  total acc: 61.18%   [EVAL] batch:  442 | acc: 50.00%,  total acc: 61.16%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 61.13%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 61.18%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 61.21%   [EVAL] batch:  446 | acc: 37.50%,  total acc: 61.16%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 61.19%   [EVAL] batch:  448 | acc: 56.25%,  total acc: 61.18%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 61.22%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 61.24%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 61.24%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 61.27%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 61.32%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 61.35%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 61.40%   [EVAL] batch:  456 | acc: 56.25%,  total acc: 61.39%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 61.34%   [EVAL] batch:  458 | acc: 31.25%,  total acc: 61.27%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 61.24%   [EVAL] batch:  460 | acc: 31.25%,  total acc: 61.17%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 61.15%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 61.26%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 61.43%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 61.51%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 61.66%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 62.29%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 62.46%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 62.45%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 62.37%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 62.31%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 62.28%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 62.23%   [EVAL] batch:  487 | acc: 62.50%,  total acc: 62.23%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 62.28%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 62.45%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 62.51%   [EVAL] batch:  494 | acc: 6.25%,  total acc: 62.40%   [EVAL] batch:  495 | acc: 0.00%,  total acc: 62.27%   [EVAL] batch:  496 | acc: 12.50%,  total acc: 62.17%   [EVAL] batch:  497 | acc: 6.25%,  total acc: 62.06%   [EVAL] batch:  498 | acc: 18.75%,  total acc: 61.97%   [EVAL] batch:  499 | acc: 6.25%,  total acc: 61.86%   
cur_acc:  ['0.9435', '0.7401', '0.6756', '0.8016', '0.6369', '0.6825', '0.5813', '0.6498']
his_acc:  ['0.9435', '0.8350', '0.7726', '0.7560', '0.7222', '0.6795', '0.6448', '0.6186']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  22.787128061056137 5.416942119598389 5.450791358947754 2.01093253493309
CurrentTrain: epoch  0, batch     0 | loss: 22.7871281Losses:  30.53381486609578 5.52187967300415 5.723836898803711 9.606950242072344
CurrentTrain: epoch  0, batch     1 | loss: 30.5338149Losses:  30.45255247503519 5.385351181030273 5.516786098480225 9.810083068907261
CurrentTrain: epoch  0, batch     2 | loss: 30.4525525Losses:  27.39434826001525 5.42724609375 5.418188095092773 6.483041878789663
CurrentTrain: epoch  0, batch     3 | loss: 27.3943483Losses:  21.512265145778656 5.295374870300293 5.153770923614502 1.5345181822776794
CurrentTrain: epoch  0, batch     4 | loss: 21.5122651Losses:  25.958977829664946 5.336272239685059 5.452311992645264 5.747582565993071
CurrentTrain: epoch  0, batch     5 | loss: 25.9589778Losses:  26.38471793010831 5.552191734313965 5.498490333557129 5.832975376397371
CurrentTrain: epoch  0, batch     6 | loss: 26.3847179Losses:  20.79436683654785 5.354863166809082 5.376839637756348 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 20.7943668Losses:  22.409232404083014 5.535107612609863 5.30332088470459 1.5973770879209042
CurrentTrain: epoch  0, batch     8 | loss: 22.4092324Losses:  24.35841989889741 5.3526811599731445 5.399234771728516 3.6987099684774876
CurrentTrain: epoch  0, batch     9 | loss: 24.3584199Losses:  25.529822681099176 5.438024520874023 5.3994035720825195 5.852401111274958
CurrentTrain: epoch  0, batch    10 | loss: 25.5298227Losses:  23.791262306272984 5.451155662536621 5.393951416015625 2.8801409378647804
CurrentTrain: epoch  0, batch    11 | loss: 23.7912623Losses:  28.89363181218505 5.663617134094238 5.410523414611816 8.798436041921377
CurrentTrain: epoch  0, batch    12 | loss: 28.8936318Losses:  20.540273666381836 5.502933979034424 5.4334564208984375 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 20.5402737Losses:  22.24433207511902 5.435533046722412 5.403367519378662 1.8921058177947998
CurrentTrain: epoch  0, batch    14 | loss: 22.2443321Losses:  25.12114017829299 5.48341703414917 5.561820030212402 3.737633403390646
CurrentTrain: epoch  0, batch    15 | loss: 25.1211402Losses:  23.296344477683306 5.362671852111816 5.31710958480835 3.8163678236305714
CurrentTrain: epoch  0, batch    16 | loss: 23.2963445Losses:  22.867986623197794 5.309394836425781 5.092680931091309 3.226629201322794
CurrentTrain: epoch  0, batch    17 | loss: 22.8679866Losses:  21.568773090839386 5.548687934875488 5.4826154708862305 1.5149553418159485
CurrentTrain: epoch  0, batch    18 | loss: 21.5687731Losses:  19.135135650634766 5.334563255310059 5.262575149536133 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.1351357Losses:  30.375288240611553 5.2880964279174805 5.202149868011475 10.967577211558819
CurrentTrain: epoch  0, batch    20 | loss: 30.3752882Losses:  26.59622160345316 5.277612686157227 5.232609748840332 7.175582565367222
CurrentTrain: epoch  0, batch    21 | loss: 26.5962216Losses:  19.32634735107422 5.312030792236328 5.33552360534668 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 19.3263474Losses:  21.74139991775155 5.42643404006958 5.251488208770752 1.439201507717371
CurrentTrain: epoch  0, batch    23 | loss: 21.7413999Losses:  20.671199798583984 5.427237033843994 5.072598457336426 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 20.6711998Losses:  19.078941345214844 5.438651084899902 5.330948352813721 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 19.0789413Losses:  31.01605175063014 5.593376159667969 5.318512439727783 11.044684868305922
CurrentTrain: epoch  0, batch    26 | loss: 31.0160518Losses:  18.341602325439453 5.268345832824707 5.232108116149902 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 18.3416023Losses:  21.528271168470383 5.419112682342529 5.205334663391113 1.6805958449840546
CurrentTrain: epoch  0, batch    28 | loss: 21.5282712Losses:  21.871490858495235 5.55634880065918 5.289116859436035 3.108707807958126
CurrentTrain: epoch  0, batch    29 | loss: 21.8714909Losses:  33.98732826113701 5.173809051513672 5.079333305358887 14.99530479311943
CurrentTrain: epoch  0, batch    30 | loss: 33.9873283Losses:  30.135915022343397 5.5899434089660645 5.435577392578125 9.993167143315077
CurrentTrain: epoch  0, batch    31 | loss: 30.1359150Losses:  26.2237650975585 5.382322788238525 5.178771495819092 6.8052661046385765
CurrentTrain: epoch  0, batch    32 | loss: 26.2237651Losses:  26.114937774837017 5.344747066497803 5.3400726318359375 5.667244903743267
CurrentTrain: epoch  0, batch    33 | loss: 26.1149378Losses:  19.380958557128906 5.429640293121338 5.231261253356934 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 19.3809586Losses:  22.484425019472837 5.598509311676025 5.340235710144043 2.242233704775572
CurrentTrain: epoch  0, batch    35 | loss: 22.4844250Losses:  18.794902801513672 5.3707146644592285 5.301457405090332 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 18.7949028Losses:  21.75990568846464 5.362298011779785 5.3513383865356445 3.049025408923626
CurrentTrain: epoch  0, batch    37 | loss: 21.7599057Losses:  23.735386081039906 5.3009796142578125 5.21006965637207 4.494491763412952
CurrentTrain: epoch  0, batch    38 | loss: 23.7353861Losses:  21.118075493723154 5.249061584472656 5.212158203125 2.883559349924326
CurrentTrain: epoch  0, batch    39 | loss: 21.1180755Losses:  24.00582892820239 5.117501258850098 4.966859817504883 5.709961008280516
CurrentTrain: epoch  0, batch    40 | loss: 24.0058289Losses:  23.734075110405684 5.123064994812012 4.985690116882324 6.25175241753459
CurrentTrain: epoch  0, batch    41 | loss: 23.7340751Losses:  20.60810113698244 5.308499813079834 5.143275260925293 2.4198019579052925
CurrentTrain: epoch  0, batch    42 | loss: 20.6081011Losses:  20.057426512241364 5.178784370422363 5.197916030883789 1.491352140903473
CurrentTrain: epoch  0, batch    43 | loss: 20.0574265Losses:  24.185451444238424 5.3683576583862305 5.184747219085693 4.513685163110495
CurrentTrain: epoch  0, batch    44 | loss: 24.1854514Losses:  25.940713427960873 5.300000190734863 5.086828231811523 6.535802386701107
CurrentTrain: epoch  0, batch    45 | loss: 25.9407134Losses:  21.438070114701986 5.1434502601623535 5.094788074493408 3.108350571244955
CurrentTrain: epoch  0, batch    46 | loss: 21.4380701Losses:  22.75966139882803 5.325202941894531 5.127750873565674 4.568371497094631
CurrentTrain: epoch  0, batch    47 | loss: 22.7596614Losses:  23.775864575058222 5.229877471923828 4.964982032775879 4.444416973739862
CurrentTrain: epoch  0, batch    48 | loss: 23.7758646Losses:  20.370182532817125 5.1716837882995605 5.051628112792969 2.9830651469528675
CurrentTrain: epoch  0, batch    49 | loss: 20.3701825Losses:  19.867290496826172 5.50327205657959 5.185521125793457 -0.0
CurrentTrain: epoch  0, batch    50 | loss: 19.8672905Losses:  19.56407756358385 5.176896572113037 4.999869346618652 1.535001941025257
CurrentTrain: epoch  0, batch    51 | loss: 19.5640776Losses:  20.539116770029068 5.185877799987793 5.049760818481445 3.029911905527115
CurrentTrain: epoch  0, batch    52 | loss: 20.5391168Losses:  20.68585602939129 5.05660343170166 4.884591102600098 3.329551860690117
CurrentTrain: epoch  0, batch    53 | loss: 20.6858560Losses:  23.90207025781274 5.396254539489746 5.23718786239624 5.20765421167016
CurrentTrain: epoch  0, batch    54 | loss: 23.9020703Losses:  23.120205122977495 5.244991302490234 5.084940433502197 3.008514601737261
CurrentTrain: epoch  0, batch    55 | loss: 23.1202051Losses:  20.078969955444336 5.436604022979736 5.205818176269531 -0.0
CurrentTrain: epoch  0, batch    56 | loss: 20.0789700Losses:  18.933883424848318 5.139039993286133 5.053905487060547 1.4224946461617947
CurrentTrain: epoch  0, batch    57 | loss: 18.9338834Losses:  25.477187775075436 5.494592666625977 5.237785339355469 5.753337524831295
CurrentTrain: epoch  0, batch    58 | loss: 25.4771878Losses:  18.301179885864258 5.088369369506836 5.0844950675964355 -0.0
CurrentTrain: epoch  0, batch    59 | loss: 18.3011799Losses:  27.951807476580143 5.173530101776123 5.061238765716553 9.317295528948307
CurrentTrain: epoch  0, batch    60 | loss: 27.9518075Losses:  28.738872081041336 5.292010307312012 4.99534797668457 10.54990342259407
CurrentTrain: epoch  0, batch    61 | loss: 28.7388721Losses:  19.966022048145533 5.227626323699951 5.020663261413574 1.528823409229517
CurrentTrain: epoch  0, batch    62 | loss: 19.9660220Losses:  17.996768951416016 5.249578475952148 5.202602386474609 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 17.9967690Losses:  18.867820739746094 5.28291130065918 5.117917060852051 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 18.8678207Losses:  19.72410928737372 5.201708793640137 5.077857971191406 1.9848838998004794
CurrentTrain: epoch  1, batch     2 | loss: 19.7241093Losses:  24.384683713316917 5.375502586364746 5.162323474884033 6.272152051329613
CurrentTrain: epoch  1, batch     3 | loss: 24.3846837Losses:  18.854859821498394 5.145174980163574 5.034211158752441 1.7587485238909721
CurrentTrain: epoch  1, batch     4 | loss: 18.8548598Losses:  18.469636917114258 5.2743940353393555 5.122060298919678 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 18.4696369Losses:  25.839258588850498 5.2748212814331055 5.2168354988098145 7.165518201887608
CurrentTrain: epoch  1, batch     6 | loss: 25.8392586Losses:  20.74550050497055 4.99867057800293 4.985482215881348 4.284704148769379
CurrentTrain: epoch  1, batch     7 | loss: 20.7455005Losses:  19.541156292892992 5.197270393371582 5.0005316734313965 1.9425044069066644
CurrentTrain: epoch  1, batch     8 | loss: 19.5411563Losses:  22.995323061943054 5.326605319976807 4.991837501525879 4.447788119316101
CurrentTrain: epoch  1, batch     9 | loss: 22.9953231Losses:  17.173728942871094 5.114602088928223 5.034222602844238 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 17.1737289Losses:  27.908812329173088 5.33017635345459 5.117199897766113 9.595503613352776
CurrentTrain: epoch  1, batch    11 | loss: 27.9088123Losses:  20.77973159402609 5.172421455383301 5.008972644805908 3.154229961335659
CurrentTrain: epoch  1, batch    12 | loss: 20.7797316Losses:  22.059572897851467 5.054852485656738 5.009025573730469 4.557408057153225
CurrentTrain: epoch  1, batch    13 | loss: 22.0595729Losses:  17.626243591308594 5.279850006103516 5.07343864440918 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 17.6262436Losses:  23.173921644687653 5.019534587860107 5.000591278076172 6.102422773838043
CurrentTrain: epoch  1, batch    15 | loss: 23.1739216Losses:  20.089680537581444 4.991964340209961 5.047494888305664 2.878403529524803
CurrentTrain: epoch  1, batch    16 | loss: 20.0896805Losses:  19.934870589524508 5.131916046142578 4.987615585327148 2.970854628831148
CurrentTrain: epoch  1, batch    17 | loss: 19.9348706Losses:  19.527277052402496 5.266636371612549 5.160826683044434 1.4875927567481995
CurrentTrain: epoch  1, batch    18 | loss: 19.5272771Losses:  23.632440343499184 5.344980716705322 5.04292631149292 4.821073308587074
CurrentTrain: epoch  1, batch    19 | loss: 23.6324403Losses:  17.079317092895508 5.16679573059082 5.004551410675049 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 17.0793171Losses:  23.389531061053276 5.1972737312316895 5.045965194702148 6.237389490008354
CurrentTrain: epoch  1, batch    21 | loss: 23.3895311Losses:  20.65285038203001 5.1797027587890625 5.083833694458008 3.0574371740221977
CurrentTrain: epoch  1, batch    22 | loss: 20.6528504Losses:  20.17394307255745 4.992432594299316 5.062191486358643 2.8713803589344025
CurrentTrain: epoch  1, batch    23 | loss: 20.1739431Losses:  36.77760078012943 5.125244617462158 5.028153419494629 19.147500529885292
CurrentTrain: epoch  1, batch    24 | loss: 36.7776008Losses:  17.266429901123047 5.132022857666016 5.019351005554199 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.2664299Losses:  16.77550506591797 5.132174968719482 5.01205587387085 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 16.7755051Losses:  17.40146255493164 5.179666519165039 5.033929824829102 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 17.4014626Losses:  22.65947075933218 5.091246128082275 5.0243024826049805 4.63694878667593
CurrentTrain: epoch  1, batch    28 | loss: 22.6594708Losses:  23.73359825089574 5.071374893188477 5.027584075927734 5.532535094767809
CurrentTrain: epoch  1, batch    29 | loss: 23.7335983Losses:  18.30974394828081 4.995668411254883 5.078631401062012 1.5043507292866707
CurrentTrain: epoch  1, batch    30 | loss: 18.3097439Losses:  29.53825681656599 5.300541877746582 4.941431045532227 11.425440959632397
CurrentTrain: epoch  1, batch    31 | loss: 29.5382568Losses:  25.896324135363102 5.224085807800293 4.934468746185303 7.801614739000797
CurrentTrain: epoch  1, batch    32 | loss: 25.8963241Losses:  20.470042876899242 5.210820198059082 4.974980354309082 2.533229522407055
CurrentTrain: epoch  1, batch    33 | loss: 20.4700429Losses:  18.795401722192764 5.057015419006348 5.050660133361816 1.4035102427005768
CurrentTrain: epoch  1, batch    34 | loss: 18.7954017Losses:  22.517425891011953 5.164250373840332 4.96041202545166 4.861538287252188
CurrentTrain: epoch  1, batch    35 | loss: 22.5174259Losses:  16.283079147338867 4.867997169494629 4.983005523681641 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 16.2830791Losses:  24.340037759393454 5.1535420417785645 4.944363117218018 6.429217752069235
CurrentTrain: epoch  1, batch    37 | loss: 24.3400378Losses:  20.330962494015694 5.067488670349121 5.096634864807129 2.2866738587617874
CurrentTrain: epoch  1, batch    38 | loss: 20.3309625Losses:  21.051889955997467 4.986600875854492 5.04263973236084 3.213900148868561
CurrentTrain: epoch  1, batch    39 | loss: 21.0518900Losses:  23.99127271771431 5.108985424041748 4.957634449005127 6.645445615053177
CurrentTrain: epoch  1, batch    40 | loss: 23.9912727Losses:  17.525348663330078 5.21715784072876 5.0408782958984375 -0.0
CurrentTrain: epoch  1, batch    41 | loss: 17.5253487Losses:  20.364732831716537 5.181203365325928 5.12621545791626 3.1600705087184906
CurrentTrain: epoch  1, batch    42 | loss: 20.3647328Losses:  19.323605716228485 5.135059356689453 5.1012163162231445 1.4596359133720398
CurrentTrain: epoch  1, batch    43 | loss: 19.3236057Losses:  19.260791391134262 5.243218898773193 5.083798408508301 1.4009509980678558
CurrentTrain: epoch  1, batch    44 | loss: 19.2607914Losses:  17.14451026916504 5.1770429611206055 5.088640213012695 -0.0
CurrentTrain: epoch  1, batch    45 | loss: 17.1445103Losses:  17.12274932861328 5.158015251159668 5.065711975097656 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 17.1227493Losses:  18.78481736406684 5.085353851318359 5.037872791290283 1.4764257930219173
CurrentTrain: epoch  1, batch    47 | loss: 18.7848174Losses:  17.405452728271484 5.043394088745117 5.012581825256348 -0.0
CurrentTrain: epoch  1, batch    48 | loss: 17.4054527Losses:  33.38539569824934 5.235703468322754 5.141227722167969 15.199183158576488
CurrentTrain: epoch  1, batch    49 | loss: 33.3853957Losses:  19.465568721294403 5.145970344543457 4.9492645263671875 2.8391286730766296
CurrentTrain: epoch  1, batch    50 | loss: 19.4655687Losses:  19.017227279022336 5.033530235290527 5.020058631896973 2.0708695519715548
CurrentTrain: epoch  1, batch    51 | loss: 19.0172273Losses:  17.729388266801834 5.025998115539551 5.031159400939941 1.4658422768115997
CurrentTrain: epoch  1, batch    52 | loss: 17.7293883Losses:  15.946744918823242 5.040472984313965 5.055751323699951 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 15.9467449Losses:  19.96582179144025 5.101785659790039 4.99196720123291 2.9449172504246235
CurrentTrain: epoch  1, batch    54 | loss: 19.9658218Losses:  16.562397003173828 5.068004608154297 4.969005584716797 -0.0
CurrentTrain: epoch  1, batch    55 | loss: 16.5623970Losses:  18.145737562328577 5.105390548706055 5.004774570465088 1.4858998395502567
CurrentTrain: epoch  1, batch    56 | loss: 18.1457376Losses:  16.872053146362305 5.15354061126709 4.994022369384766 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 16.8720531Losses:  17.575077056884766 4.972005844116211 5.002225875854492 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 17.5750771Losses:  19.67451623082161 5.188069820404053 5.014233589172363 2.976319819688797
CurrentTrain: epoch  1, batch    59 | loss: 19.6745162Losses:  18.64258437976241 5.070801258087158 5.043903350830078 1.5071607194840908
CurrentTrain: epoch  1, batch    60 | loss: 18.6425844Losses:  18.28493297472596 4.953885078430176 5.0 1.620927695184946
CurrentTrain: epoch  1, batch    61 | loss: 18.2849330Losses:  18.934947967529297 5.193116188049316 5.138116836547852 -0.0
CurrentTrain: epoch  1, batch    62 | loss: 18.9349480Losses:  20.181355752050877 5.0312418937683105 5.019012928009033 4.356551446020603
CurrentTrain: epoch  2, batch     0 | loss: 20.1813558Losses:  21.698097307235003 5.054276466369629 5.043970108032227 4.585998613387346
CurrentTrain: epoch  2, batch     1 | loss: 21.6980973Losses:  16.10260772705078 4.994193077087402 5.037851333618164 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 16.1026077Losses:  22.40878999233246 5.0658063888549805 5.0 6.055564284324646
CurrentTrain: epoch  2, batch     3 | loss: 22.4087900Losses:  19.640134550631046 4.820693016052246 5.0 4.245582319796085
CurrentTrain: epoch  2, batch     4 | loss: 19.6401346Losses:  16.766880568116903 5.013620853424072 5.017291069030762 1.4468780122697353
CurrentTrain: epoch  2, batch     5 | loss: 16.7668806Losses:  18.873547796159983 5.016846656799316 5.041049957275391 1.4396126307547092
CurrentTrain: epoch  2, batch     6 | loss: 18.8735478Losses:  17.735875606536865 4.799398899078369 4.984940052032471 1.4435534477233887
CurrentTrain: epoch  2, batch     7 | loss: 17.7358756Losses:  20.54365634173155 5.085398197174072 5.018041610717773 4.316739074885845
CurrentTrain: epoch  2, batch     8 | loss: 20.5436563Losses:  19.086482260376215 4.958823204040527 5.037672996520996 2.9181740023195744
CurrentTrain: epoch  2, batch     9 | loss: 19.0864823Losses:  16.092411041259766 4.917852878570557 5.029728889465332 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 16.0924110Losses:  23.038450933992863 5.194513320922852 5.009519577026367 4.926737524569035
CurrentTrain: epoch  2, batch    11 | loss: 23.0384509Losses:  20.54898788779974 4.966977119445801 5.136576175689697 4.466140292584896
CurrentTrain: epoch  2, batch    12 | loss: 20.5489879Losses:  20.22381104901433 4.740452766418457 4.9618754386901855 4.858944792300463
CurrentTrain: epoch  2, batch    13 | loss: 20.2238110Losses:  17.275547448545694 4.907042503356934 5.017609596252441 1.4931034483015537
CurrentTrain: epoch  2, batch    14 | loss: 17.2755474Losses:  16.713476181030273 5.008891582489014 5.0 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 16.7134762Losses:  24.430701851844788 4.982802391052246 5.105992317199707 7.232703804969788
CurrentTrain: epoch  2, batch    16 | loss: 24.4307019Losses:  25.30851563438773 4.916842460632324 5.032395839691162 9.382257547229528
CurrentTrain: epoch  2, batch    17 | loss: 25.3085156Losses:  16.479557037353516 4.920374870300293 5.0 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 16.4795570Losses:  19.975020579993725 4.966379165649414 4.9617133140563965 3.042098216712475
CurrentTrain: epoch  2, batch    19 | loss: 19.9750206Losses:  16.776630401611328 4.898902416229248 4.962510108947754 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 16.7766304Losses:  19.268796116113663 5.057872772216797 5.086780071258545 2.806096225976944
CurrentTrain: epoch  2, batch    21 | loss: 19.2687961Losses:  18.97735884785652 5.031911849975586 4.98130464553833 2.612750083208084
CurrentTrain: epoch  2, batch    22 | loss: 18.9773588Losses:  17.49819913506508 4.9610981941223145 5.047428131103516 1.4006954729557037
CurrentTrain: epoch  2, batch    23 | loss: 17.4981991Losses:  18.131939485669136 4.941738605499268 5.043438911437988 1.8613172322511673
CurrentTrain: epoch  2, batch    24 | loss: 18.1319395Losses:  19.105696350336075 5.081557273864746 4.980121612548828 2.8646322786808014
CurrentTrain: epoch  2, batch    25 | loss: 19.1056964Losses:  18.542366661131382 5.078220367431641 4.985202789306641 1.4578177109360695
CurrentTrain: epoch  2, batch    26 | loss: 18.5423667Losses:  20.856435865163803 5.137694358825684 5.10825252532959 4.264261335134506
CurrentTrain: epoch  2, batch    27 | loss: 20.8564359Losses:  18.744780838489532 4.839409828186035 5.047014236450195 2.9313071370124817
CurrentTrain: epoch  2, batch    28 | loss: 18.7447808Losses:  17.001719117164612 4.801699638366699 5.074538230895996 1.4058624505996704
CurrentTrain: epoch  2, batch    29 | loss: 17.0017191Losses:  20.222498133778572 4.984145164489746 5.016086578369141 3.260938838124275
CurrentTrain: epoch  2, batch    30 | loss: 20.2224981Losses:  22.801796678453684 4.882187843322754 5.07296895980835 6.35865855589509
CurrentTrain: epoch  2, batch    31 | loss: 22.8017967Losses:  19.554018788039684 4.944437026977539 5.064177989959717 3.0825107619166374
CurrentTrain: epoch  2, batch    32 | loss: 19.5540188Losses:  20.203672535717487 5.068488121032715 5.169750213623047 3.0016824081540108
CurrentTrain: epoch  2, batch    33 | loss: 20.2036725Losses:  17.512524601072073 5.032655715942383 4.982027530670166 1.416964527219534
CurrentTrain: epoch  2, batch    34 | loss: 17.5125246Losses:  18.027835309505463 4.795722961425781 5.069077968597412 2.8214372992515564
CurrentTrain: epoch  2, batch    35 | loss: 18.0278353Losses:  17.579452514648438 5.035305976867676 4.93692684173584 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 17.5794525Losses:  21.37386444211006 4.86566686630249 4.995625972747803 5.216384202241898
CurrentTrain: epoch  2, batch    37 | loss: 21.3738644Losses:  17.738299280405045 4.87349796295166 5.109544277191162 1.6929844915866852
CurrentTrain: epoch  2, batch    38 | loss: 17.7382993Losses:  21.726967707276344 4.805370807647705 5.042959690093994 5.228829279541969
CurrentTrain: epoch  2, batch    39 | loss: 21.7269677Losses:  19.809861958026886 4.888427257537842 5.029234886169434 2.9813860058784485
CurrentTrain: epoch  2, batch    40 | loss: 19.8098620Losses:  19.632979817688465 4.861583709716797 5.0071821212768555 2.9354853108525276
CurrentTrain: epoch  2, batch    41 | loss: 19.6329798Losses:  20.55372052639723 4.947564601898193 4.990450859069824 3.03618436306715
CurrentTrain: epoch  2, batch    42 | loss: 20.5537205Losses:  24.569342877715826 4.710349082946777 5.031587600708008 8.72785785421729
CurrentTrain: epoch  2, batch    43 | loss: 24.5693429Losses:  17.81574807316065 4.779959678649902 5.0161919593811035 1.418657161295414
CurrentTrain: epoch  2, batch    44 | loss: 17.8157481Losses:  17.490466237068176 5.000786304473877 5.071051597595215 1.432700276374817
CurrentTrain: epoch  2, batch    45 | loss: 17.4904662Losses:  17.177240673452616 4.746363639831543 5.011141777038574 1.4483397640287876
CurrentTrain: epoch  2, batch    46 | loss: 17.1772407Losses:  17.24404865503311 5.004311561584473 5.0182366371154785 1.4074321389198303
CurrentTrain: epoch  2, batch    47 | loss: 17.2440487Losses:  20.417730640619993 4.840888977050781 4.9872894287109375 4.769502948969603
CurrentTrain: epoch  2, batch    48 | loss: 20.4177306Losses:  18.236180178821087 4.93037223815918 5.055822372436523 1.560427539050579
CurrentTrain: epoch  2, batch    49 | loss: 18.2361802Losses:  15.955950736999512 4.908605575561523 4.993237495422363 -0.0
CurrentTrain: epoch  2, batch    50 | loss: 15.9559507Losses:  17.094160590320826 4.845451354980469 5.042158603668213 1.4805856086313725
CurrentTrain: epoch  2, batch    51 | loss: 17.0941606Losses:  19.13412857055664 4.869149208068848 4.983632564544678 2.9240169525146484
CurrentTrain: epoch  2, batch    52 | loss: 19.1341286Losses:  15.951642990112305 4.932051181793213 5.0 -0.0
CurrentTrain: epoch  2, batch    53 | loss: 15.9516430Losses:  18.52105975151062 4.777550220489502 5.0 3.0590789318084717
CurrentTrain: epoch  2, batch    54 | loss: 18.5210598Losses:  17.730083528906107 4.985482215881348 5.0204339027404785 1.56381231918931
CurrentTrain: epoch  2, batch    55 | loss: 17.7300835Losses:  16.89902726188302 4.841244697570801 5.010125160217285 1.5125269033014774
CurrentTrain: epoch  2, batch    56 | loss: 16.8990273Losses:  19.333619382232428 4.811397075653076 5.0 3.49208190664649
CurrentTrain: epoch  2, batch    57 | loss: 19.3336194Losses:  17.37132759951055 4.867924213409424 4.998563766479492 1.649048050865531
CurrentTrain: epoch  2, batch    58 | loss: 17.3713276Losses:  16.064502716064453 4.827399253845215 5.016287803649902 -0.0
CurrentTrain: epoch  2, batch    59 | loss: 16.0645027Losses:  18.055061534047127 5.076597213745117 5.086199760437012 1.5135519057512283
CurrentTrain: epoch  2, batch    60 | loss: 18.0550615Losses:  16.120426177978516 4.917505741119385 5.0 -0.0
CurrentTrain: epoch  2, batch    61 | loss: 16.1204262Losses:  19.654653012752533 4.9985246658325195 4.978611469268799 2.9098238348960876
CurrentTrain: epoch  2, batch    62 | loss: 19.6546530Losses:  22.723791893571615 4.885421276092529 4.980074882507324 6.246264275163412
CurrentTrain: epoch  3, batch     0 | loss: 22.7237919Losses:  18.48051580786705 5.055813312530518 5.076366901397705 1.9215310513973236
CurrentTrain: epoch  3, batch     1 | loss: 18.4805158Losses:  16.83574765920639 4.703685760498047 4.96574068069458 1.4383172392845154
CurrentTrain: epoch  3, batch     2 | loss: 16.8357477Losses:  18.922106713056564 4.6397199630737305 5.068197727203369 3.546601265668869
CurrentTrain: epoch  3, batch     3 | loss: 18.9221067Losses:  18.13675606995821 4.99225378036499 5.008157730102539 1.5050002411007881
CurrentTrain: epoch  3, batch     4 | loss: 18.1367561Losses:  15.914137840270996 4.820457935333252 4.972161293029785 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 15.9141378Losses:  18.465602070093155 4.705991744995117 5.0 2.8158561289310455
CurrentTrain: epoch  3, batch     6 | loss: 18.4656021Losses:  16.666258543729782 4.586216926574707 5.030421257019043 1.4065692126750946
CurrentTrain: epoch  3, batch     7 | loss: 16.6662585Losses:  19.380504675209522 5.020424842834473 5.045336723327637 3.0709496214985847
CurrentTrain: epoch  3, batch     8 | loss: 19.3805047Losses:  15.839341163635254 4.984298229217529 4.991789817810059 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 15.8393412Losses:  22.09318371862173 5.0450944900512695 5.037395477294922 6.011648379266262
CurrentTrain: epoch  3, batch    10 | loss: 22.0931837Losses:  23.768305830657482 4.814789772033691 5.021934986114502 8.477313093841076
CurrentTrain: epoch  3, batch    11 | loss: 23.7683058Losses:  14.93145751953125 4.6071672439575195 5.011031627655029 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 14.9314575Losses:  23.2649042904377 4.7353515625 5.006427764892578 7.311364442110062
CurrentTrain: epoch  3, batch    13 | loss: 23.2649043Losses:  21.520702607929707 4.75570011138916 4.952092170715332 5.246134050190449
CurrentTrain: epoch  3, batch    14 | loss: 21.5207026Losses:  17.021191000938416 4.709837913513184 5.033308029174805 1.4417585134506226
CurrentTrain: epoch  3, batch    15 | loss: 17.0211910Losses:  17.459952265024185 4.838624477386475 5.010984897613525 1.4167946875095367
CurrentTrain: epoch  3, batch    16 | loss: 17.4599523Losses:  16.786624282598495 4.876474380493164 5.241362571716309 1.4811957776546478
CurrentTrain: epoch  3, batch    17 | loss: 16.7866243Losses:  15.803424835205078 4.816877841949463 5.058563232421875 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 15.8034248Losses:  18.8441677428782 4.765598297119141 5.039182186126709 2.8379555083811283
CurrentTrain: epoch  3, batch    19 | loss: 18.8441677Losses:  16.439924724400043 4.65606689453125 5.0 1.5343461111187935
CurrentTrain: epoch  3, batch    20 | loss: 16.4399247Losses:  15.941801071166992 4.8720502853393555 5.0 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 15.9418011Losses:  16.32982075214386 4.688760280609131 5.0 1.3930951356887817
CurrentTrain: epoch  3, batch    22 | loss: 16.3298208Losses:  17.528764136135578 4.715539455413818 5.0 1.5610956028103828
CurrentTrain: epoch  3, batch    23 | loss: 17.5287641Losses:  21.19018303975463 4.945405006408691 5.053562641143799 4.70013939961791
CurrentTrain: epoch  3, batch    24 | loss: 21.1901830Losses:  17.909090247005224 4.751382350921631 5.028650283813477 1.7445299290120602
CurrentTrain: epoch  3, batch    25 | loss: 17.9090902Losses:  19.612688198685646 4.751315116882324 5.0 4.619694843888283
CurrentTrain: epoch  3, batch    26 | loss: 19.6126882Losses:  23.492633439600468 4.81533145904541 5.0 8.523424722254276
CurrentTrain: epoch  3, batch    27 | loss: 23.4926334Losses:  18.673761896789074 4.577052593231201 5.009199142456055 2.8743558451533318
CurrentTrain: epoch  3, batch    28 | loss: 18.6737619Losses:  18.044009875506163 4.622353553771973 5.0 3.2015320770442486
CurrentTrain: epoch  3, batch    29 | loss: 18.0440099Losses:  17.46966204047203 4.665417671203613 5.0 1.4376471936702728
CurrentTrain: epoch  3, batch    30 | loss: 17.4696620Losses:  19.75828617438674 4.780467987060547 5.0 4.360710795968771
CurrentTrain: epoch  3, batch    31 | loss: 19.7582862Losses:  20.131989113986492 4.650031566619873 5.0 4.7061124965548515
CurrentTrain: epoch  3, batch    32 | loss: 20.1319891Losses:  18.103694438934326 4.611900806427002 5.0 2.8184237480163574
CurrentTrain: epoch  3, batch    33 | loss: 18.1036944Losses:  15.437070846557617 4.5960798263549805 5.035792827606201 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 15.4370708Losses:  21.74711162596941 4.6229400634765625 5.051724433898926 6.380827255547047
CurrentTrain: epoch  3, batch    35 | loss: 21.7471116Losses:  20.08523565530777 4.595987319946289 5.0646891593933105 4.265136778354645
CurrentTrain: epoch  3, batch    36 | loss: 20.0852357Losses:  17.3202101290226 4.739272117614746 5.0 1.4115406572818756
CurrentTrain: epoch  3, batch    37 | loss: 17.3202101Losses:  16.26181310415268 4.457431316375732 4.999422073364258 1.4164475798606873
CurrentTrain: epoch  3, batch    38 | loss: 16.2618131Losses:  23.027435064315796 4.518093585968018 5.0 7.680986166000366
CurrentTrain: epoch  3, batch    39 | loss: 23.0274351Losses:  17.395062271505594 4.733777046203613 4.966466426849365 1.4798515476286411
CurrentTrain: epoch  3, batch    40 | loss: 17.3950623Losses:  19.04804815351963 4.688266277313232 5.0 3.3418780714273453
CurrentTrain: epoch  3, batch    41 | loss: 19.0480482Losses:  18.25536221265793 4.6263933181762695 5.0 1.9200236201286316
CurrentTrain: epoch  3, batch    42 | loss: 18.2553622Losses:  17.845002461224794 4.543062210083008 5.0 2.9327576644718647
CurrentTrain: epoch  3, batch    43 | loss: 17.8450025Losses:  16.134170532226562 4.777609348297119 5.122307777404785 -0.0
CurrentTrain: epoch  3, batch    44 | loss: 16.1341705Losses:  16.091346740722656 4.791561603546143 5.060767650604248 -0.0
CurrentTrain: epoch  3, batch    45 | loss: 16.0913467Losses:  17.99101573228836 4.620041847229004 5.0 2.806207001209259
CurrentTrain: epoch  3, batch    46 | loss: 17.9910157Losses:  17.234897069633007 4.6252288818359375 5.0 1.564956121146679
CurrentTrain: epoch  3, batch    47 | loss: 17.2348971Losses:  14.469552993774414 4.347837448120117 5.0 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 14.4695530Losses:  22.400548301637173 4.3552727699279785 5.0 7.999039970338345
CurrentTrain: epoch  3, batch    49 | loss: 22.4005483Losses:  19.1245531514287 4.645304203033447 5.0 4.308761067688465
CurrentTrain: epoch  3, batch    50 | loss: 19.1245532Losses:  17.981586158275604 4.85662841796875 5.193849086761475 1.4097458720207214
CurrentTrain: epoch  3, batch    51 | loss: 17.9815862Losses:  18.11272057518363 4.555697441101074 5.027549743652344 2.902999009937048
CurrentTrain: epoch  3, batch    52 | loss: 18.1127206Losses:  15.723660469055176 4.764577865600586 5.042566299438477 -0.0
CurrentTrain: epoch  3, batch    53 | loss: 15.7236605Losses:  16.49949472397566 4.553544044494629 5.0 1.5394746586680412
CurrentTrain: epoch  3, batch    54 | loss: 16.4994947Losses:  19.502268813550472 4.598756313323975 4.988076210021973 4.305538199841976
CurrentTrain: epoch  3, batch    55 | loss: 19.5022688Losses:  17.402026625350118 4.49005651473999 5.1046462059021 1.9658293444663286
CurrentTrain: epoch  3, batch    56 | loss: 17.4020266Losses:  16.22203826904297 4.766801834106445 5.068388938903809 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 16.2220383Losses:  18.785755395889282 4.460066795349121 5.0 4.276914834976196
CurrentTrain: epoch  3, batch    58 | loss: 18.7857554Losses:  16.09253967553377 4.512735366821289 5.0 1.5596398189663887
CurrentTrain: epoch  3, batch    59 | loss: 16.0925397Losses:  17.018771663308144 4.622370719909668 5.042695045471191 1.7182536274194717
CurrentTrain: epoch  3, batch    60 | loss: 17.0187717Losses:  21.483051039278507 4.79263162612915 5.06362771987915 5.504714705049992
CurrentTrain: epoch  3, batch    61 | loss: 21.4830510Losses:  16.647963523864746 4.812085151672363 5.0 1.4390249252319336
CurrentTrain: epoch  3, batch    62 | loss: 16.6479635Losses:  16.641469925642014 4.672907829284668 5.077671527862549 1.4307498633861542
CurrentTrain: epoch  4, batch     0 | loss: 16.6414699Losses:  14.853509902954102 4.543780326843262 5.0 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 14.8535099Losses:  14.686405181884766 4.4898505210876465 5.0 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 14.6864052Losses:  15.87315908074379 4.512211322784424 5.091815948486328 1.4003035128116608
CurrentTrain: epoch  4, batch     3 | loss: 15.8731591Losses:  14.981292724609375 4.5785627365112305 5.120680332183838 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 14.9812927Losses:  19.8891641497612 4.406929969787598 4.9988861083984375 5.02247828245163
CurrentTrain: epoch  4, batch     5 | loss: 19.8891641Losses:  17.233222749084234 4.568718910217285 5.107003211975098 1.4162795804440975
CurrentTrain: epoch  4, batch     6 | loss: 17.2332227Losses:  15.658292435109615 4.225277900695801 5.0 1.4226032719016075
CurrentTrain: epoch  4, batch     7 | loss: 15.6582924Losses:  15.691216468811035 4.846441268920898 5.038548469543457 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 15.6912165Losses:  18.81810010969639 4.419928550720215 5.0 4.3774291425943375
CurrentTrain: epoch  4, batch     9 | loss: 18.8181001Losses:  16.88799761980772 4.496196746826172 5.0 1.721272461116314
CurrentTrain: epoch  4, batch    10 | loss: 16.8879976Losses:  18.45779923349619 4.120177745819092 5.0 4.383844651281834
CurrentTrain: epoch  4, batch    11 | loss: 18.4577992Losses:  16.264237344264984 4.661473274230957 5.035702705383301 1.4098004698753357
CurrentTrain: epoch  4, batch    12 | loss: 16.2642373Losses:  15.925207138061523 4.6409406661987305 5.181703567504883 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 15.9252071Losses:  16.602298345416784 4.497977256774902 5.046470642089844 1.4492107294499874
CurrentTrain: epoch  4, batch    14 | loss: 16.6022983Losses:  21.26579712703824 4.516766548156738 5.11287784576416 5.787727821618319
CurrentTrain: epoch  4, batch    15 | loss: 21.2657971Losses:  14.462711334228516 4.329212188720703 4.966344356536865 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 14.4627113Losses:  15.946337163448334 4.310451984405518 5.041195869445801 1.4579805731773376
CurrentTrain: epoch  4, batch    17 | loss: 15.9463372Losses:  20.585007990244776 4.5278849601745605 5.0 5.69926198804751
CurrentTrain: epoch  4, batch    18 | loss: 20.5850080Losses:  14.059985160827637 4.367663860321045 5.046032905578613 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 14.0599852Losses:  20.66553508117795 4.57069730758667 5.0 5.835426438599825
CurrentTrain: epoch  4, batch    20 | loss: 20.6655351Losses:  17.998346496373415 4.471263885498047 5.0 3.1532899625599384
CurrentTrain: epoch  4, batch    21 | loss: 17.9983465Losses:  14.546731948852539 4.573598861694336 5.0 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 14.5467319Losses:  17.019064337015152 4.545001029968262 5.105817794799805 1.4194110929965973
CurrentTrain: epoch  4, batch    23 | loss: 17.0190643Losses:  20.36670980602503 4.409056186676025 5.0 5.806832410395145
CurrentTrain: epoch  4, batch    24 | loss: 20.3667098Losses:  14.643575668334961 4.608563423156738 5.0 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 14.6435757Losses:  14.406257629394531 4.407521724700928 5.0 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 14.4062576Losses:  22.302099615335464 4.616854667663574 5.178452491760254 5.81406632065773
CurrentTrain: epoch  4, batch    27 | loss: 22.3020996Losses:  22.91852318495512 4.463921546936035 5.0 8.672105185687542
CurrentTrain: epoch  4, batch    28 | loss: 22.9185232Losses:  17.025548551231623 4.391085624694824 5.082088947296143 1.6196952797472477
CurrentTrain: epoch  4, batch    29 | loss: 17.0255486Losses:  18.62279312312603 4.512062072753906 5.0 4.272313043475151
CurrentTrain: epoch  4, batch    30 | loss: 18.6227931Losses:  15.951968908309937 4.428420543670654 5.0 1.433013677597046
CurrentTrain: epoch  4, batch    31 | loss: 15.9519689Losses:  19.330504953861237 4.460378646850586 5.0 4.255374491214752
CurrentTrain: epoch  4, batch    32 | loss: 19.3305050Losses:  17.257634196430445 4.551449775695801 5.0 2.8294668532907963
CurrentTrain: epoch  4, batch    33 | loss: 17.2576342Losses:  19.602205865085125 4.5127129554748535 5.048486709594727 4.595200173556805
CurrentTrain: epoch  4, batch    34 | loss: 19.6022059Losses:  19.938053905963898 4.307190895080566 5.0 5.826232731342316
CurrentTrain: epoch  4, batch    35 | loss: 19.9380539Losses:  17.492001928389072 4.5278825759887695 5.0 2.839084066450596
CurrentTrain: epoch  4, batch    36 | loss: 17.4920019Losses:  19.73043079674244 4.446876049041748 5.09855842590332 4.4927064925432205
CurrentTrain: epoch  4, batch    37 | loss: 19.7304308Losses:  20.08237996697426 4.551483631134033 5.032637596130371 4.736923843622208
CurrentTrain: epoch  4, batch    38 | loss: 20.0823800Losses:  17.961131315678358 4.174251079559326 5.06452751159668 3.0752823166549206
CurrentTrain: epoch  4, batch    39 | loss: 17.9611313Losses:  14.305948257446289 4.307895183563232 5.0 -0.0
CurrentTrain: epoch  4, batch    40 | loss: 14.3059483Losses:  20.093327309936285 4.363757133483887 5.0 5.699515130370855
CurrentTrain: epoch  4, batch    41 | loss: 20.0933273Losses:  23.25171099230647 4.474381446838379 4.988607883453369 8.120069604367018
CurrentTrain: epoch  4, batch    42 | loss: 23.2517110Losses:  15.375504493713379 4.60378360748291 5.036835670471191 -0.0
CurrentTrain: epoch  4, batch    43 | loss: 15.3755045Losses:  17.242301353253424 4.463829040527344 5.014530658721924 2.2839216077700257
CurrentTrain: epoch  4, batch    44 | loss: 17.2423014Losses:  14.371377944946289 4.28835391998291 5.0 -0.0
CurrentTrain: epoch  4, batch    45 | loss: 14.3713779Losses:  17.343949142843485 4.399204254150391 5.0 2.9293697513639927
CurrentTrain: epoch  4, batch    46 | loss: 17.3439491Losses:  17.93867401406169 4.584217548370361 4.988301753997803 2.8728762082755566
CurrentTrain: epoch  4, batch    47 | loss: 17.9386740Losses:  18.07492123171687 4.370326995849609 5.0 3.100926022976637
CurrentTrain: epoch  4, batch    48 | loss: 18.0749212Losses:  14.939521789550781 4.514957904815674 5.0 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 14.9395218Losses:  17.7752420976758 4.473569393157959 4.988533020019531 2.882566697895527
CurrentTrain: epoch  4, batch    50 | loss: 17.7752421Losses:  22.138255087658763 4.689471244812012 5.03834342956543 6.045062033459544
CurrentTrain: epoch  4, batch    51 | loss: 22.1382551Losses:  15.037437438964844 4.5478315353393555 5.016688823699951 -0.0
CurrentTrain: epoch  4, batch    52 | loss: 15.0374374Losses:  19.693304197862744 4.383695602416992 5.003735542297363 4.699366705492139
CurrentTrain: epoch  4, batch    53 | loss: 19.6933042Losses:  22.176072854548693 4.346695899963379 5.0 7.608245629817247
CurrentTrain: epoch  4, batch    54 | loss: 22.1760729Losses:  15.810567736625671 4.102778434753418 5.053523540496826 1.4923933744430542
CurrentTrain: epoch  4, batch    55 | loss: 15.8105677Losses:  19.14160860143602 4.781261920928955 5.0 3.2535032089799643
CurrentTrain: epoch  4, batch    56 | loss: 19.1416086Losses:  28.31609370931983 4.416051864624023 4.973888397216797 13.17258670553565
CurrentTrain: epoch  4, batch    57 | loss: 28.3160937Losses:  17.30204526335001 4.381770610809326 5.031757354736328 2.9001764431595802
CurrentTrain: epoch  4, batch    58 | loss: 17.3020453Losses:  17.638812955468893 4.63268518447876 5.0 2.856769498437643
CurrentTrain: epoch  4, batch    59 | loss: 17.6388130Losses:  16.097647607326508 4.505397796630859 5.0 1.4136876463890076
CurrentTrain: epoch  4, batch    60 | loss: 16.0976476Losses:  19.629902727901936 4.550141334533691 5.070845603942871 3.9595908001065254
CurrentTrain: epoch  4, batch    61 | loss: 19.6299027Losses:  14.597249031066895 4.273601531982422 5.090376853942871 -0.0
CurrentTrain: epoch  4, batch    62 | loss: 14.5972490Losses:  15.897005647420883 4.428504943847656 4.973114013671875 1.3984170854091644
CurrentTrain: epoch  5, batch     0 | loss: 15.8970056Losses:  19.050359584391117 4.342022895812988 5.043334484100342 4.53072152286768
CurrentTrain: epoch  5, batch     1 | loss: 19.0503596Losses:  19.056703805923462 4.274373531341553 5.0 4.326500177383423
CurrentTrain: epoch  5, batch     2 | loss: 19.0567038Losses:  16.45923287421465 4.372292995452881 5.0 1.5680528357625008
CurrentTrain: epoch  5, batch     3 | loss: 16.4592329Losses:  17.61573188751936 4.273819923400879 5.015642166137695 3.160047225654125
CurrentTrain: epoch  5, batch     4 | loss: 17.6157319Losses:  14.976921081542969 4.518064498901367 5.0 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 14.9769211Losses:  19.259551540017128 4.131663799285889 5.045389175415039 4.8737025409936905
CurrentTrain: epoch  5, batch     6 | loss: 19.2595515Losses:  15.054562777280807 3.890446662902832 5.0 1.4047596156597137
CurrentTrain: epoch  5, batch     7 | loss: 15.0545628Losses:  18.922756493091583 4.487624645233154 5.136648178100586 3.452596962451935
CurrentTrain: epoch  5, batch     8 | loss: 18.9227565Losses:  19.80857762694359 4.488496780395508 5.021698951721191 4.490156263113022
CurrentTrain: epoch  5, batch     9 | loss: 19.8085776Losses:  14.093038558959961 4.14596700668335 5.0 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 14.0930386Losses:  18.012591015547514 4.57662296295166 4.9977874755859375 3.1621519438922405
CurrentTrain: epoch  5, batch    11 | loss: 18.0125910Losses:  17.021681249141693 4.258603572845459 5.0 2.8019365668296814
CurrentTrain: epoch  5, batch    12 | loss: 17.0216812Losses:  15.750782404094934 4.131210803985596 5.0 1.4418014623224735
CurrentTrain: epoch  5, batch    13 | loss: 15.7507824Losses:  14.9569091796875 4.392180442810059 5.050683498382568 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 14.9569092Losses:  20.262132618576288 4.2908501625061035 5.0 5.7793826796114445
CurrentTrain: epoch  5, batch    15 | loss: 20.2621326Losses:  19.081520315259695 4.242204189300537 5.0 4.397508855909109
CurrentTrain: epoch  5, batch    16 | loss: 19.0815203Losses:  15.749102290719748 4.241840839385986 5.0 1.4288737140595913
CurrentTrain: epoch  5, batch    17 | loss: 15.7491023Losses:  18.725665114820004 4.177463531494141 5.0 4.254059813916683
CurrentTrain: epoch  5, batch    18 | loss: 18.7256651Losses:  19.090683165937662 4.463629722595215 5.0 4.236227218061686
CurrentTrain: epoch  5, batch    19 | loss: 19.0906832Losses:  26.69870786368847 4.200331211090088 5.0 12.963334366679192
CurrentTrain: epoch  5, batch    20 | loss: 26.6987079Losses:  15.523287832736969 4.233739376068115 5.0 1.4331684708595276
CurrentTrain: epoch  5, batch    21 | loss: 15.5232878Losses:  15.099920272827148 4.173442840576172 5.044188022613525 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 15.0999203Losses:  14.79682731628418 4.315918922424316 5.05469274520874 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 14.7968273Losses:  16.219220511615276 4.201757431030273 5.017154216766357 1.488429419696331
CurrentTrain: epoch  5, batch    24 | loss: 16.2192205Losses:  13.588312149047852 4.273072242736816 5.0 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 13.5883121Losses:  16.41013616696 4.421088695526123 5.029493808746338 1.4465665258467197
CurrentTrain: epoch  5, batch    26 | loss: 16.4101362Losses:  14.22286605834961 4.307555198669434 5.029974937438965 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 14.2228661Losses:  17.865347411483526 4.096290111541748 5.0 4.228287246078253
CurrentTrain: epoch  5, batch    28 | loss: 17.8653474Losses:  16.254523873329163 4.376110553741455 5.047075271606445 1.6424938440322876
CurrentTrain: epoch  5, batch    29 | loss: 16.2545239Losses:  23.86262609064579 4.296763896942139 5.047934532165527 9.68163301050663
CurrentTrain: epoch  5, batch    30 | loss: 23.8626261Losses:  20.351367007941008 4.283855438232422 5.0 5.768782626837492
CurrentTrain: epoch  5, batch    31 | loss: 20.3513670Losses:  20.40934694558382 4.733144283294678 5.052489757537842 5.687400229275227
CurrentTrain: epoch  5, batch    32 | loss: 20.4093469Losses:  15.721064567565918 4.373480319976807 5.0 1.469435691833496
CurrentTrain: epoch  5, batch    33 | loss: 15.7210646Losses:  13.62312126159668 4.066781520843506 5.0 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 13.6231213Losses:  14.664676666259766 4.231963634490967 5.074644088745117 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 14.6646767Losses:  14.35198974609375 4.322221755981445 5.020415306091309 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 14.3519897Losses:  15.212865177541971 4.206461429595947 5.029950141906738 1.4208930172026157
CurrentTrain: epoch  5, batch    37 | loss: 15.2128652Losses:  22.505476348102093 4.081823348999023 5.0 8.146983496844769
CurrentTrain: epoch  5, batch    38 | loss: 22.5054763Losses:  19.367783717811108 4.01113748550415 5.024843215942383 5.349593333899975
CurrentTrain: epoch  5, batch    39 | loss: 19.3677837Losses:  19.14064346253872 4.149864196777344 4.977889060974121 4.346535071730614
CurrentTrain: epoch  5, batch    40 | loss: 19.1406435Losses:  13.961690902709961 4.187671661376953 5.0 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 13.9616909Losses:  15.535354405641556 4.04607629776001 5.036334037780762 1.408612996339798
CurrentTrain: epoch  5, batch    42 | loss: 15.5353544Losses:  14.099803924560547 4.083815097808838 5.043590068817139 -0.0
CurrentTrain: epoch  5, batch    43 | loss: 14.0998039Losses:  16.041064497083426 4.5392351150512695 5.009418964385986 1.4405701123178005
CurrentTrain: epoch  5, batch    44 | loss: 16.0410645Losses:  14.472237586975098 4.224776268005371 5.0 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 14.4722376Losses:  16.926337648183107 4.069993019104004 5.0 3.0782503373920918
CurrentTrain: epoch  5, batch    46 | loss: 16.9263376Losses:  14.346014976501465 4.48583984375 5.0558881759643555 -0.0
CurrentTrain: epoch  5, batch    47 | loss: 14.3460150Losses:  19.87704548239708 4.212761878967285 5.049157619476318 5.723980754613876
CurrentTrain: epoch  5, batch    48 | loss: 19.8770455Losses:  17.075903668999672 4.36182165145874 5.0 2.9964625984430313
CurrentTrain: epoch  5, batch    49 | loss: 17.0759037Losses:  15.03818130493164 4.401700973510742 5.0 -0.0
CurrentTrain: epoch  5, batch    50 | loss: 15.0381813Losses:  18.836766600608826 4.703540325164795 4.9716796875 4.210961699485779
CurrentTrain: epoch  5, batch    51 | loss: 18.8367666Losses:  15.43146339058876 4.232411861419678 5.0 1.4179212152957916
CurrentTrain: epoch  5, batch    52 | loss: 15.4314634Losses:  18.3973982706666 3.918405532836914 5.096280574798584 4.231500901281834
CurrentTrain: epoch  5, batch    53 | loss: 18.3973983Losses:  16.888031877577305 4.035245895385742 5.0 2.915889658033848
CurrentTrain: epoch  5, batch    54 | loss: 16.8880319Losses:  17.62573480606079 3.881561517715454 5.155651092529297 2.978257656097412
CurrentTrain: epoch  5, batch    55 | loss: 17.6257348Losses:  18.59022917598486 3.9709372520446777 5.0482258796691895 4.307830952107906
CurrentTrain: epoch  5, batch    56 | loss: 18.5902292Losses:  19.312522925436497 4.000514030456543 5.018246650695801 3.9095468893647194
CurrentTrain: epoch  5, batch    57 | loss: 19.3125229Losses:  16.56304794549942 4.190359115600586 5.0 2.8225846886634827
CurrentTrain: epoch  5, batch    58 | loss: 16.5630479Losses:  15.54265919700265 4.0106916427612305 5.026193618774414 1.4977773763239384
CurrentTrain: epoch  5, batch    59 | loss: 15.5426592Losses:  18.36834754794836 3.846494197845459 5.0 4.206188581883907
CurrentTrain: epoch  5, batch    60 | loss: 18.3683475Losses:  15.361573785543442 4.2298903465271 5.0431976318359375 1.4066139161586761
CurrentTrain: epoch  5, batch    61 | loss: 15.3615738Losses:  18.039796009659767 4.215272903442383 5.0 4.294866696000099
CurrentTrain: epoch  5, batch    62 | loss: 18.0397960Losses:  17.95095969736576 4.031275749206543 5.0 4.296844020485878
CurrentTrain: epoch  6, batch     0 | loss: 17.9509597Losses:  15.592936515808105 4.0474090576171875 5.0 1.4445266723632812
CurrentTrain: epoch  6, batch     1 | loss: 15.5929365Losses:  15.960771232843399 4.205386161804199 5.076862335205078 1.4042355120182037
CurrentTrain: epoch  6, batch     2 | loss: 15.9607712Losses:  15.538323432207108 4.314760208129883 5.0 1.4160166084766388
CurrentTrain: epoch  6, batch     3 | loss: 15.5383234Losses:  18.301468450576067 4.068222999572754 5.0 4.376486379653215
CurrentTrain: epoch  6, batch     4 | loss: 18.3014685Losses:  15.592545334249735 3.8877782821655273 5.0 1.587636772543192
CurrentTrain: epoch  6, batch     5 | loss: 15.5925453Losses:  18.799854300916195 4.357220649719238 5.052287578582764 4.2550792917609215
CurrentTrain: epoch  6, batch     6 | loss: 18.7998543Losses:  16.87573402747512 3.7777891159057617 5.0 2.8991381488740444
CurrentTrain: epoch  6, batch     7 | loss: 16.8757340Losses:  13.24484634399414 3.728969097137451 5.0 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 13.2448463Losses:  19.44899808615446 4.109823703765869 5.032824516296387 4.7698236629366875
CurrentTrain: epoch  6, batch     9 | loss: 19.4489981Losses:  13.82829475402832 3.8635714054107666 4.9915995597839355 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 13.8282948Losses:  13.958586692810059 3.9390690326690674 5.0 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 13.9585867Losses:  13.918233871459961 3.7515647411346436 5.0 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 13.9182339Losses:  16.946812596172094 4.163424015045166 5.0 2.889780964702368
CurrentTrain: epoch  6, batch    13 | loss: 16.9468126Losses:  13.826359748840332 3.766099691390991 5.0 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 13.8263597Losses:  15.353814035654068 4.411703586578369 5.0 1.4024952948093414
CurrentTrain: epoch  6, batch    15 | loss: 15.3538140Losses:  16.133842945098877 4.42591667175293 5.0 1.3914036750793457
CurrentTrain: epoch  6, batch    16 | loss: 16.1338429Losses:  16.700581427663565 4.138512134552002 5.0 2.846409674733877
CurrentTrain: epoch  6, batch    17 | loss: 16.7005814Losses:  14.299230575561523 4.227302551269531 5.0 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 14.2992306Losses:  22.193188071250916 4.005802631378174 4.975701332092285 7.500873923301697
CurrentTrain: epoch  6, batch    19 | loss: 22.1931881Losses:  15.396704103797674 4.147042274475098 5.0 1.4241355918347836
CurrentTrain: epoch  6, batch    20 | loss: 15.3967041Losses:  13.64195728302002 4.043633937835693 5.0 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 13.6419573Losses:  16.789514500647783 4.161190986633301 5.0 2.863587338477373
CurrentTrain: epoch  6, batch    22 | loss: 16.7895145Losses:  13.717073440551758 4.066184997558594 5.0 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 13.7170734Losses:  13.787933349609375 3.686223030090332 5.0 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 13.7879333Losses:  19.559276469051838 4.266434669494629 5.0 5.77189052850008
CurrentTrain: epoch  6, batch    25 | loss: 19.5592765Losses:  16.914671003818512 4.2288384437561035 5.0 2.8837090134620667
CurrentTrain: epoch  6, batch    26 | loss: 16.9146710Losses:  18.894179049879313 4.291559219360352 5.016602516174316 4.291747752577066
CurrentTrain: epoch  6, batch    27 | loss: 18.8941790Losses:  15.55477723479271 4.449396133422852 5.0 1.4213820397853851
CurrentTrain: epoch  6, batch    28 | loss: 15.5547772Losses:  17.73511728644371 3.920529842376709 5.0 4.207493156194687
CurrentTrain: epoch  6, batch    29 | loss: 17.7351173Losses:  14.003193855285645 4.187676906585693 5.052406311035156 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 14.0031939Losses:  21.21972358226776 4.374957084655762 5.0 7.37468421459198
CurrentTrain: epoch  6, batch    31 | loss: 21.2197236Losses:  18.221013497561216 3.9693844318389893 5.050456523895264 4.42147583886981
CurrentTrain: epoch  6, batch    32 | loss: 18.2210135Losses:  16.479653414338827 3.921811580657959 5.0 2.8741808496415615
CurrentTrain: epoch  6, batch    33 | loss: 16.4796534Losses:  18.850623607635498 4.3239593505859375 5.0 4.793895244598389
CurrentTrain: epoch  6, batch    34 | loss: 18.8506236Losses:  16.18279143050313 3.81337571144104 5.0 2.9094960279762745
CurrentTrain: epoch  6, batch    35 | loss: 16.1827914Losses:  15.454730197787285 4.272416591644287 5.0 1.4435207098722458
CurrentTrain: epoch  6, batch    36 | loss: 15.4547302Losses:  14.20915412902832 4.302164077758789 5.0 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 14.2091541Losses:  17.000284761190414 4.389997959136963 5.0 2.806206315755844
CurrentTrain: epoch  6, batch    38 | loss: 17.0002848Losses:  13.066722869873047 3.377364158630371 5.044992446899414 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 13.0667229Losses:  15.236696928739548 3.809643268585205 5.0 1.8212372958660126
CurrentTrain: epoch  6, batch    40 | loss: 15.2366969Losses:  13.571484565734863 4.057843208312988 5.0 -0.0
CurrentTrain: epoch  6, batch    41 | loss: 13.5714846Losses:  15.244168072938919 4.14221715927124 5.0 1.4043319523334503
CurrentTrain: epoch  6, batch    42 | loss: 15.2441681Losses:  16.986316233873367 4.212990760803223 5.065871715545654 2.8337550461292267
CurrentTrain: epoch  6, batch    43 | loss: 16.9863162Losses:  14.555527210235596 3.8139472007751465 5.0 1.4641051292419434
CurrentTrain: epoch  6, batch    44 | loss: 14.5555272Losses:  13.735340118408203 4.084566116333008 5.0 -0.0
CurrentTrain: epoch  6, batch    45 | loss: 13.7353401Losses:  15.771585553884506 4.257018089294434 5.015814304351807 1.4236489236354828
CurrentTrain: epoch  6, batch    46 | loss: 15.7715856Losses:  15.571219380944967 4.227290630340576 5.0 1.7106360755860806
CurrentTrain: epoch  6, batch    47 | loss: 15.5712194Losses:  19.70033849775791 4.116438865661621 5.039139747619629 5.909675732254982
CurrentTrain: epoch  6, batch    48 | loss: 19.7003385Losses:  18.13736965507269 3.984036445617676 4.9967546463012695 4.419526599347591
CurrentTrain: epoch  6, batch    49 | loss: 18.1373697Losses:  16.894930101931095 4.314661979675293 5.0 2.865703798830509
CurrentTrain: epoch  6, batch    50 | loss: 16.8949301Losses:  14.473843038082123 3.659093141555786 5.0 1.3905510306358337
CurrentTrain: epoch  6, batch    51 | loss: 14.4738430Losses:  18.46099280565977 4.303761005401611 5.158321380615234 2.8728504106402397
CurrentTrain: epoch  6, batch    52 | loss: 18.4609928Losses:  16.74014700576663 3.8664326667785645 5.0 2.9069846980273724
CurrentTrain: epoch  6, batch    53 | loss: 16.7401470Losses:  18.924265541136265 4.38029146194458 5.0 4.737051643431187
CurrentTrain: epoch  6, batch    54 | loss: 18.9242655Losses:  17.277991883456707 4.153571605682373 5.0 3.1504551097750664
CurrentTrain: epoch  6, batch    55 | loss: 17.2779919Losses:  14.93905484676361 3.9484035968780518 5.0 1.3989118337631226
CurrentTrain: epoch  6, batch    56 | loss: 14.9390548Losses:  14.092894554138184 3.9469194412231445 5.113640785217285 -0.0
CurrentTrain: epoch  6, batch    57 | loss: 14.0928946Losses:  14.713158462196589 3.7140729427337646 5.0 1.4584711529314518
CurrentTrain: epoch  6, batch    58 | loss: 14.7131585Losses:  16.733392920345068 4.07011604309082 5.0 2.8379509113729
CurrentTrain: epoch  6, batch    59 | loss: 16.7333929Losses:  17.65282404795289 3.8720245361328125 5.0 4.300739888101816
CurrentTrain: epoch  6, batch    60 | loss: 17.6528240Losses:  15.797669559717178 3.839207887649536 5.102466583251953 1.4202128946781158
CurrentTrain: epoch  6, batch    61 | loss: 15.7976696Losses:  12.998170852661133 3.4878504276275635 5.0 -0.0
CurrentTrain: epoch  6, batch    62 | loss: 12.9981709Losses:  23.29103922471404 3.9415090084075928 5.0 9.325740572065115
CurrentTrain: epoch  7, batch     0 | loss: 23.2910392Losses:  14.618217267096043 3.6516711711883545 5.0 1.473961628973484
CurrentTrain: epoch  7, batch     1 | loss: 14.6182173Losses:  16.765862949192524 4.070544242858887 5.0 2.9978966787457466
CurrentTrain: epoch  7, batch     2 | loss: 16.7658629Losses:  14.811109095811844 3.834702491760254 5.0 1.4071727097034454
CurrentTrain: epoch  7, batch     3 | loss: 14.8111091Losses:  15.55038034543395 4.127480506896973 5.0 1.5711456499993801
CurrentTrain: epoch  7, batch     4 | loss: 15.5503803Losses:  16.063130911439657 3.8724398612976074 5.0 2.8653331361711025
CurrentTrain: epoch  7, batch     5 | loss: 16.0631309Losses:  13.467594146728516 3.746309757232666 5.0 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 13.4675941Losses:  18.512856125831604 4.178237438201904 5.0 4.368489861488342
CurrentTrain: epoch  7, batch     7 | loss: 18.5128561Losses:  15.714991122484207 4.2480034828186035 5.0 1.4069543182849884
CurrentTrain: epoch  7, batch     8 | loss: 15.7149911Losses:  17.413623966276646 3.5995092391967773 5.0 4.246587909758091
CurrentTrain: epoch  7, batch     9 | loss: 17.4136240Losses:  13.069890975952148 3.711448907852173 5.0 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 13.0698910Losses:  16.621606204658747 4.163351058959961 5.0 2.8414987064898014
CurrentTrain: epoch  7, batch    11 | loss: 16.6216062Losses:  15.056060966104269 3.876500368118286 5.0 1.5375940166413784
CurrentTrain: epoch  7, batch    12 | loss: 15.0560610Losses:  14.91964876651764 3.8899455070495605 5.0 1.4045921564102173
CurrentTrain: epoch  7, batch    13 | loss: 14.9196488Losses:  16.098548412322998 3.7575490474700928 5.0 2.821479320526123
CurrentTrain: epoch  7, batch    14 | loss: 16.0985484Losses:  17.998285576701164 3.7278356552124023 4.994781970977783 4.320560738444328
CurrentTrain: epoch  7, batch    15 | loss: 17.9982856Losses:  16.07083970308304 3.534292697906494 5.0 2.810356914997101
CurrentTrain: epoch  7, batch    16 | loss: 16.0708397Losses:  14.27339991927147 3.4529223442077637 5.0 1.403625100851059
CurrentTrain: epoch  7, batch    17 | loss: 14.2733999Losses:  16.381021738052368 3.9895429611206055 5.0 2.810039758682251
CurrentTrain: epoch  7, batch    18 | loss: 16.3810217Losses:  13.025131225585938 3.4665560722351074 5.0 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 13.0251312Losses:  18.285312104970217 4.013325214385986 5.0 4.743928361684084
CurrentTrain: epoch  7, batch    20 | loss: 18.2853121Losses:  13.408931732177734 3.870151996612549 5.0 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 13.4089317Losses:  16.00741022825241 3.74871826171875 5.0 2.819527804851532
CurrentTrain: epoch  7, batch    22 | loss: 16.0074102Losses:  13.418375015258789 4.045053482055664 5.048458576202393 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 13.4183750Losses:  14.812908962368965 3.619879961013794 5.0 1.6465719491243362
CurrentTrain: epoch  7, batch    24 | loss: 14.8129090Losses:  13.437470436096191 3.9972236156463623 5.0 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 13.4374704Losses:  16.52005746588111 3.9583864212036133 5.0 2.848613526672125
CurrentTrain: epoch  7, batch    26 | loss: 16.5200575Losses:  17.677748207002878 3.8877875804901123 5.0 4.290305618196726
CurrentTrain: epoch  7, batch    27 | loss: 17.6777482Losses:  17.53863814473152 3.71689510345459 5.0 4.3678522408008575
CurrentTrain: epoch  7, batch    28 | loss: 17.5386381Losses:  15.860979221761227 3.62890625 5.0 2.874059818685055
CurrentTrain: epoch  7, batch    29 | loss: 15.8609792Losses:  15.127983927726746 3.960610866546631 4.988785743713379 1.3941096067428589
CurrentTrain: epoch  7, batch    30 | loss: 15.1279839Losses:  16.5433968603611 3.9873924255371094 5.0 3.0369557440280914
CurrentTrain: epoch  7, batch    31 | loss: 16.5433969Losses:  17.53298493474722 3.8475327491760254 5.0 4.304784022271633
CurrentTrain: epoch  7, batch    32 | loss: 17.5329849Losses:  13.225797653198242 3.7772889137268066 5.0 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 13.2257977Losses:  14.541025497019291 3.7749273777008057 5.0 1.4331305995583534
CurrentTrain: epoch  7, batch    34 | loss: 14.5410255Losses:  16.028440419584513 3.5054049491882324 5.0 2.9530848897993565
CurrentTrain: epoch  7, batch    35 | loss: 16.0284404Losses:  15.65034618973732 4.073275566101074 5.023416042327881 1.4339193403720856
CurrentTrain: epoch  7, batch    36 | loss: 15.6503462Losses:  13.150186538696289 3.6877641677856445 5.0 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 13.1501865Losses:  17.14555238187313 3.6530609130859375 5.0 4.201226934790611
CurrentTrain: epoch  7, batch    38 | loss: 17.1455524Losses:  24.947252690792084 4.0240373611450195 5.032059669494629 10.840905606746674
CurrentTrain: epoch  7, batch    39 | loss: 24.9472527Losses:  14.612565722316504 3.619746208190918 5.0 1.4489066265523434
CurrentTrain: epoch  7, batch    40 | loss: 14.6125657Losses:  15.956977099180222 3.621903657913208 5.0 2.842126101255417
CurrentTrain: epoch  7, batch    41 | loss: 15.9569771Losses:  16.397022109478712 3.814699172973633 5.034707069396973 2.938370566815138
CurrentTrain: epoch  7, batch    42 | loss: 16.3970221Losses:  19.01941168308258 3.9535374641418457 5.0 5.638685822486877
CurrentTrain: epoch  7, batch    43 | loss: 19.0194117Losses:  16.16367793083191 3.6787760257720947 5.0 2.8706138134002686
CurrentTrain: epoch  7, batch    44 | loss: 16.1636779Losses:  17.578398790210485 3.7249183654785156 5.04700231552124 4.287480439990759
CurrentTrain: epoch  7, batch    45 | loss: 17.5783988Losses:  17.16161298751831 4.336594581604004 5.0 2.8902125358581543
CurrentTrain: epoch  7, batch    46 | loss: 17.1616130Losses:  17.38989582657814 4.279167175292969 5.059055328369141 2.8640741407871246
CurrentTrain: epoch  7, batch    47 | loss: 17.3898958Losses:  14.520624935626984 3.6497092247009277 5.0 1.3963516354560852
CurrentTrain: epoch  7, batch    48 | loss: 14.5206249Losses:  13.214133262634277 3.5322797298431396 5.0 -0.0
CurrentTrain: epoch  7, batch    49 | loss: 13.2141333Losses:  16.397132344543934 3.9184951782226562 5.0 2.8747858479619026
CurrentTrain: epoch  7, batch    50 | loss: 16.3971323Losses:  21.632379952818155 3.511507511138916 5.0 8.849781457334757
CurrentTrain: epoch  7, batch    51 | loss: 21.6323800Losses:  15.935211416333914 3.5848801136016846 5.0 2.9291374646127224
CurrentTrain: epoch  7, batch    52 | loss: 15.9352114Losses:  17.47300922870636 3.5796523094177246 5.0 4.384210705757141
CurrentTrain: epoch  7, batch    53 | loss: 17.4730092Losses:  19.00363478064537 3.9463627338409424 5.0 5.712416023015976
CurrentTrain: epoch  7, batch    54 | loss: 19.0036348Losses:  13.40147590637207 3.8628592491149902 5.0 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 13.4014759Losses:  18.626072891056538 3.778593063354492 5.0 5.041707046329975
CurrentTrain: epoch  7, batch    56 | loss: 18.6260729Losses:  16.308139145374298 3.9154210090637207 5.0 2.976304352283478
CurrentTrain: epoch  7, batch    57 | loss: 16.3081391Losses:  17.710232108831406 3.993387460708618 5.0 4.402758926153183
CurrentTrain: epoch  7, batch    58 | loss: 17.7102321Losses:  16.452300664037466 3.705280303955078 5.0 3.322724934667349
CurrentTrain: epoch  7, batch    59 | loss: 16.4523007Losses:  17.506978765130043 3.937100410461426 5.0 4.21370293200016
CurrentTrain: epoch  7, batch    60 | loss: 17.5069788Losses:  13.569249153137207 3.8660728931427 5.0 -0.0
CurrentTrain: epoch  7, batch    61 | loss: 13.5692492Losses:  16.925691578537226 3.4209439754486084 5.0 4.263592693954706
CurrentTrain: epoch  7, batch    62 | loss: 16.9256916Losses:  23.759258672595024 3.563974142074585 5.0 10.721854612231255
CurrentTrain: epoch  8, batch     0 | loss: 23.7592587Losses:  14.289647817611694 3.6438469886779785 5.0 1.3919694423675537
CurrentTrain: epoch  8, batch     1 | loss: 14.2896478Losses:  14.541599810123444 3.7949421405792236 5.0 1.4064756035804749
CurrentTrain: epoch  8, batch     2 | loss: 14.5415998Losses:  18.120413325726986 3.8799943923950195 5.0 4.365876697003841
CurrentTrain: epoch  8, batch     3 | loss: 18.1204133Losses:  16.94393217563629 3.3747634887695312 5.0 4.2103201150894165
CurrentTrain: epoch  8, batch     4 | loss: 16.9439322Losses:  17.48185870051384 3.913889169692993 5.0 4.241670101881027
CurrentTrain: epoch  8, batch     5 | loss: 17.4818587Losses:  15.718283712863922 3.67545747756958 5.0 2.8152552247047424
CurrentTrain: epoch  8, batch     6 | loss: 15.7182837Losses:  14.492200497537851 3.7544713020324707 5.0 1.4341255463659763
CurrentTrain: epoch  8, batch     7 | loss: 14.4922005Losses:  15.508334655314684 3.842047691345215 5.0 1.8586421199142933
CurrentTrain: epoch  8, batch     8 | loss: 15.5083347Losses:  12.920524597167969 3.38035249710083 5.088074207305908 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 12.9205246Losses:  19.36512289941311 3.911191940307617 5.0 5.949341878294945
CurrentTrain: epoch  8, batch    10 | loss: 19.3651229Losses:  13.046773910522461 3.5615646839141846 5.0 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 13.0467739Losses:  14.045523911714554 3.3883635997772217 5.0 1.3959791958332062
CurrentTrain: epoch  8, batch    12 | loss: 14.0455239Losses:  17.416888117790222 3.560699462890625 5.0 4.40221107006073
CurrentTrain: epoch  8, batch    13 | loss: 17.4168881Losses:  14.443012494593859 3.5681259632110596 5.0 1.6157772727310658
CurrentTrain: epoch  8, batch    14 | loss: 14.4430125Losses:  17.14752745628357 3.629678249359131 5.050062656402588 4.292945623397827
CurrentTrain: epoch  8, batch    15 | loss: 17.1475275Losses:  19.837912417948246 3.17618989944458 5.0 7.505862094461918
CurrentTrain: epoch  8, batch    16 | loss: 19.8379124Losses:  16.122163586318493 3.6582751274108887 5.0 2.837373547255993
CurrentTrain: epoch  8, batch    17 | loss: 16.1221636Losses:  12.772412300109863 3.536261558532715 5.0 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 12.7724123Losses:  14.251560598611832 3.4841833114624023 5.0434746742248535 1.4014305174350739
CurrentTrain: epoch  8, batch    19 | loss: 14.2515606Losses:  14.183176457881927 3.5892958641052246 5.0 1.3938164114952087
CurrentTrain: epoch  8, batch    20 | loss: 14.1831765Losses:  14.871637051925063 3.8753671646118164 5.0 1.7404038365930319
CurrentTrain: epoch  8, batch    21 | loss: 14.8716371Losses:  15.999904166907072 3.7524147033691406 5.0 2.8949055783450603
CurrentTrain: epoch  8, batch    22 | loss: 15.9999042Losses:  14.22420958802104 3.387317419052124 5.0 1.4893577508628368
CurrentTrain: epoch  8, batch    23 | loss: 14.2242096Losses:  19.377816431224346 3.425837993621826 5.049957275390625 5.864449732005596
CurrentTrain: epoch  8, batch    24 | loss: 19.3778164Losses:  14.931453105062246 4.001640319824219 5.080303192138672 1.4203218184411526
CurrentTrain: epoch  8, batch    25 | loss: 14.9314531Losses:  13.76450401544571 3.0881993770599365 5.0 1.4071527123451233
CurrentTrain: epoch  8, batch    26 | loss: 13.7645040Losses:  16.767394185066223 3.2199997901916504 5.0 4.232800602912903
CurrentTrain: epoch  8, batch    27 | loss: 16.7673942Losses:  12.927022933959961 3.643932342529297 5.0 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 12.9270229Losses:  19.97397953271866 3.594388484954834 5.0 7.055732309818268
CurrentTrain: epoch  8, batch    29 | loss: 19.9739795Losses:  13.629989624023438 3.9244775772094727 5.014649391174316 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 13.6299896Losses:  12.676087379455566 3.4219963550567627 5.0 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 12.6760874Losses:  14.563080821186304 3.866285800933838 5.052362442016602 1.441327128559351
CurrentTrain: epoch  8, batch    32 | loss: 14.5630808Losses:  17.17477349564433 3.5341796875 5.0 4.294575970619917
CurrentTrain: epoch  8, batch    33 | loss: 17.1747735Losses:  13.002667427062988 3.679365634918213 5.0 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 13.0026674Losses:  16.404536809772253 2.927313804626465 5.0 4.226967420428991
CurrentTrain: epoch  8, batch    35 | loss: 16.4045368Losses:  13.942443400621414 3.3090157508850098 5.0 1.3891196548938751
CurrentTrain: epoch  8, batch    36 | loss: 13.9424434Losses:  12.310068130493164 3.0983290672302246 5.0 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 12.3100681Losses:  14.051352560520172 3.456258535385132 5.0 1.4151163697242737
CurrentTrain: epoch  8, batch    38 | loss: 14.0513526Losses:  12.811848640441895 3.393782615661621 5.0 -0.0
CurrentTrain: epoch  8, batch    39 | loss: 12.8118486Losses:  14.775255683809519 3.8125863075256348 5.0 1.4733796156942844
CurrentTrain: epoch  8, batch    40 | loss: 14.7752557Losses:  15.521215088665485 3.532951831817627 5.0 2.84114421159029
CurrentTrain: epoch  8, batch    41 | loss: 15.5212151Losses:  14.304685592651367 3.577666759490967 5.0 1.416402816772461
CurrentTrain: epoch  8, batch    42 | loss: 14.3046856Losses:  17.453164994716644 3.904787540435791 5.0 4.288480699062347
CurrentTrain: epoch  8, batch    43 | loss: 17.4531650Losses:  13.830930322408676 3.1643226146698 5.0 1.412378877401352
CurrentTrain: epoch  8, batch    44 | loss: 13.8309303Losses:  14.62243738770485 3.7436647415161133 5.0 1.4408587515354156
CurrentTrain: epoch  8, batch    45 | loss: 14.6224374Losses:  14.06882992386818 3.4516830444335938 5.0 1.4012788832187653
CurrentTrain: epoch  8, batch    46 | loss: 14.0688299Losses:  13.085906982421875 3.7811975479125977 5.0 -0.0
CurrentTrain: epoch  8, batch    47 | loss: 13.0859070Losses:  16.15011176466942 3.9338369369506836 5.038738250732422 2.8311706483364105
CurrentTrain: epoch  8, batch    48 | loss: 16.1501118Losses:  12.757917404174805 3.5006823539733887 5.0 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 12.7579174Losses:  13.15361213684082 3.8060519695281982 5.0 -0.0
CurrentTrain: epoch  8, batch    50 | loss: 13.1536121Losses:  18.293575763702393 3.5045289993286133 5.0 5.618302822113037
CurrentTrain: epoch  8, batch    51 | loss: 18.2935758Losses:  17.31349817663431 3.758683681488037 5.0 4.333424247801304
CurrentTrain: epoch  8, batch    52 | loss: 17.3134982Losses:  14.454985588788986 3.573852062225342 5.0 1.4120168387889862
CurrentTrain: epoch  8, batch    53 | loss: 14.4549856Losses:  15.791692443192005 3.4852030277252197 5.0 2.9215571358799934
CurrentTrain: epoch  8, batch    54 | loss: 15.7916924Losses:  14.219479143619537 3.3637516498565674 5.024140357971191 1.4119916558265686
CurrentTrain: epoch  8, batch    55 | loss: 14.2194791Losses:  14.27159658074379 3.437215566635132 5.0 1.429256111383438
CurrentTrain: epoch  8, batch    56 | loss: 14.2715966Losses:  13.074601173400879 3.7981648445129395 5.038630485534668 -0.0
CurrentTrain: epoch  8, batch    57 | loss: 13.0746012Losses:  12.269758224487305 3.1067752838134766 5.0 -0.0
CurrentTrain: epoch  8, batch    58 | loss: 12.2697582Losses:  14.455378711223602 3.852752447128296 5.034522533416748 1.3937923312187195
CurrentTrain: epoch  8, batch    59 | loss: 14.4553787Losses:  12.73729133605957 3.467787742614746 5.0 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 12.7372913Losses:  12.855016708374023 3.6317076683044434 5.0 -0.0
CurrentTrain: epoch  8, batch    61 | loss: 12.8550167Losses:  14.345901969820261 3.6271417140960693 5.0 1.4363627471029758
CurrentTrain: epoch  8, batch    62 | loss: 14.3459020Losses:  13.951089411973953 3.2265918254852295 5.0 1.4219193756580353
CurrentTrain: epoch  9, batch     0 | loss: 13.9510894Losses:  13.246736526489258 3.531599521636963 5.030487537384033 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 13.2467365Losses:  15.396272365003824 3.3983986377716064 5.0 2.8123423494398594
CurrentTrain: epoch  9, batch     2 | loss: 15.3962724Losses:  14.272902600467205 3.6273727416992188 5.0 1.4432898685336113
CurrentTrain: epoch  9, batch     3 | loss: 14.2729026Losses:  14.327298317104578 3.5881428718566895 5.0 1.5076018907129765
CurrentTrain: epoch  9, batch     4 | loss: 14.3272983Losses:  25.85275261104107 3.724076509475708 5.027122497558594 12.897702142596245
CurrentTrain: epoch  9, batch     5 | loss: 25.8527526Losses:  14.252250973135233 3.4193100929260254 5.0 1.4855340160429478
CurrentTrain: epoch  9, batch     6 | loss: 14.2522510Losses:  13.972712986171246 3.3013203144073486 5.0 1.43910264223814
CurrentTrain: epoch  9, batch     7 | loss: 13.9727130Losses:  12.97746753692627 3.7662343978881836 5.0 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 12.9774675Losses:  19.26034528017044 3.328312397003174 5.0 6.654249012470245
CurrentTrain: epoch  9, batch     9 | loss: 19.2603453Losses:  21.24466209858656 3.5959689617156982 5.0 8.410328678786755
CurrentTrain: epoch  9, batch    10 | loss: 21.2446621Losses:  15.493899911642075 3.4404754638671875 5.0 2.870219796895981
CurrentTrain: epoch  9, batch    11 | loss: 15.4938999Losses:  15.53517946600914 3.536320686340332 5.0 2.834026664495468
CurrentTrain: epoch  9, batch    12 | loss: 15.5351795Losses:  12.431863784790039 3.267360210418701 5.029513835906982 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 12.4318638Losses:  13.937396489083767 3.3721983432769775 5.0 1.437009297311306
CurrentTrain: epoch  9, batch    14 | loss: 13.9373965Losses:  15.878820985555649 2.6071934700012207 5.0 4.207034677267075
CurrentTrain: epoch  9, batch    15 | loss: 15.8788210Losses:  17.96232820302248 3.088383197784424 5.0 5.694334276020527
CurrentTrain: epoch  9, batch    16 | loss: 17.9623282Losses:  15.588213484734297 3.5745906829833984 5.0 2.872728865593672
CurrentTrain: epoch  9, batch    17 | loss: 15.5882135Losses:  13.848602622747421 3.136782169342041 5.060345649719238 1.4028285443782806
CurrentTrain: epoch  9, batch    18 | loss: 13.8486026Losses:  18.285417020320892 3.108271598815918 5.062165260314941 5.785447537899017
CurrentTrain: epoch  9, batch    19 | loss: 18.2854170Losses:  15.564114350825548 3.172032356262207 5.0 3.1221273131668568
CurrentTrain: epoch  9, batch    20 | loss: 15.5641144Losses:  14.0056674182415 3.3679075241088867 5.0 1.3993308246135712
CurrentTrain: epoch  9, batch    21 | loss: 14.0056674Losses:  15.139077924191952 2.9134349822998047 5.0 3.123317502439022
CurrentTrain: epoch  9, batch    22 | loss: 15.1390779Losses:  13.862492576241493 3.1989598274230957 5.0 1.4703493267297745
CurrentTrain: epoch  9, batch    23 | loss: 13.8624926Losses:  14.027678430080414 3.383903980255127 5.0 1.4072522521018982
CurrentTrain: epoch  9, batch    24 | loss: 14.0276784Losses:  15.3638596534729 3.4137823581695557 5.0 2.8132100105285645
CurrentTrain: epoch  9, batch    25 | loss: 15.3638597Losses:  15.371663928031921 3.3677093982696533 5.0 2.837741732597351
CurrentTrain: epoch  9, batch    26 | loss: 15.3716639Losses:  19.974162593483925 3.7032318115234375 5.0 7.058125987648964
CurrentTrain: epoch  9, batch    27 | loss: 19.9741626Losses:  14.157948553562164 3.668558359146118 5.0 1.391909658908844
CurrentTrain: epoch  9, batch    28 | loss: 14.1579486Losses:  14.205473687499762 3.471414089202881 5.0 1.4531944058835506
CurrentTrain: epoch  9, batch    29 | loss: 14.2054737Losses:  16.39479249343276 3.716675281524658 5.0 3.399454053491354
CurrentTrain: epoch  9, batch    30 | loss: 16.3947925Losses:  16.673331759870052 3.268761157989502 5.0 4.235360644757748
CurrentTrain: epoch  9, batch    31 | loss: 16.6733318Losses:  17.96619686484337 3.064021110534668 5.0 5.667440265417099
CurrentTrain: epoch  9, batch    32 | loss: 17.9661969Losses:  15.557872623205185 3.549778938293457 5.0 2.8142422139644623
CurrentTrain: epoch  9, batch    33 | loss: 15.5578726Losses:  12.550236701965332 3.402454376220703 5.0 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 12.5502367Losses:  17.437029629945755 3.945673704147339 5.0 4.310528546571732
CurrentTrain: epoch  9, batch    35 | loss: 17.4370296Losses:  15.174182523041964 3.110347032546997 5.0 2.9192882664501667
CurrentTrain: epoch  9, batch    36 | loss: 15.1741825Losses:  14.490243792533875 3.5421359539031982 5.023036956787109 1.4177016019821167
CurrentTrain: epoch  9, batch    37 | loss: 14.4902438Losses:  15.613850828260183 3.2503247261047363 5.0232625007629395 2.9098551236093044
CurrentTrain: epoch  9, batch    38 | loss: 15.6138508Losses:  14.223892539739609 3.606294870376587 5.0 1.4361670911312103
CurrentTrain: epoch  9, batch    39 | loss: 14.2238925Losses:  18.139170110225677 3.265932321548462 5.0 5.691532552242279
CurrentTrain: epoch  9, batch    40 | loss: 18.1391701Losses:  16.151507645845413 2.8251142501831055 5.0 4.249579697847366
CurrentTrain: epoch  9, batch    41 | loss: 16.1515076Losses:  12.241647720336914 3.1248669624328613 5.0 -0.0
CurrentTrain: epoch  9, batch    42 | loss: 12.2416477Losses:  13.38293519243598 2.848353862762451 5.0 1.4723650477826595
CurrentTrain: epoch  9, batch    43 | loss: 13.3829352Losses:  17.566491775214672 3.3041319847106934 5.038599967956543 4.998222999274731
CurrentTrain: epoch  9, batch    44 | loss: 17.5664918Losses:  15.74203535169363 3.5287394523620605 5.0 2.912365399301052
CurrentTrain: epoch  9, batch    45 | loss: 15.7420354Losses:  17.21344843879342 3.7085814476013184 5.030879020690918 4.258221540600061
CurrentTrain: epoch  9, batch    46 | loss: 17.2134484Losses:  16.423049606382847 2.9916701316833496 5.035018444061279 4.211558975279331
CurrentTrain: epoch  9, batch    47 | loss: 16.4230496Losses:  12.803309440612793 3.6688194274902344 5.0 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 12.8033094Losses:  13.622620075941086 3.114713668823242 5.0 1.3973574340343475
CurrentTrain: epoch  9, batch    49 | loss: 13.6226201Losses:  15.648216031491756 3.686110019683838 5.0 2.828994534909725
CurrentTrain: epoch  9, batch    50 | loss: 15.6482160Losses:  21.019068650901318 3.32102370262146 5.0 8.440950326621532
CurrentTrain: epoch  9, batch    51 | loss: 21.0190687Losses:  14.306217223405838 3.723702907562256 5.0 1.3978939354419708
CurrentTrain: epoch  9, batch    52 | loss: 14.3062172Losses:  24.561635851860046 3.2116096019744873 5.0 12.16557490825653
CurrentTrain: epoch  9, batch    53 | loss: 24.5616359Losses:  17.54354913532734 2.792802095413208 5.0 5.620683267712593
CurrentTrain: epoch  9, batch    54 | loss: 17.5435491Losses:  14.851751893758774 2.9415841102600098 5.0 2.835791200399399
CurrentTrain: epoch  9, batch    55 | loss: 14.8517519Losses:  17.532076194882393 2.691636562347412 5.0 5.736015632748604
CurrentTrain: epoch  9, batch    56 | loss: 17.5320762Losses:  12.195204734802246 3.089627504348755 5.0 -0.0
CurrentTrain: epoch  9, batch    57 | loss: 12.1952047Losses:  12.12490177154541 2.9780237674713135 5.0 -0.0
CurrentTrain: epoch  9, batch    58 | loss: 12.1249018Losses:  16.214040733873844 2.8288650512695312 5.0 4.201342560350895
CurrentTrain: epoch  9, batch    59 | loss: 16.2140407Losses:  15.008308175951242 2.9658517837524414 5.046874523162842 2.8302590884268284
CurrentTrain: epoch  9, batch    60 | loss: 15.0083082Losses:  14.086739033460617 3.4372923374176025 5.029145240783691 1.39596888422966
CurrentTrain: epoch  9, batch    61 | loss: 14.0867390Losses:  13.314315527677536 2.739457607269287 5.0 1.3925454318523407
CurrentTrain: epoch  9, batch    62 | loss: 13.3143155
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.66%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.95%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.66%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.95%   
cur_acc:  ['0.9395']
his_acc:  ['0.9395']
Clustering into  9  clusters
Clusters:  [7 0 8 4 5 6 3 0 2 2 0 0 1 0 0 0 0 0 1 0]
Losses:  18.619356155395508 5.285177707672119 6.128393173217773 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 18.6193562Losses:  18.645851135253906 5.21700382232666 6.040022373199463 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 18.6458511Losses:  20.921625524759293 5.261129379272461 6.053921222686768 3.0657314360141754
CurrentTrain: epoch  0, batch     2 | loss: 20.9216255Losses:  18.57885356992483 5.166774749755859 5.467095375061035 1.5310821160674095
CurrentTrain: epoch  0, batch     3 | loss: 18.5788536Losses:  19.189190838485956 5.029666423797607 5.640252113342285 1.922020886093378
CurrentTrain: epoch  1, batch     0 | loss: 19.1891908Losses:  22.218846447765827 5.2619147300720215 5.872082710266113 5.070256359875202
CurrentTrain: epoch  1, batch     1 | loss: 22.2188464Losses:  16.77875518798828 5.129360198974609 5.749703884124756 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 16.7787552Losses:  17.37522029131651 4.999137878417969 5.396274566650391 1.8656339570879936
CurrentTrain: epoch  1, batch     3 | loss: 17.3752203Losses:  19.54418770968914 5.235628604888916 5.802140235900879 3.259523555636406
CurrentTrain: epoch  2, batch     0 | loss: 19.5441877Losses:  17.187493175268173 5.037454605102539 5.648001670837402 1.4257172048091888
CurrentTrain: epoch  2, batch     1 | loss: 17.1874932Losses:  29.506937265396118 5.02844762802124 5.718853950500488 13.216348886489868
CurrentTrain: epoch  2, batch     2 | loss: 29.5069373Losses:  16.564803063869476 4.81146240234375 5.141037940979004 1.6888808608055115
CurrentTrain: epoch  2, batch     3 | loss: 16.5648031Losses:  21.52248678356409 5.147129535675049 5.607377052307129 6.124913312494755
CurrentTrain: epoch  3, batch     0 | loss: 21.5224868Losses:  16.069246292114258 5.119678020477295 5.671416759490967 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 16.0692463Losses:  15.123270034790039 4.992720127105713 5.607637882232666 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 15.1232700Losses:  15.150831438601017 4.445025444030762 5.59232234954834 1.5189735665917397
CurrentTrain: epoch  3, batch     3 | loss: 15.1508314Losses:  17.470957696437836 5.055564880371094 5.581478595733643 1.6559152007102966
CurrentTrain: epoch  4, batch     0 | loss: 17.4709577Losses:  14.922797203063965 5.026615142822266 5.524303436279297 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 14.9227972Losses:  17.85301196947694 4.917932987213135 5.51658296585083 2.9835146702826023
CurrentTrain: epoch  4, batch     2 | loss: 17.8530120Losses:  15.140015479177237 5.134568214416504 5.0 1.5421332083642483
CurrentTrain: epoch  4, batch     3 | loss: 15.1400155Losses:  14.813699722290039 4.9458231925964355 5.239137649536133 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 14.8136997Losses:  19.584901846945286 5.106529235839844 5.956202983856201 4.389982260763645
CurrentTrain: epoch  5, batch     1 | loss: 19.5849018Losses:  16.573284178972244 4.8870086669921875 5.424129962921143 1.420484572649002
CurrentTrain: epoch  5, batch     2 | loss: 16.5732842Losses:  16.811613984405994 4.92958927154541 5.0 1.546643204987049
CurrentTrain: epoch  5, batch     3 | loss: 16.8116140Losses:  16.430543959140778 5.000025749206543 5.531050682067871 1.4071760773658752
CurrentTrain: epoch  6, batch     0 | loss: 16.4305440Losses:  16.437134508043528 4.831653118133545 5.392943859100342 1.8283431567251682
CurrentTrain: epoch  6, batch     1 | loss: 16.4371345Losses:  14.351446151733398 4.958198547363281 5.4334635734558105 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 14.3514462Losses:  16.05972572416067 4.883740425109863 5.0 1.4760970696806908
CurrentTrain: epoch  6, batch     3 | loss: 16.0597257Losses:  15.623592525720596 4.882007598876953 5.344742774963379 1.4936124384403229
CurrentTrain: epoch  7, batch     0 | loss: 15.6235925Losses:  20.170102328062057 5.067348480224609 5.579470157623291 5.0066501796245575
CurrentTrain: epoch  7, batch     1 | loss: 20.1701023Losses:  15.603349052369595 4.82522439956665 5.235208511352539 1.595267616212368
CurrentTrain: epoch  7, batch     2 | loss: 15.6033491Losses:  17.54460107535124 5.014665603637695 6.276180267333984 1.5062214061617851
CurrentTrain: epoch  7, batch     3 | loss: 17.5446011Losses:  15.398511819541454 4.853060245513916 5.389440536499023 1.4496478363871574
CurrentTrain: epoch  8, batch     0 | loss: 15.3985118Losses:  22.305113036185503 4.800607204437256 5.3243608474731445 7.803870398551226
CurrentTrain: epoch  8, batch     1 | loss: 22.3051130Losses:  25.7579935900867 5.031670570373535 5.3497796058654785 11.235631834715605
CurrentTrain: epoch  8, batch     2 | loss: 25.7579936Losses:  19.645950853824615 4.830807685852051 5.0 7.294220507144928
CurrentTrain: epoch  8, batch     3 | loss: 19.6459509Losses:  14.040345191955566 4.864542007446289 5.424904823303223 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 14.0403452Losses:  15.29310031607747 4.726276874542236 5.233659267425537 1.5204810686409473
CurrentTrain: epoch  9, batch     1 | loss: 15.2931003Losses:  17.107587430626154 4.936982154846191 5.348850250244141 2.9202295280992985
CurrentTrain: epoch  9, batch     2 | loss: 17.1075874Losses:  16.86028640717268 4.929657936096191 5.177641868591309 1.8127371594309807
CurrentTrain: epoch  9, batch     3 | loss: 16.8602864
Losses:  10.387706756591797 4.568592071533203 5.121097564697266 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 10.3877068Losses:  15.56947410106659 4.754461288452148 5.0 5.602293848991394
MemoryTrain:  epoch  0, batch     1 | loss: 15.5694741Losses:  10.125062942504883 4.538437843322754 5.008527755737305 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 10.1250629Losses:  16.319009572267532 4.719069004058838 5.2212934494018555 5.636548787355423
MemoryTrain:  epoch  1, batch     1 | loss: 16.3190096Losses:  9.732807159423828 4.529486656188965 5.008173942565918 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 9.7328072Losses:  15.5390538983047 4.714257717132568 5.0 5.6648912243545055
MemoryTrain:  epoch  2, batch     1 | loss: 15.5390539Losses:  9.598601341247559 4.511384963989258 5.001858711242676 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 9.5986013Losses:  15.61407383531332 4.7485222816467285 5.0 5.762778364121914
MemoryTrain:  epoch  3, batch     1 | loss: 15.6140738Losses:  9.573480606079102 4.513017654418945 5.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 9.5734806Losses:  15.457580126821995 4.678108215332031 5.0 5.720973528921604
MemoryTrain:  epoch  4, batch     1 | loss: 15.4575801Losses:  9.703398704528809 4.657963752746582 5.0 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 9.7033987Losses:  14.768362648785114 4.037003993988037 5.0 5.6678921058773994
MemoryTrain:  epoch  5, batch     1 | loss: 14.7683626Losses:  9.486934661865234 4.455709934234619 5.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 9.4869347Losses:  15.532816223800182 4.762166976928711 5.0 5.735842041671276
MemoryTrain:  epoch  6, batch     1 | loss: 15.5328162Losses:  9.424991607666016 4.3814849853515625 5.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 9.4249916Losses:  15.518661051988602 4.85487699508667 5.0 5.640119105577469
MemoryTrain:  epoch  7, batch     1 | loss: 15.5186611Losses:  9.447879791259766 4.381957054138184 5.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 9.4478798Losses:  15.52667048573494 4.837489128112793 5.0 5.656252890825272
MemoryTrain:  epoch  8, batch     1 | loss: 15.5266705Losses:  9.584630012512207 4.559420108795166 5.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 9.5846300Losses:  14.685025446116924 3.977339267730713 5.0 5.6677858754992485
MemoryTrain:  epoch  9, batch     1 | loss: 14.6850254
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 57.54%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 55.83%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 54.23%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 54.69%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 55.68%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 56.43%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 63.18%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 62.63%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 62.86%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 63.86%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 63.79%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 63.24%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 62.91%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 62.90%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.20%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.86%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.05%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.45%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 92.19%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 91.25%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 90.15%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 89.65%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 88.69%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 88.32%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.30%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.38%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.68%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 88.73%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 88.72%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 88.78%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 88.98%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.04%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 88.41%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.03%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 87.57%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 87.28%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 86.92%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 86.21%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 85.65%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 84.76%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.82%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 82.97%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 82.13%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 81.32%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 80.65%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 80.93%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.19%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 81.31%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.49%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 81.31%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 81.19%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 80.91%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 80.45%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 80.35%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 80.13%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.92%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 79.82%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 79.89%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.96%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.99%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 79.58%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 79.34%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 79.00%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 78.76%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 78.63%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 78.40%   
cur_acc:  ['0.9395', '0.6220']
his_acc:  ['0.9395', '0.7840']
Clustering into  14  clusters
Clusters:  [11  2  8 13  1  7 12  5  3  3  4  2  6  2  4  2  2  2  6  2  0  5  2  1
 10  0  9  4  2  2]
Losses:  21.75104385241866 5.245628356933594 5.939253807067871 4.763708647340536
CurrentTrain: epoch  0, batch     0 | loss: 21.7510439Losses:  16.440195083618164 5.182596206665039 5.693778991699219 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 16.4401951Losses:  32.111133728176355 5.0233964920043945 6.0801777839660645 15.464315567165613
CurrentTrain: epoch  0, batch     2 | loss: 32.1111337Losses:  18.48325116932392 5.017242431640625 5.849784851074219 1.4805141240358353
CurrentTrain: epoch  0, batch     3 | loss: 18.4832512Losses:  27.698077905923128 5.221521377563477 5.598523139953613 11.747088182717562
CurrentTrain: epoch  1, batch     0 | loss: 27.6980779Losses:  17.389866646379232 5.121411323547363 5.904073715209961 1.494684036821127
CurrentTrain: epoch  1, batch     1 | loss: 17.3898666Losses:  16.694292783737183 4.915609836578369 5.51737642288208 1.4181773662567139
CurrentTrain: epoch  1, batch     2 | loss: 16.6942928Losses:  14.786471486091614 5.0520734786987305 4.845269203186035 1.416850209236145
CurrentTrain: epoch  1, batch     3 | loss: 14.7864715Losses:  19.19400615245104 5.042070388793945 5.451621055603027 4.272326655685902
CurrentTrain: epoch  2, batch     0 | loss: 19.1940062Losses:  18.772211775183678 5.139983177185059 5.954873085021973 2.9347741454839706
CurrentTrain: epoch  2, batch     1 | loss: 18.7722118Losses:  17.6497126147151 5.056963920593262 5.630892753601074 3.1257610842585564
CurrentTrain: epoch  2, batch     2 | loss: 17.6497126Losses:  15.860554918646812 4.932696342468262 5.3726959228515625 1.4639227241277695
CurrentTrain: epoch  2, batch     3 | loss: 15.8605549Losses:  18.048040475696325 4.970531463623047 5.615665435791016 2.9835129640996456
CurrentTrain: epoch  3, batch     0 | loss: 18.0480405Losses:  14.654191970825195 4.97237491607666 5.566807746887207 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 14.6541920Losses:  17.91917822882533 5.164667129516602 5.639102458953857 3.0761339478194714
CurrentTrain: epoch  3, batch     2 | loss: 17.9191782Losses:  13.654335737228394 4.838354110717773 4.848781585693359 1.430204153060913
CurrentTrain: epoch  3, batch     3 | loss: 13.6543357Losses:  14.6700439453125 4.950679779052734 5.621452331542969 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 14.6700439Losses:  17.32640114426613 5.033329963684082 5.297395706176758 2.8341640532016754
CurrentTrain: epoch  4, batch     1 | loss: 17.3264011Losses:  15.61302886530757 5.027710914611816 5.6750030517578125 1.5226243548095226
CurrentTrain: epoch  4, batch     2 | loss: 15.6130289Losses:  14.562749676406384 5.400260925292969 5.225939750671387 1.5056436583399773
CurrentTrain: epoch  4, batch     3 | loss: 14.5627497Losses:  15.265040844678879 5.013092994689941 5.5526275634765625 1.459792584180832
CurrentTrain: epoch  5, batch     0 | loss: 15.2650408Losses:  15.248672220855951 5.0099382400512695 5.4664435386657715 1.5217721201479435
CurrentTrain: epoch  5, batch     1 | loss: 15.2486722Losses:  15.613769888877869 4.890716552734375 5.508594036102295 1.4101299047470093
CurrentTrain: epoch  5, batch     2 | loss: 15.6137699Losses:  15.962583467364311 5.352309226989746 5.841382026672363 1.643488809466362
CurrentTrain: epoch  5, batch     3 | loss: 15.9625835#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  16.49917740188539 1.3207916021347046 1.3458771705627441 5.216895526275039
CurrentTrain: epoch  0, batch     0 | loss: 16.4991774Losses:  17.20268078893423 1.533327341079712 1.7487068176269531 5.836301051080227
CurrentTrain: epoch  0, batch     1 | loss: 17.2026808Losses:  15.89467804133892 1.487281084060669 1.4986112117767334 4.403202936053276
CurrentTrain: epoch  0, batch     2 | loss: 15.8946780Losses:  15.916874235495925 1.4154855012893677 1.6550822257995605 5.159491842612624
CurrentTrain: epoch  0, batch     3 | loss: 15.9168742Losses:  14.1779810115695 1.5333280563354492 1.585564374923706 2.524019829928875
CurrentTrain: epoch  0, batch     4 | loss: 14.1779810Losses:  14.569077752530575 1.4150830507278442 1.4812867641448975 4.3980190977454185
CurrentTrain: epoch  0, batch     5 | loss: 14.5690778Losses:  19.43271929398179 1.4697294235229492 1.4686167240142822 8.299023691564798
CurrentTrain: epoch  0, batch     6 | loss: 19.4327193Losses:  15.307086892426014 1.6380674839019775 1.7006710767745972 3.8571719601750374
CurrentTrain: epoch  0, batch     7 | loss: 15.3070869Losses:  22.352724075317383 1.57745361328125 1.4686627388000488 10.497745513916016
CurrentTrain: epoch  0, batch     8 | loss: 22.3527241Losses:  13.325421422719955 1.4296562671661377 1.544332504272461 2.891819089651108
CurrentTrain: epoch  0, batch     9 | loss: 13.3254214Losses:  14.502064134925604 1.503023386001587 1.5731306076049805 3.8528255484998226
CurrentTrain: epoch  0, batch    10 | loss: 14.5020641Losses:  14.37367394566536 1.4507566690444946 1.296718716621399 3.062284976243973
CurrentTrain: epoch  0, batch    11 | loss: 14.3736739Losses:  11.621031761169434 1.5358223915100098 1.5478073358535767 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 11.6210318Losses:  11.541261672973633 1.6040494441986084 1.5844755172729492 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 11.5412617Losses:  16.91633567586541 1.5060491561889648 1.4284179210662842 5.370505902916193
CurrentTrain: epoch  0, batch    14 | loss: 16.9163357Losses:  15.479563312605023 1.2569204568862915 1.2902894020080566 5.04369886405766
CurrentTrain: epoch  0, batch    15 | loss: 15.4795633Losses:  14.272284157574177 1.1323463916778564 1.3027175664901733 4.009559281170368
CurrentTrain: epoch  0, batch    16 | loss: 14.2722842Losses:  14.611245162785053 1.4137521982192993 1.328914761543274 3.8267746046185493
CurrentTrain: epoch  0, batch    17 | loss: 14.6112452Losses:  16.232478842139244 1.3730385303497314 1.1121057271957397 5.070417150855064
CurrentTrain: epoch  0, batch    18 | loss: 16.2324788Losses:  12.25148692727089 1.416040301322937 1.24461829662323 1.4323751032352448
CurrentTrain: epoch  0, batch    19 | loss: 12.2514869Losses:  13.821371577680111 1.342105746269226 1.302758812904358 4.267099879682064
CurrentTrain: epoch  0, batch    20 | loss: 13.8213716Losses:  14.830851960927248 1.2718918323516846 1.013612985610962 5.038415361195803
CurrentTrain: epoch  0, batch    21 | loss: 14.8308520Losses:  19.15302750468254 1.4015769958496094 1.4903185367584229 8.78389260172844
CurrentTrain: epoch  0, batch    22 | loss: 19.1530275Losses:  12.358918074518442 1.4056622982025146 1.4272955656051636 1.4455393590033054
CurrentTrain: epoch  0, batch    23 | loss: 12.3589181Losses:  16.09591058641672 1.2915797233581543 1.3821156024932861 5.736842669546604
CurrentTrain: epoch  0, batch    24 | loss: 16.0959106Losses:  10.651081085205078 1.4662606716156006 1.3848434686660767 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 10.6510811Losses:  10.249872207641602 1.5097407102584839 1.2650041580200195 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 10.2498722Losses:  15.056220568716526 1.283478021621704 1.2141642570495605 4.730676211416721
CurrentTrain: epoch  0, batch    27 | loss: 15.0562206Losses:  14.33836323954165 1.398980736732483 1.1135945320129395 4.515053341165185
CurrentTrain: epoch  0, batch    28 | loss: 14.3383632Losses:  10.374035835266113 1.5497565269470215 1.4149061441421509 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 10.3740358Losses:  9.157381057739258 1.3805556297302246 1.246152400970459 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 9.1573811Losses:  13.924585282802582 1.0763881206512451 1.069801688194275 5.100192964076996
CurrentTrain: epoch  0, batch    31 | loss: 13.9245853Losses:  9.533695220947266 1.2713803052902222 1.1934046745300293 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 9.5336952Losses:  9.952558428049088 1.255538821220398 1.185429334640503 1.4049128592014313
CurrentTrain: epoch  0, batch    33 | loss: 9.9525584Losses:  13.29386280849576 1.4546798467636108 1.1493017673492432 3.2076678164303303
CurrentTrain: epoch  0, batch    34 | loss: 13.2938628Losses:  13.570253800600767 1.4607726335525513 1.246740460395813 3.334969948977232
CurrentTrain: epoch  0, batch    35 | loss: 13.5702538Losses:  11.254455029964447 1.1136177778244019 1.1035993099212646 1.7400097250938416
CurrentTrain: epoch  0, batch    36 | loss: 11.2544550Losses:  10.853746116161346 1.2487869262695312 1.1235698461532593 1.424128234386444
CurrentTrain: epoch  0, batch    37 | loss: 10.8537461Losses:  9.178192138671875 1.500920057296753 1.2716221809387207 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 9.1781921Losses:  15.566372631117702 1.5227994918823242 1.0773528814315796 5.207954166457057
CurrentTrain: epoch  0, batch    39 | loss: 15.5663726Losses:  17.0037674754858 1.4797537326812744 1.2789785861968994 7.294074520468712
CurrentTrain: epoch  0, batch    40 | loss: 17.0037675Losses:  16.232230946421623 1.2688511610031128 1.0481401681900024 5.920465275645256
CurrentTrain: epoch  0, batch    41 | loss: 16.2322309Losses:  13.836923331022263 1.4161452054977417 1.3336660861968994 3.7932173907756805
CurrentTrain: epoch  0, batch    42 | loss: 13.8369233Losses:  15.903386352583766 1.2610247135162354 1.0494492053985596 6.702279327437282
CurrentTrain: epoch  0, batch    43 | loss: 15.9033864Losses:  11.280380122363567 1.3627221584320068 1.09707510471344 1.8289478942751884
CurrentTrain: epoch  0, batch    44 | loss: 11.2803801Losses:  9.030211448669434 1.3201239109039307 1.1931947469711304 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 9.0302114Losses:  14.579201690852642 1.3449310064315796 1.1400588750839233 4.842401497066021
CurrentTrain: epoch  0, batch    46 | loss: 14.5792017Losses:  14.75012719631195 1.3291209936141968 1.1565736532211304 4.927374243736267
CurrentTrain: epoch  0, batch    47 | loss: 14.7501272Losses:  12.35112082818523 1.3914743661880493 1.4046088457107544 2.5114935622550547
CurrentTrain: epoch  0, batch    48 | loss: 12.3511208Losses:  11.64604852348566 1.4743725061416626 1.220865249633789 1.5263633504509926
CurrentTrain: epoch  0, batch    49 | loss: 11.6460485Losses:  18.346276983618736 1.4165523052215576 1.22598135471344 8.592239126563072
CurrentTrain: epoch  0, batch    50 | loss: 18.3462770Losses:  12.928761947900057 1.2538807392120361 1.1662994623184204 4.650426376610994
CurrentTrain: epoch  0, batch    51 | loss: 12.9287619Losses:  10.418972969055176 1.5726351737976074 1.2779752016067505 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 10.4189730Losses:  14.61031356966123 1.0401371717453003 1.0051817893981934 5.400839959736913
CurrentTrain: epoch  0, batch    53 | loss: 14.6103136Losses:  16.195539988577366 1.0929374694824219 1.0157791376113892 7.369308985769749
CurrentTrain: epoch  0, batch    54 | loss: 16.1955400Losses:  10.191818904131651 1.3595696687698364 1.135431170463562 1.5428435318171978
CurrentTrain: epoch  0, batch    55 | loss: 10.1918189Losses:  21.708498939871788 1.1656861305236816 1.0324937105178833 12.2225551456213
CurrentTrain: epoch  0, batch    56 | loss: 21.7084989Losses:  11.5154770873487 1.1603187322616577 1.0943737030029297 2.984995748847723
CurrentTrain: epoch  0, batch    57 | loss: 11.5154771Losses:  12.768418740481138 1.3552796840667725 1.2129188776016235 3.765950631350279
CurrentTrain: epoch  0, batch    58 | loss: 12.7684187Losses:  10.809393592178822 1.2993147373199463 1.2024357318878174 1.5143114998936653
CurrentTrain: epoch  0, batch    59 | loss: 10.8093936Losses:  24.346875347197056 1.1387494802474976 1.0209544897079468 16.008005298674107
CurrentTrain: epoch  0, batch    60 | loss: 24.3468753Losses:  12.78988915681839 1.1575114727020264 1.0094013214111328 4.619276821613312
CurrentTrain: epoch  0, batch    61 | loss: 12.7898892Losses:  11.115606516599655 1.257122278213501 1.3858845233917236 1.6308796107769012
CurrentTrain: epoch  0, batch    62 | loss: 11.1156065Losses:  9.402813911437988 1.4918158054351807 1.0812088251113892 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 9.4028139Losses:  12.776963703799993 1.2814913988113403 1.2568440437316895 4.156757824588567
CurrentTrain: epoch  1, batch     1 | loss: 12.7769637Losses:  15.21347102150321 1.448330283164978 1.2131954431533813 6.625510778278112
CurrentTrain: epoch  1, batch     2 | loss: 15.2134710Losses:  10.211009863764048 1.2732698917388916 1.2243714332580566 1.4760760106146336
CurrentTrain: epoch  1, batch     3 | loss: 10.2110099Losses:  11.128596562892199 1.4594980478286743 1.3298563957214355 1.8540156073868275
CurrentTrain: epoch  1, batch     4 | loss: 11.1285966Losses:  9.135806530714035 1.1043962240219116 1.1343103647232056 1.401830643415451
CurrentTrain: epoch  1, batch     5 | loss: 9.1358065Losses:  8.926135063171387 1.1896181106567383 1.1069968938827515 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 8.9261351Losses:  10.537317678332329 1.1438531875610352 1.0295822620391846 1.7134956568479538
CurrentTrain: epoch  1, batch     7 | loss: 10.5373177Losses:  9.982436703518033 1.240085482597351 1.038088321685791 1.5672631729394197
CurrentTrain: epoch  1, batch     8 | loss: 9.9824367Losses:  14.779167398810387 1.3391839265823364 1.1009424924850464 6.612956270575523
CurrentTrain: epoch  1, batch     9 | loss: 14.7791674Losses:  14.337011501193047 1.2247636318206787 0.9029319286346436 5.132699176669121
CurrentTrain: epoch  1, batch    10 | loss: 14.3370115Losses:  11.745783504098654 1.2358946800231934 1.155219554901123 3.1810013614594936
CurrentTrain: epoch  1, batch    11 | loss: 11.7457835Losses:  11.200794585049152 1.098410964012146 1.0074656009674072 2.845841772854328
CurrentTrain: epoch  1, batch    12 | loss: 11.2007946Losses:  8.097565650939941 1.1369857788085938 1.0821826457977295 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 8.0975657Losses:  12.94628233090043 1.3016778230667114 0.9496756792068481 3.63598245754838
CurrentTrain: epoch  1, batch    14 | loss: 12.9462823Losses:  8.55064868927002 1.1937471628189087 1.0357123613357544 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 8.5506487Losses:  12.419788219034672 1.1271412372589111 0.9823874235153198 3.0988625064492226
CurrentTrain: epoch  1, batch    16 | loss: 12.4197882Losses:  13.702938683331013 1.1382163763046265 0.996860146522522 4.388242371380329
CurrentTrain: epoch  1, batch    17 | loss: 13.7029387Losses:  14.166942993178964 1.0329453945159912 1.1626605987548828 6.02270738221705
CurrentTrain: epoch  1, batch    18 | loss: 14.1669430Losses:  8.564180374145508 1.24186110496521 1.0741033554077148 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 8.5641804Losses:  8.228386878967285 1.0500677824020386 1.0841821432113647 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 8.2283869Losses:  18.909685470163822 1.3099883794784546 1.0616246461868286 9.700089789927006
CurrentTrain: epoch  1, batch    21 | loss: 18.9096855Losses:  8.783097267150879 1.1989774703979492 1.1738864183425903 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 8.7830973Losses:  8.012227058410645 1.1924705505371094 1.1074153184890747 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 8.0122271Losses:  11.401820596307516 1.078618049621582 1.0857993364334106 4.349089082330465
CurrentTrain: epoch  1, batch    24 | loss: 11.4018206Losses:  10.047603406012058 1.1116498708724976 1.1216459274291992 2.867476262152195
CurrentTrain: epoch  1, batch    25 | loss: 10.0476034Losses:  8.85537338256836 1.1608220338821411 1.0131149291992188 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 8.8553734Losses:  8.337143898010254 1.2304599285125732 1.051635503768921 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 8.3371439Losses:  8.637471199035645 1.2255306243896484 1.1070590019226074 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 8.6374712Losses:  12.39802223071456 1.0979918241500854 1.0582258701324463 4.324443396180868
CurrentTrain: epoch  1, batch    29 | loss: 12.3980222Losses:  12.471622355282307 1.3200119733810425 1.12044095993042 3.4904583767056465
CurrentTrain: epoch  1, batch    30 | loss: 12.4716224Losses:  12.189719825983047 1.2242175340652466 1.1398996114730835 3.2354189455509186
CurrentTrain: epoch  1, batch    31 | loss: 12.1897198Losses:  12.708878934383392 1.173412799835205 1.1206859350204468 4.057754933834076
CurrentTrain: epoch  1, batch    32 | loss: 12.7088789Losses:  7.751047134399414 1.0960613489151 0.9637285470962524 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 7.7510471Losses:  7.555199146270752 1.114851951599121 1.105473279953003 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 7.5551991Losses:  15.854880638420582 1.4843347072601318 1.196643590927124 6.733539886772633
CurrentTrain: epoch  1, batch    35 | loss: 15.8548806Losses:  10.243804328143597 1.2390331029891968 0.9947541952133179 1.6508315727114677
CurrentTrain: epoch  1, batch    36 | loss: 10.2438043Losses:  8.95159813016653 0.9477198123931885 1.0525240898132324 1.6831884011626244
CurrentTrain: epoch  1, batch    37 | loss: 8.9515981Losses:  10.360302221029997 1.063696265220642 1.0742512941360474 2.98962427303195
CurrentTrain: epoch  1, batch    38 | loss: 10.3603022Losses:  12.020495802164078 1.1634889841079712 1.0686523914337158 4.634878545999527
CurrentTrain: epoch  1, batch    39 | loss: 12.0204958Losses:  9.307996068149805 1.1498245000839233 1.0989991426467896 1.4581244327127934
CurrentTrain: epoch  1, batch    40 | loss: 9.3079961Losses:  10.558028094470501 1.097692847251892 1.0270276069641113 2.9699820205569267
CurrentTrain: epoch  1, batch    41 | loss: 10.5580281Losses:  9.913703374564648 1.2103521823883057 1.0191209316253662 1.8520035073161125
CurrentTrain: epoch  1, batch    42 | loss: 9.9137034Losses:  12.579848311841488 1.160556435585022 1.0979704856872559 4.608670257031918
CurrentTrain: epoch  1, batch    43 | loss: 12.5798483Losses:  12.850870843976736 1.2159860134124756 1.1964077949523926 3.8398225270211697
CurrentTrain: epoch  1, batch    44 | loss: 12.8508708Losses:  11.510332234203815 1.2716293334960938 1.1015149354934692 2.9892073944211006
CurrentTrain: epoch  1, batch    45 | loss: 11.5103322Losses:  8.84170150756836 1.3778228759765625 1.1083807945251465 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 8.8417015Losses:  8.220490455627441 1.277279257774353 1.025927186012268 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 8.2204905Losses:  9.713452190160751 1.0998419523239136 0.9100284576416016 1.4146011769771576
CurrentTrain: epoch  1, batch    48 | loss: 9.7134522Losses:  17.803510032594204 1.175018310546875 1.0003342628479004 9.302751861512661
CurrentTrain: epoch  1, batch    49 | loss: 17.8035100Losses:  9.831092957407236 1.059654712677002 1.0631510019302368 1.4304915703833103
CurrentTrain: epoch  1, batch    50 | loss: 9.8310930Losses:  8.29103946685791 1.3416298627853394 1.118890643119812 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 8.2910395Losses:  14.482446778565645 1.2151538133621216 1.115012764930725 6.495209325104952
CurrentTrain: epoch  1, batch    52 | loss: 14.4824468Losses:  7.202767848968506 1.179439663887024 1.0388630628585815 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 7.2027678Losses:  10.573806889355183 0.9695360660552979 0.9970442056655884 3.1366592720150948
CurrentTrain: epoch  1, batch    54 | loss: 10.5738069Losses:  10.45096905529499 1.0695122480392456 1.0781193971633911 2.9269331246614456
CurrentTrain: epoch  1, batch    55 | loss: 10.4509691Losses:  8.093917846679688 1.2218073606491089 1.0741263628005981 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 8.0939178Losses:  8.397969245910645 1.205464482307434 1.0251151323318481 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 8.3979692Losses:  7.52659797668457 1.1075385808944702 1.1701736450195312 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 7.5265980Losses:  11.699905056506395 1.1932233572006226 1.093550205230713 3.2016521878540516
CurrentTrain: epoch  1, batch    59 | loss: 11.6999051Losses:  7.691080570220947 1.2366336584091187 1.0932551622390747 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 7.6910806Losses:  9.321584701538086 1.3101718425750732 1.131872534751892 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 9.3215847Losses:  10.253158386796713 1.0861093997955322 1.0 4.3298905454576015
CurrentTrain: epoch  1, batch    62 | loss: 10.2531584Losses:  6.59492826461792 1.0629370212554932 1.1034765243530273 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 6.5949283Losses:  7.413619518280029 1.152626633644104 1.0908931493759155 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 7.4136195Losses:  8.139659851789474 0.9822078943252563 1.0042681694030762 1.417206734418869
CurrentTrain: epoch  2, batch     2 | loss: 8.1396599Losses:  12.849786184728146 1.3308250904083252 1.2378343343734741 3.2073043808341026
CurrentTrain: epoch  2, batch     3 | loss: 12.8497862Losses:  10.006894160062075 1.1652568578720093 0.9731559753417969 1.517789889127016
CurrentTrain: epoch  2, batch     4 | loss: 10.0068942Losses:  10.034614264965057 1.0850155353546143 1.0040886402130127 2.0487882494926453
CurrentTrain: epoch  2, batch     5 | loss: 10.0346143Losses:  7.710391044616699 1.279541254043579 1.1097829341888428 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 7.7103910Losses:  12.618971671909094 1.1391613483428955 1.079055905342102 4.90019353851676
CurrentTrain: epoch  2, batch     7 | loss: 12.6189717Losses:  10.839249327778816 1.0880142450332642 1.0197007656097412 3.372343733906746
CurrentTrain: epoch  2, batch     8 | loss: 10.8392493Losses:  11.843391198664904 1.2423570156097412 1.0208112001419067 3.7469737716019154
CurrentTrain: epoch  2, batch     9 | loss: 11.8433912Losses:  8.816765636205673 1.0239452123641968 0.9849019050598145 1.5026253163814545
CurrentTrain: epoch  2, batch    10 | loss: 8.8167656Losses:  8.247001107782125 0.9010213613510132 1.0 1.5036329589784145
CurrentTrain: epoch  2, batch    11 | loss: 8.2470011Losses:  13.736327469348907 1.237288475036621 1.0296940803527832 5.95490962266922
CurrentTrain: epoch  2, batch    12 | loss: 13.7363275Losses:  9.045271597802639 1.0678240060806274 1.106372356414795 1.4766156300902367
CurrentTrain: epoch  2, batch    13 | loss: 9.0452716Losses:  11.342448152601719 1.3069995641708374 1.1006611585617065 2.9172696247696877
CurrentTrain: epoch  2, batch    14 | loss: 11.3424482Losses:  9.696890277788043 1.095080018043518 0.904181957244873 2.1892332267016172
CurrentTrain: epoch  2, batch    15 | loss: 9.6968903Losses:  10.612187806516886 1.0794079303741455 0.9911969900131226 2.8570818342268467
CurrentTrain: epoch  2, batch    16 | loss: 10.6121878Losses:  10.46511422470212 1.1177657842636108 1.0 2.9152618534862995
CurrentTrain: epoch  2, batch    17 | loss: 10.4651142Losses:  17.59407138079405 1.0062496662139893 1.0 11.125378124415874
CurrentTrain: epoch  2, batch    18 | loss: 17.5940714Losses:  9.884901417419314 1.1782910823822021 0.9889962673187256 1.8353771101683378
CurrentTrain: epoch  2, batch    19 | loss: 9.8849014Losses:  6.917139530181885 1.098282814025879 1.004672646522522 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 6.9171395Losses:  7.571732044219971 1.1382040977478027 1.0109721422195435 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 7.5717320Losses:  11.78738298267126 1.2011125087738037 0.9514064788818359 4.349120520055294
CurrentTrain: epoch  2, batch    22 | loss: 11.7873830Losses:  9.481181681156158 0.9396259784698486 0.9862241744995117 1.5688582062721252
CurrentTrain: epoch  2, batch    23 | loss: 9.4811817Losses:  8.0109041929245 1.0039242506027222 1.0456600189208984 1.464880347251892
CurrentTrain: epoch  2, batch    24 | loss: 8.0109042Losses:  12.002265676856041 1.2155841588974 1.0 4.700551733374596
CurrentTrain: epoch  2, batch    25 | loss: 12.0022657Losses:  8.945825308561325 1.035591721534729 0.9755791425704956 1.4132072627544403
CurrentTrain: epoch  2, batch    26 | loss: 8.9458253Losses:  8.958109110593796 0.9570412635803223 0.9801665544509888 1.8141648471355438
CurrentTrain: epoch  2, batch    27 | loss: 8.9581091Losses:  8.552758578211069 1.07712721824646 1.0577422380447388 1.4127863682806492
CurrentTrain: epoch  2, batch    28 | loss: 8.5527586Losses:  18.964805103838444 1.211785912513733 0.9607100486755371 11.774903751909733
CurrentTrain: epoch  2, batch    29 | loss: 18.9648051Losses:  8.7418472468853 1.153030276298523 1.0471954345703125 1.4511577785015106
CurrentTrain: epoch  2, batch    30 | loss: 8.7418472Losses:  8.303547695279121 0.9802860021591187 1.0 1.497140720486641
CurrentTrain: epoch  2, batch    31 | loss: 8.3035477Losses:  9.783537652343512 0.9211448431015015 0.9912823438644409 2.927764680236578
CurrentTrain: epoch  2, batch    32 | loss: 9.7835377Losses:  10.257865317165852 1.1001473665237427 1.031846284866333 2.0511878803372383
CurrentTrain: epoch  2, batch    33 | loss: 10.2578653Losses:  12.91807321459055 1.1685311794281006 1.052923321723938 5.65043929964304
CurrentTrain: epoch  2, batch    34 | loss: 12.9180732Losses:  14.125251032412052 1.093150019645691 0.9517797231674194 5.908819414675236
CurrentTrain: epoch  2, batch    35 | loss: 14.1252510Losses:  10.208646476268768 0.9244735240936279 1.0087014436721802 2.861427962779999
CurrentTrain: epoch  2, batch    36 | loss: 10.2086465Losses:  9.710960552096367 1.0499215126037598 1.020886778831482 1.8188788145780563
CurrentTrain: epoch  2, batch    37 | loss: 9.7109606Losses:  8.619153589010239 1.0875179767608643 1.025022268295288 1.4074011743068695
CurrentTrain: epoch  2, batch    38 | loss: 8.6191536Losses:  11.178846396505833 1.0525344610214233 0.9829636812210083 3.2982759848237038
CurrentTrain: epoch  2, batch    39 | loss: 11.1788464Losses:  11.163292422890663 0.9198993444442749 0.9856957197189331 4.411009803414345
CurrentTrain: epoch  2, batch    40 | loss: 11.1632924Losses:  10.070363733917475 1.0866650342941284 1.1048201322555542 1.4614340998232365
CurrentTrain: epoch  2, batch    41 | loss: 10.0703637Losses:  8.308357119560242 0.859613299369812 1.0027012825012207 1.4072502851486206
CurrentTrain: epoch  2, batch    42 | loss: 8.3083571Losses:  7.6606292724609375 1.1778888702392578 1.0031774044036865 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 7.6606293Losses:  8.103521820157766 0.8868266344070435 1.0052961111068726 1.4474201165139675
CurrentTrain: epoch  2, batch    44 | loss: 8.1035218Losses:  10.47095699608326 1.162631630897522 1.0585780143737793 3.034071162343025
CurrentTrain: epoch  2, batch    45 | loss: 10.4709570Losses:  6.840373992919922 0.9128375053405762 1.0113091468811035 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 6.8403740Losses:  7.461309909820557 1.1002237796783447 1.0183874368667603 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 7.4613099Losses:  6.708901882171631 1.1272060871124268 1.007404088973999 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 6.7089019Losses:  9.197085496038198 1.1551111936569214 0.961212158203125 1.4491997919976711
CurrentTrain: epoch  2, batch    49 | loss: 9.1970855Losses:  16.213432498276234 1.0286282300949097 1.0 8.770800299942493
CurrentTrain: epoch  2, batch    50 | loss: 16.2134325Losses:  10.089997934177518 1.0750457048416138 1.0323628187179565 3.4662291277199984
CurrentTrain: epoch  2, batch    51 | loss: 10.0899979Losses:  8.255110755562782 0.8525923490524292 0.9943437576293945 1.659653201699257
CurrentTrain: epoch  2, batch    52 | loss: 8.2551108Losses:  9.638208031654358 0.9167923927307129 1.0051419734954834 2.837147355079651
CurrentTrain: epoch  2, batch    53 | loss: 9.6382080Losses:  8.712932106107473 0.9384521245956421 1.0199434757232666 1.4306678734719753
CurrentTrain: epoch  2, batch    54 | loss: 8.7129321Losses:  9.526191782206297 1.0835812091827393 0.9918719530105591 2.974234651774168
CurrentTrain: epoch  2, batch    55 | loss: 9.5261918Losses:  8.554877595975995 0.9743223190307617 1.046049952507019 1.7957747746258974
CurrentTrain: epoch  2, batch    56 | loss: 8.5548776Losses:  7.571381092071533 1.0297677516937256 0.9814145565032959 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 7.5713811Losses:  7.086509704589844 0.9387274980545044 0.9920291900634766 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 7.0865097Losses:  18.509953290224075 1.1063109636306763 0.9879388809204102 11.744730740785599
CurrentTrain: epoch  2, batch    59 | loss: 18.5099533Losses:  7.066859245300293 0.9860565662384033 0.9841837882995605 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 7.0668592Losses:  10.118036702275276 1.1565355062484741 1.0784510374069214 1.9555210620164871
CurrentTrain: epoch  2, batch    61 | loss: 10.1180367Losses:  6.7333245277404785 0.9725077152252197 0.9992797374725342 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 6.7333245Losses:  8.076724175363779 1.1425864696502686 1.0 1.4069129265844822
CurrentTrain: epoch  3, batch     0 | loss: 8.0767242Losses:  8.165889501571655 1.0069286823272705 0.988041877746582 1.4121973514556885
CurrentTrain: epoch  3, batch     1 | loss: 8.1658895Losses:  7.074280738830566 0.9606627225875854 0.9451577663421631 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 7.0742807Losses:  11.455251161009073 1.209484577178955 0.9918105602264404 4.337795201689005
CurrentTrain: epoch  3, batch     3 | loss: 11.4552512Losses:  9.538403395563364 0.8594739437103271 1.011370301246643 3.4173792637884617
CurrentTrain: epoch  3, batch     4 | loss: 9.5384034Losses:  11.546863961964846 0.9466781616210938 0.9811817407608032 4.317107606679201
CurrentTrain: epoch  3, batch     5 | loss: 11.5468640Losses:  8.192956864833832 0.9575269222259521 1.0102038383483887 1.3984994292259216
CurrentTrain: epoch  3, batch     6 | loss: 8.1929569Losses:  11.057540372014046 1.010802149772644 1.0261790752410889 4.255520775914192
CurrentTrain: epoch  3, batch     7 | loss: 11.0575404Losses:  8.042944751679897 0.9702664613723755 0.9964902400970459 1.465631328523159
CurrentTrain: epoch  3, batch     8 | loss: 8.0429448Losses:  7.306264877319336 1.0142050981521606 1.0331906080245972 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 7.3062649Losses:  6.841905117034912 1.0618358850479126 0.9679361581802368 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 6.8419051Losses:  9.644188605248928 1.0557615756988525 1.0 2.922684393823147
CurrentTrain: epoch  3, batch    11 | loss: 9.6441886Losses:  10.831426739692688 1.170230507850647 1.0588345527648926 4.219674229621887
CurrentTrain: epoch  3, batch    12 | loss: 10.8314267Losses:  10.990888874977827 1.2412245273590088 1.0030211210250854 2.9078467302024364
CurrentTrain: epoch  3, batch    13 | loss: 10.9908889Losses:  17.8841502815485 1.0377901792526245 1.0 10.611151471734047
CurrentTrain: epoch  3, batch    14 | loss: 17.8841503Losses:  8.703788373619318 1.1603494882583618 1.108696699142456 1.415474507957697
CurrentTrain: epoch  3, batch    15 | loss: 8.7037884Losses:  13.249571949243546 0.9888650178909302 1.0135797262191772 6.094038635492325
CurrentTrain: epoch  3, batch    16 | loss: 13.2495719Losses:  7.936940938234329 1.0693564414978027 1.0530369281768799 1.4516251385211945
CurrentTrain: epoch  3, batch    17 | loss: 7.9369409Losses:  10.813694320619106 0.7779724597930908 0.9608296155929565 4.4044307097792625
CurrentTrain: epoch  3, batch    18 | loss: 10.8136943Losses:  7.419179458171129 0.849246621131897 1.0 1.5471730418503284
CurrentTrain: epoch  3, batch    19 | loss: 7.4191795Losses:  10.25676778703928 0.9437439441680908 0.9894181489944458 4.2391910925507545
CurrentTrain: epoch  3, batch    20 | loss: 10.2567678Losses:  9.817848965525627 0.700201153755188 0.9852199554443359 3.4148252457380295
CurrentTrain: epoch  3, batch    21 | loss: 9.8178490Losses:  13.515006959438324 1.1343588829040527 1.0499181747436523 6.208849370479584
CurrentTrain: epoch  3, batch    22 | loss: 13.5150070Losses:  8.689134389162064 0.9217820167541504 1.059646487236023 1.4465568363666534
CurrentTrain: epoch  3, batch    23 | loss: 8.6891344Losses:  8.680354461073875 1.038925051689148 1.053490161895752 1.4662440866231918
CurrentTrain: epoch  3, batch    24 | loss: 8.6803545Losses:  10.242870885878801 1.0620348453521729 0.9997020959854126 2.92059763148427
CurrentTrain: epoch  3, batch    25 | loss: 10.2428709Losses:  8.882775891572237 0.8983210325241089 1.0 2.908328641206026
CurrentTrain: epoch  3, batch    26 | loss: 8.8827759Losses:  7.421871997416019 0.8184107542037964 1.0076571702957153 1.4558490291237831
CurrentTrain: epoch  3, batch    27 | loss: 7.4218720Losses:  12.104829274117947 0.9886734485626221 1.03171706199646 4.4334001168608665
CurrentTrain: epoch  3, batch    28 | loss: 12.1048293Losses:  12.142485070973635 0.9558982849121094 0.952391505241394 5.716304708272219
CurrentTrain: epoch  3, batch    29 | loss: 12.1424851Losses:  6.391660690307617 0.9201258420944214 1.0216717720031738 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 6.3916607Losses:  6.263707637786865 0.8929581642150879 0.9768955707550049 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 6.2637076Losses:  12.047329507768154 0.8135857582092285 0.9955607652664185 5.767733655869961
CurrentTrain: epoch  3, batch    32 | loss: 12.0473295Losses:  9.709342800080776 1.0043846368789673 0.95960533618927 2.9566129073500633
CurrentTrain: epoch  3, batch    33 | loss: 9.7093428Losses:  6.887301921844482 0.9403798580169678 1.0087345838546753 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 6.8873019Losses:  8.890877457335591 0.9961748123168945 0.9688416719436646 1.5703236777335405
CurrentTrain: epoch  3, batch    35 | loss: 8.8908775Losses:  6.089594841003418 0.8138571977615356 1.0306333303451538 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 6.0895948Losses:  6.3691840171813965 0.9421091079711914 1.0213531255722046 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 6.3691840Losses:  9.207191735506058 0.9329590797424316 1.0011467933654785 2.8152134716510773
CurrentTrain: epoch  3, batch    38 | loss: 9.2071917Losses:  10.76105734333396 0.9356123208999634 1.0074121952056885 4.285531964153051
CurrentTrain: epoch  3, batch    39 | loss: 10.7610573Losses:  16.085529517382383 1.0050381422042847 1.1129847764968872 7.609003257006407
CurrentTrain: epoch  3, batch    40 | loss: 16.0855295Losses:  7.191730976104736 0.8288108110427856 0.9962880611419678 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 7.1917310Losses:  7.269054681062698 0.8469142913818359 0.9843736886978149 1.4087264835834503
CurrentTrain: epoch  3, batch    42 | loss: 7.2690547Losses:  8.232439130544662 0.9239767789840698 1.0201141834259033 1.7886100709438324
CurrentTrain: epoch  3, batch    43 | loss: 8.2324391Losses:  10.162923537194729 0.8667337894439697 1.0 3.1286141499876976
CurrentTrain: epoch  3, batch    44 | loss: 10.1629235Losses:  9.224434670060873 0.8136929273605347 1.0 2.8862116895616055
CurrentTrain: epoch  3, batch    45 | loss: 9.2244347Losses:  10.621217966079712 0.9818375110626221 0.9951471090316772 3.366135358810425
CurrentTrain: epoch  3, batch    46 | loss: 10.6212180Losses:  8.898824155330658 0.866106390953064 1.0 2.838612973690033
CurrentTrain: epoch  3, batch    47 | loss: 8.8988242Losses:  6.515401840209961 1.0551726818084717 1.0 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 6.5154018Losses:  17.543489050120115 0.8352088928222656 0.9696232080459595 10.664893697947264
CurrentTrain: epoch  3, batch    49 | loss: 17.5434891Losses:  6.956031799316406 0.9638386964797974 1.0301209688186646 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 6.9560318Losses:  8.038695126771927 0.770354151725769 0.9876055717468262 1.418484479188919
CurrentTrain: epoch  3, batch    51 | loss: 8.0386951Losses:  10.45709402859211 0.8978139162063599 0.9616048336029053 4.354790523648262
CurrentTrain: epoch  3, batch    52 | loss: 10.4570940Losses:  11.808107364922762 1.1363637447357178 1.0935169458389282 4.316674221307039
CurrentTrain: epoch  3, batch    53 | loss: 11.8081074Losses:  7.296073794364929 0.7653723955154419 1.0 1.3967608213424683
CurrentTrain: epoch  3, batch    54 | loss: 7.2960738Losses:  9.286946326494217 0.786142110824585 1.0 2.822997123003006
CurrentTrain: epoch  3, batch    55 | loss: 9.2869463Losses:  6.026657581329346 0.7477103471755981 1.0 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 6.0266576Losses:  6.574735164642334 0.7671542167663574 1.0808308124542236 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 6.5747352Losses:  11.488942213356495 0.7785016298294067 1.0 5.429873533546925
CurrentTrain: epoch  3, batch    58 | loss: 11.4889422Losses:  16.220716506242752 0.7646982669830322 0.9584165811538696 9.366924792528152
CurrentTrain: epoch  3, batch    59 | loss: 16.2207165Losses:  6.877410411834717 0.8276042938232422 1.0194551944732666 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 6.8774104Losses:  11.638944417238235 0.8366588354110718 0.9729131460189819 4.797080785036087
CurrentTrain: epoch  3, batch    61 | loss: 11.6389444Losses:  7.546011924743652 0.720057487487793 0.965325117111206 1.404745101928711
CurrentTrain: epoch  3, batch    62 | loss: 7.5460119Losses:  10.919557992368937 0.9161931276321411 1.0111572742462158 4.3342003263533115
CurrentTrain: epoch  4, batch     0 | loss: 10.9195580Losses:  11.41565027832985 0.7635856866836548 1.0631929636001587 4.947943598031998
CurrentTrain: epoch  4, batch     1 | loss: 11.4156503Losses:  5.889303684234619 0.9484069347381592 0.9632456302642822 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 5.8893037Losses:  9.755758106708527 0.773491621017456 1.0 4.211567223072052
CurrentTrain: epoch  4, batch     3 | loss: 9.7557581Losses:  14.948878861963749 0.9047189950942993 1.0252621173858643 8.599161244928837
CurrentTrain: epoch  4, batch     4 | loss: 14.9488789Losses:  7.5023654997348785 0.700736403465271 1.023768424987793 1.4044322073459625
CurrentTrain: epoch  4, batch     5 | loss: 7.5023655Losses:  7.581066604703665 0.9564086198806763 1.0 1.5132398568093777
CurrentTrain: epoch  4, batch     6 | loss: 7.5810666Losses:  5.486538887023926 0.6739788055419922 1.0 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 5.4865389Losses:  9.669643372297287 0.8839101791381836 0.9788165092468262 2.8795308768749237
CurrentTrain: epoch  4, batch     8 | loss: 9.6696434Losses:  8.471085995435715 0.8218809366226196 1.0383951663970947 1.5258359611034393
CurrentTrain: epoch  4, batch     9 | loss: 8.4710860Losses:  10.755421306937933 0.9168487787246704 0.9841576814651489 4.346591141074896
CurrentTrain: epoch  4, batch    10 | loss: 10.7554213Losses:  9.597640357911587 0.917395830154419 1.0048555135726929 2.931738220155239
CurrentTrain: epoch  4, batch    11 | loss: 9.5976404Losses:  7.909338563680649 0.8031059503555298 1.0 1.4306765496730804
CurrentTrain: epoch  4, batch    12 | loss: 7.9093386Losses:  6.268400192260742 0.870813250541687 1.0130984783172607 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 6.2684002Losses:  8.056451976299286 0.729718804359436 1.012328863143921 1.4264890551567078
CurrentTrain: epoch  4, batch    14 | loss: 8.0564520Losses:  7.896548509597778 0.862206220626831 0.9907798767089844 1.413827657699585
CurrentTrain: epoch  4, batch    15 | loss: 7.8965485Losses:  10.18107577972114 0.9487013816833496 1.0241564512252808 3.220509259030223
CurrentTrain: epoch  4, batch    16 | loss: 10.1810758Losses:  5.958741664886475 0.6727150678634644 1.0 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 5.9587417Losses:  10.673895113170147 0.8297461271286011 1.0 4.391110651195049
CurrentTrain: epoch  4, batch    18 | loss: 10.6738951Losses:  15.131458975374699 0.8117821216583252 0.9665324687957764 8.759093500673771
CurrentTrain: epoch  4, batch    19 | loss: 15.1314590Losses:  9.657988846302032 0.8320918083190918 1.0776958465576172 2.8555052876472473
CurrentTrain: epoch  4, batch    20 | loss: 9.6579888Losses:  9.30111188068986 0.9861547946929932 1.0736916065216064 2.8544003404676914
CurrentTrain: epoch  4, batch    21 | loss: 9.3011119Losses:  7.424761142581701 0.6497517824172974 1.0 1.5000313185155392
CurrentTrain: epoch  4, batch    22 | loss: 7.4247611Losses:  13.049903899431229 0.8719688653945923 1.034203290939331 7.033622294664383
CurrentTrain: epoch  4, batch    23 | loss: 13.0499039Losses:  7.437239624559879 0.7323508262634277 1.09831964969635 1.4693898931145668
CurrentTrain: epoch  4, batch    24 | loss: 7.4372396Losses:  6.993568420410156 0.8973290920257568 1.0415818691253662 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 6.9935684Losses:  7.225097179412842 0.8400970697402954 1.0425231456756592 1.4463071823120117
CurrentTrain: epoch  4, batch    26 | loss: 7.2250972Losses:  9.284542575478554 0.9115375280380249 1.071764588356018 2.2174530178308487
CurrentTrain: epoch  4, batch    27 | loss: 9.2845426Losses:  7.863938421010971 0.8616930246353149 1.0 1.4084196984767914
CurrentTrain: epoch  4, batch    28 | loss: 7.8639384Losses:  12.423954598605633 0.9582297801971436 1.0559061765670776 6.008207432925701
CurrentTrain: epoch  4, batch    29 | loss: 12.4239546Losses:  11.682051062583923 0.8646457195281982 1.0 5.815241694450378
CurrentTrain: epoch  4, batch    30 | loss: 11.6820511Losses:  6.201577663421631 0.9634839296340942 1.0 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 6.2015777Losses:  8.567949242889881 0.6460694074630737 1.0 2.928831048309803
CurrentTrain: epoch  4, batch    32 | loss: 8.5679492Losses:  10.888071119785309 0.8493839502334595 1.0 4.985645830631256
CurrentTrain: epoch  4, batch    33 | loss: 10.8880711Losses:  5.842295169830322 0.7488293647766113 1.0520756244659424 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 5.8422952Losses:  7.670223921537399 0.67955482006073 0.9678955078125 1.4120800197124481
CurrentTrain: epoch  4, batch    35 | loss: 7.6702239Losses:  11.024854578077793 0.8869894742965698 1.0 4.766671098768711
CurrentTrain: epoch  4, batch    36 | loss: 11.0248546Losses:  10.209768420085311 0.7997775077819824 1.0715240240097046 3.579242831096053
CurrentTrain: epoch  4, batch    37 | loss: 10.2097684Losses:  5.843769073486328 0.8162810802459717 0.9825758934020996 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 5.8437691Losses:  10.36914849281311 0.7499592304229736 1.0 4.375797510147095
CurrentTrain: epoch  4, batch    39 | loss: 10.3691485Losses:  10.90481636673212 0.8193414211273193 1.0 4.653192736208439
CurrentTrain: epoch  4, batch    40 | loss: 10.9048164Losses:  5.961512565612793 0.9655615091323853 1.0 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 5.9615126Losses:  12.142023902386427 0.5560973882675171 1.0 6.166504722088575
CurrentTrain: epoch  4, batch    42 | loss: 12.1420239Losses:  8.72476166114211 0.9638829231262207 0.9837956428527832 2.881435092538595
CurrentTrain: epoch  4, batch    43 | loss: 8.7247617Losses:  6.2113518714904785 0.7540711164474487 1.0 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 6.2113519Losses:  7.646060269325972 0.8122509717941284 1.0347520112991333 1.5328376702964306
CurrentTrain: epoch  4, batch    45 | loss: 7.6460603Losses:  9.834034569561481 0.9315738677978516 1.0212061405181885 2.992826111614704
CurrentTrain: epoch  4, batch    46 | loss: 9.8340346Losses:  7.4624469466507435 0.6719378232955933 1.0 1.6729963012039661
CurrentTrain: epoch  4, batch    47 | loss: 7.4624469Losses:  12.314958930015564 0.971159815788269 0.9660041332244873 5.790004134178162
CurrentTrain: epoch  4, batch    48 | loss: 12.3149589Losses:  5.896487236022949 0.8263345956802368 1.0182052850723267 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 5.8964872Losses:  8.84213337302208 0.7947869300842285 1.0 2.9697082936763763
CurrentTrain: epoch  4, batch    50 | loss: 8.8421334Losses:  5.6458916664123535 0.7761307954788208 1.0 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 5.6458917Losses:  9.980759259313345 0.814948320388794 1.0298094749450684 4.286594029515982
CurrentTrain: epoch  4, batch    52 | loss: 9.9807593Losses:  9.351344652473927 0.9631483554840088 1.0612621307373047 1.507039614021778
CurrentTrain: epoch  4, batch    53 | loss: 9.3513447Losses:  6.749499291181564 0.6647235155105591 1.0 1.3957619369029999
CurrentTrain: epoch  4, batch    54 | loss: 6.7494993Losses:  6.517829418182373 0.813645601272583 1.0262547731399536 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 6.5178294Losses:  16.861482605338097 0.8004475831985474 1.0 10.78519819676876
CurrentTrain: epoch  4, batch    56 | loss: 16.8614826Losses:  10.267914652824402 0.8529735803604126 1.049436092376709 4.368769526481628
CurrentTrain: epoch  4, batch    57 | loss: 10.2679147Losses:  9.428207114338875 0.8210477828979492 1.0127869844436646 3.048625186085701
CurrentTrain: epoch  4, batch    58 | loss: 9.4282071Losses:  8.947762053459883 0.7153596878051758 0.9927432537078857 3.0044160299003124
CurrentTrain: epoch  4, batch    59 | loss: 8.9477621Losses:  7.3850759863853455 0.8288722038269043 1.0429937839508057 1.405643880367279
CurrentTrain: epoch  4, batch    60 | loss: 7.3850760Losses:  7.502776626497507 0.6636754274368286 0.9692103862762451 1.4454956091940403
CurrentTrain: epoch  4, batch    61 | loss: 7.5027766Losses:  11.424859374761581 0.6055314540863037 1.0 5.976886600255966
CurrentTrain: epoch  4, batch    62 | loss: 11.4248594Losses:  6.167011737823486 0.6609439849853516 1.0266506671905518 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.1670117Losses:  8.707816760987043 0.7388689517974854 1.0453647375106812 2.8765322379767895
CurrentTrain: epoch  5, batch     1 | loss: 8.7078168Losses:  8.901284836232662 0.962106466293335 1.011204481124878 2.833582065999508
CurrentTrain: epoch  5, batch     2 | loss: 8.9012848Losses:  9.115455828607082 0.7706879377365112 1.0 2.9460675343871117
CurrentTrain: epoch  5, batch     3 | loss: 9.1154558Losses:  10.014010038226843 0.7285065650939941 1.0 4.252462949603796
CurrentTrain: epoch  5, batch     4 | loss: 10.0140100Losses:  11.493965357542038 0.8681732416152954 1.0 5.827950686216354
CurrentTrain: epoch  5, batch     5 | loss: 11.4939654Losses:  6.177029609680176 0.6753735542297363 1.0 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 6.1770296Losses:  7.3070968091487885 0.6628110408782959 1.0 1.407444328069687
CurrentTrain: epoch  5, batch     7 | loss: 7.3070968Losses:  10.797886714339256 0.9354217052459717 1.0486266613006592 4.387839183211327
CurrentTrain: epoch  5, batch     8 | loss: 10.7978867Losses:  12.114578943699598 0.6835700273513794 1.0596522092819214 6.151387434452772
CurrentTrain: epoch  5, batch     9 | loss: 12.1145789Losses:  9.222296237945557 0.7712011337280273 1.0102330446243286 2.7960939407348633
CurrentTrain: epoch  5, batch    10 | loss: 9.2222962Losses:  28.34319445490837 0.7892088890075684 1.0352178812026978 22.386323899030685
CurrentTrain: epoch  5, batch    11 | loss: 28.3431945Losses:  8.689726859331131 0.7259782552719116 1.0 2.8074765503406525
CurrentTrain: epoch  5, batch    12 | loss: 8.6897269Losses:  7.136113882064819 0.7178177833557129 1.0691190958023071 1.398428201675415
CurrentTrain: epoch  5, batch    13 | loss: 7.1361139Losses:  7.670361280441284 0.7141200304031372 1.0 1.7238729000091553
CurrentTrain: epoch  5, batch    14 | loss: 7.6703613Losses:  7.672899425029755 0.8055704832077026 1.0 1.4589068293571472
CurrentTrain: epoch  5, batch    15 | loss: 7.6728994Losses:  7.245417147874832 0.7524340152740479 1.028872013092041 1.4229626953601837
CurrentTrain: epoch  5, batch    16 | loss: 7.2454171Losses:  9.296088814735413 0.744071364402771 1.0 2.8680607080459595
CurrentTrain: epoch  5, batch    17 | loss: 9.2960888Losses:  10.038231994956732 0.5680216550827026 1.0458916425704956 4.551070358604193
CurrentTrain: epoch  5, batch    18 | loss: 10.0382320Losses:  7.090402901172638 0.7765270471572876 1.0338478088378906 1.4180272221565247
CurrentTrain: epoch  5, batch    19 | loss: 7.0904029Losses:  8.071099728345871 0.48422694206237793 1.0 2.800352066755295
CurrentTrain: epoch  5, batch    20 | loss: 8.0710997Losses:  11.0689857006073 0.6926666498184204 1.0 5.794903516769409
CurrentTrain: epoch  5, batch    21 | loss: 11.0689857Losses:  8.372658547013998 0.6698106527328491 1.0 3.017649944871664
CurrentTrain: epoch  5, batch    22 | loss: 8.3726585Losses:  10.121399074792862 0.7112674713134766 1.0 3.9651509821414948
CurrentTrain: epoch  5, batch    23 | loss: 10.1213991Losses:  8.25986322388053 0.6698487997055054 1.0 2.936258163303137
CurrentTrain: epoch  5, batch    24 | loss: 8.2598632Losses:  22.29349485039711 0.8061988353729248 1.0223052501678467 15.661147743463516
CurrentTrain: epoch  5, batch    25 | loss: 22.2934949Losses:  8.59574306756258 0.7522691488265991 1.0407747030258179 2.9555624797940254
CurrentTrain: epoch  5, batch    26 | loss: 8.5957431Losses:  7.538684248924255 0.7866246700286865 1.0 1.4153865575790405
CurrentTrain: epoch  5, batch    27 | loss: 7.5386842Losses:  6.711699970066547 0.4129927158355713 1.0 1.4835062101483345
CurrentTrain: epoch  5, batch    28 | loss: 6.7117000Losses:  10.453757625073195 0.7976564168930054 0.9902733564376831 4.331967692822218
CurrentTrain: epoch  5, batch    29 | loss: 10.4537576Losses:  7.984675370156765 0.7049140930175781 0.9645518064498901 1.519169770181179
CurrentTrain: epoch  5, batch    30 | loss: 7.9846754Losses:  10.03273543715477 0.5886954069137573 1.0 4.304588884115219
CurrentTrain: epoch  5, batch    31 | loss: 10.0327354Losses:  6.931005150079727 0.525815486907959 1.0 1.4061857759952545
CurrentTrain: epoch  5, batch    32 | loss: 6.9310052Losses:  21.59855007752776 0.7899177074432373 1.0798481702804565 15.118294473737478
CurrentTrain: epoch  5, batch    33 | loss: 21.5985501Losses:  8.94888124242425 0.8416838645935059 1.0 2.8858529068529606
CurrentTrain: epoch  5, batch    34 | loss: 8.9488812Losses:  9.879091687500477 0.7589592933654785 1.0 4.258405156433582
CurrentTrain: epoch  5, batch    35 | loss: 9.8790917Losses:  12.816585622727871 0.8124947547912598 1.0 6.740895353257656
CurrentTrain: epoch  5, batch    36 | loss: 12.8165856Losses:  7.059660531580448 0.7763264179229736 1.0117008686065674 1.4472862258553505
CurrentTrain: epoch  5, batch    37 | loss: 7.0596605Losses:  7.145808577537537 0.5994019508361816 1.03957998752594 1.4654701948165894
CurrentTrain: epoch  5, batch    38 | loss: 7.1458086Losses:  7.075508892536163 0.737594723701477 1.0355883836746216 1.4021113514900208
CurrentTrain: epoch  5, batch    39 | loss: 7.0755089Losses:  7.132920891046524 0.8168593645095825 1.0351389646530151 1.406304508447647
CurrentTrain: epoch  5, batch    40 | loss: 7.1329209Losses:  6.187355041503906 0.733529806137085 0.9659267663955688 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 6.1873550Losses:  7.825051888823509 0.7512484788894653 1.0 1.5167733281850815
CurrentTrain: epoch  5, batch    42 | loss: 7.8250519Losses:  10.454526659101248 0.7751424312591553 1.0580625534057617 4.291811224073172
CurrentTrain: epoch  5, batch    43 | loss: 10.4545267Losses:  5.974689483642578 0.761117696762085 1.0717355012893677 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 5.9746895Losses:  5.351300239562988 0.537887692451477 1.0 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 5.3513002Losses:  8.440387599170208 0.6813931465148926 1.0 3.0494774505496025
CurrentTrain: epoch  5, batch    46 | loss: 8.4403876Losses:  8.183024048805237 0.5585740804672241 1.0 1.4040595293045044
CurrentTrain: epoch  5, batch    47 | loss: 8.1830240Losses:  9.37381398677826 0.8176634311676025 1.070056438446045 2.8626941442489624
CurrentTrain: epoch  5, batch    48 | loss: 9.3738140Losses:  7.466284155845642 0.6281130313873291 0.9987554550170898 1.3978666067123413
CurrentTrain: epoch  5, batch    49 | loss: 7.4662842Losses:  7.493350714445114 0.6897830963134766 1.0 1.4020530879497528
CurrentTrain: epoch  5, batch    50 | loss: 7.4933507Losses:  15.84967552870512 0.6389490365982056 1.0374046564102173 10.233714930713177
CurrentTrain: epoch  5, batch    51 | loss: 15.8496755Losses:  10.039899192750454 0.6769530773162842 1.0 4.5236461982131
CurrentTrain: epoch  5, batch    52 | loss: 10.0398992Losses:  14.240607418119907 0.6922752857208252 1.0 8.58952347189188
CurrentTrain: epoch  5, batch    53 | loss: 14.2406074Losses:  7.745157653465867 0.6569911241531372 1.037003993988037 1.803502494469285
CurrentTrain: epoch  5, batch    54 | loss: 7.7451577Losses:  8.427234292030334 0.7118865251541138 1.0296494960784912 2.8263226747512817
CurrentTrain: epoch  5, batch    55 | loss: 8.4272343Losses:  9.1074578166008 0.787935733795166 0.9848358631134033 2.815624415874481
CurrentTrain: epoch  5, batch    56 | loss: 9.1074578Losses:  6.69700400531292 0.5735740661621094 1.0 1.4589773863554
CurrentTrain: epoch  5, batch    57 | loss: 6.6970040Losses:  10.78014625236392 0.6899405717849731 1.0780565738677979 4.762715470045805
CurrentTrain: epoch  5, batch    58 | loss: 10.7801463Losses:  11.51878809928894 0.6524726152420044 1.0288323163986206 4.994231939315796
CurrentTrain: epoch  5, batch    59 | loss: 11.5187881Losses:  7.375734031200409 0.8128809928894043 1.0 1.531061828136444
CurrentTrain: epoch  5, batch    60 | loss: 7.3757340Losses:  7.0908514857292175 0.6120573282241821 0.9724189043045044 1.4779292941093445
CurrentTrain: epoch  5, batch    61 | loss: 7.0908515Losses:  10.120325271040201 0.6788301467895508 1.0827569961547852 4.284361068159342
CurrentTrain: epoch  5, batch    62 | loss: 10.1203253Losses:  13.383385498076677 0.7442241907119751 1.0516910552978516 7.494389373809099
CurrentTrain: epoch  6, batch     0 | loss: 13.3833855Losses:  8.422250263392925 0.5290727615356445 1.0 2.848378174006939
CurrentTrain: epoch  6, batch     1 | loss: 8.4222503Losses:  7.304940667003393 0.5893567800521851 1.038568377494812 1.5042323730885983
CurrentTrain: epoch  6, batch     2 | loss: 7.3049407Losses:  7.3475258350372314 0.7184430360794067 1.0 1.4901096820831299
CurrentTrain: epoch  6, batch     3 | loss: 7.3475258Losses:  6.667258948087692 0.4453270435333252 1.0 1.4115459620952606
CurrentTrain: epoch  6, batch     4 | loss: 6.6672589Losses:  7.559725522994995 0.5712103843688965 1.025511384010315 1.5472862720489502
CurrentTrain: epoch  6, batch     5 | loss: 7.5597255Losses:  5.607471942901611 0.6515264511108398 1.0 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 5.6074719Losses:  6.993286281824112 0.6472630500793457 1.0383777618408203 1.48833766579628
CurrentTrain: epoch  6, batch     7 | loss: 6.9932863Losses:  12.139569364488125 0.7075201272964478 1.0 6.164258562028408
CurrentTrain: epoch  6, batch     8 | loss: 12.1395694Losses:  8.350453674793243 0.6099616289138794 0.9730209112167358 2.991375744342804
CurrentTrain: epoch  6, batch     9 | loss: 8.3504537Losses:  9.977082435041666 0.9140505790710449 1.1191891431808472 3.060932818800211
CurrentTrain: epoch  6, batch    10 | loss: 9.9770824Losses:  9.685462530702353 0.7515115737915039 1.0158525705337524 3.2921748720109463
CurrentTrain: epoch  6, batch    11 | loss: 9.6854625Losses:  7.598453193902969 0.6538796424865723 1.0 1.4371286928653717
CurrentTrain: epoch  6, batch    12 | loss: 7.5984532Losses:  7.279687702655792 0.8018453121185303 1.0 1.4065435528755188
CurrentTrain: epoch  6, batch    13 | loss: 7.2796877Losses:  6.824788600206375 0.590151309967041 1.0 1.4188895523548126
CurrentTrain: epoch  6, batch    14 | loss: 6.8247886Losses:  9.092379719018936 0.735143780708313 1.0 2.8421851694583893
CurrentTrain: epoch  6, batch    15 | loss: 9.0923797Losses:  5.488992214202881 0.5807522535324097 1.0 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 5.4889922Losses:  8.514824569225311 0.6279729604721069 1.0 2.82895964384079
CurrentTrain: epoch  6, batch    17 | loss: 8.5148246Losses:  7.1697505712509155 0.6329609155654907 1.0 1.5403183698654175
CurrentTrain: epoch  6, batch    18 | loss: 7.1697506Losses:  5.53167200088501 0.557923436164856 1.0 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 5.5316720Losses:  10.699992686510086 0.715882420539856 1.0151184797286987 4.553000956773758
CurrentTrain: epoch  6, batch    20 | loss: 10.6999927Losses:  7.4932178519666195 0.6211819648742676 1.0471853017807007 1.5950850509107113
CurrentTrain: epoch  6, batch    21 | loss: 7.4932179Losses:  8.146724037826061 0.6325002908706665 1.0 2.839098744094372
CurrentTrain: epoch  6, batch    22 | loss: 8.1467240Losses:  7.358249019831419 0.6272776126861572 1.0 1.473239254206419
CurrentTrain: epoch  6, batch    23 | loss: 7.3582490Losses:  10.030776843428612 0.6741291284561157 1.0 4.352088317275047
CurrentTrain: epoch  6, batch    24 | loss: 10.0307768Losses:  8.788914300501347 0.7995920181274414 1.0 2.9467187896370888
CurrentTrain: epoch  6, batch    25 | loss: 8.7889143Losses:  7.177227407693863 0.731479287147522 1.0 1.425793081521988
CurrentTrain: epoch  6, batch    26 | loss: 7.1772274Losses:  6.993999540805817 0.5770498514175415 1.0321712493896484 1.4935794472694397
CurrentTrain: epoch  6, batch    27 | loss: 6.9939995Losses:  11.179272878915071 0.4990260601043701 1.0 5.823024023324251
CurrentTrain: epoch  6, batch    28 | loss: 11.1792729Losses:  11.476589053869247 0.7037898302078247 1.0 5.73653444647789
CurrentTrain: epoch  6, batch    29 | loss: 11.4765891Losses:  9.664962768554688 0.599882960319519 1.0408202409744263 4.225729942321777
CurrentTrain: epoch  6, batch    30 | loss: 9.6649628Losses:  7.038232773542404 0.5295135974884033 1.0 1.4922828376293182
CurrentTrain: epoch  6, batch    31 | loss: 7.0382328Losses:  8.683962106704712 0.8271716833114624 1.0426268577575684 2.9695417881011963
CurrentTrain: epoch  6, batch    32 | loss: 8.6839621Losses:  6.647555410861969 0.4076915979385376 1.0540827512741089 1.3993883728981018
CurrentTrain: epoch  6, batch    33 | loss: 6.6475554Losses:  5.701056480407715 0.6603716611862183 1.0214442014694214 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 5.7010565Losses:  9.531500924378633 0.5004091262817383 1.0 3.431723225861788
CurrentTrain: epoch  6, batch    35 | loss: 9.5315009Losses:  8.65798894688487 0.5633108615875244 1.0576328039169312 2.987826745957136
CurrentTrain: epoch  6, batch    36 | loss: 8.6579889Losses:  5.735352993011475 0.6542328596115112 1.0 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 5.7353530Losses:  13.098143517971039 0.6078362464904785 1.0 7.6564196944236755
CurrentTrain: epoch  6, batch    38 | loss: 13.0981435Losses:  5.5447492599487305 0.5846477746963501 1.0 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 5.5447493Losses:  5.504091739654541 0.6090911626815796 0.9731847047805786 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 5.5040917Losses:  6.736884266138077 0.621391773223877 1.0 1.411590725183487
CurrentTrain: epoch  6, batch    41 | loss: 6.7368843Losses:  8.806510925292969 0.447007417678833 1.0418509244918823 2.8276638984680176
CurrentTrain: epoch  6, batch    42 | loss: 8.8065109Losses:  8.835666358470917 0.6695505380630493 1.0464892387390137 3.2138888239860535
CurrentTrain: epoch  6, batch    43 | loss: 8.8356664Losses:  6.79755000770092 0.5800187587738037 1.0 1.5096033066511154
CurrentTrain: epoch  6, batch    44 | loss: 6.7975500Losses:  6.904126759618521 0.6385958194732666 1.0418922901153564 1.4449626170098782
CurrentTrain: epoch  6, batch    45 | loss: 6.9041268Losses:  8.99261736869812 0.6775574684143066 1.029731273651123 2.3935296535491943
CurrentTrain: epoch  6, batch    46 | loss: 8.9926174Losses:  8.34424839541316 0.4797111749649048 1.0 2.9738737158477306
CurrentTrain: epoch  6, batch    47 | loss: 8.3442484Losses:  6.8502202071249485 0.6316879987716675 1.0598596334457397 1.520983699709177
CurrentTrain: epoch  6, batch    48 | loss: 6.8502202Losses:  7.070501625537872 0.6787704229354858 1.0222117900848389 1.4376314282417297
CurrentTrain: epoch  6, batch    49 | loss: 7.0705016Losses:  6.926875025033951 0.7157274484634399 1.0532381534576416 1.3976391851902008
CurrentTrain: epoch  6, batch    50 | loss: 6.9268750Losses:  5.351646423339844 0.5877528190612793 1.0429003238677979 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 5.3516464Losses:  5.59596061706543 0.6498513221740723 1.0 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 5.5959606Losses:  9.497474901378155 0.5534995794296265 1.0 4.298181764781475
CurrentTrain: epoch  6, batch    53 | loss: 9.4974749Losses:  5.111812591552734 0.43071532249450684 1.0 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 5.1118126Losses:  9.782843708992004 0.7652440071105957 1.0 4.429054379463196
CurrentTrain: epoch  6, batch    55 | loss: 9.7828437Losses:  6.889638505876064 0.6387457847595215 1.0 1.502057634294033
CurrentTrain: epoch  6, batch    56 | loss: 6.8896385Losses:  8.23002079129219 0.5499272346496582 1.0 2.8729698956012726
CurrentTrain: epoch  6, batch    57 | loss: 8.2300208Losses:  6.916663333773613 0.740408182144165 1.0 1.454927608370781
CurrentTrain: epoch  6, batch    58 | loss: 6.9166633Losses:  7.087734453380108 0.5916898250579834 1.0 1.433548204600811
CurrentTrain: epoch  6, batch    59 | loss: 7.0877345Losses:  5.15180778503418 0.6538643836975098 1.0 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 5.1518078Losses:  6.545177847146988 0.6943686008453369 1.0 1.4184745848178864
CurrentTrain: epoch  6, batch    61 | loss: 6.5451778Losses:  9.512286275625229 0.4752044677734375 1.0 4.230825513601303
CurrentTrain: epoch  6, batch    62 | loss: 9.5122863Losses:  6.913710951805115 0.7896081209182739 1.0642577409744263 1.4183229207992554
CurrentTrain: epoch  7, batch     0 | loss: 6.9137110Losses:  8.199033349752426 0.5365737676620483 1.0622891187667847 2.815528005361557
CurrentTrain: epoch  7, batch     1 | loss: 8.1990333Losses:  5.482100963592529 0.5943342447280884 1.1038262844085693 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 5.4821010Losses:  12.935509279370308 0.45375871658325195 1.0630463361740112 7.649750784039497
CurrentTrain: epoch  7, batch     3 | loss: 12.9355093Losses:  12.67945009469986 0.4561072587966919 1.0 7.626387178897858
CurrentTrain: epoch  7, batch     4 | loss: 12.6794501Losses:  8.048454344272614 0.5721638202667236 1.0 2.8180375695228577
CurrentTrain: epoch  7, batch     5 | loss: 8.0484543Losses:  5.351548194885254 0.4769577980041504 1.0 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 5.3515482Losses:  11.013324055820704 0.5812121629714966 1.0 5.769753251224756
CurrentTrain: epoch  7, batch     7 | loss: 11.0133241Losses:  6.6354725658893585 0.5586557388305664 1.0 1.4080493748188019
CurrentTrain: epoch  7, batch     8 | loss: 6.6354726Losses:  6.629639241844416 0.48596811294555664 1.0 1.4577384926378727
CurrentTrain: epoch  7, batch     9 | loss: 6.6296392Losses:  10.170560833066702 0.6531916856765747 1.0425478219985962 4.772347446531057
CurrentTrain: epoch  7, batch    10 | loss: 10.1705608Losses:  5.47650146484375 0.5631885528564453 1.0 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 5.4765015Losses:  8.969298005104065 0.5192998647689819 0.9850883483886719 2.822466015815735
CurrentTrain: epoch  7, batch    12 | loss: 8.9692980Losses:  6.413890242576599 0.4666687250137329 1.0453433990478516 1.3988646268844604
CurrentTrain: epoch  7, batch    13 | loss: 6.4138902Losses:  8.404274344444275 0.45750510692596436 1.0 3.2816590070724487
CurrentTrain: epoch  7, batch    14 | loss: 8.4042743Losses:  8.23215663060546 0.5860552787780762 1.0 2.8222798071801662
CurrentTrain: epoch  7, batch    15 | loss: 8.2321566Losses:  12.61827353388071 0.552969217300415 0.9658267498016357 7.271720208227634
CurrentTrain: epoch  7, batch    16 | loss: 12.6182735Losses:  6.67304915189743 0.7195132970809937 1.0 1.3914348483085632
CurrentTrain: epoch  7, batch    17 | loss: 6.6730492Losses:  5.438672065734863 0.6788411140441895 1.0618302822113037 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 5.4386721Losses:  10.913757726550102 0.38888025283813477 1.0 5.86127607524395
CurrentTrain: epoch  7, batch    19 | loss: 10.9137577Losses:  6.60429784655571 0.5645928382873535 1.0332261323928833 1.408699244260788
CurrentTrain: epoch  7, batch    20 | loss: 6.6042978Losses:  9.545593328773975 0.5940053462982178 1.0 4.358570642769337
CurrentTrain: epoch  7, batch    21 | loss: 9.5455933Losses:  7.8572012186050415 0.43035614490509033 1.0 2.8046904802322388
CurrentTrain: epoch  7, batch    22 | loss: 7.8572012Losses:  5.370846748352051 0.7492144107818604 1.0 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 5.3708467Losses:  5.289636611938477 0.6782084703445435 1.0006065368652344 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 5.2896366Losses:  7.144223179668188 0.718726396560669 1.0 1.4290103577077389
CurrentTrain: epoch  7, batch    25 | loss: 7.1442232Losses:  8.285858985036612 0.6393742561340332 1.0364336967468262 2.8542841635644436
CurrentTrain: epoch  7, batch    26 | loss: 8.2858590Losses:  6.537330511957407 0.5039639472961426 1.0 1.4164274968206882
CurrentTrain: epoch  7, batch    27 | loss: 6.5373305Losses:  7.744279861450195 0.4055969715118408 1.0 2.795518398284912
CurrentTrain: epoch  7, batch    28 | loss: 7.7442799Losses:  6.417923718690872 0.412664532661438 1.0 1.3942892849445343
CurrentTrain: epoch  7, batch    29 | loss: 6.4179237Losses:  4.974291801452637 0.43220293521881104 1.0 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 4.9742918Losses:  12.151150438934565 0.5944321155548096 1.0 7.15500757470727
CurrentTrain: epoch  7, batch    31 | loss: 12.1511504Losses:  7.826548635959625 0.49146485328674316 1.0 2.845625936985016
CurrentTrain: epoch  7, batch    32 | loss: 7.8265486Losses:  12.92113371193409 0.40251898765563965 1.0 7.879739478230476
CurrentTrain: epoch  7, batch    33 | loss: 12.9211337Losses:  9.401849769055843 0.6891568899154663 1.0 4.205401919782162
CurrentTrain: epoch  7, batch    34 | loss: 9.4018498Losses:  5.165726661682129 0.651710033416748 1.0 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 5.1657267Losses:  10.5797678232193 0.8528425693511963 1.0585359334945679 4.516999363899231
CurrentTrain: epoch  7, batch    36 | loss: 10.5797678Losses:  8.549778701737523 0.5893988609313965 1.0 3.153378250077367
CurrentTrain: epoch  7, batch    37 | loss: 8.5497787Losses:  7.930809289216995 0.6570937633514404 1.0 2.8008172810077667
CurrentTrain: epoch  7, batch    38 | loss: 7.9308093Losses:  5.449223041534424 0.6458045244216919 1.0 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 5.4492230Losses:  7.968856655061245 0.5753835439682007 1.0 2.8473437651991844
CurrentTrain: epoch  7, batch    40 | loss: 7.9688567Losses:  8.039250373840332 0.6092112064361572 1.0 2.8054862022399902
CurrentTrain: epoch  7, batch    41 | loss: 8.0392504Losses:  5.143582344055176 0.6645264625549316 1.0 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 5.1435823Losses:  6.7115108370780945 0.5413863658905029 1.0468546152114868 1.407003104686737
CurrentTrain: epoch  7, batch    43 | loss: 6.7115108Losses:  9.812120139598846 0.4364330768585205 1.0325007438659668 4.6669880747795105
CurrentTrain: epoch  7, batch    44 | loss: 9.8121201Losses:  7.906983785331249 0.5186313390731812 1.0 2.8282355591654778
CurrentTrain: epoch  7, batch    45 | loss: 7.9069838Losses:  10.994532853364944 0.6286559104919434 0.9883626699447632 5.616359502077103
CurrentTrain: epoch  7, batch    46 | loss: 10.9945329Losses:  6.501484155654907 0.562579870223999 1.0 1.396496057510376
CurrentTrain: epoch  7, batch    47 | loss: 6.5014842Losses:  6.60114249587059 0.4843103885650635 1.0 1.4140625894069672
CurrentTrain: epoch  7, batch    48 | loss: 6.6011425Losses:  6.785904198884964 0.6003981828689575 1.0 1.393704205751419
CurrentTrain: epoch  7, batch    49 | loss: 6.7859042Losses:  5.969292640686035 0.6267749071121216 1.0617586374282837 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 5.9692926Losses:  6.7939721532166 0.629653811454773 1.0 1.4529978223145008
CurrentTrain: epoch  7, batch    51 | loss: 6.7939722Losses:  9.287777952849865 0.526010274887085 1.0 4.2760181948542595
CurrentTrain: epoch  7, batch    52 | loss: 9.2877780Losses:  6.431263744831085 0.6008082628250122 1.0 1.4175527691841125
CurrentTrain: epoch  7, batch    53 | loss: 6.4312637Losses:  9.330722689628601 0.5657082796096802 1.0 4.268973231315613
CurrentTrain: epoch  7, batch    54 | loss: 9.3307227Losses:  5.12495231628418 0.5000017881393433 1.0 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 5.1249523Losses:  4.866004467010498 0.3224269151687622 1.0 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 4.8660045Losses:  5.461752891540527 0.4308300018310547 1.0 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 5.4617529Losses:  10.78097066655755 0.4523566961288452 1.0 5.788755510002375
CurrentTrain: epoch  7, batch    58 | loss: 10.7809707Losses:  5.071651458740234 0.5564154386520386 1.0 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 5.0716515Losses:  6.443603005260229 0.47334253787994385 1.0 1.4400119446218014
CurrentTrain: epoch  7, batch    60 | loss: 6.4436030Losses:  6.496447287499905 0.5693109035491943 1.0 1.4351975545287132
CurrentTrain: epoch  7, batch    61 | loss: 6.4964473Losses:  5.068233489990234 0.610323429107666 1.0 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 5.0682335Losses:  6.690668638795614 0.4283183813095093 1.0 1.5134153924882412
CurrentTrain: epoch  8, batch     0 | loss: 6.6906686Losses:  8.24608988314867 0.5661972761154175 1.0 3.0418486073613167
CurrentTrain: epoch  8, batch     1 | loss: 8.2460899Losses:  7.8987574018538 0.5792620182037354 1.021077036857605 2.8442458547651768
CurrentTrain: epoch  8, batch     2 | loss: 7.8987574Losses:  6.297659073024988 0.25171470642089844 1.0 1.4232136346399784
CurrentTrain: epoch  8, batch     3 | loss: 6.2976591Losses:  6.667051255702972 0.5518877506256104 1.0 1.4134697318077087
CurrentTrain: epoch  8, batch     4 | loss: 6.6670513Losses:  5.044833183288574 0.6014964580535889 1.0 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 5.0448332Losses:  5.061223983764648 0.5853680372238159 1.0 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 5.0612240Losses:  7.931745771318674 0.6627960205078125 1.0 2.8272287882864475
CurrentTrain: epoch  8, batch     7 | loss: 7.9317458Losses:  7.977897796779871 0.50947105884552 1.0568597316741943 2.8803526498377323
CurrentTrain: epoch  8, batch     8 | loss: 7.9778978Losses:  9.1539042070508 0.5932539701461792 1.0327985286712646 4.21331001073122
CurrentTrain: epoch  8, batch     9 | loss: 9.1539042Losses:  5.017269134521484 0.5126509666442871 1.0 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 5.0172691Losses:  6.374793343245983 0.35661351680755615 1.0 1.501786045730114
CurrentTrain: epoch  8, batch    11 | loss: 6.3747933Losses:  6.701179265975952 0.4678722620010376 1.0 1.4638230800628662
CurrentTrain: epoch  8, batch    12 | loss: 6.7011793Losses:  6.478649329394102 0.5416873693466187 1.0 1.48664016649127
CurrentTrain: epoch  8, batch    13 | loss: 6.4786493Losses:  6.393453359603882 0.3960552215576172 1.0 1.4134747982025146
CurrentTrain: epoch  8, batch    14 | loss: 6.3934534Losses:  6.438558429479599 0.43859565258026123 1.0 1.3909991681575775
CurrentTrain: epoch  8, batch    15 | loss: 6.4385584Losses:  10.44177695736289 0.4427056312561035 0.9819309711456299 4.342956747859716
CurrentTrain: epoch  8, batch    16 | loss: 10.4417770Losses:  4.912909030914307 0.34007585048675537 1.0435068607330322 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 4.9129090Losses:  6.414368242025375 0.364660382270813 1.0308564901351929 1.449068158864975
CurrentTrain: epoch  8, batch    18 | loss: 6.4143682Losses:  9.372335452586412 0.5315353870391846 1.0 4.331566829234362
CurrentTrain: epoch  8, batch    19 | loss: 9.3723355Losses:  5.118635177612305 0.650809645652771 1.041709065437317 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 5.1186352Losses:  5.128710746765137 0.6453745365142822 1.0 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 5.1287107Losses:  11.263461388647556 0.6079206466674805 1.0 6.075978554785252
CurrentTrain: epoch  8, batch    22 | loss: 11.2634614Losses:  15.011522315442562 0.5723793506622314 1.0 9.931469462811947
CurrentTrain: epoch  8, batch    23 | loss: 15.0115223Losses:  5.035074234008789 0.6071460247039795 1.0 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 5.0350742Losses:  17.591305263340473 0.2547123432159424 1.0 12.828412063419819
CurrentTrain: epoch  8, batch    25 | loss: 17.5913053Losses:  6.469953566789627 0.5878430604934692 1.0 1.3996076881885529
CurrentTrain: epoch  8, batch    26 | loss: 6.4699536Losses:  15.533911341801286 0.7926771640777588 0.9850187301635742 10.025244349613786
CurrentTrain: epoch  8, batch    27 | loss: 15.5339113Losses:  4.962601661682129 0.4740908145904541 1.0 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 4.9626017Losses:  9.296914935112 0.5487978458404541 1.0 4.215435862541199
CurrentTrain: epoch  8, batch    29 | loss: 9.2969149Losses:  6.39122000336647 0.429473876953125 1.0 1.4386958181858063
CurrentTrain: epoch  8, batch    30 | loss: 6.3912200Losses:  11.77545902878046 0.4841465950012207 1.0 6.766886450350285
CurrentTrain: epoch  8, batch    31 | loss: 11.7754590Losses:  7.811190187931061 0.59035325050354 1.0 2.7893815636634827
CurrentTrain: epoch  8, batch    32 | loss: 7.8111902Losses:  4.901063442230225 0.3115816116333008 1.0 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 4.9010634Losses:  7.26667645573616 0.5480661392211914 1.0726734399795532 1.4213104546070099
CurrentTrain: epoch  8, batch    34 | loss: 7.2666765Losses:  6.424149312078953 0.4085724353790283 1.0 1.4845545664429665
CurrentTrain: epoch  8, batch    35 | loss: 6.4241493Losses:  6.417238742113113 0.5068702697753906 1.0 1.3993130028247833
CurrentTrain: epoch  8, batch    36 | loss: 6.4172387Losses:  4.90181827545166 0.39481520652770996 1.0 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 4.9018183Losses:  13.816164754331112 0.5986956357955933 1.0 8.776873372495174
CurrentTrain: epoch  8, batch    38 | loss: 13.8161648Losses:  9.193487465381622 0.6040240526199341 1.0 4.232792675495148
CurrentTrain: epoch  8, batch    39 | loss: 9.1934875Losses:  6.34888082742691 0.4194796085357666 1.0 1.3945971131324768
CurrentTrain: epoch  8, batch    40 | loss: 6.3488808Losses:  6.413740962743759 0.42314958572387695 1.0 1.399876445531845
CurrentTrain: epoch  8, batch    41 | loss: 6.4137410Losses:  7.946497533470392 0.6164944171905518 1.0 2.854413602501154
CurrentTrain: epoch  8, batch    42 | loss: 7.9464975Losses:  14.097009435296059 0.7350150346755981 1.0 9.050802007317543
CurrentTrain: epoch  8, batch    43 | loss: 14.0970094Losses:  6.351839303970337 0.4660450220108032 1.0 1.3976304531097412
CurrentTrain: epoch  8, batch    44 | loss: 6.3518393Losses:  6.359672874212265 0.329767107963562 1.0 1.3974822461605072
CurrentTrain: epoch  8, batch    45 | loss: 6.3596729Losses:  9.23861276358366 0.3938930034637451 1.0209667682647705 4.326033227145672
CurrentTrain: epoch  8, batch    46 | loss: 9.2386128Losses:  6.300996568053961 0.40248870849609375 1.0 1.4321096204221249
CurrentTrain: epoch  8, batch    47 | loss: 6.3009966Losses:  5.462681770324707 0.36500096321105957 1.0506411790847778 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 5.4626818Losses:  4.900774955749512 0.40892577171325684 1.0 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 4.9007750Losses:  7.906532909721136 0.5832172632217407 1.0419234037399292 2.8024384044110775
CurrentTrain: epoch  8, batch    50 | loss: 7.9065329Losses:  6.445813566446304 0.5657634735107422 1.0 1.4132474958896637
CurrentTrain: epoch  8, batch    51 | loss: 6.4458136Losses:  6.409707278013229 0.47401928901672363 1.0264830589294434 1.4057981669902802
CurrentTrain: epoch  8, batch    52 | loss: 6.4097073Losses:  4.954719066619873 0.5166569948196411 1.045031189918518 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 4.9547191Losses:  9.29170036315918 0.2882798910140991 1.0 4.437939643859863
CurrentTrain: epoch  8, batch    54 | loss: 9.2917004Losses:  6.289856731891632 0.26497888565063477 1.0 1.4211023449897766
CurrentTrain: epoch  8, batch    55 | loss: 6.2898567Losses:  9.455079957842827 0.478543758392334 1.0 4.494830057024956
CurrentTrain: epoch  8, batch    56 | loss: 9.4550800Losses:  9.115937948226929 0.3746519088745117 1.0 4.261403799057007
CurrentTrain: epoch  8, batch    57 | loss: 9.1159379Losses:  6.481651723384857 0.4659940004348755 1.0 1.4022254347801208
CurrentTrain: epoch  8, batch    58 | loss: 6.4816517Losses:  6.358155190944672 0.44872939586639404 1.0 1.4078373312950134
CurrentTrain: epoch  8, batch    59 | loss: 6.3581552Losses:  4.863701820373535 0.43588685989379883 1.0 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 4.8637018Losses:  10.567893460392952 0.5269136428833008 1.0 5.611231759190559
CurrentTrain: epoch  8, batch    61 | loss: 10.5678935Losses:  6.516602210700512 0.7904238700866699 1.0 1.4534956738352776
CurrentTrain: epoch  8, batch    62 | loss: 6.5166022Losses:  8.01317374408245 0.3143486976623535 1.0 3.1690336912870407
CurrentTrain: epoch  9, batch     0 | loss: 8.0131737Losses:  4.699026107788086 0.17616236209869385 1.0 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.6990261Losses:  4.964770793914795 0.5177510976791382 1.0 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 4.9647708Losses:  7.732642829418182 0.4483991861343384 1.0 2.8503629565238953
CurrentTrain: epoch  9, batch     3 | loss: 7.7326428Losses:  7.754091203212738 0.5507080554962158 1.0 2.7899617552757263
CurrentTrain: epoch  9, batch     4 | loss: 7.7540912Losses:  9.167110666632652 0.3324514627456665 1.0 4.316231951117516
CurrentTrain: epoch  9, batch     5 | loss: 9.1671107Losses:  6.244377434253693 0.37811553478240967 1.0 1.3929041028022766
CurrentTrain: epoch  9, batch     6 | loss: 6.2443774Losses:  7.680645763874054 0.40732240676879883 1.0 2.810395061969757
CurrentTrain: epoch  9, batch     7 | loss: 7.6806458Losses:  7.747034549713135 0.6288793087005615 1.0 2.791477680206299
CurrentTrain: epoch  9, batch     8 | loss: 7.7470345Losses:  4.786296844482422 0.306554913520813 1.0 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 4.7862968Losses:  7.796416785567999 0.5413209199905396 1.0 2.890538241714239
CurrentTrain: epoch  9, batch    10 | loss: 7.7964168Losses:  6.865365743637085 0.44569921493530273 1.0 1.419503927230835
CurrentTrain: epoch  9, batch    11 | loss: 6.8653657Losses:  7.753823935985565 0.5032050609588623 1.0 2.8358041644096375
CurrentTrain: epoch  9, batch    12 | loss: 7.7538239Losses:  4.8567399978637695 0.3781616687774658 1.0 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 4.8567400Losses:  7.666971355676651 0.329845666885376 1.0 2.834634929895401
CurrentTrain: epoch  9, batch    14 | loss: 7.6669714Losses:  8.965860337018967 0.36834394931793213 1.0 4.181608647108078
CurrentTrain: epoch  9, batch    15 | loss: 8.9658603Losses:  7.846159398555756 0.4611293077468872 1.0 2.8648528456687927
CurrentTrain: epoch  9, batch    16 | loss: 7.8461594Losses:  4.793334007263184 0.33852195739746094 1.0 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 4.7933340Losses:  6.344398979097605 0.4986530542373657 1.0 1.4441342391073704
CurrentTrain: epoch  9, batch    18 | loss: 6.3443990Losses:  11.8562351167202 0.30416905879974365 1.0 7.091379731893539
CurrentTrain: epoch  9, batch    19 | loss: 11.8562351Losses:  6.200645565986633 0.34407615661621094 1.0 1.4047037363052368
CurrentTrain: epoch  9, batch    20 | loss: 6.2006456Losses:  6.575711518526077 0.5174918174743652 1.0029207468032837 1.3952453434467316
CurrentTrain: epoch  9, batch    21 | loss: 6.5757115Losses:  4.866753578186035 0.5031225681304932 1.0 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 4.8667536Losses:  7.973921671509743 0.6769770383834839 1.0 2.9319576174020767
CurrentTrain: epoch  9, batch    23 | loss: 7.9739217Losses:  5.465926647186279 0.4764688014984131 1.0486122369766235 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 5.4659266Losses:  9.234473545104265 0.30953216552734375 1.0 4.499031383544207
CurrentTrain: epoch  9, batch    25 | loss: 9.2344735Losses:  4.989980697631836 0.35015320777893066 1.0 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 4.9899807Losses:  7.64420810341835 0.42161881923675537 1.0 2.844684273004532
CurrentTrain: epoch  9, batch    27 | loss: 7.6442081Losses:  7.598468154668808 0.4147171974182129 1.0 2.8015244901180267
CurrentTrain: epoch  9, batch    28 | loss: 7.5984682Losses:  7.739949643611908 0.5503027439117432 1.0 2.807541787624359
CurrentTrain: epoch  9, batch    29 | loss: 7.7399496Losses:  6.214978188276291 0.27693426609039307 1.0 1.3982453048229218
CurrentTrain: epoch  9, batch    30 | loss: 6.2149782Losses:  9.085154265165329 0.45897138118743896 1.0 4.251311033964157
CurrentTrain: epoch  9, batch    31 | loss: 9.0851543Losses:  12.787949085235596 0.41145193576812744 1.0 7.90625
CurrentTrain: epoch  9, batch    32 | loss: 12.7879491Losses:  9.133948478847742 0.473124623298645 1.0 4.279766712337732
CurrentTrain: epoch  9, batch    33 | loss: 9.1339485Losses:  4.738332748413086 0.2863161563873291 1.0 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 4.7383327Losses:  10.57661934196949 0.42143476009368896 1.0 5.709601119160652
CurrentTrain: epoch  9, batch    35 | loss: 10.5766193Losses:  7.643992066383362 0.5079053640365601 1.0 2.7879797220230103
CurrentTrain: epoch  9, batch    36 | loss: 7.6439921Losses:  5.019165992736816 0.4063241481781006 1.0 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 5.0191660Losses:  6.3913606107234955 0.2961076498031616 1.0 1.4234937131404877
CurrentTrain: epoch  9, batch    38 | loss: 6.3913606Losses:  4.878561973571777 0.3902895450592041 1.0 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 4.8785620Losses:  6.275099582970142 0.4458881616592407 1.0 1.4208057597279549
CurrentTrain: epoch  9, batch    40 | loss: 6.2750996Losses:  4.820537090301514 0.41066229343414307 1.0 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 4.8205371Losses:  9.164369408041239 0.3061182498931885 1.0 4.273386780172586
CurrentTrain: epoch  9, batch    42 | loss: 9.1643694Losses:  7.712599575519562 0.5379172563552856 1.0 2.820542633533478
CurrentTrain: epoch  9, batch    43 | loss: 7.7125996Losses:  6.285408556461334 0.4852076768875122 1.0 1.393757402896881
CurrentTrain: epoch  9, batch    44 | loss: 6.2854086Losses:  6.35571688786149 0.5947500467300415 1.0 1.429303828626871
CurrentTrain: epoch  9, batch    45 | loss: 6.3557169Losses:  7.93038484826684 0.4920542240142822 1.0 2.9000589586794376
CurrentTrain: epoch  9, batch    46 | loss: 7.9303848Losses:  7.674408465623856 0.3460865020751953 1.0 2.8397426903247833
CurrentTrain: epoch  9, batch    47 | loss: 7.6744085Losses:  4.832298278808594 0.37396788597106934 1.0 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 4.8322983Losses:  7.749950289726257 0.4022552967071533 1.0 2.8193949460983276
CurrentTrain: epoch  9, batch    49 | loss: 7.7499503Losses:  9.173720311373472 0.40702998638153076 1.0 4.326337289065123
CurrentTrain: epoch  9, batch    50 | loss: 9.1737203Losses:  4.920886516571045 0.5767905712127686 1.0 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 4.9208865Losses:  7.648129492998123 0.3512554168701172 1.0 2.8431444466114044
CurrentTrain: epoch  9, batch    52 | loss: 7.6481295Losses:  7.574913650751114 0.2262880802154541 1.0 2.8115431368350983
CurrentTrain: epoch  9, batch    53 | loss: 7.5749137Losses:  7.698179420083761 0.5118107795715332 1.0 2.8398916088044643
CurrentTrain: epoch  9, batch    54 | loss: 7.6981794Losses:  9.171759128570557 0.3296372890472412 1.0 4.232644557952881
CurrentTrain: epoch  9, batch    55 | loss: 9.1717591Losses:  9.238709837198257 0.47175002098083496 1.0 4.33113756775856
CurrentTrain: epoch  9, batch    56 | loss: 9.2387098Losses:  13.754675343632698 0.43873560428619385 1.0 8.917229130864143
CurrentTrain: epoch  9, batch    57 | loss: 13.7546753Losses:  6.216318607330322 0.34703147411346436 1.0 1.4194555282592773
CurrentTrain: epoch  9, batch    58 | loss: 6.2163186Losses:  9.124778889119625 0.2739678621292114 1.0 4.204154156148434
CurrentTrain: epoch  9, batch    59 | loss: 9.1247789Losses:  6.263580027967691 0.2710379362106323 1.0 1.4529320634901524
CurrentTrain: epoch  9, batch    60 | loss: 6.2635800Losses:  10.183508217334747 0.48633790016174316 1.026915431022644 4.202740967273712
CurrentTrain: epoch  9, batch    61 | loss: 10.1835082Losses:  10.708622369915247 0.3796360492706299 1.0 5.859933767467737
CurrentTrain: epoch  9, batch    62 | loss: 10.7086224
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  8  clusters
Clusters:  [5 0 2 6 1 4 2 0 0 0 0 0 0 1 7 0 3 1 0 0]
Losses:  9.491920612752438 1.2754297256469727 1.8883813619613647 1.535078190267086
CurrentTrain: epoch  0, batch     0 | loss: 9.4919206Losses:  13.548576802015305 1.2738925218582153 1.8959293365478516 6.006614655256271
CurrentTrain: epoch  0, batch     1 | loss: 13.5485768Losses:  10.341331154108047 1.2682081460952759 1.449947714805603 2.868809849023819
CurrentTrain: epoch  0, batch     2 | loss: 10.3413312Losses:  10.219109516590834 0.9072799682617188 1.5954055786132812 1.620728474110365
CurrentTrain: epoch  0, batch     3 | loss: 10.2191095Losses:  8.021281242370605 1.2537933588027954 1.9222370386123657 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 8.0212812Losses:  9.089093673974276 1.2148058414459229 1.5213314294815063 1.7878031618893147
CurrentTrain: epoch  1, batch     1 | loss: 9.0890937Losses:  7.934918813407421 1.2877055406570435 1.5562236309051514 1.5133537575602531
CurrentTrain: epoch  1, batch     2 | loss: 7.9349188Losses:  8.273842506110668 1.2524080276489258 0.8030328750610352 1.4506823346018791
CurrentTrain: epoch  1, batch     3 | loss: 8.2738425Losses:  6.220101833343506 1.1663559675216675 1.3039945363998413 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 6.2201018Losses:  9.85854023694992 1.2679659128189087 1.6920098066329956 2.971596419811249
CurrentTrain: epoch  2, batch     1 | loss: 9.8585402Losses:  6.4588942527771 1.1553093194961548 1.3577518463134766 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 6.4588943Losses:  5.745638906955719 1.2945938110351562 1.679574966430664 1.4247427582740784
CurrentTrain: epoch  2, batch     3 | loss: 5.7456389Losses:  10.070700373500586 1.1973124742507935 1.5313923358917236 3.2481791637837887
CurrentTrain: epoch  3, batch     0 | loss: 10.0707004Losses:  8.630986848846078 1.1724121570587158 1.4562416076660156 3.3762041721493006
CurrentTrain: epoch  3, batch     1 | loss: 8.6309868Losses:  7.020199239253998 1.1277215480804443 1.3597794771194458 1.4041027426719666
CurrentTrain: epoch  3, batch     2 | loss: 7.0201992Losses:  6.565030887722969 1.387995719909668 1.3861160278320312 1.4958198815584183
CurrentTrain: epoch  3, batch     3 | loss: 6.5650309Losses:  4.960653305053711 1.1737622022628784 1.3257256746292114 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 4.9606533Losses:  10.85676464624703 1.2656711339950562 1.4521675109863281 5.246463628485799
CurrentTrain: epoch  4, batch     1 | loss: 10.8567646Losses:  5.0909295082092285 1.0626152753829956 1.2477562427520752 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 5.0909295Losses:  6.842268295586109 1.1229639053344727 0.9266071319580078 1.4922397807240486
CurrentTrain: epoch  4, batch     3 | loss: 6.8422683Losses:  5.37277889251709 1.1781038045883179 1.276400089263916 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 5.3727789Losses:  4.795062065124512 1.2051490545272827 1.247463345527649 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 4.7950621Losses:  9.01834236457944 1.0846222639083862 1.3864645957946777 4.054839480668306
CurrentTrain: epoch  5, batch     2 | loss: 9.0183424Losses:  5.817926660180092 0.6802568435668945 1.0 1.515201821923256
CurrentTrain: epoch  5, batch     3 | loss: 5.8179267Losses:  4.489447593688965 1.0926814079284668 1.1853492259979248 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 4.4894476Losses:  4.529691696166992 1.1756349802017212 1.1801748275756836 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 4.5296917Losses:  4.4570488929748535 1.039365530014038 1.1713321208953857 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 4.4570489Losses:  5.847098506987095 1.186166763305664 1.3143119812011719 1.4920837059617043
CurrentTrain: epoch  6, batch     3 | loss: 5.8470985Losses:  6.754440322518349 1.1501991748809814 1.0938745737075806 2.3882990032434464
CurrentTrain: epoch  7, batch     0 | loss: 6.7544403Losses:  6.033877864480019 1.12544846534729 1.2658627033233643 1.5567493587732315
CurrentTrain: epoch  7, batch     1 | loss: 6.0338779Losses:  5.4181847013533115 1.0155627727508545 1.169933557510376 1.467734757810831
CurrentTrain: epoch  7, batch     2 | loss: 5.4181847Losses:  5.066085729748011 1.2518749237060547 0.8409481048583984 1.49787512794137
CurrentTrain: epoch  7, batch     3 | loss: 5.0660857Losses:  8.458631355315447 1.1905755996704102 1.212885856628418 4.362226326018572
CurrentTrain: epoch  8, batch     0 | loss: 8.4586314Losses:  7.08031490072608 1.028151273727417 1.2850662469863892 2.886610295623541
CurrentTrain: epoch  8, batch     1 | loss: 7.0803149Losses:  3.8505873680114746 1.0576963424682617 1.1653497219085693 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 3.8505874Losses:  4.692025423049927 0.8963375091552734 1.0 1.4153177738189697
CurrentTrain: epoch  8, batch     3 | loss: 4.6920254Losses:  5.188191797584295 1.0359485149383545 1.142341136932373 1.417275097221136
CurrentTrain: epoch  9, batch     0 | loss: 5.1881918Losses:  5.1568500734865665 1.1726242303848267 1.1787515878677368 1.471785519272089
CurrentTrain: epoch  9, batch     1 | loss: 5.1568501Losses:  5.996582750231028 1.0288833379745483 1.2308858633041382 1.8451478518545628
CurrentTrain: epoch  9, batch     2 | loss: 5.9965828Losses:  5.373939584940672 1.2181196212768555 1.3604240417480469 1.5587621442973614
CurrentTrain: epoch  9, batch     3 | loss: 5.3739396
Losses:  2.9368181228637695 0.7250409126281738 0.9911501407623291 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 2.9368181Losses:  7.044406298547983 1.1422696113586426 1.0582637786865234 5.743639353662729
MemoryTrain:  epoch  0, batch     1 | loss: 7.0444063Losses:  2.7138118743896484 0.8354684114456177 1.0260188579559326 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 2.7138119Losses:  6.592567350715399 0.5487380027770996 1.0 5.635300900787115
MemoryTrain:  epoch  1, batch     1 | loss: 6.5925674Losses:  1.9338805675506592 0.8054388761520386 1.1136888265609741 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.9338806Losses:  8.893077041953802 0.6284623146057129 1.0 5.6273566745221615
MemoryTrain:  epoch  2, batch     1 | loss: 8.8930770Losses:  1.9579644203186035 0.6658269166946411 1.0676811933517456 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.9579644Losses:  7.24031000956893 1.1512179374694824 1.0 5.681567009538412
MemoryTrain:  epoch  3, batch     1 | loss: 7.2403100Losses:  1.8807094097137451 0.699997067451477 1.0729392766952515 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 1.8807094Losses:  6.759781859815121 0.8045425415039062 1.0 5.649041913449764
MemoryTrain:  epoch  4, batch     1 | loss: 6.7597819Losses:  1.3847320079803467 0.7508394718170166 1.1231064796447754 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 1.3847320Losses:  7.848359860479832 0.6270918846130371 1.0 5.681175746023655
MemoryTrain:  epoch  5, batch     1 | loss: 7.8483599Losses:  1.5448001623153687 0.7154461145401001 0.9767954349517822 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 1.5448002Losses:  6.644009053707123 0.8320040702819824 1.0 5.657653987407684
MemoryTrain:  epoch  6, batch     1 | loss: 6.6440091Losses:  1.5548820495605469 0.7438044548034668 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 1.5548820Losses:  6.657537553459406 0.5685930252075195 1.3583569526672363 5.656603071838617
MemoryTrain:  epoch  7, batch     1 | loss: 6.6575376Losses:  1.4620792865753174 0.7109717130661011 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 1.4620793Losses:  6.755953352898359 0.6636910438537598 1.2987604141235352 5.737630169838667
MemoryTrain:  epoch  8, batch     1 | loss: 6.7559534Losses:  1.318746566772461 0.665475606918335 1.093352198600769 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 1.3187466Losses:  6.592658158391714 0.7499804496765137 1.0 5.692143674939871
MemoryTrain:  epoch  9, batch     1 | loss: 6.5926582
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 75.80%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 74.54%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 73.98%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 71.33%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 68.37%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 67.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.31%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.64%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 93.54%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.34%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.25%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 93.36%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.66%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.66%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.66%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.58%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.26%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 93.03%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.80%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 92.66%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 92.59%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 92.45%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 92.09%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 92.11%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.99%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.86%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.59%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 91.41%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 91.15%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 90.69%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 90.59%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 90.35%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 89.85%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 89.30%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 88.75%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 88.35%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 88.02%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 87.69%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 87.44%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 87.06%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 86.57%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 86.40%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 85.92%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.70%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.26%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.81%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 84.20%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.66%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 83.12%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.66%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 82.25%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.02%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.73%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.02%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.57%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 83.60%   
cur_acc:  ['0.9484', '0.7331']
his_acc:  ['0.9484', '0.8360']
Clustering into  14  clusters
Clusters:  [13  2 11  0  1  5 12  0  2  2  0  4  4  1  8 10  7  5  2  2  6  2  0  2
  9  5  0  3  2  2]
Losses:  7.73254669085145 1.0589629411697388 1.1706384420394897 1.4855383671820164
CurrentTrain: epoch  0, batch     0 | loss: 7.7325467Losses:  8.14578628540039 0.9992295503616333 1.222541093826294 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 8.1457863Losses:  10.779151428490877 0.9845858812332153 1.4406912326812744 3.8875198252499104
CurrentTrain: epoch  0, batch     2 | loss: 10.7791514Losses:  11.01935639232397 1.0784368515014648 0.8468465805053711 1.5206781849265099
CurrentTrain: epoch  0, batch     3 | loss: 11.0193564Losses:  8.203922033309937 0.9796189069747925 1.340128779411316 1.4077088832855225
CurrentTrain: epoch  1, batch     0 | loss: 8.2039220Losses:  13.719650898128748 0.9294185638427734 1.1594940423965454 8.174002323299646
CurrentTrain: epoch  1, batch     1 | loss: 13.7196509Losses:  9.00661038979888 1.0419994592666626 1.3458149433135986 1.4897074662148952
CurrentTrain: epoch  1, batch     2 | loss: 9.0066104Losses:  9.043943502008915 1.079507827758789 1.1546268463134766 1.4741865172982216
CurrentTrain: epoch  1, batch     3 | loss: 9.0439435Losses:  15.309512097388506 1.0026216506958008 1.376734972000122 9.042484242469072
CurrentTrain: epoch  2, batch     0 | loss: 15.3095121Losses:  8.149867609143257 0.9417831897735596 1.2774282693862915 1.8807521611452103
CurrentTrain: epoch  2, batch     1 | loss: 8.1498676Losses:  7.621389664709568 0.9893240928649902 1.229346752166748 1.443239487707615
CurrentTrain: epoch  2, batch     2 | loss: 7.6213897Losses:  8.255625605583191 1.0382585525512695 1.7355594635009766 1.5595024824142456
CurrentTrain: epoch  2, batch     3 | loss: 8.2556256Losses:  5.65817928314209 0.9907760620117188 1.0608785152435303 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 5.6581793Losses:  8.701809730380774 0.8547471761703491 1.2266613245010376 3.2822707556188107
CurrentTrain: epoch  3, batch     1 | loss: 8.7018097Losses:  9.554598972201347 0.9833688735961914 1.5155948400497437 3.0085770338773727
CurrentTrain: epoch  3, batch     2 | loss: 9.5545990Losses:  8.335724771022797 0.9799375534057617 1.6911354064941406 1.7557453513145447
CurrentTrain: epoch  3, batch     3 | loss: 8.3357248Losses:  9.23104476928711 1.042423129081726 1.1398587226867676 3.352402687072754
CurrentTrain: epoch  4, batch     0 | loss: 9.2310448Losses:  6.72141569852829 0.917771577835083 1.2969270944595337 1.41131991147995
CurrentTrain: epoch  4, batch     1 | loss: 6.7214157Losses:  9.396529838442802 0.8991235494613647 1.4492148160934448 3.1099302023649216
CurrentTrain: epoch  4, batch     2 | loss: 9.3965298Losses:  5.275746840983629 0.9460363388061523 1.0371665954589844 1.5337853617966175
CurrentTrain: epoch  4, batch     3 | loss: 5.2757468Losses:  5.467552185058594 0.9088281393051147 1.4759095907211304 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 5.4675522Losses:  6.027189284563065 0.920514702796936 1.1076838970184326 1.4666490852832794
CurrentTrain: epoch  5, batch     1 | loss: 6.0271893Losses:  7.280046917498112 0.9806478023529053 1.2814321517944336 1.4402875676751137
CurrentTrain: epoch  5, batch     2 | loss: 7.2800469Losses:  7.3747991025447845 1.0165414810180664 1.0 1.452021449804306
CurrentTrain: epoch  5, batch     3 | loss: 7.3747991Losses:  7.815016929060221 0.9304502010345459 1.1799218654632568 2.888250533491373
CurrentTrain: epoch  6, batch     0 | loss: 7.8150169Losses:  8.625088717788458 0.8511625528335571 1.4457952976226807 2.9032511971890926
CurrentTrain: epoch  6, batch     1 | loss: 8.6250887Losses:  10.350739687681198 0.9707105159759521 1.1843202114105225 5.055492609739304
CurrentTrain: epoch  6, batch     2 | loss: 10.3507397Losses:  7.143680185079575 0.8009128570556641 0.9938688278198242 1.4757443368434906
CurrentTrain: epoch  6, batch     3 | loss: 7.1436802Losses:  4.225630283355713 0.9149229526519775 1.0366960763931274 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 4.2256303Losses:  6.73652109503746 0.938561201095581 1.2734676599502563 1.4156998097896576
CurrentTrain: epoch  7, batch     1 | loss: 6.7365211Losses:  9.86730071529746 0.8702465295791626 1.3411043882369995 4.681011404842138
CurrentTrain: epoch  7, batch     2 | loss: 9.8673007Losses:  7.502956725656986 1.0767765045166016 1.0 1.4946487918496132
CurrentTrain: epoch  7, batch     3 | loss: 7.5029567Losses:  9.270492348819971 1.0035265684127808 1.1975252628326416 4.468124661594629
CurrentTrain: epoch  8, batch     0 | loss: 9.2704923Losses:  8.246661931276321 0.7910504341125488 1.227665901184082 2.814010888338089
CurrentTrain: epoch  8, batch     1 | loss: 8.2466619Losses:  8.322889558970928 0.895110011100769 1.2146207094192505 4.291515104472637
CurrentTrain: epoch  8, batch     2 | loss: 8.3228896Losses:  6.184633433818817 0.9382696151733398 1.0 1.4002687335014343
CurrentTrain: epoch  8, batch     3 | loss: 6.1846334Losses:  4.9490966796875 0.8640804290771484 1.341537356376648 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 4.9490967Losses:  5.760779440402985 0.8871212005615234 1.2000603675842285 1.443697988986969
CurrentTrain: epoch  9, batch     1 | loss: 5.7607794Losses:  6.663801908493042 0.8995287418365479 1.1660445928573608 1.4685676097869873
CurrentTrain: epoch  9, batch     2 | loss: 6.6638019Losses:  4.696627274155617 0.8676662445068359 1.2510480880737305 1.4344376474618912
CurrentTrain: epoch  9, batch     3 | loss: 4.6966273
Losses:  1.4567158222198486 0.9994637966156006 1.2004083395004272 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 1.4567158Losses:  1.2486943006515503 0.7492191195487976 1.0053755044937134 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 1.2486943Losses:  1.4108824729919434 0.7738770246505737 1.0 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.4108825Losses:  1.6188582181930542 0.9533321261405945 1.1452007293701172 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.6188582Losses:  1.3388569355010986 0.8471146821975708 1.1102765798568726 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.3388569Losses:  1.1174838542938232 0.8622555732727051 0.999407947063446 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.1174839Losses:  1.0784845352172852 0.8172752857208252 1.1075525283813477 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.0784845Losses:  1.0698692798614502 0.8678126335144043 1.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 1.0698693Losses:  1.1222732067108154 0.8221577405929565 1.1157442331314087 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 1.1222732Losses:  1.0219461917877197 0.8553417921066284 1.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 1.0219462Losses:  1.1119532585144043 0.8772900104522705 1.1104544401168823 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 1.1119533Losses:  0.9508994221687317 0.7669191956520081 1.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 0.9508994Losses:  1.065941572189331 0.8796374797821045 1.1031140089035034 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 1.0659416Losses:  0.911907434463501 0.7345460057258606 1.021160364151001 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 0.9119074Losses:  0.9096643328666687 0.7282180786132812 1.0071996450424194 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 0.9096643Losses:  1.0506353378295898 0.8710559010505676 1.1484968662261963 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 1.0506353Losses:  0.9390859603881836 0.814297080039978 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 0.9390860Losses:  0.9528328776359558 0.7420793771743774 1.1106882095336914 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 0.9528329Losses:  0.9342586994171143 0.7127604484558105 1.0969277620315552 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 0.9342587Losses:  0.911942720413208 0.7999083995819092 0.9536149501800537 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 0.9119427
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 61.11%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 57.97%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 56.67%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 55.04%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 55.86%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 56.82%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 57.72%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 59.90%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 67.06%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 67.43%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.45%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.95%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.25%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 91.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.78%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.98%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.93%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 91.27%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.64%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 91.49%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 91.47%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 91.45%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 91.03%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.82%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 90.70%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 90.14%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 89.88%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 89.71%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 89.61%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 89.37%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 89.20%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.97%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 88.68%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 88.53%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 88.18%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 87.70%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 87.17%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 86.58%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 86.20%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.95%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 85.65%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 85.48%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 85.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.72%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.50%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.16%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.95%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.55%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.12%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 82.47%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.88%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 81.19%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 80.69%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.30%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.09%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.20%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 81.20%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.60%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 81.30%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 80.95%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 80.52%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 80.14%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 79.71%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 79.39%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 79.45%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.71%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.81%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 80.06%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 80.03%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 79.54%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 79.11%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 78.81%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 78.52%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 78.28%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 77.99%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 78.44%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 77.96%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 77.57%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 77.15%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 76.77%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 76.32%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 76.42%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.41%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 77.76%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 77.74%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 77.65%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 77.57%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 77.48%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 77.41%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 77.30%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 77.20%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 77.22%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 77.18%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 77.16%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.28%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 77.26%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 77.19%   
cur_acc:  ['0.9484', '0.7331', '0.6845']
his_acc:  ['0.9484', '0.8360', '0.7719']
Clustering into  19  clusters
Clusters:  [11  0 13  6  1 18 16  6  3  3  6  2  2  1 10 12 15 14  0  0  7  0  9  0
  4 14  6  5  0  0  0  6  0  5 17  0  2  8  0  0]
Losses:  6.391802787780762 1.1193491220474243 1.2989284992218018 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 6.3918028Losses:  7.287175923585892 0.9575774669647217 1.5715899467468262 1.4287174046039581
CurrentTrain: epoch  0, batch     1 | loss: 7.2871759Losses:  13.927800703793764 0.9660230875015259 1.3567713499069214 8.397178698331118
CurrentTrain: epoch  0, batch     2 | loss: 13.9278007Losses:  7.042598329484463 1.0456724166870117 1.229257583618164 1.437031351029873
CurrentTrain: epoch  0, batch     3 | loss: 7.0425983Losses:  8.572452910244465 0.9440741539001465 1.3493261337280273 3.121143229305744
CurrentTrain: epoch  1, batch     0 | loss: 8.5724529Losses:  4.955294609069824 1.0152990818023682 1.3874824047088623 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 4.9552946Losses:  5.642175875604153 0.9131460189819336 1.2285635471343994 1.4384095296263695
CurrentTrain: epoch  1, batch     2 | loss: 5.6421759Losses:  6.287569664418697 1.1028203964233398 1.263840675354004 1.4399239048361778
CurrentTrain: epoch  1, batch     3 | loss: 6.2875697Losses:  9.028162382543087 1.0763320922851562 1.398727536201477 4.262811563909054
CurrentTrain: epoch  2, batch     0 | loss: 9.0281624Losses:  7.493783477693796 0.9269908666610718 1.2479692697525024 2.943675521761179
CurrentTrain: epoch  2, batch     1 | loss: 7.4937835Losses:  6.769341208040714 0.8976532220840454 1.2041089534759521 2.863475538790226
CurrentTrain: epoch  2, batch     2 | loss: 6.7693412Losses:  9.175971075892448 0.40381908416748047 1.0 6.684973761439323
CurrentTrain: epoch  2, batch     3 | loss: 9.1759711Losses:  6.3632292076945305 0.9402920007705688 1.0406001806259155 2.003289632499218
CurrentTrain: epoch  3, batch     0 | loss: 6.3632292Losses:  8.512284837663174 0.9744361639022827 1.3227708339691162 4.344256483018398
CurrentTrain: epoch  3, batch     1 | loss: 8.5122848Losses:  6.756916791200638 0.8617614507675171 1.283186674118042 2.867110997438431
CurrentTrain: epoch  3, batch     2 | loss: 6.7569168Losses:  4.400482900440693 0.6721220016479492 1.0 1.434324987232685
CurrentTrain: epoch  3, batch     3 | loss: 4.4004829Losses:  5.418959729373455 0.9565975666046143 1.133009433746338 1.4293421432375908
CurrentTrain: epoch  4, batch     0 | loss: 5.4189597Losses:  4.006291389465332 0.9265819787979126 1.2111821174621582 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 4.0062914Losses:  5.548382077366114 0.881873607635498 1.180324912071228 1.4728443957865238
CurrentTrain: epoch  4, batch     2 | loss: 5.5483821Losses:  3.995606154203415 0.7981290817260742 0.5981588363647461 1.4802462756633759
CurrentTrain: epoch  4, batch     3 | loss: 3.9956062Losses:  6.315830938518047 0.8296654224395752 1.1948226690292358 2.91118573397398
CurrentTrain: epoch  5, batch     0 | loss: 6.3158309Losses:  3.768005609512329 0.9968992471694946 1.1295732259750366 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 3.7680056Losses:  5.667253497987986 0.864474892616272 1.1880522966384888 1.5021505393087864
CurrentTrain: epoch  5, batch     2 | loss: 5.6672535Losses:  5.0115042217075825 0.9675788879394531 1.206772804260254 1.6507108695805073
CurrentTrain: epoch  5, batch     3 | loss: 5.0115042Losses:  3.5517683029174805 0.8696447610855103 1.2580301761627197 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 3.5517683Losses:  5.154834885150194 0.8197389841079712 1.1964622735977173 1.4479593224823475
CurrentTrain: epoch  6, batch     1 | loss: 5.1548349Losses:  9.608301877975464 0.8758478164672852 1.014919400215149 5.898575782775879
CurrentTrain: epoch  6, batch     2 | loss: 9.6083019Losses:  4.446302890777588 0.7943792343139648 1.4456300735473633 1.4452495574951172
CurrentTrain: epoch  6, batch     3 | loss: 4.4463029Losses:  4.90491595864296 0.757665753364563 0.9494017362594604 1.4571281969547272
CurrentTrain: epoch  7, batch     0 | loss: 4.9049160Losses:  3.93354868888855 0.8709597587585449 1.060708999633789 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 3.9335487Losses:  7.41595683246851 0.8818968534469604 1.1553611755371094 4.193459369242191
CurrentTrain: epoch  7, batch     2 | loss: 7.4159568Losses:  4.247534200549126 0.8102378845214844 1.062967300415039 1.466719076037407
CurrentTrain: epoch  7, batch     3 | loss: 4.2475342Losses:  4.9059533178806305 0.772842526435852 1.0832613706588745 1.4166392385959625
CurrentTrain: epoch  8, batch     0 | loss: 4.9059533Losses:  4.932573992758989 0.8168320655822754 1.0471391677856445 1.5318803377449512
CurrentTrain: epoch  8, batch     1 | loss: 4.9325740Losses:  3.0856494903564453 0.8439925909042358 0.9082406759262085 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 3.0856495Losses:  4.760637417435646 0.5455150604248047 1.0 1.4771472364664078
CurrentTrain: epoch  8, batch     3 | loss: 4.7606374Losses:  6.159192502498627 0.909704327583313 1.0628910064697266 2.8011428713798523
CurrentTrain: epoch  9, batch     0 | loss: 6.1591925Losses:  3.458281993865967 0.8257628679275513 0.9634472131729126 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 3.4582820Losses:  8.786650776863098 0.6958426237106323 0.9931536912918091 5.6529165506362915
CurrentTrain: epoch  9, batch     2 | loss: 8.7866508Losses:  3.8673438727855682 0.24495220184326172 1.0 1.5228283107280731
CurrentTrain: epoch  9, batch     3 | loss: 3.8673439
Losses:  1.4879651069641113 0.8850281238555908 0.9506042003631592 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 1.4879651Losses:  1.2033045291900635 0.8033945560455322 1.0575300455093384 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 1.2033045Losses:  1.992588758468628 0.5628566741943359 1.0722389221191406 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 1.9925888Losses:  1.1189885139465332 0.7725212574005127 0.9530551433563232 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.1189885Losses:  1.9347329139709473 0.8002101182937622 1.100502848625183 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.9347329Losses:  1.3598955869674683 0.7344434261322021 1.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 1.3598956Losses:  1.2541162967681885 0.7763804197311401 1.112229824066162 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.2541163Losses:  1.070129156112671 0.6616648435592651 0.9138542413711548 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.0701292Losses:  1.3842631578445435 0.9147608280181885 1.052626609802246 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 1.3842632Losses:  0.9509353637695312 0.7123328447341919 1.0691248178482056 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 0.9509354Losses:  1.0011212825775146 0.7818379402160645 1.072325348854065 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 1.0011213Losses:  1.2729620933532715 0.8592607975006104 0.9334683418273926 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 1.2729621Losses:  1.0099525451660156 0.7610194683074951 1.0790711641311646 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 1.0099525Losses:  0.8850136995315552 0.6571431159973145 0.9562346935272217 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 0.8850137Losses:  1.083094835281372 0.9319429397583008 1.0461726188659668 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 1.0830948Losses:  0.9922049641609192 0.7179994583129883 1.0953718423843384 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 0.9922050Losses:  0.9008514881134033 0.6590268611907959 0.9827965497970581 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 0.9008515Losses:  0.97286057472229 0.8426060676574707 1.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 0.9728606Losses:  0.9087008237838745 0.6138085126876831 1.117485761642456 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 0.9087008Losses:  1.0162893533706665 0.8620129823684692 1.022730827331543 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 1.0162894Losses:  0.8881741762161255 0.7245333194732666 1.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 0.8881742Losses:  0.9358359575271606 0.6886910200119019 1.0884544849395752 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 0.9358360Losses:  0.8953691720962524 0.7106081247329712 1.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 0.8953692Losses:  0.8783577680587769 0.7097914218902588 1.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 0.8783578Losses:  0.91194748878479 0.7791461944580078 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 0.9119475Losses:  0.89382004737854 0.6616467237472534 1.0593510866165161 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 0.8938200Losses:  0.8301820158958435 0.6028170585632324 1.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 0.8301820Losses:  0.8791597485542297 0.7016729116439819 1.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 0.8791597Losses:  0.8716076612472534 0.6605279445648193 1.0382542610168457 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 0.8716077Losses:  0.8559389114379883 0.6754124164581299 1.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 0.8559389
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 76.52%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.48%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 81.01%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.45%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 80.25%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.15%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.62%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 87.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.83%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 87.61%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 87.28%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.29%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.99%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.01%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.21%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 87.31%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 87.59%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 87.77%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 87.93%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.08%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 87.91%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.82%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 87.66%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.58%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 87.58%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.42%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 86.90%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 86.40%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 86.26%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 85.92%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 85.65%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 85.32%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 84.93%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 84.62%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 84.24%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 83.67%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 83.11%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 82.63%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 82.29%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 82.02%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 81.95%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 81.76%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 81.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 81.13%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 80.70%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 80.53%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 80.42%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 80.25%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.85%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.22%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 78.67%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.01%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 77.53%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.18%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.94%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 78.52%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.20%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 77.93%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 77.66%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 77.26%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 77.00%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 77.63%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 77.16%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 76.74%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.42%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 76.14%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.92%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.65%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 76.12%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 75.62%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.12%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 74.63%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 74.15%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.72%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 74.20%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 73.98%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 73.95%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.88%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 73.71%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 73.52%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 73.46%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 73.33%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.20%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 73.07%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 72.97%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 72.76%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 73.15%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 73.23%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 73.45%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 73.80%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 74.01%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 74.18%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 74.21%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 74.25%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 74.25%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 74.29%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.29%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 74.14%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 74.03%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 73.86%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 73.74%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 73.54%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 73.37%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 73.17%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 73.87%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 73.88%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 73.78%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 74.43%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 75.05%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 75.03%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 74.95%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 74.92%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 75.08%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 75.42%   
cur_acc:  ['0.9484', '0.7331', '0.6845', '0.8115']
his_acc:  ['0.9484', '0.8360', '0.7719', '0.7542']
Clustering into  24  clusters
Clusters:  [23  0 19  4  3 13 20 10 22 22  4 10  8  3 16 12 15  0  7  7  1  7 18  2
 21  0  4  9  0  7  7 10  2  4 11  7  8 17  5  0  0 14 10  7  4  6  7  7
  0  1]
Losses:  9.70728287473321 1.1662209033966064 1.4933627843856812 2.0163453556597233
CurrentTrain: epoch  0, batch     0 | loss: 9.7072829Losses:  13.927340187132359 1.030006766319275 1.2870374917984009 7.071222461760044
CurrentTrain: epoch  0, batch     1 | loss: 13.9273402Losses:  10.323447238653898 1.1136492490768433 1.3395806550979614 3.0832481496036053
CurrentTrain: epoch  0, batch     2 | loss: 10.3234472Losses:  9.253101840615273 1.0710973739624023 1.1120128631591797 1.7169361263513565
CurrentTrain: epoch  0, batch     3 | loss: 9.2531018Losses:  14.43765713274479 1.0712324380874634 1.459883451461792 8.274043813347816
CurrentTrain: epoch  1, batch     0 | loss: 14.4376571Losses:  6.718783855438232 1.074955701828003 1.2927993535995483 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 6.7187839Losses:  6.708728790283203 1.117152452468872 1.1990025043487549 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 6.7087288Losses:  7.1994601264595985 1.0477075576782227 1.1379899978637695 1.5161863341927528
CurrentTrain: epoch  1, batch     3 | loss: 7.1994601Losses:  8.780894082039595 1.0136641263961792 1.338079810142517 1.5275876931846142
CurrentTrain: epoch  2, batch     0 | loss: 8.7808941Losses:  7.148234784603119 1.0409588813781738 1.2165217399597168 1.4671091437339783
CurrentTrain: epoch  2, batch     1 | loss: 7.1482348Losses:  14.378130301833153 1.0383150577545166 1.1915442943572998 9.365537509322166
CurrentTrain: epoch  2, batch     2 | loss: 14.3781303Losses:  6.521821588277817 1.0897808074951172 0.5217447280883789 1.4276667535305023
CurrentTrain: epoch  2, batch     3 | loss: 6.5218216Losses:  6.034073352813721 1.0168355703353882 1.2991126775741577 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.0340734Losses:  6.664781216531992 1.020937442779541 1.2028499841690063 1.4147850312292576
CurrentTrain: epoch  3, batch     1 | loss: 6.6647812Losses:  8.703747108578682 1.0454164743423462 1.1648106575012207 3.284090355038643
CurrentTrain: epoch  3, batch     2 | loss: 8.7037471Losses:  6.628753516823053 0.8882579803466797 1.2793464660644531 1.5457809902727604
CurrentTrain: epoch  3, batch     3 | loss: 6.6287535Losses:  12.183597054332495 1.0242071151733398 1.0800715684890747 6.631821598857641
CurrentTrain: epoch  4, batch     0 | loss: 12.1835971Losses:  6.635213766247034 1.0445468425750732 1.2109402418136597 1.661376390606165
CurrentTrain: epoch  4, batch     1 | loss: 6.6352138Losses:  6.105457816272974 0.9828431606292725 1.1218430995941162 1.5058031417429447
CurrentTrain: epoch  4, batch     2 | loss: 6.1054578Losses:  5.985133685171604 0.8390913009643555 1.045088768005371 1.4724703207612038
CurrentTrain: epoch  4, batch     3 | loss: 5.9851337Losses:  9.098075781017542 1.0380823612213135 1.1523181200027466 3.387079630047083
CurrentTrain: epoch  5, batch     0 | loss: 9.0980758Losses:  4.296249866485596 0.9815443754196167 1.1462773084640503 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 4.2962499Losses:  4.649399757385254 0.997895359992981 1.1645821332931519 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 4.6493998Losses:  4.509710848331451 1.0187406539916992 1.0 1.408692181110382
CurrentTrain: epoch  5, batch     3 | loss: 4.5097108Losses:  8.599461264908314 0.93352210521698 0.9240895509719849 3.6153933480381966
CurrentTrain: epoch  6, batch     0 | loss: 8.5994613Losses:  7.167688757181168 0.9957982301712036 1.1291472911834717 2.979431539773941
CurrentTrain: epoch  6, batch     1 | loss: 7.1676888Losses:  6.429316908121109 1.0953987836837769 1.232993483543396 1.4246129095554352
CurrentTrain: epoch  6, batch     2 | loss: 6.4293169Losses:  4.407369539141655 1.0189008712768555 1.0 1.4195417612791061
CurrentTrain: epoch  6, batch     3 | loss: 4.4073695Losses:  7.159377723932266 0.9343844652175903 1.075093388557434 2.847222477197647
CurrentTrain: epoch  7, batch     0 | loss: 7.1593777Losses:  4.553608417510986 1.0597342252731323 1.0787678956985474 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 4.5536084Losses:  5.77509930357337 0.9662624597549438 1.0583000183105469 1.4393773339688778
CurrentTrain: epoch  7, batch     2 | loss: 5.7750993Losses:  6.470143802464008 0.8306741714477539 1.0 1.4279742315411568
CurrentTrain: epoch  7, batch     3 | loss: 6.4701438Losses:  4.644621849060059 0.9181774854660034 0.9975188970565796 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 4.6446218Losses:  9.804990462958813 0.9607857465744019 1.1371543407440186 5.646724872291088
CurrentTrain: epoch  8, batch     1 | loss: 9.8049905Losses:  7.63924403488636 0.9542694091796875 1.0253416299819946 3.2700266391038895
CurrentTrain: epoch  8, batch     2 | loss: 7.6392440Losses:  4.704860806465149 0.9289731979370117 0.9683704376220703 1.3941010236740112
CurrentTrain: epoch  8, batch     3 | loss: 4.7048608Losses:  7.134247615933418 0.9411572217941284 1.1168212890625 2.9838498383760452
CurrentTrain: epoch  9, batch     0 | loss: 7.1342476Losses:  5.632094144821167 0.9956276416778564 1.007304072380066 1.450382947921753
CurrentTrain: epoch  9, batch     1 | loss: 5.6320941Losses:  4.166485786437988 0.9726999998092651 1.0680277347564697 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 4.1664858Losses:  5.007873706519604 0.8211784362792969 0.9234333038330078 1.4432741925120354
CurrentTrain: epoch  9, batch     3 | loss: 5.0078737
Losses:  1.460219144821167 0.8679064512252808 1.061112880706787 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 1.4602191Losses:  1.344275951385498 0.8537783622741699 0.9695414304733276 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 1.3442760Losses:  0.8895261287689209 0.7081143856048584 0.9883166551589966 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 0.8895261Losses:  3.352778745815158 0.5347414016723633 1.0018281936645508 1.754808260127902
MemoryTrain:  epoch  0, batch     3 | loss: 3.3527787Losses:  1.4770907163619995 0.8600507974624634 1.0 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.4770907Losses:  1.3232173919677734 0.677396297454834 1.0261719226837158 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.3232174Losses:  1.2722666263580322 0.7509759664535522 0.9855917692184448 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 1.2722666Losses:  2.5711972936987877 0.7283382415771484 1.0 1.4909373745322227
MemoryTrain:  epoch  1, batch     3 | loss: 2.5711973Losses:  1.2323466539382935 0.7407280206680298 0.9917328357696533 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.2323467Losses:  1.1258784532546997 0.8035372495651245 1.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.1258785Losses:  1.0323522090911865 0.7187515497207642 1.0112552642822266 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 1.0323522Losses:  2.509758934378624 0.9222660064697266 1.0 1.451548919081688
MemoryTrain:  epoch  2, batch     3 | loss: 2.5097589Losses:  1.1747735738754272 0.8616611957550049 1.0236926078796387 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.1747736Losses:  0.9790540933609009 0.7343354225158691 1.0 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 0.9790541Losses:  0.9670217037200928 0.7182109355926514 1.0079717636108398 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 0.9670217Losses:  2.1462182253599167 0.3623056411743164 1.0 1.426398441195488
MemoryTrain:  epoch  3, batch     3 | loss: 2.1462182Losses:  0.9809519648551941 0.7589733600616455 1.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 0.9809520Losses:  0.9605597257614136 0.8009432554244995 1.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 0.9605597Losses:  0.9849170446395874 0.6758178472518921 1.0209441184997559 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 0.9849170Losses:  2.515277773141861 0.8786325454711914 1.0 1.4546431601047516
MemoryTrain:  epoch  4, batch     3 | loss: 2.5152778Losses:  0.9087408781051636 0.6657086610794067 0.9973874092102051 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 0.9087409Losses:  0.9613251686096191 0.699164628982544 1.0356237888336182 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 0.9613252Losses:  0.9747306704521179 0.8354028463363647 0.9818532466888428 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 0.9747307Losses:  2.6804842799901962 1.107065200805664 1.1781072616577148 1.5180427879095078
MemoryTrain:  epoch  5, batch     3 | loss: 2.6804843Losses:  0.962581217288971 0.7833386659622192 1.0256916284561157 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 0.9625812Losses:  1.0187376737594604 0.7386531829833984 1.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 1.0187377Losses:  0.877086877822876 0.6800180673599243 1.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 0.8770869Losses:  2.357480950653553 0.7156352996826172 1.0 1.469148464500904
MemoryTrain:  epoch  6, batch     3 | loss: 2.3574810Losses:  0.9254430532455444 0.7565994262695312 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 0.9254431Losses:  0.8740121126174927 0.6735521554946899 0.9827649593353271 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 0.8740121Losses:  0.898444652557373 0.7146751880645752 1.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 0.8984447Losses:  2.504168875515461 0.8494091033935547 1.0 1.4997760131955147
MemoryTrain:  epoch  7, batch     3 | loss: 2.5041689Losses:  0.8636647462844849 0.6355173587799072 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 0.8636647Losses:  0.9243490695953369 0.7453410625457764 1.009990930557251 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 0.9243491Losses:  0.8999985456466675 0.6950756311416626 1.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 0.8999985Losses:  2.3660906180739403 0.7957859039306641 1.0 1.4278007373213768
MemoryTrain:  epoch  8, batch     3 | loss: 2.3660906Losses:  0.9191318154335022 0.6936650276184082 1.0215667486190796 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 0.9191318Losses:  0.8611494302749634 0.6368041038513184 1.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 0.8611494Losses:  0.9111847281455994 0.7396893501281738 1.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 0.9111847Losses:  2.3826559334993362 0.7592172622680664 1.0 1.441750481724739
MemoryTrain:  epoch  9, batch     3 | loss: 2.3826559
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 48.96%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 60.88%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 60.27%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 60.13%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 59.79%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 59.27%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 59.28%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 58.27%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 57.68%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 56.77%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 55.41%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 55.76%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 56.41%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 56.88%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 57.47%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 58.58%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 59.09%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 59.03%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 60.84%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.72%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.86%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 85.09%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 84.81%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.85%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.79%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.63%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.68%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 84.86%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 85.45%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 85.57%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 86.22%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 86.23%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 86.17%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 86.02%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 85.63%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.28%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 85.23%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 84.76%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 84.19%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 83.60%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 83.43%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 83.05%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 82.65%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 82.28%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 81.93%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 81.65%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 81.18%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 80.66%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 80.27%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 80.03%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 79.61%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 79.12%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.90%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 78.74%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 78.37%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.27%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 78.18%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 77.80%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.20%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 76.02%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 75.51%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.11%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 74.94%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 76.64%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 76.28%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 75.93%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 75.68%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.19%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 74.86%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 74.86%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.19%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.64%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 75.63%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 75.18%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 74.82%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 74.51%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 74.30%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 74.08%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.83%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 73.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.30%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 73.81%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.33%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 72.85%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 72.38%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.96%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 72.52%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 72.46%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 72.25%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 71.89%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 71.84%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 71.86%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 71.73%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 71.57%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 71.52%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 71.52%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 71.40%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 71.28%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 71.13%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 71.51%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 72.53%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 72.94%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.97%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 72.82%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 72.64%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 72.48%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 72.31%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 72.14%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 71.98%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 71.76%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 72.44%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 72.42%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 72.43%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 72.33%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.74%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 73.81%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 73.79%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 73.82%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 74.05%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 73.78%   [EVAL] batch:  252 | acc: 18.75%,  total acc: 73.57%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 73.38%   [EVAL] batch:  254 | acc: 12.50%,  total acc: 73.14%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 72.92%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 72.86%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.90%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 72.82%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 72.77%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 72.74%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 72.65%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 72.80%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 73.19%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 73.04%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 72.93%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 72.87%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 72.69%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 72.65%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 72.59%   [EVAL] batch:  283 | acc: 25.00%,  total acc: 72.43%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 72.30%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 71.91%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 72.03%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 72.17%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 73.22%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 73.14%   
cur_acc:  ['0.9484', '0.7331', '0.6845', '0.8115', '0.6835']
his_acc:  ['0.9484', '0.8360', '0.7719', '0.7542', '0.7314']
Clustering into  29  clusters
Clusters:  [23  0 25  2 20 27 24  6  5  5  2  1  1 20 21 16 15  0  9  9  4  9 18 10
 11  0  2 14  0  9  9  6 10  2 28  9  1 19 13  0  0 17  6  9  2  8  9  9
  0  4 22  9  7  6  9  9  3 26  9 12]
Losses:  7.749142646789551 1.048435926437378 1.144972562789917 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 7.7491426Losses:  9.067442461848259 1.0047286748886108 1.2369470596313477 1.62929205596447
CurrentTrain: epoch  0, batch     1 | loss: 9.0674425Losses:  9.03202149271965 1.0874205827713013 1.0941358804702759 1.5207752883434296
CurrentTrain: epoch  0, batch     2 | loss: 9.0320215Losses:  10.792137013748288 1.0571699142456055 1.4231863021850586 1.7907007802277803
CurrentTrain: epoch  0, batch     3 | loss: 10.7921370Losses:  7.911523640155792 1.0381184816360474 1.0318843126296997 1.422413170337677
CurrentTrain: epoch  1, batch     0 | loss: 7.9115236Losses:  8.54958177357912 1.0183970928192139 1.1169315576553345 1.7371213510632515
CurrentTrain: epoch  1, batch     1 | loss: 8.5495818Losses:  5.76411771774292 1.0172442197799683 1.011644959449768 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 5.7641177Losses:  6.842666335403919 1.0714349746704102 1.0 1.5730978921055794
CurrentTrain: epoch  1, batch     3 | loss: 6.8426663Losses:  7.499662704765797 1.0623297691345215 1.0793193578720093 1.8450720980763435
CurrentTrain: epoch  2, batch     0 | loss: 7.4996627Losses:  6.233860969543457 0.9595174789428711 1.2221533060073853 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 6.2338610Losses:  8.503265339881182 1.020882248878479 1.128638505935669 2.904252488166094
CurrentTrain: epoch  2, batch     2 | loss: 8.5032653Losses:  13.315877303481102 1.0 1.0 9.019437178969383
CurrentTrain: epoch  2, batch     3 | loss: 13.3158773Losses:  9.837798148393631 1.0147089958190918 1.0649497509002686 4.426332026720047
CurrentTrain: epoch  3, batch     0 | loss: 9.8377981Losses:  7.468792383559048 1.0082734823226929 1.1079891920089722 2.2674493240192533
CurrentTrain: epoch  3, batch     1 | loss: 7.4687924Losses:  6.946909546852112 0.9840761423110962 1.0454214811325073 1.4922744035720825
CurrentTrain: epoch  3, batch     2 | loss: 6.9469095Losses:  8.300200156867504 0.9058666229248047 1.2152013778686523 1.5115205571055412
CurrentTrain: epoch  3, batch     3 | loss: 8.3002002Losses:  6.230324298143387 1.0145128965377808 1.040450930595398 1.4434085190296173
CurrentTrain: epoch  4, batch     0 | loss: 6.2303243Losses:  5.655857086181641 0.9560668468475342 0.9911761283874512 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 5.6558571Losses:  6.607566177845001 0.952674150466919 1.076508641242981 1.44671231508255
CurrentTrain: epoch  4, batch     2 | loss: 6.6075662Losses:  5.009565278887749 1.0446414947509766 1.0 1.4737626761198044
CurrentTrain: epoch  4, batch     3 | loss: 5.0095653Losses:  12.83842933177948 0.949684739112854 1.0371623039245605 8.703823924064636
CurrentTrain: epoch  5, batch     0 | loss: 12.8384293Losses:  6.252620868384838 0.9315582513809204 0.9051511287689209 1.4540287777781487
CurrentTrain: epoch  5, batch     1 | loss: 6.2526209Losses:  11.96107292175293 1.0146877765655518 1.1302690505981445 6.293666839599609
CurrentTrain: epoch  5, batch     2 | loss: 11.9610729Losses:  4.703323811292648 1.0 0.6840887069702148 1.4266009032726288
CurrentTrain: epoch  5, batch     3 | loss: 4.7033238Losses:  7.408787190914154 1.0153919458389282 1.0298007726669312 3.0576462149620056
CurrentTrain: epoch  6, batch     0 | loss: 7.4087872Losses:  7.8357439041137695 0.9580512046813965 1.0044282674789429 2.8610053062438965
CurrentTrain: epoch  6, batch     1 | loss: 7.8357439Losses:  6.047589361667633 0.9164307117462158 1.052530288696289 1.4100914597511292
CurrentTrain: epoch  6, batch     2 | loss: 6.0475894Losses:  5.931832864880562 0.8750419616699219 0.9974842071533203 1.5216518193483353
CurrentTrain: epoch  6, batch     3 | loss: 5.9318329Losses:  8.330004211515188 0.9601610898971558 1.012386441230774 2.9321045838296413
CurrentTrain: epoch  7, batch     0 | loss: 8.3300042Losses:  7.153278294950724 0.95562744140625 1.0389444828033447 2.9216184057295322
CurrentTrain: epoch  7, batch     1 | loss: 7.1532783Losses:  9.081399913877249 0.9205540418624878 1.0990042686462402 4.905961509793997
CurrentTrain: epoch  7, batch     2 | loss: 9.0813999Losses:  4.7751903012394905 0.957188606262207 1.0 1.432882733643055
CurrentTrain: epoch  7, batch     3 | loss: 4.7751903Losses:  6.085509806871414 0.9177554845809937 1.027103066444397 1.4072861969470978
CurrentTrain: epoch  8, batch     0 | loss: 6.0855098Losses:  7.0037555657327175 0.9467247724533081 1.0059441328048706 2.856964584439993
CurrentTrain: epoch  8, batch     1 | loss: 7.0037556Losses:  7.162605494260788 0.9711228609085083 1.0116573572158813 2.937866896390915
CurrentTrain: epoch  8, batch     2 | loss: 7.1626055Losses:  4.511382021009922 0.9416799545288086 1.0 1.4845575466752052
CurrentTrain: epoch  8, batch     3 | loss: 4.5113820Losses:  6.8389019928872585 0.952484130859375 1.0044654607772827 2.845976110547781
CurrentTrain: epoch  9, batch     0 | loss: 6.8389020Losses:  5.3627690076828 0.9387708902359009 0.9998033046722412 1.4108065366744995
CurrentTrain: epoch  9, batch     1 | loss: 5.3627690Losses:  4.506171703338623 0.9114270210266113 1.032656192779541 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 4.5061717Losses:  9.248056277632713 1.0 1.0 6.274267062544823
CurrentTrain: epoch  9, batch     3 | loss: 9.2480563
Losses:  1.4921088218688965 0.7884641885757446 1.0819834470748901 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 1.4921088Losses:  1.0133967399597168 0.7820380926132202 0.9208875894546509 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 1.0133967Losses:  1.4897570610046387 0.722655177116394 0.9999583959579468 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 1.4897571Losses:  1.2150505781173706 0.7540076971054077 1.051605463027954 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 1.2150506Losses:  1.7061436176300049 0.732579231262207 1.0229889154434204 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.7061436Losses:  1.7681833505630493 0.8250560760498047 0.9966130256652832 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.7681834Losses:  1.0316407680511475 0.7715526819229126 1.0 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 1.0316408Losses:  1.5418727397918701 0.6276664733886719 1.0793966054916382 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 1.5418727Losses:  1.2101125717163086 0.7246787548065186 0.9773839712142944 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.2101126Losses:  1.2660608291625977 0.8245972394943237 1.0 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.2660608Losses:  1.0381730794906616 0.6488767862319946 1.0 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 1.0381731Losses:  1.3756213188171387 0.7673290967941284 0.9409736394882202 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 1.3756213Losses:  1.034805178642273 0.6429691314697266 1.0396920442581177 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.0348052Losses:  1.0575659275054932 0.77229905128479 1.003975749015808 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 1.0575659Losses:  1.106696367263794 0.7603508234024048 1.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 1.1066964Losses:  0.9395734667778015 0.6848909258842468 1.0 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 0.9395735Losses:  0.9960200786590576 0.8133044242858887 1.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 0.9960201Losses:  0.9054397344589233 0.6270838975906372 1.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 0.9054397Losses:  0.9113449454307556 0.627386212348938 0.9651505947113037 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 0.9113449Losses:  1.0128185749053955 0.7871346473693848 1.0 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 1.0128186Losses:  0.9073854088783264 0.6362780332565308 0.9837824106216431 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 0.9073854Losses:  0.9248749017715454 0.6289721727371216 1.0 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 0.9248749Losses:  0.9640558958053589 0.7799710035324097 1.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 0.9640559Losses:  1.2780334949493408 0.7856000661849976 1.0324034690856934 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 1.2780335Losses:  0.9585365056991577 0.7251391410827637 1.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 0.9585365Losses:  0.9246581196784973 0.7098040580749512 0.9826772212982178 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 0.9246581Losses:  0.8665059208869934 0.6228287220001221 1.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 0.8665059Losses:  1.0702648162841797 0.6589142680168152 0.9880927801132202 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 1.0702648Losses:  0.9817554950714111 0.7601503133773804 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 0.9817555Losses:  0.8610631227493286 0.6260708570480347 1.0 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 0.8610631Losses:  0.8596588373184204 0.644849419593811 1.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 0.8596588Losses:  0.8575536012649536 0.6191900968551636 1.0 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 0.8575536Losses:  0.8512293100357056 0.6107549667358398 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 0.8512293Losses:  0.8978783488273621 0.6929508447647095 1.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 0.8978783Losses:  0.8764958381652832 0.6662074327468872 1.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 0.8764958Losses:  0.8547155857086182 0.6954101324081421 0.9460711479187012 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 0.8547156Losses:  0.9197426438331604 0.7665892839431763 1.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 0.9197426Losses:  0.7581927180290222 0.4534720182418823 1.0 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 0.7581927Losses:  0.9028549790382385 0.7193981409072876 1.0214236974716187 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 0.9028550Losses:  0.9463309049606323 0.7144169807434082 1.048999309539795 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 0.9463309
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 75.66%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 18.75%,  total acc: 72.81%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 68.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 71.27%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 70.57%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 70.52%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.63%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 81.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 82.87%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 82.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 82.57%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.90%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 81.45%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 81.45%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 80.97%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 80.97%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 80.71%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 81.50%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 81.41%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 80.84%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 80.69%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 80.55%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 80.40%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 80.18%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 79.67%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 79.19%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 79.14%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 78.81%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 78.62%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 78.51%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 78.40%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 77.72%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 77.49%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 77.13%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 76.64%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 76.24%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 75.97%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 75.19%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 74.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 74.26%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 74.09%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 73.98%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.82%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 73.48%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.42%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 71.45%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 71.09%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 70.96%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 72.52%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 72.68%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 72.42%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 71.73%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 71.51%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 71.15%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 70.85%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 70.93%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 71.45%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 71.12%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 70.64%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.23%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 70.86%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 70.39%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.93%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 69.48%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 69.03%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.59%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 69.02%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 68.67%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 68.37%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 68.04%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 67.75%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 67.49%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 67.32%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 67.32%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 67.15%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 66.98%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 66.76%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 66.81%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 66.79%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.03%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  194 | acc: 18.75%,  total acc: 67.98%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 67.73%   [EVAL] batch:  196 | acc: 31.25%,  total acc: 67.54%   [EVAL] batch:  197 | acc: 18.75%,  total acc: 67.30%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 67.02%   [EVAL] batch:  199 | acc: 18.75%,  total acc: 66.78%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 66.73%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 66.78%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 66.70%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 66.62%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 66.04%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 66.90%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 66.98%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.75%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 68.67%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 68.88%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 69.30%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 69.05%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 68.82%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 68.68%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 68.50%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 68.26%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 68.17%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 68.24%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 68.18%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 68.09%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 68.00%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 68.61%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 68.46%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 68.35%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 68.24%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:  281 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 67.98%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 67.78%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 67.63%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.44%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 67.23%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 67.17%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 67.18%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 67.26%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 67.27%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 67.15%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 68.13%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 67.98%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 67.84%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 67.71%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 67.59%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 67.50%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 67.87%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 67.82%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 67.84%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 67.81%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 68.38%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 68.73%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 69.11%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 68.93%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 68.79%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 68.66%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 68.49%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 68.35%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 68.72%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 68.67%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 68.61%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 68.67%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 68.70%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 68.83%   
cur_acc:  ['0.9484', '0.7331', '0.6845', '0.8115', '0.6835', '0.7063']
his_acc:  ['0.9484', '0.8360', '0.7719', '0.7542', '0.7314', '0.6883']
Clustering into  34  clusters
Clusters:  [23  2 21 32  1 27 20  0 15 15 26 28 10  1 22 13 31  2  7  7  5  7 18  4
 25  2 26 24  2  7  7  0  4  3 29  7 10 19 30  2  2 17  0  7  3 14 12  7
  6 12  9  7 33  0  7  7  8 16  3 11  2  7 26 10  5  7 12  7 12  7]
Losses:  10.31301735341549 1.1925852298736572 1.571650505065918 2.4199495166540146
CurrentTrain: epoch  0, batch     0 | loss: 10.3130174Losses:  9.89156538899988 1.124601125717163 1.2696256637573242 1.9934383099898696
CurrentTrain: epoch  0, batch     1 | loss: 9.8915654Losses:  12.416001625359058 1.005979299545288 1.1811612844467163 4.9878614619374275
CurrentTrain: epoch  0, batch     2 | loss: 12.4160016Losses:  12.511873612180352 1.2369747161865234 2.047138214111328 2.0571550223976374
CurrentTrain: epoch  0, batch     3 | loss: 12.5118736Losses:  8.461906045675278 1.1265372037887573 1.3781849145889282 1.4112792909145355
CurrentTrain: epoch  1, batch     0 | loss: 8.4619060Losses:  7.031487941741943 1.1096371412277222 1.3845006227493286 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 7.0314879Losses:  8.49513989686966 0.993073582649231 1.1163638830184937 1.8046382069587708
CurrentTrain: epoch  1, batch     2 | loss: 8.4951399Losses:  8.512421540915966 1.559412956237793 1.531702995300293 1.4307088181376457
CurrentTrain: epoch  1, batch     3 | loss: 8.5124215Losses:  9.570277594029903 1.0798927545547485 1.217306137084961 3.33065452426672
CurrentTrain: epoch  2, batch     0 | loss: 9.5702776Losses:  8.1716128885746 1.041430950164795 1.2849888801574707 1.4199496805667877
CurrentTrain: epoch  2, batch     1 | loss: 8.1716129Losses:  9.476221263408661 1.1225056648254395 1.4701734781265259 2.1029593348503113
CurrentTrain: epoch  2, batch     2 | loss: 9.4762213Losses:  9.00180321931839 1.05828857421875 1.483870506286621 1.4940913319587708
CurrentTrain: epoch  2, batch     3 | loss: 9.0018032Losses:  6.34954833984375 1.0504155158996582 1.3853400945663452 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.3495483Losses:  11.633265674114227 1.1193269491195679 1.422782063484192 4.385736644268036
CurrentTrain: epoch  3, batch     1 | loss: 11.6332657Losses:  7.476095475256443 0.938628077507019 1.2196986675262451 1.6071150675415993
CurrentTrain: epoch  3, batch     2 | loss: 7.4760955Losses:  10.085908904671669 1.2737798690795898 1.3622312545776367 1.5523729473352432
CurrentTrain: epoch  3, batch     3 | loss: 10.0859089Losses:  9.78254797309637 1.143860101699829 1.418131947517395 3.053257964551449
CurrentTrain: epoch  4, batch     0 | loss: 9.7825480Losses:  7.815887480974197 0.9568449258804321 1.278157353401184 2.064419776201248
CurrentTrain: epoch  4, batch     1 | loss: 7.8158875Losses:  6.4811811447143555 1.0286692380905151 1.2661380767822266 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.4811811Losses:  6.495687790215015 0.9881076812744141 1.3855876922607422 1.4856808856129646
CurrentTrain: epoch  4, batch     3 | loss: 6.4956878Losses:  6.477861404418945 1.116779088973999 1.3862727880477905 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 6.4778614Losses:  10.824387580156326 0.9982147216796875 1.2064595222473145 4.777119666337967
CurrentTrain: epoch  5, batch     1 | loss: 10.8243876Losses:  5.64451265335083 0.9894713163375854 1.2660733461380005 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 5.6445127Losses:  8.04682545363903 0.7904138565063477 1.0852127075195312 1.5005827397108078
CurrentTrain: epoch  5, batch     3 | loss: 8.0468255Losses:  9.06543674506247 0.8607287406921387 1.1890755891799927 3.1895288471132517
CurrentTrain: epoch  6, batch     0 | loss: 9.0654367Losses:  16.404446236789227 1.0889461040496826 1.3706417083740234 10.0378123447299
CurrentTrain: epoch  6, batch     1 | loss: 16.4044462Losses:  11.558715507388115 0.9869579076766968 1.189702033996582 6.057165786623955
CurrentTrain: epoch  6, batch     2 | loss: 11.5587155Losses:  6.185799174010754 1.207315444946289 1.1184558868408203 1.4449611231684685
CurrentTrain: epoch  6, batch     3 | loss: 6.1857992Losses:  8.248717200011015 0.8866431713104248 1.158380389213562 2.941139589995146
CurrentTrain: epoch  7, batch     0 | loss: 8.2487172Losses:  7.399597726762295 1.0613304376602173 1.3937560319900513 1.4604884013533592
CurrentTrain: epoch  7, batch     1 | loss: 7.3995977Losses:  9.312521122395992 1.0728620290756226 1.33378267288208 4.068618915975094
CurrentTrain: epoch  7, batch     2 | loss: 9.3125211Losses:  7.32569121196866 0.8098583221435547 0.9703407287597656 1.8009519465267658
CurrentTrain: epoch  7, batch     3 | loss: 7.3256912Losses:  8.869104146957397 0.9401675462722778 1.2042813301086426 2.991440534591675
CurrentTrain: epoch  8, batch     0 | loss: 8.8691041Losses:  8.000485870987177 0.957650899887085 1.1877690553665161 2.9459728933870792
CurrentTrain: epoch  8, batch     1 | loss: 8.0004859Losses:  12.790019541978836 1.038895845413208 1.4015599489212036 7.797852069139481
CurrentTrain: epoch  8, batch     2 | loss: 12.7900195Losses:  6.549961410462856 1.442225456237793 1.6923456192016602 1.5203016623854637
CurrentTrain: epoch  8, batch     3 | loss: 6.5499614Losses:  5.145280838012695 0.8529187440872192 1.2336106300354004 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 5.1452808Losses:  4.862335681915283 1.1449553966522217 1.3231555223464966 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.8623357Losses:  6.954681485891342 0.9450293779373169 1.2377794981002808 1.430384248495102
CurrentTrain: epoch  9, batch     2 | loss: 6.9546815Losses:  5.4555190578103065 0.9114055633544922 0.9584236145019531 1.4486506953835487
CurrentTrain: epoch  9, batch     3 | loss: 5.4555191
Losses:  1.079329490661621 0.7506424188613892 1.0344343185424805 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 1.0793295Losses:  1.3051612377166748 0.7150014638900757 1.1053491830825806 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 1.3051612Losses:  1.04831862449646 0.8299187421798706 1.0685611963272095 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 1.0483186Losses:  1.0474374294281006 0.7911547422409058 0.9559979438781738 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 1.0474374Losses:  5.912103198468685 0.733653724193573 0.9479573965072632 4.422157548367977
MemoryTrain:  epoch  0, batch     4 | loss: 5.9121032Losses:  1.7086660861968994 0.7928749322891235 1.1009796857833862 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.7086661Losses:  1.355830430984497 0.7677583694458008 1.0772547721862793 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.3558304Losses:  1.046294927597046 0.719760537147522 1.0061951875686646 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 1.0462949Losses:  1.0990691184997559 0.7617512941360474 1.0234155654907227 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 1.0990691Losses:  5.404659805819392 0.7774588465690613 1.0 4.327361999079585
MemoryTrain:  epoch  1, batch     4 | loss: 5.4046598Losses:  1.0973501205444336 0.845711350440979 0.9375 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.0973501Losses:  0.9484720826148987 0.5571444034576416 0.9938545227050781 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 0.9484721Losses:  1.0218493938446045 0.8447153568267822 1.027118444442749 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 1.0218494Losses:  1.3113737106323242 0.7743309736251831 1.069228172302246 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 1.3113737Losses:  5.154759202152491 0.5721883773803711 1.0 4.32452330365777
MemoryTrain:  epoch  2, batch     4 | loss: 5.1547592Losses:  1.0512317419052124 0.802527904510498 1.0058186054229736 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 1.0512317Losses:  0.9222339987754822 0.5837653875350952 1.0310685634613037 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 0.9222340Losses:  1.0379801988601685 0.7747836112976074 1.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 1.0379802Losses:  1.0562751293182373 0.8280767202377319 1.0259698629379272 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 1.0562751Losses:  5.25000574067235 0.541037917137146 1.0 4.371631283313036
MemoryTrain:  epoch  3, batch     4 | loss: 5.2500057Losses:  0.9011629223823547 0.6680611371994019 1.0 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 0.9011629Losses:  0.9265528917312622 0.7443339824676514 1.0 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 0.9265529Losses:  0.9375813603401184 0.72672438621521 1.0 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 0.9375814Losses:  0.9750890135765076 0.7375677824020386 1.0526764392852783 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 0.9750890Losses:  5.181937728077173 0.6901992559432983 1.0 4.256943259388208
MemoryTrain:  epoch  4, batch     4 | loss: 5.1819377Losses:  0.9838718175888062 0.8161816596984863 1.0519862174987793 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 0.9838718Losses:  0.9564598798751831 0.7775405645370483 0.9900370836257935 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 0.9564599Losses:  0.8693666458129883 0.6198924779891968 1.0234700441360474 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 0.8693666Losses:  0.8883043527603149 0.6135972738265991 1.0261415243148804 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 0.8883044Losses:  5.250730939209461 0.8231258392333984 1.0 4.309723503887653
MemoryTrain:  epoch  5, batch     4 | loss: 5.2507309Losses:  0.9148203730583191 0.7295113801956177 0.9894614219665527 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 0.9148204Losses:  0.9411101341247559 0.7958654165267944 0.980293869972229 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 0.9411101Losses:  0.867999792098999 0.656598687171936 1.0 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 0.8679998Losses:  0.8659402132034302 0.6558247804641724 1.0 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 0.8659402Losses:  5.012563645839691 0.5385755300521851 1.0 4.200455129146576
MemoryTrain:  epoch  6, batch     4 | loss: 5.0125636Losses:  0.8798895478248596 0.6723685264587402 1.0064961910247803 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 0.8798895Losses:  0.8475579023361206 0.5646955966949463 1.0303925275802612 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 0.8475579Losses:  0.8827706575393677 0.6760309934616089 1.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 0.8827707Losses:  0.9727725982666016 0.8631683588027954 1.0 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 0.9727726Losses:  5.054780021309853 0.4566793441772461 1.0 4.305053249001503
MemoryTrain:  epoch  7, batch     4 | loss: 5.0547800Losses:  0.8499840497970581 0.6340049505233765 1.0 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 0.8499840Losses:  0.8873409032821655 0.6889219284057617 1.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 0.8873409Losses:  0.8764284253120422 0.6071027517318726 1.033728003501892 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 0.8764284Losses:  0.9176632761955261 0.7609188556671143 1.0 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 0.9176633Losses:  5.20523401722312 0.6288652420043945 1.0 4.359044399112463
MemoryTrain:  epoch  8, batch     4 | loss: 5.2052340Losses:  0.8461809158325195 0.6037591695785522 1.0 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 0.8461809Losses:  0.9286361932754517 0.7238569259643555 1.0552716255187988 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 0.9286362Losses:  0.8742905855178833 0.6508545875549316 1.0081167221069336 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 0.8742906Losses:  0.8777865767478943 0.6963626146316528 1.0 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 0.8777866Losses:  5.146129425615072 0.5910600423812866 1.0 4.310491140931845
MemoryTrain:  epoch  9, batch     4 | loss: 5.1461294
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 47.77%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 54.37%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 52.27%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 50.52%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 49.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 48.08%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.30%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 44.64%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 43.10%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 41.67%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 40.32%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 42.80%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 44.30%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 45.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 47.05%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 48.31%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 49.51%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 50.48%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 51.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 52.90%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 53.72%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 54.80%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.68%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 56.79%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 57.18%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 57.55%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 58.16%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 58.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.89%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 59.20%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 60.11%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 60.49%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 60.20%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 59.48%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 59.22%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 58.50%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 58.57%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 57.84%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 77.64%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 77.66%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 77.73%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 78.06%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.73%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 78.23%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 78.07%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.18%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 78.27%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 77.94%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 78.08%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 77.85%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 78.30%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 78.67%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 78.54%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 78.00%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 77.88%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 77.85%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 77.73%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 77.29%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 76.81%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 76.40%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 76.31%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 76.01%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 75.71%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 75.42%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 75.07%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 74.52%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 73.98%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 73.59%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 73.14%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 72.20%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 71.91%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 71.49%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 71.15%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 70.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 70.42%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 70.40%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 70.27%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 69.80%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 69.33%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 68.35%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 68.02%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 67.75%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 67.59%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 69.62%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 69.54%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 69.34%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 69.14%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 68.80%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.88%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 69.51%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 68.93%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 69.04%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 68.59%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.14%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 67.69%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 67.26%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 66.83%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  157 | acc: 75.00%,  total acc: 66.93%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 67.02%   [EVAL] batch:  159 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 67.18%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 66.81%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 66.48%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 65.48%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 65.24%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 65.22%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 65.11%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 65.02%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 64.71%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 64.61%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.66%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.82%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 65.59%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 66.49%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 66.31%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 65.75%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 65.72%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 65.47%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 65.40%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 65.11%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 64.94%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 65.93%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 65.92%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 65.96%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 67.95%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 68.45%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 68.25%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 68.01%   [EVAL] batch:  253 | acc: 25.00%,  total acc: 67.84%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 67.70%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 67.48%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 67.39%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.42%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.37%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 67.35%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 67.31%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 67.22%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 67.04%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 67.69%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:  277 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 67.17%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 66.84%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 66.64%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 66.46%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:  287 | acc: 18.75%,  total acc: 66.08%   [EVAL] batch:  288 | acc: 18.75%,  total acc: 65.92%   [EVAL] batch:  289 | acc: 18.75%,  total acc: 65.75%   [EVAL] batch:  290 | acc: 25.00%,  total acc: 65.61%   [EVAL] batch:  291 | acc: 25.00%,  total acc: 65.48%   [EVAL] batch:  292 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 65.31%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 65.09%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 65.04%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 66.06%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 65.89%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 65.76%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 65.60%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 65.47%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 65.66%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 65.84%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 66.68%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 67.12%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 67.01%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 66.93%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 66.76%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 66.66%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 67.17%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 67.09%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 67.02%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.02%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 67.12%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 66.95%   [EVAL] batch:  376 | acc: 0.00%,  total acc: 66.78%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 66.62%   [EVAL] batch:  378 | acc: 0.00%,  total acc: 66.44%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 66.11%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 66.08%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 66.34%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.40%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 66.47%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 66.38%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 66.29%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 66.22%   [EVAL] batch:  398 | acc: 25.00%,  total acc: 66.12%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 66.03%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.88%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 65.72%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 65.39%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 65.23%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  407 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.63%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 66.14%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  432 | acc: 18.75%,  total acc: 66.09%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  434 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:  435 | acc: 31.25%,  total acc: 65.91%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 65.90%   [EVAL] batch:  437 | acc: 12.50%,  total acc: 65.78%   
cur_acc:  ['0.9484', '0.7331', '0.6845', '0.8115', '0.6835', '0.7063', '0.5784']
his_acc:  ['0.9484', '0.8360', '0.7719', '0.7542', '0.7314', '0.6883', '0.6578']
Clustering into  39  clusters
Clusters:  [31  0 23  2  1 35 24 10 14 14  8 38  7  1 25 20 19  0  3  3  9  3 22  3
 26  0  8 29  0  3  3  2 12  6 27  3  7 33 28  0  0 21  3  3 10 37  5  3
 34  5 36  3 13 17  3  3  4 18  6 32  0  3  8  7  9  3  5  3  5  3 11 30
 15  3  3  3 16  4  3  4]
Losses:  11.34533330053091 0.9515422582626343 1.237507939338684 4.339103423058987
CurrentTrain: epoch  0, batch     0 | loss: 11.3453333Losses:  12.063015133142471 0.811004638671875 1.4174525737762451 6.142357498407364
CurrentTrain: epoch  0, batch     1 | loss: 12.0630151Losses:  10.41152460873127 1.2209104299545288 1.4611592292785645 1.6937368661165237
CurrentTrain: epoch  0, batch     2 | loss: 10.4115246Losses:  21.17789937183261 1.0 1.1836614608764648 13.941627513617277
CurrentTrain: epoch  0, batch     3 | loss: 21.1778994Losses:  13.389123283326626 1.0153169631958008 1.4616448879241943 6.61322196573019
CurrentTrain: epoch  1, batch     0 | loss: 13.3891233Losses:  8.132981091737747 0.9960812330245972 1.2208102941513062 1.4267485439777374
CurrentTrain: epoch  1, batch     1 | loss: 8.1329811Losses:  7.498057991266251 0.8690005540847778 1.1916025876998901 1.4269262850284576
CurrentTrain: epoch  1, batch     2 | loss: 7.4980580Losses:  8.067028380930424 1.1230621337890625 1.1150341033935547 1.5355571284890175
CurrentTrain: epoch  1, batch     3 | loss: 8.0670284Losses:  10.660485908389091 0.9518799781799316 1.3039456605911255 4.032951042056084
CurrentTrain: epoch  2, batch     0 | loss: 10.6604859Losses:  6.454588234424591 0.7745481729507446 1.1686851978302002 1.4207895398139954
CurrentTrain: epoch  2, batch     1 | loss: 6.4545882Losses:  12.729962788522243 1.2264084815979004 1.379840612411499 5.345138989388943
CurrentTrain: epoch  2, batch     2 | loss: 12.7299628Losses:  5.172353446483612 0.6409702301025391 1.4028215408325195 1.424430787563324
CurrentTrain: epoch  2, batch     3 | loss: 5.1723534Losses:  6.097801208496094 0.969113826751709 1.2528554201126099 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 6.0978012Losses:  6.827848017215729 0.9078491926193237 1.0927315950393677 1.4124069809913635
CurrentTrain: epoch  3, batch     1 | loss: 6.8278480Losses:  6.040431499481201 0.917603611946106 1.4367409944534302 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 6.0404315Losses:  10.249615475535393 1.3463258743286133 1.2096977233886719 1.4519117325544357
CurrentTrain: epoch  3, batch     3 | loss: 10.2496155Losses:  7.799482703208923 0.9977538585662842 1.1906448602676392 1.4242604970932007
CurrentTrain: epoch  4, batch     0 | loss: 7.7994827Losses:  5.3059234619140625 0.8405871391296387 1.218038558959961 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 5.3059235Losses:  10.000598264858127 0.9177032709121704 1.107670545578003 4.980157686397433
CurrentTrain: epoch  4, batch     2 | loss: 10.0005983Losses:  10.010116763412952 1.2833003997802734 1.6763525009155273 1.5060693696141243
CurrentTrain: epoch  4, batch     3 | loss: 10.0101168Losses:  6.929183930158615 1.0017659664154053 1.3112571239471436 1.4010338485240936
CurrentTrain: epoch  5, batch     0 | loss: 6.9291839Losses:  9.088564787060022 0.8719650506973267 1.0724661350250244 4.280122194439173
CurrentTrain: epoch  5, batch     1 | loss: 9.0885648Losses:  7.558689385652542 0.9468804597854614 1.3314989805221558 1.4133751690387726
CurrentTrain: epoch  5, batch     2 | loss: 7.5586894Losses:  6.594623409211636 1.09039306640625 0.6715488433837891 1.4385312423110008
CurrentTrain: epoch  5, batch     3 | loss: 6.5946234Losses:  5.341982841491699 0.9830095767974854 1.1619168519973755 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 5.3419828Losses:  7.843791224062443 0.9583530426025391 1.0754327774047852 2.876708723604679
CurrentTrain: epoch  6, batch     1 | loss: 7.8437912Losses:  5.089936256408691 0.8440966606140137 1.2179487943649292 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 5.0899363Losses:  6.981385946273804 1.0 1.170975685119629 1.405432939529419
CurrentTrain: epoch  6, batch     3 | loss: 6.9813859Losses:  4.403319835662842 0.83492112159729 1.1060482263565063 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 4.4033198Losses:  11.498958393931389 1.1065716743469238 1.2437610626220703 5.825764462351799
CurrentTrain: epoch  7, batch     1 | loss: 11.4989584Losses:  9.369737178087234 0.8656983375549316 1.1322176456451416 4.242001086473465
CurrentTrain: epoch  7, batch     2 | loss: 9.3697372Losses:  4.369688399136066 0.48571300506591797 0.9656705856323242 1.437518484890461
CurrentTrain: epoch  7, batch     3 | loss: 4.3696884Losses:  13.611017536371946 0.8998687267303467 1.0874638557434082 8.894125293940306
CurrentTrain: epoch  8, batch     0 | loss: 13.6110175Losses:  10.120610073208809 0.9439684152603149 1.288115382194519 4.894472911953926
CurrentTrain: epoch  8, batch     1 | loss: 10.1206101Losses:  6.247327525168657 0.8781116008758545 1.2334527969360352 1.7393872328102589
CurrentTrain: epoch  8, batch     2 | loss: 6.2473275Losses:  7.573907792568207 1.0616321563720703 1.0 1.3889159560203552
CurrentTrain: epoch  8, batch     3 | loss: 7.5739078Losses:  7.834443539381027 0.8882673978805542 1.1942447423934937 2.8332729041576385
CurrentTrain: epoch  9, batch     0 | loss: 7.8344435Losses:  5.432032197713852 0.813692569732666 1.111618995666504 1.413221925497055
CurrentTrain: epoch  9, batch     1 | loss: 5.4320322Losses:  6.301392234861851 1.006972312927246 1.2499752044677734 1.4745203629136086
CurrentTrain: epoch  9, batch     2 | loss: 6.3013922Losses:  6.756758149713278 1.1209287643432617 1.0 1.6146530471742153
CurrentTrain: epoch  9, batch     3 | loss: 6.7567581
Losses:  1.0842419862747192 0.6584638357162476 0.9488461017608643 -0.0
MemoryTrain:  epoch  0, batch     0 | loss: 1.0842420Losses:  1.3908439874649048 0.773928165435791 1.0076065063476562 -0.0
MemoryTrain:  epoch  0, batch     1 | loss: 1.3908440Losses:  1.0549514293670654 0.7540963888168335 0.9960716962814331 -0.0
MemoryTrain:  epoch  0, batch     2 | loss: 1.0549514Losses:  1.0151861906051636 0.738205075263977 1.0975931882858276 -0.0
MemoryTrain:  epoch  0, batch     3 | loss: 1.0151862Losses:  1.1244497299194336 0.638268232345581 1.0548532009124756 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 1.1244497Losses:  1.1830143928527832 0.5291337966918945 1.0807182788848877 -0.0
MemoryTrain:  epoch  1, batch     0 | loss: 1.1830144Losses:  1.4380091428756714 0.6261202096939087 1.0054755210876465 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 1.4380091Losses:  1.1908985376358032 0.7634881734848022 1.0014839172363281 -0.0
MemoryTrain:  epoch  1, batch     2 | loss: 1.1908985Losses:  1.0658947229385376 0.8179864883422852 1.0 -0.0
MemoryTrain:  epoch  1, batch     3 | loss: 1.0658947Losses:  0.9656272530555725 0.7507842779159546 1.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 0.9656273Losses:  1.1129734516143799 0.7423394918441772 1.0242786407470703 -0.0
MemoryTrain:  epoch  2, batch     0 | loss: 1.1129735Losses:  1.055944800376892 0.7043479681015015 1.1197779178619385 -0.0
MemoryTrain:  epoch  2, batch     1 | loss: 1.0559448Losses:  0.9941691160202026 0.6457405090332031 1.0184966325759888 -0.0
MemoryTrain:  epoch  2, batch     2 | loss: 0.9941691Losses:  0.9765278100967407 0.6570754051208496 1.0 -0.0
MemoryTrain:  epoch  2, batch     3 | loss: 0.9765278Losses:  0.9763616919517517 0.6783137321472168 1.0417907238006592 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 0.9763617Losses:  0.9530752897262573 0.6599750518798828 0.9976068735122681 -0.0
MemoryTrain:  epoch  3, batch     0 | loss: 0.9530753Losses:  0.9009524583816528 0.6703096628189087 1.0262539386749268 -0.0
MemoryTrain:  epoch  3, batch     1 | loss: 0.9009525Losses:  0.893785834312439 0.6574432849884033 1.0 -0.0
MemoryTrain:  epoch  3, batch     2 | loss: 0.8937858Losses:  1.0514721870422363 0.8662838935852051 1.04847252368927 -0.0
MemoryTrain:  epoch  3, batch     3 | loss: 1.0514722Losses:  0.8120836019515991 0.5502527952194214 1.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 0.8120836Losses:  0.9837860465049744 0.7402737140655518 1.0655885934829712 -0.0
MemoryTrain:  epoch  4, batch     0 | loss: 0.9837860Losses:  0.8251630067825317 0.5415433645248413 0.992104172706604 -0.0
MemoryTrain:  epoch  4, batch     1 | loss: 0.8251630Losses:  0.897991418838501 0.6424083709716797 1.0434660911560059 -0.0
MemoryTrain:  epoch  4, batch     2 | loss: 0.8979914Losses:  0.8377782702445984 0.5638482570648193 1.0 -0.0
MemoryTrain:  epoch  4, batch     3 | loss: 0.8377783Losses:  0.9259546995162964 0.774126410484314 1.0049160718917847 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 0.9259547Losses:  0.8888849020004272 0.692371129989624 1.0060487985610962 -0.0
MemoryTrain:  epoch  5, batch     0 | loss: 0.8888849Losses:  0.8187510967254639 0.52794349193573 0.9959160089492798 -0.0
MemoryTrain:  epoch  5, batch     1 | loss: 0.8187511Losses:  0.8133741617202759 0.531603217124939 1.0 -0.0
MemoryTrain:  epoch  5, batch     2 | loss: 0.8133742Losses:  0.9459398984909058 0.7077631950378418 1.032781720161438 -0.0
MemoryTrain:  epoch  5, batch     3 | loss: 0.9459399Losses:  0.9817209243774414 0.8173121213912964 1.0339165925979614 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 0.9817209Losses:  0.7330341935157776 0.3954368829727173 1.0 -0.0
MemoryTrain:  epoch  6, batch     0 | loss: 0.7330342Losses:  0.8601503372192383 0.6482893228530884 1.0 -0.0
MemoryTrain:  epoch  6, batch     1 | loss: 0.8601503Losses:  0.9203155636787415 0.7057955265045166 1.0203968286514282 -0.0
MemoryTrain:  epoch  6, batch     2 | loss: 0.9203156Losses:  0.9193281531333923 0.6942596435546875 1.017137885093689 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 0.9193282Losses:  0.9274618625640869 0.7856063842773438 1.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 0.9274619Losses:  0.8459349870681763 0.6161977052688599 1.0 -0.0
MemoryTrain:  epoch  7, batch     0 | loss: 0.8459350Losses:  0.8097270727157593 0.5427157878875732 1.0133419036865234 -0.0
MemoryTrain:  epoch  7, batch     1 | loss: 0.8097271Losses:  0.8567324280738831 0.6414240598678589 1.0 -0.0
MemoryTrain:  epoch  7, batch     2 | loss: 0.8567324Losses:  0.8781405687332153 0.6256746053695679 1.0311890840530396 -0.0
MemoryTrain:  epoch  7, batch     3 | loss: 0.8781406Losses:  0.8712570071220398 0.6899150609970093 0.9957196712493896 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 0.8712570Losses:  0.7799463868141174 0.4740370512008667 1.0071431398391724 -0.0
MemoryTrain:  epoch  8, batch     0 | loss: 0.7799464Losses:  0.8234733939170837 0.5891463756561279 1.0 -0.0
MemoryTrain:  epoch  8, batch     1 | loss: 0.8234734Losses:  0.8803726434707642 0.6843059062957764 1.0 -0.0
MemoryTrain:  epoch  8, batch     2 | loss: 0.8803726Losses:  0.886932373046875 0.6640887260437012 1.009083867073059 -0.0
MemoryTrain:  epoch  8, batch     3 | loss: 0.8869324Losses:  0.8863775730133057 0.6334508657455444 1.0583763122558594 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 0.8863776Losses:  0.7607983350753784 0.49640369415283203 0.9452528953552246 -0.0
MemoryTrain:  epoch  9, batch     0 | loss: 0.7607983Losses:  0.8445158004760742 0.6574831008911133 0.9780219793319702 -0.0
MemoryTrain:  epoch  9, batch     1 | loss: 0.8445158Losses:  0.8158602714538574 0.5468082427978516 1.0 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 0.8158603Losses:  0.9145553112030029 0.7509081363677979 1.0 -0.0
MemoryTrain:  epoch  9, batch     3 | loss: 0.9145553Losses:  0.7860602140426636 0.5198333263397217 1.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 0.7860602
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 51.63%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 51.30%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 51.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.74%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 69.86%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 67.32%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 68.53%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 68.00%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 66.56%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.38%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 62.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.13%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 77.22%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 76.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 77.55%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 77.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 77.41%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 76.83%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 76.69%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 76.71%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 75.78%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 75.10%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 74.34%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 73.60%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 73.07%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 72.74%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 73.80%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 74.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 73.78%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 73.80%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 73.92%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 73.27%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 73.14%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 72.94%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 72.82%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 72.56%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 71.63%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 71.33%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 70.97%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 70.55%   [EVAL] batch:   94 | acc: 18.75%,  total acc: 70.00%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 69.66%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 69.39%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 68.88%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 68.56%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 68.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 67.88%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 67.83%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 67.29%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 66.78%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 66.34%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 65.85%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 65.54%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 65.29%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 67.54%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 67.26%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 66.70%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 66.47%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 66.30%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 66.27%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 66.86%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 66.18%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 66.31%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 65.44%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 65.02%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 64.60%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 64.18%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 64.25%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 64.36%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 64.52%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 64.28%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 64.01%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 63.66%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 63.39%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 63.17%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 63.11%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 63.11%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 63.14%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 62.93%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 62.78%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 62.57%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 62.47%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 62.33%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 62.22%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 62.29%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 62.47%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 63.56%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 64.37%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 64.20%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 64.12%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 64.05%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 63.91%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 63.81%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 63.81%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 63.76%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 63.81%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 63.77%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 63.58%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 63.31%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 63.10%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 62.97%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 62.79%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 62.79%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 63.85%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 63.85%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 63.90%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 63.83%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 65.53%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 65.60%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 66.16%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 65.97%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 65.74%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  254 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 65.21%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 65.15%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.17%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.09%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 65.09%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 65.01%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 64.93%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 64.73%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 64.65%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 64.57%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 65.15%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 64.92%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 64.79%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 64.76%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 64.73%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 64.57%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 64.50%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 64.31%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 64.11%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 63.93%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:  289 | acc: 31.25%,  total acc: 63.71%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 63.60%   [EVAL] batch:  291 | acc: 37.50%,  total acc: 63.51%   [EVAL] batch:  292 | acc: 37.50%,  total acc: 63.42%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 63.33%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 63.26%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 63.18%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 63.09%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 63.07%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 63.00%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 62.96%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 64.09%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 63.93%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 63.75%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 63.54%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 63.38%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 63.30%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 63.38%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 63.75%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 63.59%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 63.43%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 63.28%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 63.14%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 63.03%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.41%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 63.70%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 63.77%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 63.96%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  343 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  350 | acc: 18.75%,  total acc: 64.60%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 64.47%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 64.38%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 64.21%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 64.05%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 63.94%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 64.37%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 64.48%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 64.43%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 64.39%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 64.33%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 64.28%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 64.30%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 64.40%   [EVAL] batch:  375 | acc: 0.00%,  total acc: 64.23%   [EVAL] batch:  376 | acc: 0.00%,  total acc: 64.06%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 63.91%   [EVAL] batch:  378 | acc: 0.00%,  total acc: 63.74%   [EVAL] batch:  379 | acc: 0.00%,  total acc: 63.57%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 63.42%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 63.40%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.56%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.61%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 63.69%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 63.73%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 63.78%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 63.68%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 63.54%   [EVAL] batch:  397 | acc: 12.50%,  total acc: 63.41%   [EVAL] batch:  398 | acc: 6.25%,  total acc: 63.27%   [EVAL] batch:  399 | acc: 18.75%,  total acc: 63.16%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.01%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 62.86%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.70%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 62.55%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.24%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 62.24%   [EVAL] batch:  407 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 62.56%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 62.62%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 62.68%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 62.99%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 63.20%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 63.21%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 63.21%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 63.28%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  425 | acc: 12.50%,  total acc: 63.15%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 63.13%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 63.16%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:  431 | acc: 31.25%,  total acc: 63.09%   [EVAL] batch:  432 | acc: 25.00%,  total acc: 63.01%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 62.96%   [EVAL] batch:  434 | acc: 50.00%,  total acc: 62.93%   [EVAL] batch:  435 | acc: 37.50%,  total acc: 62.87%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 62.87%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 62.77%   [EVAL] batch:  438 | acc: 18.75%,  total acc: 62.67%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 62.60%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 62.51%   [EVAL] batch:  441 | acc: 6.25%,  total acc: 62.39%   [EVAL] batch:  442 | acc: 31.25%,  total acc: 62.32%   [EVAL] batch:  443 | acc: 31.25%,  total acc: 62.25%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 62.32%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:  446 | acc: 43.75%,  total acc: 62.32%   [EVAL] batch:  447 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 62.39%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 62.43%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 62.44%   [EVAL] batch:  451 | acc: 56.25%,  total acc: 62.43%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 62.43%   [EVAL] batch:  453 | acc: 75.00%,  total acc: 62.46%   [EVAL] batch:  454 | acc: 62.50%,  total acc: 62.46%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 62.53%   [EVAL] batch:  456 | acc: 56.25%,  total acc: 62.51%   [EVAL] batch:  457 | acc: 31.25%,  total acc: 62.45%   [EVAL] batch:  458 | acc: 25.00%,  total acc: 62.36%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 62.32%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 62.24%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 62.22%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 62.49%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 62.57%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 63.25%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 63.35%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 63.41%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 63.55%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 63.52%   [EVAL] batch:  482 | acc: 12.50%,  total acc: 63.42%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 63.35%   [EVAL] batch:  484 | acc: 31.25%,  total acc: 63.29%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 63.26%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 63.22%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 63.23%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 63.45%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 63.52%   [EVAL] batch:  494 | acc: 31.25%,  total acc: 63.46%   [EVAL] batch:  495 | acc: 25.00%,  total acc: 63.38%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 63.33%   [EVAL] batch:  497 | acc: 37.50%,  total acc: 63.28%   [EVAL] batch:  498 | acc: 43.75%,  total acc: 63.24%   [EVAL] batch:  499 | acc: 50.00%,  total acc: 63.21%   
cur_acc:  ['0.9484', '0.7331', '0.6845', '0.8115', '0.6835', '0.7063', '0.5784', '0.6538']
his_acc:  ['0.9484', '0.8360', '0.7719', '0.7542', '0.7314', '0.6883', '0.6578', '0.6321']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  13.353259652853012 1.4169422388076782 1.4507910013198853 2.01093253493309
CurrentTrain: epoch  0, batch     0 | loss: 13.3532597Losses:  20.91691228747368 1.523011565208435 1.7265498638153076 9.610010355710983
CurrentTrain: epoch  0, batch     1 | loss: 20.9169123Losses:  21.00192254781723 1.3859732151031494 1.519740104675293 9.806890428066254
CurrentTrain: epoch  0, batch     2 | loss: 21.0019225Losses:  17.99686022475362 1.428276777267456 1.4726219177246094 6.478463847190142
CurrentTrain: epoch  0, batch     3 | loss: 17.9968602Losses:  12.29169612377882 1.2961335182189941 1.155660629272461 1.5342517420649529
CurrentTrain: epoch  0, batch     4 | loss: 12.2916961Losses:  16.578583024442196 1.33970308303833 1.459069848060608 5.750015519559383
CurrentTrain: epoch  0, batch     5 | loss: 16.5785830Losses:  16.87842920422554 1.5578303337097168 1.5239052772521973 5.828154355287552
CurrentTrain: epoch  0, batch     6 | loss: 16.8784292Losses:  11.482415199279785 1.358588457107544 1.4566891193389893 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 11.4824152Losses:  13.013418808579445 1.5420299768447876 1.3172519207000732 1.598598137497902
CurrentTrain: epoch  0, batch     8 | loss: 13.0134188Losses:  15.030201517045498 1.3572938442230225 1.4633649587631226 3.694493852555752
CurrentTrain: epoch  0, batch     9 | loss: 15.0302015Losses:  16.145959351211786 1.4454379081726074 1.4334533214569092 5.854457352310419
CurrentTrain: epoch  0, batch    10 | loss: 16.1459594Losses:  14.443139847368002 1.4566117525100708 1.4792051315307617 2.8950880132615566
CurrentTrain: epoch  0, batch    11 | loss: 14.4431398Losses:  19.395230285823345 1.6795024871826172 1.4793615341186523 8.77459429949522
CurrentTrain: epoch  0, batch    12 | loss: 19.3952303Losses:  11.114007949829102 1.517399787902832 1.478278636932373 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 11.1140079Losses:  12.896070439368486 1.449026107788086 1.4407209157943726 1.9042300768196583
CurrentTrain: epoch  0, batch    14 | loss: 12.8960704Losses:  15.606763485819101 1.4939171075820923 1.5823631286621094 3.7013632096350193
CurrentTrain: epoch  0, batch    15 | loss: 15.6067635Losses:  13.991776004433632 1.375128149986267 1.339031457901001 3.8069682270288467
CurrentTrain: epoch  0, batch    16 | loss: 13.9917760Losses:  13.717798620462418 1.3199546337127686 1.134092092514038 3.227780729532242
CurrentTrain: epoch  0, batch    17 | loss: 13.7177986Losses:  12.12402131780982 1.5672954320907593 1.562687635421753 1.5142123959958553
CurrentTrain: epoch  0, batch    18 | loss: 12.1240213Losses:  9.93795394897461 1.354144811630249 1.3351857662200928 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 9.9379539Losses:  21.16217464208603 1.3049426078796387 1.3356610536575317 10.894659459590912
CurrentTrain: epoch  0, batch    20 | loss: 21.1621746Losses:  17.36032623797655 1.2976531982421875 1.2489144802093506 7.154046483337879
CurrentTrain: epoch  0, batch    21 | loss: 17.3603262Losses:  10.057522773742676 1.3315869569778442 1.3705097436904907 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 10.0575228Losses:  12.473432689905167 1.447637915611267 1.3059579133987427 1.438807636499405
CurrentTrain: epoch  0, batch    23 | loss: 12.4734327Losses:  11.536558151245117 1.4451042413711548 1.2011760473251343 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 11.5365582Losses:  9.798608779907227 1.4641021490097046 1.4551362991333008 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 9.7986088Losses:  21.739759635180235 1.6348284482955933 1.463531255722046 11.095129203051329
CurrentTrain: epoch  0, batch    26 | loss: 21.7397596Losses:  9.163548469543457 1.290783405303955 1.2947049140930176 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 9.1635485Losses:  12.307687614113092 1.4511840343475342 1.3009449243545532 1.673026893287897
CurrentTrain: epoch  0, batch    28 | loss: 12.3076876Losses:  12.54837192222476 1.6109973192214966 1.393234372138977 3.0858913771808147
CurrentTrain: epoch  0, batch    29 | loss: 12.5483719Losses:  24.791784025728703 1.1995769739151 1.0971261262893677 14.874353148043156
CurrentTrain: epoch  0, batch    30 | loss: 24.7917840Losses:  20.685154147446156 1.6371266841888428 1.5818761587142944 9.911116786301136
CurrentTrain: epoch  0, batch    31 | loss: 20.6851541Losses:  17.02800602838397 1.4255163669586182 1.2756767272949219 6.790879677981138
CurrentTrain: epoch  0, batch    32 | loss: 17.0280060Losses:  16.804868042469025 1.3700039386749268 1.388246774673462 5.6107457280159
CurrentTrain: epoch  0, batch    33 | loss: 16.8048680Losses:  10.153867721557617 1.471203088760376 1.3310542106628418 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 10.1538677Losses:  13.065890368074179 1.641338586807251 1.4469170570373535 2.1756048761308193
CurrentTrain: epoch  0, batch    35 | loss: 13.0658904Losses:  9.555397987365723 1.413589358329773 1.3836729526519775 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 9.5553980Losses:  12.484191231429577 1.4028695821762085 1.4270868301391602 3.0425217673182487
CurrentTrain: epoch  0, batch    37 | loss: 12.4841912Losses:  14.591995470225811 1.322693109512329 1.2718229293823242 4.529356233775616
CurrentTrain: epoch  0, batch    38 | loss: 14.5919955Losses:  11.946444869041443 1.2787389755249023 1.299178957939148 2.8773969411849976
CurrentTrain: epoch  0, batch    39 | loss: 11.9464449Losses:  14.836152914911509 1.1367473602294922 0.9702929258346558 5.553054694086313
CurrentTrain: epoch  0, batch    40 | loss: 14.8361529Losses:  14.736049162223935 1.1659302711486816 0.9899232387542725 6.263190733268857
CurrentTrain: epoch  0, batch    41 | loss: 14.7360492Losses:  11.452302396297455 1.3677061796188354 1.2041231393814087 2.4102339148521423
CurrentTrain: epoch  0, batch    42 | loss: 11.4523024Losses:  10.90891107916832 1.1977357864379883 1.246259331703186 1.4824250638484955
CurrentTrain: epoch  0, batch    43 | loss: 10.9089111Losses:  14.851675990968943 1.417192816734314 1.2557806968688965 4.3564109839499
CurrentTrain: epoch  0, batch    44 | loss: 14.8516760Losses:  16.633207745850086 1.3465813398361206 1.1403746604919434 6.339806981384754
CurrentTrain: epoch  0, batch    45 | loss: 16.6332077Losses:  12.37448351830244 1.1810253858566284 1.1265617609024048 3.098687581717968
CurrentTrain: epoch  0, batch    46 | loss: 12.3744835Losses:  13.625654723495245 1.4017422199249268 1.2320095300674438 4.547855880111456
CurrentTrain: epoch  0, batch    47 | loss: 13.6256547Losses:  14.727577719837427 1.2682366371154785 0.9957687854766846 4.455904517322779
CurrentTrain: epoch  0, batch    48 | loss: 14.7275777Losses:  11.294263325631618 1.22311270236969 1.0758867263793945 2.9761156663298607
CurrentTrain: epoch  0, batch    49 | loss: 11.2942633Losses:  10.613432884216309 1.569059133529663 1.2268531322479248 -0.0
CurrentTrain: epoch  0, batch    50 | loss: 10.6134329Losses:  10.568395730108023 1.200379729270935 1.070604920387268 1.5387889109551907
CurrentTrain: epoch  0, batch    51 | loss: 10.5683957Losses:  11.505445338785648 1.2467751502990723 1.095903754234314 3.0347498431801796
CurrentTrain: epoch  0, batch    52 | loss: 11.5054453Losses:  11.742874525487423 1.0858285427093506 0.8761396408081055 3.337713621556759
CurrentTrain: epoch  0, batch    53 | loss: 11.7428745Losses:  14.643353275954723 1.467392086982727 1.2991383075714111 5.164823345839977
CurrentTrain: epoch  0, batch    54 | loss: 14.6433533Losses:  14.025007661432028 1.261993646621704 1.1018199920654297 3.0104479156434536
CurrentTrain: epoch  0, batch    55 | loss: 14.0250077Losses:  10.850263595581055 1.49623703956604 1.3044036626815796 -0.0
CurrentTrain: epoch  0, batch    56 | loss: 10.8502636Losses:  9.982801407575607 1.1978015899658203 1.239215612411499 1.4198283851146698
CurrentTrain: epoch  0, batch    57 | loss: 9.9828014Losses:  16.183576591312885 1.563560128211975 1.296388864517212 5.730540283024311
CurrentTrain: epoch  0, batch    58 | loss: 16.1835766Losses:  9.297971725463867 1.1315518617630005 1.1198501586914062 -0.0
CurrentTrain: epoch  0, batch    59 | loss: 9.2979717Losses:  18.855471283197403 1.197438359260559 1.0876809358596802 9.27608647942543
CurrentTrain: epoch  0, batch    60 | loss: 18.8554713Losses:  19.48869238421321 1.3575632572174072 1.1136524677276611 10.317072968930006
CurrentTrain: epoch  0, batch    61 | loss: 19.4886924Losses:  10.95811340212822 1.3005359172821045 1.1442809104919434 1.5264041125774384
CurrentTrain: epoch  0, batch    62 | loss: 10.9581134Losses:  8.877187728881836 1.332606554031372 1.3254858255386353 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 8.8771877Losses:  9.762504577636719 1.3376084566116333 1.2014532089233398 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 9.7625046Losses:  10.689067487604916 1.2698246240615845 1.1233466863632202 1.9999185847118497
CurrentTrain: epoch  1, batch     2 | loss: 10.6890675Losses:  15.3200525008142 1.4958151578903198 1.3424382209777832 6.294586535543203
CurrentTrain: epoch  1, batch     3 | loss: 15.3200525Losses:  9.833421867340803 1.2200047969818115 1.052327275276184 1.7591945342719555
CurrentTrain: epoch  1, batch     4 | loss: 9.8334219Losses:  9.39013957977295 1.3308415412902832 1.2000211477279663 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 9.3901396Losses:  16.619063556194305 1.306626796722412 1.2972356081008911 7.109858691692352
CurrentTrain: epoch  1, batch     6 | loss: 16.6190636Losses:  11.776478700339794 1.0657364130020142 0.9691096544265747 4.278566293418407
CurrentTrain: epoch  1, batch     7 | loss: 11.7764787Losses:  10.563149822875857 1.2915066480636597 1.0544381141662598 1.9417213331907988
CurrentTrain: epoch  1, batch     8 | loss: 10.5631498Losses:  13.966544076800346 1.4006155729293823 1.1549451351165771 4.444545671343803
CurrentTrain: epoch  1, batch     9 | loss: 13.9665441Losses:  8.189509391784668 1.2087377309799194 1.081699013710022 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 8.1895094Losses:  18.707335812970996 1.4253987073898315 1.1958017349243164 9.51630531065166
CurrentTrain: epoch  1, batch    11 | loss: 18.7073358Losses:  11.7400395385921 1.212577223777771 1.0351066589355469 3.1529108993709087
CurrentTrain: epoch  1, batch    12 | loss: 11.7400395Losses:  13.068701285868883 1.1142460107803345 1.0396885871887207 4.5410838313400745
CurrentTrain: epoch  1, batch    13 | loss: 13.0687013Losses:  8.604694366455078 1.3544843196868896 1.2627549171447754 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 8.6046944Losses:  14.21716906875372 1.0929065942764282 1.040798306465149 6.086275361478329
CurrentTrain: epoch  1, batch    15 | loss: 14.2171691Losses:  11.14113088697195 1.063576579093933 1.0522258281707764 2.8827223405241966
CurrentTrain: epoch  1, batch    16 | loss: 11.1411309Losses:  10.959657203406096 1.2476186752319336 1.0551320314407349 2.97264052554965
CurrentTrain: epoch  1, batch    17 | loss: 10.9596572Losses:  10.439400792121887 1.3625882863998413 1.2722351551055908 1.5016690492630005
CurrentTrain: epoch  1, batch    18 | loss: 10.4394008Losses:  14.537778824567795 1.4297822713851929 1.09225332736969 4.826690644025803
CurrentTrain: epoch  1, batch    19 | loss: 14.5377788Losses:  8.04636287689209 1.2338844537734985 1.0378129482269287 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 8.0463629Losses:  14.350993983447552 1.282833218574524 1.112783432006836 6.197419039905071
CurrentTrain: epoch  1, batch    21 | loss: 14.3509940Losses:  11.6472522392869 1.2818880081176758 1.1980421543121338 3.069609798491001
CurrentTrain: epoch  1, batch    22 | loss: 11.6472522Losses:  11.231721844524145 1.1037864685058594 1.1366556882858276 2.8596973083913326
CurrentTrain: epoch  1, batch    23 | loss: 11.2317218Losses:  27.776206769049168 1.206242322921753 1.029245376586914 19.184257306158543
CurrentTrain: epoch  1, batch    24 | loss: 27.7762068Losses:  8.248262405395508 1.1972272396087646 1.0449424982070923 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 8.2482624Losses:  7.7772722244262695 1.228906273841858 1.06022310256958 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 7.7772722Losses:  8.398506164550781 1.2482959032058716 1.1096594333648682 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 8.3985062Losses:  13.687321864068508 1.1717407703399658 1.0519405603408813 4.623743258416653
CurrentTrain: epoch  1, batch    28 | loss: 13.6873219Losses:  14.752113569527864 1.1669756174087524 1.0861854553222656 5.535794485360384
CurrentTrain: epoch  1, batch    29 | loss: 14.7521136Losses:  9.293043874204159 1.0804067850112915 1.0613517761230469 1.5059626325964928
CurrentTrain: epoch  1, batch    30 | loss: 9.2930439Losses:  20.467487514019012 1.398823857307434 1.0071430206298828 11.413791835308075
CurrentTrain: epoch  1, batch    31 | loss: 20.4674875Losses:  16.837444730103016 1.336930274963379 0.9450328350067139 7.744288869202137
CurrentTrain: epoch  1, batch    32 | loss: 16.8374447Losses:  11.395695872604847 1.3102327585220337 1.063434362411499 2.484420008957386
CurrentTrain: epoch  1, batch    33 | loss: 11.3956959Losses:  9.793643951416016 1.1273218393325806 1.086344599723816 1.402318000793457
CurrentTrain: epoch  1, batch    34 | loss: 9.7936440Losses:  13.52431577257812 1.2657723426818848 0.9589433670043945 4.879992423579097
CurrentTrain: epoch  1, batch    35 | loss: 13.5243158Losses:  7.461418151855469 0.9947928190231323 1.0151110887527466 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 7.4614182Losses:  15.295728210359812 1.2302184104919434 0.9988096952438354 6.351286415010691
CurrentTrain: epoch  1, batch    37 | loss: 15.2957282Losses:  11.30894860625267 1.1691590547561646 1.1485750675201416 2.262059301137924
CurrentTrain: epoch  1, batch    38 | loss: 11.3089486Losses:  12.07442893832922 1.0549521446228027 1.0829050540924072 3.1941646561026573
CurrentTrain: epoch  1, batch    39 | loss: 12.0744289Losses:  15.107643894851208 1.1957565546035767 1.000243067741394 6.738166622817516
CurrentTrain: epoch  1, batch    40 | loss: 15.1076439Losses:  8.483148574829102 1.2952948808670044 1.0800306797027588 -0.0
CurrentTrain: epoch  1, batch    41 | loss: 8.4831486Losses:  11.30191844701767 1.260530710220337 1.1839638948440552 3.1861461997032166
CurrentTrain: epoch  1, batch    42 | loss: 11.3019184Losses:  10.263372868299484 1.219279170036316 1.1602500677108765 1.467767208814621
CurrentTrain: epoch  1, batch    43 | loss: 10.2633729Losses:  10.175088226795197 1.3508821725845337 1.160394310951233 1.3998416066169739
CurrentTrain: epoch  1, batch    44 | loss: 10.1750882Losses:  8.044711112976074 1.2603120803833008 1.1005536317825317 -0.0
CurrentTrain: epoch  1, batch    45 | loss: 8.0447111Losses:  8.098824501037598 1.2791286706924438 1.1692770719528198 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 8.0988245Losses:  9.765742216259241 1.2371480464935303 1.0204821825027466 1.472127828747034
CurrentTrain: epoch  1, batch    47 | loss: 9.7657422Losses:  8.429703712463379 1.1961815357208252 1.0228807926177979 -0.0
CurrentTrain: epoch  1, batch    48 | loss: 8.4297037Losses:  24.408883895725012 1.3851269483566284 1.187500238418579 15.290106620639563
CurrentTrain: epoch  1, batch    49 | loss: 24.4088839Losses:  10.500827878713608 1.2281802892684937 1.0481523275375366 2.836352914571762
CurrentTrain: epoch  1, batch    50 | loss: 10.5008279Losses:  10.038930796086788 1.1657909154891968 1.059759259223938 2.023307703435421
CurrentTrain: epoch  1, batch    51 | loss: 10.0389308Losses:  8.78616163507104 1.1114568710327148 1.0715155601501465 1.465576384216547
CurrentTrain: epoch  1, batch    52 | loss: 8.7861616Losses:  7.0094757080078125 1.1944003105163574 1.1488873958587646 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 7.0094757Losses:  10.953362099826336 1.1719236373901367 1.0389047861099243 2.919116608798504
CurrentTrain: epoch  1, batch    54 | loss: 10.9533621Losses:  7.6107072830200195 1.190321922302246 0.9476228952407837 -0.0
CurrentTrain: epoch  1, batch    55 | loss: 7.6107073Losses:  9.17121770977974 1.2415775060653687 1.0751593112945557 1.4823944866657257
CurrentTrain: epoch  1, batch    56 | loss: 9.1712177Losses:  7.9247026443481445 1.294616460800171 1.0497130155563354 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 7.9247026Losses:  8.68117618560791 1.0713263750076294 1.0425974130630493 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 8.6811762Losses:  10.72928225621581 1.322067141532898 1.1469224691390991 2.9903949461877346
CurrentTrain: epoch  1, batch    59 | loss: 10.7292823Losses:  9.721371226012707 1.2132508754730225 1.1191331148147583 1.4942603632807732
CurrentTrain: epoch  1, batch    60 | loss: 9.7213712Losses:  9.36888680420816 1.1028225421905518 0.9573982954025269 1.6132172103971243
CurrentTrain: epoch  1, batch    61 | loss: 9.3688868Losses:  9.915212631225586 1.2918813228607178 1.1549227237701416 -0.0
CurrentTrain: epoch  1, batch    62 | loss: 9.9152126Losses:  11.296180330216885 1.2185449600219727 1.0788748264312744 4.356376729905605
CurrentTrain: epoch  2, batch     0 | loss: 11.2961803Losses:  12.819891970604658 1.2108728885650635 1.1112250089645386 4.650754015892744
CurrentTrain: epoch  2, batch     1 | loss: 12.8198920Losses:  7.188164234161377 1.1008778810501099 1.0781419277191162 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 7.1881642Losses:  13.492559935897589 1.2537057399749756 1.0416120290756226 6.027578379958868
CurrentTrain: epoch  2, batch     3 | loss: 13.4925599Losses:  10.875398814678192 0.9747145175933838 1.0926601886749268 4.248129069805145
CurrentTrain: epoch  2, batch     4 | loss: 10.8753988Losses:  7.81886288523674 1.1226295232772827 1.052945613861084 1.438141793012619
CurrentTrain: epoch  2, batch     5 | loss: 7.8188629Losses:  9.934581998735666 1.1347405910491943 1.0587667226791382 1.4399693049490452
CurrentTrain: epoch  2, batch     6 | loss: 9.9345820Losses:  8.939349502325058 0.9528530836105347 0.9745993614196777 1.4401191174983978
CurrentTrain: epoch  2, batch     7 | loss: 8.9393495Losses:  11.593317374587059 1.2317237854003906 1.0297460556030273 4.3144372552633286
CurrentTrain: epoch  2, batch     8 | loss: 11.5933174Losses:  10.218076143413782 1.1166481971740723 1.0570944547653198 2.9355548955500126
CurrentTrain: epoch  2, batch     9 | loss: 10.2180761Losses:  7.197190761566162 1.055619716644287 1.0440471172332764 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 7.1971908Losses:  14.053346022963524 1.3838688135147095 1.0167590379714966 4.905868873000145
CurrentTrain: epoch  2, batch    11 | loss: 14.0533460Losses:  11.600839756429195 1.120636224746704 1.1713306903839111 4.460278175771236
CurrentTrain: epoch  2, batch    12 | loss: 11.6008398Losses:  11.458012621849775 0.9211052656173706 0.9908847808837891 4.8342643193900585
CurrentTrain: epoch  2, batch    13 | loss: 11.4580126Losses:  8.46749198436737 1.0992575883865356 1.0892068147659302 1.4947117567062378
CurrentTrain: epoch  2, batch    14 | loss: 8.4674920Losses:  7.766205787658691 1.1280306577682495 1.0525610446929932 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 7.7662058Losses:  15.741867303848267 1.119482398033142 1.1128334999084473 7.476703882217407
CurrentTrain: epoch  2, batch    16 | loss: 15.7418673Losses:  16.370208002626896 1.0278100967407227 1.0406113862991333 9.364722944796085
CurrentTrain: epoch  2, batch    17 | loss: 16.3702080Losses:  7.62454080581665 1.0870825052261353 1.0111615657806396 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 7.6245408Losses:  11.104294270277023 1.080668568611145 0.9617811441421509 3.0443491637706757
CurrentTrain: epoch  2, batch    19 | loss: 11.1042943Losses:  7.988348007202148 1.0570405721664429 1.0070340633392334 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 7.9883480Losses:  10.25709953904152 1.122893214225769 1.1191458702087402 2.814096838235855
CurrentTrain: epoch  2, batch    21 | loss: 10.2570995Losses:  10.126967966556549 1.187523365020752 0.9834786653518677 2.666995584964752
CurrentTrain: epoch  2, batch    22 | loss: 10.1269680Losses:  8.5778429210186 1.1145251989364624 1.015723466873169 1.4036318957805634
CurrentTrain: epoch  2, batch    23 | loss: 8.5778429Losses:  9.286152124404907 1.0930835008621216 1.0759607553482056 1.879894495010376
CurrentTrain: epoch  2, batch    24 | loss: 9.2861521Losses:  10.237925425171852 1.2492560148239136 1.0383658409118652 2.8870629221200943
CurrentTrain: epoch  2, batch    25 | loss: 10.2379254Losses:  9.6553575694561 1.2364704608917236 1.0680829286575317 1.443571299314499
CurrentTrain: epoch  2, batch    26 | loss: 9.6553576Losses:  11.9880610704422 1.2657750844955444 1.2122769355773926 4.357091069221497
CurrentTrain: epoch  2, batch    27 | loss: 11.9880611Losses:  9.945064336061478 1.0352576971054077 1.0629075765609741 2.959023743867874
CurrentTrain: epoch  2, batch    28 | loss: 9.9450643Losses:  8.199245873838663 0.9936654567718506 1.1243400573730469 1.409086648374796
CurrentTrain: epoch  2, batch    29 | loss: 8.1992459Losses:  11.385464996099472 1.1493960618972778 1.0799771547317505 3.2496112287044525
CurrentTrain: epoch  2, batch    30 | loss: 11.3854650Losses:  13.90503380075097 0.9920905828475952 1.085858702659607 6.335431311279535
CurrentTrain: epoch  2, batch    31 | loss: 13.9050338Losses:  10.623109959065914 1.1161712408065796 1.117457389831543 2.999041222035885
CurrentTrain: epoch  2, batch    32 | loss: 10.6231100Losses:  11.209846444427967 1.168483018875122 1.2482668161392212 3.0023517087101936
CurrentTrain: epoch  2, batch    33 | loss: 11.2098464Losses:  8.628166373819113 1.166670560836792 1.005963683128357 1.4210659824311733
CurrentTrain: epoch  2, batch    34 | loss: 8.6281664Losses:  9.23728695511818 1.0226587057113647 1.085638165473938 2.8306101858615875
CurrentTrain: epoch  2, batch    35 | loss: 9.2372870Losses:  8.732029914855957 1.1634153127670288 1.0133827924728394 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 8.7320299Losses:  12.525197088718414 1.0325701236724854 0.9552637338638306 5.226612627506256
CurrentTrain: epoch  2, batch    37 | loss: 12.5251971Losses:  8.874820481985807 1.040489673614502 1.0484366416931152 1.7340743653476238
CurrentTrain: epoch  2, batch    38 | loss: 8.8748205Losses:  12.948489971458912 1.0157850980758667 1.0672693252563477 5.207399673759937
CurrentTrain: epoch  2, batch    39 | loss: 12.9484900Losses:  10.975748270750046 1.0959738492965698 1.0418614149093628 2.9710762202739716
CurrentTrain: epoch  2, batch    40 | loss: 10.9757483Losses:  10.841722279787064 1.064548134803772 1.0570493936538696 2.943804055452347
CurrentTrain: epoch  2, batch    41 | loss: 10.8417223Losses:  11.738369457423687 1.158141016960144 1.0045970678329468 3.077149860560894
CurrentTrain: epoch  2, batch    42 | loss: 11.7383695Losses:  15.890440754592419 0.9467813968658447 1.0523009300231934 8.761403851211071
CurrentTrain: epoch  2, batch    43 | loss: 15.8904408Losses:  9.069645643234253 0.9675710201263428 1.021070957183838 1.4185054302215576
CurrentTrain: epoch  2, batch    44 | loss: 9.0696456Losses:  8.601177871227264 1.2142775058746338 1.1304547786712646 1.426846206188202
CurrentTrain: epoch  2, batch    45 | loss: 8.6011779Losses:  8.433867007493973 0.9405418634414673 1.0329283475875854 1.4533148109912872
CurrentTrain: epoch  2, batch    46 | loss: 8.4338670Losses:  8.4307701587677 1.2603732347488403 1.0833064317703247 1.398585557937622
CurrentTrain: epoch  2, batch    47 | loss: 8.4307702Losses:  11.6463269200176 1.0687495470046997 0.9691431522369385 4.795520206913352
CurrentTrain: epoch  2, batch    48 | loss: 11.6463269Losses:  9.349150124937296 1.102445125579834 1.014471173286438 1.561179582029581
CurrentTrain: epoch  2, batch    49 | loss: 9.3491501Losses:  7.212460041046143 1.1485943794250488 1.1018402576446533 -0.0
CurrentTrain: epoch  2, batch    50 | loss: 7.2124600Losses:  8.224481526762247 1.0376088619232178 1.0137770175933838 1.486627522855997
CurrentTrain: epoch  2, batch    51 | loss: 8.2244815Losses:  10.33807411044836 1.067754864692688 0.9879173040390015 2.940924547612667
CurrentTrain: epoch  2, batch    52 | loss: 10.3380741Losses:  7.118962287902832 1.1116970777511597 1.0 -0.0
CurrentTrain: epoch  2, batch    53 | loss: 7.1189623Losses:  9.730289794504642 0.9537771940231323 1.0451616048812866 3.0530088916420937
CurrentTrain: epoch  2, batch    54 | loss: 9.7302898Losses:  8.85078577697277 1.1554993391036987 1.0695949792861938 1.5623875111341476
CurrentTrain: epoch  2, batch    55 | loss: 8.8507858Losses:  8.161232389509678 1.106210470199585 0.991820216178894 1.5250825062394142
CurrentTrain: epoch  2, batch    56 | loss: 8.1612324Losses:  10.484416328370571 1.0043610334396362 1.0 3.4572609290480614
CurrentTrain: epoch  2, batch    57 | loss: 10.4844163Losses:  8.52852151915431 1.0580400228500366 0.9588966369628906 1.6110410504043102
CurrentTrain: epoch  2, batch    58 | loss: 8.5285215Losses:  7.2897210121154785 1.0886893272399902 0.9677647352218628 -0.0
CurrentTrain: epoch  2, batch    59 | loss: 7.2897210Losses:  9.085337426513433 1.2566229104995728 1.0964915752410889 1.5038678906857967
CurrentTrain: epoch  2, batch    60 | loss: 9.0853374Losses:  7.32585334777832 1.1100488901138306 0.9867699146270752 -0.0
CurrentTrain: epoch  2, batch    61 | loss: 7.3258533Losses:  10.815761242061853 1.2698709964752197 0.9795372486114502 2.904749546200037
CurrentTrain: epoch  2, batch    62 | loss: 10.8157612Losses:  14.059384420514107 1.1243876218795776 1.0214076042175293 6.268591955304146
CurrentTrain: epoch  3, batch     0 | loss: 14.0593844Losses:  9.56772817671299 1.1683399677276611 1.222840666770935 1.9238702207803726
CurrentTrain: epoch  3, batch     1 | loss: 9.5677282Losses:  8.164505984634161 0.9645735025405884 0.9700415134429932 1.4281783364713192
CurrentTrain: epoch  3, batch     2 | loss: 8.1645060Losses:  10.221205070614815 0.9400118589401245 1.0606454610824585 3.536300018429756
CurrentTrain: epoch  3, batch     3 | loss: 10.2212051Losses:  9.334994979202747 1.197541356086731 1.0356930494308472 1.5070888474583626
CurrentTrain: epoch  3, batch     4 | loss: 9.3349950Losses:  7.197275161743164 1.0199651718139648 1.0047346353530884 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 7.1972752Losses:  9.773672938346863 0.9537371397018433 1.008447289466858 2.812801241874695
CurrentTrain: epoch  3, batch     6 | loss: 9.7736729Losses:  8.063806682825089 0.8991872072219849 1.0334006547927856 1.4060370028018951
CurrentTrain: epoch  3, batch     7 | loss: 8.0638067Losses:  10.502689272165298 1.1972897052764893 1.0320385694503784 3.07582226395607
CurrentTrain: epoch  3, batch     8 | loss: 10.5026893Losses:  7.048508167266846 1.2419625520706177 1.0232019424438477 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 7.0485082Losses:  13.194532223045826 1.1450464725494385 1.1051030158996582 6.027455158531666
CurrentTrain: epoch  3, batch    10 | loss: 13.1945322Losses:  15.313328877091408 1.0585228204727173 1.0298967361450195 8.769888535141945
CurrentTrain: epoch  3, batch    11 | loss: 15.3133289Losses:  6.322749614715576 0.9234402179718018 1.0360221862792969 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 6.3227496Losses:  14.476260177791119 0.9432158470153809 1.0040583610534668 7.221684448421001
CurrentTrain: epoch  3, batch    13 | loss: 14.4762602Losses:  13.078382171690464 1.0082008838653564 0.9749940633773804 5.476360000669956
CurrentTrain: epoch  3, batch    14 | loss: 13.0783822Losses:  8.319733053445816 0.939401388168335 1.0458533763885498 1.437300592660904
CurrentTrain: epoch  3, batch    15 | loss: 8.3197331Losses:  8.728727728128433 1.1124227046966553 1.0097453594207764 1.4232634603977203
CurrentTrain: epoch  3, batch    16 | loss: 8.7287277Losses:  7.903716534376144 1.0772100687026978 1.276219367980957 1.5100063979625702
CurrentTrain: epoch  3, batch    17 | loss: 7.9037165Losses:  7.052710056304932 1.0544426441192627 1.0665626525878906 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 7.0527101Losses:  10.066707402467728 0.9900968074798584 1.042750597000122 2.8237688839435577
CurrentTrain: epoch  3, batch    19 | loss: 10.0667074Losses:  7.747961960732937 0.9201345443725586 1.0 1.522514782845974
CurrentTrain: epoch  3, batch    20 | loss: 7.7479620Losses:  7.173562049865723 1.0700470209121704 1.01305091381073 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 7.1735620Losses:  7.687083274126053 0.9708404541015625 1.009119987487793 1.3921318352222443
CurrentTrain: epoch  3, batch    22 | loss: 7.6870833Losses:  8.86925719305873 0.9559454917907715 1.0211478471755981 1.5849730782210827
CurrentTrain: epoch  3, batch    23 | loss: 8.8692572Losses:  12.273239893838763 1.1217775344848633 1.074806809425354 4.626783175393939
CurrentTrain: epoch  3, batch    24 | loss: 12.2732399Losses:  9.256466701626778 1.0536737442016602 1.0314936637878418 1.7542188912630081
CurrentTrain: epoch  3, batch    25 | loss: 9.2564667Losses:  10.877919666469097 1.0121198892593384 1.0 4.583875171840191
CurrentTrain: epoch  3, batch    26 | loss: 10.8779197Losses:  14.805289551615715 1.1251329183578491 1.006230354309082 8.545034691691399
CurrentTrain: epoch  3, batch    27 | loss: 14.8052896Losses:  10.081014767289162 0.9026243686676025 1.0297592878341675 2.872456207871437
CurrentTrain: epoch  3, batch    28 | loss: 10.0810148Losses:  9.390586078166962 0.9518013000488281 0.9853522777557373 3.1871396899223328
CurrentTrain: epoch  3, batch    29 | loss: 9.3905861Losses:  8.778816882520914 0.9580092430114746 1.0038633346557617 1.427477065473795
CurrentTrain: epoch  3, batch    30 | loss: 8.7788169Losses:  11.05181934311986 1.0791908502578735 1.0 4.368114966899157
CurrentTrain: epoch  3, batch    31 | loss: 11.0518193Losses:  11.39070950075984 0.8920420408248901 1.0222488641738892 4.636078458279371
CurrentTrain: epoch  3, batch    32 | loss: 11.3907095Losses:  9.467899888753891 0.90378737449646 1.0278069972991943 2.814487546682358
CurrentTrain: epoch  3, batch    33 | loss: 9.4678999Losses:  6.776668548583984 0.8286014795303345 1.080162525177002 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 6.7766685Losses:  13.086427599191666 0.9283928871154785 1.0683711767196655 6.317137628793716
CurrentTrain: epoch  3, batch    35 | loss: 13.0864276Losses:  11.424514591693878 0.9212881326675415 1.019339680671692 4.283735573291779
CurrentTrain: epoch  3, batch    36 | loss: 11.4245146Losses:  8.570425897836685 0.9659709930419922 1.0 1.400306612253189
CurrentTrain: epoch  3, batch    37 | loss: 8.5704259Losses:  7.77038886770606 0.8315892219543457 0.9994000196456909 1.416590478271246
CurrentTrain: epoch  3, batch    38 | loss: 7.7703889Losses:  14.433479614555836 0.7997647523880005 1.0123395919799805 7.634614296257496
CurrentTrain: epoch  3, batch    39 | loss: 14.4334796Losses:  8.727654956281185 1.0215771198272705 0.9350346326828003 1.4858784899115562
CurrentTrain: epoch  3, batch    40 | loss: 8.7276550Losses:  10.454960942268372 1.0563760995864868 1.0253947973251343 3.343293309211731
CurrentTrain: epoch  3, batch    41 | loss: 10.4549609Losses:  9.704628147184849 0.9238734245300293 1.0495810508728027 1.9707824364304543
CurrentTrain: epoch  3, batch    42 | loss: 9.7046281Losses:  9.264896739274263 0.8626515865325928 1.0 2.9499114640057087
CurrentTrain: epoch  3, batch    43 | loss: 9.2648967Losses:  7.3553667068481445 1.0042234659194946 1.1299554109573364 -0.0
CurrentTrain: epoch  3, batch    44 | loss: 7.3553667Losses:  7.294580936431885 1.051459550857544 1.0650522708892822 -0.0
CurrentTrain: epoch  3, batch    45 | loss: 7.2945809Losses:  9.354975402355194 0.8835968971252441 1.0 2.801801383495331
CurrentTrain: epoch  3, batch    46 | loss: 9.3549754Losses:  8.708282697945833 0.9550560712814331 1.0173081159591675 1.6303102858364582
CurrentTrain: epoch  3, batch    47 | loss: 8.7082827Losses:  6.035618782043457 0.7398420572280884 1.0295714139938354 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 6.0356188Losses:  13.984832175076008 0.7737003564834595 1.0 7.964923746883869
CurrentTrain: epoch  3, batch    49 | loss: 13.9848322Losses:  10.447098039090633 0.9657855033874512 1.0 4.253953240811825
CurrentTrain: epoch  3, batch    50 | loss: 10.4470980Losses:  9.072310775518417 1.0688669681549072 1.1446400880813599 1.403390258550644
CurrentTrain: epoch  3, batch    51 | loss: 9.0723108Losses:  9.511118680238724 0.9563987255096436 1.0010944604873657 2.8733246624469757
CurrentTrain: epoch  3, batch    52 | loss: 9.5111187Losses:  6.992761135101318 1.038532018661499 1.0405699014663696 -0.0
CurrentTrain: epoch  3, batch    53 | loss: 6.9927611Losses:  7.926205277442932 0.9340865612030029 1.0 1.55231773853302
CurrentTrain: epoch  3, batch    54 | loss: 7.9262053Losses:  10.875279165804386 0.8287590742111206 1.0299971103668213 4.3031832948327065
CurrentTrain: epoch  3, batch    55 | loss: 10.8752792Losses:  8.679145317524672 0.8728914260864258 1.1093369722366333 1.871834259480238
CurrentTrain: epoch  3, batch    56 | loss: 8.6791453Losses:  7.407937049865723 1.0243722200393677 1.0743041038513184 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 7.4079370Losses:  10.237042784690857 0.8247075080871582 1.0 4.264119505882263
CurrentTrain: epoch  3, batch    58 | loss: 10.2370428Losses:  7.397189017385244 0.9644004106521606 0.9802669286727905 1.4503625594079494
CurrentTrain: epoch  3, batch    59 | loss: 7.3971890Losses:  8.444177374243736 0.9124517440795898 1.047977089881897 1.7801554054021835
CurrentTrain: epoch  3, batch    60 | loss: 8.4441774Losses:  12.618201576173306 1.1430734395980835 1.1182703971862793 5.364434562623501
CurrentTrain: epoch  3, batch    61 | loss: 12.6182016Losses:  7.896792940795422 1.1360926628112793 1.0 1.4408641383051872
CurrentTrain: epoch  3, batch    62 | loss: 7.8967929Losses:  7.966706719249487 0.9740084409713745 1.0572482347488403 1.4267906807363033
CurrentTrain: epoch  4, batch     0 | loss: 7.9667067Losses:  6.277750492095947 0.8628147840499878 1.0 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 6.2777505Losses:  6.1544880867004395 0.8571987152099609 1.0 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 6.1544881Losses:  7.315124869346619 0.8867725133895874 1.1186443567276 1.4014796018600464
CurrentTrain: epoch  4, batch     3 | loss: 7.3151249Losses:  6.341700077056885 0.8340834379196167 1.1673851013183594 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 6.3417001Losses:  11.13439330458641 0.7632732391357422 1.0 4.848164647817612
CurrentTrain: epoch  4, batch     5 | loss: 11.1343933Losses:  8.656589835882187 0.9640153646469116 1.1160850524902344 1.4252069890499115
CurrentTrain: epoch  4, batch     6 | loss: 8.6565898Losses:  7.352899521589279 0.7631412744522095 1.0439084768295288 1.416894406080246
CurrentTrain: epoch  4, batch     7 | loss: 7.3528995Losses:  6.97147798538208 1.0923086404800415 1.0785504579544067 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 6.9714780Losses:  10.315954849123955 0.8597028255462646 1.0 4.35860650241375
CurrentTrain: epoch  4, batch     9 | loss: 10.3159548Losses:  8.336761720478535 0.9269154071807861 1.0120826959609985 1.6686837747693062
CurrentTrain: epoch  4, batch    10 | loss: 8.3367617Losses:  10.178328663110733 0.6980974674224854 0.9812408685684204 4.344859272241592
CurrentTrain: epoch  4, batch    11 | loss: 10.1783287Losses:  7.608915090560913 0.9212406873703003 1.0363264083862305 1.406968355178833
CurrentTrain: epoch  4, batch    12 | loss: 7.6089151Losses:  7.170003414154053 0.9391535520553589 1.15054452419281 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 7.1700034Losses:  8.114426076412201 0.8493356704711914 1.0513147115707397 1.4553746581077576
CurrentTrain: epoch  4, batch    14 | loss: 8.1144261Losses:  12.684610515832901 0.9276690483093262 1.1129438877105713 5.7669622004032135
CurrentTrain: epoch  4, batch    15 | loss: 12.6846105Losses:  6.077508926391602 0.7379028797149658 0.9772902727127075 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 6.0775089Losses:  7.539951853454113 0.7836071252822876 1.0469081401824951 1.4657364413142204
CurrentTrain: epoch  4, batch    17 | loss: 7.5399519Losses:  12.089799425564706 0.8858915567398071 1.0540939569473267 5.684146902523935
CurrentTrain: epoch  4, batch    18 | loss: 12.0897994Losses:  5.595280647277832 0.7318203449249268 1.0913114547729492 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 5.5952806Losses:  12.297415234148502 0.9074224233627319 1.07572603225708 5.99261424690485
CurrentTrain: epoch  4, batch    20 | loss: 12.2974152Losses:  9.490607526153326 0.7974222898483276 1.043412685394287 3.1231787465512753
CurrentTrain: epoch  4, batch    21 | loss: 9.4906075Losses:  6.018277168273926 0.9121299982070923 1.09150230884552 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 6.0182772Losses:  8.389078587293625 0.9128146171569824 1.0642781257629395 1.4162211120128632
CurrentTrain: epoch  4, batch    23 | loss: 8.3890786Losses:  11.953979194164276 0.9399303197860718 1.0 5.778362929821014
CurrentTrain: epoch  4, batch    24 | loss: 11.9539792Losses:  6.0825514793396 0.9523817300796509 1.0646270513534546 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 6.0825515Losses:  5.987717151641846 0.9306414127349854 1.0216974020004272 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 5.9877172Losses:  13.34682086110115 0.9590448141098022 1.1465810537338257 5.6616425812244415
CurrentTrain: epoch  4, batch    27 | loss: 13.3468209Losses:  14.417989119887352 0.8511812686920166 1.0425952672958374 8.675096854567528
CurrentTrain: epoch  4, batch    28 | loss: 14.4179891Losses:  8.563249487429857 0.8150737285614014 1.0697835683822632 1.664276022464037
CurrentTrain: epoch  4, batch    29 | loss: 8.5632495Losses:  10.02801437675953 0.8133825063705444 1.0 4.246720507740974
CurrentTrain: epoch  4, batch    30 | loss: 10.0280144Losses:  7.469407349824905 0.8247430324554443 1.0424671173095703 1.42605522274971
CurrentTrain: epoch  4, batch    31 | loss: 7.4694073Losses:  10.95609674602747 0.8582818508148193 0.9632073640823364 4.2679978385567665
CurrentTrain: epoch  4, batch    32 | loss: 10.9560967Losses:  8.663598954677582 0.8236629962921143 1.0 2.8149704337120056
CurrentTrain: epoch  4, batch    33 | loss: 8.6635990Losses:  11.151577539741993 0.984813928604126 1.075441837310791 4.625630922615528
CurrentTrain: epoch  4, batch    34 | loss: 11.1515775Losses:  11.605033442378044 0.7640360593795776 1.0 5.8747697323560715
CurrentTrain: epoch  4, batch    35 | loss: 11.6050334Losses:  8.91486343741417 0.8950697183609009 1.0 2.83956179022789
CurrentTrain: epoch  4, batch    36 | loss: 8.9148634Losses:  11.339169681072235 0.9772846698760986 1.1124833822250366 4.535756289958954
CurrentTrain: epoch  4, batch    37 | loss: 11.3391697Losses:  11.515914052724838 1.0499932765960693 1.0716122388839722 4.649512857198715
CurrentTrain: epoch  4, batch    38 | loss: 11.5159141Losses:  9.576294740661979 0.7644191980361938 0.9947774410247803 3.2094429340213537
CurrentTrain: epoch  4, batch    39 | loss: 9.5762947Losses:  5.8930134773254395 0.9179179668426514 1.0 -0.0
CurrentTrain: epoch  4, batch    40 | loss: 5.8930135Losses:  11.705127600580454 0.8512269258499146 1.0 5.710460070520639
CurrentTrain: epoch  4, batch    41 | loss: 11.7051276Losses:  14.714788353070617 0.9862368106842041 0.9973893165588379 8.105848228558898
CurrentTrain: epoch  4, batch    42 | loss: 14.7147884Losses:  6.840057373046875 1.013401746749878 1.0653003454208374 -0.0
CurrentTrain: epoch  4, batch    43 | loss: 6.8400574Losses:  8.431068317964673 0.8642668724060059 1.01547110080719 1.9098199773579836
CurrentTrain: epoch  4, batch    44 | loss: 8.4310683Losses:  6.043090343475342 0.7896279096603394 1.0 -0.0
CurrentTrain: epoch  4, batch    45 | loss: 6.0430903Losses:  8.880010291934013 0.8039195537567139 1.0443663597106934 2.957954093813896
CurrentTrain: epoch  4, batch    46 | loss: 8.8800103Losses:  9.420566607266665 1.0136096477508545 0.992547869682312 2.8702641017735004
CurrentTrain: epoch  4, batch    47 | loss: 9.4205666Losses:  9.753171999007463 0.8737905025482178 1.0247939825057983 3.1334353275597095
CurrentTrain: epoch  4, batch    48 | loss: 9.7531720Losses:  6.417660236358643 0.9090875387191772 1.0 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 6.4176602Losses:  9.259449504315853 0.8542635440826416 1.0 2.89858391135931
CurrentTrain: epoch  4, batch    50 | loss: 9.2594495Losses:  13.39402537420392 1.0708286762237549 1.0043569803237915 5.993590880185366
CurrentTrain: epoch  4, batch    51 | loss: 13.3940254Losses:  6.451361179351807 0.8545941114425659 1.0065968036651611 -0.0
CurrentTrain: epoch  4, batch    52 | loss: 6.4513612Losses:  11.343776810914278 0.9716124534606934 0.9446479082107544 4.733625043183565
CurrentTrain: epoch  4, batch    53 | loss: 11.3437768Losses:  13.48657512664795 0.8451393842697144 0.9672907590866089 7.307366371154785
CurrentTrain: epoch  4, batch    54 | loss: 13.4865751Losses:  7.498916480690241 0.6755372285842896 1.0851836204528809 1.473131988197565
CurrentTrain: epoch  4, batch    55 | loss: 7.4989165Losses:  10.497044179588556 1.0220755338668823 1.0514709949493408 3.211500260978937
CurrentTrain: epoch  4, batch    56 | loss: 10.4970442Losses:  19.563965260982513 0.8277466297149658 0.9635912179946899 12.881839215755463
CurrentTrain: epoch  4, batch    57 | loss: 19.5639653Losses:  8.878317337483168 0.8503482341766357 1.0385088920593262 2.902605514973402
CurrentTrain: epoch  4, batch    58 | loss: 8.8783173Losses:  9.078331552445889 0.9409202337265015 1.0436382293701172 2.872005544602871
CurrentTrain: epoch  4, batch    59 | loss: 9.0783316Losses:  7.486054062843323 0.7991293668746948 1.0 1.4019981622695923
CurrentTrain: epoch  4, batch    60 | loss: 7.4860541Losses:  10.969320438802242 0.8843828439712524 1.0217512845993042 4.005473278462887
CurrentTrain: epoch  4, batch    61 | loss: 10.9693204Losses:  6.187823295593262 0.7787680625915527 1.1151044368743896 -0.0
CurrentTrain: epoch  4, batch    62 | loss: 6.1878233Losses:  7.368933886289597 0.8258836269378662 0.9720635414123535 1.3987142741680145
CurrentTrain: epoch  5, batch     0 | loss: 7.3689339Losses:  10.648663558065891 0.8271346092224121 1.0481081008911133 4.578771151602268
CurrentTrain: epoch  5, batch     1 | loss: 10.6486636Losses:  10.782994110137224 0.8244798183441162 1.0399224758148193 4.299063999205828
CurrentTrain: epoch  5, batch     2 | loss: 10.7829941Losses:  8.1328866481781 0.807076096534729 1.0787612199783325 1.6553184986114502
CurrentTrain: epoch  5, batch     3 | loss: 8.1328866Losses:  9.251224648207426 0.7862273454666138 1.0216939449310303 3.163188111037016
CurrentTrain: epoch  5, batch     4 | loss: 9.2512246Losses:  6.519000053405762 0.9458982944488525 0.9684735536575317 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 6.5190001Losses:  11.158037073910236 0.6740180253982544 1.0 5.016232855618
CurrentTrain: epoch  5, batch     6 | loss: 11.1580371Losses:  6.984390944242477 0.5582211017608643 1.0 1.4029805362224579
CurrentTrain: epoch  5, batch     7 | loss: 6.9843909Losses:  10.177869208157063 0.9158329963684082 1.1366468667984009 3.269291765987873
CurrentTrain: epoch  5, batch     8 | loss: 10.1778692Losses:  11.197111375629902 0.8921670913696289 1.0026072263717651 4.3642036989331245
CurrentTrain: epoch  5, batch     9 | loss: 11.1971114Losses:  5.777472496032715 0.731407880783081 1.0 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 5.7774725Losses:  9.464693956077099 0.9362469911575317 1.0 3.166047029197216
CurrentTrain: epoch  5, batch    11 | loss: 9.4646940Losses:  8.752777755260468 0.828066349029541 1.0 2.8004462122917175
CurrentTrain: epoch  5, batch    12 | loss: 8.7527778Losses:  7.525242179632187 0.7039790153503418 1.0 1.392402023077011
CurrentTrain: epoch  5, batch    13 | loss: 7.5252422Losses:  6.473196506500244 0.8267215490341187 1.0589624643325806 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 6.4731965Losses:  11.833315569907427 0.8761522769927979 1.0 5.723964411765337
CurrentTrain: epoch  5, batch    15 | loss: 11.8333156Losses:  10.67352319508791 0.7560436725616455 0.9810910224914551 4.353174455463886
CurrentTrain: epoch  5, batch    16 | loss: 10.6735232Losses:  7.429824024438858 0.7578243017196655 1.0 1.4177657663822174
CurrentTrain: epoch  5, batch    17 | loss: 7.4298240Losses:  10.478502102196217 0.8275506496429443 1.0 4.285634823143482
CurrentTrain: epoch  5, batch    18 | loss: 10.4785021Losses:  10.627781003713608 0.8320430517196655 1.0 4.238262265920639
CurrentTrain: epoch  5, batch    19 | loss: 10.6277810Losses:  18.326861798763275 0.6495041847229004 1.0 12.958024442195892
CurrentTrain: epoch  5, batch    20 | loss: 18.3268618Losses:  7.217058569192886 0.7455848455429077 1.031740427017212 1.4353202879428864
CurrentTrain: epoch  5, batch    21 | loss: 7.2170586Losses:  6.921237468719482 0.7388103008270264 1.1529226303100586 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 6.9212375Losses:  6.520482540130615 0.7580281496047974 1.0328006744384766 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 6.5204825Losses:  7.991017229855061 0.7491734027862549 1.000058889389038 1.4899944141507149
CurrentTrain: epoch  5, batch    24 | loss: 7.9910172Losses:  5.213541507720947 0.7674665451049805 1.0 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 5.2135415Losses:  7.876387894153595 0.9282832145690918 1.0003327131271362 1.4368141293525696
CurrentTrain: epoch  5, batch    26 | loss: 7.8763879Losses:  5.84050178527832 0.7595301866531372 1.031791090965271 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 5.8405018Losses:  9.626147598028183 0.7618855237960815 1.0275291204452515 4.201284736394882
CurrentTrain: epoch  5, batch    28 | loss: 9.6261476Losses:  7.669398803263903 0.8183352947235107 1.0946667194366455 1.5480838008224964
CurrentTrain: epoch  5, batch    29 | loss: 7.6693988Losses:  15.395155437290668 0.8558086156845093 1.0494110584259033 9.607503898441792
CurrentTrain: epoch  5, batch    30 | loss: 15.3951554Losses:  11.995196308940649 0.917386531829834 0.9691147804260254 5.779514279216528
CurrentTrain: epoch  5, batch    31 | loss: 11.9951963Losses:  11.656500466167927 0.9958062171936035 1.0623819828033447 5.6644531562924385
CurrentTrain: epoch  5, batch    32 | loss: 11.6565005Losses:  7.3149517476558685 0.8594610691070557 1.0 1.4413021504878998
CurrentTrain: epoch  5, batch    33 | loss: 7.3149517Losses:  5.425863265991211 0.7425366640090942 1.0 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 5.4258633Losses:  6.297464370727539 0.8086224794387817 1.0885941982269287 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 6.2974644Losses:  6.070714473724365 0.8687589168548584 1.029298186302185 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 6.0707145Losses:  6.874485284090042 0.7404340505599976 1.0946965217590332 1.4082386791706085
CurrentTrain: epoch  5, batch    37 | loss: 6.8744853Losses:  14.157379232347012 0.6820106506347656 1.0 7.982446752488613
CurrentTrain: epoch  5, batch    38 | loss: 14.1573792Losses:  11.000939503312111 0.726953387260437 0.9876487255096436 5.29098142683506
CurrentTrain: epoch  5, batch    39 | loss: 11.0009395Losses:  10.802646555006504 0.6213388442993164 0.9591408967971802 4.392834581434727
CurrentTrain: epoch  5, batch    40 | loss: 10.8026466Losses:  5.610745906829834 0.8352545499801636 1.0 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 5.6107459Losses:  7.28064751625061 0.8511060476303101 1.0357253551483154 1.3991363048553467
CurrentTrain: epoch  5, batch    42 | loss: 7.2806475Losses:  5.72950553894043 0.6862189769744873 1.0331693887710571 -0.0
CurrentTrain: epoch  5, batch    43 | loss: 5.7295055Losses:  7.5286043882369995 0.907628059387207 1.0124611854553223 1.4601868391036987
CurrentTrain: epoch  5, batch    44 | loss: 7.5286044Losses:  6.228926181793213 0.9137439727783203 1.0 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 6.2289262Losses:  8.72281888127327 0.6435546875 1.0 3.133832961320877
CurrentTrain: epoch  5, batch    46 | loss: 8.7228189Losses:  5.74780797958374 0.8916677236557007 1.0602096319198608 -0.0
CurrentTrain: epoch  5, batch    47 | loss: 5.7478080Losses:  11.706128917634487 0.7917605638504028 1.0192841291427612 5.738172851502895
CurrentTrain: epoch  5, batch    48 | loss: 11.7061289Losses:  8.63771552219987 0.8402653932571411 1.0 2.9904338754713535
CurrentTrain: epoch  5, batch    49 | loss: 8.6377155Losses:  6.588054656982422 0.8887801170349121 1.0467677116394043 -0.0
CurrentTrain: epoch  5, batch    50 | loss: 6.5880547Losses:  10.155381806194782 0.9665966033935547 0.9482544660568237 4.2159105613827705
CurrentTrain: epoch  5, batch    51 | loss: 10.1553818Losses:  7.01350411772728 0.6597210168838501 0.9688169956207275 1.412223905324936
CurrentTrain: epoch  5, batch    52 | loss: 7.0135041Losses:  10.2862004712224 0.711236834526062 1.053186058998108 4.24022526293993
CurrentTrain: epoch  5, batch    53 | loss: 10.2862005Losses:  8.674734599888325 0.7037866115570068 1.0 2.9126815870404243
CurrentTrain: epoch  5, batch    54 | loss: 8.6747346Losses:  9.355128109455109 0.65704345703125 1.0225247144699097 2.9644506573677063
CurrentTrain: epoch  5, batch    55 | loss: 9.3551281Losses:  10.310898333787918 0.5992430448532104 1.0549900531768799 4.308154135942459
CurrentTrain: epoch  5, batch    56 | loss: 10.3108983Losses:  10.711883433163166 0.6792199611663818 0.988728404045105 3.739261992275715
CurrentTrain: epoch  5, batch    57 | loss: 10.7118834Losses:  8.349056486040354 0.7308468818664551 1.0 2.8343117274343967
CurrentTrain: epoch  5, batch    58 | loss: 8.3490565Losses:  7.183640517294407 0.6105152368545532 0.9355400800704956 1.546053446829319
CurrentTrain: epoch  5, batch    59 | loss: 7.1836405Losses:  10.186919450759888 0.6879411935806274 1.0 4.21649432182312
CurrentTrain: epoch  5, batch    60 | loss: 10.1869195Losses:  7.058362632989883 0.7483794689178467 1.0475115776062012 1.3981854021549225
CurrentTrain: epoch  5, batch    61 | loss: 7.0583626Losses:  9.663525484502316 0.6241576671600342 1.0 4.287237547338009
CurrentTrain: epoch  5, batch    62 | loss: 9.6635255Losses:  9.73662580549717 0.6429007053375244 1.0 4.294571056962013
CurrentTrain: epoch  6, batch     0 | loss: 9.7366258Losses:  7.278438299894333 0.6795682907104492 1.0 1.4712865054607391
CurrentTrain: epoch  6, batch     1 | loss: 7.2784383Losses:  7.621063560247421 0.7989128828048706 1.0895401239395142 1.40372833609581
CurrentTrain: epoch  6, batch     2 | loss: 7.6210636Losses:  7.045358777046204 0.7968084812164307 1.0 1.4093705415725708
CurrentTrain: epoch  6, batch     3 | loss: 7.0453588Losses:  9.987532168626785 0.6769043207168579 0.9838666915893555 4.282085448503494
CurrentTrain: epoch  6, batch     4 | loss: 9.9875322Losses:  7.194841712713242 0.6470496654510498 1.0 1.436835139989853
CurrentTrain: epoch  6, batch     5 | loss: 7.1948417Losses:  10.313493810594082 0.8310281038284302 1.061977744102478 4.240279279649258
CurrentTrain: epoch  6, batch     6 | loss: 10.3134938Losses:  8.590131644159555 0.6323337554931641 1.041131615638733 2.867655161768198
CurrentTrain: epoch  6, batch     7 | loss: 8.5901316Losses:  5.294101238250732 0.49267446994781494 1.0 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 5.2941012Losses:  11.364902079105377 0.6339743137359619 1.0210380554199219 5.003123342990875
CurrentTrain: epoch  6, batch     9 | loss: 11.3649021Losses:  5.681731224060059 0.6389565467834473 0.9898114204406738 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 5.6817312Losses:  5.909700393676758 0.6528537273406982 0.961685061454773 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 5.9097004Losses:  5.636295795440674 0.5226986408233643 1.0 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 5.6362958Losses:  8.651455469429493 0.7150442600250244 1.0 2.922355242073536
CurrentTrain: epoch  6, batch    13 | loss: 8.6514555Losses:  5.6679229736328125 0.5423264503479004 1.0 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 5.6679230Losses:  6.865518659353256 0.7760881185531616 0.9653220176696777 1.4089957177639008
CurrentTrain: epoch  6, batch    15 | loss: 6.8655187Losses:  7.628685414791107 0.8654767274856567 1.0 1.39071124792099
CurrentTrain: epoch  6, batch    16 | loss: 7.6286854Losses:  8.324621319770813 0.7184286117553711 1.0 2.8821521997451782
CurrentTrain: epoch  6, batch    17 | loss: 8.3246213Losses:  5.841952800750732 0.7518913745880127 1.0 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 5.8419528Losses:  13.882428407669067 0.6702146530151367 0.9694329500198364 7.346053838729858
CurrentTrain: epoch  6, batch    19 | loss: 13.8824284Losses:  7.040779173374176 0.6274886131286621 1.0319286584854126 1.4404144883155823
CurrentTrain: epoch  6, batch    20 | loss: 7.0407792Losses:  5.421308994293213 0.6795456409454346 1.0 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 5.4213090Losses:  8.37881076335907 0.6426519155502319 1.0 2.857836127281189
CurrentTrain: epoch  6, batch    22 | loss: 8.3788108Losses:  5.532788276672363 0.7703036069869995 1.0 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 5.5327883Losses:  5.802006721496582 0.5223592519760132 1.0 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 5.8020067Losses:  11.198883190751076 0.8232028484344482 1.0 5.748752728104591
CurrentTrain: epoch  6, batch    25 | loss: 11.1988832Losses:  8.628896776586771 0.8252928256988525 1.0 2.8558149971067905
CurrentTrain: epoch  6, batch    26 | loss: 8.6288968Losses:  10.450193468481302 0.6901490688323975 1.053101658821106 4.32259089127183
CurrentTrain: epoch  6, batch    27 | loss: 10.4501935Losses:  6.994944244623184 0.8421617746353149 1.0 1.419422298669815
CurrentTrain: epoch  6, batch    28 | loss: 6.9949442Losses:  9.649514973163605 0.6380479335784912 1.0 4.205894291400909
CurrentTrain: epoch  6, batch    29 | loss: 9.6495150Losses:  5.601230144500732 0.7030646800994873 1.0177994966506958 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 5.6012301Losses:  12.750350177288055 0.7208130359649658 1.0 7.412526309490204
CurrentTrain: epoch  6, batch    31 | loss: 12.7503502Losses:  9.986057225614786 0.607966423034668 1.0549436807632446 4.376005116850138
CurrentTrain: epoch  6, batch    32 | loss: 9.9860572Losses:  8.40365007519722 0.5930105447769165 1.0024735927581787 2.840450555086136
CurrentTrain: epoch  6, batch    33 | loss: 8.4036501Losses:  10.37949388474226 0.7147579193115234 1.0538851022720337 4.772732429206371
CurrentTrain: epoch  6, batch    34 | loss: 10.3794939Losses:  8.251347985118628 0.6410813331604004 0.9614651203155518 2.903737034648657
CurrentTrain: epoch  6, batch    35 | loss: 8.2513480Losses:  7.029046654701233 0.6927531957626343 1.0 1.4462043046951294
CurrentTrain: epoch  6, batch    36 | loss: 7.0290467Losses:  5.7753167152404785 0.751237154006958 1.0 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 5.7753167Losses:  8.575075685977936 0.7511752843856812 1.0 2.808450758457184
CurrentTrain: epoch  6, batch    38 | loss: 8.5750757Losses:  5.370457649230957 0.35575127601623535 1.0433541536331177 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 5.3704576Losses:  7.037359997630119 0.5723580121994019 1.0 1.6054542511701584
CurrentTrain: epoch  6, batch    40 | loss: 7.0373600Losses:  5.370534896850586 0.6077250242233276 1.0351910591125488 -0.0
CurrentTrain: epoch  6, batch    41 | loss: 5.3705349Losses:  7.038597822189331 0.7838400602340698 1.0 1.4091079235076904
CurrentTrain: epoch  6, batch    42 | loss: 7.0385978Losses:  8.546003729104996 0.7036564350128174 1.0577586889266968 2.8199447691440582
CurrentTrain: epoch  6, batch    43 | loss: 8.5460037Losses:  6.58294989541173 0.5858392715454102 1.0 1.4834129996597767
CurrentTrain: epoch  6, batch    44 | loss: 6.5829499Losses:  5.522303581237793 0.6731212139129639 1.0433189868927002 -0.0
CurrentTrain: epoch  6, batch    45 | loss: 5.5223036Losses:  7.350113540887833 0.7818559408187866 1.0125532150268555 1.4071036875247955
CurrentTrain: epoch  6, batch    46 | loss: 7.3501135Losses:  7.29496094584465 0.8036530017852783 1.0 1.773334950208664
CurrentTrain: epoch  6, batch    47 | loss: 7.2949609Losses:  11.422023296356201 0.7586284875869751 1.055404782295227 5.868867874145508
CurrentTrain: epoch  6, batch    48 | loss: 11.4220233Losses:  10.10162116959691 0.7120393514633179 0.9907581806182861 4.509576816111803
CurrentTrain: epoch  6, batch    49 | loss: 10.1016212Losses:  8.584529146552086 0.7823257446289062 1.0310395956039429 2.9472243636846542
CurrentTrain: epoch  6, batch    50 | loss: 8.5845291Losses:  6.614545375108719 0.5624352693557739 1.0 1.3908291161060333
CurrentTrain: epoch  6, batch    51 | loss: 6.6145454Losses:  9.696517497301102 0.8536832332611084 1.1354455947875977 2.8498750030994415
CurrentTrain: epoch  6, batch    52 | loss: 9.6965175Losses:  8.69552619382739 0.6411374807357788 1.0 2.8807368986308575
CurrentTrain: epoch  6, batch    53 | loss: 8.6955262Losses:  10.479037329554558 0.8194198608398438 1.0 4.800733610987663
CurrentTrain: epoch  6, batch    54 | loss: 10.4790373Losses:  9.128387734293938 0.7374354600906372 1.0 3.1587074249982834
CurrentTrain: epoch  6, batch    55 | loss: 9.1283877Losses:  6.7462745904922485 0.6008552312850952 1.0 1.3994165658950806
CurrentTrain: epoch  6, batch    56 | loss: 6.7462746Losses:  5.839509010314941 0.550603985786438 1.0506584644317627 -0.0
CurrentTrain: epoch  6, batch    57 | loss: 5.8395090Losses:  6.740513574331999 0.5081665515899658 1.0 1.4485857598483562
CurrentTrain: epoch  6, batch    58 | loss: 6.7405136Losses:  8.693960636854172 0.799060583114624 0.9647802114486694 2.8435401618480682
CurrentTrain: epoch  6, batch    59 | loss: 8.6939606Losses:  9.555543534457684 0.5944808721542358 1.0 4.274628274142742
CurrentTrain: epoch  6, batch    60 | loss: 9.5555435Losses:  7.884124666452408 0.6758959293365479 1.1140944957733154 1.4230112135410309
CurrentTrain: epoch  6, batch    61 | loss: 7.8841247Losses:  5.263854503631592 0.4966161251068115 1.0 -0.0
CurrentTrain: epoch  6, batch    62 | loss: 5.2638545Losses:  15.008627898991108 0.7823930978775024 1.0 9.112883098423481
CurrentTrain: epoch  7, batch     0 | loss: 15.0086279Losses:  6.752673357725143 0.5784652233123779 1.0 1.4556481540203094
CurrentTrain: epoch  7, batch     1 | loss: 6.7526734Losses:  8.481128975749016 0.7401177883148193 1.0 2.9335020035505295
CurrentTrain: epoch  7, batch     2 | loss: 8.4811290Losses:  6.781623125076294 0.515662670135498 1.0 1.4203226566314697
CurrentTrain: epoch  7, batch     3 | loss: 6.7816231Losses:  7.319864273071289 0.6746913194656372 1.067242980003357 1.579012393951416
CurrentTrain: epoch  7, batch     4 | loss: 7.3198643Losses:  7.928533375263214 0.5312349796295166 1.0 2.832529842853546
CurrentTrain: epoch  7, batch     5 | loss: 7.9285334Losses:  5.467772483825684 0.582720160484314 1.0 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 5.4677725Losses:  10.225556638091803 0.7570093870162964 1.0 4.374759938567877
CurrentTrain: epoch  7, batch     7 | loss: 10.2255566Losses:  7.4689174592494965 0.735113263130188 1.049708604812622 1.401230901479721
CurrentTrain: epoch  7, batch     8 | loss: 7.4689175Losses:  9.672404527664185 0.47792625427246094 0.9636504650115967 4.251542329788208
CurrentTrain: epoch  7, batch     9 | loss: 9.6724045Losses:  5.10867166519165 0.503318190574646 1.0 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 5.1086717Losses:  8.368589460849762 0.7884293794631958 1.0 2.823182165622711
CurrentTrain: epoch  7, batch    11 | loss: 8.3685895Losses:  6.878489974886179 0.6150350570678711 1.0 1.471922878175974
CurrentTrain: epoch  7, batch    12 | loss: 6.8784900Losses:  6.802819013595581 0.5947973728179932 1.0 1.4044177532196045
CurrentTrain: epoch  7, batch    13 | loss: 6.8028190Losses:  8.14473058655858 0.5619720220565796 1.0 2.8594269938766956
CurrentTrain: epoch  7, batch    14 | loss: 8.1447306Losses:  9.71370504796505 0.5904816389083862 0.9936995506286621 4.276392921805382
CurrentTrain: epoch  7, batch    15 | loss: 9.7137050Losses:  8.197097420692444 0.5274361371994019 1.0 2.800057053565979
CurrentTrain: epoch  7, batch    16 | loss: 8.1970974Losses:  6.5514815747737885 0.5021616220474243 1.0 1.402747005224228
CurrentTrain: epoch  7, batch    17 | loss: 6.5514816Losses:  8.231746524572372 0.7341961860656738 1.0 2.817849963903427
CurrentTrain: epoch  7, batch    18 | loss: 8.2317465Losses:  5.2980217933654785 0.5011893510818481 1.0 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 5.2980218Losses:  10.128088045865297 0.6578260660171509 1.0 4.761413145810366
CurrentTrain: epoch  7, batch    20 | loss: 10.1280880Losses:  5.308051586151123 0.5724434852600098 0.9837504625320435 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 5.3080516Losses:  7.9930768348276615 0.5941001176834106 1.0 2.8256178237497807
CurrentTrain: epoch  7, batch    22 | loss: 7.9930768Losses:  5.161030292510986 0.6762173175811768 1.0 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 5.1610303Losses:  6.942100495100021 0.4822300672531128 1.0 1.6393847167491913
CurrentTrain: epoch  7, batch    24 | loss: 6.9421005Losses:  5.282912254333496 0.7402455806732178 1.0 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 5.2829123Losses:  8.28265442326665 0.6524959802627563 1.0 2.865671295672655
CurrentTrain: epoch  7, batch    26 | loss: 8.2826544Losses:  9.52854049578309 0.5630366802215576 1.0 4.273542765527964
CurrentTrain: epoch  7, batch    27 | loss: 9.5285405Losses:  9.541981779038906 0.553152322769165 1.0 4.372458539903164
CurrentTrain: epoch  7, batch    28 | loss: 9.5419818Losses:  8.021039750427008 0.5510814189910889 1.0 2.8597724698483944
CurrentTrain: epoch  7, batch    29 | loss: 8.0210398Losses:  6.963403642177582 0.627638578414917 1.0 1.3929285407066345
CurrentTrain: epoch  7, batch    30 | loss: 6.9634036Losses:  8.22892089560628 0.6213151216506958 1.0 2.9847783632576466
CurrentTrain: epoch  7, batch    31 | loss: 8.2289209Losses:  9.446098029613495 0.5622451305389404 1.0 4.319444835186005
CurrentTrain: epoch  7, batch    32 | loss: 9.4460980Losses:  5.225301742553711 0.5991902351379395 1.0 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 5.2253017Losses:  6.5582581497728825 0.6030696630477905 1.0 1.4370585419237614
CurrentTrain: epoch  7, batch    34 | loss: 6.5582581Losses:  8.158891417086124 0.4224933385848999 1.0 2.9397961869835854
CurrentTrain: epoch  7, batch    35 | loss: 8.1588914Losses:  7.309839457273483 0.7064833641052246 1.0029460191726685 1.4281871020793915
CurrentTrain: epoch  7, batch    36 | loss: 7.3098395Losses:  5.19850492477417 0.5457793474197388 1.0 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 5.1985049Losses:  9.287016496062279 0.5498032569885254 1.0482218265533447 4.228957757353783
CurrentTrain: epoch  7, batch    38 | loss: 9.2870165Losses:  15.84746041148901 0.7130916118621826 1.0329101085662842 9.956204555928707
CurrentTrain: epoch  7, batch    39 | loss: 15.8474604Losses:  6.751759584993124 0.5558512210845947 1.0 1.458318766206503
CurrentTrain: epoch  7, batch    40 | loss: 6.7517596Losses:  8.071324829012156 0.5395940542221069 1.0 2.844399932771921
CurrentTrain: epoch  7, batch    41 | loss: 8.0713248Losses:  8.168005809187889 0.5766875743865967 1.00808846950531 2.88612399995327
CurrentTrain: epoch  7, batch    42 | loss: 8.1680058Losses:  10.892874665558338 0.6793214082717896 1.0 5.621138043701649
CurrentTrain: epoch  7, batch    43 | loss: 10.8928747Losses:  8.134001761674881 0.6362031698226929 1.0 2.832823783159256
CurrentTrain: epoch  7, batch    44 | loss: 8.1340018Losses:  9.608916718512774 0.523661732673645 1.05321204662323 4.279861886054277
CurrentTrain: epoch  7, batch    45 | loss: 9.6089167Losses:  8.719004023820162 0.8517866134643555 1.0195165872573853 2.916674006730318
CurrentTrain: epoch  7, batch    46 | loss: 8.7190040Losses:  8.729701317846775 0.7896867990493774 1.0157763957977295 2.8668902292847633
CurrentTrain: epoch  7, batch    47 | loss: 8.7297013Losses:  6.606401294469833 0.5724465847015381 1.0 1.3955205380916595
CurrentTrain: epoch  7, batch    48 | loss: 6.6064013Losses:  5.519322395324707 0.45143330097198486 1.0 -0.0
CurrentTrain: epoch  7, batch    49 | loss: 5.5193224Losses:  8.213354203850031 0.6460897922515869 1.0 2.861738298088312
CurrentTrain: epoch  7, batch    50 | loss: 8.2133542Losses:  13.926623560488224 0.4753589630126953 1.0 8.934614397585392
CurrentTrain: epoch  7, batch    51 | loss: 13.9266236Losses:  8.06597926095128 0.4898343086242676 1.0 2.9171421714127064
CurrentTrain: epoch  7, batch    52 | loss: 8.0659793Losses:  9.68223286792636 0.5010271072387695 1.0 4.425498973578215
CurrentTrain: epoch  7, batch    53 | loss: 9.6822329Losses:  10.822555862367153 0.6705278158187866 1.0 5.655413471162319
CurrentTrain: epoch  7, batch    54 | loss: 10.8225559Losses:  5.282983779907227 0.6145063638687134 1.0 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 5.2829838Losses:  9.99396250396967 0.5897842645645142 1.0 4.492104269564152
CurrentTrain: epoch  7, batch    56 | loss: 9.9939625Losses:  8.086898822337389 0.5650607347488403 1.0 2.9160933680832386
CurrentTrain: epoch  7, batch    57 | loss: 8.0868988Losses:  9.555555194616318 0.6756737232208252 1.0 4.420440047979355
CurrentTrain: epoch  7, batch    58 | loss: 9.5555552Losses:  8.351112239062786 0.5362133979797363 1.0428062677383423 3.13253103941679
CurrentTrain: epoch  7, batch    59 | loss: 8.3511122Losses:  9.427657999098301 0.6737039089202881 1.0 4.210160173475742
CurrentTrain: epoch  7, batch    60 | loss: 9.4276580Losses:  5.533276557922363 0.6752151250839233 1.0 -0.0
CurrentTrain: epoch  7, batch    61 | loss: 5.5332766Losses:  9.257544722408056 0.40734148025512695 1.0 4.242442335933447
CurrentTrain: epoch  7, batch    62 | loss: 9.2575447Losses:  15.797889560461044 0.4875572919845581 1.0 10.625934928655624
CurrentTrain: epoch  8, batch     0 | loss: 15.7978896Losses:  6.447429150342941 0.5884618759155273 1.0 1.390931099653244
CurrentTrain: epoch  8, batch     1 | loss: 6.4474292Losses:  6.564231783151627 0.6776794195175171 1.0 1.3924597799777985
CurrentTrain: epoch  8, batch     2 | loss: 6.5642318Losses:  10.054163113236427 0.6713942289352417 1.0 4.347512856125832
CurrentTrain: epoch  8, batch     3 | loss: 10.0541631Losses:  9.320329762995243 0.4934037923812866 1.0 4.199647046625614
CurrentTrain: epoch  8, batch     4 | loss: 9.3203298Losses:  9.44706006348133 0.6819523572921753 1.0 4.246623948216438
CurrentTrain: epoch  8, batch     5 | loss: 9.4470601Losses:  7.8435872830450535 0.5493557453155518 1.0 2.8204273022711277
CurrentTrain: epoch  8, batch     6 | loss: 7.8435873Losses:  6.5309306383132935 0.563544750213623 1.0 1.4148999452590942
CurrentTrain: epoch  8, batch     7 | loss: 6.5309306Losses:  7.416508831083775 0.5850405693054199 0.9616345167160034 1.7700688019394875
CurrentTrain: epoch  8, batch     8 | loss: 7.4165088Losses:  5.230971813201904 0.42698192596435547 1.0446240901947021 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 5.2309718Losses:  11.434011735022068 0.6733230352401733 1.0 6.109217919409275
CurrentTrain: epoch  8, batch    10 | loss: 11.4340117Losses:  5.220670700073242 0.43914270401000977 1.0 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 5.2206707Losses:  6.413210570812225 0.46594905853271484 1.0 1.4044315218925476
CurrentTrain: epoch  8, batch    12 | loss: 6.4132106Losses:  9.70783931016922 0.5336785316467285 1.0 4.52214652299881
CurrentTrain: epoch  8, batch    13 | loss: 9.7078393Losses:  6.6631652526557446 0.49753499031066895 1.0 1.6259905509650707
CurrentTrain: epoch  8, batch    14 | loss: 6.6631653Losses:  9.259436130523682 0.5717164278030396 1.0 4.292947292327881
CurrentTrain: epoch  8, batch    15 | loss: 9.2594361Losses:  12.436473160982132 0.3307880163192749 1.0 7.585766106843948
CurrentTrain: epoch  8, batch    16 | loss: 12.4364732Losses:  8.130967147648335 0.5045143365859985 1.0 2.8436818197369576
CurrentTrain: epoch  8, batch    17 | loss: 8.1309671Losses:  4.967548370361328 0.47808098793029785 1.0 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 4.9675484Losses:  6.549441907554865 0.48546767234802246 1.0 1.4129520393908024
CurrentTrain: epoch  8, batch    19 | loss: 6.5494419Losses:  6.3522613644599915 0.48325252532958984 1.0 1.392020046710968
CurrentTrain: epoch  8, batch    20 | loss: 6.3522614Losses:  6.873835848644376 0.6291590929031372 1.0 1.8027942646294832
CurrentTrain: epoch  8, batch    21 | loss: 6.8738358Losses:  8.044611282646656 0.6137435436248779 1.0 2.863406963646412
CurrentTrain: epoch  8, batch    22 | loss: 8.0446113Losses:  6.539647284895182 0.4917137622833252 1.0 1.468887034803629
CurrentTrain: epoch  8, batch    23 | loss: 6.5396473Losses:  11.91003204882145 0.5422132015228271 1.0311683416366577 5.938516393303871
CurrentTrain: epoch  8, batch    24 | loss: 11.9100320Losses:  6.671662002801895 0.6677629947662354 1.0308079719543457 1.4096762239933014
CurrentTrain: epoch  8, batch    25 | loss: 6.6716620Losses:  6.355593204498291 0.39301764965057373 1.0 1.4214658737182617
CurrentTrain: epoch  8, batch    26 | loss: 6.3555932Losses:  9.238777130842209 0.37191498279571533 1.0 4.226444691419601
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [3 0 1 0 0 2 1 0 0 0]
Losses:  19.18977378308773 1.3207916021347046 1.3458771705627441 7.907491907477379
CurrentTrain: epoch  0, batch     0 | loss: 19.1897738Losses:  19.210852682590485 1.5247585773468018 1.6876516342163086 7.944523870944977
CurrentTrain: epoch  0, batch     1 | loss: 19.2108527Losses:  16.759836986660957 1.4700276851654053 1.4787743091583252 5.398321941494942
CurrentTrain: epoch  0, batch     2 | loss: 16.7598370Losses:  18.140047177672386 1.3889496326446533 1.496239185333252 7.702575787901878
CurrentTrain: epoch  0, batch     3 | loss: 18.1400472Losses:  14.045259267091751 1.4918127059936523 1.5287195444107056 2.6917674839496613
CurrentTrain: epoch  0, batch     4 | loss: 14.0452593Losses:  17.153204172849655 1.3584920167922974 1.3438310623168945 7.428911417722702
CurrentTrain: epoch  0, batch     5 | loss: 17.1532042Losses:  21.253479689359665 1.4273053407669067 1.3849155902862549 10.632128447294235
CurrentTrain: epoch  0, batch     6 | loss: 21.2534797Losses:  16.22461025416851 1.5807945728302002 1.5106048583984375 5.3497113436460495
CurrentTrain: epoch  0, batch     7 | loss: 16.2246103Losses:  22.056477025151253 1.4920270442962646 1.1740288734436035 10.78440423309803
CurrentTrain: epoch  0, batch     8 | loss: 22.0564770Losses:  14.665868133306503 1.3666996955871582 1.3416755199432373 4.99943670630455
CurrentTrain: epoch  0, batch     9 | loss: 14.6658681Losses:  15.067694336175919 1.397971510887146 1.337960958480835 5.277576118707657
CurrentTrain: epoch  0, batch    10 | loss: 15.0676943Losses:  15.724786177277565 1.3607726097106934 1.1261059045791626 5.097143545746803
CurrentTrain: epoch  0, batch    11 | loss: 15.7247862Losses:  10.757131576538086 1.4334156513214111 1.3816404342651367 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 10.7571316Losses:  10.685158729553223 1.486514925956726 1.3707386255264282 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 10.6851587Losses:  18.462083771824837 1.3792624473571777 1.2135896682739258 7.9057197123765945
CurrentTrain: epoch  0, batch    14 | loss: 18.4620838Losses:  17.045981720089912 1.1730740070343018 1.0949692726135254 7.703425720334053
CurrentTrain: epoch  0, batch    15 | loss: 17.0459817Losses:  14.424705639481544 1.0686943531036377 1.0708407163619995 5.093170300126076
CurrentTrain: epoch  0, batch    16 | loss: 14.4247056Losses:  15.303474470973015 1.227520227432251 1.0919933319091797 5.339455649256706
CurrentTrain: epoch  0, batch    17 | loss: 15.3034745Losses:  15.685340404510498 1.2311136722564697 1.046138048171997 5.475919246673584
CurrentTrain: epoch  0, batch    18 | loss: 15.6853404Losses:  12.345913603901863 1.2202751636505127 0.9504005908966064 2.4592101126909256
CurrentTrain: epoch  0, batch    19 | loss: 12.3459136Losses:  16.022246748209 1.1746174097061157 1.0538519620895386 7.303298383951187
CurrentTrain: epoch  0, batch    20 | loss: 16.0222467Losses:  16.61632549762726 1.1282594203948975 1.0172536373138428 7.400664448738098
CurrentTrain: epoch  0, batch    21 | loss: 16.6163255Losses:  17.144716531038284 1.2495720386505127 1.25508713722229 7.845645219087601
CurrentTrain: epoch  0, batch    22 | loss: 17.1447165Losses:  12.171224877238274 1.2758759260177612 1.1105403900146484 2.4234450310468674
CurrentTrain: epoch  0, batch    23 | loss: 12.1712249Losses:  17.50444607436657 1.1569992303848267 1.1715304851531982 7.702400252223015
CurrentTrain: epoch  0, batch    24 | loss: 17.5044461Losses:  10.071629524230957 1.2624984979629517 1.1745034456253052 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 10.0716295Losses:  9.164066314697266 1.2904762029647827 0.9522082805633545 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 9.1640663Losses:  14.755229696631432 1.1213853359222412 0.9756834506988525 5.367488607764244
CurrentTrain: epoch  0, batch    27 | loss: 14.7552297Losses:  14.489109605550766 1.184147834777832 1.001866340637207 5.3273206651210785
CurrentTrain: epoch  0, batch    28 | loss: 14.4891096Losses:  9.734155654907227 1.3622770309448242 1.120418667793274 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 9.7341557Losses:  8.6403169631958 1.1997697353363037 1.144549012184143 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 8.6403170Losses:  15.6122185587883 0.9856758117675781 1.0435316562652588 7.38365238904953
CurrentTrain: epoch  0, batch    31 | loss: 15.6122186Losses:  8.800700187683105 1.1228893995285034 1.133582353591919 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 8.8007002Losses:  10.231954708695412 1.078710675239563 1.1189298629760742 2.37030853331089
CurrentTrain: epoch  0, batch    33 | loss: 10.2319547Losses:  14.058926284313202 1.2247531414031982 1.0194884538650513 5.102245032787323
CurrentTrain: epoch  0, batch    34 | loss: 14.0589263Losses:  14.662831112742424 1.2977982759475708 1.0734448432922363 5.072555348277092
CurrentTrain: epoch  0, batch    35 | loss: 14.6628311Losses:  11.465005308389664 1.003764271736145 1.0472078323364258 2.643409162759781
CurrentTrain: epoch  0, batch    36 | loss: 11.4650053Losses:  11.011601746082306 1.155826210975647 1.0660966634750366 2.4988158345222473
CurrentTrain: epoch  0, batch    37 | loss: 11.0116017Losses:  8.335567474365234 1.3208850622177124 1.0660555362701416 -0.0
CurrentTrain: epoch  0, batch    38 | loss: 8.3355675Losses:  17.10979725420475 1.3546546697616577 1.0324649810791016 7.538773313164711
CurrentTrain: epoch  0, batch    39 | loss: 17.1097973Losses:  16.865079075098038 1.3748446702957153 1.2299129962921143 7.785263210535049
CurrentTrain: epoch  0, batch    40 | loss: 16.8650791Losses:  15.1200390458107 1.137081265449524 1.0290675163269043 5.502256453037262
CurrentTrain: epoch  0, batch    41 | loss: 15.1200390Losses:  14.503549098968506 1.267886996269226 1.2721731662750244 5.0952534675598145
CurrentTrain: epoch  0, batch    42 | loss: 14.5035491Losses:  16.32294252514839 1.128624677658081 1.0068851709365845 7.6687906086444855
CurrentTrain: epoch  0, batch    43 | loss: 16.3229425Losses:  11.176200672984123 1.2194161415100098 1.0942046642303467 2.5704982727766037
CurrentTrain: epoch  0, batch    44 | loss: 11.1762007Losses:  8.48220157623291 1.207781195640564 1.0960613489151 -0.0
CurrentTrain: epoch  0, batch    45 | loss: 8.4822016Losses:  14.644154623150826 1.2336970567703247 1.0970044136047363 5.337506368756294
CurrentTrain: epoch  0, batch    46 | loss: 14.6441546Losses:  14.709177568554878 1.2087217569351196 1.0842610597610474 5.49355848133564
CurrentTrain: epoch  0, batch    47 | loss: 14.7091776Losses:  11.474295496940613 1.2464274168014526 1.1934852600097656 2.478585124015808
CurrentTrain: epoch  0, batch    48 | loss: 11.4742955Losses:  11.800024181604385 1.3006879091262817 1.0902107954025269 2.5234386026859283
CurrentTrain: epoch  0, batch    49 | loss: 11.8000242Losses:  19.21104997396469 1.2637230157852173 1.0845097303390503 10.272383630275726
CurrentTrain: epoch  0, batch    50 | loss: 19.2110500Losses:  14.641277447342873 1.1205922365188599 1.0856783390045166 7.07793964445591
CurrentTrain: epoch  0, batch    51 | loss: 14.6412774Losses:  9.637345314025879 1.3995410203933716 1.1790282726287842 -0.0
CurrentTrain: epoch  0, batch    52 | loss: 9.6373453Losses:  15.861914336681366 0.9347776174545288 0.973969578742981 7.325250327587128
CurrentTrain: epoch  0, batch    53 | loss: 15.8619143Losses:  16.093301758170128 0.9639394283294678 0.9453397989273071 7.898318275809288
CurrentTrain: epoch  0, batch    54 | loss: 16.0933018Losses:  10.464099392294884 1.1806707382202148 1.056309700012207 2.5577292293310165
CurrentTrain: epoch  0, batch    55 | loss: 10.4640994Losses:  16.628278478980064 1.0277806520462036 1.0530035495758057 8.066814169287682
CurrentTrain: epoch  0, batch    56 | loss: 16.6282785Losses:  12.474671989679337 0.9717879295349121 0.9301493167877197 4.670568138360977
CurrentTrain: epoch  0, batch    57 | loss: 12.4746720Losses:  12.920799925923347 1.1358765363693237 1.1218006610870361 5.000442698597908
CurrentTrain: epoch  0, batch    58 | loss: 12.9207999Losses:  10.756054565310478 1.1564518213272095 1.1140689849853516 2.3047472685575485
CurrentTrain: epoch  0, batch    59 | loss: 10.7560546Losses:  23.452036380767822 0.9239131212234497 0.9992237091064453 15.865234375
CurrentTrain: epoch  0, batch    60 | loss: 23.4520364Losses:  14.460736259818077 1.0170931816101074 0.979479193687439 7.057901367545128
CurrentTrain: epoch  0, batch    61 | loss: 14.4607363Losses:  11.066593289375305 1.1233267784118652 1.2436408996582031 2.4248820543289185
CurrentTrain: epoch  0, batch    62 | loss: 11.0665933Losses:  8.148103713989258 1.2453513145446777 0.9856748580932617 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 8.1481037Losses:  12.886878862977028 1.067906141281128 1.0298718214035034 5.057890310883522
CurrentTrain: epoch  1, batch     1 | loss: 12.8868789Losses:  15.180432260036469 1.2178527116775513 1.0717062950134277 7.394529283046722
CurrentTrain: epoch  1, batch     2 | loss: 15.1804323Losses:  10.246264666318893 1.0124421119689941 0.9682341814041138 2.392414778470993
CurrentTrain: epoch  1, batch     3 | loss: 10.2462647Losses:  10.853823855519295 1.1964696645736694 1.0809683799743652 2.700636103749275
CurrentTrain: epoch  1, batch     4 | loss: 10.8538239Losses:  9.331454068422318 0.8922771215438843 1.0 2.2747147381305695
CurrentTrain: epoch  1, batch     5 | loss: 9.3314541Losses:  7.897912979125977 0.9871900081634521 0.9452687501907349 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 7.8979130Losses:  10.45118522644043 0.9200494289398193 0.9091176986694336 2.472560405731201
CurrentTrain: epoch  1, batch     7 | loss: 10.4511852Losses:  9.836142748594284 0.9855098724365234 0.9818416833877563 2.3319399058818817
CurrentTrain: epoch  1, batch     8 | loss: 9.8361427Losses:  14.65146128833294 1.0136795043945312 1.0059351921081543 7.333257362246513
CurrentTrain: epoch  1, batch     9 | loss: 14.6514613Losses:  15.75710192322731 1.024080753326416 0.9187018871307373 7.470608621835709
CurrentTrain: epoch  1, batch    10 | loss: 15.7571019Losses:  12.770171910524368 1.0418390035629272 1.1015956401824951 4.9962723553180695
CurrentTrain: epoch  1, batch    11 | loss: 12.7701719Losses:  12.644381582736969 0.9181127548217773 0.955573320388794 4.819823324680328
CurrentTrain: epoch  1, batch    12 | loss: 12.6443816Losses:  7.393280029296875 0.8382915258407593 1.016485571861267 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 7.3932800Losses:  13.73682726919651 1.064528226852417 0.9516571760177612 5.169972792267799
CurrentTrain: epoch  1, batch    14 | loss: 13.7368273Losses:  7.642033100128174 0.979477047920227 1.0 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 7.6420331Losses:  13.200204581022263 0.9458440542221069 1.0166677236557007 4.955715864896774
CurrentTrain: epoch  1, batch    16 | loss: 13.2002046Losses:  13.692546412348747 0.8525941371917725 0.9932817220687866 5.362975642085075
CurrentTrain: epoch  1, batch    17 | loss: 13.6925464Losses:  14.587947681546211 0.7834349870681763 1.0012558698654175 7.456170871853828
CurrentTrain: epoch  1, batch    18 | loss: 14.5879477Losses:  7.553679466247559 1.0577795505523682 1.0314539670944214 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 7.5536795Losses:  7.082745552062988 0.809249997138977 1.0464086532592773 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 7.0827456Losses:  18.785245403647423 1.0929714441299438 1.0042877197265625 10.67337940633297
CurrentTrain: epoch  1, batch    21 | loss: 18.7852454Losses:  7.892932891845703 0.9436432123184204 1.0463430881500244 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 7.8929329Losses:  7.022818565368652 0.9636484384536743 1.0216152667999268 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 7.0228186Losses:  13.256323084235191 0.8151688575744629 1.0 6.846686586737633
CurrentTrain: epoch  1, batch    24 | loss: 13.2563231Losses:  11.185232102870941 0.8862252235412598 1.0028140544891357 4.7612451910972595
CurrentTrain: epoch  1, batch    25 | loss: 11.1852321Losses:  7.661160945892334 0.921638011932373 0.9320249557495117 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 7.6611609Losses:  7.284351825714111 1.0352073907852173 0.9838458299636841 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 7.2843518Losses:  7.771039009094238 1.0017075538635254 1.0055938959121704 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 7.7710390Losses:  13.936652570962906 0.9158490896224976 1.055221676826477 6.997247606515884
CurrentTrain: epoch  1, batch    29 | loss: 13.9366526Losses:  12.876487478613853 1.1226245164871216 1.0865293741226196 5.068439230322838
CurrentTrain: epoch  1, batch    30 | loss: 12.8764875Losses:  12.694249257445335 0.9190473556518555 1.0787967443466187 4.896095380187035
CurrentTrain: epoch  1, batch    31 | loss: 12.6942493Losses:  12.769551530480385 0.8911128044128418 1.0473370552062988 5.097520127892494
CurrentTrain: epoch  1, batch    32 | loss: 12.7695515Losses:  6.883250713348389 0.8391211032867432 0.9704617261886597 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 6.8832507Losses:  6.701076507568359 0.8948312997817993 1.0250177383422852 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 6.7010765Losses:  15.341974183917046 1.2359157800674438 1.0558072328567505 7.530061170458794
CurrentTrain: epoch  1, batch    35 | loss: 15.3419742Losses:  10.300965443253517 1.0233652591705322 1.0315628051757812 2.5499397665262222
CurrentTrain: epoch  1, batch    36 | loss: 10.3009654Losses:  9.003631800413132 0.6312506198883057 0.986842155456543 2.6632921397686005
CurrentTrain: epoch  1, batch    37 | loss: 9.0036318Losses:  11.643444553017616 0.8263887166976929 1.0257819890975952 4.951520457863808
CurrentTrain: epoch  1, batch    38 | loss: 11.6434446Losses:  11.538482993841171 0.9227486848831177 1.0 4.994927734136581
CurrentTrain: epoch  1, batch    39 | loss: 11.5384830Losses:  9.338727816939354 0.8594822883605957 1.0786962509155273 2.4069570153951645
CurrentTrain: epoch  1, batch    40 | loss: 9.3387278Losses:  11.251332521438599 0.8055808544158936 1.0 4.732099294662476
CurrentTrain: epoch  1, batch    41 | loss: 11.2513325Losses:  9.622426941990852 0.9528552293777466 1.0 2.570290520787239
CurrentTrain: epoch  1, batch    42 | loss: 9.6224269Losses:  12.006647020578384 0.8791344165802002 1.0475366115570068 4.972828775644302
CurrentTrain: epoch  1, batch    43 | loss: 12.0066470Losses:  12.485998928546906 0.8910832405090332 1.0295674800872803 5.011121571063995
CurrentTrain: epoch  1, batch    44 | loss: 12.4859989Losses:  12.166785821318626 0.9090105295181274 1.0131034851074219 4.757864102721214
CurrentTrain: epoch  1, batch    45 | loss: 12.1667858Losses:  7.426143169403076 1.0272486209869385 1.0842845439910889 -0.0
CurrentTrain: epoch  1, batch    46 | loss: 7.4261432Losses:  7.077377796173096 0.9347387552261353 0.9680120944976807 -0.0
CurrentTrain: epoch  1, batch    47 | loss: 7.0773778Losses:  9.789475321769714 0.8377325534820557 0.954472541809082 2.491047739982605
CurrentTrain: epoch  1, batch    48 | loss: 9.7894753Losses:  15.052734598517418 0.7735925912857056 1.0068491697311401 8.129642233252525
CurrentTrain: epoch  1, batch    49 | loss: 15.0527346Losses:  9.579883128404617 0.7319183349609375 1.0489381551742554 2.406220465898514
CurrentTrain: epoch  1, batch    50 | loss: 9.5798831Losses:  7.030546188354492 1.0663024187088013 0.9831287860870361 -0.0
CurrentTrain: epoch  1, batch    51 | loss: 7.0305462Losses:  14.100673854351044 1.0066512823104858 1.0115830898284912 7.284767329692841
CurrentTrain: epoch  1, batch    52 | loss: 14.1006739Losses:  6.26716423034668 0.8494890928268433 1.0293806791305542 -0.0
CurrentTrain: epoch  1, batch    53 | loss: 6.2671642Losses:  11.534246936440468 0.7092329263687134 0.9665026664733887 4.931691661477089
CurrentTrain: epoch  1, batch    54 | loss: 11.5342469Losses:  10.979443475604057 0.8984004259109497 1.0209310054779053 4.60025779902935
CurrentTrain: epoch  1, batch    55 | loss: 10.9794435Losses:  6.855142116546631 0.8872579336166382 1.048549771308899 -0.0
CurrentTrain: epoch  1, batch    56 | loss: 6.8551421Losses:  7.6140899658203125 0.996333122253418 1.0504541397094727 -0.0
CurrentTrain: epoch  1, batch    57 | loss: 7.6140900Losses:  6.447059631347656 0.8617264032363892 1.1068121194839478 -0.0
CurrentTrain: epoch  1, batch    58 | loss: 6.4470596Losses:  12.253539577126503 0.9218950271606445 1.0741387605667114 4.74759341776371
CurrentTrain: epoch  1, batch    59 | loss: 12.2535396Losses:  6.611093044281006 0.8988528251647949 1.0431588888168335 -0.0
CurrentTrain: epoch  1, batch    60 | loss: 6.6110930Losses:  8.509109497070312 1.0074477195739746 1.0594336986541748 -0.0
CurrentTrain: epoch  1, batch    61 | loss: 8.5091095Losses:  12.326723113656044 0.7347993850708008 1.0 6.913745418190956
CurrentTrain: epoch  1, batch    62 | loss: 12.3267231Losses:  5.75763463973999 0.7375780344009399 1.0 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 5.7576346Losses:  6.4310197830200195 0.8542327880859375 1.0 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 6.4310198Losses:  8.205885395407677 0.626728892326355 1.0 2.293210968375206
CurrentTrain: epoch  2, batch     2 | loss: 8.2058854Losses:  12.663724571466446 1.0302668809890747 1.0301872491836548 5.038890510797501
CurrentTrain: epoch  2, batch     3 | loss: 12.6637246Losses:  9.54331836104393 0.7136517763137817 1.0063507556915283 2.375985234975815
CurrentTrain: epoch  2, batch     4 | loss: 9.5433184Losses:  9.696229487657547 0.7462925910949707 1.0430563688278198 2.704610377550125
CurrentTrain: epoch  2, batch     5 | loss: 9.6962295Losses:  6.558770656585693 0.9307876825332642 1.00369393825531 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 6.5587707Losses:  13.569051966071129 0.7638556957244873 1.0 7.143565401434898
CurrentTrain: epoch  2, batch     7 | loss: 13.5690520Losses:  10.888301193714142 0.8127366304397583 1.0 4.633173286914825
CurrentTrain: epoch  2, batch     8 | loss: 10.8883012Losses:  11.90371648967266 0.858046293258667 1.007082223892212 5.146534845232964
CurrentTrain: epoch  2, batch     9 | loss: 11.9037165Losses:  8.391425922513008 0.6547454595565796 1.0 2.279625251889229
CurrentTrain: epoch  2, batch    10 | loss: 8.3914259Losses:  8.07555866241455 0.5969568490982056 1.0 2.2533602714538574
CurrentTrain: epoch  2, batch    11 | loss: 8.0755587Losses:  13.54989169538021 0.9925887584686279 0.9671639204025269 7.238789305090904
CurrentTrain: epoch  2, batch    12 | loss: 13.5498917Losses:  8.97811770439148 0.7515876293182373 1.0449196100234985 2.4859731197357178
CurrentTrain: epoch  2, batch    13 | loss: 8.9781177Losses:  11.786909386515617 1.0566480159759521 1.0863865613937378 4.7777755707502365
CurrentTrain: epoch  2, batch    14 | loss: 11.7869094Losses:  8.814538031816483 0.7445023059844971 1.0 2.3460512459278107
CurrentTrain: epoch  2, batch    15 | loss: 8.8145380Losses:  11.047410175204277 0.6569979190826416 1.0106041431427002 4.598644897341728
CurrentTrain: epoch  2, batch    16 | loss: 11.0474102Losses:  10.696327105164528 0.7897660732269287 1.0 4.571959391236305
CurrentTrain: epoch  2, batch    17 | loss: 10.6963271Losses:  15.10705304145813 0.5690634250640869 1.0 9.569496393203735
CurrentTrain: epoch  2, batch    18 | loss: 15.1070530Losses:  9.029371470212936 0.7589948177337646 0.9906789064407349 2.3277771174907684
CurrentTrain: epoch  2, batch    19 | loss: 9.0293715Losses:  6.03449010848999 0.775843620300293 1.0 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 6.0344901Losses:  6.639584541320801 0.7613240480422974 1.0 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 6.6395845Losses:  11.444745555520058 0.8311505317687988 0.9908409118652344 5.009618297219276
CurrentTrain: epoch  2, batch    22 | loss: 11.4447456Losses:  8.775222674012184 0.6134448051452637 1.0686899423599243 2.5003093630075455
CurrentTrain: epoch  2, batch    23 | loss: 8.7752227Losses:  8.003499284386635 0.689852237701416 0.9681839942932129 2.303330674767494
CurrentTrain: epoch  2, batch    24 | loss: 8.0034993Losses:  13.3069067299366 0.8395900726318359 1.0 7.152006179094315
CurrentTrain: epoch  2, batch    25 | loss: 13.3069067Losses:  8.37357947230339 0.5288732051849365 1.0 2.258925884962082
CurrentTrain: epoch  2, batch    26 | loss: 8.3735795Losses:  8.520286694169044 0.5731970071792603 1.0 2.525633469223976
CurrentTrain: epoch  2, batch    27 | loss: 8.5202867Losses:  8.444910809397697 0.6918970346450806 1.0499697923660278 2.408898636698723
CurrentTrain: epoch  2, batch    28 | loss: 8.4449108Losses:  18.79712800681591 0.8605964183807373 1.0554975271224976 12.276887699961662
CurrentTrain: epoch  2, batch    29 | loss: 18.7971280Losses:  8.21105508506298 0.696143388748169 1.0 2.304853245615959
CurrentTrain: epoch  2, batch    30 | loss: 8.2110551Losses:  8.085894450545311 0.6515637636184692 1.0 2.2977436631917953
CurrentTrain: epoch  2, batch    31 | loss: 8.0858945Losses:  11.040382742881775 0.5621911287307739 0.9759056568145752 4.607941508293152
CurrentTrain: epoch  2, batch    32 | loss: 11.0403827Losses:  9.623219981789589 0.7613749504089355 1.0304993391036987 2.6415949016809464
CurrentTrain: epoch  2, batch    33 | loss: 9.6232200Losses:  13.868114307522774 0.820148229598999 1.0278911590576172 7.611465290188789
CurrentTrain: epoch  2, batch    34 | loss: 13.8681143Losses:  13.92677317559719 0.6795381307601929 1.0 7.457591637969017
CurrentTrain: epoch  2, batch    35 | loss: 13.9267732Losses:  10.635945245623589 0.6090649366378784 1.0 4.693839475512505
CurrentTrain: epoch  2, batch    36 | loss: 10.6359452Losses:  8.540310516953468 0.6108345985412598 1.0 2.485037460923195
CurrentTrain: epoch  2, batch    37 | loss: 8.5403105Losses:  8.173494413495064 0.6207324266433716 1.0 2.2447524815797806
CurrentTrain: epoch  2, batch    38 | loss: 8.1734944Losses:  10.922824576497078 0.666765570640564 1.0 4.788553908467293
CurrentTrain: epoch  2, batch    39 | loss: 10.9228246Losses:  12.751361593604088 0.603161096572876 1.0 6.958056196570396
CurrentTrain: epoch  2, batch    40 | loss: 12.7513616Losses:  8.54006128013134 0.7360999584197998 1.0560067892074585 2.326922222971916
CurrentTrain: epoch  2, batch    41 | loss: 8.5400613Losses:  7.8762630224227905 0.5134847164154053 1.0 2.241473078727722
CurrentTrain: epoch  2, batch    42 | loss: 7.8762630Losses:  6.0212249755859375 0.8019751310348511 1.0 -0.0
CurrentTrain: epoch  2, batch    43 | loss: 6.0212250Losses:  7.92792746424675 0.620348334312439 1.0 2.290847271680832
CurrentTrain: epoch  2, batch    44 | loss: 7.9279275Losses:  10.539070919156075 0.6179506778717041 0.9663997888565063 4.539451912045479
CurrentTrain: epoch  2, batch    45 | loss: 10.5390709Losses:  5.570749282836914 0.5430408716201782 1.0 -0.0
CurrentTrain: epoch  2, batch    46 | loss: 5.5707493Losses:  6.382389068603516 0.7869663238525391 0.9986767768859863 -0.0
CurrentTrain: epoch  2, batch    47 | loss: 6.3823891Losses:  5.649593353271484 0.762441873550415 1.0 -0.0
CurrentTrain: epoch  2, batch    48 | loss: 5.6495934Losses:  8.227797970175743 0.6483676433563232 1.0 2.252108559012413
CurrentTrain: epoch  2, batch    49 | loss: 8.2277980Losses:  15.364698484539986 0.4361990690231323 1.0 9.68508394062519
CurrentTrain: epoch  2, batch    50 | loss: 15.3646985Losses:  10.139874339103699 0.5708093643188477 1.0434577465057373 4.560747504234314
CurrentTrain: epoch  2, batch    51 | loss: 10.1398743Losses:  7.7475747764110565 0.49235010147094727 1.0 2.3298549354076385
CurrentTrain: epoch  2, batch    52 | loss: 7.7475748Losses:  10.259765043854713 0.6487689018249512 1.0 4.611422911286354
CurrentTrain: epoch  2, batch    53 | loss: 10.2597650Losses:  7.884158223867416 0.5124090909957886 1.0 2.242426484823227
CurrentTrain: epoch  2, batch    54 | loss: 7.8841582Losses:  10.12631668150425 0.5902019739151001 1.0 4.639033928513527
CurrentTrain: epoch  2, batch    55 | loss: 10.1263167Losses:  7.99709540605545 0.6421569585800171 1.0 2.388737976551056
CurrentTrain: epoch  2, batch    56 | loss: 7.9970954Losses:  6.299533367156982 0.6269882917404175 1.043125033378601 -0.0
CurrentTrain: epoch  2, batch    57 | loss: 6.2995334Losses:  5.641900539398193 0.47834694385528564 0.9955543279647827 -0.0
CurrentTrain: epoch  2, batch    58 | loss: 5.6419005Losses:  18.004861637949944 0.6071512699127197 1.0 12.209687992930412
CurrentTrain: epoch  2, batch    59 | loss: 18.0048616Losses:  6.005538463592529 0.6464325189590454 1.0429104566574097 -0.0
CurrentTrain: epoch  2, batch    60 | loss: 6.0055385Losses:  8.737090095877647 0.6862189769744873 1.0429555177688599 2.475280746817589
CurrentTrain: epoch  2, batch    61 | loss: 8.7370901Losses:  5.631297588348389 0.5220766067504883 1.0 -0.0
CurrentTrain: epoch  2, batch    62 | loss: 5.6312976Losses:  7.966969013214111 0.6950418949127197 1.0 2.3044943809509277
CurrentTrain: epoch  3, batch     0 | loss: 7.9669690Losses:  7.781412243843079 0.6040123701095581 1.0 2.2476998567581177
CurrentTrain: epoch  3, batch     1 | loss: 7.7814122Losses:  5.657190799713135 0.4803133010864258 1.0 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 5.6571908Losses:  12.63233833014965 0.6353145837783813 1.0042173862457275 6.785157963633537
CurrentTrain: epoch  3, batch     3 | loss: 12.6323383Losses:  9.928732767701149 0.46686506271362305 1.0 4.7475699335336685
CurrentTrain: epoch  3, batch     4 | loss: 9.9287328Losses:  12.641408875584602 0.45845019817352295 1.0 6.777669385075569
CurrentTrain: epoch  3, batch     5 | loss: 12.6414089Losses:  7.551719471812248 0.31938350200653076 1.0182759761810303 2.22977714240551
CurrentTrain: epoch  3, batch     6 | loss: 7.5517195Losses:  10.543479159474373 0.4444091320037842 1.044789433479309 4.956396773457527
CurrentTrain: epoch  3, batch     7 | loss: 10.5434792Losses:  7.938819169998169 0.570306658744812 0.9888699054718018 2.2424304485321045
CurrentTrain: epoch  3, batch     8 | loss: 7.9388192Losses:  5.957627773284912 0.5893154144287109 1.0569757223129272 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 5.9576278Losses:  5.6130194664001465 0.6018927097320557 1.045919418334961 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 5.6130195Losses:  10.048940375447273 0.4049215316772461 1.0 4.66824646294117
CurrentTrain: epoch  3, batch    11 | loss: 10.0489404Losses:  12.192184254527092 0.7335695028305054 1.0 6.620522782206535
CurrentTrain: epoch  3, batch    12 | loss: 12.1921843Losses:  10.890099748969078 0.8570699691772461 1.0161195993423462 4.568483576178551
CurrentTrain: epoch  3, batch    13 | loss: 10.8900997Losses:  18.444301560521126 0.6756705045700073 1.1042379140853882 12.381382420659065
CurrentTrain: epoch  3, batch    14 | loss: 18.4443016Losses:  8.13964107632637 0.6581405401229858 1.0328510999679565 2.2403667271137238
CurrentTrain: epoch  3, batch    15 | loss: 8.1396411Losses:  12.81856156885624 0.5157537460327148 1.0 7.308202281594276
CurrentTrain: epoch  3, batch    16 | loss: 12.8185616Losses:  7.739993184804916 0.6970356702804565 1.0 2.3240853250026703
CurrentTrain: epoch  3, batch    17 | loss: 7.7399932Losses:  10.189727157354355 0.3772439956665039 1.0 4.909670203924179
CurrentTrain: epoch  3, batch    18 | loss: 10.1897272Losses:  7.556673750281334 0.5859870910644531 1.0 2.3644893020391464
CurrentTrain: epoch  3, batch    19 | loss: 7.5566738Losses:  12.049857750535011 0.6343227624893188 0.974551796913147 6.7083593755960464
CurrentTrain: epoch  3, batch    20 | loss: 12.0498578Losses:  10.010644614696503 0.3604816198348999 1.0 4.768422782421112
CurrentTrain: epoch  3, batch    21 | loss: 10.0106446Losses:  10.979780718684196 0.5852769613265991 1.0209331512451172 4.982689902186394
CurrentTrain: epoch  3, batch    22 | loss: 10.9797807Losses:  7.671378970146179 0.4418870210647583 1.0 2.211753726005554
CurrentTrain: epoch  3, batch    23 | loss: 7.6713790Losses:  7.817670568823814 0.49446773529052734 1.0 2.2879536002874374
CurrentTrain: epoch  3, batch    24 | loss: 7.8176706Losses:  10.120372906327248 0.5299677848815918 1.0 4.613803997635841
CurrentTrain: epoch  3, batch    25 | loss: 10.1203729Losses:  9.78928528726101 0.5223859548568726 1.0 4.504336461424828
CurrentTrain: epoch  3, batch    26 | loss: 9.7892853Losses:  7.348953679203987 0.37583208084106445 1.0 2.1725325137376785
CurrentTrain: epoch  3, batch    27 | loss: 7.3489537Losses:  12.764870509505272 0.577114462852478 1.0 7.151273593306541
CurrentTrain: epoch  3, batch    28 | loss: 12.7648705Losses:  12.74567686021328 0.41409003734588623 1.0 7.337952956557274
CurrentTrain: epoch  3, batch    29 | loss: 12.7456769Losses:  5.3307695388793945 0.5053293704986572 1.0 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 5.3307695Losses:  5.290683746337891 0.47474586963653564 1.0 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 5.2906837Losses:  14.39923645555973 0.4175119400024414 1.0 9.16931463778019
CurrentTrain: epoch  3, batch    32 | loss: 14.3992365Losses:  10.3667863458395 0.5628577470779419 1.0 4.46549879014492
CurrentTrain: epoch  3, batch    33 | loss: 10.3667863Losses:  5.815645694732666 0.5268092155456543 1.010811686515808 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 5.8156457Losses:  8.151809945702553 0.598802924156189 0.9904910326004028 2.266336217522621
CurrentTrain: epoch  3, batch    35 | loss: 8.1518099Losses:  5.009477615356445 0.34171295166015625 1.0 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 5.0094776Losses:  5.346467018127441 0.5050257444381714 1.0 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 5.3464670Losses:  9.881868168711662 0.5803724527359009 1.0 4.4612801522016525
CurrentTrain: epoch  3, batch    38 | loss: 9.8818682Losses:  12.478536039590836 0.6932861804962158 1.0 6.958038717508316
CurrentTrain: epoch  3, batch    39 | loss: 12.4785360Losses:  15.694793596863747 0.5380911827087402 1.0057694911956787 9.689921274781227
CurrentTrain: epoch  3, batch    40 | loss: 15.6947936Losses:  5.599265098571777 0.4516429901123047 1.0 -0.0
CurrentTrain: epoch  3, batch    41 | loss: 5.5992651Losses:  7.36898347735405 0.39303267002105713 1.0 2.2537305057048798
CurrentTrain: epoch  3, batch    42 | loss: 7.3689835Losses:  7.831821843981743 0.7004770040512085 1.0 2.4670190066099167
CurrentTrain: epoch  3, batch    43 | loss: 7.8318218Losses:  10.17599768936634 0.425167441368103 1.0 4.744055703282356
CurrentTrain: epoch  3, batch    44 | loss: 10.1759977Losses:  9.728691264986992 0.3872544765472412 1.0 4.512893840670586
CurrentTrain: epoch  3, batch    45 | loss: 9.7286913Losses:  10.02128441631794 0.4902224540710449 1.0 4.607048347592354
CurrentTrain: epoch  3, batch    46 | loss: 10.0212844Losses:  9.461271807551384 0.4786946773529053 1.0 4.419545695185661
CurrentTrain: epoch  3, batch    47 | loss: 9.4612718Losses:  5.321163177490234 0.5660531520843506 1.0 -0.0
CurrentTrain: epoch  3, batch    48 | loss: 5.3211632Losses:  15.15365083515644 0.4104195833206177 1.0 9.869429185986519
CurrentTrain: epoch  3, batch    49 | loss: 15.1536508Losses:  5.739756107330322 0.5122619867324829 1.0461794137954712 -0.0
CurrentTrain: epoch  3, batch    50 | loss: 5.7397561Losses:  8.00639571249485 0.36856532096862793 1.0 2.196190729737282
CurrentTrain: epoch  3, batch    51 | loss: 8.0063957Losses:  11.801347136497498 0.49218666553497314 1.0 6.723272204399109
CurrentTrain: epoch  3, batch    52 | loss: 11.8013471Losses:  13.00912356376648 0.6970285177230835 1.0721157789230347 6.822633504867554
CurrentTrain: epoch  3, batch    53 | loss: 13.0091236Losses:  7.345750778913498 0.40055179595947266 1.0 2.3165087401866913
CurrentTrain: epoch  3, batch    54 | loss: 7.3457508Losses:  9.713487774133682 0.2593812942504883 1.0 4.52999272942543
CurrentTrain: epoch  3, batch    55 | loss: 9.7134878Losses:  5.090848445892334 0.3860046863555908 1.0 -0.0
CurrentTrain: epoch  3, batch    56 | loss: 5.0908484Losses:  5.135549068450928 0.4427534341812134 1.0 -0.0
CurrentTrain: epoch  3, batch    57 | loss: 5.1355491Losses:  9.878217950463295 0.2558943033218384 1.0 4.855635896325111
CurrentTrain: epoch  3, batch    58 | loss: 9.8782180Losses:  16.12323835492134 0.37119901180267334 0.9851675033569336 9.929065972566605
CurrentTrain: epoch  3, batch    59 | loss: 16.1232384Losses:  5.078420162200928 0.42576348781585693 1.0 -0.0
CurrentTrain: epoch  3, batch    60 | loss: 5.0784202Losses:  12.396882593631744 0.4242286682128906 1.0 6.928582727909088
CurrentTrain: epoch  3, batch    61 | loss: 12.3968826Losses:  7.180252403020859 0.2710990905761719 1.0 2.2088931500911713
CurrentTrain: epoch  3, batch    62 | loss: 7.1802524Losses:  11.974281787872314 0.37373459339141846 1.0 6.794475555419922
CurrentTrain: epoch  4, batch     0 | loss: 11.9742818Losses:  12.034052640199661 0.5732288360595703 1.0 6.962587624788284
CurrentTrain: epoch  4, batch     1 | loss: 12.0340526Losses:  5.124617576599121 0.533373236656189 1.0 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 5.1246176Losses:  11.649448454380035 0.30271410942077637 1.0 6.6921029686927795
CurrentTrain: epoch  4, batch     3 | loss: 11.6494485Losses:  14.818058490753174 0.5406062602996826 1.0 9.717927932739258
CurrentTrain: epoch  4, batch     4 | loss: 14.8180585Losses:  7.181910932064056 0.34207773208618164 1.0 2.201603353023529
CurrentTrain: epoch  4, batch     5 | loss: 7.1819109Losses:  7.329506665468216 0.468349814414978 1.0 2.287673741579056
CurrentTrain: epoch  4, batch     6 | loss: 7.3295067Losses:  4.878458023071289 0.35759496688842773 1.0 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 4.8784580Losses:  9.504479303956032 0.299177885055542 1.0 4.461961165070534
CurrentTrain: epoch  4, batch     8 | loss: 9.5044793Losses:  7.378789812326431 0.35623490810394287 1.0 2.310017019510269
CurrentTrain: epoch  4, batch     9 | loss: 7.3787898Losses:  11.978735029697418 0.47677600383758545 1.0 6.856366217136383
CurrentTrain: epoch  4, batch    10 | loss: 11.9787350Losses:  9.710169330239296 0.553421139717102 1.0 4.591069713234901
CurrentTrain: epoch  4, batch    11 | loss: 9.7101693Losses:  7.377285495400429 0.3519105911254883 1.0 2.270251289010048
CurrentTrain: epoch  4, batch    12 | loss: 7.3772855Losses:  4.983249187469482 0.44217169284820557 1.0 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 4.9832492Losses:  7.3710817992687225 0.3195692300796509 1.0 2.3944253623485565
CurrentTrain: epoch  4, batch    14 | loss: 7.3710818Losses:  7.371847182512283 0.4190390110015869 1.0 2.2044754326343536
CurrentTrain: epoch  4, batch    15 | loss: 7.3718472Losses:  9.865893259644508 0.5557186603546143 1.0 4.597297564148903
CurrentTrain: epoch  4, batch    16 | loss: 9.8658933Losses:  4.951230525970459 0.2871590852737427 1.0 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 4.9512305Losses:  9.834916189312935 0.23484694957733154 1.0 4.8478914052248
CurrentTrain: epoch  4, batch    18 | loss: 9.8349162Losses:  15.03705307841301 0.3865365982055664 1.0 9.837696522474289
CurrentTrain: epoch  4, batch    19 | loss: 15.0370531Losses:  9.845570608973503 0.3588336706161499 1.0 4.6593742817640305
CurrentTrain: epoch  4, batch    20 | loss: 9.8455706Losses:  9.64867115020752 0.5538651943206787 1.0 4.561742305755615
CurrentTrain: epoch  4, batch    21 | loss: 9.6486712Losses:  7.133527755737305 0.2661365270614624 1.0 2.2335662841796875
CurrentTrain: epoch  4, batch    22 | loss: 7.1335278Losses:  14.362618327140808 0.39369070529937744 1.0 9.352879881858826
CurrentTrain: epoch  4, batch    23 | loss: 14.3626183Losses:  7.263123959302902 0.37244176864624023 1.0 2.291758030653
CurrentTrain: epoch  4, batch    24 | loss: 7.2631240Losses:  5.488132953643799 0.41966187953948975 0.9955055713653564 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 5.4881330Losses:  7.26880356669426 0.40787506103515625 1.0 2.270121544599533
CurrentTrain: epoch  4, batch    26 | loss: 7.2688036Losses:  7.493603959679604 0.5079327821731567 1.0 2.273068681359291
CurrentTrain: epoch  4, batch    27 | loss: 7.4936040Losses:  7.372568339109421 0.3438760042190552 1.0 2.182664602994919
CurrentTrain: epoch  4, batch    28 | loss: 7.3725683Losses:  12.345192149281502 0.37121903896331787 1.0 7.239915087819099
CurrentTrain: epoch  4, batch    29 | loss: 12.3451921Losses:  11.95166064798832 0.4309051036834717 1.0 7.01477338373661
CurrentTrain: epoch  4, batch    30 | loss: 11.9516606Losses:  5.110382556915283 0.41110920906066895 1.0 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 5.1103826Losses:  9.27354048192501 0.2123100757598877 1.0 4.453495487570763
CurrentTrain: epoch  4, batch    32 | loss: 9.2735405Losses:  12.02795073390007 0.3238849639892578 1.0 7.088063687086105
CurrentTrain: epoch  4, batch    33 | loss: 12.0279507Losses:  4.889345169067383 0.30444562435150146 1.0 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 4.8893452Losses:  7.266142517328262 0.24058687686920166 1.0 2.2482434809207916
CurrentTrain: epoch  4, batch    35 | loss: 7.2661425Losses:  9.997163131833076 0.33111119270324707 1.0 4.910745933651924
CurrentTrain: epoch  4, batch    36 | loss: 9.9971631Losses:  9.711828410625458 0.3417545557022095 1.0 4.706613719463348
CurrentTrain: epoch  4, batch    37 | loss: 9.7118284Losses:  4.925689697265625 0.3622514009475708 1.0 -0.0
CurrentTrain: epoch  4, batch    38 | loss: 4.9256897Losses:  9.859891384840012 0.25444495677948 1.0 4.967267960309982
CurrentTrain: epoch  4, batch    39 | loss: 9.8598914Losses:  11.632497742772102 0.3704650402069092 1.0 6.638699010014534
CurrentTrain: epoch  4, batch    40 | loss: 11.6324977Losses:  4.9867753982543945 0.3900034427642822 1.0 -0.0
CurrentTrain: epoch  4, batch    41 | loss: 4.9867754Losses:  13.932450696825981 0.1987590789794922 1.0 9.084573194384575
CurrentTrain: epoch  4, batch    42 | loss: 13.9324507Losses:  9.486821562051773 0.43377840518951416 1.0 4.54030504822731
CurrentTrain: epoch  4, batch    43 | loss: 9.4868216Losses:  4.924313545227051 0.26355504989624023 1.0 -0.0
CurrentTrain: epoch  4, batch    44 | loss: 4.9243135Losses:  7.154839560389519 0.205702543258667 1.0 2.246059939265251
CurrentTrain: epoch  4, batch    45 | loss: 7.1548396Losses:  9.639363005757332 0.4298948049545288 1.0 4.5364820510149
CurrentTrain: epoch  4, batch    46 | loss: 9.6393630Losses:  7.198051601648331 0.28390729427337646 1.0 2.3304111063480377
CurrentTrain: epoch  4, batch    47 | loss: 7.1980516Losses:  12.018338650465012 0.2337435483932495 1.0 7.123322933912277
CurrentTrain: epoch  4, batch    48 | loss: 12.0183387Losses:  4.821465492248535 0.4285091161727905 1.0 -0.0
CurrentTrain: epoch  4, batch    49 | loss: 4.8214655Losses:  9.533813044428825 0.3449016809463501 1.0 4.583849474787712
CurrentTrain: epoch  4, batch    50 | loss: 9.5338130Losses:  4.89795446395874 0.4097050428390503 1.0 -0.0
CurrentTrain: epoch  4, batch    51 | loss: 4.8979545Losses:  11.50788140296936 0.3365771770477295 1.0 6.644186735153198
CurrentTrain: epoch  4, batch    52 | loss: 11.5078814Losses:  9.37512917816639 0.4257235527038574 1.0676145553588867 2.6015462428331375
CurrentTrain: epoch  4, batch    53 | loss: 9.3751292Losses:  6.946365937590599 0.31675612926483154 1.0 2.1925803273916245
CurrentTrain: epoch  4, batch    54 | loss: 6.9463659Losses:  4.9535393714904785 0.3351297378540039 1.0 -0.0
CurrentTrain: epoch  4, batch    55 | loss: 4.9535394Losses:  10.165501862764359 0.20866763591766357 1.0 5.071849137544632
CurrentTrain: epoch  4, batch    56 | loss: 10.1655019Losses:  11.74099713563919 0.46367502212524414 1.0134841203689575 6.785896599292755
CurrentTrain: epoch  4, batch    57 | loss: 11.7409971Losses:  9.385691925883293 0.24360084533691406 1.0 4.48190002143383
CurrentTrain: epoch  4, batch    58 | loss: 9.3856919Losses:  9.361550688743591 0.2177215814590454 1.0 4.571256041526794
CurrentTrain: epoch  4, batch    59 | loss: 9.3615507Losses:  7.263919323682785 0.4535776376724243 1.0 2.2075099647045135
CurrentTrain: epoch  4, batch    60 | loss: 7.2639193Losses:  6.945781081914902 0.0966944694519043 1.0 2.184668868780136
CurrentTrain: epoch  4, batch    61 | loss: 6.9457811Losses:  11.67108616232872 0.16056180000305176 1.0 7.042466014623642
CurrentTrain: epoch  4, batch    62 | loss: 11.6710862Losses:  4.753166198730469 0.16620886325836182 1.0 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 4.7531662Losses:  9.244765818119049 0.22750210762023926 1.0 4.431366980075836
CurrentTrain: epoch  5, batch     1 | loss: 9.2447658Losses:  9.382806077599525 0.2784106731414795 1.0 4.518527761101723
CurrentTrain: epoch  5, batch     2 | loss: 9.3828061Losses:  9.290861964225769 0.23829782009124756 1.0 4.468931555747986
CurrentTrain: epoch  5, batch     3 | loss: 9.2908620Losses:  11.572666957974434 0.2911057472229004 1.0 6.753113582730293
CurrentTrain: epoch  5, batch     4 | loss: 11.5726670Losses:  13.785282865166664 0.45823776721954346 1.0 8.905139222741127
CurrentTrain: epoch  5, batch     5 | loss: 13.7852829Losses:  4.7819061279296875 0.26380133628845215 1.0 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 4.7819061Losses:  6.9954355508089066 0.1393570899963379 1.0 2.231755092740059
CurrentTrain: epoch  5, batch     7 | loss: 6.9954356Losses:  11.587526559829712 0.345561146736145 1.0 6.667053937911987
CurrentTrain: epoch  5, batch     8 | loss: 11.5875266Losses:  11.929100528359413 0.2375349998474121 1.0 7.134486690163612
CurrentTrain: epoch  5, batch     9 | loss: 11.9291005Losses:  9.224096536636353 0.19011187553405762 1.0 4.410653829574585
CurrentTrain: epoch  5, batch    10 | loss: 9.2240965Losses:  14.683971762657166 0.20397603511810303 1.0 9.932353377342224
CurrentTrain: epoch  5, batch    11 | loss: 14.6839718Losses:  9.317848309874535 0.2231755256652832 1.0 4.556478604674339
CurrentTrain: epoch  5, batch    12 | loss: 9.3178483Losses:  6.927496910095215 0.3556067943572998 1.0 2.1813926696777344
CurrentTrain: epoch  5, batch    13 | loss: 6.9274969Losses:  7.047715440392494 0.11178004741668701 1.0 2.309187188744545
CurrentTrain: epoch  5, batch    14 | loss: 7.0477154Losses:  7.0515440702438354 0.20718657970428467 1.0 2.2289079427719116
CurrentTrain: epoch  5, batch    15 | loss: 7.0515441Losses:  7.192895233631134 0.41095972061157227 1.0 2.2905848622322083
CurrentTrain: epoch  5, batch    16 | loss: 7.1928952Losses:  9.57992398738861 0.40628159046173096 1.0 4.657230734825134
CurrentTrain: epoch  5, batch    17 | loss: 9.5799240Losses:  11.491312742233276 0.21754014492034912 1.0 6.720824480056763
CurrentTrain: epoch  5, batch    18 | loss: 11.4913127Losses:  6.8914530873298645 0.20843219757080078 1.0 2.1395795941352844
CurrentTrain: epoch  5, batch    19 | loss: 6.8914531Losses:  8.985816597938538 0.20169806480407715 1.0 4.351635575294495
CurrentTrain: epoch  5, batch    20 | loss: 8.9858166Losses:  11.714153975248337 0.2896759510040283 1.0 6.988107889890671
CurrentTrain: epoch  5, batch    21 | loss: 11.7141540Losses:  9.062616646289825 0.27682018280029297 1.0 4.406311810016632
CurrentTrain: epoch  5, batch    22 | loss: 9.0626166Losses:  9.314851269125938 0.1373993158340454 1.0 4.535665974020958
CurrentTrain: epoch  5, batch    23 | loss: 9.3148513Losses:  9.197538986802101 0.27750885486602783 1.0 4.5396605879068375
CurrentTrain: epoch  5, batch    24 | loss: 9.1975390Losses:  15.010086074471474 0.5344674587249756 0.9743258953094482 9.760598197579384
CurrentTrain: epoch  5, batch    25 | loss: 15.0100861Losses:  9.106243953108788 0.1766047477722168 1.052783489227295 4.367841586470604
CurrentTrain: epoch  5, batch    26 | loss: 9.1062440Losses:  6.922465562820435 0.1712707281112671 1.0 2.156381368637085
CurrentTrain: epoch  5, batch    27 | loss: 6.9224656Losses:  6.761957332491875 0.07865095138549805 1.0 2.1816980093717575
CurrentTrain: epoch  5, batch    28 | loss: 6.7619573Losses:  11.61445914208889 0.19678187370300293 1.0 6.840291127562523
CurrentTrain: epoch  5, batch    29 | loss: 11.6144591Losses:  6.919964402914047 0.15755009651184082 1.0 2.2317090928554535
CurrentTrain: epoch  5, batch    30 | loss: 6.9199644Losses:  11.468959733843803 0.04184889793395996 1.0 6.7778071612119675
CurrentTrain: epoch  5, batch    31 | loss: 11.4689597Losses:  6.831551790237427 0.08073282241821289 1.0 2.1791746616363525
CurrentTrain: epoch  5, batch    32 | loss: 6.8315518Losses:  17.240232586860657 0.4792226552963257 1.0 12.303807854652405
CurrentTrain: epoch  5, batch    33 | loss: 17.2402326Losses:  9.221151739358902 0.25739049911499023 1.0 4.42391911149025
CurrentTrain: epoch  5, batch    34 | loss: 9.2211517Losses:  11.386334672570229 0.2101658582687378 1.0 6.6802399307489395
CurrentTrain: epoch  5, batch    35 | loss: 11.3863347Losses:  11.966797575354576 0.15085268020629883 1.0 7.229917272925377
CurrentTrain: epoch  5, batch    36 | loss: 11.9667976Losses:  7.020010843873024 0.40242528915405273 1.0 2.2270716577768326
CurrentTrain: epoch  5, batch    37 | loss: 7.0200108Losses:  7.098682478070259 0.22382628917694092 1.039109706878662 2.3409558087587357
CurrentTrain: epoch  5, batch    38 | loss: 7.0986825Losses:  6.961764186620712 0.26339447498321533 1.0 2.180591434240341
CurrentTrain: epoch  5, batch    39 | loss: 6.9617642Losses:  7.022390902042389 0.3390672206878662 1.0 2.2449026703834534
CurrentTrain: epoch  5, batch    40 | loss: 7.0223909Losses:  4.828956127166748 0.21881592273712158 1.0 -0.0
CurrentTrain: epoch  5, batch    41 | loss: 4.8289561Losses:  7.091459885239601 0.2853192090988159 1.0 2.2607470899820328
CurrentTrain: epoch  5, batch    42 | loss: 7.0914599Losses:  11.618017002940178 0.33348822593688965 1.0 6.775157257914543
CurrentTrain: epoch  5, batch    43 | loss: 11.6180170Losses:  4.827349662780762 0.3579230308532715 1.0 -0.0
CurrentTrain: epoch  5, batch    44 | loss: 4.8273497Losses:  4.612141132354736 0.06629681587219238 1.0 -0.0
CurrentTrain: epoch  5, batch    45 | loss: 4.6121411Losses:  9.269198879599571 0.25888943672180176 1.0 4.564803585410118
CurrentTrain: epoch  5, batch    46 | loss: 9.2691989Losses:  6.931175589561462 0.19765591621398926 1.0 2.2306450605392456
CurrentTrain: epoch  5, batch    47 | loss: 6.9311756Losses:  10.03466646373272 0.4109046459197998 1.0243847370147705 4.4567999094724655
CurrentTrain: epoch  5, batch    48 | loss: 10.0346665Losses:  7.356440424919128 0.16770386695861816 1.0 2.1885584592819214
CurrentTrain: epoch  5, batch    49 | loss: 7.3564404Losses:  6.98232801258564 0.32103872299194336 1.0 2.156045511364937
CurrentTrain: epoch  5, batch    50 | loss: 6.9823280Losses:  16.957442983984947 0.3172307014465332 1.0 12.223647817969322
CurrentTrain: epoch  5, batch    51 | loss: 16.9574430Losses:  9.588353946805 0.3179882764816284 1.0 4.811322525143623
CurrentTrain: epoch  5, batch    52 | loss: 9.5883539Losses:  14.353596299886703 0.15420734882354736 1.0 9.660672754049301
CurrentTrain: epoch  5, batch    53 | loss: 14.3535963Losses:  7.00959612429142 0.20452356338500977 1.0 2.2976076751947403
CurrentTrain: epoch  5, batch    54 | loss: 7.0095961Losses:  9.210045248270035 0.207505464553833 1.0 4.476736932992935
CurrentTrain: epoch  5, batch    55 | loss: 9.2100452Losses:  9.184917077422142 0.33544957637786865 1.0 4.330121621489525
CurrentTrain: epoch  5, batch    56 | loss: 9.1849171Losses:  6.840905144810677 0.19317364692687988 1.0 2.1716990023851395
CurrentTrain: epoch  5, batch    57 | loss: 6.8409051Losses:  9.64350226521492 0.21023964881896973 1.0 4.892739802598953
CurrentTrain: epoch  5, batch    58 | loss: 9.6435023Losses:  10.373268067836761 0.26598596572875977 1.0 4.845056474208832
CurrentTrain: epoch  5, batch    59 | loss: 10.3732681Losses:  6.956994593143463 0.28464841842651367 1.0 2.1997562050819397
CurrentTrain: epoch  5, batch    60 | loss: 6.9569946Losses:  6.960026904940605 0.0908651351928711 1.0 2.316077873110771
CurrentTrain: epoch  5, batch    61 | loss: 6.9600269Losses:  11.30095188319683 0.1299886703491211 1.094219446182251 6.58511583507061
CurrentTrain: epoch  5, batch    62 | loss: 11.3009519Losses:  11.783388838171959 0.20854735374450684 1.0 7.061401590704918
CurrentTrain: epoch  6, batch     0 | loss: 11.7833888Losses:  9.018778532743454 0.19720828533172607 1.0 4.347831457853317
CurrentTrain: epoch  6, batch     1 | loss: 9.0187785Losses:  6.999670192599297 0.2615445852279663 1.0 2.265662834048271
CurrentTrain: epoch  6, batch     2 | loss: 6.9996702Losses:  6.887042209506035 0.13177251815795898 1.0 2.231606647372246
CurrentTrain: epoch  6, batch     3 | loss: 6.8870422Losses:  6.931539326906204 0.20850014686584473 1.0 2.2867133915424347
CurrentTrain: epoch  6, batch     4 | loss: 6.9315393Losses:  7.055441111326218 0.11657440662384033 1.0462669134140015 2.3714320361614227
CurrentTrain: epoch  6, batch     5 | loss: 7.0554411Losses:  4.742332935333252 0.2641744613647461 1.0 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 4.7423329Losses:  6.904528826475143 0.1875 1.0 2.248390406370163
CurrentTrain: epoch  6, batch     7 | loss: 6.9045288Losses:  11.893816590309143 0.44446420669555664 1.0 6.99479067325592
CurrentTrain: epoch  6, batch     8 | loss: 11.8938166Losses:  9.051729679107666 0.07828795909881592 1.0 4.455140113830566
CurrentTrain: epoch  6, batch     9 | loss: 9.0517297Losses:  9.828857481479645 0.29629504680633545 1.0 4.648262083530426
CurrentTrain: epoch  6, batch    10 | loss: 9.8288575Losses:  9.270946830511093 0.5247417688369751 1.0 4.415839999914169
CurrentTrain: epoch  6, batch    11 | loss: 9.2709468Losses:  6.8177530616521835 0.1330016851425171 1.0 2.1971752494573593
CurrentTrain: epoch  6, batch    12 | loss: 6.8177531Losses:  6.899120211601257 0.19935142993927002 1.0 2.206640601158142
CurrentTrain: epoch  6, batch    13 | loss: 6.8991202Losses:  6.8359416872262955 0.1875 1.0 2.195648565888405
CurrentTrain: epoch  6, batch    14 | loss: 6.8359417Losses:  9.146360456943512 0.13358116149902344 1.0 4.446024000644684
CurrentTrain: epoch  6, batch    15 | loss: 9.1463605Losses:  4.667728900909424 0.25 1.0 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 4.6677289Losses:  9.32223379611969 0.2507669925689697 1.040000319480896 4.554254651069641
CurrentTrain: epoch  6, batch    17 | loss: 9.3222338Losses:  6.959991231560707 0.14349186420440674 1.0 2.3381431251764297
CurrentTrain: epoch  6, batch    18 | loss: 6.9599912Losses:  4.756243705749512 0.3126636743545532 1.0 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 4.7562437Losses:  11.303913608193398 0.3125 1.0 6.572833552956581
CurrentTrain: epoch  6, batch    20 | loss: 11.3039136Losses:  6.980150684714317 0.25390851497650146 1.0 2.269677624106407
CurrentTrain: epoch  6, batch    21 | loss: 6.9801507Losses:  8.943128734827042 0.1389249563217163 1.0 4.319186836481094
CurrentTrain: epoch  6, batch    22 | loss: 8.9431287Losses:  6.837449863553047 0.1452803611755371 1.0 2.1783302575349808
CurrentTrain: epoch  6, batch    23 | loss: 6.8374499Losses:  11.211491420865059 0.06411635875701904 1.0 6.565624549984932
CurrentTrain: epoch  6, batch    24 | loss: 11.2114914Losses:  9.107205986976624 0.18780910968780518 1.0 4.416727662086487
CurrentTrain: epoch  6, batch    25 | loss: 9.1072060Losses:  6.880404695868492 0.2176896333694458 1.0 2.1833603233098984
CurrentTrain: epoch  6, batch    26 | loss: 6.8804047Losses:  6.855753377079964 0.13238930702209473 1.0 2.2084674388170242
CurrentTrain: epoch  6, batch    27 | loss: 6.8557534Losses:  11.582230016589165 0.08453726768493652 1.0 7.016617223620415
CurrentTrain: epoch  6, batch    28 | loss: 11.5822300Losses:  13.615768000483513 0.2512166500091553 1.0 8.930050417780876
CurrentTrain: epoch  6, batch    29 | loss: 13.6157680Losses:  11.24870590865612 0.3125 1.0 6.587372824549675
CurrentTrain: epoch  6, batch    30 | loss: 11.2487059Losses:  6.986866280436516 0.19138014316558838 1.0 2.260334298014641
CurrentTrain: epoch  6, batch    31 | loss: 6.9868663Losses:  9.381305262446404 0.4375 1.0 4.589952990412712
CurrentTrain: epoch  6, batch    32 | loss: 9.3813053Losses:  6.8229063004255295 0.1875 1.0 2.176209732890129
CurrentTrain: epoch  6, batch    33 | loss: 6.8229063Losses:  5.363397598266602 0.4138408899307251 1.0 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 5.3633976Losses:  9.248587280511856 0.13081908226013184 1.0398727655410767 4.476309448480606
CurrentTrain: epoch  6, batch    35 | loss: 9.2485873Losses:  9.072570115327835 0.2598583698272705 1.0 4.371955186128616
CurrentTrain: epoch  6, batch    36 | loss: 9.0725701Losses:  4.645344257354736 0.14203321933746338 1.0 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 4.6453443Losses:  12.692212298512459 0.3197840452194214 1.0 7.894419863820076
CurrentTrain: epoch  6, batch    38 | loss: 12.6922123Losses:  4.6239213943481445 0.06694209575653076 1.0 -0.0
CurrentTrain: epoch  6, batch    39 | loss: 4.6239214Losses:  4.683315277099609 0.1890416145324707 1.0 -0.0
CurrentTrain: epoch  6, batch    40 | loss: 4.6833153Losses:  6.853260040283203 0.13548362255096436 1.0 2.2420616149902344
CurrentTrain: epoch  6, batch    41 | loss: 6.8532600Losses:  9.177911937236786 0.133506178855896 1.0440278053283691 4.468787372112274
CurrentTrain: epoch  6, batch    42 | loss: 9.1779119Losses:  9.254200160503387 0.3436380624771118 1.0 4.516385734081268
CurrentTrain: epoch  6, batch    43 | loss: 9.2542002Losses:  6.8464027643203735 0.13007545471191406 1.0 2.1958857774734497
CurrentTrain: epoch  6, batch    44 | loss: 6.8464028Losses:  6.89767287671566 0.3152048587799072 1.0 2.1709844917058945
CurrentTrain: epoch  6, batch    45 | loss: 6.8976729Losses:  7.033125191926956 0.3218132257461548 1.0 2.29472616314888
CurrentTrain: epoch  6, batch    46 | loss: 7.0331252Losses:  9.105416357517242 0.2551230192184448 1.0 4.401060163974762
CurrentTrain: epoch  6, batch    47 | loss: 9.1054164Losses:  6.77855783700943 0.12668168544769287 1.0 2.1775513291358948
CurrentTrain: epoch  6, batch    48 | loss: 6.7785578Losses:  7.003644794225693 0.4026157855987549 1.0 2.175219386816025
CurrentTrain: epoch  6, batch    49 | loss: 7.0036448Losses:  6.906145736575127 0.25 1.0 2.1969005316495895
CurrentTrain: epoch  6, batch    50 | loss: 6.9061457Losses:  4.694964408874512 0.25581932067871094 1.0 -0.0
CurrentTrain: epoch  6, batch    51 | loss: 4.6949644Losses:  4.734358310699463 0.28689253330230713 1.0 -0.0
CurrentTrain: epoch  6, batch    52 | loss: 4.7343583Losses:  11.270223438739777 0.14243018627166748 1.0 6.64405757188797
CurrentTrain: epoch  6, batch    53 | loss: 11.2702234Losses:  4.621116638183594 0.19284284114837646 1.0 -0.0
CurrentTrain: epoch  6, batch    54 | loss: 4.6211166Losses:  9.40609085559845 0.06976568698883057 1.0 4.836073517799377
CurrentTrain: epoch  6, batch    55 | loss: 9.4060909Losses:  6.8367423713207245 0.06892693042755127 1.0 2.2306847274303436
CurrentTrain: epoch  6, batch    56 | loss: 6.8367424Losses:  9.124471366405487 0.06745553016662598 1.0 4.530378997325897
CurrentTrain: epoch  6, batch    57 | loss: 9.1244714Losses:  6.7923509776592255 0.14411747455596924 1.0 2.166362017393112
CurrentTrain: epoch  6, batch    58 | loss: 6.7923510Losses:  6.818370372056961 0.21359825134277344 1.0 2.1675439178943634
CurrentTrain: epoch  6, batch    59 | loss: 6.8183704Losses:  4.576093673706055 0.125 1.0 -0.0
CurrentTrain: epoch  6, batch    60 | loss: 4.5760937Losses:  6.908306524157524 0.32327497005462646 1.0 2.2067336291074753
CurrentTrain: epoch  6, batch    61 | loss: 6.9083065Losses:  11.182304054498672 0.03901505470275879 1.0 6.614273697137833
CurrentTrain: epoch  6, batch    62 | loss: 11.1823041Losses:  7.045478627085686 0.319439172744751 1.0460196733474731 2.273593232035637
CurrentTrain: epoch  7, batch     0 | loss: 7.0454786Losses:  9.032566666603088 0.19551873207092285 1.0 4.352328896522522
CurrentTrain: epoch  7, batch     1 | loss: 9.0325667Losses:  4.7443976402282715 0.31311094760894775 1.0 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 4.7443976Losses:  12.48645780980587 0.1875 1.0448521375656128 7.837902054190636
CurrentTrain: epoch  7, batch     3 | loss: 12.4864578Losses:  12.431396886706352 0.06352365016937256 1.0 7.940589353442192
CurrentTrain: epoch  7, batch     4 | loss: 12.4313969Losses:  9.158339008688927 0.25 1.0 4.465959057211876
CurrentTrain: epoch  7, batch     5 | loss: 9.1583390Losses:  4.645079612731934 0.12654876708984375 1.0 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 4.6450796Losses:  13.398490369319916 0.19220483303070068 1.0 8.750783860683441
CurrentTrain: epoch  7, batch     7 | loss: 13.3984904Losses:  6.78911679983139 0.1875 1.0 2.148718774318695
CurrentTrain: epoch  7, batch     8 | loss: 6.7891168Losses:  6.737086355686188 0.0008980035781860352 1.0 2.20024973154068
CurrentTrain: epoch  7, batch     9 | loss: 6.7370864Losses:  9.559379905462265 0.2765510082244873 1.0 4.856496661901474
CurrentTrain: epoch  7, batch    10 | loss: 9.5593799Losses:  4.6291656494140625 0.1919422149658203 1.0 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 4.6291656Losses:  8.867856815457344 0.01076364517211914 1.0 4.345499828457832
CurrentTrain: epoch  7, batch    12 | loss: 8.8678568Losses:  6.8043143302202225 0.1875 1.0 2.1938006430864334
CurrentTrain: epoch  7, batch    13 | loss: 6.8043143Losses:  9.098919346928596 0.0824124813079834 1.0 4.5352291613817215
CurrentTrain: epoch  7, batch    14 | loss: 9.0989193Losses:  8.958930909633636 0.25 1.0 4.308408677577972
CurrentTrain: epoch  7, batch    15 | loss: 8.9589309Losses:  14.306963354349136 0.3423588275909424 1.0437361001968384 9.045020014047623
CurrentTrain: epoch  7, batch    16 | loss: 14.3069634Losses:  6.754562824964523 0.12978601455688477 1.0 2.144921749830246
CurrentTrain: epoch  7, batch    17 | loss: 6.7545628Losses:  4.663972854614258 0.20636367797851562 1.0 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 4.6639729Losses:  11.542843788862228 0.0625 1.0 7.004548519849777
CurrentTrain: epoch  7, batch    19 | loss: 11.5428438Losses:  6.77596640586853 0.2516446113586426 1.0 2.1020772457122803
CurrentTrain: epoch  7, batch    20 | loss: 6.7759664Losses:  11.120033890008926 0.12601137161254883 1.0 6.528891235589981
CurrentTrain: epoch  7, batch    21 | loss: 11.1200339Losses:  8.945120126008987 0.12780869007110596 1.0 4.339506894350052
CurrentTrain: epoch  7, batch    22 | loss: 8.9451201Losses:  4.772626876831055 0.44986391067504883 1.0 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 4.7726269Losses:  4.600887298583984 0.0998682975769043 1.0 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 4.6008873Losses:  6.923789694905281 0.3265575170516968 1.0 2.211868479847908
CurrentTrain: epoch  7, batch    25 | loss: 6.9237897Losses:  8.886523962020874 0.1526472568511963 1.0 4.33111834526062
CurrentTrain: epoch  7, batch    26 | loss: 8.8865240Losses:  6.770446419715881 0.125 1.0 2.175143837928772
CurrentTrain: epoch  7, batch    27 | loss: 6.7704464Losses:  8.835178911685944 0.125 1.0 4.29135662317276
CurrentTrain: epoch  7, batch    28 | loss: 8.8351789Losses:  6.768425136804581 0.1293247938156128 1.0 2.189683586359024
CurrentTrain: epoch  7, batch    29 | loss: 6.7684251Losses:  4.559653282165527 0.1256181001663208 1.0 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 4.5596533Losses:  13.92827083170414 0.3125187158584595 1.0 9.275847926735878
CurrentTrain: epoch  7, batch    31 | loss: 13.9282708Losses:  9.022165551781654 0.2541787624359131 1.0 4.4082038551568985
CurrentTrain: epoch  7, batch    32 | loss: 9.0221656Losses:  12.387752026319504 0.1875 1.0 7.807067364454269
CurrentTrain: epoch  7, batch    33 | loss: 12.3877520Losses:  9.652217835187912 0.4375 1.0 4.875355213880539
CurrentTrain: epoch  7, batch    34 | loss: 9.6522178Losses:  4.673576354980469 0.2540273666381836 1.0 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 4.6735764Losses:  11.226943239569664 0.20403873920440674 1.0 6.518418058753014
CurrentTrain: epoch  7, batch    36 | loss: 11.2269432Losses:  9.10230439901352 0.14008450508117676 1.0 4.5003222823143005
CurrentTrain: epoch  7, batch    37 | loss: 9.1023044Losses:  9.109023749828339 0.375 1.0 4.3866230845451355
CurrentTrain: epoch  7, batch    38 | loss: 9.1090237Losses:  4.633762836456299 0.18893516063690186 1.0 -0.0
CurrentTrain: epoch  7, batch    39 | loss: 4.6337628Losses:  8.977207392454147 0.2534947395324707 1.0 4.338466852903366
CurrentTrain: epoch  7, batch    40 | loss: 8.9772074Losses:  9.19132037460804 0.375 1.0 4.469753220677376
CurrentTrain: epoch  7, batch    41 | loss: 9.1913204Losses:  4.636700630187988 0.2563365697860718 1.0 -0.0
CurrentTrain: epoch  7, batch    42 | loss: 4.6367006Losses:  6.990302786231041 0.19158995151519775 1.0 2.3714525550603867
CurrentTrain: epoch  7, batch    43 | loss: 6.9903028Losses:  9.513884201645851 0.1948380470275879 1.0 4.868060722947121
CurrentTrain: epoch  7, batch    44 | loss: 9.5138842Losses:  9.012625843286514 0.19188666343688965 1.0 4.4102984964847565
CurrentTrain: epoch  7, batch    45 | loss: 9.0126258Losses:  11.75368943810463 0.37914490699768066 1.0 7.00698247551918
CurrentTrain: epoch  7, batch    46 | loss: 11.7536894Losses:  6.84388630092144 0.1932997703552246 1.0 2.2240494936704636
CurrentTrain: epoch  7, batch    47 | loss: 6.8438863Losses:  6.821355864405632 0.18837547302246094 1.0 2.1689000576734543
CurrentTrain: epoch  7, batch    48 | loss: 6.8213559Losses:  6.8222825974226 0.2603846788406372 1.0 2.185323044657707
CurrentTrain: epoch  7, batch    49 | loss: 6.8222826Losses:  4.678066253662109 0.10900020599365234 1.0 -0.0
CurrentTrain: epoch  7, batch    50 | loss: 4.6780663Losses:  6.850268244743347 0.19118988513946533 1.0 2.1708463430404663
CurrentTrain: epoch  7, batch    51 | loss: 6.8502682Losses:  11.149105295538902 0.18750202655792236 1.0 6.574563726782799
CurrentTrain: epoch  7, batch    52 | loss: 11.1491053Losses:  6.778689429163933 0.25 1.0 2.156600520014763
CurrentTrain: epoch  7, batch    53 | loss: 6.7786894Losses:  11.302390843629837 0.3141545057296753 1.0 6.612095147371292
CurrentTrain: epoch  7, batch    54 | loss: 11.3023908Losses:  4.609638214111328 0.12849938869476318 1.0 -0.0
CurrentTrain: epoch  7, batch    55 | loss: 4.6096382Losses:  4.518074035644531 0.125 1.0 -0.0
CurrentTrain: epoch  7, batch    56 | loss: 4.5180740Losses:  4.599865436553955 0.18765294551849365 1.0 -0.0
CurrentTrain: epoch  7, batch    57 | loss: 4.5998654Losses:  13.1032145768404 0.0625 1.0 8.557854011654854
CurrentTrain: epoch  7, batch    58 | loss: 13.1032146Losses:  4.690878391265869 0.3126957416534424 1.0 -0.0
CurrentTrain: epoch  7, batch    59 | loss: 4.6908784Losses:  6.825376749038696 0.1875 1.0 2.2098867893218994
CurrentTrain: epoch  7, batch    60 | loss: 6.8253767Losses:  6.772805079817772 0.25664281845092773 1.0 2.127942904829979
CurrentTrain: epoch  7, batch    61 | loss: 6.7728051Losses:  4.692970275878906 0.3947262763977051 1.0 -0.0
CurrentTrain: epoch  7, batch    62 | loss: 4.6929703Losses:  6.890228092670441 0.20555377006530762 1.0 2.2194965481758118
CurrentTrain: epoch  8, batch     0 | loss: 6.8902281Losses:  8.983632519841194 0.1913890838623047 1.0 4.353061154484749
CurrentTrain: epoch  8, batch     1 | loss: 8.9836325Losses:  8.933239862322807 0.125 1.0 4.359564229846001
CurrentTrain: epoch  8, batch     2 | loss: 8.9332399Losses:  6.785323977470398 0.125 1.0 2.202502131462097
CurrentTrain: epoch  8, batch     3 | loss: 6.7853240Losses:  6.787718832492828 0.12534701824188232 1.0 2.1763020157814026
CurrentTrain: epoch  8, batch     4 | loss: 6.7877188Losses:  4.6266679763793945 0.19064438343048096 1.0 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 4.6266680Losses:  4.648218631744385 0.25 1.0 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 4.6482186Losses:  9.068991839885712 0.4021575450897217 1.0 4.335031688213348
CurrentTrain: epoch  8, batch     7 | loss: 9.0689918Losses:  9.182630568742752 0.3125 1.0 4.476207286119461
CurrentTrain: epoch  8, batch     8 | loss: 9.1826306Losses:  9.51242358982563 0.4375 1.0 4.800830438733101
CurrentTrain: epoch  8, batch     9 | loss: 9.5124236Losses:  4.539988994598389 0.0625 1.0 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 4.5399890Losses:  6.815955623984337 0.125 1.0 2.2602133601903915
CurrentTrain: epoch  8, batch    11 | loss: 6.8159556Losses:  6.732876807451248 0.0736929178237915 1.0 2.1356158554553986
CurrentTrain: epoch  8, batch    12 | loss: 6.7328768Losses:  6.724387675523758 0.125 1.0 2.160924941301346
CurrentTrain: epoch  8, batch    13 | loss: 6.7243877Losses:  6.748871177434921 0.2177891731262207 1.0 2.1413935124874115
CurrentTrain: epoch  8, batch    14 | loss: 6.7488712Losses:  6.766810774803162 0.13528382778167725 1.0 2.182303309440613
CurrentTrain: epoch  8, batch    15 | loss: 6.7668108Losses:  11.077665239572525 0.0625 1.0 6.500807195901871
CurrentTrain: epoch  8, batch    16 | loss: 11.0776652Losses:  4.558558940887451 0.19227349758148193 1.0 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 4.5585589Losses:  6.7618149518966675 0.125 1.0 2.173258662223816
CurrentTrain: epoch  8, batch    18 | loss: 6.7618150Losses:  11.594080045819283 0.3125 1.0 6.898504331707954
CurrentTrain: epoch  8, batch    19 | loss: 11.5940800Losses:  4.723860740661621 0.3125 1.0384775400161743 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 4.7238607Losses:  4.667840957641602 0.25 1.0 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 4.6678410Losses:  11.810036763548851 0.20300495624542236 1.0 7.1194602102041245
CurrentTrain: epoch  8, batch    22 | loss: 11.8100368Losses:  16.516404777765274 0.25 1.0 11.843866974115372
CurrentTrain: epoch  8, batch    23 | loss: 16.5164048Losses:  4.670095443725586 0.3162651062011719 1.0 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 4.6700954Losses:  18.785355389118195 0.003299236297607422 1.0 14.343189060688019
CurrentTrain: epoch  8, batch    25 | loss: 18.7853554Losses:  6.802846848964691 0.3125 1.0 2.12030690908432
CurrentTrain: epoch  8, batch    26 | loss: 6.8028468Losses:  16.633804976940155 0.5 1.0 11.868325889110565
CurrentTrain: epoch  8, batch    27 | loss: 16.6338050Losses:  4.622885704040527 0.25 1.0 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 4.6228857Losses:  11.275140434503555 0.375 1.0 6.554324775934219
CurrentTrain: epoch  8, batch    29 | loss: 11.2751404Losses:  6.757396906614304 0.0625 1.0 2.2062894999980927
CurrentTrain: epoch  8, batch    30 | loss: 6.7573969Losses:  11.943916112184525 0.25 1.0386532545089722 7.269394665956497
CurrentTrain: epoch  8, batch    31 | loss: 11.9439161Losses:  9.067486062645912 0.37770092487335205 1.0 4.356791749596596
CurrentTrain: epoch  8, batch    32 | loss: 9.0674861Losses:  4.614439964294434 0.19437527656555176 1.0 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 4.6144400Losses:  6.847861185669899 0.12863075733184814 1.0 2.230181112885475
CurrentTrain: epoch  8, batch    34 | loss: 6.8478612Losses:  6.750092193484306 0.1875 1.0 2.1361014097929
CurrentTrain: epoch  8, batch    35 | loss: 6.7500922Losses:  6.811407774686813 0.1875 1.0 2.20498725771904
CurrentTrain: epoch  8, batch    36 | loss: 6.8114078Losses:  4.587841510772705 0.1337805986404419 1.0 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 4.5878415Losses:  16.105556905269623 0.19460153579711914 1.0 11.492027699947357
CurrentTrain: epoch  8, batch    38 | loss: 16.1055569Losses:  10.954317688941956 0.1898646354675293 1.0 6.372965455055237
CurrentTrain: epoch  8, batch    39 | loss: 10.9543177Losses:  6.806271389126778 0.2556115388870239 1.0 2.169961765408516
CurrentTrain: epoch  8, batch    40 | loss: 6.8062714Losses:  6.806170478463173 0.07000398635864258 1.0 2.2238168865442276
CurrentTrain: epoch  8, batch    41 | loss: 6.8061705Losses:  8.973940059542656 0.25 1.0 4.301615878939629
CurrentTrain: epoch  8, batch    42 | loss: 8.9739401Losses:  14.68573945760727 0.5625 1.0 9.893523156642914
CurrentTrain: epoch  8, batch    43 | loss: 14.6857395Losses:  6.776028960943222 0.1875 1.0 2.1658218801021576
CurrentTrain: epoch  8, batch    44 | loss: 6.7760290Losses:  6.776405140757561 0.125 1.0 2.178828999400139
CurrentTrain: epoch  8, batch    45 | loss: 6.7764051Losses:  11.196058377623558 0.12692928314208984 1.0 6.656003579497337
CurrentTrain: epoch  8, batch    46 | loss: 11.1960584Losses:  6.831013530492783 0.18980085849761963 1.0 2.2526415288448334
CurrentTrain: epoch  8, batch    47 | loss: 6.8310135Losses:  4.569591045379639 0.08616375923156738 1.0 -0.0
CurrentTrain: epoch  8, batch    48 | loss: 4.5695910Losses:  4.580109596252441 0.125 1.0 -0.0
CurrentTrain: epoch  8, batch    49 | loss: 4.5801096Losses:  9.096519440412521 0.4375 1.0 4.342136353254318
CurrentTrain: epoch  8, batch    50 | loss: 9.0965194Losses:  6.789136067032814 0.2024904489517212 1.0 2.1679827123880386
CurrentTrain: epoch  8, batch    51 | loss: 6.7891361Losses:  6.886016860604286 0.25171172618865967 1.0 2.246572032570839
CurrentTrain: epoch  8, batch    52 | loss: 6.8860169Losses:  4.589655876159668 0.1875 1.0 -0.0
CurrentTrain: epoch  8, batch    53 | loss: 4.5896559Losses:  9.43547423183918 0.03543257713317871 1.0 4.83173306286335
CurrentTrain: epoch  8, batch    54 | loss: 9.4354742Losses:  6.637546926736832 0.06425559520721436 1.0 2.1279548704624176
CurrentTrain: epoch  8, batch    55 | loss: 6.6375469Losses:  9.340952411293983 0.125 1.0 4.803419128060341
CurrentTrain: epoch  8, batch    56 | loss: 9.3409524Losses:  9.306207582354546 0.0625 1.0 4.8020123690366745
CurrentTrain: epoch  8, batch    57 | loss: 9.3062076Losses:  6.738941162824631 0.13217830657958984 1.0 2.1564888656139374
CurrentTrain: epoch  8, batch    58 | loss: 6.7389412Losses:  6.678334549069405 0.0662006139755249 1.0 2.116694763302803
CurrentTrain: epoch  8, batch    59 | loss: 6.6783345Losses:  4.620326519012451 0.25 1.0 -0.0
CurrentTrain: epoch  8, batch    60 | loss: 4.6203265Losses:  11.617495074868202 0.2600809335708618 1.0 6.990535274147987
CurrentTrain: epoch  8, batch    61 | loss: 11.6174951Losses:  6.880814164876938 0.5 1.0 2.10817489027977
CurrentTrain: epoch  8, batch    62 | loss: 6.8808142Losses:  8.943267315626144 0.25 1.0 4.325348824262619
CurrentTrain: epoch  9, batch     0 | loss: 8.9432673Losses:  4.438081741333008 0.0 1.0 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 4.4380817Losses:  4.6193695068359375 0.18880629539489746 1.0 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 4.6193695Losses:  8.863712444901466 0.13280582427978516 1.0 4.305194988846779
CurrentTrain: epoch  9, batch     3 | loss: 8.8637124Losses:  9.00525812804699 0.3125 1.0 4.318155810236931
CurrentTrain: epoch  9, batch     4 | loss: 9.0052581Losses:  9.318928554654121 0.0625 1.0 4.761882618069649
CurrentTrain: epoch  9, batch     5 | loss: 9.3189286Losses:  6.691818743944168 0.1875 1.0 2.1236667931079865
CurrentTrain: epoch  9, batch     6 | loss: 6.6918187Losses:  8.883363768458366 0.18903112411499023 1.0 4.322434946894646
CurrentTrain: epoch  9, batch     7 | loss: 8.8833638Losses:  9.03797510266304 0.4375 1.0 4.330305367708206
CurrentTrain: epoch  9, batch     8 | loss: 9.0379751Losses:  4.517557621002197 0.0647956132888794 1.0 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 4.5175576Losses:  9.115285858511925 0.375 1.0 4.418495640158653
CurrentTrain: epoch  9, batch    10 | loss: 9.1152859Losses:  6.671831846237183 0.125 1.0 2.1161935329437256
CurrentTrain: epoch  9, batch    11 | loss: 6.6718318Losses:  9.051294222474098 0.375 1.0 4.3848356157541275
CurrentTrain: epoch  9, batch    12 | loss: 9.0512942Losses:  4.559609889984131 0.125 1.0 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 4.5596099Losses:  8.83552497625351 0.0639568567276001 1.0 4.299745500087738
CurrentTrain: epoch  9, batch    14 | loss: 8.8355250Losses:  9.308739766478539 0.1875 1.0 4.764456853270531
CurrentTrain: epoch  9, batch    15 | loss: 9.3087398Losses:  9.132749363780022 0.19390428066253662 1.0 4.488245293498039
CurrentTrain: epoch  9, batch    16 | loss: 9.1327494Losses:  4.577033042907715 0.1875 1.0 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 4.5770330Losses:  6.8259032517671585 0.1875 1.0 2.2419236451387405
CurrentTrain: epoch  9, batch    18 | loss: 6.8259033Losses:  13.80903334915638 0.18967413902282715 1.0 9.239991143345833
CurrentTrain: epoch  9, batch    19 | loss: 13.8090333Losses:  6.749955713748932 0.25 1.0 2.147943079471588
CurrentTrain: epoch  9, batch    20 | loss: 6.7499557Losses:  6.794409886002541 0.2750363349914551 1.0 2.14117731153965
CurrentTrain: epoch  9, batch    21 | loss: 6.7944099Losses:  4.60949182510376 0.2514442205429077 1.0 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 4.6094918Losses:  9.10221603512764 0.4375 1.0 4.368893414735794
CurrentTrain: epoch  9, batch    23 | loss: 9.1022160Losses:  4.591964244842529 0.08878958225250244 1.0 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 4.5919642Losses:  11.13500639796257 0.1875 1.0 6.572664707899094
CurrentTrain: epoch  9, batch    25 | loss: 11.1350064Losses:  4.582864761352539 0.07373607158660889 1.0 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 4.5828648Losses:  8.812007054686546 0.1875 1.0 4.240962132811546
CurrentTrain: epoch  9, batch    27 | loss: 8.8120071Losses:  8.904755115509033 0.25 1.0 4.319332599639893
CurrentTrain: epoch  9, batch    28 | loss: 8.9047551Losses:  8.955913126468658 0.25505924224853516 1.0 4.309663832187653
CurrentTrain: epoch  9, batch    29 | loss: 8.9559131Losses:  6.807305410504341 0.13131284713745117 1.0 2.2214012891054153
CurrentTrain: epoch  9, batch    30 | loss: 6.8073054Losses:  9.462936624884605 0.1961803436279297 1.0 4.894919618964195
CurrentTrain: epoch  9, batch    31 | loss: 9.4629366Losses:  11.913473919034004 0.125 1.0 7.3362558633089066
CurrentTrain: epoch  9, batch    32 | loss: 11.9134739Losses:  11.215804159641266 0.314450740814209 1.0 6.580726683139801
CurrentTrain: epoch  9, batch    33 | loss: 11.2158042Losses:  4.529141902923584 0.125 1.0 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 4.5291419Losses:  11.538977682590485 0.125 1.0 6.963968336582184
CurrentTrain: epoch  9, batch    35 | loss: 11.5389777Losses:  8.804801240563393 0.1884211301803589 1.0 4.2373311668634415
CurrentTrain: epoch  9, batch    36 | loss: 8.8048012Losses:  4.573418140411377 0.1875 1.0 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 4.5734181Losses:  6.72280253469944 0.125 1.0 2.1458124071359634
CurrentTrain: epoch  9, batch    38 | loss: 6.7228025Losses:  4.637518882751465 0.25 1.0 -0.0
CurrentTrain: epoch  9, batch    39 | loss: 4.6375189Losses:  6.844570815563202 0.1875 1.0 2.265617549419403
CurrentTrain: epoch  9, batch    40 | loss: 6.8445708Losses:  4.554617404937744 0.1914522647857666 1.0 -0.0
CurrentTrain: epoch  9, batch    41 | loss: 4.5546174Losses:  11.282117068767548 0.12717926502227783 1.0 6.699816405773163
CurrentTrain: epoch  9, batch    42 | loss: 11.2821171Losses:  8.935072436928749 0.25 1.0 4.318996921181679
CurrentTrain: epoch  9, batch    43 | loss: 8.9350724Losses:  6.803185284137726 0.375 1.0 2.1190856099128723
CurrentTrain: epoch  9, batch    44 | loss: 6.8031853Losses:  6.91136708855629 0.375 1.0 2.250713974237442
CurrentTrain: epoch  9, batch    45 | loss: 6.9113671Losses:  9.047360464930534 0.20247578620910645 1.0 4.422737643122673
CurrentTrain: epoch  9, batch    46 | loss: 9.0473605Losses:  8.86950607597828 0.125 1.0 4.315713599324226
CurrentTrain: epoch  9, batch    47 | loss: 8.8695061Losses:  4.59278678894043 0.1875 1.0 -0.0
CurrentTrain: epoch  9, batch    48 | loss: 4.5927868Losses:  9.019253507256508 0.1875 1.0 4.4031031280756
CurrentTrain: epoch  9, batch    49 | loss: 9.0192535Losses:  11.12176801264286 0.1875 1.0 6.546038165688515
CurrentTrain: epoch  9, batch    50 | loss: 11.1217680Losses:  4.688635349273682 0.375 1.0 -0.0
CurrentTrain: epoch  9, batch    51 | loss: 4.6886353Losses:  8.889687061309814 0.125 1.0 4.32744836807251
CurrentTrain: epoch  9, batch    52 | loss: 8.8896871Losses:  8.827667266130447 0.08270573616027832 1.0 4.309861212968826
CurrentTrain: epoch  9, batch    53 | loss: 8.8276673Losses:  9.076272159814835 0.375 1.0 4.413084179162979
CurrentTrain: epoch  9, batch    54 | loss: 9.0762722Losses:  11.369156956672668 0.1947687864303589 1.0 6.756978154182434
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
