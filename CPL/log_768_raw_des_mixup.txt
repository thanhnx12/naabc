#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.027637481689453 1.8446838855743408 0.993945837020874
CurrentTrain: epoch  0, batch     0 | loss: 12.8662672Losses:  9.96487045288086 2.194693088531494 1.0027965307235718
CurrentTrain: epoch  0, batch     1 | loss: 13.1623592Losses:  10.061714172363281 1.7269537448883057 0.986219584941864
CurrentTrain: epoch  0, batch     2 | loss: 12.7748871Losses:  10.003978729248047 1.7329843044281006 0.9977442622184753
CurrentTrain: epoch  0, batch     3 | loss: 12.7347078Losses:  9.244211196899414 1.607071876525879 0.9957209229469299
CurrentTrain: epoch  0, batch     4 | loss: 11.8470039Losses:  10.406216621398926 1.5616636276245117 0.9905915260314941
CurrentTrain: epoch  0, batch     5 | loss: 12.9584713Losses:  9.538984298706055 1.8090895414352417 0.9733504056930542
CurrentTrain: epoch  0, batch     6 | loss: 12.3214245Losses:  8.967475891113281 1.498184323310852 0.9888925552368164
CurrentTrain: epoch  0, batch     7 | loss: 11.4545527Losses:  9.131485939025879 1.5278093814849854 0.9789913892745972
CurrentTrain: epoch  0, batch     8 | loss: 11.6382866Losses:  9.712904930114746 1.7479490041732788 0.9925455451011658
CurrentTrain: epoch  0, batch     9 | loss: 12.4533987Losses:  9.00164794921875 1.4566264152526855 0.9893059134483337
CurrentTrain: epoch  0, batch    10 | loss: 11.4475803Losses:  8.4696626663208 1.3136649131774902 0.9703270792961121
CurrentTrain: epoch  0, batch    11 | loss: 10.7536545Losses:  9.15142822265625 1.5641123056411743 0.9832565784454346
CurrentTrain: epoch  0, batch    12 | loss: 11.6987972Losses:  9.029424667358398 1.4821889400482178 0.9751160144805908
CurrentTrain: epoch  0, batch    13 | loss: 11.4867296Losses:  8.189871788024902 1.5523664951324463 0.9765104055404663
CurrentTrain: epoch  0, batch    14 | loss: 10.7187481Losses:  8.844123840332031 1.7182519435882568 0.9743671417236328
CurrentTrain: epoch  0, batch    15 | loss: 11.5367432Losses:  9.300430297851562 1.3294026851654053 0.9736911654472351
CurrentTrain: epoch  0, batch    16 | loss: 11.6035242Losses:  9.099592208862305 1.7369450330734253 0.9759347438812256
CurrentTrain: epoch  0, batch    17 | loss: 11.8124723Losses:  8.978792190551758 1.410965919494629 0.9741964936256409
CurrentTrain: epoch  0, batch    18 | loss: 11.3639545Losses:  8.711965560913086 1.6972343921661377 0.9857692122459412
CurrentTrain: epoch  0, batch    19 | loss: 11.3949690Losses:  8.103774070739746 1.3228403329849243 0.9552783370018005
CurrentTrain: epoch  0, batch    20 | loss: 10.3818932Losses:  8.22382926940918 1.489548921585083 0.9712133407592773
CurrentTrain: epoch  0, batch    21 | loss: 10.6845913Losses:  8.749902725219727 1.4578025341033936 0.9596371650695801
CurrentTrain: epoch  0, batch    22 | loss: 11.1673431Losses:  8.137306213378906 1.189234972000122 0.9400699138641357
CurrentTrain: epoch  0, batch    23 | loss: 10.2666111Losses:  7.8329644203186035 1.4353141784667969 0.9556270837783813
CurrentTrain: epoch  0, batch    24 | loss: 10.2239056Losses:  8.039034843444824 1.2165346145629883 0.9654861688613892
CurrentTrain: epoch  0, batch    25 | loss: 10.2210560Losses:  8.255914688110352 1.2255983352661133 0.965693473815918
CurrentTrain: epoch  0, batch    26 | loss: 10.4472065Losses:  7.70166015625 1.3911654949188232 0.9689233899116516
CurrentTrain: epoch  0, batch    27 | loss: 10.0617495Losses:  8.226593017578125 1.278422474861145 0.9620240926742554
CurrentTrain: epoch  0, batch    28 | loss: 10.4670391Losses:  8.08165454864502 1.282105565071106 0.9399919509887695
CurrentTrain: epoch  0, batch    29 | loss: 10.3037519Losses:  8.572827339172363 1.4876651763916016 0.9665138721466064
CurrentTrain: epoch  0, batch    30 | loss: 11.0270061Losses:  8.005037307739258 1.0783054828643799 0.940352201461792
CurrentTrain: epoch  0, batch    31 | loss: 10.0236950Losses:  8.041349411010742 1.0322340726852417 0.9701181650161743
CurrentTrain: epoch  0, batch    32 | loss: 10.0437021Losses:  7.513108253479004 0.9847418069839478 0.9658466577529907
CurrentTrain: epoch  0, batch    33 | loss: 9.4636974Losses:  7.897279739379883 1.0982943773269653 0.9278118014335632
CurrentTrain: epoch  0, batch    34 | loss: 9.9233856Losses:  7.860299110412598 1.123136281967163 0.965857744216919
CurrentTrain: epoch  0, batch    35 | loss: 9.9492931Losses:  7.753383636474609 1.3585331439971924 0.9584769010543823
CurrentTrain: epoch  0, batch    36 | loss: 10.0703936Losses:  7.819258689880371 1.13386869430542 0.9392199516296387
CurrentTrain: epoch  0, batch    37 | loss: 9.8923473Losses:  8.501477241516113 1.3647456169128418 0.9683001041412354
CurrentTrain: epoch  0, batch    38 | loss: 10.8345222Losses:  8.14481258392334 1.253771185874939 0.9483441114425659
CurrentTrain: epoch  0, batch    39 | loss: 10.3469276Losses:  8.496698379516602 1.2751208543777466 0.9675871133804321
CurrentTrain: epoch  0, batch    40 | loss: 10.7394066Losses:  7.449670314788818 1.1210620403289795 0.964385449886322
CurrentTrain: epoch  0, batch    41 | loss: 9.5351171Losses:  7.324828147888184 1.1504371166229248 0.9443845748901367
CurrentTrain: epoch  0, batch    42 | loss: 9.4196501Losses:  7.026811599731445 0.8651424646377563 0.9475593566894531
CurrentTrain: epoch  0, batch    43 | loss: 8.8395138Losses:  7.4349470138549805 1.0451595783233643 0.9459471702575684
CurrentTrain: epoch  0, batch    44 | loss: 9.4260540Losses:  7.370561122894287 1.1975243091583252 0.9529187679290771
CurrentTrain: epoch  0, batch    45 | loss: 9.5210047Losses:  8.16391372680664 0.9620398879051208 0.9615123271942139
CurrentTrain: epoch  0, batch    46 | loss: 10.0874662Losses:  6.69692325592041 1.156333565711975 0.946716845035553
CurrentTrain: epoch  0, batch    47 | loss: 8.7999735Losses:  7.65617036819458 1.3480509519577026 0.9465823173522949
CurrentTrain: epoch  0, batch    48 | loss: 9.9508038Losses:  7.294140338897705 1.230470061302185 0.9487496614456177
CurrentTrain: epoch  0, batch    49 | loss: 9.4733601Losses:  7.6609296798706055 1.1850523948669434 0.9391082525253296
CurrentTrain: epoch  0, batch    50 | loss: 9.7850895Losses:  7.232954025268555 1.1513339281082153 0.9416936635971069
CurrentTrain: epoch  0, batch    51 | loss: 9.3259811Losses:  7.0513081550598145 0.9492706060409546 0.9213511347770691
CurrentTrain: epoch  0, batch    52 | loss: 8.9219303Losses:  7.194843769073486 1.1656066179275513 0.9464023113250732
CurrentTrain: epoch  0, batch    53 | loss: 9.3068533Losses:  7.789189338684082 1.355767011642456 0.9611056447029114
CurrentTrain: epoch  0, batch    54 | loss: 10.1060619Losses:  6.908290386199951 0.999500572681427 0.9561248421669006
CurrentTrain: epoch  0, batch    55 | loss: 8.8639164Losses:  6.884699821472168 0.9154952764511108 0.8957436680793762
CurrentTrain: epoch  0, batch    56 | loss: 8.6959391Losses:  7.557592391967773 1.063366174697876 0.9595432877540588
CurrentTrain: epoch  0, batch    57 | loss: 9.5805016Losses:  7.525515556335449 0.8577101230621338 0.9271811246871948
CurrentTrain: epoch  0, batch    58 | loss: 9.3104067Losses:  7.1150617599487305 0.7918233275413513 0.9379379749298096
CurrentTrain: epoch  0, batch    59 | loss: 8.8448229Losses:  6.631606101989746 0.900898277759552 0.9186105132102966
CurrentTrain: epoch  0, batch    60 | loss: 8.4511147Losses:  7.604911804199219 0.7387619614601135 0.9249292612075806
CurrentTrain: epoch  0, batch    61 | loss: 9.2686033Losses:  9.112034797668457 0.7786229252815247 0.9528177976608276
CurrentTrain: epoch  0, batch    62 | loss: 10.8434753Losses:  7.470609664916992 0.9218761920928955 0.9419594407081604
CurrentTrain: epoch  1, batch     0 | loss: 9.3344450Losses:  7.030239105224609 0.7962238192558289 0.925810694694519
CurrentTrain: epoch  1, batch     1 | loss: 8.7522736Losses:  6.788773536682129 1.0052762031555176 0.9259535074234009
CurrentTrain: epoch  1, batch     2 | loss: 8.7200031Losses:  7.158047676086426 0.935357391834259 0.9291023015975952
CurrentTrain: epoch  1, batch     3 | loss: 9.0225067Losses:  7.3651533126831055 0.991623044013977 0.9254885315895081
CurrentTrain: epoch  1, batch     4 | loss: 9.2822647Losses:  6.925470352172852 0.7576432228088379 0.926389217376709
CurrentTrain: epoch  1, batch     5 | loss: 8.6095028Losses:  6.845396518707275 0.8810020685195923 0.9131362438201904
CurrentTrain: epoch  1, batch     6 | loss: 8.6395350Losses:  6.257286071777344 0.8584094047546387 0.9154193997383118
CurrentTrain: epoch  1, batch     7 | loss: 8.0311146Losses:  6.466061115264893 0.6989730596542358 0.9021333456039429
CurrentTrain: epoch  1, batch     8 | loss: 8.0671673Losses:  7.112127304077148 0.9799088835716248 0.9350661039352417
CurrentTrain: epoch  1, batch     9 | loss: 9.0271025Losses:  7.041997909545898 0.8863050937652588 0.947472870349884
CurrentTrain: epoch  1, batch    10 | loss: 8.8757753Losses:  6.610807418823242 1.0627264976501465 0.9367022514343262
CurrentTrain: epoch  1, batch    11 | loss: 8.6102362Losses:  6.8996124267578125 0.9535242319107056 0.9356957674026489
CurrentTrain: epoch  1, batch    12 | loss: 8.7888327Losses:  7.234256267547607 0.7051136493682861 0.9378178119659424
CurrentTrain: epoch  1, batch    13 | loss: 8.8771877Losses:  6.2577972412109375 0.8205576539039612 0.9122954607009888
CurrentTrain: epoch  1, batch    14 | loss: 7.9906502Losses:  7.749372959136963 0.8665144443511963 0.955380916595459
CurrentTrain: epoch  1, batch    15 | loss: 9.5712681Losses:  6.623441696166992 0.7682756781578064 0.8911854028701782
CurrentTrain: epoch  1, batch    16 | loss: 8.2829027Losses:  6.0358171463012695 0.5764404535293579 0.9133827686309814
CurrentTrain: epoch  1, batch    17 | loss: 7.5256405Losses:  6.498442649841309 0.6656584739685059 0.9052641987800598
CurrentTrain: epoch  1, batch    18 | loss: 8.0693655Losses:  6.708970069885254 0.8009702563285828 0.9032471179962158
CurrentTrain: epoch  1, batch    19 | loss: 8.4131870Losses:  7.082722187042236 0.905227780342102 0.9278963804244995
CurrentTrain: epoch  1, batch    20 | loss: 8.9158459Losses:  7.083571434020996 0.9298202991485596 0.9248424768447876
CurrentTrain: epoch  1, batch    21 | loss: 8.9382343Losses:  6.703644275665283 0.874315619468689 0.9063012599945068
CurrentTrain: epoch  1, batch    22 | loss: 8.4842615Losses:  6.877073764801025 0.6132023334503174 0.9037994146347046
CurrentTrain: epoch  1, batch    23 | loss: 8.3940754Losses:  6.588057518005371 0.7733047008514404 0.8874216079711914
CurrentTrain: epoch  1, batch    24 | loss: 8.2487841Losses:  5.610895156860352 0.6715787649154663 0.8777465224266052
CurrentTrain: epoch  1, batch    25 | loss: 7.1602206Losses:  6.190025329589844 0.8616677522659302 0.9139800071716309
CurrentTrain: epoch  1, batch    26 | loss: 7.9656730Losses:  6.072565078735352 0.8366384506225586 0.9151281714439392
CurrentTrain: epoch  1, batch    27 | loss: 7.8243318Losses:  6.635380268096924 0.7365122437477112 0.906810998916626
CurrentTrain: epoch  1, batch    28 | loss: 8.2787037Losses:  5.994776725769043 0.805855929851532 0.905523419380188
CurrentTrain: epoch  1, batch    29 | loss: 7.7061558Losses:  6.469067573547363 0.5756672620773315 0.922904908657074
CurrentTrain: epoch  1, batch    30 | loss: 7.9676399Losses:  5.671387672424316 0.7890619039535522 0.8922196626663208
CurrentTrain: epoch  1, batch    31 | loss: 7.3526692Losses:  5.598254203796387 0.7767610549926758 0.8761619329452515
CurrentTrain: epoch  1, batch    32 | loss: 7.2511773Losses:  5.9885358810424805 0.5545744299888611 0.8954497575759888
CurrentTrain: epoch  1, batch    33 | loss: 7.4385600Losses:  6.705390930175781 0.8795469999313354 0.913764238357544
CurrentTrain: epoch  1, batch    34 | loss: 8.4987020Losses:  5.661331653594971 0.7416199445724487 0.8744466304779053
CurrentTrain: epoch  1, batch    35 | loss: 7.2773981Losses:  6.970442771911621 0.5651410222053528 0.949045717716217
CurrentTrain: epoch  1, batch    36 | loss: 8.4846296Losses:  6.594025611877441 0.8348022699356079 0.9308326840400696
CurrentTrain: epoch  1, batch    37 | loss: 8.3596601Losses:  7.053765296936035 0.8805581331253052 0.9064749479293823
CurrentTrain: epoch  1, batch    38 | loss: 8.8407984Losses:  5.629578113555908 0.6074978113174438 0.8989259004592896
CurrentTrain: epoch  1, batch    39 | loss: 7.1360016Losses:  6.599320411682129 0.5857117772102356 0.8999795913696289
CurrentTrain: epoch  1, batch    40 | loss: 8.0850124Losses:  6.84726095199585 0.94189453125 0.91778564453125
CurrentTrain: epoch  1, batch    41 | loss: 8.7069416Losses:  7.061438083648682 0.7505261898040771 0.881308913230896
CurrentTrain: epoch  1, batch    42 | loss: 8.6932726Losses:  7.3234639167785645 0.8606691360473633 0.9133721590042114
CurrentTrain: epoch  1, batch    43 | loss: 9.0975056Losses:  7.412381172180176 0.6897286176681519 0.940376877784729
CurrentTrain: epoch  1, batch    44 | loss: 9.0424871Losses:  5.940338611602783 0.6764063835144043 0.9018276929855347
CurrentTrain: epoch  1, batch    45 | loss: 7.5185728Losses:  5.474442481994629 0.7094690799713135 0.8865364789962769
CurrentTrain: epoch  1, batch    46 | loss: 7.0704479Losses:  6.450819969177246 0.851226270198822 0.8916349411010742
CurrentTrain: epoch  1, batch    47 | loss: 8.1936817Losses:  6.284430503845215 0.7874619960784912 0.9015300869941711
CurrentTrain: epoch  1, batch    48 | loss: 7.9734230Losses:  5.835488319396973 0.6664191484451294 0.91489577293396
CurrentTrain: epoch  1, batch    49 | loss: 7.4168034Losses:  6.2121052742004395 0.6338375806808472 0.8690451383590698
CurrentTrain: epoch  1, batch    50 | loss: 7.7149882Losses:  7.043379306793213 0.8642504215240479 0.928929328918457
CurrentTrain: epoch  1, batch    51 | loss: 8.8365593Losses:  6.27379035949707 0.6952781677246094 0.901749849319458
CurrentTrain: epoch  1, batch    52 | loss: 7.8708181Losses:  6.11222505569458 0.6206498742103577 0.9237198829650879
CurrentTrain: epoch  1, batch    53 | loss: 7.6565948Losses:  6.351109504699707 0.725588858127594 0.8818892240524292
CurrentTrain: epoch  1, batch    54 | loss: 7.9585876Losses:  5.187173366546631 0.38801273703575134 0.8983877897262573
CurrentTrain: epoch  1, batch    55 | loss: 6.4735742Losses:  5.360992431640625 0.5410925149917603 0.9096733331680298
CurrentTrain: epoch  1, batch    56 | loss: 6.8117580Losses:  6.104486465454102 0.7087967991828918 0.8978629112243652
CurrentTrain: epoch  1, batch    57 | loss: 7.7111464Losses:  5.594871520996094 0.517542839050293 0.8814207315444946
CurrentTrain: epoch  1, batch    58 | loss: 6.9938350Losses:  6.6603474617004395 0.8492746949195862 0.9135361909866333
CurrentTrain: epoch  1, batch    59 | loss: 8.4231586Losses:  6.29307222366333 0.8039650917053223 0.8874287605285645
CurrentTrain: epoch  1, batch    60 | loss: 7.9844661Losses:  5.572465896606445 0.6903622150421143 0.9092313647270203
CurrentTrain: epoch  1, batch    61 | loss: 7.1720591Losses:  4.78929328918457 0.25815361738204956 0.8474934697151184
CurrentTrain: epoch  1, batch    62 | loss: 5.8949404Losses:  5.904687881469727 0.5570951104164124 0.8728014826774597
CurrentTrain: epoch  2, batch     0 | loss: 7.3345842Losses:  5.7922563552856445 0.5242022275924683 0.9087997674942017
CurrentTrain: epoch  2, batch     1 | loss: 7.2252584Losses:  6.099969387054443 0.6480282545089722 0.9014819264411926
CurrentTrain: epoch  2, batch     2 | loss: 7.6494799Losses:  5.186589241027832 0.5848166942596436 0.8473437428474426
CurrentTrain: epoch  2, batch     3 | loss: 6.6187501Losses:  5.749510765075684 0.5352312326431274 0.8708852529525757
CurrentTrain: epoch  2, batch     4 | loss: 7.1556273Losses:  5.3900299072265625 0.6506139039993286 0.8618345260620117
CurrentTrain: epoch  2, batch     5 | loss: 6.9024782Losses:  5.54429292678833 0.4503559470176697 0.8994011878967285
CurrentTrain: epoch  2, batch     6 | loss: 6.8940501Losses:  6.00715446472168 0.5422660112380981 0.9038702845573425
CurrentTrain: epoch  2, batch     7 | loss: 7.4532905Losses:  6.6275835037231445 0.5636005401611328 0.8811901211738586
CurrentTrain: epoch  2, batch     8 | loss: 8.0723743Losses:  5.241330623626709 0.3976406455039978 0.9089652895927429
CurrentTrain: epoch  2, batch     9 | loss: 6.5479364Losses:  5.192553520202637 0.5602896213531494 0.857040524482727
CurrentTrain: epoch  2, batch    10 | loss: 6.6098833Losses:  5.878048896789551 0.5311898589134216 0.8798493146896362
CurrentTrain: epoch  2, batch    11 | loss: 7.2890882Losses:  6.125490188598633 0.5678385496139526 0.9409998655319214
CurrentTrain: epoch  2, batch    12 | loss: 7.6343288Losses:  5.801527500152588 0.4776309132575989 0.8870786428451538
CurrentTrain: epoch  2, batch    13 | loss: 7.1662374Losses:  5.590243816375732 0.5267871618270874 0.9018244743347168
CurrentTrain: epoch  2, batch    14 | loss: 7.0188556Losses:  5.385074138641357 0.4862130284309387 0.9096692800521851
CurrentTrain: epoch  2, batch    15 | loss: 6.7809567Losses:  4.987842082977295 0.445544958114624 0.9081094861030579
CurrentTrain: epoch  2, batch    16 | loss: 6.3414965Losses:  4.7532830238342285 0.40087997913360596 0.8392280340194702
CurrentTrain: epoch  2, batch    17 | loss: 5.9933910Losses:  5.993048191070557 0.5108569860458374 0.8848698735237122
CurrentTrain: epoch  2, batch    18 | loss: 7.3887753Losses:  5.523049354553223 0.4213411808013916 0.9227688312530518
CurrentTrain: epoch  2, batch    19 | loss: 6.8671589Losses:  5.496354103088379 0.536591112613678 0.8680795431137085
CurrentTrain: epoch  2, batch    20 | loss: 6.9010248Losses:  5.240774154663086 0.4070497155189514 0.8826225399971008
CurrentTrain: epoch  2, batch    21 | loss: 6.5304465Losses:  5.040188312530518 0.4991440176963806 0.8853216171264648
CurrentTrain: epoch  2, batch    22 | loss: 6.4246540Losses:  5.64974308013916 0.4474904537200928 0.8940137624740601
CurrentTrain: epoch  2, batch    23 | loss: 6.9912477Losses:  5.638611793518066 0.6021258234977722 0.8403079509735107
CurrentTrain: epoch  2, batch    24 | loss: 7.0810452Losses:  5.211853981018066 0.4163641333580017 0.8587780594825745
CurrentTrain: epoch  2, batch    25 | loss: 6.4869962Losses:  5.048757076263428 0.38198322057724 0.8404232263565063
CurrentTrain: epoch  2, batch    26 | loss: 6.2711635Losses:  5.2833662033081055 0.40515631437301636 0.8591206073760986
CurrentTrain: epoch  2, batch    27 | loss: 6.5476427Losses:  4.992734432220459 0.4362780451774597 0.8214579820632935
CurrentTrain: epoch  2, batch    28 | loss: 6.2504702Losses:  5.335065841674805 0.5113528966903687 0.8729724884033203
CurrentTrain: epoch  2, batch    29 | loss: 6.7193913Losses:  5.616918087005615 0.6349397897720337 0.8575284481048584
CurrentTrain: epoch  2, batch    30 | loss: 7.1093864Losses:  5.1384053230285645 0.48548975586891174 0.8767699003219604
CurrentTrain: epoch  2, batch    31 | loss: 6.5006652Losses:  6.365908622741699 0.5295010805130005 0.8622710704803467
CurrentTrain: epoch  2, batch    32 | loss: 7.7576809Losses:  4.866303443908691 0.4942285418510437 0.8515177965164185
CurrentTrain: epoch  2, batch    33 | loss: 6.2120495Losses:  5.533883094787598 0.5558117628097534 0.8371175527572632
CurrentTrain: epoch  2, batch    34 | loss: 6.9268126Losses:  5.524378776550293 0.40712517499923706 0.8889796733856201
CurrentTrain: epoch  2, batch    35 | loss: 6.8204832Losses:  4.995429039001465 0.41485321521759033 0.8653365969657898
CurrentTrain: epoch  2, batch    36 | loss: 6.2756186Losses:  5.3488054275512695 0.40408214926719666 0.8235527276992798
CurrentTrain: epoch  2, batch    37 | loss: 6.5764403Losses:  5.262211799621582 0.3327365219593048 0.8765783309936523
CurrentTrain: epoch  2, batch    38 | loss: 6.4715266Losses:  4.69287633895874 0.3975312113761902 0.8386738300323486
CurrentTrain: epoch  2, batch    39 | loss: 5.9290810Losses:  5.5013885498046875 0.4490166902542114 0.835379958152771
CurrentTrain: epoch  2, batch    40 | loss: 6.7857852Losses:  5.875926971435547 0.40376630425453186 0.838584840297699
CurrentTrain: epoch  2, batch    41 | loss: 7.1182780Losses:  5.183026313781738 0.456356406211853 0.8271331787109375
CurrentTrain: epoch  2, batch    42 | loss: 6.4665160Losses:  5.297079086303711 0.427786260843277 0.844476044178009
CurrentTrain: epoch  2, batch    43 | loss: 6.5693417Losses:  5.133496284484863 0.4791198968887329 0.8432408571243286
CurrentTrain: epoch  2, batch    44 | loss: 6.4558568Losses:  5.459275245666504 0.37743115425109863 0.8390899896621704
CurrentTrain: epoch  2, batch    45 | loss: 6.6757960Losses:  4.879055976867676 0.32603389024734497 0.8806185126304626
CurrentTrain: epoch  2, batch    46 | loss: 6.0857086Losses:  5.871329307556152 0.5107053518295288 0.8748123049736023
CurrentTrain: epoch  2, batch    47 | loss: 7.2568469Losses:  4.7639946937561035 0.35733407735824585 0.8384943008422852
CurrentTrain: epoch  2, batch    48 | loss: 5.9598231Losses:  5.311062335968018 0.49792933464050293 0.8654264211654663
CurrentTrain: epoch  2, batch    49 | loss: 6.6744180Losses:  5.306423187255859 0.29698264598846436 0.873004674911499
CurrentTrain: epoch  2, batch    50 | loss: 6.4764109Losses:  4.823793888092041 0.3684355318546295 0.8489828109741211
CurrentTrain: epoch  2, batch    51 | loss: 6.0412121Losses:  5.182277679443359 0.3028513789176941 0.9124364852905273
CurrentTrain: epoch  2, batch    52 | loss: 6.3975654Losses:  5.137544631958008 0.42659640312194824 0.8705005645751953
CurrentTrain: epoch  2, batch    53 | loss: 6.4346418Losses:  4.90882682800293 0.4073799252510071 0.8580237030982971
CurrentTrain: epoch  2, batch    54 | loss: 6.1742306Losses:  4.982666015625 0.39813584089279175 0.8688884377479553
CurrentTrain: epoch  2, batch    55 | loss: 6.2496901Losses:  5.361403942108154 0.5826765298843384 0.8438199162483215
CurrentTrain: epoch  2, batch    56 | loss: 6.7879004Losses:  5.131444931030273 0.4430806040763855 0.9122876524925232
CurrentTrain: epoch  2, batch    57 | loss: 6.4868131Losses:  4.910375595092773 0.3593597412109375 0.8603605031967163
CurrentTrain: epoch  2, batch    58 | loss: 6.1300960Losses:  4.933643341064453 0.36031678318977356 0.8474754095077515
CurrentTrain: epoch  2, batch    59 | loss: 6.1414356Losses:  4.661974906921387 0.4511237144470215 0.8073421716690063
CurrentTrain: epoch  2, batch    60 | loss: 5.9204407Losses:  4.878481864929199 0.41478630900382996 0.8626020550727844
CurrentTrain: epoch  2, batch    61 | loss: 6.1558704Losses:  4.994693756103516 0.14305897057056427 0.8408831357955933
CurrentTrain: epoch  2, batch    62 | loss: 5.9786358Losses:  5.5738983154296875 0.3997618556022644 0.8709771633148193
CurrentTrain: epoch  3, batch     0 | loss: 6.8446369Losses:  6.001247406005859 0.46805694699287415 0.9036403298377991
CurrentTrain: epoch  3, batch     1 | loss: 7.3729448Losses:  4.8715057373046875 0.38513392210006714 0.8377140760421753
CurrentTrain: epoch  3, batch     2 | loss: 6.0943537Losses:  5.481085777282715 0.32053905725479126 0.865164041519165
CurrentTrain: epoch  3, batch     3 | loss: 6.6667891Losses:  5.209165096282959 0.3410264849662781 0.8146908283233643
CurrentTrain: epoch  3, batch     4 | loss: 6.3648825Losses:  5.218079566955566 0.2717858552932739 0.8149833083152771
CurrentTrain: epoch  3, batch     5 | loss: 6.3048487Losses:  4.872410774230957 0.3743087351322174 0.8878607749938965
CurrentTrain: epoch  3, batch     6 | loss: 6.1345801Losses:  4.977991580963135 0.3254370093345642 0.8705257177352905
CurrentTrain: epoch  3, batch     7 | loss: 6.1739545Losses:  5.445038795471191 0.437408447265625 0.8070476055145264
CurrentTrain: epoch  3, batch     8 | loss: 6.6894951Losses:  5.235391139984131 0.376520037651062 0.8251198530197144
CurrentTrain: epoch  3, batch     9 | loss: 6.4370313Losses:  5.018089294433594 0.44029700756073 0.8421430587768555
CurrentTrain: epoch  3, batch    10 | loss: 6.3005295Losses:  5.018609046936035 0.34728023409843445 0.8338568210601807
CurrentTrain: epoch  3, batch    11 | loss: 6.1997461Losses:  4.530571937561035 0.2558884024620056 0.8326034545898438
CurrentTrain: epoch  3, batch    12 | loss: 5.6190639Losses:  4.979612827301025 0.49329131841659546 0.8291121125221252
CurrentTrain: epoch  3, batch    13 | loss: 6.3020163Losses:  4.700002670288086 0.34977108240127563 0.8239429593086243
CurrentTrain: epoch  3, batch    14 | loss: 5.8737168Losses:  4.667171955108643 0.3654141128063202 0.8235263824462891
CurrentTrain: epoch  3, batch    15 | loss: 5.8561125Losses:  4.899362087249756 0.41230708360671997 0.8554917573928833
CurrentTrain: epoch  3, batch    16 | loss: 6.1671610Losses:  5.162967681884766 0.3301333785057068 0.8714877367019653
CurrentTrain: epoch  3, batch    17 | loss: 6.3645887Losses:  5.129693031311035 0.2945703864097595 0.8366067409515381
CurrentTrain: epoch  3, batch    18 | loss: 6.2608700Losses:  4.946010112762451 0.3679218292236328 0.8245301246643066
CurrentTrain: epoch  3, batch    19 | loss: 6.1384621Losses:  4.756961822509766 0.30462390184402466 0.8579297065734863
CurrentTrain: epoch  3, batch    20 | loss: 5.9195156Losses:  4.784798622131348 0.33973634243011475 0.860858678817749
CurrentTrain: epoch  3, batch    21 | loss: 5.9853935Losses:  4.843842506408691 0.42873382568359375 0.8987939953804016
CurrentTrain: epoch  3, batch    22 | loss: 6.1713705Losses:  4.659816741943359 0.3856236934661865 0.8228168487548828
CurrentTrain: epoch  3, batch    23 | loss: 5.8682575Losses:  4.700055122375488 0.22683829069137573 0.778297483921051
CurrentTrain: epoch  3, batch    24 | loss: 5.7051907Losses:  4.581075668334961 0.3422018885612488 0.8166937828063965
CurrentTrain: epoch  3, batch    25 | loss: 5.7399712Losses:  4.677035808563232 0.4029353857040405 0.8591283559799194
CurrentTrain: epoch  3, batch    26 | loss: 5.9390998Losses:  4.744809150695801 0.38873961567878723 0.8569197654724121
CurrentTrain: epoch  3, batch    27 | loss: 5.9904685Losses:  4.84056282043457 0.38897186517715454 0.847472071647644
CurrentTrain: epoch  3, batch    28 | loss: 6.0770068Losses:  5.144354820251465 0.35999059677124023 0.8420105576515198
CurrentTrain: epoch  3, batch    29 | loss: 6.3463559Losses:  5.4101762771606445 0.36796197295188904 0.8658158779144287
CurrentTrain: epoch  3, batch    30 | loss: 6.6439543Losses:  4.378979682922363 0.2648429870605469 0.8419697284698486
CurrentTrain: epoch  3, batch    31 | loss: 5.4857922Losses:  4.8060712814331055 0.2987295985221863 0.8197464942932129
CurrentTrain: epoch  3, batch    32 | loss: 5.9245472Losses:  4.764158248901367 0.29375314712524414 0.7819216251373291
CurrentTrain: epoch  3, batch    33 | loss: 5.8398333Losses:  4.5072526931762695 0.34682661294937134 0.7942322492599487
CurrentTrain: epoch  3, batch    34 | loss: 5.6483116Losses:  4.575688362121582 0.38162243366241455 0.815767765045166
CurrentTrain: epoch  3, batch    35 | loss: 5.7730784Losses:  5.466386795043945 0.5261772274971008 0.8393811583518982
CurrentTrain: epoch  3, batch    36 | loss: 6.8319454Losses:  4.665888786315918 0.243103489279747 0.8455731868743896
CurrentTrain: epoch  3, batch    37 | loss: 5.7545652Losses:  4.477690696716309 0.26234930753707886 0.8387621641159058
CurrentTrain: epoch  3, batch    38 | loss: 5.5788021Losses:  5.643886566162109 0.2878683805465698 0.9096856713294983
CurrentTrain: epoch  3, batch    39 | loss: 6.8414407Losses:  4.639769077301025 0.32826298475265503 0.8284900188446045
CurrentTrain: epoch  3, batch    40 | loss: 5.7965221Losses:  4.509584426879883 0.2531542181968689 0.7651324272155762
CurrentTrain: epoch  3, batch    41 | loss: 5.5278711Losses:  5.002979278564453 0.2860340476036072 0.8472166657447815
CurrentTrain: epoch  3, batch    42 | loss: 6.1362300Losses:  4.660050392150879 0.21046654880046844 0.7917347550392151
CurrentTrain: epoch  3, batch    43 | loss: 5.6622515Losses:  4.879314422607422 0.390455424785614 0.8440225124359131
CurrentTrain: epoch  3, batch    44 | loss: 6.1137924Losses:  5.109094619750977 0.25831279158592224 0.8552626371383667
CurrentTrain: epoch  3, batch    45 | loss: 6.2226701Losses:  4.412243843078613 0.2751898169517517 0.8587509393692017
CurrentTrain: epoch  3, batch    46 | loss: 5.5461845Losses:  4.603170871734619 0.272896409034729 0.9091684222221375
CurrentTrain: epoch  3, batch    47 | loss: 5.7852354Losses:  4.471151351928711 0.25271210074424744 0.8502759337425232
CurrentTrain: epoch  3, batch    48 | loss: 5.5741396Losses:  4.573123455047607 0.2752572298049927 0.827468752861023
CurrentTrain: epoch  3, batch    49 | loss: 5.6758494Losses:  4.5340471267700195 0.326448917388916 0.8504050374031067
CurrentTrain: epoch  3, batch    50 | loss: 5.7109013Losses:  4.7391357421875 0.23994946479797363 0.8846340179443359
CurrentTrain: epoch  3, batch    51 | loss: 5.8637190Losses:  4.478409767150879 0.33261847496032715 0.8327550292015076
CurrentTrain: epoch  3, batch    52 | loss: 5.6437836Losses:  5.144697666168213 0.3380928039550781 0.8299949169158936
CurrentTrain: epoch  3, batch    53 | loss: 6.3127851Losses:  4.520771026611328 0.31889382004737854 0.7934194803237915
CurrentTrain: epoch  3, batch    54 | loss: 5.6330843Losses:  4.669471740722656 0.33807313442230225 0.8555521965026855
CurrentTrain: epoch  3, batch    55 | loss: 5.8630972Losses:  4.38154411315918 0.31706997752189636 0.8200814723968506
CurrentTrain: epoch  3, batch    56 | loss: 5.5186958Losses:  4.823657512664795 0.3412068784236908 0.8601675033569336
CurrentTrain: epoch  3, batch    57 | loss: 6.0250320Losses:  4.745685577392578 0.22244198620319366 0.849320650100708
CurrentTrain: epoch  3, batch    58 | loss: 5.8174486Losses:  4.633810997009277 0.20721474289894104 0.8822163343429565
CurrentTrain: epoch  3, batch    59 | loss: 5.7232423Losses:  4.542225360870361 0.36580920219421387 0.8101511001586914
CurrentTrain: epoch  3, batch    60 | loss: 5.7181854Losses:  4.421500205993652 0.22647890448570251 0.8407914042472839
CurrentTrain: epoch  3, batch    61 | loss: 5.4887705Losses:  4.407645225524902 0.07714387774467468 0.7919065356254578
CurrentTrain: epoch  3, batch    62 | loss: 5.2766953Losses:  4.352834701538086 0.242520272731781 0.7889556884765625
CurrentTrain: epoch  4, batch     0 | loss: 5.3843107Losses:  4.43719482421875 0.259771466255188 0.8235775828361511
CurrentTrain: epoch  4, batch     1 | loss: 5.5205436Losses:  4.5196027755737305 0.21635524928569794 0.8207206130027771
CurrentTrain: epoch  4, batch     2 | loss: 5.5566788Losses:  4.332070350646973 0.20462608337402344 0.8155699968338013
CurrentTrain: epoch  4, batch     3 | loss: 5.3522663Losses:  4.4937591552734375 0.2954930067062378 0.8558335304260254
CurrentTrain: epoch  4, batch     4 | loss: 5.6450858Losses:  4.593442916870117 0.3098727762699127 0.8281276822090149
CurrentTrain: epoch  4, batch     5 | loss: 5.7314434Losses:  4.496644973754883 0.26851245760917664 0.7945533990859985
CurrentTrain: epoch  4, batch     6 | loss: 5.5597105Losses:  4.3709917068481445 0.22006678581237793 0.8194023966789246
CurrentTrain: epoch  4, batch     7 | loss: 5.4104609Losses:  4.366966247558594 0.26869913935661316 0.8382875919342041
CurrentTrain: epoch  4, batch     8 | loss: 5.4739532Losses:  4.432684898376465 0.25679612159729004 0.8421542644500732
CurrentTrain: epoch  4, batch     9 | loss: 5.5316353Losses:  4.4780378341674805 0.1887534260749817 0.8291207551956177
CurrentTrain: epoch  4, batch    10 | loss: 5.4959121Losses:  4.768014907836914 0.3589351773262024 0.852419376373291
CurrentTrain: epoch  4, batch    11 | loss: 5.9793696Losses:  4.413813591003418 0.2481844425201416 0.8791340589523315
CurrentTrain: epoch  4, batch    12 | loss: 5.5411320Losses:  4.542356491088867 0.3192039430141449 0.8003120422363281
CurrentTrain: epoch  4, batch    13 | loss: 5.6618724Losses:  4.2082366943359375 0.16877306997776031 0.768048882484436
CurrentTrain: epoch  4, batch    14 | loss: 5.1450586Losses:  4.38623046875 0.18971607089042664 0.7997084259986877
CurrentTrain: epoch  4, batch    15 | loss: 5.3756547Losses:  4.454197883605957 0.2811487019062042 0.8199010491371155
CurrentTrain: epoch  4, batch    16 | loss: 5.5552478Losses:  4.356288909912109 0.14899146556854248 0.8759872913360596
CurrentTrain: epoch  4, batch    17 | loss: 5.3812675Losses:  4.410023212432861 0.2777317762374878 0.809756338596344
CurrentTrain: epoch  4, batch    18 | loss: 5.4975114Losses:  4.4041643142700195 0.32172349095344543 0.8078353404998779
CurrentTrain: epoch  4, batch    19 | loss: 5.5337229Losses:  4.919157981872559 0.2963756322860718 0.8773293495178223
CurrentTrain: epoch  4, batch    20 | loss: 6.0928631Losses:  4.627716541290283 0.1992035061120987 0.868592381477356
CurrentTrain: epoch  4, batch    21 | loss: 5.6955123Losses:  4.239435195922852 0.2642250955104828 0.7695704698562622
CurrentTrain: epoch  4, batch    22 | loss: 5.2732306Losses:  4.218153953552246 0.09594421088695526 0.8486194610595703
CurrentTrain: epoch  4, batch    23 | loss: 5.1627178Losses:  4.577378273010254 0.22665585577487946 0.7922506332397461
CurrentTrain: epoch  4, batch    24 | loss: 5.5962849Losses:  4.388933181762695 0.24681341648101807 0.8308334946632385
CurrentTrain: epoch  4, batch    25 | loss: 5.4665799Losses:  4.287469863891602 0.24399635195732117 0.780318558216095
CurrentTrain: epoch  4, batch    26 | loss: 5.3117847Losses:  4.236234664916992 0.15716224908828735 0.8136821985244751
CurrentTrain: epoch  4, batch    27 | loss: 5.2070789Losses:  4.390750408172607 0.24762549996376038 0.776566743850708
CurrentTrain: epoch  4, batch    28 | loss: 5.4149427Losses:  4.352998733520508 0.17674973607063293 0.8045822978019714
CurrentTrain: epoch  4, batch    29 | loss: 5.3343306Losses:  4.24094295501709 0.14426898956298828 0.8184429407119751
CurrentTrain: epoch  4, batch    30 | loss: 5.2036548Losses:  4.422965049743652 0.22984088957309723 0.8125380277633667
CurrentTrain: epoch  4, batch    31 | loss: 5.4653440Losses:  4.745691299438477 0.3470801115036011 0.8022342920303345
CurrentTrain: epoch  4, batch    32 | loss: 5.8950057Losses:  4.387231826782227 0.3006061613559723 0.7795013189315796
CurrentTrain: epoch  4, batch    33 | loss: 5.4673395Losses:  4.31649112701416 0.1474313735961914 0.8671184182167053
CurrentTrain: epoch  4, batch    34 | loss: 5.3310409Losses:  4.669591903686523 0.2556891143321991 0.8386968374252319
CurrentTrain: epoch  4, batch    35 | loss: 5.7639780Losses:  4.381848335266113 0.26339051127433777 0.8248049020767212
CurrentTrain: epoch  4, batch    36 | loss: 5.4700437Losses:  4.613368988037109 0.289806067943573 0.8207619190216064
CurrentTrain: epoch  4, batch    37 | loss: 5.7239370Losses:  4.31949520111084 0.1814807802438736 0.8732364177703857
CurrentTrain: epoch  4, batch    38 | loss: 5.3742123Losses:  4.280129909515381 0.25558027625083923 0.8052998781204224
CurrentTrain: epoch  4, batch    39 | loss: 5.3410101Losses:  4.253633975982666 0.16637803614139557 0.8190001845359802
CurrentTrain: epoch  4, batch    40 | loss: 5.2390122Losses:  4.1807050704956055 0.251854807138443 0.8281489610671997
CurrentTrain: epoch  4, batch    41 | loss: 5.2607088Losses:  5.3714799880981445 0.33182549476623535 0.8655475974082947
CurrentTrain: epoch  4, batch    42 | loss: 6.5688529Losses:  4.73486328125 0.15222296118736267 0.7377989888191223
CurrentTrain: epoch  4, batch    43 | loss: 5.6248856Losses:  4.171114921569824 0.16408532857894897 0.851604163646698
CurrentTrain: epoch  4, batch    44 | loss: 5.1868043Losses:  4.647266387939453 0.2339903861284256 0.7994610667228699
CurrentTrain: epoch  4, batch    45 | loss: 5.6807175Losses:  5.052499771118164 0.20670336484909058 0.7931147217750549
CurrentTrain: epoch  4, batch    46 | loss: 6.0523176Losses:  4.862673759460449 0.2488526701927185 0.829190731048584
CurrentTrain: epoch  4, batch    47 | loss: 5.9407172Losses:  4.464254379272461 0.23029246926307678 0.7885705232620239
CurrentTrain: epoch  4, batch    48 | loss: 5.4831171Losses:  4.293916702270508 0.192037433385849 0.7719020247459412
CurrentTrain: epoch  4, batch    49 | loss: 5.2578564Losses:  4.715121269226074 0.30932527780532837 0.7916333675384521
CurrentTrain: epoch  4, batch    50 | loss: 5.8160801Losses:  4.304372310638428 0.14112229645252228 0.7906317710876465
CurrentTrain: epoch  4, batch    51 | loss: 5.2361264Losses:  4.227217197418213 0.24416711926460266 0.8079848289489746
CurrentTrain: epoch  4, batch    52 | loss: 5.2793694Losses:  4.3905463218688965 0.22971363365650177 0.7991694211959839
CurrentTrain: epoch  4, batch    53 | loss: 5.4194293Losses:  4.874329090118408 0.21403402090072632 0.8429813981056213
CurrentTrain: epoch  4, batch    54 | loss: 5.9313445Losses:  4.214066505432129 0.23099547624588013 0.7581819295883179
CurrentTrain: epoch  4, batch    55 | loss: 5.2032442Losses:  4.499641418457031 0.16713561117649078 0.8239222764968872
CurrentTrain: epoch  4, batch    56 | loss: 5.4906993Losses:  4.37370491027832 0.24381297826766968 0.8244289755821228
CurrentTrain: epoch  4, batch    57 | loss: 5.4419470Losses:  4.211359977722168 0.16335907578468323 0.8157216310501099
CurrentTrain: epoch  4, batch    58 | loss: 5.1904407Losses:  4.264313697814941 0.16350716352462769 0.8225630521774292
CurrentTrain: epoch  4, batch    59 | loss: 5.2503839Losses:  4.215046405792236 0.1985982358455658 0.8494758605957031
CurrentTrain: epoch  4, batch    60 | loss: 5.2631207Losses:  4.47537088394165 0.18993493914604187 0.8243969678878784
CurrentTrain: epoch  4, batch    61 | loss: 5.4897027Losses:  4.226320266723633 0.10211008042097092 0.8818924427032471
CurrentTrain: epoch  4, batch    62 | loss: 5.2103224Losses:  4.355146884918213 0.241064190864563 0.8332342505455017
CurrentTrain: epoch  5, batch     0 | loss: 5.4294453Losses:  4.290225028991699 0.24878723919391632 0.7599691152572632
CurrentTrain: epoch  5, batch     1 | loss: 5.2989817Losses:  4.238319396972656 0.1396818459033966 0.7802734375
CurrentTrain: epoch  5, batch     2 | loss: 5.1582747Losses:  4.1902055740356445 0.16645586490631104 0.8308429718017578
CurrentTrain: epoch  5, batch     3 | loss: 5.1875043Losses:  4.323205471038818 0.16124796867370605 0.8426775336265564
CurrentTrain: epoch  5, batch     4 | loss: 5.3271308Losses:  4.495846748352051 0.21320593357086182 0.798865795135498
CurrentTrain: epoch  5, batch     5 | loss: 5.5079184Losses:  4.329927921295166 0.2006441354751587 0.8278514742851257
CurrentTrain: epoch  5, batch     6 | loss: 5.3584232Losses:  4.1988701820373535 0.2206074595451355 0.7775408029556274
CurrentTrain: epoch  5, batch     7 | loss: 5.1970181Losses:  4.284505844116211 0.20398680865764618 0.858989417552948
CurrentTrain: epoch  5, batch     8 | loss: 5.3474817Losses:  4.210289001464844 0.1701466143131256 0.7385455965995789
CurrentTrain: epoch  5, batch     9 | loss: 5.1189809Losses:  4.562540531158447 0.32079872488975525 0.7821559906005859
CurrentTrain: epoch  5, batch    10 | loss: 5.6654954Losses:  4.229391574859619 0.19708864390850067 0.8349586725234985
CurrentTrain: epoch  5, batch    11 | loss: 5.2614388Losses:  4.322061538696289 0.21297892928123474 0.7475005984306335
CurrentTrain: epoch  5, batch    12 | loss: 5.2825408Losses:  4.200984001159668 0.18319155275821686 0.8223475813865662
CurrentTrain: epoch  5, batch    13 | loss: 5.2065234Losses:  4.130638122558594 0.2321048229932785 0.8049410581588745
CurrentTrain: epoch  5, batch    14 | loss: 5.1676841Losses:  4.266518592834473 0.1781885325908661 0.8605930805206299
CurrentTrain: epoch  5, batch    15 | loss: 5.3052998Losses:  4.276655197143555 0.22938479483127594 0.8229705095291138
CurrentTrain: epoch  5, batch    16 | loss: 5.3290105Losses:  4.150509357452393 0.2187121957540512 0.8026350736618042
CurrentTrain: epoch  5, batch    17 | loss: 5.1718569Losses:  4.30290412902832 0.25199374556541443 0.8720768690109253
CurrentTrain: epoch  5, batch    18 | loss: 5.4269748Losses:  4.229108810424805 0.14352072775363922 0.773933470249176
CurrentTrain: epoch  5, batch    19 | loss: 5.1465631Losses:  4.140531539916992 0.1677175611257553 0.8039790987968445
CurrentTrain: epoch  5, batch    20 | loss: 5.1122279Losses:  4.394382476806641 0.19989679753780365 0.8370659947395325
CurrentTrain: epoch  5, batch    21 | loss: 5.4313455Losses:  4.234096527099609 0.1916443258523941 0.7870746850967407
CurrentTrain: epoch  5, batch    22 | loss: 5.2128153Losses:  4.254490852355957 0.19506074488162994 0.8400354385375977
CurrentTrain: epoch  5, batch    23 | loss: 5.2895870Losses:  4.183266639709473 0.11873134225606918 0.7607541084289551
CurrentTrain: epoch  5, batch    24 | loss: 5.0627522Losses:  4.306110382080078 0.22084644436836243 0.8538475632667542
CurrentTrain: epoch  5, batch    25 | loss: 5.3808045Losses:  4.243432998657227 0.20600396394729614 0.783747673034668
CurrentTrain: epoch  5, batch    26 | loss: 5.2331848Losses:  4.175512313842773 0.19693195819854736 0.8613630533218384
CurrentTrain: epoch  5, batch    27 | loss: 5.2338071Losses:  4.198032855987549 0.20531879365444183 0.8192138075828552
CurrentTrain: epoch  5, batch    28 | loss: 5.2225657Losses:  4.155251979827881 0.21923118829727173 0.7899670600891113
CurrentTrain: epoch  5, batch    29 | loss: 5.1644502Losses:  4.145894527435303 0.12158247828483582 0.7893792390823364
CurrentTrain: epoch  5, batch    30 | loss: 5.0568562Losses:  4.178422927856445 0.1569618284702301 0.7692276239395142
CurrentTrain: epoch  5, batch    31 | loss: 5.1046124Losses:  4.258580207824707 0.13824068009853363 0.8024479746818542
CurrentTrain: epoch  5, batch    32 | loss: 5.1992688Losses:  4.1112213134765625 0.20351466536521912 0.7505733966827393
CurrentTrain: epoch  5, batch    33 | loss: 5.0653095Losses:  4.2773356437683105 0.19923806190490723 0.771653413772583
CurrentTrain: epoch  5, batch    34 | loss: 5.2482271Losses:  4.365024566650391 0.18031345307826996 0.8138436079025269
CurrentTrain: epoch  5, batch    35 | loss: 5.3591819Losses:  4.200142860412598 0.17060986161231995 0.7625129222869873
CurrentTrain: epoch  5, batch    36 | loss: 5.1332655Losses:  4.123246192932129 0.19866420328617096 0.7377623319625854
CurrentTrain: epoch  5, batch    37 | loss: 5.0596728Losses:  4.162782669067383 0.17519156634807587 0.8051628470420837
CurrentTrain: epoch  5, batch    38 | loss: 5.1431370Losses:  4.260826110839844 0.20024347305297852 0.7535687685012817
CurrentTrain: epoch  5, batch    39 | loss: 5.2146382Losses:  4.115417957305908 0.20237162709236145 0.7882629036903381
CurrentTrain: epoch  5, batch    40 | loss: 5.1060524Losses:  4.182724475860596 0.17371076345443726 0.7693024277687073
CurrentTrain: epoch  5, batch    41 | loss: 5.1257377Losses:  4.302357196807861 0.27635499835014343 0.833721399307251
CurrentTrain: epoch  5, batch    42 | loss: 5.4124336Losses:  4.185531139373779 0.1893194615840912 0.7813893556594849
CurrentTrain: epoch  5, batch    43 | loss: 5.1562400Losses:  4.146923065185547 0.21361587941646576 0.7850639224052429
CurrentTrain: epoch  5, batch    44 | loss: 5.1456027Losses:  4.104251384735107 0.2430969476699829 0.7888914346694946
CurrentTrain: epoch  5, batch    45 | loss: 5.1362395Losses:  4.177392959594727 0.2230047732591629 0.7822128534317017
CurrentTrain: epoch  5, batch    46 | loss: 5.1826105Losses:  4.136463642120361 0.22439593076705933 0.7879946827888489
CurrentTrain: epoch  5, batch    47 | loss: 5.1488543Losses:  4.097259044647217 0.22744745016098022 0.8245402574539185
CurrentTrain: epoch  5, batch    48 | loss: 5.1492467Losses:  4.163117408752441 0.20238134264945984 0.8185224533081055
CurrentTrain: epoch  5, batch    49 | loss: 5.1840210Losses:  4.128265857696533 0.17531564831733704 0.7987505197525024
CurrentTrain: epoch  5, batch    50 | loss: 5.1023321Losses:  4.145550727844238 0.18530213832855225 0.8881775736808777
CurrentTrain: epoch  5, batch    51 | loss: 5.2190304Losses:  4.215655326843262 0.25336119532585144 0.7567225694656372
CurrentTrain: epoch  5, batch    52 | loss: 5.2257390Losses:  4.174163818359375 0.1842089295387268 0.8184598088264465
CurrentTrain: epoch  5, batch    53 | loss: 5.1768327Losses:  4.085680961608887 0.22886842489242554 0.8068763613700867
CurrentTrain: epoch  5, batch    54 | loss: 5.1214256Losses:  4.213974952697754 0.22316163778305054 0.7483570575714111
CurrentTrain: epoch  5, batch    55 | loss: 5.1854935Losses:  4.1453776359558105 0.2024461328983307 0.7646124958992004
CurrentTrain: epoch  5, batch    56 | loss: 5.1124363Losses:  4.209019184112549 0.15822435915470123 0.7585482597351074
CurrentTrain: epoch  5, batch    57 | loss: 5.1257920Losses:  4.077665328979492 0.15616992115974426 0.7664661407470703
CurrentTrain: epoch  5, batch    58 | loss: 5.0003014Losses:  4.180970668792725 0.1862824559211731 0.7572079300880432
CurrentTrain: epoch  5, batch    59 | loss: 5.1244612Losses:  4.107763290405273 0.14077872037887573 0.7610849142074585
CurrentTrain: epoch  5, batch    60 | loss: 5.0096269Losses:  4.1318488121032715 0.15481194853782654 0.8257220983505249
CurrentTrain: epoch  5, batch    61 | loss: 5.1123829Losses:  4.21257209777832 0.10519939661026001 0.8802984952926636
CurrentTrain: epoch  5, batch    62 | loss: 5.1980700Losses:  4.14902925491333 0.18382799625396729 0.7665100693702698
CurrentTrain: epoch  6, batch     0 | loss: 5.0993671Losses:  4.122122764587402 0.15144656598567963 0.863060712814331
CurrentTrain: epoch  6, batch     1 | loss: 5.1366301Losses:  4.06820821762085 0.14204257726669312 0.757805347442627
CurrentTrain: epoch  6, batch     2 | loss: 4.9680562Losses:  4.236193656921387 0.1997866928577423 0.7922934293746948
CurrentTrain: epoch  6, batch     3 | loss: 5.2282739Losses:  4.143843650817871 0.13960136473178864 0.7450839281082153
CurrentTrain: epoch  6, batch     4 | loss: 5.0285287Losses:  4.12945556640625 0.13328155875205994 0.8223865032196045
CurrentTrain: epoch  6, batch     5 | loss: 5.0851240Losses:  4.1288676261901855 0.18405191600322723 0.7890591621398926
CurrentTrain: epoch  6, batch     6 | loss: 5.1019788Losses:  4.153323173522949 0.1437092125415802 0.8687371015548706
CurrentTrain: epoch  6, batch     7 | loss: 5.1657696Losses:  4.091208457946777 0.19056421518325806 0.7729278802871704
CurrentTrain: epoch  6, batch     8 | loss: 5.0547004Losses:  4.142632961273193 0.13442528247833252 0.84810471534729
CurrentTrain: epoch  6, batch     9 | loss: 5.1251631Losses:  4.1320085525512695 0.17306241393089294 0.7783204317092896
CurrentTrain: epoch  6, batch    10 | loss: 5.0833912Losses:  4.189645767211914 0.12368184328079224 0.8823281526565552
CurrentTrain: epoch  6, batch    11 | loss: 5.1956558Losses:  4.172478675842285 0.18853142857551575 0.7666124105453491
CurrentTrain: epoch  6, batch    12 | loss: 5.1276226Losses:  4.119759559631348 0.10777260363101959 0.7720174193382263
CurrentTrain: epoch  6, batch    13 | loss: 4.9995499Losses:  4.118137836456299 0.18433576822280884 0.7887207269668579
CurrentTrain: epoch  6, batch    14 | loss: 5.0911942Losses:  4.150385856628418 0.17929531633853912 0.8424457907676697
CurrentTrain: epoch  6, batch    15 | loss: 5.1721272Losses:  4.10270881652832 0.17341944575309753 0.7483774423599243
CurrentTrain: epoch  6, batch    16 | loss: 5.0245056Losses:  4.0988664627075195 0.17035958170890808 0.8081141710281372
CurrentTrain: epoch  6, batch    17 | loss: 5.0773401Losses:  4.141997337341309 0.07575326412916183 0.8462051749229431
CurrentTrain: epoch  6, batch    18 | loss: 5.0639558Losses:  4.0942535400390625 0.18262159824371338 0.7759045362472534
CurrentTrain: epoch  6, batch    19 | loss: 5.0527797Losses:  4.073970317840576 0.20296819508075714 0.7552720904350281
CurrentTrain: epoch  6, batch    20 | loss: 5.0322104Losses:  4.187238693237305 0.15030242502689362 0.7977380156517029
CurrentTrain: epoch  6, batch    21 | loss: 5.1352792Losses:  4.155451774597168 0.15827569365501404 0.8054418563842773
CurrentTrain: epoch  6, batch    22 | loss: 5.1191692Losses:  4.302289009094238 0.142327681183815 0.7683610916137695
CurrentTrain: epoch  6, batch    23 | loss: 5.2129779Losses:  4.15093994140625 0.1463387906551361 0.7663450837135315
CurrentTrain: epoch  6, batch    24 | loss: 5.0636239Losses:  4.152846336364746 0.1494549661874771 0.8176207542419434
CurrentTrain: epoch  6, batch    25 | loss: 5.1199222Losses:  4.1449174880981445 0.17351195216178894 0.7985515594482422
CurrentTrain: epoch  6, batch    26 | loss: 5.1169810Losses:  4.109552383422852 0.1450786292552948 0.77155601978302
CurrentTrain: epoch  6, batch    27 | loss: 5.0261869Losses:  4.0670366287231445 0.1685646027326584 0.7378954887390137
CurrentTrain: epoch  6, batch    28 | loss: 4.9734969Losses:  4.100199222564697 0.16536375880241394 0.7463898658752441
CurrentTrain: epoch  6, batch    29 | loss: 5.0119529Losses:  4.100749969482422 0.16832539439201355 0.7884353995323181
CurrentTrain: epoch  6, batch    30 | loss: 5.0575109Losses:  4.132248878479004 0.11750081181526184 0.8315384387969971
CurrentTrain: epoch  6, batch    31 | loss: 5.0812883Losses:  4.138710975646973 0.11701080203056335 0.744895339012146
CurrentTrain: epoch  6, batch    32 | loss: 5.0006170Losses:  4.100708961486816 0.19800922274589539 0.770599901676178
CurrentTrain: epoch  6, batch    33 | loss: 5.0693178Losses:  4.115103721618652 0.19027608633041382 0.7045626044273376
CurrentTrain: epoch  6, batch    34 | loss: 5.0099425Losses:  4.0607194900512695 0.10146855562925339 0.7180503606796265
CurrentTrain: epoch  6, batch    35 | loss: 4.8802385Losses:  4.172183036804199 0.09381706267595291 0.7496095895767212
CurrentTrain: epoch  6, batch    36 | loss: 5.0156097Losses:  4.119350433349609 0.15202008187770844 0.8134914636611938
CurrentTrain: epoch  6, batch    37 | loss: 5.0848618Losses:  4.106785774230957 0.1907036155462265 0.7952998876571655
CurrentTrain: epoch  6, batch    38 | loss: 5.0927892Losses:  4.067464351654053 0.11467839777469635 0.824692964553833
CurrentTrain: epoch  6, batch    39 | loss: 5.0068359Losses:  4.137743949890137 0.16063641011714935 0.775704562664032
CurrentTrain: epoch  6, batch    40 | loss: 5.0740848Losses:  4.092439651489258 0.17968228459358215 0.7883650660514832
CurrentTrain: epoch  6, batch    41 | loss: 5.0604868Losses:  4.065699577331543 0.16856792569160461 0.7691324949264526
CurrentTrain: epoch  6, batch    42 | loss: 5.0034003Losses:  4.0345659255981445 0.15207578241825104 0.7640009522438049
CurrentTrain: epoch  6, batch    43 | loss: 4.9506426Losses:  4.062272071838379 0.1493345946073532 0.8467308878898621
CurrentTrain: epoch  6, batch    44 | loss: 5.0583372Losses:  4.088586330413818 0.13232506811618805 0.7797518968582153
CurrentTrain: epoch  6, batch    45 | loss: 5.0006633Losses:  4.087489128112793 0.1616753786802292 0.7177577018737793
CurrentTrain: epoch  6, batch    46 | loss: 4.9669223Losses:  4.070306301116943 0.12262818217277527 0.8438877463340759
CurrentTrain: epoch  6, batch    47 | loss: 5.0368223Losses:  4.061168670654297 0.17327280342578888 0.7937332391738892
CurrentTrain: epoch  6, batch    48 | loss: 5.0281744Losses:  4.079233169555664 0.08972415328025818 0.7675747871398926
CurrentTrain: epoch  6, batch    49 | loss: 4.9365320Losses:  4.112692832946777 0.13425201177597046 0.7888575792312622
CurrentTrain: epoch  6, batch    50 | loss: 5.0358024Losses:  4.037374019622803 0.13618652522563934 0.8178744316101074
CurrentTrain: epoch  6, batch    51 | loss: 4.9914351Losses:  4.090813636779785 0.15192890167236328 0.814678430557251
CurrentTrain: epoch  6, batch    52 | loss: 5.0574207Losses:  4.097164154052734 0.06727708876132965 0.7434332370758057
CurrentTrain: epoch  6, batch    53 | loss: 4.9078741Losses:  4.069553375244141 0.13858087360858917 0.723953366279602
CurrentTrain: epoch  6, batch    54 | loss: 4.9320874Losses:  4.045079231262207 0.14690977334976196 0.761461615562439
CurrentTrain: epoch  6, batch    55 | loss: 4.9534507Losses:  4.11637020111084 0.09307973086833954 0.6894237399101257
CurrentTrain: epoch  6, batch    56 | loss: 4.8988733Losses:  4.044322490692139 0.1256571114063263 0.7473626732826233
CurrentTrain: epoch  6, batch    57 | loss: 4.9173422Losses:  4.075359344482422 0.1362224668264389 0.8028146028518677
CurrentTrain: epoch  6, batch    58 | loss: 5.0143962Losses:  4.065973281860352 0.15809974074363708 0.7118923664093018
CurrentTrain: epoch  6, batch    59 | loss: 4.9359655Losses:  4.139255046844482 0.11160075664520264 0.7744624614715576
CurrentTrain: epoch  6, batch    60 | loss: 5.0253181Losses:  4.074211120605469 0.12493389844894409 0.7049481868743896
CurrentTrain: epoch  6, batch    61 | loss: 4.9040928Losses:  4.014455795288086 0.16336235404014587 0.705069899559021
CurrentTrain: epoch  6, batch    62 | loss: 4.8828883Losses:  4.0895843505859375 0.09508650004863739 0.7531781196594238
CurrentTrain: epoch  7, batch     0 | loss: 4.9378490Losses:  4.086972236633301 0.11480223387479782 0.7530907392501831
CurrentTrain: epoch  7, batch     1 | loss: 4.9548655Losses:  4.069581985473633 0.1198996752500534 0.7321081161499023
CurrentTrain: epoch  7, batch     2 | loss: 4.9215899Losses:  4.060246467590332 0.13392940163612366 0.7703802585601807
CurrentTrain: epoch  7, batch     3 | loss: 4.9645557Losses:  4.070939064025879 0.11175091564655304 0.7655074000358582
CurrentTrain: epoch  7, batch     4 | loss: 4.9481974Losses:  4.046885013580322 0.10671339184045792 0.8074681758880615
CurrentTrain: epoch  7, batch     5 | loss: 4.9610662Losses:  4.042639255523682 0.12557446956634521 0.795251727104187
CurrentTrain: epoch  7, batch     6 | loss: 4.9634657Losses:  4.122082710266113 0.1665571928024292 0.8126557469367981
CurrentTrain: epoch  7, batch     7 | loss: 5.1012959Losses:  3.996558427810669 0.09375593066215515 0.8194071054458618
CurrentTrain: epoch  7, batch     8 | loss: 4.9097214Losses:  4.022148132324219 0.15063956379890442 0.6901808977127075
CurrentTrain: epoch  7, batch     9 | loss: 4.8629684Losses:  4.029494285583496 0.17337137460708618 0.7529155015945435
CurrentTrain: epoch  7, batch    10 | loss: 4.9557810Losses:  4.004159927368164 0.11581924557685852 0.7525951862335205
CurrentTrain: epoch  7, batch    11 | loss: 4.8725748Losses:  4.016751766204834 0.12290742993354797 0.7116752862930298
CurrentTrain: epoch  7, batch    12 | loss: 4.8513346Losses:  4.055041790008545 0.10171730816364288 0.7185559272766113
CurrentTrain: epoch  7, batch    13 | loss: 4.8753152Losses:  4.014760971069336 0.14054003357887268 0.7549797892570496
CurrentTrain: epoch  7, batch    14 | loss: 4.9102807Losses:  4.004974365234375 0.17009131610393524 0.7553372383117676
CurrentTrain: epoch  7, batch    15 | loss: 4.9304028Losses:  4.083553314208984 0.181878924369812 0.7284661531448364
CurrentTrain: epoch  7, batch    16 | loss: 4.9938984Losses:  4.108997344970703 0.20189276337623596 0.6962660551071167
CurrentTrain: epoch  7, batch    17 | loss: 5.0071564Losses:  4.056674957275391 0.12121865898370743 0.7844198942184448
CurrentTrain: epoch  7, batch    18 | loss: 4.9623137Losses:  4.1071367263793945 0.1319272369146347 0.813117265701294
CurrentTrain: epoch  7, batch    19 | loss: 5.0521812Losses:  4.076234817504883 0.0896415263414383 0.7551380395889282
CurrentTrain: epoch  7, batch    20 | loss: 4.9210143Losses:  4.009055137634277 0.1218559592962265 0.7825585007667542
CurrentTrain: epoch  7, batch    21 | loss: 4.9134693Losses:  4.048422813415527 0.11696796119213104 0.8312835097312927
CurrentTrain: epoch  7, batch    22 | loss: 4.9966745Losses:  4.041604995727539 0.10368901491165161 0.7064632773399353
CurrentTrain: epoch  7, batch    23 | loss: 4.8517575Losses:  4.025964260101318 0.1507597118616104 0.7152725458145142
CurrentTrain: epoch  7, batch    24 | loss: 4.8919964Losses:  4.15710973739624 0.13312257826328278 0.781940221786499
CurrentTrain: epoch  7, batch    25 | loss: 5.0721722Losses:  4.014760494232178 0.09181327372789383 0.7477717995643616
CurrentTrain: epoch  7, batch    26 | loss: 4.8543453Losses:  4.044772624969482 0.12016569077968597 0.7665343284606934
CurrentTrain: epoch  7, batch    27 | loss: 4.9314728Losses:  4.049821853637695 0.13506606221199036 0.6964775323867798
CurrentTrain: epoch  7, batch    28 | loss: 4.8813653Losses:  4.059517860412598 0.09172713756561279 0.7955144643783569
CurrentTrain: epoch  7, batch    29 | loss: 4.9467597Losses:  4.0780029296875 0.11246803402900696 0.8139859437942505
CurrentTrain: epoch  7, batch    30 | loss: 5.0044570Losses:  4.071563720703125 0.12177644670009613 0.7358353734016418
CurrentTrain: epoch  7, batch    31 | loss: 4.9291759Losses:  4.065328598022461 0.0807214081287384 0.8011445999145508
CurrentTrain: epoch  7, batch    32 | loss: 4.9471946Losses:  4.1422319412231445 0.12311029434204102 0.7999848127365112
CurrentTrain: epoch  7, batch    33 | loss: 5.0653272Losses:  4.0971903800964355 0.09685364365577698 0.8821146488189697
CurrentTrain: epoch  7, batch    34 | loss: 5.0761585Losses:  4.045672416687012 0.12624770402908325 0.8465418219566345
CurrentTrain: epoch  7, batch    35 | loss: 5.0184622Losses:  4.115550994873047 0.12029841542243958 0.8252941370010376
CurrentTrain: epoch  7, batch    36 | loss: 5.0611434Losses:  4.011693954467773 0.09667430073022842 0.7146625518798828
CurrentTrain: epoch  7, batch    37 | loss: 4.8230309Losses:  4.026717185974121 0.12249325215816498 0.7526670694351196
CurrentTrain: epoch  7, batch    38 | loss: 4.9018774Losses:  4.011569023132324 0.09323492646217346 0.7580595016479492
CurrentTrain: epoch  7, batch    39 | loss: 4.8628635Losses:  4.038608551025391 0.15861213207244873 0.7889825105667114
CurrentTrain: epoch  7, batch    40 | loss: 4.9862032Losses:  4.020382404327393 0.10698437690734863 0.7415552139282227
CurrentTrain: epoch  7, batch    41 | loss: 4.8689222Losses:  4.02039098739624 0.10940702259540558 0.7596896886825562
CurrentTrain: epoch  7, batch    42 | loss: 4.8894877Losses:  4.073086738586426 0.12209554761648178 0.8177231550216675
CurrentTrain: epoch  7, batch    43 | loss: 5.0129056Losses:  4.033107280731201 0.11826349049806595 0.8070588111877441
CurrentTrain: epoch  7, batch    44 | loss: 4.9584298Losses:  3.977450370788574 0.1024991124868393 0.727225661277771
CurrentTrain: epoch  7, batch    45 | loss: 4.8071752Losses:  4.039220333099365 0.10800858587026596 0.790910542011261
CurrentTrain: epoch  7, batch    46 | loss: 4.9381394Losses:  4.031800270080566 0.12288656830787659 0.7802126407623291
CurrentTrain: epoch  7, batch    47 | loss: 4.9348993Losses:  4.023866653442383 0.16383147239685059 0.743310272693634
CurrentTrain: epoch  7, batch    48 | loss: 4.9310088Losses:  3.9859628677368164 0.17043042182922363 0.7768969535827637
CurrentTrain: epoch  7, batch    49 | loss: 4.9332900Losses:  3.9662365913391113 0.1015671119093895 0.7696559429168701
CurrentTrain: epoch  7, batch    50 | loss: 4.8374596Losses:  4.018863677978516 0.10604815185070038 0.7025334239006042
CurrentTrain: epoch  7, batch    51 | loss: 4.8274450Losses:  4.076423645019531 0.15841397643089294 0.6809133887290955
CurrentTrain: epoch  7, batch    52 | loss: 4.9157510Losses:  4.074515342712402 0.12593382596969604 0.7622488141059875
CurrentTrain: epoch  7, batch    53 | loss: 4.9626980Losses:  4.070426940917969 0.11161015927791595 0.8037805557250977
CurrentTrain: epoch  7, batch    54 | loss: 4.9858174Losses:  4.031877517700195 0.11516319215297699 0.7442539930343628
CurrentTrain: epoch  7, batch    55 | loss: 4.8912950Losses:  4.047109127044678 0.09900008887052536 0.8462872505187988
CurrentTrain: epoch  7, batch    56 | loss: 4.9923964Losses:  4.019686698913574 0.12687194347381592 0.8108499050140381
CurrentTrain: epoch  7, batch    57 | loss: 4.9574089Losses:  4.0039544105529785 0.1526666134595871 0.74386066198349
CurrentTrain: epoch  7, batch    58 | loss: 4.9004817Losses:  4.051020622253418 0.0824553444981575 0.7966095209121704
CurrentTrain: epoch  7, batch    59 | loss: 4.9300852Losses:  4.047892093658447 0.1365210860967636 0.7892210483551025
CurrentTrain: epoch  7, batch    60 | loss: 4.9736338Losses:  4.040462493896484 0.08475116640329361 0.6945732831954956
CurrentTrain: epoch  7, batch    61 | loss: 4.8197870Losses:  4.0638275146484375 0.06471672654151917 0.7310044765472412
CurrentTrain: epoch  7, batch    62 | loss: 4.8595486Losses:  4.028290748596191 0.1419753134250641 0.7732744216918945
CurrentTrain: epoch  8, batch     0 | loss: 4.9435406Losses:  4.063157081604004 0.12574562430381775 0.7593421936035156
CurrentTrain: epoch  8, batch     1 | loss: 4.9482450Losses:  4.03217887878418 0.15538489818572998 0.7667916417121887
CurrentTrain: epoch  8, batch     2 | loss: 4.9543557Losses:  4.049165725708008 0.07038742303848267 0.6974303722381592
CurrentTrain: epoch  8, batch     3 | loss: 4.8169832Losses:  3.9798853397369385 0.0883127748966217 0.7074097394943237
CurrentTrain: epoch  8, batch     4 | loss: 4.7756081Losses:  4.039473533630371 0.12245015054941177 0.76714688539505
CurrentTrain: epoch  8, batch     5 | loss: 4.9290709Losses:  4.021939754486084 0.10098776966333389 0.7284570932388306
CurrentTrain: epoch  8, batch     6 | loss: 4.8513846Losses:  4.053310394287109 0.102267324924469 0.7603033781051636
CurrentTrain: epoch  8, batch     7 | loss: 4.9158812Losses:  4.024432182312012 0.11588513106107712 0.8125717639923096
CurrentTrain: epoch  8, batch     8 | loss: 4.9528894Losses:  4.0472869873046875 0.1154504343867302 0.6997978091239929
CurrentTrain: epoch  8, batch     9 | loss: 4.8625350Losses:  3.994896411895752 0.09911123663187027 0.822677493095398
CurrentTrain: epoch  8, batch    10 | loss: 4.9166851Losses:  4.018073081970215 0.11799611151218414 0.7876273989677429
CurrentTrain: epoch  8, batch    11 | loss: 4.9236965Losses:  4.070291519165039 0.11151112616062164 0.829997718334198
CurrentTrain: epoch  8, batch    12 | loss: 5.0118003Losses:  4.03629207611084 0.09602148830890656 0.7971603870391846
CurrentTrain: epoch  8, batch    13 | loss: 4.9294739Losses:  3.984434127807617 0.1337340623140335 0.7337443232536316
CurrentTrain: epoch  8, batch    14 | loss: 4.8519125Losses:  4.011758804321289 0.11624988168478012 0.7730971574783325
CurrentTrain: epoch  8, batch    15 | loss: 4.9011059Losses:  3.9927234649658203 0.0735648050904274 0.737946629524231
CurrentTrain: epoch  8, batch    16 | loss: 4.8042350Losses:  4.025553226470947 0.11646813154220581 0.7869325876235962
CurrentTrain: epoch  8, batch    17 | loss: 4.9289536Losses:  4.037759780883789 0.11155940592288971 0.7756087779998779
CurrentTrain: epoch  8, batch    18 | loss: 4.9249277Losses:  4.034620761871338 0.10020491480827332 0.7904132604598999
CurrentTrain: epoch  8, batch    19 | loss: 4.9252391Losses:  3.9457621574401855 0.11570876836776733 0.7751097679138184
CurrentTrain: epoch  8, batch    20 | loss: 4.8365808Losses:  4.011777877807617 0.10436874628067017 0.7786798477172852
CurrentTrain: epoch  8, batch    21 | loss: 4.8948264Losses:  3.995176315307617 0.11057275533676147 0.7427513599395752
CurrentTrain: epoch  8, batch    22 | loss: 4.8485003Losses:  4.026438236236572 0.13671588897705078 0.7767098546028137
CurrentTrain: epoch  8, batch    23 | loss: 4.9398642Losses:  4.032657623291016 0.14406123757362366 0.7646640539169312
CurrentTrain: epoch  8, batch    24 | loss: 4.9413829Losses:  4.007312774658203 0.07689763605594635 0.7678127884864807
CurrentTrain: epoch  8, batch    25 | loss: 4.8520231Losses:  4.021190643310547 0.10307949781417847 0.6551506519317627
CurrentTrain: epoch  8, batch    26 | loss: 4.7794209Losses:  4.06539249420166 0.12304647266864777 0.7926241159439087
CurrentTrain: epoch  8, batch    27 | loss: 4.9810629Losses:  3.9994351863861084 0.14395102858543396 0.7397756576538086
CurrentTrain: epoch  8, batch    28 | loss: 4.8831620Losses:  4.022125720977783 0.11146556586027145 0.7817494869232178
CurrentTrain: epoch  8, batch    29 | loss: 4.9153404Losses:  4.008155822753906 0.11098181456327438 0.7410697937011719
CurrentTrain: epoch  8, batch    30 | loss: 4.8602076Losses:  4.0110344886779785 0.11002839356660843 0.7620278596878052
CurrentTrain: epoch  8, batch    31 | loss: 4.8830905Losses:  4.037592887878418 0.10194104164838791 0.7352718114852905
CurrentTrain: epoch  8, batch    32 | loss: 4.8748059Losses:  4.026573181152344 0.10189174115657806 0.760189950466156
CurrentTrain: epoch  8, batch    33 | loss: 4.8886547Losses:  4.016016006469727 0.1290561556816101 0.731625497341156
CurrentTrain: epoch  8, batch    34 | loss: 4.8766975Losses:  4.006902694702148 0.11146098375320435 0.6783703565597534
CurrentTrain: epoch  8, batch    35 | loss: 4.7967343Losses:  4.0272626876831055 0.09057515114545822 0.8496702909469604
CurrentTrain: epoch  8, batch    36 | loss: 4.9675083Losses:  4.051605224609375 0.10509653389453888 0.6931499242782593
CurrentTrain: epoch  8, batch    37 | loss: 4.8498516Losses:  3.9652557373046875 0.1092028021812439 0.6954712867736816
CurrentTrain: epoch  8, batch    38 | loss: 4.7699299Losses:  4.049154758453369 0.11880437284708023 0.731536328792572
CurrentTrain: epoch  8, batch    39 | loss: 4.8994956Losses:  4.0489912033081055 0.12935498356819153 0.7327444553375244
CurrentTrain: epoch  8, batch    40 | loss: 4.9110909Losses:  4.021000862121582 0.10589106380939484 0.7729855179786682
CurrentTrain: epoch  8, batch    41 | loss: 4.8998775Losses:  4.024412155151367 0.11498598754405975 0.7807155847549438
CurrentTrain: epoch  8, batch    42 | loss: 4.9201136Losses:  4.041131496429443 0.13641008734703064 0.7718814611434937
CurrentTrain: epoch  8, batch    43 | loss: 4.9494233Losses:  4.028287887573242 0.11577356606721878 0.7640640735626221
CurrentTrain: epoch  8, batch    44 | loss: 4.9081259Losses:  4.066446304321289 0.07562131434679031 0.7417723536491394
CurrentTrain: epoch  8, batch    45 | loss: 4.8838396Losses:  3.9786148071289062 0.09007471799850464 0.7050483822822571
CurrentTrain: epoch  8, batch    46 | loss: 4.7737379Losses:  4.010705947875977 0.10993198305368423 0.6390604972839355
CurrentTrain: epoch  8, batch    47 | loss: 4.7596984Losses:  4.044299125671387 0.08567768335342407 0.8166170716285706
CurrentTrain: epoch  8, batch    48 | loss: 4.9465938Losses:  3.9947803020477295 0.08643744140863419 0.7041489481925964
CurrentTrain: epoch  8, batch    49 | loss: 4.7853665Losses:  3.9935522079467773 0.08881017565727234 0.7574627995491028
CurrentTrain: epoch  8, batch    50 | loss: 4.8398252Losses:  4.030930519104004 0.08654386550188065 0.7475185394287109
CurrentTrain: epoch  8, batch    51 | loss: 4.8649931Losses:  4.011959075927734 0.1549280285835266 0.7312257289886475
CurrentTrain: epoch  8, batch    52 | loss: 4.8981133Losses:  4.023316383361816 0.11554165184497833 0.7510905861854553
CurrentTrain: epoch  8, batch    53 | loss: 4.8899484Losses:  3.990384578704834 0.12247852981090546 0.7519881129264832
CurrentTrain: epoch  8, batch    54 | loss: 4.8648510Losses:  3.9883346557617188 0.10287967324256897 0.7610107660293579
CurrentTrain: epoch  8, batch    55 | loss: 4.8522248Losses:  4.011544704437256 0.11881580948829651 0.7924600839614868
CurrentTrain: epoch  8, batch    56 | loss: 4.9228206Losses:  4.0000834465026855 0.12641960382461548 0.7751343250274658
CurrentTrain: epoch  8, batch    57 | loss: 4.9016371Losses:  4.02809476852417 0.1114407479763031 0.7802628874778748
CurrentTrain: epoch  8, batch    58 | loss: 4.9197984Losses:  4.017525672912598 0.11743465065956116 0.6748506426811218
CurrentTrain: epoch  8, batch    59 | loss: 4.8098106Losses:  3.954871416091919 0.13778293132781982 0.7060805559158325
CurrentTrain: epoch  8, batch    60 | loss: 4.7987347Losses:  4.026494026184082 0.12084498256444931 0.7785653471946716
CurrentTrain: epoch  8, batch    61 | loss: 4.9259043Losses:  3.977565288543701 0.04854895919561386 0.9130657911300659
CurrentTrain: epoch  8, batch    62 | loss: 4.9391804Losses:  3.995004653930664 0.07218248397111893 0.6797356605529785
CurrentTrain: epoch  9, batch     0 | loss: 4.7469230Losses:  4.057098865509033 0.09369206428527832 0.7012295722961426
CurrentTrain: epoch  9, batch     1 | loss: 4.8520207Losses:  4.028382301330566 0.10439202189445496 0.7321488857269287
CurrentTrain: epoch  9, batch     2 | loss: 4.8649235Losses:  4.0500078201293945 0.09396006911993027 0.7923047542572021
CurrentTrain: epoch  9, batch     3 | loss: 4.9362726Losses:  4.002695083618164 0.07679696381092072 0.7042020559310913
CurrentTrain: epoch  9, batch     4 | loss: 4.7836943Losses:  4.00339937210083 0.08462708443403244 0.7375273108482361
CurrentTrain: epoch  9, batch     5 | loss: 4.8255539Losses:  4.025022506713867 0.10281750559806824 0.7497698664665222
CurrentTrain: epoch  9, batch     6 | loss: 4.8776097Losses:  4.002414703369141 0.09319566190242767 0.8457595705986023
CurrentTrain: epoch  9, batch     7 | loss: 4.9413695Losses:  3.9746367931365967 0.08389732241630554 0.7407163381576538
CurrentTrain: epoch  9, batch     8 | loss: 4.7992506Losses:  3.9916157722473145 0.1183662861585617 0.7215768694877625
CurrentTrain: epoch  9, batch     9 | loss: 4.8315587Losses:  4.021251678466797 0.10988549888134003 0.7250182628631592
CurrentTrain: epoch  9, batch    10 | loss: 4.8561554Losses:  4.025794506072998 0.11595237255096436 0.6804762482643127
CurrentTrain: epoch  9, batch    11 | loss: 4.8222232Losses:  4.004535675048828 0.10955993831157684 0.6859933137893677
CurrentTrain: epoch  9, batch    12 | loss: 4.8000889Losses:  4.006014823913574 0.11288309097290039 0.7980651259422302
CurrentTrain: epoch  9, batch    13 | loss: 4.9169631Losses:  3.9997825622558594 0.13901054859161377 0.7138564586639404
CurrentTrain: epoch  9, batch    14 | loss: 4.8526497Losses:  4.036203384399414 0.11088244616985321 0.7494555711746216
CurrentTrain: epoch  9, batch    15 | loss: 4.8965411Losses:  3.993989944458008 0.10954418778419495 0.7114803791046143
CurrentTrain: epoch  9, batch    16 | loss: 4.8150148Losses:  4.032995700836182 0.11664685606956482 0.7639234066009521
CurrentTrain: epoch  9, batch    17 | loss: 4.9135656Losses:  3.9956507682800293 0.08275388181209564 0.7851489782333374
CurrentTrain: epoch  9, batch    18 | loss: 4.8635535Losses:  3.9496655464172363 0.05333118885755539 0.7168996334075928
CurrentTrain: epoch  9, batch    19 | loss: 4.7198963Losses:  4.052310466766357 0.09810178726911545 0.7799263596534729
CurrentTrain: epoch  9, batch    20 | loss: 4.9303384Losses:  3.9916374683380127 0.09553208202123642 0.7211514711380005
CurrentTrain: epoch  9, batch    21 | loss: 4.8083210Losses:  4.04013204574585 0.1078985407948494 0.7487503290176392
CurrentTrain: epoch  9, batch    22 | loss: 4.8967810Losses:  3.991222858428955 0.0926070362329483 0.7470104694366455
CurrentTrain: epoch  9, batch    23 | loss: 4.8308401Losses:  3.9971184730529785 0.07668140530586243 0.7521892786026001
CurrentTrain: epoch  9, batch    24 | loss: 4.8259892Losses:  4.009250640869141 0.08644236624240875 0.6832146644592285
CurrentTrain: epoch  9, batch    25 | loss: 4.7789078Losses:  3.9578304290771484 0.11766886711120605 0.6958518028259277
CurrentTrain: epoch  9, batch    26 | loss: 4.7713513Losses:  3.991182327270508 0.09293162077665329 0.8147870302200317
CurrentTrain: epoch  9, batch    27 | loss: 4.8989010Losses:  3.984279155731201 0.12254917621612549 0.7272776365280151
CurrentTrain: epoch  9, batch    28 | loss: 4.8341060Losses:  3.986419916152954 0.10504704713821411 0.7514665126800537
CurrentTrain: epoch  9, batch    29 | loss: 4.8429337Losses:  4.015798568725586 0.1159878596663475 0.7599433660507202
CurrentTrain: epoch  9, batch    30 | loss: 4.8917298Losses:  3.97607684135437 0.13154181838035583 0.7131506204605103
CurrentTrain: epoch  9, batch    31 | loss: 4.8207693Losses:  3.965672016143799 0.09600047767162323 0.7775502800941467
CurrentTrain: epoch  9, batch    32 | loss: 4.8392229Losses:  3.965770721435547 0.0571012869477272 0.8497104048728943
CurrentTrain: epoch  9, batch    33 | loss: 4.8725824Losses:  3.9989264011383057 0.11007557809352875 0.7452411651611328
CurrentTrain: epoch  9, batch    34 | loss: 4.8542433Losses:  3.987821102142334 0.12973317503929138 0.7377948760986328
CurrentTrain: epoch  9, batch    35 | loss: 4.8553491Losses:  3.9920341968536377 0.06964360922574997 0.8494824767112732
CurrentTrain: epoch  9, batch    36 | loss: 4.9111605Losses:  3.9840705394744873 0.10331916064023972 0.7925730347633362
CurrentTrain: epoch  9, batch    37 | loss: 4.8799624Losses:  4.029633522033691 0.07710914313793182 0.7558853626251221
CurrentTrain: epoch  9, batch    38 | loss: 4.8626280Losses:  3.981581211090088 0.1207481399178505 0.740564227104187
CurrentTrain: epoch  9, batch    39 | loss: 4.8428936Losses:  4.077674865722656 0.07814282178878784 0.7402638792991638
CurrentTrain: epoch  9, batch    40 | loss: 4.8960814Losses:  3.986776828765869 0.060138363391160965 0.7758134603500366
CurrentTrain: epoch  9, batch    41 | loss: 4.8227286Losses:  4.001780986785889 0.056493598967790604 0.7148339152336121
CurrentTrain: epoch  9, batch    42 | loss: 4.7731085Losses:  3.980562210083008 0.05897148326039314 0.7895560264587402
CurrentTrain: epoch  9, batch    43 | loss: 4.8290896Losses:  4.002546310424805 0.09020189195871353 0.7885719537734985
CurrentTrain: epoch  9, batch    44 | loss: 4.8813200Losses:  3.973842144012451 0.09763255715370178 0.6942912340164185
CurrentTrain: epoch  9, batch    45 | loss: 4.7657657Losses:  3.966259479522705 0.06495846807956696 0.737822413444519
CurrentTrain: epoch  9, batch    46 | loss: 4.7690406Losses:  4.016878128051758 0.07549606263637543 0.7357102632522583
CurrentTrain: epoch  9, batch    47 | loss: 4.8280845Losses:  3.9957869052886963 0.08642376959323883 0.712466299533844
CurrentTrain: epoch  9, batch    48 | loss: 4.7946768Losses:  4.056190490722656 0.11699764430522919 0.702227771282196
CurrentTrain: epoch  9, batch    49 | loss: 4.8754158Losses:  4.020008087158203 0.11714296042919159 0.7642807960510254
CurrentTrain: epoch  9, batch    50 | loss: 4.9014320Losses:  4.030673980712891 0.10969841480255127 0.7735843658447266
CurrentTrain: epoch  9, batch    51 | loss: 4.9139566Losses:  3.994727373123169 0.07322193682193756 0.7673783302307129
CurrentTrain: epoch  9, batch    52 | loss: 4.8353276Losses:  3.97208309173584 0.10789814591407776 0.7287471294403076
CurrentTrain: epoch  9, batch    53 | loss: 4.8087282Losses:  3.9890904426574707 0.10226325690746307 0.782879650592804
CurrentTrain: epoch  9, batch    54 | loss: 4.8742337Losses:  3.9868340492248535 0.07393115758895874 0.7412601113319397
CurrentTrain: epoch  9, batch    55 | loss: 4.8020253Losses:  3.951456069946289 0.10815675556659698 0.7675907611846924
CurrentTrain: epoch  9, batch    56 | loss: 4.8272038Losses:  3.9884603023529053 0.11246718466281891 0.6757100224494934
CurrentTrain: epoch  9, batch    57 | loss: 4.7766376Losses:  3.9654202461242676 0.07877889275550842 0.7143800258636475
CurrentTrain: epoch  9, batch    58 | loss: 4.7585793Losses:  3.957894802093506 0.10845187306404114 0.7501931190490723
CurrentTrain: epoch  9, batch    59 | loss: 4.8165398Losses:  3.994624137878418 0.09496057778596878 0.685906171798706
CurrentTrain: epoch  9, batch    60 | loss: 4.7754908Losses:  3.9896674156188965 0.09428495168685913 0.6949318647384644
CurrentTrain: epoch  9, batch    61 | loss: 4.7788844Losses:  3.9871397018432617 0.037060659378767014 0.6947036385536194
CurrentTrain: epoch  9, batch    62 | loss: 4.7189040
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.29%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.26%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.54%   
cur_acc:  ['0.9454']
his_acc:  ['0.9454']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  7.239468574523926 1.2893117666244507 0.9780229330062866
CurrentTrain: epoch  0, batch     0 | loss: 9.5068026Losses:  6.579294204711914 1.0402462482452393 0.9921958446502686
CurrentTrain: epoch  0, batch     1 | loss: 8.6117363Losses:  6.68896484375 1.3197908401489258 0.9757776260375977
CurrentTrain: epoch  0, batch     2 | loss: 8.9845333Losses:  8.34952163696289 2.0861628513557662e-07 0.9804301261901855
CurrentTrain: epoch  0, batch     3 | loss: 9.3299522Losses:  6.442209243774414 1.256045937538147 0.9966621398925781
CurrentTrain: epoch  1, batch     0 | loss: 8.6949177Losses:  6.1440958976745605 1.2916899919509888 0.9696201086044312
CurrentTrain: epoch  1, batch     1 | loss: 8.4054060Losses:  5.322137832641602 1.2314882278442383 0.9699082374572754
CurrentTrain: epoch  1, batch     2 | loss: 7.5235343Losses:  3.361729860305786 0.4010998606681824 0.9799411296844482
CurrentTrain: epoch  1, batch     3 | loss: 4.7427711Losses:  5.300045967102051 1.1273843050003052 0.979312539100647
CurrentTrain: epoch  2, batch     0 | loss: 7.4067426Losses:  4.489542007446289 1.0774800777435303 0.958895742893219
CurrentTrain: epoch  2, batch     1 | loss: 6.5259180Losses:  4.452700614929199 1.0481934547424316 0.9914343953132629
CurrentTrain: epoch  2, batch     2 | loss: 6.4923286Losses:  6.860801696777344 0.08431719243526459 0.995368480682373
CurrentTrain: epoch  2, batch     3 | loss: 7.9404874Losses:  5.252109527587891 1.1376526355743408 0.9694229364395142
CurrentTrain: epoch  3, batch     0 | loss: 7.3591847Losses:  3.8904597759246826 0.948311448097229 0.9833554029464722
CurrentTrain: epoch  3, batch     1 | loss: 5.8221269Losses:  4.14577579498291 1.2507059574127197 0.9615567922592163
CurrentTrain: epoch  3, batch     2 | loss: 6.3580384Losses:  3.003142833709717 0.3431240916252136 1.0465906858444214
CurrentTrain: epoch  3, batch     3 | loss: 4.3928576Losses:  3.7345499992370605 0.909450888633728 0.9593050479888916
CurrentTrain: epoch  4, batch     0 | loss: 5.6033058Losses:  3.5495076179504395 0.937134861946106 1.0010645389556885
CurrentTrain: epoch  4, batch     1 | loss: 5.4877071Losses:  4.38516092300415 1.0167872905731201 0.9613654613494873
CurrentTrain: epoch  4, batch     2 | loss: 6.3633137Losses:  3.4366893768310547 0.0836578831076622 0.9488505721092224
CurrentTrain: epoch  4, batch     3 | loss: 4.4691978Losses:  4.018120765686035 0.8792888522148132 0.9715577363967896
CurrentTrain: epoch  5, batch     0 | loss: 5.8689671Losses:  2.9187753200531006 0.9637824296951294 0.9648694396018982
CurrentTrain: epoch  5, batch     1 | loss: 4.8474274Losses:  3.239543914794922 1.0844627618789673 0.9717127084732056
CurrentTrain: epoch  5, batch     2 | loss: 5.2957191Losses:  4.947535037994385 0.07815992832183838 0.9775358438491821
CurrentTrain: epoch  5, batch     3 | loss: 6.0032306Losses:  3.2890663146972656 0.9519459009170532 0.9531633257865906
CurrentTrain: epoch  6, batch     0 | loss: 5.1941752Losses:  3.4240992069244385 0.85777348279953 0.9692347049713135
CurrentTrain: epoch  6, batch     1 | loss: 5.2511072Losses:  3.164120674133301 1.0889993906021118 0.9860702753067017
CurrentTrain: epoch  6, batch     2 | loss: 5.2391901Losses:  2.9625589847564697 1.1920928955078125e-07 1.0
CurrentTrain: epoch  6, batch     3 | loss: 3.9625592Losses:  3.1227035522460938 0.8848878145217896 0.9783662557601929
CurrentTrain: epoch  7, batch     0 | loss: 4.9859576Losses:  2.6324563026428223 1.0543009042739868 0.9857184290885925
CurrentTrain: epoch  7, batch     1 | loss: 4.6724753Losses:  2.616462230682373 0.8673787713050842 0.949781596660614
CurrentTrain: epoch  7, batch     2 | loss: 4.4336224Losses:  2.280839681625366 8.94069742685133e-08 0.9203312397003174
CurrentTrain: epoch  7, batch     3 | loss: 3.2011709Losses:  2.557370185852051 0.9274587631225586 0.9726042747497559
CurrentTrain: epoch  8, batch     0 | loss: 4.4574332Losses:  2.6046223640441895 0.7886408567428589 0.9508156776428223
CurrentTrain: epoch  8, batch     1 | loss: 4.3440790Losses:  2.6008663177490234 0.8958401083946228 0.9676886200904846
CurrentTrain: epoch  8, batch     2 | loss: 4.4643950Losses:  1.8426711559295654 0.05842885375022888 1.0535070896148682
CurrentTrain: epoch  8, batch     3 | loss: 2.9546070Losses:  2.362614393234253 0.9827947020530701 0.965029239654541
CurrentTrain: epoch  9, batch     0 | loss: 4.3104382Losses:  2.7629101276397705 0.7424488067626953 0.9728329181671143
CurrentTrain: epoch  9, batch     1 | loss: 4.4781919Losses:  2.238154888153076 0.8151795864105225 0.9534356594085693
CurrentTrain: epoch  9, batch     2 | loss: 4.0067701Losses:  1.890559196472168 0.08902294933795929 0.9452677965164185
CurrentTrain: epoch  9, batch     3 | loss: 2.9248500
Losses:  2.263199806213379 0.9953901767730713 0.8547160625457764
MemoryTrain:  epoch  0, batch     0 | loss: 4.1133060Losses:  0.9052305817604065 0.08463756740093231 0.9438853859901428
MemoryTrain:  epoch  0, batch     1 | loss: 1.9337535Losses:  1.610011339187622 0.9091812372207642 0.8888766765594482
MemoryTrain:  epoch  1, batch     0 | loss: 3.4080694Losses:  1.552391767501831 0.5328433513641357 0.7838141918182373
MemoryTrain:  epoch  1, batch     1 | loss: 2.8690493Losses:  1.3203434944152832 0.9197292327880859 0.8492235541343689
MemoryTrain:  epoch  2, batch     0 | loss: 3.0892963Losses:  0.15221549570560455 0.25980961322784424 0.9483897686004639
MemoryTrain:  epoch  2, batch     1 | loss: 1.3604149Losses:  0.5698193907737732 0.81960129737854 0.8626946210861206
MemoryTrain:  epoch  3, batch     0 | loss: 2.2521152Losses:  1.8714755773544312 0.42944276332855225 0.8960317969322205
MemoryTrain:  epoch  3, batch     1 | loss: 3.1969502Losses:  0.8150271773338318 0.9655038118362427 0.8870755434036255
MemoryTrain:  epoch  4, batch     0 | loss: 2.6676064Losses:  0.016067609190940857 0.22838613390922546 0.7934231162071228
MemoryTrain:  epoch  4, batch     1 | loss: 1.0378768Losses:  0.5930962562561035 0.922344982624054 0.8633251190185547
MemoryTrain:  epoch  5, batch     0 | loss: 2.3787663Losses:  0.08588758856058121 0.14402449131011963 0.8880066871643066
MemoryTrain:  epoch  5, batch     1 | loss: 1.1179187Losses:  0.41969916224479675 0.8296003341674805 0.8849246501922607
MemoryTrain:  epoch  6, batch     0 | loss: 2.1342242Losses:  0.15880483388900757 0.25346043705940247 0.803357720375061
MemoryTrain:  epoch  6, batch     1 | loss: 1.2156230Losses:  0.19394488632678986 0.8461340665817261 0.8661545515060425
MemoryTrain:  epoch  7, batch     0 | loss: 1.9062335Losses:  1.2160301208496094 0.14987525343894958 0.8695182204246521
MemoryTrain:  epoch  7, batch     1 | loss: 2.2354236Losses:  0.4140453338623047 0.8160022497177124 0.8614214658737183
MemoryTrain:  epoch  8, batch     0 | loss: 2.0914690Losses:  0.02097824588418007 0.3086472749710083 0.8912723064422607
MemoryTrain:  epoch  8, batch     1 | loss: 1.2208978Losses:  0.2030257135629654 0.8161394596099854 0.8696140646934509
MemoryTrain:  epoch  9, batch     0 | loss: 1.8887792Losses:  0.598135232925415 0.11490204930305481 0.8634721040725708
MemoryTrain:  epoch  9, batch     1 | loss: 1.5765094
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 73.68%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.03%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.53%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.33%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.04%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.07%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 93.48%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.57%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.49%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.41%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.33%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.26%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.10%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 92.63%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 92.25%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 91.80%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.82%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.77%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.49%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 91.18%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.06%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.73%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 90.55%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 90.31%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 89.86%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 89.54%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 89.05%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 88.63%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 88.22%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 87.70%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.37%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 86.86%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.55%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 86.00%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 85.52%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 85.17%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 84.65%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 84.05%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 83.67%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 83.29%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 82.75%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.17%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 81.70%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 81.25%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.97%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.81%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 81.46%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.35%   
cur_acc:  ['0.9454', '0.7103']
his_acc:  ['0.9454', '0.8235']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  7.058818817138672 1.6570979356765747 0.9834647178649902
CurrentTrain: epoch  0, batch     0 | loss: 9.6993809Losses:  6.17169189453125 1.5994852781295776 0.9840171337127686
CurrentTrain: epoch  0, batch     1 | loss: 8.7551947Losses:  6.8090620040893555 1.8134487867355347 0.989450991153717
CurrentTrain: epoch  0, batch     2 | loss: 9.6119623Losses:  7.821070671081543 0.4277382493019104 0.9667454361915588
CurrentTrain: epoch  0, batch     3 | loss: 9.2155542Losses:  5.431544303894043 1.6081795692443848 0.9752341508865356
CurrentTrain: epoch  1, batch     0 | loss: 8.0149584Losses:  6.484132289886475 1.6437666416168213 0.9934107065200806
CurrentTrain: epoch  1, batch     1 | loss: 9.1213102Losses:  4.638843059539795 1.5357081890106201 0.9805554747581482
CurrentTrain: epoch  1, batch     2 | loss: 7.1551065Losses:  6.042096138000488 0.254304975271225 0.9422464370727539
CurrentTrain: epoch  1, batch     3 | loss: 7.2386475Losses:  4.08908748626709 1.2173713445663452 0.9778841733932495
CurrentTrain: epoch  2, batch     0 | loss: 6.2843432Losses:  4.867619514465332 1.5142508745193481 0.9719319939613342
CurrentTrain: epoch  2, batch     1 | loss: 7.3538022Losses:  6.04404878616333 1.6466643810272217 0.9873920679092407
CurrentTrain: epoch  2, batch     2 | loss: 8.6781054Losses:  2.3853392601013184 0.17307978868484497 0.9145910143852234
CurrentTrain: epoch  2, batch     3 | loss: 3.4730101Losses:  4.781947612762451 1.497917890548706 0.9634730815887451
CurrentTrain: epoch  3, batch     0 | loss: 7.2433386Losses:  4.177711009979248 1.2700459957122803 0.9746360778808594
CurrentTrain: epoch  3, batch     1 | loss: 6.4223928Losses:  5.021285057067871 1.4603379964828491 0.9728182554244995
CurrentTrain: epoch  3, batch     2 | loss: 7.4544415Losses:  5.347470283508301 0.1751980483531952 0.9695987701416016
CurrentTrain: epoch  3, batch     3 | loss: 6.4922671Losses:  5.116794586181641 1.375377893447876 0.9719934463500977
CurrentTrain: epoch  4, batch     0 | loss: 7.4641657Losses:  4.350276947021484 1.4131736755371094 0.9744120836257935
CurrentTrain: epoch  4, batch     1 | loss: 6.7378626Losses:  3.153595447540283 1.1659026145935059 0.9585914015769958
CurrentTrain: epoch  4, batch     2 | loss: 5.2780895Losses:  4.288524150848389 0.28844156861305237 1.0
CurrentTrain: epoch  4, batch     3 | loss: 5.5769658Losses:  4.818107604980469 1.3026869297027588 0.9641344547271729
CurrentTrain: epoch  5, batch     0 | loss: 7.0849285Losses:  3.516568899154663 1.0615026950836182 0.9746524095535278
CurrentTrain: epoch  5, batch     1 | loss: 5.5527239Losses:  3.556973934173584 0.9953433275222778 0.9607274532318115
CurrentTrain: epoch  5, batch     2 | loss: 5.5130444Losses:  7.344699859619141 0.5997701287269592 0.9952859282493591
CurrentTrain: epoch  5, batch     3 | loss: 8.9397554Losses:  4.262145519256592 1.447842001914978 0.9749317765235901
CurrentTrain: epoch  6, batch     0 | loss: 6.6849194Losses:  4.123863220214844 1.2625517845153809 0.9557390213012695
CurrentTrain: epoch  6, batch     1 | loss: 6.3421540Losses:  3.210261344909668 1.0580627918243408 0.9666139483451843
CurrentTrain: epoch  6, batch     2 | loss: 5.2349377Losses:  2.7963147163391113 0.19487500190734863 0.8850042819976807
CurrentTrain: epoch  6, batch     3 | loss: 3.8761940Losses:  3.3552112579345703 1.1041669845581055 0.974111795425415
CurrentTrain: epoch  7, batch     0 | loss: 5.4334898Losses:  3.71096134185791 1.078385591506958 0.9627170562744141
CurrentTrain: epoch  7, batch     1 | loss: 5.7520638Losses:  3.585463762283325 1.1199908256530762 0.9371957182884216
CurrentTrain: epoch  7, batch     2 | loss: 5.6426506Losses:  5.163808822631836 0.5106241106987 0.9399044513702393
CurrentTrain: epoch  7, batch     3 | loss: 6.6143370Losses:  3.41266131401062 1.1908817291259766 0.9450985193252563
CurrentTrain: epoch  8, batch     0 | loss: 5.5486417Losses:  3.4601378440856934 1.1922030448913574 0.9592958092689514
CurrentTrain: epoch  8, batch     1 | loss: 5.6116366Losses:  3.364935874938965 0.8641959428787231 0.9478144645690918
CurrentTrain: epoch  8, batch     2 | loss: 5.1769462Losses:  2.3758773803710938 0.06080237403512001 0.9592834711074829
CurrentTrain: epoch  8, batch     3 | loss: 3.3959632Losses:  2.980921983718872 0.9688576459884644 0.9220119714736938
CurrentTrain: epoch  9, batch     0 | loss: 4.8717914Losses:  2.5915684700012207 0.8091282844543457 0.971996545791626
CurrentTrain: epoch  9, batch     1 | loss: 4.3726931Losses:  3.7007670402526855 1.0131895542144775 0.9555203318595886
CurrentTrain: epoch  9, batch     2 | loss: 5.6694770Losses:  4.944933891296387 0.5654593110084534 0.9173189401626587
CurrentTrain: epoch  9, batch     3 | loss: 6.4277120
Losses:  1.0814940929412842 1.123338222503662 0.9077451229095459
MemoryTrain:  epoch  0, batch     0 | loss: 3.1125774Losses:  0.6487934589385986 0.9844213724136353 0.869714081287384
MemoryTrain:  epoch  0, batch     1 | loss: 2.5029290Losses:  0.8925313949584961 1.1194241046905518 0.9316049814224243
MemoryTrain:  epoch  1, batch     0 | loss: 2.9435606Losses:  0.8205176591873169 0.8164549469947815 0.845573365688324
MemoryTrain:  epoch  1, batch     1 | loss: 2.4825461Losses:  0.4118909239768982 0.8954042792320251 0.8781493902206421
MemoryTrain:  epoch  2, batch     0 | loss: 2.1854446Losses:  0.8287645578384399 0.9262570142745972 0.9011664986610413
MemoryTrain:  epoch  2, batch     1 | loss: 2.6561880Losses:  0.2886756360530853 0.9378014206886292 0.8539546132087708
MemoryTrain:  epoch  3, batch     0 | loss: 2.0804317Losses:  0.19337736070156097 0.8865926265716553 0.9232323169708252
MemoryTrain:  epoch  3, batch     1 | loss: 2.0032024Losses:  0.2190784513950348 1.0591437816619873 0.8570342063903809
MemoryTrain:  epoch  4, batch     0 | loss: 2.1352563Losses:  0.2166704386472702 0.7261955142021179 0.9148505330085754
MemoryTrain:  epoch  4, batch     1 | loss: 1.8577166Losses:  0.13493654131889343 1.044451117515564 0.9196884632110596
MemoryTrain:  epoch  5, batch     0 | loss: 2.0990763Losses:  0.09542220085859299 0.7632520794868469 0.8494599461555481
MemoryTrain:  epoch  5, batch     1 | loss: 1.7081342Losses:  0.11528589576482773 0.879318118095398 0.8835281133651733
MemoryTrain:  epoch  6, batch     0 | loss: 1.8781321Losses:  0.03355978801846504 0.829008162021637 0.8904540538787842
MemoryTrain:  epoch  6, batch     1 | loss: 1.7530220Losses:  0.23982040584087372 0.799872875213623 0.866784930229187
MemoryTrain:  epoch  7, batch     0 | loss: 1.9064782Losses:  0.11296775192022324 0.9053221940994263 0.9019616842269897
MemoryTrain:  epoch  7, batch     1 | loss: 1.9202516Losses:  0.06104482710361481 1.05067777633667 0.911272406578064
MemoryTrain:  epoch  8, batch     0 | loss: 2.0229950Losses:  0.07602629065513611 0.5973122715950012 0.8502265214920044
MemoryTrain:  epoch  8, batch     1 | loss: 1.5235651Losses:  0.03831619769334793 1.005957841873169 0.8948729038238525
MemoryTrain:  epoch  9, batch     0 | loss: 1.9391470Losses:  0.03711177408695221 0.5922601222991943 0.8684147596359253
MemoryTrain:  epoch  9, batch     1 | loss: 1.4977866
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 60.48%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 61.13%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 62.12%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 71.01%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 70.29%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.37%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 70.52%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 70.77%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.24%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.53%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 93.02%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.73%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.64%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.10%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 93.20%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.32%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.24%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 93.07%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.93%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.78%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 92.55%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.33%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.03%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 91.77%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 91.42%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 91.22%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 90.88%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 90.70%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 90.23%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 90.13%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 89.89%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 89.58%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 89.35%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 89.20%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 88.84%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 88.43%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 88.03%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 87.57%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.24%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 86.73%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 86.49%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.00%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 85.19%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.98%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.61%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.17%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.62%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.08%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 82.61%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.15%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 81.81%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.31%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 82.74%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 82.53%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 82.23%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 82.22%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 81.97%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.95%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 82.04%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 82.13%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 82.16%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 81.70%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 81.21%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 80.81%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 80.33%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 79.85%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 79.51%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.89%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 79.93%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 79.65%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 79.37%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 79.22%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 78.87%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 78.49%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.84%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.19%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 79.79%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 79.52%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 79.49%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 79.54%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 79.44%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 79.27%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 79.14%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 79.09%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.01%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 78.96%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 79.01%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 78.92%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.93%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 78.69%   
cur_acc:  ['0.9454', '0.7103', '0.7024']
his_acc:  ['0.9454', '0.8235', '0.7869']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  5.979862689971924 1.6129547357559204 0.9676281213760376
CurrentTrain: epoch  0, batch     0 | loss: 8.5604458Losses:  4.197955131530762 1.5195963382720947 0.9942248463630676
CurrentTrain: epoch  0, batch     1 | loss: 6.7117763Losses:  5.104171276092529 1.649766445159912 0.9883266687393188
CurrentTrain: epoch  0, batch     2 | loss: 7.7422643Losses:  3.2512290477752686 0.4543897211551666 0.9483108520507812
CurrentTrain: epoch  0, batch     3 | loss: 4.6539297Losses:  4.523331642150879 1.4831340312957764 0.9777559041976929
CurrentTrain: epoch  1, batch     0 | loss: 6.9842219Losses:  3.6608476638793945 1.2883024215698242 0.9533320665359497
CurrentTrain: epoch  1, batch     1 | loss: 5.9024820Losses:  3.857123613357544 1.8602735996246338 0.9944037199020386
CurrentTrain: epoch  1, batch     2 | loss: 6.7118011Losses:  2.838240146636963 0.16740018129348755 0.963925838470459
CurrentTrain: epoch  1, batch     3 | loss: 3.9695661Losses:  4.513341903686523 1.5847344398498535 0.9773789644241333
CurrentTrain: epoch  2, batch     0 | loss: 7.0754552Losses:  3.0426559448242188 1.4098297357559204 0.9649834632873535
CurrentTrain: epoch  2, batch     1 | loss: 5.4174690Losses:  2.388434410095215 1.210068941116333 0.9827607274055481
CurrentTrain: epoch  2, batch     2 | loss: 4.5812640Losses:  1.8840477466583252 0.1405395269393921 0.9178668856620789
CurrentTrain: epoch  2, batch     3 | loss: 2.9424541Losses:  3.1226649284362793 1.210459589958191 0.976664662361145
CurrentTrain: epoch  3, batch     0 | loss: 5.3097892Losses:  3.034588098526001 1.4409685134887695 0.9473723769187927
CurrentTrain: epoch  3, batch     1 | loss: 5.4229288Losses:  3.001302719116211 1.3746705055236816 0.9686689972877502
CurrentTrain: epoch  3, batch     2 | loss: 5.3446422Losses:  2.166419744491577 0.11804846674203873 1.0
CurrentTrain: epoch  3, batch     3 | loss: 3.2844682Losses:  2.8663716316223145 1.0259032249450684 0.940048336982727
CurrentTrain: epoch  4, batch     0 | loss: 4.8323231Losses:  2.691305160522461 1.0683717727661133 0.9388057589530945
CurrentTrain: epoch  4, batch     1 | loss: 4.6984825Losses:  3.1457228660583496 0.9145748019218445 1.0017768144607544
CurrentTrain: epoch  4, batch     2 | loss: 5.0620742Losses:  2.0956759452819824 0.21848079562187195 0.9298970699310303
CurrentTrain: epoch  4, batch     3 | loss: 3.2440538Losses:  3.150306463241577 1.1321096420288086 0.9620209336280823
CurrentTrain: epoch  5, batch     0 | loss: 5.2444372Losses:  3.1740622520446777 1.0409144163131714 0.9274271726608276
CurrentTrain: epoch  5, batch     1 | loss: 5.1424041Losses:  2.4555437564849854 1.2374749183654785 0.9651777148246765
CurrentTrain: epoch  5, batch     2 | loss: 4.6581964Losses:  2.1276488304138184 8.94069742685133e-08 1.0
CurrentTrain: epoch  5, batch     3 | loss: 3.1276488Losses:  2.1766083240509033 0.8988913297653198 0.9795409440994263
CurrentTrain: epoch  6, batch     0 | loss: 4.0550404Losses:  2.8531923294067383 0.859847903251648 0.9416860342025757
CurrentTrain: epoch  6, batch     1 | loss: 4.6547265Losses:  3.005122661590576 1.1524302959442139 0.9255519509315491
CurrentTrain: epoch  6, batch     2 | loss: 5.0831046Losses:  2.448094367980957 0.3039463460445404 0.9205260276794434
CurrentTrain: epoch  6, batch     3 | loss: 3.6725667Losses:  2.3136210441589355 0.8807747960090637 0.9562395215034485
CurrentTrain: epoch  7, batch     0 | loss: 4.1506352Losses:  2.315429210662842 1.1083741188049316 0.9519138336181641
CurrentTrain: epoch  7, batch     1 | loss: 4.3757172Losses:  2.8033816814422607 1.1358801126480103 0.91350257396698
CurrentTrain: epoch  7, batch     2 | loss: 4.8527646Losses:  2.826322078704834 0.38051003217697144 0.9807660579681396
CurrentTrain: epoch  7, batch     3 | loss: 4.1875982Losses:  2.6277408599853516 1.0007972717285156 0.9496728181838989
CurrentTrain: epoch  8, batch     0 | loss: 4.5782108Losses:  2.389331102371216 0.8642271757125854 0.9295587539672852
CurrentTrain: epoch  8, batch     1 | loss: 4.1831169Losses:  2.064073324203491 0.8171276450157166 0.94347083568573
CurrentTrain: epoch  8, batch     2 | loss: 3.8246717Losses:  2.519714832305908 0.16945308446884155 0.8774625062942505
CurrentTrain: epoch  8, batch     3 | loss: 3.5666304Losses:  2.318803310394287 0.8739477396011353 0.9362495541572571
CurrentTrain: epoch  9, batch     0 | loss: 4.1290007Losses:  2.7135026454925537 0.8313522338867188 0.9223963022232056
CurrentTrain: epoch  9, batch     1 | loss: 4.4672513Losses:  2.0294549465179443 0.8181678056716919 0.9387643337249756
CurrentTrain: epoch  9, batch     2 | loss: 3.7863872Losses:  2.0902862548828125 0.42371776700019836 0.9288792014122009
CurrentTrain: epoch  9, batch     3 | loss: 3.4428833
Losses:  0.42218613624572754 1.143847942352295 0.9109460115432739
MemoryTrain:  epoch  0, batch     0 | loss: 2.4769802Losses:  1.02096688747406 0.8594512939453125 0.8766160607337952
MemoryTrain:  epoch  0, batch     1 | loss: 2.7570343Losses:  1.0302422046661377 0.5400090217590332 0.9106049537658691
MemoryTrain:  epoch  0, batch     2 | loss: 2.4808562Losses:  0.9460722208023071 0.8741332292556763 0.9324573278427124
MemoryTrain:  epoch  1, batch     0 | loss: 2.7526627Losses:  0.7495085597038269 0.7496550679206848 0.8578317165374756
MemoryTrain:  epoch  1, batch     1 | loss: 2.3569953Losses:  1.5960874557495117 0.8139951229095459 0.8982090353965759
MemoryTrain:  epoch  1, batch     2 | loss: 3.3082917Losses:  0.5581680536270142 1.05232572555542 0.897462785243988
MemoryTrain:  epoch  2, batch     0 | loss: 2.5079565Losses:  0.7623323202133179 0.8645514249801636 0.9322875738143921
MemoryTrain:  epoch  2, batch     1 | loss: 2.5591712Losses:  0.5937933921813965 0.4852074384689331 0.8205143213272095
MemoryTrain:  epoch  2, batch     2 | loss: 1.8995152Losses:  0.5753163695335388 1.07309889793396 0.9044084548950195
MemoryTrain:  epoch  3, batch     0 | loss: 2.5528238Losses:  0.378149151802063 0.6618772745132446 0.8909295797348022
MemoryTrain:  epoch  3, batch     1 | loss: 1.9309560Losses:  0.21176490187644958 0.5416853427886963 0.8647050857543945
MemoryTrain:  epoch  3, batch     2 | loss: 1.6181554Losses:  0.14089477062225342 0.9677067399024963 0.9058288335800171
MemoryTrain:  epoch  4, batch     0 | loss: 2.0144305Losses:  0.6757625341415405 0.8308476209640503 0.8731535077095032
MemoryTrain:  epoch  4, batch     1 | loss: 2.3797636Losses:  0.5062341690063477 0.5616488456726074 0.9008967876434326
MemoryTrain:  epoch  4, batch     2 | loss: 1.9687798Losses:  0.18793201446533203 0.7831878662109375 0.8962551355361938
MemoryTrain:  epoch  5, batch     0 | loss: 1.8673750Losses:  0.12845902144908905 1.0413497686386108 0.8501447439193726
MemoryTrain:  epoch  5, batch     1 | loss: 2.0199535Losses:  0.12242966890335083 0.45480024814605713 0.9580888748168945
MemoryTrain:  epoch  5, batch     2 | loss: 1.5353189Losses:  0.0989137589931488 0.7013885974884033 0.8619692325592041
MemoryTrain:  epoch  6, batch     0 | loss: 1.6622716Losses:  0.09486111998558044 0.8522024154663086 0.9140443801879883
MemoryTrain:  epoch  6, batch     1 | loss: 1.8611079Losses:  0.10880938172340393 0.6899406313896179 0.8908978700637817
MemoryTrain:  epoch  6, batch     2 | loss: 1.6896479Losses:  0.06741125881671906 0.7452453970909119 0.8796539306640625
MemoryTrain:  epoch  7, batch     0 | loss: 1.6923106Losses:  0.06898695230484009 0.7653641700744629 0.8959187269210815
MemoryTrain:  epoch  7, batch     1 | loss: 1.7302699Losses:  0.059493061155080795 0.5441083908081055 0.881653904914856
MemoryTrain:  epoch  7, batch     2 | loss: 1.4852554Losses:  0.05059428513050079 0.595259428024292 0.9041035771369934
MemoryTrain:  epoch  8, batch     0 | loss: 1.5499573Losses:  0.09342508018016815 0.977518618106842 0.8905118703842163
MemoryTrain:  epoch  8, batch     1 | loss: 1.9614556Losses:  0.05003760755062103 0.49594879150390625 0.8422927856445312
MemoryTrain:  epoch  8, batch     2 | loss: 1.3882792Losses:  0.034111618995666504 0.9167919158935547 0.8934904336929321
MemoryTrain:  epoch  9, batch     0 | loss: 1.8443940Losses:  0.06256384402513504 0.6405892372131348 0.8572142720222473
MemoryTrain:  epoch  9, batch     1 | loss: 1.5603673Losses:  0.034653984010219574 0.42273029685020447 0.9255349040031433
MemoryTrain:  epoch  9, batch     2 | loss: 1.3829192
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 59.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.20%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.46%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.74%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.27%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.83%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.22%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.03%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.80%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.85%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.79%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.98%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 90.89%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 90.84%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 90.71%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.50%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.38%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.19%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 89.92%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.89%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 89.48%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 88.93%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 88.69%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 88.16%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 87.86%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 87.07%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 86.86%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 86.52%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 86.25%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 85.99%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 85.67%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 85.22%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 84.71%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 84.28%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 83.72%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 83.38%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 82.97%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 82.70%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 82.12%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 81.81%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 81.56%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.31%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 81.13%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 80.84%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 80.37%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.75%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.19%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 78.58%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.10%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.73%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.49%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.35%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 79.22%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.89%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 78.66%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 78.59%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 78.20%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.54%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 78.71%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 78.19%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.81%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 77.35%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 76.85%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 76.40%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 76.09%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 76.61%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 76.32%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 76.02%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.56%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 75.32%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 75.40%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.70%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 76.09%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 75.98%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 75.88%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 75.77%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 75.68%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 75.70%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 75.73%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 75.96%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 75.86%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 75.82%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 75.53%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 75.34%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.34%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 75.15%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.12%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.06%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 74.94%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 74.76%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 74.52%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 74.32%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.08%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 73.82%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 73.80%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 74.61%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 74.56%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.19%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 75.87%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 76.03%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 76.15%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.45%   
cur_acc:  ['0.9454', '0.7103', '0.7024', '0.7827']
his_acc:  ['0.9454', '0.8235', '0.7869', '0.7645']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  6.012415885925293 1.4231693744659424 0.9680505990982056
CurrentTrain: epoch  0, batch     0 | loss: 8.4036360Losses:  6.446987152099609 1.649661898612976 0.9561353325843811
CurrentTrain: epoch  0, batch     1 | loss: 9.0527849Losses:  6.050317764282227 1.3380603790283203 0.9831227660179138
CurrentTrain: epoch  0, batch     2 | loss: 8.3715010Losses:  3.6072239875793457 1.1920930376163597e-07 1.0
CurrentTrain: epoch  0, batch     3 | loss: 4.6072245Losses:  5.719051361083984 1.4001811742782593 0.944844126701355
CurrentTrain: epoch  1, batch     0 | loss: 8.0640764Losses:  4.823723793029785 1.4683876037597656 0.967255175113678
CurrentTrain: epoch  1, batch     1 | loss: 7.2593665Losses:  4.879448890686035 1.3254897594451904 0.9912172555923462
CurrentTrain: epoch  1, batch     2 | loss: 7.1961560Losses:  4.230643272399902 0.464910626411438 0.8991570472717285
CurrentTrain: epoch  1, batch     3 | loss: 5.5947108Losses:  4.384340763092041 1.5671474933624268 0.9470903873443604
CurrentTrain: epoch  2, batch     0 | loss: 6.8985786Losses:  5.442630767822266 1.4691216945648193 0.9670661687850952
CurrentTrain: epoch  2, batch     1 | loss: 7.8788190Losses:  4.566225051879883 1.2331230640411377 0.9569706916809082
CurrentTrain: epoch  2, batch     2 | loss: 6.7563186Losses:  3.125429153442383 0.06908734887838364 1.0231003761291504
CurrentTrain: epoch  2, batch     3 | loss: 4.2176170Losses:  4.8708038330078125 1.326702356338501 0.9604724645614624
CurrentTrain: epoch  3, batch     0 | loss: 7.1579785Losses:  4.127130508422852 1.0782015323638916 0.9528980255126953
CurrentTrain: epoch  3, batch     1 | loss: 6.1582298Losses:  3.844075918197632 1.166010856628418 0.9523395895957947
CurrentTrain: epoch  3, batch     2 | loss: 5.9624267Losses:  4.9019365310668945 0.5041415095329285 0.930983304977417
CurrentTrain: epoch  3, batch     3 | loss: 6.3370609Losses:  3.6752116680145264 1.1075794696807861 0.9588234424591064
CurrentTrain: epoch  4, batch     0 | loss: 5.7416143Losses:  4.361199378967285 1.005791187286377 0.9366869926452637
CurrentTrain: epoch  4, batch     1 | loss: 6.3036776Losses:  3.8685991764068604 1.2246140241622925 0.9580989480018616
CurrentTrain: epoch  4, batch     2 | loss: 6.0513120Losses:  1.7165846824645996 0.20179632306098938 0.8576235771179199
CurrentTrain: epoch  4, batch     3 | loss: 2.7760046Losses:  3.396761178970337 1.31412672996521 0.9254896640777588
CurrentTrain: epoch  5, batch     0 | loss: 5.6363773Losses:  3.4233906269073486 0.9859002232551575 0.9412472248077393
CurrentTrain: epoch  5, batch     1 | loss: 5.3505383Losses:  3.8385980129241943 1.096996545791626 0.9606026411056519
CurrentTrain: epoch  5, batch     2 | loss: 5.8961973Losses:  5.216464042663574 0.5141849517822266 0.934921383857727
CurrentTrain: epoch  5, batch     3 | loss: 6.6655703Losses:  3.4489076137542725 0.9904230833053589 0.9675341248512268
CurrentTrain: epoch  6, batch     0 | loss: 5.4068646Losses:  3.3903121948242188 0.9628920555114746 0.9379764795303345
CurrentTrain: epoch  6, batch     1 | loss: 5.2911806Losses:  3.4822793006896973 1.1348954439163208 0.9268812537193298
CurrentTrain: epoch  6, batch     2 | loss: 5.5440559Losses:  2.7444353103637695 0.24673527479171753 0.8419342041015625
CurrentTrain: epoch  6, batch     3 | loss: 3.8331048Losses:  3.0705699920654297 1.0948855876922607 0.9472206234931946
CurrentTrain: epoch  7, batch     0 | loss: 5.1126766Losses:  3.2728872299194336 1.0766606330871582 0.9552895426750183
CurrentTrain: epoch  7, batch     1 | loss: 5.3048372Losses:  3.2604541778564453 0.9733836650848389 0.912678599357605
CurrentTrain: epoch  7, batch     2 | loss: 5.1465168Losses:  2.8772454261779785 0.43950605392456055 0.8936030268669128
CurrentTrain: epoch  7, batch     3 | loss: 4.2103543Losses:  2.6831061840057373 1.0576038360595703 0.9153537154197693
CurrentTrain: epoch  8, batch     0 | loss: 4.6560636Losses:  3.6422479152679443 1.0022913217544556 0.9113859534263611
CurrentTrain: epoch  8, batch     1 | loss: 5.5559254Losses:  3.004100799560547 0.9238983392715454 0.9603866338729858
CurrentTrain: epoch  8, batch     2 | loss: 4.8883858Losses:  1.7697982788085938 0.13867908716201782 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.9084773Losses:  3.005082845687866 1.0193839073181152 0.9180561304092407
CurrentTrain: epoch  9, batch     0 | loss: 4.9425225Losses:  2.2916455268859863 0.7879248261451721 0.9049978852272034
CurrentTrain: epoch  9, batch     1 | loss: 3.9845681Losses:  3.281851291656494 0.9186915755271912 0.961341381072998
CurrentTrain: epoch  9, batch     2 | loss: 5.1618843Losses:  2.3077175617218018 0.2224900871515274 0.9524593949317932
CurrentTrain: epoch  9, batch     3 | loss: 3.4826670
Losses:  0.641917884349823 0.7325974702835083 0.9025501012802124
MemoryTrain:  epoch  0, batch     0 | loss: 2.2770653Losses:  0.5860721468925476 1.1837574243545532 0.9425927400588989
MemoryTrain:  epoch  0, batch     1 | loss: 2.7124224Losses:  0.45699262619018555 0.7992465496063232 0.90845787525177
MemoryTrain:  epoch  0, batch     2 | loss: 2.1646972Losses:  2.082695245742798 0.07210761308670044 0.8018280267715454
MemoryTrain:  epoch  0, batch     3 | loss: 2.9566307Losses:  0.9556736946105957 0.8943847417831421 0.9250843524932861
MemoryTrain:  epoch  1, batch     0 | loss: 2.7751427Losses:  0.7542139291763306 1.009519338607788 0.9116014242172241
MemoryTrain:  epoch  1, batch     1 | loss: 2.6753347Losses:  0.4736010432243347 0.868874192237854 0.8800569772720337
MemoryTrain:  epoch  1, batch     2 | loss: 2.2225323Losses:  0.02934415079653263 0.031534068286418915 1.0
MemoryTrain:  epoch  1, batch     3 | loss: 1.0608783Losses:  0.6978682279586792 0.8459824323654175 0.8856232166290283
MemoryTrain:  epoch  2, batch     0 | loss: 2.4294739Losses:  0.14840362966060638 0.8212836980819702 0.8825204372406006
MemoryTrain:  epoch  2, batch     1 | loss: 1.8522078Losses:  0.5383393168449402 0.8043882846832275 0.9697310328483582
MemoryTrain:  epoch  2, batch     2 | loss: 2.3124588Losses:  0.0683165043592453 0.4788684546947479 0.8339715003967285
MemoryTrain:  epoch  2, batch     3 | loss: 1.3811564Losses:  0.4041607677936554 0.6928238868713379 0.9450415372848511
MemoryTrain:  epoch  3, batch     0 | loss: 2.0420260Losses:  0.12918369472026825 1.1020978689193726 0.8809906244277954
MemoryTrain:  epoch  3, batch     1 | loss: 2.1122723Losses:  0.4061257839202881 0.8710351586341858 0.8920952081680298
MemoryTrain:  epoch  3, batch     2 | loss: 2.1692562Losses:  0.08159554749727249 0.19225430488586426 0.9440313577651978
MemoryTrain:  epoch  3, batch     3 | loss: 1.2178812Losses:  0.29292404651641846 0.755095362663269 0.9207624197006226
MemoryTrain:  epoch  4, batch     0 | loss: 1.9687818Losses:  0.1388581246137619 0.9621615409851074 0.9095875024795532
MemoryTrain:  epoch  4, batch     1 | loss: 2.0106072Losses:  0.08316795527935028 0.8545218706130981 0.8782517910003662
MemoryTrain:  epoch  4, batch     2 | loss: 1.8159416Losses:  0.1685347855091095 0.05333159863948822 1.0
MemoryTrain:  epoch  4, batch     3 | loss: 1.2218664Losses:  0.18718014657497406 0.6487575769424438 0.8971887230873108
MemoryTrain:  epoch  5, batch     0 | loss: 1.7331264Losses:  0.08388251066207886 0.8471551537513733 0.9190925359725952
MemoryTrain:  epoch  5, batch     1 | loss: 1.8501302Losses:  0.09840907156467438 1.050036907196045 0.900780200958252
MemoryTrain:  epoch  5, batch     2 | loss: 2.0492263Losses:  0.01403738372027874 0.017248710617423058 0.9210376739501953
MemoryTrain:  epoch  5, batch     3 | loss: 0.9523238Losses:  0.13016590476036072 0.7329180240631104 0.9168499708175659
MemoryTrain:  epoch  6, batch     0 | loss: 1.7799339Losses:  0.08672043681144714 0.872644305229187 0.8612850904464722
MemoryTrain:  epoch  6, batch     1 | loss: 1.8206499Losses:  0.06079576537013054 0.9283676147460938 0.9302910566329956
MemoryTrain:  epoch  6, batch     2 | loss: 1.9194545Losses:  0.012863002717494965 0.04760412499308586 0.9430134296417236
MemoryTrain:  epoch  6, batch     3 | loss: 1.0034806Losses:  0.0855424702167511 0.9717296957969666 0.9258024096488953
MemoryTrain:  epoch  7, batch     0 | loss: 1.9830747Losses:  0.05612686648964882 0.6875638961791992 0.8821971416473389
MemoryTrain:  epoch  7, batch     1 | loss: 1.6258879Losses:  0.055396437644958496 0.7846132516860962 0.9060709476470947
MemoryTrain:  epoch  7, batch     2 | loss: 1.7460806Losses:  0.0166478231549263 0.01097829733043909 0.9068999290466309
MemoryTrain:  epoch  7, batch     3 | loss: 0.9345260Losses:  0.06384827941656113 0.9209890365600586 0.8612818717956543
MemoryTrain:  epoch  8, batch     0 | loss: 1.8461192Losses:  0.050866737961769104 0.6180983781814575 0.9436980485916138
MemoryTrain:  epoch  8, batch     1 | loss: 1.6126631Losses:  0.05046696960926056 0.7558884620666504 0.9000319838523865
MemoryTrain:  epoch  8, batch     2 | loss: 1.7063874Losses:  0.05245514586567879 0.14634177088737488 0.933253288269043
MemoryTrain:  epoch  8, batch     3 | loss: 1.1320502Losses:  0.0497654527425766 0.7777554392814636 0.8601438999176025
MemoryTrain:  epoch  9, batch     0 | loss: 1.6876647Losses:  0.06428039073944092 0.8401631116867065 0.8896798491477966
MemoryTrain:  epoch  9, batch     1 | loss: 1.7941234Losses:  0.04005132615566254 0.6814897060394287 0.9566379189491272
MemoryTrain:  epoch  9, batch     2 | loss: 1.6781790Losses:  0.014874674379825592 0.005284335929900408 0.8785011172294617
MemoryTrain:  epoch  9, batch     3 | loss: 0.8986601
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 61.40%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 60.54%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 59.38%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 57.94%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 58.06%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 59.06%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 59.91%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 60.57%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 60.76%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 61.08%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 61.41%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 61.70%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 62.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.64%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.50%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 89.35%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 88.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 88.38%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 88.14%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.90%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 88.53%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 88.69%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.04%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 89.02%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 88.88%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 88.86%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.69%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 88.59%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 88.50%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.19%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 87.73%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 87.43%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.99%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 86.70%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 85.99%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 85.67%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 85.35%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 85.16%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.85%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 84.61%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 84.11%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 83.62%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 83.01%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 82.27%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 82.01%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 81.44%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 80.82%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 80.58%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 80.35%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 80.18%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 80.07%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.67%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.05%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 78.50%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 77.90%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 77.36%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.01%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.77%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 78.72%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 78.44%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 78.17%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 78.05%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 77.93%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 77.77%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.51%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 78.35%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 77.97%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.59%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 77.13%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 76.67%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 76.22%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 75.91%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 76.45%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 76.15%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 75.82%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 75.61%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 75.20%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 74.84%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.92%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.53%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 75.49%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 75.91%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 75.92%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 75.81%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 75.59%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.56%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.51%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.77%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 75.80%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 75.86%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 75.92%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 76.03%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 75.73%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 75.53%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 75.38%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 75.34%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.34%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 75.21%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.18%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.12%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 75.09%   [EVAL] batch:  207 | acc: 75.00%,  total acc: 75.09%   [EVAL] batch:  208 | acc: 68.75%,  total acc: 75.06%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 74.97%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 74.79%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 74.65%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 75.26%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 75.25%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 75.08%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.46%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 76.61%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.91%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.20%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 77.07%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 77.06%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 76.87%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 76.74%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 76.66%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 76.52%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 76.47%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.34%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 76.28%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 76.18%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 75.99%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 75.91%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 75.82%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 76.27%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 76.18%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 76.02%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 75.92%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 75.71%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 75.60%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 75.58%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 75.31%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 75.15%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 74.96%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 74.72%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 74.65%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 74.79%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 74.77%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.77%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 74.79%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.53%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.68%   
cur_acc:  ['0.9454', '0.7103', '0.7024', '0.7827', '0.6964']
his_acc:  ['0.9454', '0.8235', '0.7869', '0.7645', '0.7568']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  5.966897010803223 1.453460931777954 0.9708168506622314
CurrentTrain: epoch  0, batch     0 | loss: 8.3911743Losses:  5.894072532653809 1.219193935394287 0.9665285348892212
CurrentTrain: epoch  0, batch     1 | loss: 8.0797949Losses:  6.845088005065918 1.4352695941925049 0.9520028233528137
CurrentTrain: epoch  0, batch     2 | loss: 9.2323599Losses:  5.194033622741699 2.9802322387695312e-08 0.901060163974762
CurrentTrain: epoch  0, batch     3 | loss: 6.0950937Losses:  5.906428813934326 1.5483038425445557 0.9626624584197998
CurrentTrain: epoch  1, batch     0 | loss: 8.4173956Losses:  4.900913715362549 1.2274185419082642 0.946999192237854
CurrentTrain: epoch  1, batch     1 | loss: 7.0753312Losses:  4.146501541137695 1.2583703994750977 0.9550508260726929
CurrentTrain: epoch  1, batch     2 | loss: 6.3599229Losses:  4.696113586425781 0.3591970205307007 0.9394989013671875
CurrentTrain: epoch  1, batch     3 | loss: 5.9948096Losses:  4.334125995635986 1.169900894165039 0.9369444251060486
CurrentTrain: epoch  2, batch     0 | loss: 6.4409714Losses:  5.023970603942871 1.2736625671386719 0.949688196182251
CurrentTrain: epoch  2, batch     1 | loss: 7.2473211Losses:  3.9682161808013916 1.2365657091140747 0.967771589756012
CurrentTrain: epoch  2, batch     2 | loss: 6.1725535Losses:  4.542456150054932 0.3671877384185791 0.9284133911132812
CurrentTrain: epoch  2, batch     3 | loss: 5.8380575Losses:  3.5024707317352295 1.159416913986206 0.9640797972679138
CurrentTrain: epoch  3, batch     0 | loss: 5.6259675Losses:  4.587180137634277 1.320106029510498 0.9472241401672363
CurrentTrain: epoch  3, batch     1 | loss: 6.8545103Losses:  4.246241569519043 1.3467730283737183 0.9404703378677368
CurrentTrain: epoch  3, batch     2 | loss: 6.5334849Losses:  5.297881126403809 0.2122163623571396 0.8929351568222046
CurrentTrain: epoch  3, batch     3 | loss: 6.4030328Losses:  3.258127212524414 0.9616612195968628 0.9328269958496094
CurrentTrain: epoch  4, batch     0 | loss: 5.1526155Losses:  4.783474922180176 1.0639349222183228 0.9459555149078369
CurrentTrain: epoch  4, batch     1 | loss: 6.7933655Losses:  3.824997901916504 1.1769746541976929 0.951614499092102
CurrentTrain: epoch  4, batch     2 | loss: 5.9535871Losses:  2.6278557777404785 0.397344708442688 0.9720810651779175
CurrentTrain: epoch  4, batch     3 | loss: 3.9972816Losses:  3.36596941947937 1.134016513824463 0.9397017955780029
CurrentTrain: epoch  5, batch     0 | loss: 5.4396877Losses:  4.164831161499023 0.9088963270187378 0.9626899361610413
CurrentTrain: epoch  5, batch     1 | loss: 6.0364175Losses:  3.1426894664764404 1.126878261566162 0.9269393086433411
CurrentTrain: epoch  5, batch     2 | loss: 5.1965070Losses:  3.351762294769287 0.14148953557014465 0.8631700873374939
CurrentTrain: epoch  5, batch     3 | loss: 4.3564219Losses:  2.6416447162628174 0.9875562787055969 0.9399797916412354
CurrentTrain: epoch  6, batch     0 | loss: 4.5691805Losses:  3.5847854614257812 1.1372008323669434 0.9301673173904419
CurrentTrain: epoch  6, batch     1 | loss: 5.6521535Losses:  3.594475746154785 1.1705069541931152 0.9381450414657593
CurrentTrain: epoch  6, batch     2 | loss: 5.7031279Losses:  4.0239081382751465 0.028446365147829056 0.936157763004303
CurrentTrain: epoch  6, batch     3 | loss: 4.9885120Losses:  3.1749207973480225 0.9851069450378418 0.9255862236022949
CurrentTrain: epoch  7, batch     0 | loss: 5.0856137Losses:  3.1036453247070312 1.011109709739685 0.9395890235900879
CurrentTrain: epoch  7, batch     1 | loss: 5.0543442Losses:  2.680016040802002 0.8266539573669434 0.922469973564148
CurrentTrain: epoch  7, batch     2 | loss: 4.4291401Losses:  3.7215821743011475 0.1546095758676529 0.9624538421630859
CurrentTrain: epoch  7, batch     3 | loss: 4.8386459Losses:  2.656825065612793 0.6976234912872314 0.9252332448959351
CurrentTrain: epoch  8, batch     0 | loss: 4.2796817Losses:  2.4429068565368652 0.8198105096817017 0.9216724634170532
CurrentTrain: epoch  8, batch     1 | loss: 4.1843896Losses:  3.43709135055542 1.0234203338623047 0.9377455711364746
CurrentTrain: epoch  8, batch     2 | loss: 5.3982573Losses:  1.9871140718460083 0.060891516506671906 0.9336721897125244
CurrentTrain: epoch  8, batch     3 | loss: 2.9816778Losses:  2.3821756839752197 0.813464879989624 0.9332897663116455
CurrentTrain: epoch  9, batch     0 | loss: 4.1289301Losses:  2.4428207874298096 0.8051893711090088 0.9308163523674011
CurrentTrain: epoch  9, batch     1 | loss: 4.1788263Losses:  2.99196720123291 0.8704838752746582 0.9228459000587463
CurrentTrain: epoch  9, batch     2 | loss: 4.7852969Losses:  5.22865104675293 0.4220414459705353 0.8886967897415161
CurrentTrain: epoch  9, batch     3 | loss: 6.5393891
Losses:  0.6353126168251038 0.8934178352355957 0.9194449782371521
MemoryTrain:  epoch  0, batch     0 | loss: 2.4481754Losses:  0.9056302905082703 1.0625125169754028 0.9319530725479126
MemoryTrain:  epoch  0, batch     1 | loss: 2.9000959Losses:  0.48459312319755554 0.8808491230010986 0.917853593826294
MemoryTrain:  epoch  0, batch     2 | loss: 2.2832959Losses:  0.6755816340446472 0.7172650098800659 0.871729850769043
MemoryTrain:  epoch  0, batch     3 | loss: 2.2645764Losses:  1.0384962558746338 0.8686231374740601 0.9022467136383057
MemoryTrain:  epoch  1, batch     0 | loss: 2.8093662Losses:  0.6053734421730042 0.8652533292770386 0.9117000102996826
MemoryTrain:  epoch  1, batch     1 | loss: 2.3823268Losses:  0.9118883609771729 0.9561141133308411 0.9248635768890381
MemoryTrain:  epoch  1, batch     2 | loss: 2.7928660Losses:  0.3634389340877533 0.7170966863632202 0.9073063135147095
MemoryTrain:  epoch  1, batch     3 | loss: 1.9878420Losses:  0.6640241146087646 0.9229305982589722 0.9029805660247803
MemoryTrain:  epoch  2, batch     0 | loss: 2.4899354Losses:  0.15379421412944794 0.8362884521484375 0.9139530658721924
MemoryTrain:  epoch  2, batch     1 | loss: 1.9040358Losses:  0.5928329229354858 0.7533046007156372 0.918117105960846
MemoryTrain:  epoch  2, batch     2 | loss: 2.2642546Losses:  0.6221188306808472 0.628200352191925 0.8977358341217041
MemoryTrain:  epoch  2, batch     3 | loss: 2.1480551Losses:  0.4693686366081238 0.7693158388137817 0.85256028175354
MemoryTrain:  epoch  3, batch     0 | loss: 2.0912447Losses:  0.21253909170627594 0.7635375261306763 0.9249557256698608
MemoryTrain:  epoch  3, batch     1 | loss: 1.9010323Losses:  0.5926578640937805 0.6667223572731018 0.9355988502502441
MemoryTrain:  epoch  3, batch     2 | loss: 2.1949792Losses:  0.4111367464065552 0.9198940992355347 0.9182918071746826
MemoryTrain:  epoch  3, batch     3 | loss: 2.2493227Losses:  0.3557082712650299 1.1090145111083984 0.8996257781982422
MemoryTrain:  epoch  4, batch     0 | loss: 2.3643484Losses:  0.15532946586608887 0.6104247570037842 0.9304342269897461
MemoryTrain:  epoch  4, batch     1 | loss: 1.6961884Losses:  0.09617701917886734 0.6945466995239258 0.9007758498191833
MemoryTrain:  epoch  4, batch     2 | loss: 1.6914996Losses:  0.1336008459329605 0.6079139113426208 0.8889166116714478
MemoryTrain:  epoch  4, batch     3 | loss: 1.6304314Losses:  0.1412118375301361 0.6513065099716187 0.9152516722679138
MemoryTrain:  epoch  5, batch     0 | loss: 1.7077701Losses:  0.16141225397586823 0.9659242630004883 0.9022131562232971
MemoryTrain:  epoch  5, batch     1 | loss: 2.0295496Losses:  0.08047102391719818 0.7866445779800415 0.8786674737930298
MemoryTrain:  epoch  5, batch     2 | loss: 1.7457831Losses:  0.18507739901542664 0.6967588663101196 0.9316713809967041
MemoryTrain:  epoch  5, batch     3 | loss: 1.8135077Losses:  0.11380307376384735 0.7189090251922607 0.9015167951583862
MemoryTrain:  epoch  6, batch     0 | loss: 1.7342288Losses:  0.12830424308776855 0.7997053861618042 0.9082664251327515
MemoryTrain:  epoch  6, batch     1 | loss: 1.8362761Losses:  0.05838575214147568 0.807931661605835 0.9016642570495605
MemoryTrain:  epoch  6, batch     2 | loss: 1.7679816Losses:  0.055678047239780426 0.6629194617271423 0.9015932083129883
MemoryTrain:  epoch  6, batch     3 | loss: 1.6201907Losses:  0.08225779235363007 0.7257691025733948 0.882477343082428
MemoryTrain:  epoch  7, batch     0 | loss: 1.6905043Losses:  0.07517187297344208 0.8208573460578918 0.9055067300796509
MemoryTrain:  epoch  7, batch     1 | loss: 1.8015360Losses:  0.06546605378389359 0.7134302854537964 0.912209153175354
MemoryTrain:  epoch  7, batch     2 | loss: 1.6911055Losses:  0.07752959430217743 0.6537872552871704 0.9148725271224976
MemoryTrain:  epoch  7, batch     3 | loss: 1.6461895Losses:  0.027229055762290955 0.728691816329956 0.8872285485267639
MemoryTrain:  epoch  8, batch     0 | loss: 1.6431494Losses:  0.054472941905260086 0.6696509122848511 0.9156267642974854
MemoryTrain:  epoch  8, batch     1 | loss: 1.6397506Losses:  0.03558046370744705 0.8659420013427734 0.874860405921936
MemoryTrain:  epoch  8, batch     2 | loss: 1.7763829Losses:  0.18018630146980286 0.6368901133537292 0.9408539533615112
MemoryTrain:  epoch  8, batch     3 | loss: 1.7579304Losses:  0.09199132025241852 0.6642993688583374 0.877974271774292
MemoryTrain:  epoch  9, batch     0 | loss: 1.6342649Losses:  0.10101980715990067 0.8913029432296753 0.8726373910903931
MemoryTrain:  epoch  9, batch     1 | loss: 1.8649602Losses:  0.04072455316781998 0.7736843228340149 0.9493379592895508
MemoryTrain:  epoch  9, batch     2 | loss: 1.7637469Losses:  0.0447578951716423 0.5501999855041504 0.9048509001731873
MemoryTrain:  epoch  9, batch     3 | loss: 1.4998088
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 71.79%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 70.91%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 70.37%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 70.24%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 89.35%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 88.50%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 86.97%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.68%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 86.49%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 86.41%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 86.23%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 85.82%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 85.85%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 86.47%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 86.43%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 86.04%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.98%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.84%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.70%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 85.57%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 84.79%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 84.30%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 83.82%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 83.43%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 82.69%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.46%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 82.16%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 81.81%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 81.66%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 81.39%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 81.18%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 80.72%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 80.26%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 79.82%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 79.64%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 79.15%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 78.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 78.22%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 77.85%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 77.70%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 77.65%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 77.34%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.20%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 75.62%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 75.17%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 74.78%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 74.56%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 75.88%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 75.91%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 75.81%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 75.65%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 75.50%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 75.15%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 74.80%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 74.47%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 74.28%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.91%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 74.86%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 74.33%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 73.84%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 73.36%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 72.84%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 72.38%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 72.76%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 72.49%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 72.18%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 71.65%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.23%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.26%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 72.57%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.29%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 72.31%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 72.08%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 72.05%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.39%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 72.82%   [EVAL] batch:  195 | acc: 31.25%,  total acc: 72.61%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 72.53%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 72.35%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 72.14%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 71.97%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 72.00%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 71.91%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 71.81%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:  207 | acc: 56.25%,  total acc: 71.72%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 71.46%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 71.33%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 71.14%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.13%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 72.04%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 71.94%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 73.40%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 73.43%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 73.46%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.52%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 73.57%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 73.70%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 73.76%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 73.98%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 74.03%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 73.99%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 73.95%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 73.88%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 73.89%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 73.87%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.68%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 73.58%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 73.49%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 73.31%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 73.24%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 73.13%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 73.14%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.22%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 73.59%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 73.47%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 73.38%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 73.32%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 73.19%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 73.09%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 73.03%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 72.95%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 72.71%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 72.52%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 72.31%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 72.06%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 72.15%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 72.03%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 71.96%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 71.82%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 71.66%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 71.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 72.45%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 72.52%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 72.37%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 72.20%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 72.03%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 71.90%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 71.82%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 71.69%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.87%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 71.96%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 71.75%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 71.67%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 71.64%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 71.61%   [EVAL] batch:  330 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 72.13%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.44%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 72.52%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 72.39%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 72.33%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 72.23%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 72.06%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 71.98%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 72.42%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 72.39%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 72.30%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 72.24%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 72.21%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 72.29%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 72.28%   
cur_acc:  ['0.9454', '0.7103', '0.7024', '0.7827', '0.6964', '0.7024']
his_acc:  ['0.9454', '0.8235', '0.7869', '0.7645', '0.7568', '0.7228']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  5.736159324645996 1.3393398523330688 0.9658651947975159
CurrentTrain: epoch  0, batch     0 | loss: 8.0413647Losses:  7.403104782104492 1.4428913593292236 0.9672937989234924
CurrentTrain: epoch  0, batch     1 | loss: 9.8132896Losses:  6.64803409576416 1.6920684576034546 0.9480504393577576
CurrentTrain: epoch  0, batch     2 | loss: 9.2881527Losses:  8.121692657470703 0.3754848837852478 0.9526796340942383
CurrentTrain: epoch  0, batch     3 | loss: 9.4498568Losses:  5.806744575500488 1.7220957279205322 0.9623258113861084
CurrentTrain: epoch  1, batch     0 | loss: 8.4911661Losses:  5.693459510803223 1.3747258186340332 0.9464300274848938
CurrentTrain: epoch  1, batch     1 | loss: 8.0146151Losses:  5.149930477142334 1.6658341884613037 0.959296703338623
CurrentTrain: epoch  1, batch     2 | loss: 7.7750611Losses:  6.501960754394531 0.35088855028152466 0.9622994661331177
CurrentTrain: epoch  1, batch     3 | loss: 7.8151488Losses:  4.934506893157959 1.5001659393310547 0.9460334777832031
CurrentTrain: epoch  2, batch     0 | loss: 7.3807063Losses:  4.612379550933838 1.3317437171936035 0.9452444314956665
CurrentTrain: epoch  2, batch     1 | loss: 6.8893676Losses:  4.8278093338012695 1.7262613773345947 0.9532663822174072
CurrentTrain: epoch  2, batch     2 | loss: 7.5073366Losses:  6.531661033630371 0.2976185381412506 0.9793502688407898
CurrentTrain: epoch  2, batch     3 | loss: 7.8086295Losses:  4.646414279937744 1.3812388181686401 0.9448886513710022
CurrentTrain: epoch  3, batch     0 | loss: 6.9725418Losses:  4.524331569671631 1.5049266815185547 0.9507118463516235
CurrentTrain: epoch  3, batch     1 | loss: 6.9799700Losses:  3.86004638671875 1.3141924142837524 0.9408758878707886
CurrentTrain: epoch  3, batch     2 | loss: 6.1151147Losses:  4.9077558517456055 0.3671143651008606 0.8938474655151367
CurrentTrain: epoch  3, batch     3 | loss: 6.1687179Losses:  4.615701675415039 1.520881175994873 0.9356532096862793
CurrentTrain: epoch  4, batch     0 | loss: 7.0722361Losses:  3.3991518020629883 1.1842260360717773 0.9382123947143555
CurrentTrain: epoch  4, batch     1 | loss: 5.5215902Losses:  4.084896564483643 1.4222773313522339 0.9375900030136108
CurrentTrain: epoch  4, batch     2 | loss: 6.4447641Losses:  6.630744934082031 0.3699699938297272 1.007645845413208
CurrentTrain: epoch  4, batch     3 | loss: 8.0083609Losses:  4.109149932861328 1.122693657875061 0.9666309356689453
CurrentTrain: epoch  5, batch     0 | loss: 6.1984744Losses:  3.737022876739502 1.2838329076766968 0.9136437773704529
CurrentTrain: epoch  5, batch     1 | loss: 5.9344997Losses:  3.782209873199463 1.425645112991333 0.9251622557640076
CurrentTrain: epoch  5, batch     2 | loss: 6.1330175Losses:  3.7495498657226562 0.4780389368534088 0.9621536731719971
CurrentTrain: epoch  5, batch     3 | loss: 5.1897421Losses:  3.907259941101074 1.2913188934326172 0.9340460896492004
CurrentTrain: epoch  6, batch     0 | loss: 6.1326251Losses:  4.19816255569458 1.144658088684082 0.9364212155342102
CurrentTrain: epoch  6, batch     1 | loss: 6.2792420Losses:  2.778688430786133 0.9788450002670288 0.9266754388809204
CurrentTrain: epoch  6, batch     2 | loss: 4.6842089Losses:  4.999894142150879 0.36491626501083374 0.9423524737358093
CurrentTrain: epoch  6, batch     3 | loss: 6.3071628Losses:  2.882819414138794 1.1137704849243164 0.9335048198699951
CurrentTrain: epoch  7, batch     0 | loss: 4.9300947Losses:  3.90280818939209 1.1796646118164062 0.9292142391204834
CurrentTrain: epoch  7, batch     1 | loss: 6.0116873Losses:  3.226508140563965 1.3189306259155273 0.9242017269134521
CurrentTrain: epoch  7, batch     2 | loss: 5.4696407Losses:  4.665565490722656 0.9323441982269287 0.969472348690033
CurrentTrain: epoch  7, batch     3 | loss: 6.5673823Losses:  3.2716639041900635 1.2530639171600342 0.9123277068138123
CurrentTrain: epoch  8, batch     0 | loss: 5.4370556Losses:  3.1059186458587646 0.9804849028587341 0.9320285320281982
CurrentTrain: epoch  8, batch     1 | loss: 5.0184317Losses:  3.1441268920898438 1.3378480672836304 0.9330927729606628
CurrentTrain: epoch  8, batch     2 | loss: 5.4150677Losses:  2.681351661682129 0.31417039036750793 0.9826686978340149
CurrentTrain: epoch  8, batch     3 | loss: 3.9781907Losses:  2.7484970092773438 1.282205581665039 0.906749427318573
CurrentTrain: epoch  9, batch     0 | loss: 4.9374518Losses:  3.4581751823425293 1.212315559387207 0.9129111766815186
CurrentTrain: epoch  9, batch     1 | loss: 5.5834017Losses:  2.907339572906494 1.1578724384307861 0.9494093060493469
CurrentTrain: epoch  9, batch     2 | loss: 5.0146217Losses:  2.0099411010742188 0.5630797743797302 0.9475567936897278
CurrentTrain: epoch  9, batch     3 | loss: 3.5205777
Losses:  0.6905776858329773 0.8203526139259338 0.9148030281066895
MemoryTrain:  epoch  0, batch     0 | loss: 2.4257333Losses:  0.2822704613208771 1.005479097366333 0.8843970894813538
MemoryTrain:  epoch  0, batch     1 | loss: 2.1721466Losses:  1.0362505912780762 1.0433913469314575 0.9454706311225891
MemoryTrain:  epoch  0, batch     2 | loss: 3.0251124Losses:  0.44528818130493164 0.9076536297798157 0.9233708381652832
MemoryTrain:  epoch  0, batch     3 | loss: 2.2763126Losses:  0.06432424485683441 0.23264500498771667 0.7948808073997498
MemoryTrain:  epoch  0, batch     4 | loss: 1.0918500Losses:  0.6751944422721863 0.655427098274231 0.9458804726600647
MemoryTrain:  epoch  1, batch     0 | loss: 2.2765019Losses:  0.8857591152191162 0.7716195583343506 0.9324659109115601
MemoryTrain:  epoch  1, batch     1 | loss: 2.5898447Losses:  0.5719843506813049 0.8987513780593872 0.8954738974571228
MemoryTrain:  epoch  1, batch     2 | loss: 2.3662097Losses:  0.9149308204650879 0.9462289810180664 0.8546545505523682
MemoryTrain:  epoch  1, batch     3 | loss: 2.7158144Losses:  0.1179996132850647 0.5406953692436218 0.8616120219230652
MemoryTrain:  epoch  1, batch     4 | loss: 1.5203071Losses:  0.4040377140045166 0.720379114151001 0.8673908710479736
MemoryTrain:  epoch  2, batch     0 | loss: 1.9918077Losses:  0.25187087059020996 0.8114320635795593 0.9193808436393738
MemoryTrain:  epoch  2, batch     1 | loss: 1.9826839Losses:  0.10006354749202728 0.9150602221488953 0.8892566561698914
MemoryTrain:  epoch  2, batch     2 | loss: 1.9043803Losses:  0.45576202869415283 0.9916000366210938 0.9296124577522278
MemoryTrain:  epoch  2, batch     3 | loss: 2.3769746Losses:  0.5910401940345764 0.3984810709953308 0.922179102897644
MemoryTrain:  epoch  2, batch     4 | loss: 1.9117004Losses:  0.0813617929816246 0.9352359771728516 0.8723958730697632
MemoryTrain:  epoch  3, batch     0 | loss: 1.8889936Losses:  0.316162109375 0.7777988314628601 0.8852397203445435
MemoryTrain:  epoch  3, batch     1 | loss: 1.9792007Losses:  0.1472271978855133 0.710034191608429 0.9232910871505737
MemoryTrain:  epoch  3, batch     2 | loss: 1.7805525Losses:  0.34811922907829285 0.9043411016464233 0.901561439037323
MemoryTrain:  epoch  3, batch     3 | loss: 2.1540217Losses:  0.1674930453300476 0.4165865182876587 0.9828311204910278
MemoryTrain:  epoch  3, batch     4 | loss: 1.5669107Losses:  0.08772900700569153 0.7289073467254639 0.9080547094345093
MemoryTrain:  epoch  4, batch     0 | loss: 1.7246910Losses:  0.33684831857681274 0.7356826066970825 0.9037142992019653
MemoryTrain:  epoch  4, batch     1 | loss: 1.9762453Losses:  0.10639487951993942 0.7151645421981812 0.9214800596237183
MemoryTrain:  epoch  4, batch     2 | loss: 1.7430395Losses:  0.0995587483048439 1.0252299308776855 0.8916397094726562
MemoryTrain:  epoch  4, batch     3 | loss: 2.0164285Losses:  0.10487189143896103 0.5203635096549988 0.8491219282150269
MemoryTrain:  epoch  4, batch     4 | loss: 1.4743574Losses:  0.08227938413619995 0.8053779602050781 0.9000831842422485
MemoryTrain:  epoch  5, batch     0 | loss: 1.7877405Losses:  0.14179188013076782 0.8297291994094849 0.8838638663291931
MemoryTrain:  epoch  5, batch     1 | loss: 1.8553849Losses:  0.041830435395240784 0.8409602642059326 0.9401048421859741
MemoryTrain:  epoch  5, batch     2 | loss: 1.8228955Losses:  0.06144562363624573 0.8873765468597412 0.9092850685119629
MemoryTrain:  epoch  5, batch     3 | loss: 1.8581072Losses:  0.04007526487112045 0.19362443685531616 0.820947527885437
MemoryTrain:  epoch  5, batch     4 | loss: 1.0546472Losses:  0.06133667379617691 0.811220109462738 0.8971757888793945
MemoryTrain:  epoch  6, batch     0 | loss: 1.7697326Losses:  0.07203823328018188 0.6748269200325012 0.9038764238357544
MemoryTrain:  epoch  6, batch     1 | loss: 1.6507416Losses:  0.09521983563899994 0.8088469505310059 0.8697996139526367
MemoryTrain:  epoch  6, batch     2 | loss: 1.7738664Losses:  0.07195088267326355 0.7688724994659424 0.9131832122802734
MemoryTrain:  epoch  6, batch     3 | loss: 1.7540066Losses:  0.04165182262659073 0.3376550078392029 0.945279598236084
MemoryTrain:  epoch  6, batch     4 | loss: 1.3245864Losses:  0.03612320125102997 0.7261862754821777 0.8984999060630798
MemoryTrain:  epoch  7, batch     0 | loss: 1.6608094Losses:  0.05634745955467224 0.8852795958518982 0.858852744102478
MemoryTrain:  epoch  7, batch     1 | loss: 1.8004798Losses:  0.06336913257837296 0.74204021692276 0.9091243743896484
MemoryTrain:  epoch  7, batch     2 | loss: 1.7145338Losses:  0.08745496720075607 0.6822460293769836 0.925723671913147
MemoryTrain:  epoch  7, batch     3 | loss: 1.6954247Losses:  0.037068791687488556 0.1938878297805786 0.898786187171936
MemoryTrain:  epoch  7, batch     4 | loss: 1.1297429Losses:  0.04138202965259552 0.6426528692245483 0.9343793392181396
MemoryTrain:  epoch  8, batch     0 | loss: 1.6184142Losses:  0.05904920771718025 0.8976026177406311 0.8589545488357544
MemoryTrain:  epoch  8, batch     1 | loss: 1.8156064Losses:  0.07381393015384674 0.856760561466217 0.8901251554489136
MemoryTrain:  epoch  8, batch     2 | loss: 1.8206997Losses:  0.10339902341365814 0.6311579942703247 0.8928782939910889
MemoryTrain:  epoch  8, batch     3 | loss: 1.6274353Losses:  0.05139167234301567 0.3074738681316376 0.9125006198883057
MemoryTrain:  epoch  8, batch     4 | loss: 1.2713661Losses:  0.08850999176502228 0.794851541519165 0.8605899214744568
MemoryTrain:  epoch  9, batch     0 | loss: 1.7439514Losses:  0.05688817426562309 0.6713109016418457 0.9222185611724854
MemoryTrain:  epoch  9, batch     1 | loss: 1.6504176Losses:  0.07995056360960007 0.7947690486907959 0.8734533786773682
MemoryTrain:  epoch  9, batch     2 | loss: 1.7481730Losses:  0.047416020184755325 0.694657564163208 0.8907291889190674
MemoryTrain:  epoch  9, batch     3 | loss: 1.6328027Losses:  0.03380005061626434 0.27345800399780273 0.9683425426483154
MemoryTrain:  epoch  9, batch     4 | loss: 1.2756007
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 56.52%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 53.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 51.68%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 49.77%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.99%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 46.34%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 44.79%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 43.35%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 43.95%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 45.27%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 45.96%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 47.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 48.26%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 48.99%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 50.16%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 51.12%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 53.51%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 54.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 55.38%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 55.69%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 55.30%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 54.79%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 54.30%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 54.59%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 53.87%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 54.29%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 54.93%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 55.42%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 56.13%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 56.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 57.03%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 57.13%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 56.79%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 56.78%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 57.08%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 56.97%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 57.26%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 56.75%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 87.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 86.82%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 86.29%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 85.67%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 85.38%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.35%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 85.18%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 85.12%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 84.33%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 84.28%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 84.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.05%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.95%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 84.50%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 84.21%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 83.91%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 83.64%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 83.38%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 82.98%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 82.66%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 82.06%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 81.83%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 81.03%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 80.76%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 80.16%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 79.77%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 79.39%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 78.88%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 78.39%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.22%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 77.74%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 77.53%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 77.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 76.73%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 76.53%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 76.33%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.20%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 76.01%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.58%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 75.06%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.54%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 74.03%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 73.59%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.21%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 74.39%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 74.20%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 74.01%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 73.67%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 73.34%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 73.03%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 72.81%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.29%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 73.46%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 72.93%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 72.46%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 71.99%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 71.48%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 71.02%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 70.70%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 70.86%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 71.40%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 71.09%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 70.71%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.16%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 69.79%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 69.75%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 70.25%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 69.74%   [EVAL] batch:  164 | acc: 6.25%,  total acc: 69.36%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 68.98%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 68.60%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 68.27%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 68.01%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 68.05%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 67.70%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 67.70%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 67.74%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 67.78%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 68.94%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 68.65%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 68.31%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 68.16%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 68.04%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 67.83%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 67.78%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 68.08%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 68.00%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 69.55%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 69.52%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 69.54%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 70.16%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 70.13%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 70.02%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.94%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 69.86%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:  265 | acc: 12.50%,  total acc: 69.57%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 69.43%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 69.88%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 69.83%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 69.80%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 69.69%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 69.23%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 69.06%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.86%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 68.62%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 68.53%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 68.51%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 68.31%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 68.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 68.99%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 69.03%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 68.85%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 68.73%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 68.53%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.77%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 68.62%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 68.52%   [EVAL] batch:  328 | acc: 43.75%,  total acc: 68.45%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 68.41%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 68.32%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 68.97%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 69.17%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 69.52%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 69.39%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 69.12%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.39%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 69.35%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 69.26%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.21%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 69.07%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 69.07%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 69.07%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 68.78%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 68.63%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 68.55%   [EVAL] batch:  379 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 68.32%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 68.54%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 68.69%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 68.81%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  395 | acc: 0.00%,  total acc: 68.51%   [EVAL] batch:  396 | acc: 12.50%,  total acc: 68.37%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 68.29%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 68.15%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 68.05%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 67.89%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 67.72%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 67.56%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 67.39%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 67.22%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 67.06%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 67.10%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 67.59%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 67.43%   [EVAL] batch:  422 | acc: 31.25%,  total acc: 67.35%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 67.35%   [EVAL] batch:  424 | acc: 18.75%,  total acc: 67.24%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  432 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 67.35%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 67.37%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 67.33%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 67.35%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 67.25%   
cur_acc:  ['0.9454', '0.7103', '0.7024', '0.7827', '0.6964', '0.7024', '0.5675']
his_acc:  ['0.9454', '0.8235', '0.7869', '0.7645', '0.7568', '0.7228', '0.6725']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  5.998672962188721 1.5417263507843018 0.9793701171875
CurrentTrain: epoch  0, batch     0 | loss: 8.5197697Losses:  6.162166118621826 1.4655177593231201 0.952938973903656
CurrentTrain: epoch  0, batch     1 | loss: 8.5806227Losses:  5.7863359451293945 1.2802678346633911 0.975169837474823
CurrentTrain: epoch  0, batch     2 | loss: 8.0417738Losses:  5.810452938079834 0.25630027055740356 1.0
CurrentTrain: epoch  0, batch     3 | loss: 7.0667534Losses:  5.3887434005737305 1.0528602600097656 0.9924413561820984
CurrentTrain: epoch  1, batch     0 | loss: 7.4340448Losses:  5.224734306335449 1.3400485515594482 0.9611713886260986
CurrentTrain: epoch  1, batch     1 | loss: 7.5259542Losses:  5.371181964874268 1.194745421409607 0.936477780342102
CurrentTrain: epoch  1, batch     2 | loss: 7.5024052Losses:  2.771843433380127 8.94069742685133e-08 1.0
CurrentTrain: epoch  1, batch     3 | loss: 3.7718434Losses:  4.622096538543701 1.1774572134017944 0.941931962966919
CurrentTrain: epoch  2, batch     0 | loss: 6.7414856Losses:  5.126730918884277 1.10628080368042 0.9854562878608704
CurrentTrain: epoch  2, batch     1 | loss: 7.2184682Losses:  4.650938034057617 1.0481549501419067 0.9522137641906738
CurrentTrain: epoch  2, batch     2 | loss: 6.6513066Losses:  3.424365758895874 0.33578628301620483 1.0
CurrentTrain: epoch  2, batch     3 | loss: 4.7601519Losses:  4.599858283996582 1.1978965997695923 0.9527230262756348
CurrentTrain: epoch  3, batch     0 | loss: 6.7504778Losses:  4.276451587677002 1.0181983709335327 0.9553295373916626
CurrentTrain: epoch  3, batch     1 | loss: 6.2499795Losses:  4.066424369812012 1.050581693649292 0.9707276821136475
CurrentTrain: epoch  3, batch     2 | loss: 6.0877342Losses:  5.383851528167725 0.8538861870765686 0.9820830225944519
CurrentTrain: epoch  3, batch     3 | loss: 7.2198205Losses:  3.935490846633911 0.9744614362716675 0.9601774215698242
CurrentTrain: epoch  4, batch     0 | loss: 5.8701296Losses:  3.506397247314453 1.0152372121810913 0.95418381690979
CurrentTrain: epoch  4, batch     1 | loss: 5.4758186Losses:  4.631966590881348 1.2857539653778076 0.9592739939689636
CurrentTrain: epoch  4, batch     2 | loss: 6.8769946Losses:  6.473898887634277 0.18940949440002441 0.9643962383270264
CurrentTrain: epoch  4, batch     3 | loss: 7.6277046Losses:  3.9844462871551514 1.2319597005844116 0.9689151048660278
CurrentTrain: epoch  5, batch     0 | loss: 6.1853209Losses:  4.6325225830078125 0.8218081593513489 0.9424840211868286
CurrentTrain: epoch  5, batch     1 | loss: 6.3968148Losses:  3.5427534580230713 0.776877224445343 0.957315981388092
CurrentTrain: epoch  5, batch     2 | loss: 5.2769465Losses:  2.053834915161133 0.14763924479484558 0.9596042633056641
CurrentTrain: epoch  5, batch     3 | loss: 3.1610785Losses:  4.107515335083008 1.214904546737671 0.9640734195709229
CurrentTrain: epoch  6, batch     0 | loss: 6.2864933Losses:  3.723862648010254 0.9264446496963501 0.9534391164779663
CurrentTrain: epoch  6, batch     1 | loss: 5.6037464Losses:  3.0309410095214844 0.6749539971351624 0.9484224319458008
CurrentTrain: epoch  6, batch     2 | loss: 4.6543174Losses:  2.596991539001465 0.33237868547439575 0.9617252945899963
CurrentTrain: epoch  6, batch     3 | loss: 3.8910954Losses:  3.479897975921631 0.9594079256057739 0.9436451196670532
CurrentTrain: epoch  7, batch     0 | loss: 5.3829508Losses:  3.342268228530884 0.770359992980957 0.9581074118614197
CurrentTrain: epoch  7, batch     1 | loss: 5.0707355Losses:  3.1713294982910156 1.0241655111312866 0.954997181892395
CurrentTrain: epoch  7, batch     2 | loss: 5.1504922Losses:  2.47458815574646 0.1189049631357193 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.5934932Losses:  3.500593900680542 0.938628613948822 0.9256381988525391
CurrentTrain: epoch  8, batch     0 | loss: 5.3648605Losses:  3.032078742980957 0.7790959477424622 0.9818336963653564
CurrentTrain: epoch  8, batch     1 | loss: 4.7930083Losses:  2.531038761138916 0.8255326151847839 0.9430627822875977
CurrentTrain: epoch  8, batch     2 | loss: 4.2996340Losses:  3.2472620010375977 0.13832402229309082 0.941243588924408
CurrentTrain: epoch  8, batch     3 | loss: 4.3268294Losses:  2.321119546890259 0.6529679298400879 0.9460177421569824
CurrentTrain: epoch  9, batch     0 | loss: 3.9201052Losses:  2.9008846282958984 1.0385507345199585 0.9418613910675049
CurrentTrain: epoch  9, batch     1 | loss: 4.8812971Losses:  2.808410882949829 0.8349117040634155 0.9536040425300598
CurrentTrain: epoch  9, batch     2 | loss: 4.5969267Losses:  2.6860806941986084 0.14278942346572876 0.9590861797332764
CurrentTrain: epoch  9, batch     3 | loss: 3.7879562
Losses:  0.27923154830932617 0.7187081575393677 0.8498636484146118
MemoryTrain:  epoch  0, batch     0 | loss: 1.8478034Losses:  0.42830678820610046 0.9184004068374634 0.96469646692276
MemoryTrain:  epoch  0, batch     1 | loss: 2.3114038Losses:  0.4268173575401306 1.0632588863372803 0.8764128684997559
MemoryTrain:  epoch  0, batch     2 | loss: 2.3664892Losses:  0.11572927981615067 0.859999418258667 0.8636655807495117
MemoryTrain:  epoch  0, batch     3 | loss: 1.8393943Losses:  0.4652485251426697 0.7832002639770508 0.9445274472236633
MemoryTrain:  epoch  0, batch     4 | loss: 2.1929762Losses:  1.1479852199554443 0.7152745127677917 0.9136995077133179
MemoryTrain:  epoch  1, batch     0 | loss: 2.7769594Losses:  0.38835588097572327 0.7118430137634277 0.9470206499099731
MemoryTrain:  epoch  1, batch     1 | loss: 2.0472195Losses:  0.29010552167892456 0.7690396308898926 0.9230237603187561
MemoryTrain:  epoch  1, batch     2 | loss: 1.9821689Losses:  0.5287629961967468 0.9174264073371887 0.8778703212738037
MemoryTrain:  epoch  1, batch     3 | loss: 2.3240597Losses:  0.44156283140182495 0.7878641486167908 0.8356000781059265
MemoryTrain:  epoch  1, batch     4 | loss: 2.0650270Losses:  0.3750205636024475 0.7692716121673584 0.9039149284362793
MemoryTrain:  epoch  2, batch     0 | loss: 2.0482070Losses:  0.3934783935546875 0.9381263256072998 0.8987960815429688
MemoryTrain:  epoch  2, batch     1 | loss: 2.2304008Losses:  0.2697571814060211 0.7963587045669556 0.9143531322479248
MemoryTrain:  epoch  2, batch     2 | loss: 1.9804690Losses:  0.1278235763311386 0.8216531276702881 0.8691225051879883
MemoryTrain:  epoch  2, batch     3 | loss: 1.8185992Losses:  0.09584562480449677 0.5630675554275513 0.9132094383239746
MemoryTrain:  epoch  2, batch     4 | loss: 1.5721226Losses:  0.37882983684539795 0.8334548473358154 0.8861104249954224
MemoryTrain:  epoch  3, batch     0 | loss: 2.0983951Losses:  0.367225706577301 0.8258718848228455 0.8458714485168457
MemoryTrain:  epoch  3, batch     1 | loss: 2.0389690Losses:  0.12401383370161057 0.6126559376716614 0.9075018167495728
MemoryTrain:  epoch  3, batch     2 | loss: 1.6441716Losses:  0.20588013529777527 0.7120285034179688 0.9156069755554199
MemoryTrain:  epoch  3, batch     3 | loss: 1.8335156Losses:  0.17064182460308075 0.7254851460456848 0.9331333041191101
MemoryTrain:  epoch  3, batch     4 | loss: 1.8292603Losses:  0.09672736376523972 0.7090703248977661 0.8805327415466309
MemoryTrain:  epoch  4, batch     0 | loss: 1.6863304Losses:  0.09256678074598312 0.6671302318572998 0.8443148136138916
MemoryTrain:  epoch  4, batch     1 | loss: 1.6040118Losses:  0.2479741871356964 0.6331126093864441 0.907701313495636
MemoryTrain:  epoch  4, batch     2 | loss: 1.7887881Losses:  0.1068907082080841 0.7968915700912476 0.9434843063354492
MemoryTrain:  epoch  4, batch     3 | loss: 1.8472666Losses:  0.13428108394145966 0.8905688524246216 0.9020979404449463
MemoryTrain:  epoch  4, batch     4 | loss: 1.9269478Losses:  0.08278773725032806 0.48724329471588135 0.9046977758407593
MemoryTrain:  epoch  5, batch     0 | loss: 1.4747288Losses:  0.06836913526058197 0.8219774961471558 0.8599685430526733
MemoryTrain:  epoch  5, batch     1 | loss: 1.7503152Losses:  0.07166358828544617 0.7851240634918213 0.8858105540275574
MemoryTrain:  epoch  5, batch     2 | loss: 1.7425983Losses:  0.09575359523296356 0.79503333568573 0.9038673639297485
MemoryTrain:  epoch  5, batch     3 | loss: 1.7946544Losses:  0.0883478969335556 0.75958251953125 0.9318956732749939
MemoryTrain:  epoch  5, batch     4 | loss: 1.7798262Losses:  0.10833848267793655 0.7116187810897827 0.9202061891555786
MemoryTrain:  epoch  6, batch     0 | loss: 1.7401634Losses:  0.055249541997909546 0.5683444142341614 0.8709108829498291
MemoryTrain:  epoch  6, batch     1 | loss: 1.4945048Losses:  0.05807805061340332 0.7358295917510986 0.9360876083374023
MemoryTrain:  epoch  6, batch     2 | loss: 1.7299953Losses:  0.09206235408782959 0.8782721757888794 0.8555387258529663
MemoryTrain:  epoch  6, batch     3 | loss: 1.8258733Losses:  0.045101065188646317 0.7196370363235474 0.8938144445419312
MemoryTrain:  epoch  6, batch     4 | loss: 1.6585525Losses:  0.05710206180810928 0.8472093343734741 0.8372929096221924
MemoryTrain:  epoch  7, batch     0 | loss: 1.7416043Losses:  0.05810306966304779 0.704094648361206 0.9153646230697632
MemoryTrain:  epoch  7, batch     1 | loss: 1.6775624Losses:  0.0452328696846962 0.6570557355880737 0.9159038066864014
MemoryTrain:  epoch  7, batch     2 | loss: 1.6181924Losses:  0.045384012162685394 0.850104808807373 0.903191089630127
MemoryTrain:  epoch  7, batch     3 | loss: 1.7986798Losses:  0.06167040392756462 0.502092719078064 0.9041846990585327
MemoryTrain:  epoch  7, batch     4 | loss: 1.4679478Losses:  0.06826505064964294 0.7744215726852417 0.9550551176071167
MemoryTrain:  epoch  8, batch     0 | loss: 1.7977418Losses:  0.0373045913875103 0.7043390274047852 0.8259648680686951
MemoryTrain:  epoch  8, batch     1 | loss: 1.5676085Losses:  0.08791395276784897 0.6168413162231445 0.9452067017555237
MemoryTrain:  epoch  8, batch     2 | loss: 1.6499619Losses:  0.04509696736931801 0.7013081908226013 0.8776812553405762
MemoryTrain:  epoch  8, batch     3 | loss: 1.6240864Losses:  0.04373481124639511 0.6840115189552307 0.8681754469871521
MemoryTrain:  epoch  8, batch     4 | loss: 1.5959218Losses:  0.07006306201219559 0.641229510307312 0.8924539089202881
MemoryTrain:  epoch  9, batch     0 | loss: 1.6037464Losses:  0.07253403216600418 0.6121183633804321 0.8863229155540466
MemoryTrain:  epoch  9, batch     1 | loss: 1.5709753Losses:  0.07690633088350296 0.9366806149482727 0.8868158459663391
MemoryTrain:  epoch  9, batch     2 | loss: 1.9004028Losses:  0.05121779441833496 0.5633695125579834 0.9286856055259705
MemoryTrain:  epoch  9, batch     3 | loss: 1.5432730Losses:  0.05939333140850067 0.6892041563987732 0.8656109571456909
MemoryTrain:  epoch  9, batch     4 | loss: 1.6142085
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 63.99%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 60.16%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 73.24%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 73.01%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 70.65%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 69.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 71.32%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 70.55%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 69.15%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.57%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 86.16%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 85.53%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 84.81%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 83.96%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.94%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 81.93%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 80.77%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 79.83%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 79.10%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 78.22%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 77.63%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 78.87%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.49%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 77.71%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 77.23%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 76.69%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 76.45%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 75.72%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 75.21%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 75.07%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 74.80%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 74.46%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 74.07%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 73.75%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 73.37%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 72.90%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 72.79%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 72.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 71.97%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 71.67%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 71.12%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 70.64%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 70.11%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 69.65%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 69.25%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 69.08%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 70.68%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 70.80%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 70.87%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 70.80%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 70.59%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 70.28%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 70.02%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 69.91%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 69.76%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 69.61%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.74%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 68.84%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.35%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 67.92%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.62%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 68.46%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 68.13%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 67.61%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 67.18%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 66.79%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 66.76%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 67.37%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:  164 | acc: 6.25%,  total acc: 66.63%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 66.27%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 65.55%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 65.35%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 65.55%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 65.50%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 65.34%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 65.15%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 64.96%   [EVAL] batch:  178 | acc: 25.00%,  total acc: 64.73%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 64.65%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 65.93%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 65.58%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 65.44%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 65.26%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 65.12%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 65.11%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 65.01%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.99%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 64.78%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 64.73%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 64.51%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 64.35%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 64.17%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.98%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 63.77%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 64.86%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 65.12%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.81%   [EVAL] batch:  238 | acc: 37.50%,  total acc: 66.68%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  241 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 66.51%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 67.10%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 67.09%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 67.00%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 67.04%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 66.84%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 66.73%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 67.08%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 67.04%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 66.98%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 66.79%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.73%   [EVAL] batch:  282 | acc: 37.50%,  total acc: 66.63%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 66.42%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 66.06%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 65.83%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 65.73%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  292 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 65.55%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 65.50%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 65.24%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 65.17%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  313 | acc: 18.75%,  total acc: 66.20%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 65.88%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 65.71%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 65.48%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 65.46%   [EVAL] batch:  321 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 65.47%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 65.33%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 64.88%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.95%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 65.68%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 66.11%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 65.93%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 65.79%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 65.68%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 65.31%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  365 | acc: 25.00%,  total acc: 65.51%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 65.46%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 65.29%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 65.29%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 65.22%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 65.13%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 64.91%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 64.86%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 64.75%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 64.65%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 65.26%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:  395 | acc: 6.25%,  total acc: 64.99%   [EVAL] batch:  396 | acc: 0.00%,  total acc: 64.83%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 64.73%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 64.60%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 64.50%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 64.35%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 64.19%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 64.04%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 63.88%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 63.72%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 63.56%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 63.62%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 63.69%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 63.73%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 63.90%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 64.29%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 64.24%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 64.22%   [EVAL] batch:  422 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 64.21%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 64.13%   [EVAL] batch:  425 | acc: 37.50%,  total acc: 64.07%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 64.14%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 64.18%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 64.21%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 64.19%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 64.15%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 64.13%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 64.15%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.12%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 64.10%   [EVAL] batch:  438 | acc: 31.25%,  total acc: 64.02%   [EVAL] batch:  439 | acc: 68.75%,  total acc: 64.03%   [EVAL] batch:  440 | acc: 62.50%,  total acc: 64.03%   [EVAL] batch:  441 | acc: 37.50%,  total acc: 63.97%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 63.95%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 63.89%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 63.94%   [EVAL] batch:  446 | acc: 37.50%,  total acc: 63.88%   [EVAL] batch:  447 | acc: 56.25%,  total acc: 63.87%   [EVAL] batch:  448 | acc: 50.00%,  total acc: 63.84%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 63.99%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 64.02%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 64.13%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 64.16%   [EVAL] batch:  458 | acc: 25.00%,  total acc: 64.08%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 63.95%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 63.91%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 64.16%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 64.85%   [EVAL] batch:  476 | acc: 75.00%,  total acc: 64.87%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 64.85%   [EVAL] batch:  478 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 64.92%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 64.89%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 64.82%   [EVAL] batch:  483 | acc: 31.25%,  total acc: 64.75%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 64.63%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 64.83%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 64.89%   [EVAL] batch:  495 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:  496 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:  497 | acc: 31.25%,  total acc: 64.80%   [EVAL] batch:  498 | acc: 56.25%,  total acc: 64.78%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 64.78%   
cur_acc:  ['0.9454', '0.7103', '0.7024', '0.7827', '0.6964', '0.7024', '0.5675', '0.6875']
his_acc:  ['0.9454', '0.8235', '0.7869', '0.7645', '0.7568', '0.7228', '0.6725', '0.6478']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.395576477050781 2.1285247802734375 1.0004911422729492
CurrentTrain: epoch  0, batch     0 | loss: 13.5245924Losses:  9.95632266998291 2.196532726287842 1.0013961791992188
CurrentTrain: epoch  0, batch     1 | loss: 13.1542511Losses:  9.733715057373047 1.7144880294799805 0.9797720909118652
CurrentTrain: epoch  0, batch     2 | loss: 12.4279747Losses:  10.036149978637695 2.022874355316162 1.0016165971755981
CurrentTrain: epoch  0, batch     3 | loss: 13.0606413Losses:  9.934274673461914 1.7206825017929077 0.9912773966789246
CurrentTrain: epoch  0, batch     4 | loss: 12.6462345Losses:  9.19350814819336 1.8672386407852173 0.9908105134963989
CurrentTrain: epoch  0, batch     5 | loss: 12.0515575Losses:  9.599605560302734 1.6326230764389038 0.9964696168899536
CurrentTrain: epoch  0, batch     6 | loss: 12.2286978Losses:  9.257240295410156 1.5581412315368652 0.982520580291748
CurrentTrain: epoch  0, batch     7 | loss: 11.7979031Losses:  9.523378372192383 1.6411457061767578 0.9805698990821838
CurrentTrain: epoch  0, batch     8 | loss: 12.1450939Losses:  8.964700698852539 1.7715983390808105 0.9726009368896484
CurrentTrain: epoch  0, batch     9 | loss: 11.7089005Losses:  9.484038352966309 1.6582833528518677 0.98026442527771
CurrentTrain: epoch  0, batch    10 | loss: 12.1225863Losses:  8.66258430480957 1.6251428127288818 0.9791347980499268
CurrentTrain: epoch  0, batch    11 | loss: 11.2668619Losses:  8.812623977661133 1.6282930374145508 0.9916883707046509
CurrentTrain: epoch  0, batch    12 | loss: 11.4326057Losses:  8.713969230651855 1.6346678733825684 0.9671812653541565
CurrentTrain: epoch  0, batch    13 | loss: 11.3158178Losses:  9.110370635986328 1.3072547912597656 0.9718450903892517
CurrentTrain: epoch  0, batch    14 | loss: 11.3894701Losses:  8.217812538146973 1.4239234924316406 0.955223798751831
CurrentTrain: epoch  0, batch    15 | loss: 10.5969601Losses:  8.614496231079102 1.6083004474639893 0.9755482077598572
CurrentTrain: epoch  0, batch    16 | loss: 11.1983442Losses:  8.471575736999512 1.4255037307739258 0.9647303819656372
CurrentTrain: epoch  0, batch    17 | loss: 10.8618097Losses:  8.848459243774414 1.460442304611206 0.970124363899231
CurrentTrain: epoch  0, batch    18 | loss: 11.2790260Losses:  7.97108793258667 1.490835428237915 0.9609936475753784
CurrentTrain: epoch  0, batch    19 | loss: 10.4229174Losses:  8.096352577209473 1.4188765287399292 0.9544996023178101
CurrentTrain: epoch  0, batch    20 | loss: 10.4697285Losses:  8.535558700561523 1.307245135307312 0.9774389266967773
CurrentTrain: epoch  0, batch    21 | loss: 10.8202429Losses:  7.898794174194336 1.146942138671875 0.9529106020927429
CurrentTrain: epoch  0, batch    22 | loss: 9.9986467Losses:  8.032020568847656 1.3306801319122314 0.9725164175033569
CurrentTrain: epoch  0, batch    23 | loss: 10.3352165Losses:  8.605348587036133 1.488003134727478 0.9519686698913574
CurrentTrain: epoch  0, batch    24 | loss: 11.0453205Losses:  7.970670700073242 1.1895477771759033 0.937038779258728
CurrentTrain: epoch  0, batch    25 | loss: 10.0972567Losses:  8.610027313232422 1.4459476470947266 0.9580671787261963
CurrentTrain: epoch  0, batch    26 | loss: 11.0140419Losses:  8.580364227294922 1.6585304737091064 0.9742270708084106
CurrentTrain: epoch  0, batch    27 | loss: 11.2131214Losses:  8.30466079711914 1.3572943210601807 0.9661475419998169
CurrentTrain: epoch  0, batch    28 | loss: 10.6281023Losses:  8.02176284790039 1.1734089851379395 0.9577592015266418
CurrentTrain: epoch  0, batch    29 | loss: 10.1529303Losses:  7.988525867462158 1.3245452642440796 0.9632166028022766
CurrentTrain: epoch  0, batch    30 | loss: 10.2762880Losses:  8.18901252746582 1.7031865119934082 0.9576863050460815
CurrentTrain: epoch  0, batch    31 | loss: 10.8498850Losses:  8.506616592407227 1.4581687450408936 0.9712315201759338
CurrentTrain: epoch  0, batch    32 | loss: 10.9360170Losses:  8.014287948608398 1.3558776378631592 0.9635836482048035
CurrentTrain: epoch  0, batch    33 | loss: 10.3337498Losses:  7.75661039352417 1.251341462135315 0.9361282587051392
CurrentTrain: epoch  0, batch    34 | loss: 9.9440804Losses:  7.376279830932617 1.1702258586883545 0.9547768831253052
CurrentTrain: epoch  0, batch    35 | loss: 9.5012827Losses:  8.247455596923828 1.2054429054260254 0.9704523086547852
CurrentTrain: epoch  0, batch    36 | loss: 10.4233503Losses:  7.550239562988281 1.3362160921096802 0.9372902512550354
CurrentTrain: epoch  0, batch    37 | loss: 9.8237457Losses:  7.935853004455566 1.220292091369629 0.9440616965293884
CurrentTrain: epoch  0, batch    38 | loss: 10.1002064Losses:  8.092578887939453 1.3101001977920532 0.9527101516723633
CurrentTrain: epoch  0, batch    39 | loss: 10.3553896Losses:  7.7260918617248535 1.3092472553253174 0.9378786087036133
CurrentTrain: epoch  0, batch    40 | loss: 9.9732180Losses:  7.700713157653809 1.2493517398834229 0.9563674926757812
CurrentTrain: epoch  0, batch    41 | loss: 9.9064322Losses:  7.490001201629639 1.0482532978057861 0.9289942383766174
CurrentTrain: epoch  0, batch    42 | loss: 9.4672489Losses:  7.779841423034668 1.0267945528030396 0.9629828929901123
CurrentTrain: epoch  0, batch    43 | loss: 9.7696190Losses:  7.462011814117432 1.1298081874847412 0.9390217065811157
CurrentTrain: epoch  0, batch    44 | loss: 9.5308418Losses:  7.774659156799316 1.1046414375305176 0.944715142250061
CurrentTrain: epoch  0, batch    45 | loss: 9.8240166Losses:  7.883038520812988 1.2461731433868408 0.9712000489234924
CurrentTrain: epoch  0, batch    46 | loss: 10.1004114Losses:  7.2830963134765625 1.224001407623291 0.963376522064209
CurrentTrain: epoch  0, batch    47 | loss: 9.4704742Losses:  7.871213912963867 1.1863644123077393 0.9368442296981812
CurrentTrain: epoch  0, batch    48 | loss: 9.9944220Losses:  7.112994194030762 1.2564606666564941 0.940575361251831
CurrentTrain: epoch  0, batch    49 | loss: 9.3100309Losses:  8.651886940002441 0.8553383350372314 0.9427182674407959
CurrentTrain: epoch  0, batch    50 | loss: 10.4499435Losses:  6.726480007171631 0.9960804581642151 0.9289464950561523
CurrentTrain: epoch  0, batch    51 | loss: 8.6515064Losses:  7.51746940612793 1.1405117511749268 0.9204769134521484
CurrentTrain: epoch  0, batch    52 | loss: 9.5784578Losses:  7.6526312828063965 0.9078645706176758 0.968654990196228
CurrentTrain: epoch  0, batch    53 | loss: 9.5291500Losses:  8.80870246887207 1.1075241565704346 0.9682855010032654
CurrentTrain: epoch  0, batch    54 | loss: 10.8845119Losses:  8.354961395263672 1.257575273513794 0.9384382963180542
CurrentTrain: epoch  0, batch    55 | loss: 10.5509748Losses:  6.698238372802734 1.0431723594665527 0.927251935005188
CurrentTrain: epoch  0, batch    56 | loss: 8.6686630Losses:  6.808315753936768 0.7901599407196045 0.9401750564575195
CurrentTrain: epoch  0, batch    57 | loss: 8.5386505Losses:  7.335873603820801 1.0455501079559326 0.9467388391494751
CurrentTrain: epoch  0, batch    58 | loss: 9.3281631Losses:  7.846944808959961 0.9433037042617798 0.9414799213409424
CurrentTrain: epoch  0, batch    59 | loss: 9.7317286Losses:  7.364711761474609 0.9957813620567322 0.9150512218475342
CurrentTrain: epoch  0, batch    60 | loss: 9.2755442Losses:  7.075621604919434 0.9714870452880859 0.9467800855636597
CurrentTrain: epoch  0, batch    61 | loss: 8.9938889Losses:  6.60211181640625 0.9575527310371399 0.9343518018722534
CurrentTrain: epoch  0, batch    62 | loss: 8.4940166Losses:  8.48502254486084 0.9661847949028015 0.9741742014884949
CurrentTrain: epoch  1, batch     0 | loss: 10.4253817Losses:  6.5115647315979 0.8503005504608154 0.9076358675956726
CurrentTrain: epoch  1, batch     1 | loss: 8.2695007Losses:  7.24055290222168 1.1119282245635986 0.9266828894615173
CurrentTrain: epoch  1, batch     2 | loss: 9.2791634Losses:  6.84999942779541 0.9036338329315186 0.8988527059555054
CurrentTrain: epoch  1, batch     3 | loss: 8.6524858Losses:  6.301817893981934 0.9253919124603271 0.9252128601074219
CurrentTrain: epoch  1, batch     4 | loss: 8.1524229Losses:  6.376055717468262 0.673956036567688 0.9287624359130859
CurrentTrain: epoch  1, batch     5 | loss: 7.9787741Losses:  7.3865647315979 0.7650834321975708 0.9499611258506775
CurrentTrain: epoch  1, batch     6 | loss: 9.1016092Losses:  7.163662910461426 1.0362863540649414 0.9192839860916138
CurrentTrain: epoch  1, batch     7 | loss: 9.1192331Losses:  7.001255989074707 1.0073256492614746 0.9424611926078796
CurrentTrain: epoch  1, batch     8 | loss: 8.9510422Losses:  6.5365495681762695 0.7187389135360718 0.8867483139038086
CurrentTrain: epoch  1, batch     9 | loss: 8.1420364Losses:  6.9233198165893555 0.9393935203552246 0.9167084097862244
CurrentTrain: epoch  1, batch    10 | loss: 8.7794218Losses:  7.779010772705078 0.9472366571426392 0.9414398670196533
CurrentTrain: epoch  1, batch    11 | loss: 9.6676874Losses:  6.899404525756836 0.9364397525787354 0.9206167459487915
CurrentTrain: epoch  1, batch    12 | loss: 8.7564611Losses:  7.097716331481934 1.0380998849868774 0.9162111282348633
CurrentTrain: epoch  1, batch    13 | loss: 9.0520277Losses:  7.383243560791016 1.0577772855758667 0.9231398105621338
CurrentTrain: epoch  1, batch    14 | loss: 9.3641605Losses:  7.00642728805542 0.8488011360168457 0.908657431602478
CurrentTrain: epoch  1, batch    15 | loss: 8.7638855Losses:  6.44382381439209 0.8111872673034668 0.9193502068519592
CurrentTrain: epoch  1, batch    16 | loss: 8.1743612Losses:  7.940461158752441 0.8233603239059448 0.9352231621742249
CurrentTrain: epoch  1, batch    17 | loss: 9.6990452Losses:  6.585526943206787 0.8951352834701538 0.922443151473999
CurrentTrain: epoch  1, batch    18 | loss: 8.4031057Losses:  6.653315544128418 0.9105872511863708 0.9023903012275696
CurrentTrain: epoch  1, batch    19 | loss: 8.4662933Losses:  6.846918106079102 0.9060556888580322 0.9201643466949463
CurrentTrain: epoch  1, batch    20 | loss: 8.6731377Losses:  6.721846103668213 0.8130807876586914 0.926708459854126
CurrentTrain: epoch  1, batch    21 | loss: 8.4616356Losses:  6.426826477050781 0.8740285634994507 0.9217720627784729
CurrentTrain: epoch  1, batch    22 | loss: 8.2226276Losses:  6.8163862228393555 0.8914653062820435 0.9032188653945923
CurrentTrain: epoch  1, batch    23 | loss: 8.6110706Losses:  6.969368934631348 0.846130907535553 0.8943463563919067
CurrentTrain: epoch  1, batch    24 | loss: 8.7098465Losses:  6.204468250274658 0.8738388419151306 0.9088843464851379
CurrentTrain: epoch  1, batch    25 | loss: 7.9871917Losses:  7.277569770812988 0.912535548210144 0.9242275953292847
CurrentTrain: epoch  1, batch    26 | loss: 9.1143332Losses:  5.995573043823242 0.536530077457428 0.889801025390625
CurrentTrain: epoch  1, batch    27 | loss: 7.4219041Losses:  5.92710018157959 0.5957586765289307 0.9339420795440674
CurrentTrain: epoch  1, batch    28 | loss: 7.4568005Losses:  6.418942928314209 0.8226741552352905 0.9027506709098816
CurrentTrain: epoch  1, batch    29 | loss: 8.1443682Losses:  6.59977912902832 0.8174910545349121 0.9159202575683594
CurrentTrain: epoch  1, batch    30 | loss: 8.3331909Losses:  6.755760192871094 0.7538557052612305 0.9172406792640686
CurrentTrain: epoch  1, batch    31 | loss: 8.4268570Losses:  6.180210590362549 0.5167126655578613 0.9266254901885986
CurrentTrain: epoch  1, batch    32 | loss: 7.6235485Losses:  6.2916579246521 0.8478429913520813 0.9083365797996521
CurrentTrain: epoch  1, batch    33 | loss: 8.0478373Losses:  6.45765495300293 0.6340195536613464 0.8975014686584473
CurrentTrain: epoch  1, batch    34 | loss: 7.9891758Losses:  6.554446220397949 0.8117872476577759 0.9129486083984375
CurrentTrain: epoch  1, batch    35 | loss: 8.2791824Losses:  6.2171430587768555 0.8323951959609985 0.8985053300857544
CurrentTrain: epoch  1, batch    36 | loss: 7.9480433Losses:  6.252915382385254 0.8591451644897461 0.8952308297157288
CurrentTrain: epoch  1, batch    37 | loss: 8.0072918Losses:  6.7420878410339355 0.8202749490737915 0.9055366516113281
CurrentTrain: epoch  1, batch    38 | loss: 8.4678993Losses:  5.414594650268555 0.5059016942977905 0.86181640625
CurrentTrain: epoch  1, batch    39 | loss: 6.7823129Losses:  6.714164733886719 0.7206733822822571 0.9289987683296204
CurrentTrain: epoch  1, batch    40 | loss: 8.3638372Losses:  6.982243061065674 0.9133464694023132 0.9245268106460571
CurrentTrain: epoch  1, batch    41 | loss: 8.8201160Losses:  5.778491973876953 0.8412716388702393 0.8897704482078552
CurrentTrain: epoch  1, batch    42 | loss: 7.5095339Losses:  6.441656589508057 0.7558766603469849 0.912868320941925
CurrentTrain: epoch  1, batch    43 | loss: 8.1104012Losses:  7.243206024169922 0.8909845948219299 0.9169923067092896
CurrentTrain: epoch  1, batch    44 | loss: 9.0511827Losses:  6.005589008331299 0.699388861656189 0.8785345554351807
CurrentTrain: epoch  1, batch    45 | loss: 7.5835123Losses:  6.012261390686035 0.6088229417800903 0.9077838659286499
CurrentTrain: epoch  1, batch    46 | loss: 7.5288682Losses:  6.108768463134766 0.6321420669555664 0.8793252110481262
CurrentTrain: epoch  1, batch    47 | loss: 7.6202359Losses:  7.074635028839111 0.6616280674934387 0.919232189655304
CurrentTrain: epoch  1, batch    48 | loss: 8.6554956Losses:  5.766633987426758 0.5703463554382324 0.8941055536270142
CurrentTrain: epoch  1, batch    49 | loss: 7.2310858Losses:  5.8401384353637695 0.6173207759857178 0.907496452331543
CurrentTrain: epoch  1, batch    50 | loss: 7.3649559Losses:  5.210946083068848 0.3441922068595886 0.8840360045433044
CurrentTrain: epoch  1, batch    51 | loss: 6.4391742Losses:  6.050271987915039 0.527273416519165 0.8777600526809692
CurrentTrain: epoch  1, batch    52 | loss: 7.4553051Losses:  5.2675700187683105 0.6498651504516602 0.8607226610183716
CurrentTrain: epoch  1, batch    53 | loss: 6.7781577Losses:  6.699732780456543 0.6543192863464355 0.9130796194076538
CurrentTrain: epoch  1, batch    54 | loss: 8.2671318Losses:  5.566614151000977 0.33878815174102783 0.8994791507720947
CurrentTrain: epoch  1, batch    55 | loss: 6.8048811Losses:  6.4250688552856445 0.7187354564666748 0.8969665169715881
CurrentTrain: epoch  1, batch    56 | loss: 8.0407715Losses:  6.477293491363525 0.688662052154541 0.9415351152420044
CurrentTrain: epoch  1, batch    57 | loss: 8.1074905Losses:  6.066106796264648 0.5800672769546509 0.9110674858093262
CurrentTrain: epoch  1, batch    58 | loss: 7.5572414Losses:  5.622759819030762 0.6969685554504395 0.8802825808525085
CurrentTrain: epoch  1, batch    59 | loss: 7.2000108Losses:  5.4253435134887695 0.4885842502117157 0.8976976871490479
CurrentTrain: epoch  1, batch    60 | loss: 6.8116255Losses:  6.32197380065918 0.5644709467887878 0.919060468673706
CurrentTrain: epoch  1, batch    61 | loss: 7.8055048Losses:  6.720472812652588 0.38981491327285767 0.9198736548423767
CurrentTrain: epoch  1, batch    62 | loss: 8.0301609Losses:  5.749084949493408 0.5736079812049866 0.8911198973655701
CurrentTrain: epoch  2, batch     0 | loss: 7.2138128Losses:  5.145293235778809 0.45849865674972534 0.8676620125770569
CurrentTrain: epoch  2, batch     1 | loss: 6.4714537Losses:  5.107902526855469 0.5491867065429688 0.8830575346946716
CurrentTrain: epoch  2, batch     2 | loss: 6.5401468Losses:  5.297080039978027 0.5240654349327087 0.8489956855773926
CurrentTrain: epoch  2, batch     3 | loss: 6.6701412Losses:  5.311185836791992 0.5935249328613281 0.8515955209732056
CurrentTrain: epoch  2, batch     4 | loss: 6.7563062Losses:  5.935915946960449 0.5185974836349487 0.8689318895339966
CurrentTrain: epoch  2, batch     5 | loss: 7.3234453Losses:  5.359350204467773 0.5475163459777832 0.8605297803878784
CurrentTrain: epoch  2, batch     6 | loss: 6.7673965Losses:  5.9601335525512695 0.6661278605461121 0.8981598615646362
CurrentTrain: epoch  2, batch     7 | loss: 7.5244212Losses:  5.844815254211426 0.6112796068191528 0.8900100588798523
CurrentTrain: epoch  2, batch     8 | loss: 7.3461046Losses:  5.011752605438232 0.4625680148601532 0.8654751777648926
CurrentTrain: epoch  2, batch     9 | loss: 6.3397956Losses:  5.501776218414307 0.48197174072265625 0.8649020195007324
CurrentTrain: epoch  2, batch    10 | loss: 6.8486500Losses:  5.2895097732543945 0.5533210039138794 0.8793978691101074
CurrentTrain: epoch  2, batch    11 | loss: 6.7222285Losses:  5.386030197143555 0.37012094259262085 0.8530168533325195
CurrentTrain: epoch  2, batch    12 | loss: 6.6091681Losses:  5.44284200668335 0.5815785527229309 0.8661986589431763
CurrentTrain: epoch  2, batch    13 | loss: 6.8906193Losses:  5.779799461364746 0.42384883761405945 0.900198221206665
CurrentTrain: epoch  2, batch    14 | loss: 7.1038465Losses:  5.193113327026367 0.4422653317451477 0.8801959156990051
CurrentTrain: epoch  2, batch    15 | loss: 6.5155749Losses:  5.431456089019775 0.4343438446521759 0.9038076400756836
CurrentTrain: epoch  2, batch    16 | loss: 6.7696075Losses:  6.676146030426025 0.6221603155136108 0.9125852584838867
CurrentTrain: epoch  2, batch    17 | loss: 8.2108917Losses:  5.304289817810059 0.5454841256141663 0.837455689907074
CurrentTrain: epoch  2, batch    18 | loss: 6.6872296Losses:  5.535163402557373 0.4514016807079315 0.8669260144233704
CurrentTrain: epoch  2, batch    19 | loss: 6.8534913Losses:  6.730187892913818 0.3197542130947113 0.9166026711463928
CurrentTrain: epoch  2, batch    20 | loss: 7.9665446Losses:  5.7569146156311035 0.603344202041626 0.8826814889907837
CurrentTrain: epoch  2, batch    21 | loss: 7.2429404Losses:  4.948569297790527 0.43396270275115967 0.855089545249939
CurrentTrain: epoch  2, batch    22 | loss: 6.2376218Losses:  5.490496635437012 0.4288918673992157 0.8867691159248352
CurrentTrain: epoch  2, batch    23 | loss: 6.8061576Losses:  4.9546966552734375 0.5073968172073364 0.8332902193069458
CurrentTrain: epoch  2, batch    24 | loss: 6.2953835Losses:  5.314425945281982 0.29106804728507996 0.8751083016395569
CurrentTrain: epoch  2, batch    25 | loss: 6.4806023Losses:  4.801656723022461 0.37128114700317383 0.8589160442352295
CurrentTrain: epoch  2, batch    26 | loss: 6.0318537Losses:  5.14901065826416 0.4240230917930603 0.8806991577148438
CurrentTrain: epoch  2, batch    27 | loss: 6.4537330Losses:  4.887794494628906 0.3172377049922943 0.856751561164856
CurrentTrain: epoch  2, batch    28 | loss: 6.0617838Losses:  5.587502479553223 0.5851845741271973 0.884030282497406
CurrentTrain: epoch  2, batch    29 | loss: 7.0567174Losses:  5.310850143432617 0.4660770893096924 0.8953919410705566
CurrentTrain: epoch  2, batch    30 | loss: 6.6723189Losses:  5.718427658081055 0.5411503314971924 0.8560674786567688
CurrentTrain: epoch  2, batch    31 | loss: 7.1156454Losses:  5.318017959594727 0.38886559009552 0.8974798917770386
CurrentTrain: epoch  2, batch    32 | loss: 6.6043634Losses:  5.60890531539917 0.4965896010398865 0.8521355986595154
CurrentTrain: epoch  2, batch    33 | loss: 6.9576306Losses:  6.057200908660889 0.521666407585144 0.9081545472145081
CurrentTrain: epoch  2, batch    34 | loss: 7.4870219Losses:  5.238053321838379 0.4002420902252197 0.8563964366912842
CurrentTrain: epoch  2, batch    35 | loss: 6.4946918Losses:  6.039021015167236 0.7098802924156189 0.861503005027771
CurrentTrain: epoch  2, batch    36 | loss: 7.6104045Losses:  5.39459228515625 0.415202796459198 0.8725391626358032
CurrentTrain: epoch  2, batch    37 | loss: 6.6823339Losses:  5.634581089019775 0.4925607442855835 0.8352303504943848
CurrentTrain: epoch  2, batch    38 | loss: 6.9623723Losses:  5.508621692657471 0.5380876660346985 0.8713216781616211
CurrentTrain: epoch  2, batch    39 | loss: 6.9180312Losses:  6.115100860595703 0.5037099719047546 0.8669064044952393
CurrentTrain: epoch  2, batch    40 | loss: 7.4857168Losses:  5.033916473388672 0.3286709189414978 0.845458984375
CurrentTrain: epoch  2, batch    41 | loss: 6.2080464Losses:  5.105010032653809 0.4274735748767853 0.8938601613044739
CurrentTrain: epoch  2, batch    42 | loss: 6.4263439Losses:  5.510880947113037 0.42718595266342163 0.8410643935203552
CurrentTrain: epoch  2, batch    43 | loss: 6.7791314Losses:  5.124225616455078 0.39759042859077454 0.8637937307357788
CurrentTrain: epoch  2, batch    44 | loss: 6.3856101Losses:  4.821071624755859 0.24572628736495972 0.9322470426559448
CurrentTrain: epoch  2, batch    45 | loss: 5.9990449Losses:  5.538491725921631 0.48328179121017456 0.8814050555229187
CurrentTrain: epoch  2, batch    46 | loss: 6.9031782Losses:  5.01881742477417 0.3694394528865814 0.8504893779754639
CurrentTrain: epoch  2, batch    47 | loss: 6.2387466Losses:  5.616329193115234 0.39196228981018066 0.8738350868225098
CurrentTrain: epoch  2, batch    48 | loss: 6.8821263Losses:  5.410732746124268 0.34429579973220825 0.8442052006721497
CurrentTrain: epoch  2, batch    49 | loss: 6.5992341Losses:  5.243831157684326 0.368684858083725 0.8513544797897339
CurrentTrain: epoch  2, batch    50 | loss: 6.4638705Losses:  5.458508014678955 0.3653740882873535 0.8544412851333618
CurrentTrain: epoch  2, batch    51 | loss: 6.6783233Losses:  5.308803558349609 0.39533233642578125 0.8253525495529175
CurrentTrain: epoch  2, batch    52 | loss: 6.5294886Losses:  5.291800022125244 0.29549533128738403 0.8764820694923401
CurrentTrain: epoch  2, batch    53 | loss: 6.4637775Losses:  5.58311128616333 0.32052987813949585 0.8625773191452026
CurrentTrain: epoch  2, batch    54 | loss: 6.7662187Losses:  5.54487419128418 0.511935830116272 0.8338567018508911
CurrentTrain: epoch  2, batch    55 | loss: 6.8906665Losses:  5.240052700042725 0.3831312954425812 0.8610302209854126
CurrentTrain: epoch  2, batch    56 | loss: 6.4842143Losses:  5.4153642654418945 0.44024017453193665 0.8813009262084961
CurrentTrain: epoch  2, batch    57 | loss: 6.7369056Losses:  4.776804447174072 0.3142700493335724 0.8823599815368652
CurrentTrain: epoch  2, batch    58 | loss: 5.9734344Losses:  4.8942437171936035 0.40446072816848755 0.8786197900772095
CurrentTrain: epoch  2, batch    59 | loss: 6.1773243Losses:  4.759014129638672 0.3122560977935791 0.833579421043396
CurrentTrain: epoch  2, batch    60 | loss: 5.9048495Losses:  4.654549598693848 0.3648402988910675 0.8332614898681641
CurrentTrain: epoch  2, batch    61 | loss: 5.8526516Losses:  4.748202323913574 0.22389523684978485 0.8023693561553955
CurrentTrain: epoch  2, batch    62 | loss: 5.7744665Losses:  4.775472164154053 0.3817715644836426 0.8291003108024597
CurrentTrain: epoch  3, batch     0 | loss: 5.9863439Losses:  4.811150074005127 0.24495534598827362 0.8849276900291443
CurrentTrain: epoch  3, batch     1 | loss: 5.9410334Losses:  4.75762939453125 0.38076168298721313 0.8391532301902771
CurrentTrain: epoch  3, batch     2 | loss: 5.9775443Losses:  4.583751678466797 0.31733202934265137 0.8145241737365723
CurrentTrain: epoch  3, batch     3 | loss: 5.7156081Losses:  4.852762699127197 0.2770501971244812 0.8828980326652527
CurrentTrain: epoch  3, batch     4 | loss: 6.0127106Losses:  5.355138301849365 0.4267968237400055 0.84059739112854
CurrentTrain: epoch  3, batch     5 | loss: 6.6225328Losses:  4.803181171417236 0.3850119709968567 0.8652348518371582
CurrentTrain: epoch  3, batch     6 | loss: 6.0534282Losses:  4.936478614807129 0.3186977505683899 0.8832746148109436
CurrentTrain: epoch  3, batch     7 | loss: 6.1384511Losses:  4.954153537750244 0.32451239228248596 0.8714865446090698
CurrentTrain: epoch  3, batch     8 | loss: 6.1501527Losses:  5.005779266357422 0.26791730523109436 0.8492180109024048
CurrentTrain: epoch  3, batch     9 | loss: 6.1229143Losses:  5.215975761413574 0.461607962846756 0.8420183062553406
CurrentTrain: epoch  3, batch    10 | loss: 6.5196018Losses:  4.716268539428711 0.2654551863670349 0.8284026384353638
CurrentTrain: epoch  3, batch    11 | loss: 5.8101263Losses:  5.027810573577881 0.3551526367664337 0.8367446660995483
CurrentTrain: epoch  3, batch    12 | loss: 6.2197080Losses:  4.637530326843262 0.2831975817680359 0.8323269486427307
CurrentTrain: epoch  3, batch    13 | loss: 5.7530546Losses:  5.692439079284668 0.34315192699432373 0.8848458528518677
CurrentTrain: epoch  3, batch    14 | loss: 6.9204369Losses:  5.513944149017334 0.47233015298843384 0.8711450099945068
CurrentTrain: epoch  3, batch    15 | loss: 6.8574190Losses:  5.177816867828369 0.37531620264053345 0.8831808567047119
CurrentTrain: epoch  3, batch    16 | loss: 6.4363136Losses:  4.817320346832275 0.17700347304344177 0.8231214284896851
CurrentTrain: epoch  3, batch    17 | loss: 5.8174453Losses:  4.809082984924316 0.3062670826911926 0.8920749425888062
CurrentTrain: epoch  3, batch    18 | loss: 6.0074253Losses:  4.658913612365723 0.3238023817539215 0.8515149354934692
CurrentTrain: epoch  3, batch    19 | loss: 5.8342309Losses:  4.633667945861816 0.32796457409858704 0.7898536324501038
CurrentTrain: epoch  3, batch    20 | loss: 5.7514863Losses:  5.226900577545166 0.2889677882194519 0.8158941268920898
CurrentTrain: epoch  3, batch    21 | loss: 6.3317623Losses:  4.663271427154541 0.3276965022087097 0.8236234784126282
CurrentTrain: epoch  3, batch    22 | loss: 5.8145914Losses:  5.229763031005859 0.20950847864151 0.8392622470855713
CurrentTrain: epoch  3, batch    23 | loss: 6.2785339Losses:  4.772965431213379 0.2603241503238678 0.8112564086914062
CurrentTrain: epoch  3, batch    24 | loss: 5.8445458Losses:  4.863401889801025 0.30084049701690674 0.8612014651298523
CurrentTrain: epoch  3, batch    25 | loss: 6.0254436Losses:  4.679713249206543 0.29814183712005615 0.8280885815620422
CurrentTrain: epoch  3, batch    26 | loss: 5.8059440Losses:  4.974792957305908 0.4303276538848877 0.8247314691543579
CurrentTrain: epoch  3, batch    27 | loss: 6.2298522Losses:  5.591919898986816 0.2897679805755615 0.8372774720191956
CurrentTrain: epoch  3, batch    28 | loss: 6.7189655Losses:  4.770002841949463 0.29471516609191895 0.8672323822975159
CurrentTrain: epoch  3, batch    29 | loss: 5.9319506Losses:  4.523005485534668 0.32721686363220215 0.8324127793312073
CurrentTrain: epoch  3, batch    30 | loss: 5.6826353Losses:  5.706684589385986 0.36818546056747437 0.8554959893226624
CurrentTrain: epoch  3, batch    31 | loss: 6.9303660Losses:  4.631635665893555 0.31574738025665283 0.8757834434509277
CurrentTrain: epoch  3, batch    32 | loss: 5.8231664Losses:  5.007294654846191 0.3261483311653137 0.9043713808059692
CurrentTrain: epoch  3, batch    33 | loss: 6.2378144Losses:  5.342808723449707 0.4156925082206726 0.842127799987793
CurrentTrain: epoch  3, batch    34 | loss: 6.6006289Losses:  4.491791248321533 0.26434943079948425 0.8194373846054077
CurrentTrain: epoch  3, batch    35 | loss: 5.5755782Losses:  4.69986629486084 0.27359211444854736 0.8218581676483154
CurrentTrain: epoch  3, batch    36 | loss: 5.7953167Losses:  4.463554382324219 0.23158234357833862 0.8233736753463745
CurrentTrain: epoch  3, batch    37 | loss: 5.5185103Losses:  4.699517250061035 0.21421778202056885 0.8471154570579529
CurrentTrain: epoch  3, batch    38 | loss: 5.7608504Losses:  4.453662872314453 0.15816161036491394 0.7930629253387451
CurrentTrain: epoch  3, batch    39 | loss: 5.4048872Losses:  4.788434982299805 0.35997387766838074 0.8198120594024658
CurrentTrain: epoch  3, batch    40 | loss: 5.9682207Losses:  5.053096771240234 0.280084490776062 0.8244845271110535
CurrentTrain: epoch  3, batch    41 | loss: 6.1576657Losses:  4.956636428833008 0.26107358932495117 0.834456741809845
CurrentTrain: epoch  3, batch    42 | loss: 6.0521669Losses:  4.604081630706787 0.25933951139450073 0.8683958053588867
CurrentTrain: epoch  3, batch    43 | loss: 5.7318168Losses:  4.590090274810791 0.2669294476509094 0.8363416194915771
CurrentTrain: epoch  3, batch    44 | loss: 5.6933613Losses:  5.004303932189941 0.36966657638549805 0.834449291229248
CurrentTrain: epoch  3, batch    45 | loss: 6.2084198Losses:  4.433608531951904 0.25200068950653076 0.8658351898193359
CurrentTrain: epoch  3, batch    46 | loss: 5.5514445Losses:  4.74363374710083 0.3462689518928528 0.844040036201477
CurrentTrain: epoch  3, batch    47 | loss: 5.9339428Losses:  4.700372695922852 0.3509635627269745 0.8375694155693054
CurrentTrain: epoch  3, batch    48 | loss: 5.8889055Losses:  4.291327476501465 0.2187773883342743 0.7991750240325928
CurrentTrain: epoch  3, batch    49 | loss: 5.3092794Losses:  5.082605361938477 0.38546302914619446 0.8319230079650879
CurrentTrain: epoch  3, batch    50 | loss: 6.2999916Losses:  4.4814772605896 0.30044931173324585 0.8396376371383667
CurrentTrain: epoch  3, batch    51 | loss: 5.6215644Losses:  4.545576095581055 0.2103784680366516 0.8402923345565796
CurrentTrain: epoch  3, batch    52 | loss: 5.5962472Losses:  4.595846176147461 0.2588346600532532 0.8500484228134155
CurrentTrain: epoch  3, batch    53 | loss: 5.7047296Losses:  4.628036975860596 0.2771868407726288 0.8210763931274414
CurrentTrain: epoch  3, batch    54 | loss: 5.7263002Losses:  4.532105445861816 0.25144803524017334 0.759645938873291
CurrentTrain: epoch  3, batch    55 | loss: 5.5431995Losses:  4.617234230041504 0.28436410427093506 0.8353919982910156
CurrentTrain: epoch  3, batch    56 | loss: 5.7369905Losses:  4.295808792114258 0.2582494020462036 0.8353866338729858
CurrentTrain: epoch  3, batch    57 | loss: 5.3894448Losses:  4.93039608001709 0.3368467688560486 0.8364101648330688
CurrentTrain: epoch  3, batch    58 | loss: 6.1036530Losses:  4.638997554779053 0.30090954899787903 0.7807006239891052
CurrentTrain: epoch  3, batch    59 | loss: 5.7206078Losses:  5.033190727233887 0.31717297434806824 0.815885066986084
CurrentTrain: epoch  3, batch    60 | loss: 6.1662488Losses:  4.640007972717285 0.2927128076553345 0.8470779657363892
CurrentTrain: epoch  3, batch    61 | loss: 5.7797985Losses:  4.968757629394531 0.23492681980133057 0.7898544073104858
CurrentTrain: epoch  3, batch    62 | loss: 5.9935389Losses:  4.464276313781738 0.3154029846191406 0.8321069478988647
CurrentTrain: epoch  4, batch     0 | loss: 5.6117864Losses:  4.363918304443359 0.33226722478866577 0.7815878391265869
CurrentTrain: epoch  4, batch     1 | loss: 5.4777737Losses:  4.996659278869629 0.31076258420944214 0.7948105931282043
CurrentTrain: epoch  4, batch     2 | loss: 6.1022325Losses:  4.741559028625488 0.34521472454071045 0.8517053127288818
CurrentTrain: epoch  4, batch     3 | loss: 5.9384794Losses:  4.494386672973633 0.29115307331085205 0.8086814880371094
CurrentTrain: epoch  4, batch     4 | loss: 5.5942211Losses:  4.656610488891602 0.2349971979856491 0.8977004289627075
CurrentTrain: epoch  4, batch     5 | loss: 5.7893081Losses:  4.520356178283691 0.24028414487838745 0.8613777160644531
CurrentTrain: epoch  4, batch     6 | loss: 5.6220179Losses:  4.459123611450195 0.2478584200143814 0.8167723417282104
CurrentTrain: epoch  4, batch     7 | loss: 5.5237546Losses:  4.65757417678833 0.30786311626434326 0.812015175819397
CurrentTrain: epoch  4, batch     8 | loss: 5.7774525Losses:  4.555788993835449 0.313773512840271 0.8276253938674927
CurrentTrain: epoch  4, batch     9 | loss: 5.6971879Losses:  4.405831813812256 0.28346383571624756 0.8071357011795044
CurrentTrain: epoch  4, batch    10 | loss: 5.4964314Losses:  4.377422332763672 0.19607700407505035 0.7634139060974121
CurrentTrain: epoch  4, batch    11 | loss: 5.3369131Losses:  4.4032697677612305 0.2752573490142822 0.8828448057174683
CurrentTrain: epoch  4, batch    12 | loss: 5.5613718Losses:  4.603092193603516 0.35464224219322205 0.8058615922927856
CurrentTrain: epoch  4, batch    13 | loss: 5.7635961Losses:  4.496578693389893 0.25351786613464355 0.8400602340698242
CurrentTrain: epoch  4, batch    14 | loss: 5.5901566Losses:  4.496236801147461 0.25484734773635864 0.8272683620452881
CurrentTrain: epoch  4, batch    15 | loss: 5.5783529Losses:  4.395593643188477 0.27481377124786377 0.811836838722229
CurrentTrain: epoch  4, batch    16 | loss: 5.4822440Losses:  4.466545104980469 0.24380236864089966 0.8165761232376099
CurrentTrain: epoch  4, batch    17 | loss: 5.5269237Losses:  4.383831977844238 0.27997663617134094 0.8062425851821899
CurrentTrain: epoch  4, batch    18 | loss: 5.4700513Losses:  4.443926811218262 0.25718382000923157 0.7816125154495239
CurrentTrain: epoch  4, batch    19 | loss: 5.4827232Losses:  4.305002689361572 0.2249361276626587 0.810280442237854
CurrentTrain: epoch  4, batch    20 | loss: 5.3402190Losses:  4.228026866912842 0.18417739868164062 0.8622147440910339
CurrentTrain: epoch  4, batch    21 | loss: 5.2744188Losses:  4.682229518890381 0.2585764527320862 0.8295198678970337
CurrentTrain: epoch  4, batch    22 | loss: 5.7703257Losses:  4.402312278747559 0.29389509558677673 0.8051007986068726
CurrentTrain: epoch  4, batch    23 | loss: 5.5013084Losses:  4.350330352783203 0.29958730936050415 0.8110801577568054
CurrentTrain: epoch  4, batch    24 | loss: 5.4609976Losses:  4.424491882324219 0.33608415722846985 0.826757550239563
CurrentTrain: epoch  4, batch    25 | loss: 5.5873337Losses:  4.309557914733887 0.24736306071281433 0.8139686584472656
CurrentTrain: epoch  4, batch    26 | loss: 5.3708897Losses:  4.401183128356934 0.2660132348537445 0.812585711479187
CurrentTrain: epoch  4, batch    27 | loss: 5.4797821Losses:  4.446951866149902 0.28561365604400635 0.7918087244033813
CurrentTrain: epoch  4, batch    28 | loss: 5.5243740Losses:  4.322646141052246 0.13634157180786133 0.8705047369003296
CurrentTrain: epoch  4, batch    29 | loss: 5.3294926Losses:  4.402647495269775 0.18859362602233887 0.8357702493667603
CurrentTrain: epoch  4, batch    30 | loss: 5.4270110Losses:  4.375400543212891 0.16496959328651428 0.8777408599853516
CurrentTrain: epoch  4, batch    31 | loss: 5.4181108Losses:  4.425223350524902 0.2442224621772766 0.7884084582328796
CurrentTrain: epoch  4, batch    32 | loss: 5.4578543Losses:  4.447014331817627 0.17763835191726685 0.8231161236763
CurrentTrain: epoch  4, batch    33 | loss: 5.4477692Losses:  4.355081558227539 0.12386052310466766 0.8248880505561829
CurrentTrain: epoch  4, batch    34 | loss: 5.3038301Losses:  4.394888877868652 0.24491333961486816 0.7850669622421265
CurrentTrain: epoch  4, batch    35 | loss: 5.4248691Losses:  4.216925621032715 0.16675642132759094 0.8436335325241089
CurrentTrain: epoch  4, batch    36 | loss: 5.2273159Losses:  4.510288238525391 0.2088637351989746 0.8513185977935791
CurrentTrain: epoch  4, batch    37 | loss: 5.5704708Losses:  4.265378952026367 0.1757526695728302 0.8196641206741333
CurrentTrain: epoch  4, batch    38 | loss: 5.2607956Losses:  5.286398887634277 0.27892574667930603 0.8757189512252808
CurrentTrain: epoch  4, batch    39 | loss: 6.4410439Losses:  4.361811637878418 0.22903890907764435 0.7883127927780151
CurrentTrain: epoch  4, batch    40 | loss: 5.3791633Losses:  4.332904815673828 0.16923266649246216 0.838265061378479
CurrentTrain: epoch  4, batch    41 | loss: 5.3404026Losses:  4.522407054901123 0.21012642979621887 0.8382560610771179
CurrentTrain: epoch  4, batch    42 | loss: 5.5707893Losses:  4.385908126831055 0.24733883142471313 0.7616006135940552
CurrentTrain: epoch  4, batch    43 | loss: 5.3948474Losses:  4.309208869934082 0.16379353404045105 0.8200145959854126
CurrentTrain: epoch  4, batch    44 | loss: 5.2930169Losses:  4.313382625579834 0.2413339763879776 0.819495439529419
CurrentTrain: epoch  4, batch    45 | loss: 5.3742123Losses:  4.363126277923584 0.19901323318481445 0.7731147408485413
CurrentTrain: epoch  4, batch    46 | loss: 5.3352542Losses:  4.242313861846924 0.10908152163028717 0.8548940420150757
CurrentTrain: epoch  4, batch    47 | loss: 5.2062898Losses:  4.4026103019714355 0.19880351424217224 0.8153737783432007
CurrentTrain: epoch  4, batch    48 | loss: 5.4167876Losses:  4.366659164428711 0.1661747246980667 0.8018110394477844
CurrentTrain: epoch  4, batch    49 | loss: 5.3346453Losses:  4.251799583435059 0.202955961227417 0.7884171009063721
CurrentTrain: epoch  4, batch    50 | loss: 5.2431726Losses:  4.279209136962891 0.2183125913143158 0.781665563583374
CurrentTrain: epoch  4, batch    51 | loss: 5.2791872Losses:  4.413422584533691 0.18873029947280884 0.7880148887634277
CurrentTrain: epoch  4, batch    52 | loss: 5.3901677Losses:  4.395954132080078 0.23979362845420837 0.8014430999755859
CurrentTrain: epoch  4, batch    53 | loss: 5.4371910Losses:  4.3936638832092285 0.2267899066209793 0.7342076301574707
CurrentTrain: epoch  4, batch    54 | loss: 5.3546615Losses:  4.415698528289795 0.24936522543430328 0.8195194005966187
CurrentTrain: epoch  4, batch    55 | loss: 5.4845834Losses:  4.168686389923096 0.22299492359161377 0.8017301559448242
CurrentTrain: epoch  4, batch    56 | loss: 5.1934114Losses:  4.183568477630615 0.21671998500823975 0.8168835639953613
CurrentTrain: epoch  4, batch    57 | loss: 5.2171721Losses:  4.494508266448975 0.2858535647392273 0.808890700340271
CurrentTrain: epoch  4, batch    58 | loss: 5.5892525Losses:  4.235836029052734 0.17122723162174225 0.7944237589836121
CurrentTrain: epoch  4, batch    59 | loss: 5.2014871Losses:  4.374334335327148 0.1940121352672577 0.8137057423591614
CurrentTrain: epoch  4, batch    60 | loss: 5.3820524Losses:  4.166503429412842 0.22004099190235138 0.7967884540557861
CurrentTrain: epoch  4, batch    61 | loss: 5.1833324Losses:  4.166994571685791 0.07177648693323135 0.8014171123504639
CurrentTrain: epoch  4, batch    62 | loss: 5.0401878Losses:  4.370537757873535 0.18446724116802216 0.8186701536178589
CurrentTrain: epoch  5, batch     0 | loss: 5.3736753Losses:  4.217159271240234 0.2159920036792755 0.8075239062309265
CurrentTrain: epoch  5, batch     1 | loss: 5.2406750Losses:  4.249232292175293 0.16353672742843628 0.7723379135131836
CurrentTrain: epoch  5, batch     2 | loss: 5.1851068Losses:  4.240088939666748 0.1884852647781372 0.8126182556152344
CurrentTrain: epoch  5, batch     3 | loss: 5.2411923Losses:  4.222146511077881 0.244778111577034 0.7717548608779907
CurrentTrain: epoch  5, batch     4 | loss: 5.2386794Losses:  4.155543327331543 0.11202791333198547 0.8372704386711121
CurrentTrain: epoch  5, batch     5 | loss: 5.1048417Losses:  4.220268249511719 0.23028317093849182 0.7988004684448242
CurrentTrain: epoch  5, batch     6 | loss: 5.2493520Losses:  4.319358825683594 0.22449232637882233 0.8203369379043579
CurrentTrain: epoch  5, batch     7 | loss: 5.3641882Losses:  4.385179042816162 0.25076815485954285 0.8073861002922058
CurrentTrain: epoch  5, batch     8 | loss: 5.4433331Losses:  4.227837562561035 0.15412922203540802 0.7660056352615356
CurrentTrain: epoch  5, batch     9 | loss: 5.1479721Losses:  4.160102844238281 0.17137731611728668 0.8481924533843994
CurrentTrain: epoch  5, batch    10 | loss: 5.1796722Losses:  4.251852035522461 0.12390044331550598 0.7548278570175171
CurrentTrain: epoch  5, batch    11 | loss: 5.1305804Losses:  4.141208648681641 0.15046578645706177 0.7734067440032959
CurrentTrain: epoch  5, batch    12 | loss: 5.0650816Losses:  4.168450832366943 0.1974705457687378 0.8024896383285522
CurrentTrain: epoch  5, batch    13 | loss: 5.1684113Losses:  4.283731460571289 0.17325305938720703 0.8760493993759155
CurrentTrain: epoch  5, batch    14 | loss: 5.3330340Losses:  4.156831741333008 0.16781604290008545 0.8220622539520264
CurrentTrain: epoch  5, batch    15 | loss: 5.1467104Losses:  4.201774597167969 0.21032075583934784 0.8386293649673462
CurrentTrain: epoch  5, batch    16 | loss: 5.2507248Losses:  4.159176826477051 0.18943992257118225 0.8005026578903198
CurrentTrain: epoch  5, batch    17 | loss: 5.1491194Losses:  4.191917896270752 0.20005980134010315 0.8086804151535034
CurrentTrain: epoch  5, batch    18 | loss: 5.2006583Losses:  4.1972737312316895 0.21159768104553223 0.776604413986206
CurrentTrain: epoch  5, batch    19 | loss: 5.1854763Losses:  4.2718048095703125 0.18120887875556946 0.7783399820327759
CurrentTrain: epoch  5, batch    20 | loss: 5.2313538Losses:  4.147397994995117 0.17218956351280212 0.8427661657333374
CurrentTrain: epoch  5, batch    21 | loss: 5.1623540Losses:  4.2373552322387695 0.21529178321361542 0.766995906829834
CurrentTrain: epoch  5, batch    22 | loss: 5.2196431Losses:  4.209353446960449 0.17049679160118103 0.8294119834899902
CurrentTrain: epoch  5, batch    23 | loss: 5.2092624Losses:  4.236456394195557 0.17326857149600983 0.7930111885070801
CurrentTrain: epoch  5, batch    24 | loss: 5.2027364Losses:  4.269262313842773 0.21678297221660614 0.7848339676856995
CurrentTrain: epoch  5, batch    25 | loss: 5.2708793Losses:  4.195163726806641 0.14680823683738708 0.8142701387405396
CurrentTrain: epoch  5, batch    26 | loss: 5.1562419Losses:  4.237793445587158 0.21812403202056885 0.80656898021698
CurrentTrain: epoch  5, batch    27 | loss: 5.2624865Losses:  4.331209182739258 0.15410944819450378 0.7975124716758728
CurrentTrain: epoch  5, batch    28 | loss: 5.2828312Losses:  4.15147590637207 0.180122971534729 0.8165870308876038
CurrentTrain: epoch  5, batch    29 | loss: 5.1481857Losses:  4.136517524719238 0.20586799085140228 0.8113647699356079
CurrentTrain: epoch  5, batch    30 | loss: 5.1537499Losses:  4.139380931854248 0.18295958638191223 0.8183085918426514
CurrentTrain: epoch  5, batch    31 | loss: 5.1406488Losses:  4.1966657638549805 0.18663820624351501 0.8028911352157593
CurrentTrain: epoch  5, batch    32 | loss: 5.1861954Losses:  4.254014492034912 0.16753685474395752 0.7906820178031921
CurrentTrain: epoch  5, batch    33 | loss: 5.2122331Losses:  4.19521951675415 0.19652724266052246 0.7943401336669922
CurrentTrain: epoch  5, batch    34 | loss: 5.1860867Losses:  4.358484268188477 0.2092740833759308 0.8206740617752075
CurrentTrain: epoch  5, batch    35 | loss: 5.3884325Losses:  4.302549839019775 0.1689424067735672 0.8293851613998413
CurrentTrain: epoch  5, batch    36 | loss: 5.3008776Losses:  4.359240531921387 0.18456828594207764 0.7493741512298584
CurrentTrain: epoch  5, batch    37 | loss: 5.2931833Losses:  4.164000511169434 0.1765582412481308 0.7983750700950623
CurrentTrain: epoch  5, batch    38 | loss: 5.1389337Losses:  4.283585548400879 0.17686894536018372 0.754664957523346
CurrentTrain: epoch  5, batch    39 | loss: 5.2151194Losses:  4.202489852905273 0.1908240020275116 0.7842905521392822
CurrentTrain: epoch  5, batch    40 | loss: 5.1776047Losses:  4.328003883361816 0.19186365604400635 0.7599933743476868
CurrentTrain: epoch  5, batch    41 | loss: 5.2798610Losses:  4.154893398284912 0.1912868469953537 0.7662822008132935
CurrentTrain: epoch  5, batch    42 | loss: 5.1124625Losses:  4.202128887176514 0.16165074706077576 0.7684473991394043
CurrentTrain: epoch  5, batch    43 | loss: 5.1322269Losses:  4.138876438140869 0.1398756355047226 0.8525777459144592
CurrentTrain: epoch  5, batch    44 | loss: 5.1313295Losses:  4.151334285736084 0.13967294991016388 0.770898163318634
CurrentTrain: epoch  5, batch    45 | loss: 5.0619054Losses:  4.079380512237549 0.18517789244651794 0.7248119115829468
CurrentTrain: epoch  5, batch    46 | loss: 4.9893703Losses:  4.882673263549805 0.2781619429588318 0.8912731409072876
CurrentTrain: epoch  5, batch    47 | loss: 6.0521083Losses:  4.101202487945557 0.19547481834888458 0.7924937605857849
CurrentTrain: epoch  5, batch    48 | loss: 5.0891709Losses:  4.149670600891113 0.20781156420707703 0.738396942615509
CurrentTrain: epoch  5, batch    49 | loss: 5.0958791Losses:  4.192378044128418 0.16666489839553833 0.8119679689407349
CurrentTrain: epoch  5, batch    50 | loss: 5.1710110Losses:  4.2153000831604 0.1972685158252716 0.7464307546615601
CurrentTrain: epoch  5, batch    51 | loss: 5.1589994Losses:  4.161947250366211 0.204056978225708 0.7962096929550171
CurrentTrain: epoch  5, batch    52 | loss: 5.1622138Losses:  4.1676788330078125 0.19528359174728394 0.7530876398086548
CurrentTrain: epoch  5, batch    53 | loss: 5.1160498Losses:  4.158891201019287 0.20566681027412415 0.761442244052887
CurrentTrain: epoch  5, batch    54 | loss: 5.1260004Losses:  4.152029991149902 0.19544163346290588 0.7891029715538025
CurrentTrain: epoch  5, batch    55 | loss: 5.1365747Losses:  4.055736541748047 0.1727590411901474 0.7213716506958008
CurrentTrain: epoch  5, batch    56 | loss: 4.9498672Losses:  4.181331157684326 0.16228005290031433 0.785759449005127
CurrentTrain: epoch  5, batch    57 | loss: 5.1293707Losses:  4.110662460327148 0.10420594364404678 0.8018658757209778
CurrentTrain: epoch  5, batch    58 | loss: 5.0167346Losses:  4.147439479827881 0.16074474155902863 0.7587568163871765
CurrentTrain: epoch  5, batch    59 | loss: 5.0669408Losses:  4.167695045471191 0.15281541645526886 0.8663764595985413
CurrentTrain: epoch  5, batch    60 | loss: 5.1868868Losses:  4.179171562194824 0.15568438172340393 0.7836233377456665
CurrentTrain: epoch  5, batch    61 | loss: 5.1184793Losses:  4.138707160949707 0.1157592311501503 0.8385796546936035
CurrentTrain: epoch  5, batch    62 | loss: 5.0930462Losses:  4.157489776611328 0.1717105209827423 0.7299809455871582
CurrentTrain: epoch  6, batch     0 | loss: 5.0591812Losses:  4.152663230895996 0.11502446979284286 0.7471218109130859
CurrentTrain: epoch  6, batch     1 | loss: 5.0148096Losses:  4.151735782623291 0.1696961671113968 0.8396561145782471
CurrentTrain: epoch  6, batch     2 | loss: 5.1610880Losses:  4.148232460021973 0.18903636932373047 0.797978401184082
CurrentTrain: epoch  6, batch     3 | loss: 5.1352472Losses:  4.128728866577148 0.1721840500831604 0.7538068890571594
CurrentTrain: epoch  6, batch     4 | loss: 5.0547199Losses:  4.137486934661865 0.16476264595985413 0.7555602788925171
CurrentTrain: epoch  6, batch     5 | loss: 5.0578098Losses:  4.113847732543945 0.18112292885780334 0.7533613443374634
CurrentTrain: epoch  6, batch     6 | loss: 5.0483317Losses:  4.1122894287109375 0.15103712677955627 0.8076801300048828
CurrentTrain: epoch  6, batch     7 | loss: 5.0710068Losses:  4.20473575592041 0.1799434870481491 0.8118090033531189
CurrentTrain: epoch  6, batch     8 | loss: 5.1964884Losses:  4.18828821182251 0.14418695867061615 0.8686617612838745
CurrentTrain: epoch  6, batch     9 | loss: 5.2011371Losses:  4.142342567443848 0.14155974984169006 0.794669508934021
CurrentTrain: epoch  6, batch    10 | loss: 5.0785718Losses:  4.098076820373535 0.1530187427997589 0.7549930214881897
CurrentTrain: epoch  6, batch    11 | loss: 5.0060887Losses:  4.087305068969727 0.13078927993774414 0.7909342646598816
CurrentTrain: epoch  6, batch    12 | loss: 5.0090284Losses:  4.15080451965332 0.18096287548542023 0.8002502918243408
CurrentTrain: epoch  6, batch    13 | loss: 5.1320181Losses:  4.085196495056152 0.16371794044971466 0.7839488983154297
CurrentTrain: epoch  6, batch    14 | loss: 5.0328631Losses:  4.1406941413879395 0.14720004796981812 0.8211801052093506
CurrentTrain: epoch  6, batch    15 | loss: 5.1090746Losses:  4.1064772605896 0.187297523021698 0.7561570405960083
CurrentTrain: epoch  6, batch    16 | loss: 5.0499315Losses:  4.13373327255249 0.1404438614845276 0.7193036079406738
CurrentTrain: epoch  6, batch    17 | loss: 4.9934807Losses:  4.151629447937012 0.14192065596580505 0.7661641836166382
CurrentTrain: epoch  6, batch    18 | loss: 5.0597143Losses:  4.110654354095459 0.16589252650737762 0.8256696462631226
CurrentTrain: epoch  6, batch    19 | loss: 5.1022167Losses:  4.1898393630981445 0.12066520750522614 0.8774187564849854
CurrentTrain: epoch  6, batch    20 | loss: 5.1879234Losses:  4.7115278244018555 0.23871339857578278 0.8150285482406616
CurrentTrain: epoch  6, batch    21 | loss: 5.7652698Losses:  4.1512322425842285 0.16644695401191711 0.8527736663818359
CurrentTrain: epoch  6, batch    22 | loss: 5.1704531Losses:  4.121311187744141 0.12118474394083023 0.818459153175354
CurrentTrain: epoch  6, batch    23 | loss: 5.0609550Losses:  4.143548488616943 0.18151842057704926 0.7913140058517456
CurrentTrain: epoch  6, batch    24 | loss: 5.1163812Losses:  4.100494384765625 0.19801491498947144 0.8007335662841797
CurrentTrain: epoch  6, batch    25 | loss: 5.0992427Losses:  4.153781890869141 0.1263907253742218 0.7757245302200317
CurrentTrain: epoch  6, batch    26 | loss: 5.0558972Losses:  4.140216827392578 0.13745778799057007 0.8854634761810303
CurrentTrain: epoch  6, batch    27 | loss: 5.1631384Losses:  4.092613220214844 0.16916176676750183 0.7345431447029114
CurrentTrain: epoch  6, batch    28 | loss: 4.9963183Losses:  4.150356292724609 0.11778600513935089 0.7921921014785767
CurrentTrain: epoch  6, batch    29 | loss: 5.0603342Losses:  4.094788551330566 0.17305788397789001 0.7173585891723633
CurrentTrain: epoch  6, batch    30 | loss: 4.9852052Losses:  4.103847503662109 0.16151928901672363 0.7570860385894775
CurrentTrain: epoch  6, batch    31 | loss: 5.0224524Losses:  4.0851731300354 0.1835612952709198 0.7561897039413452
CurrentTrain: epoch  6, batch    32 | loss: 5.0249243Losses:  4.099612712860107 0.16001860797405243 0.7755408883094788
CurrentTrain: epoch  6, batch    33 | loss: 5.0351720Losses:  4.090222358703613 0.18235483765602112 0.7601780891418457
CurrentTrain: epoch  6, batch    34 | loss: 5.0327554Losses:  4.073860168457031 0.17448467016220093 0.7995595335960388
CurrentTrain: epoch  6, batch    35 | loss: 5.0479045Losses:  4.107852935791016 0.12970107793807983 0.7985230684280396
CurrentTrain: epoch  6, batch    36 | loss: 5.0360770Losses:  4.040236949920654 0.08735854923725128 0.8139846324920654
CurrentTrain: epoch  6, batch    37 | loss: 4.9415798Losses:  4.057215213775635 0.13216270506381989 0.8135392665863037
CurrentTrain: epoch  6, batch    38 | loss: 5.0029173Losses:  4.071746826171875 0.14944788813591003 0.7418042421340942
CurrentTrain: epoch  6, batch    39 | loss: 4.9629989Losses:  4.088348388671875 0.12989425659179688 0.6609309911727905
CurrentTrain: epoch  6, batch    40 | loss: 4.8791738Losses:  4.111647605895996 0.16132986545562744 0.7805525660514832
CurrentTrain: epoch  6, batch    41 | loss: 5.0535297Losses:  4.143141269683838 0.14989601075649261 0.8531762361526489
CurrentTrain: epoch  6, batch    42 | loss: 5.1462135Losses:  4.092831611633301 0.13734889030456543 0.8509005904197693
CurrentTrain: epoch  6, batch    43 | loss: 5.0810814Losses:  4.094022750854492 0.09946519136428833 0.7989368438720703
CurrentTrain: epoch  6, batch    44 | loss: 4.9924250Losses:  4.140771865844727 0.12757588922977448 0.7808793783187866
CurrentTrain: epoch  6, batch    45 | loss: 5.0492272Losses:  4.068819046020508 0.15196426212787628 0.762447714805603
CurrentTrain: epoch  6, batch    46 | loss: 4.9832311Losses:  4.129771709442139 0.1269015669822693 0.7684382796287537
CurrentTrain: epoch  6, batch    47 | loss: 5.0251117Losses:  4.108719825744629 0.14140093326568604 0.7466883063316345
CurrentTrain: epoch  6, batch    48 | loss: 4.9968090Losses:  4.179683685302734 0.11099424958229065 0.8340543508529663
CurrentTrain: epoch  6, batch    49 | loss: 5.1247325Losses:  4.100032806396484 0.18985137343406677 0.7428582310676575
CurrentTrain: epoch  6, batch    50 | loss: 5.0327425Losses:  4.099124431610107 0.13263997435569763 0.79621422290802
CurrentTrain: epoch  6, batch    51 | loss: 5.0279784Losses:  4.032435417175293 0.13399961590766907 0.7153433561325073
CurrentTrain: epoch  6, batch    52 | loss: 4.8817787Losses:  4.011046409606934 0.14368698000907898 0.7242889404296875
CurrentTrain: epoch  6, batch    53 | loss: 4.8790221Losses:  4.038079738616943 0.17115238308906555 0.7067886590957642
CurrentTrain: epoch  6, batch    54 | loss: 4.9160209Losses:  4.08736515045166 0.14001861214637756 0.8059149980545044
CurrentTrain: epoch  6, batch    55 | loss: 5.0332985Losses:  4.102285385131836 0.15440118312835693 0.8105980157852173
CurrentTrain: epoch  6, batch    56 | loss: 5.0672846Losses:  4.05433988571167 0.11819601058959961 0.7099411487579346
CurrentTrain: epoch  6, batch    57 | loss: 4.8824768Losses:  4.1000518798828125 0.1648591160774231 0.7423610687255859
CurrentTrain: epoch  6, batch    58 | loss: 5.0072722Losses:  4.0737504959106445 0.11831910163164139 0.7341325283050537
CurrentTrain: epoch  6, batch    59 | loss: 4.9262018Losses:  4.098602294921875 0.1329183578491211 0.8056656718254089
CurrentTrain: epoch  6, batch    60 | loss: 5.0371861Losses:  4.026325702667236 0.1345795840024948 0.7776025533676147
CurrentTrain: epoch  6, batch    61 | loss: 4.9385080Losses:  4.125849723815918 0.13509957492351532 0.7618407011032104
CurrentTrain: epoch  6, batch    62 | loss: 5.0227900Losses:  4.022736072540283 0.1108771562576294 0.8232729434967041
CurrentTrain: epoch  7, batch     0 | loss: 4.9568863Losses:  4.101958751678467 0.14909479022026062 0.7749215960502625
CurrentTrain: epoch  7, batch     1 | loss: 5.0259748Losses:  4.057130813598633 0.15678580105304718 0.7491907477378845
CurrentTrain: epoch  7, batch     2 | loss: 4.9631076Losses:  4.360659122467041 0.15044349431991577 0.8275121450424194
CurrentTrain: epoch  7, batch     3 | loss: 5.3386149Losses:  4.087884902954102 0.16813498735427856 0.7934572100639343
CurrentTrain: epoch  7, batch     4 | loss: 5.0494771Losses:  4.114015579223633 0.13012143969535828 0.7872796058654785
CurrentTrain: epoch  7, batch     5 | loss: 5.0314164Losses:  4.043699264526367 0.1306033730506897 0.7558578252792358
CurrentTrain: epoch  7, batch     6 | loss: 4.9301605Losses:  4.048047065734863 0.1398530751466751 0.816356897354126
CurrentTrain: epoch  7, batch     7 | loss: 5.0042572Losses:  4.096518516540527 0.1224992573261261 0.7974859476089478
CurrentTrain: epoch  7, batch     8 | loss: 5.0165038Losses:  4.094199180603027 0.14529132843017578 0.7238183617591858
CurrentTrain: epoch  7, batch     9 | loss: 4.9633088Losses:  4.071747779846191 0.13086619973182678 0.7445529103279114
CurrentTrain: epoch  7, batch    10 | loss: 4.9471669Losses:  4.096520900726318 0.137305349111557 0.8388518691062927
CurrentTrain: epoch  7, batch    11 | loss: 5.0726781Losses:  4.064131259918213 0.10269342362880707 0.7408220767974854
CurrentTrain: epoch  7, batch    12 | loss: 4.9076471Losses:  4.119344711303711 0.16806036233901978 0.7912449836730957
CurrentTrain: epoch  7, batch    13 | loss: 5.0786500Losses:  4.071228981018066 0.1475830376148224 0.7534818649291992
CurrentTrain: epoch  7, batch    14 | loss: 4.9722939Losses:  4.051608085632324 0.15854224562644958 0.780966579914093
CurrentTrain: epoch  7, batch    15 | loss: 4.9911170Losses:  4.063867568969727 0.10513173043727875 0.8494566082954407
CurrentTrain: epoch  7, batch    16 | loss: 5.0184560Losses:  4.168667793273926 0.10278603434562683 0.7481019496917725
CurrentTrain: epoch  7, batch    17 | loss: 5.0195560Losses:  4.090549468994141 0.09866657853126526 0.7986990809440613
CurrentTrain: epoch  7, batch    18 | loss: 4.9879150Losses:  4.090337753295898 0.14964216947555542 0.8258816599845886
CurrentTrain: epoch  7, batch    19 | loss: 5.0658612Losses:  4.040494918823242 0.13763870298862457 0.7810442447662354
CurrentTrain: epoch  7, batch    20 | loss: 4.9591780Losses:  4.053821563720703 0.11313296854496002 0.7962518930435181
CurrentTrain: epoch  7, batch    21 | loss: 4.9632063Losses:  3.9951062202453613 0.1574699878692627 0.7194991111755371
CurrentTrain: epoch  7, batch    22 | loss: 4.8720756Losses:  4.1036810874938965 0.15525944530963898 0.796848475933075
CurrentTrain: epoch  7, batch    23 | loss: 5.0557890Losses:  4.044637203216553 0.15069232881069183 0.7854995727539062
CurrentTrain: epoch  7, batch    24 | loss: 4.9808292Losses:  4.076704978942871 0.14057902991771698 0.824126124382019
CurrentTrain: epoch  7, batch    25 | loss: 5.0414104Losses:  4.053677558898926 0.14544981718063354 0.7422974705696106
CurrentTrain: epoch  7, batch    26 | loss: 4.9414248Losses:  4.096714973449707 0.09812477976083755 0.7442513108253479
CurrentTrain: epoch  7, batch    27 | loss: 4.9390912Losses:  4.068045616149902 0.1090373694896698 0.8127667903900146
CurrentTrain: epoch  7, batch    28 | loss: 4.9898500Losses:  4.100457191467285 0.14916567504405975 0.7602944374084473
CurrentTrain: epoch  7, batch    29 | loss: 5.0099173Losses:  4.063444137573242 0.15494072437286377 0.7637937068939209
CurrentTrain: epoch  7, batch    30 | loss: 4.9821787Losses:  4.07241153717041 0.11003763973712921 0.8069819808006287
CurrentTrain: epoch  7, batch    31 | loss: 4.9894314Losses:  4.007641792297363 0.11498608440160751 0.8136739730834961
CurrentTrain: epoch  7, batch    32 | loss: 4.9363017Losses:  4.055506706237793 0.11105171591043472 0.7298191785812378
CurrentTrain: epoch  7, batch    33 | loss: 4.8963776Losses:  4.057069778442383 0.14062286913394928 0.767362117767334
CurrentTrain: epoch  7, batch    34 | loss: 4.9650550Losses:  4.023787021636963 0.10765323042869568 0.8132853507995605
CurrentTrain: epoch  7, batch    35 | loss: 4.9447255Losses:  4.07746696472168 0.12645593285560608 0.7065272927284241
CurrentTrain: epoch  7, batch    36 | loss: 4.9104500Losses:  4.077417373657227 0.13187985122203827 0.7310914993286133
CurrentTrain: epoch  7, batch    37 | loss: 4.9403887Losses:  4.073910713195801 0.07852832227945328 0.7001649737358093
CurrentTrain: epoch  7, batch    38 | loss: 4.8526039Losses:  4.046462059020996 0.08890795707702637 0.8608790636062622
CurrentTrain: epoch  7, batch    39 | loss: 4.9962492Losses:  3.9858126640319824 0.13077914714813232 0.783446729183197
CurrentTrain: epoch  7, batch    40 | loss: 4.9000387Losses:  4.015303611755371 0.10208811610937119 0.8085214495658875
CurrentTrain: epoch  7, batch    41 | loss: 4.9259129Losses:  4.032760143280029 0.0879586786031723 0.6935964226722717
CurrentTrain: epoch  7, batch    42 | loss: 4.8143153Losses:  4.046185493469238 0.09438814967870712 0.7098987102508545
CurrentTrain: epoch  7, batch    43 | loss: 4.8504725Losses:  4.073144912719727 0.08817610889673233 0.687813401222229
CurrentTrain: epoch  7, batch    44 | loss: 4.8491344Losses:  4.043301582336426 0.12735679745674133 0.7424609661102295
CurrentTrain: epoch  7, batch    45 | loss: 4.9131193Losses:  4.028857231140137 0.13704517483711243 0.8201836943626404
CurrentTrain: epoch  7, batch    46 | loss: 4.9860864Losses:  4.091006278991699 0.14577999711036682 0.7882457971572876
CurrentTrain: epoch  7, batch    47 | loss: 5.0250320Losses:  4.035775661468506 0.15408295392990112 0.7741591334342957
CurrentTrain: epoch  7, batch    48 | loss: 4.9640174Losses:  4.023784637451172 0.06750445067882538 0.6866114735603333
CurrentTrain: epoch  7, batch    49 | loss: 4.7779007Losses:  4.041018486022949 0.13076016306877136 0.7405893802642822
CurrentTrain: epoch  7, batch    50 | loss: 4.9123678Losses:  4.069087982177734 0.11779876053333282 0.7051417231559753
CurrentTrain: epoch  7, batch    51 | loss: 4.8920283Losses:  4.042076587677002 0.11880751699209213 0.7406944036483765
CurrentTrain: epoch  7, batch    52 | loss: 4.9015784Losses:  4.024616241455078 0.12932837009429932 0.7651498913764954
CurrentTrain: epoch  7, batch    53 | loss: 4.9190946Losses:  4.078166961669922 0.14214131236076355 0.7098730802536011
CurrentTrain: epoch  7, batch    54 | loss: 4.9301815Losses:  3.9358606338500977 0.10391312092542648 0.6947922706604004
CurrentTrain: epoch  7, batch    55 | loss: 4.7345662Losses:  4.050973892211914 0.11441531032323837 0.7604614496231079
CurrentTrain: epoch  7, batch    56 | loss: 4.9258504Losses:  4.006406784057617 0.13116233050823212 0.7734812498092651
CurrentTrain: epoch  7, batch    57 | loss: 4.9110503Losses:  4.044576168060303 0.13371992111206055 0.7863027453422546
CurrentTrain: epoch  7, batch    58 | loss: 4.9645987Losses:  4.095145225524902 0.1087011992931366 0.7664329409599304
CurrentTrain: epoch  7, batch    59 | loss: 4.9702792Losses:  4.042562007904053 0.14668482542037964 0.7106112241744995
CurrentTrain: epoch  7, batch    60 | loss: 4.8998580Losses:  3.9945991039276123 0.15215156972408295 0.7851364612579346
CurrentTrain: epoch  7, batch    61 | loss: 4.9318867Losses:  4.109714508056641 0.0692841112613678 0.689980149269104
CurrentTrain: epoch  7, batch    62 | loss: 4.8689785Losses:  4.036547660827637 0.13185420632362366 0.7077966928482056
CurrentTrain: epoch  8, batch     0 | loss: 4.8761983Losses:  4.0309038162231445 0.13330644369125366 0.7384595274925232
CurrentTrain: epoch  8, batch     1 | loss: 4.9026699Losses:  4.034931182861328 0.15506990253925323 0.727108359336853
CurrentTrain: epoch  8, batch     2 | loss: 4.9171095Losses:  4.022552490234375 0.11992812156677246 0.7708141803741455
CurrentTrain: epoch  8, batch     3 | loss: 4.9132948Losses:  4.040780544281006 0.09954464435577393 0.8249510526657104
CurrentTrain: epoch  8, batch     4 | loss: 4.9652762Losses:  4.039485454559326 0.10705196857452393 0.7026569843292236
CurrentTrain: epoch  8, batch     5 | loss: 4.8491945Losses:  3.9971659183502197 0.14184962213039398 0.744661808013916
CurrentTrain: epoch  8, batch     6 | loss: 4.8836775Losses:  4.0123090744018555 0.13454343378543854 0.7384951114654541
CurrentTrain: epoch  8, batch     7 | loss: 4.8853474Losses:  4.03406286239624 0.08359174430370331 0.7194405794143677
CurrentTrain: epoch  8, batch     8 | loss: 4.8370953Losses:  3.9847898483276367 0.09112438559532166 0.7581387758255005
CurrentTrain: epoch  8, batch     9 | loss: 4.8340530Losses:  4.030704975128174 0.12269209325313568 0.7349100112915039
CurrentTrain: epoch  8, batch    10 | loss: 4.8883071Losses:  4.006681442260742 0.1051371619105339 0.6985049247741699
CurrentTrain: epoch  8, batch    11 | loss: 4.8103237Losses:  4.00880241394043 0.1283208727836609 0.7994989156723022
CurrentTrain: epoch  8, batch    12 | loss: 4.9366221Losses:  4.052306175231934 0.11875953525304794 0.7946689128875732
CurrentTrain: epoch  8, batch    13 | loss: 4.9657345Losses:  4.073109149932861 0.11064732074737549 0.8437192440032959
CurrentTrain: epoch  8, batch    14 | loss: 5.0274754Losses:  3.9957504272460938 0.1349380910396576 0.7811682224273682
CurrentTrain: epoch  8, batch    15 | loss: 4.9118567Losses:  4.0146803855896 0.0995536670088768 0.7965192794799805
CurrentTrain: epoch  8, batch    16 | loss: 4.9107533Losses:  4.059008598327637 0.10209330171346664 0.7880910634994507
CurrentTrain: epoch  8, batch    17 | loss: 4.9491930Losses:  4.037298202514648 0.117073655128479 0.7048592567443848
CurrentTrain: epoch  8, batch    18 | loss: 4.8592310Losses:  4.013223648071289 0.13063545525074005 0.7617121338844299
CurrentTrain: epoch  8, batch    19 | loss: 4.9055710Losses:  4.03952169418335 0.08143109083175659 0.7525005340576172
CurrentTrain: epoch  8, batch    20 | loss: 4.8734531Losses:  4.065503120422363 0.10886000096797943 0.7620521187782288
CurrentTrain: epoch  8, batch    21 | loss: 4.9364152Losses:  4.042028427124023 0.11017034202814102 0.795190691947937
CurrentTrain: epoch  8, batch    22 | loss: 4.9473896Losses:  4.026228904724121 0.12024044990539551 0.7699680328369141
CurrentTrain: epoch  8, batch    23 | loss: 4.9164371Losses:  4.028808116912842 0.13122183084487915 0.7430014610290527
CurrentTrain: epoch  8, batch    24 | loss: 4.9030313Losses:  4.0784759521484375 0.11394289135932922 0.7034189701080322
CurrentTrain: epoch  8, batch    25 | loss: 4.8958378Losses:  3.991872787475586 0.12561598420143127 0.7396339178085327
CurrentTrain: epoch  8, batch    26 | loss: 4.8571229Losses:  4.054732322692871 0.11099688708782196 0.7501818537712097
CurrentTrain: epoch  8, batch    27 | loss: 4.9159107Losses:  4.007166862487793 0.12007233500480652 0.7276426553726196
CurrentTrain: epoch  8, batch    28 | loss: 4.8548818Losses:  3.999535083770752 0.10935893654823303 0.7660785913467407
CurrentTrain: epoch  8, batch    29 | loss: 4.8749723Losses:  3.9997527599334717 0.09024012833833694 0.8396055698394775
CurrentTrain: epoch  8, batch    30 | loss: 4.9295988Losses:  4.064139366149902 0.10934007167816162 0.7549751400947571
CurrentTrain: epoch  8, batch    31 | loss: 4.9284549Losses:  4.027286529541016 0.1264096349477768 0.8075972199440002
CurrentTrain: epoch  8, batch    32 | loss: 4.9612932Losses:  3.9887804985046387 0.11483107507228851 0.7784422636032104
CurrentTrain: epoch  8, batch    33 | loss: 4.8820539Losses:  4.024068832397461 0.11672475188970566 0.7710434198379517
CurrentTrain: epoch  8, batch    34 | loss: 4.9118371Losses:  4.014066696166992 0.12391789257526398 0.776820957660675
CurrentTrain: epoch  8, batch    35 | loss: 4.9148059Losses:  4.029300212860107 0.08731081336736679 0.7742593884468079
CurrentTrain: epoch  8, batch    36 | loss: 4.8908706Losses:  4.280604362487793 0.13491594791412354 0.668730616569519
CurrentTrain: epoch  8, batch    37 | loss: 5.0842509Losses:  3.9834752082824707 0.09351974725723267 0.6502957940101624
CurrentTrain: epoch  8, batch    38 | loss: 4.7272906Losses:  4.060317516326904 0.11241184175014496 0.7274900674819946
CurrentTrain: epoch  8, batch    39 | loss: 4.9002194Losses:  4.08746337890625 0.09475289285182953 0.7732031345367432
CurrentTrain: epoch  8, batch    40 | loss: 4.9554195Losses:  4.073178291320801 0.11747770011425018 0.7361364960670471
CurrentTrain: epoch  8, batch    41 | loss: 4.9267926Losses:  4.0757856369018555 0.09877877682447433 0.795153796672821
CurrentTrain: epoch  8, batch    42 | loss: 4.9697180Losses:  4.04574728012085 0.08360418677330017 0.706067681312561
CurrentTrain: epoch  8, batch    43 | loss: 4.8354192Losses:  4.011911392211914 0.1292111873626709 0.7318406701087952
CurrentTrain: epoch  8, batch    44 | loss: 4.8729634Losses:  4.00153923034668 0.09019723534584045 0.7653907537460327
CurrentTrain: epoch  8, batch    45 | loss: 4.8571272Losses:  4.0483832359313965 0.1168387308716774 0.7763718366622925
CurrentTrain: epoch  8, batch    46 | loss: 4.9415941Losses:  4.04453706741333 0.09036917239427567 0.7670422196388245
CurrentTrain: epoch  8, batch    47 | loss: 4.9019485Losses:  4.070333480834961 0.11403131484985352 0.7662638425827026
CurrentTrain: epoch  8, batch    48 | loss: 4.9506288Losses:  4.030475616455078 0.1032962054014206 0.7835108041763306
CurrentTrain: epoch  8, batch    49 | loss: 4.9172826Losses:  4.033564567565918 0.09259675443172455 0.7560445070266724
CurrentTrain: epoch  8, batch    50 | loss: 4.8822055Losses:  4.047792434692383 0.1380709409713745 0.7826038002967834
CurrentTrain: epoch  8, batch    51 | loss: 4.9684672Losses:  4.052759170532227 0.08835232257843018 0.7508468627929688
CurrentTrain: epoch  8, batch    52 | loss: 4.8919582Losses:  4.0114946365356445 0.14181970059871674 0.7442450523376465
CurrentTrain: epoch  8, batch    53 | loss: 4.8975592Losses:  4.011270523071289 0.11064062267541885 0.767524242401123
CurrentTrain: epoch  8, batch    54 | loss: 4.8894353Losses:  4.051766395568848 0.0924685075879097 0.803531289100647
CurrentTrain: epoch  8, batch    55 | loss: 4.9477663Losses:  4.056473731994629 0.07801096141338348 0.8185595273971558
CurrentTrain: epoch  8, batch    56 | loss: 4.9530444Losses:  4.006911277770996 0.11770675331354141 0.7791363596916199
CurrentTrain: epoch  8, batch    57 | loss: 4.9037542Losses:  4.027263164520264 0.0699411928653717 0.7319000959396362
CurrentTrain: epoch  8, batch    58 | loss: 4.8291044Losses:  4.031477928161621 0.10001114755868912 0.6862455606460571
CurrentTrain: epoch  8, batch    59 | loss: 4.8177347Losses:  4.045382499694824 0.09042862057685852 0.7158587574958801
CurrentTrain: epoch  8, batch    60 | loss: 4.8516703Losses:  4.035504341125488 0.10950067639350891 0.7222172021865845
CurrentTrain: epoch  8, batch    61 | loss: 4.8672223Losses:  4.014976501464844 0.09407676756381989 0.7525265216827393
CurrentTrain: epoch  8, batch    62 | loss: 4.8615799Losses:  4.041350841522217 0.12218800187110901 0.7466021180152893
CurrentTrain: epoch  9, batch     0 | loss: 4.9101410Losses:  4.007062911987305 0.09734104573726654 0.6851829290390015
CurrentTrain: epoch  9, batch     1 | loss: 4.7895870Losses:  4.030519485473633 0.10584380477666855 0.6979227066040039
CurrentTrain: epoch  9, batch     2 | loss: 4.8342862Losses:  4.0038886070251465 0.11170502752065659 0.7823591232299805
CurrentTrain: epoch  9, batch     3 | loss: 4.8979526Losses:  4.047341346740723 0.08855736255645752 0.7636982798576355
CurrentTrain: epoch  9, batch     4 | loss: 4.8995967Losses:  4.059205055236816 0.09797847270965576 0.7787905335426331
CurrentTrain: epoch  9, batch     5 | loss: 4.9359741Losses:  4.020169258117676 0.12909144163131714 0.7891370058059692
CurrentTrain: epoch  9, batch     6 | loss: 4.9383974Losses:  4.032751560211182 0.09336961805820465 0.7114514708518982
CurrentTrain: epoch  9, batch     7 | loss: 4.8375726Losses:  3.9675168991088867 0.10160009562969208 0.7018274068832397
CurrentTrain: epoch  9, batch     8 | loss: 4.7709446Losses:  4.016074180603027 0.10892442613840103 0.6789987087249756
CurrentTrain: epoch  9, batch     9 | loss: 4.8039970Losses:  4.034631252288818 0.0993792861700058 0.7056169509887695
CurrentTrain: epoch  9, batch    10 | loss: 4.8396273Losses:  4.035347938537598 0.1287180483341217 0.7679440379142761
CurrentTrain: epoch  9, batch    11 | loss: 4.9320097Losses:  4.015243053436279 0.10284237563610077 0.7441945672035217
CurrentTrain: epoch  9, batch    12 | loss: 4.8622799Losses:  4.014299392700195 0.07806939631700516 0.722549319267273
CurrentTrain: epoch  9, batch    13 | loss: 4.8149180Losses:  3.9829816818237305 0.1145700067281723 0.7048789262771606
CurrentTrain: epoch  9, batch    14 | loss: 4.8024306Losses:  4.028510570526123 0.11050795018672943 0.8092159032821655
CurrentTrain: epoch  9, batch    15 | loss: 4.9482346Losses:  4.000751972198486 0.12414903938770294 0.6871802806854248
CurrentTrain: epoch  9, batch    16 | loss: 4.8120813Losses:  4.003323078155518 0.10875596106052399 0.8227129578590393
CurrentTrain: epoch  9, batch    17 | loss: 4.9347920Losses:  3.9814364910125732 0.09919260442256927 0.7234765291213989
CurrentTrain: epoch  9, batch    18 | loss: 4.8041053Losses:  4.028733253479004 0.1386999785900116 0.7789440155029297
CurrentTrain: epoch  9, batch    19 | loss: 4.9463773Losses:  3.996748447418213 0.11839570105075836 0.7734724283218384
CurrentTrain: epoch  9, batch    20 | loss: 4.8886166Losses:  4.01028299331665 0.1006980612874031 0.7542816996574402
CurrentTrain: epoch  9, batch    21 | loss: 4.8652625Losses:  3.9796388149261475 0.11968683451414108 0.7134071588516235
CurrentTrain: epoch  9, batch    22 | loss: 4.8127327Losses:  3.998137950897217 0.10434970259666443 0.6775140762329102
CurrentTrain: epoch  9, batch    23 | loss: 4.7800016Losses:  4.028725624084473 0.09406913071870804 0.7570104598999023
CurrentTrain: epoch  9, batch    24 | loss: 4.8798051Losses:  4.001293659210205 0.10587232559919357 0.7797478437423706
CurrentTrain: epoch  9, batch    25 | loss: 4.8869138Losses:  4.012173652648926 0.14542298018932343 0.7426999807357788
CurrentTrain: epoch  9, batch    26 | loss: 4.9002967Losses:  4.047747611999512 0.11754821240901947 0.8030660152435303
CurrentTrain: epoch  9, batch    27 | loss: 4.9683619Losses:  3.9938669204711914 0.116251140832901 0.7039954662322998
CurrentTrain: epoch  9, batch    28 | loss: 4.8141136Losses:  3.9897522926330566 0.08358747512102127 0.7319218516349792
CurrentTrain: epoch  9, batch    29 | loss: 4.8052616Losses:  4.03187370300293 0.12795555591583252 0.7923270463943481
CurrentTrain: epoch  9, batch    30 | loss: 4.9521561Losses:  3.996835947036743 0.10054778307676315 0.6999891996383667
CurrentTrain: epoch  9, batch    31 | loss: 4.7973728Losses:  3.9953174591064453 0.09414985775947571 0.8373579978942871
CurrentTrain: epoch  9, batch    32 | loss: 4.9268255Losses:  4.038311958312988 0.08497296273708344 0.7641613483428955
CurrentTrain: epoch  9, batch    33 | loss: 4.8874464Losses:  3.998347759246826 0.10174068808555603 0.7796519994735718
CurrentTrain: epoch  9, batch    34 | loss: 4.8797407Losses:  3.966047763824463 0.09549952298402786 0.6906847953796387
CurrentTrain: epoch  9, batch    35 | loss: 4.7522321Losses:  3.9638757705688477 0.07209364324808121 0.6863908767700195
CurrentTrain: epoch  9, batch    36 | loss: 4.7223601Losses:  4.003685474395752 0.0956193208694458 0.7808732986450195
CurrentTrain: epoch  9, batch    37 | loss: 4.8801780Losses:  3.991969347000122 0.09273337572813034 0.7065127491950989
CurrentTrain: epoch  9, batch    38 | loss: 4.7912154Losses:  3.9918603897094727 0.09810246527194977 0.7468826174736023
CurrentTrain: epoch  9, batch    39 | loss: 4.8368454Losses:  4.013967990875244 0.11432304233312607 0.7398966550827026
CurrentTrain: epoch  9, batch    40 | loss: 4.8681879Losses:  3.9464046955108643 0.06701131165027618 0.7998151183128357
CurrentTrain: epoch  9, batch    41 | loss: 4.8132310Losses:  4.0247697830200195 0.05573834478855133 0.8205024600028992
CurrentTrain: epoch  9, batch    42 | loss: 4.9010105Losses:  4.01107120513916 0.10370554774999619 0.7343246340751648
CurrentTrain: epoch  9, batch    43 | loss: 4.8491011Losses:  3.983200788497925 0.09205645322799683 0.7332751750946045
CurrentTrain: epoch  9, batch    44 | loss: 4.8085327Losses:  3.974543809890747 0.06603197008371353 0.6888687610626221
CurrentTrain: epoch  9, batch    45 | loss: 4.7294445Losses:  3.9869186878204346 0.08683472871780396 0.7134886980056763
CurrentTrain: epoch  9, batch    46 | loss: 4.7872419Losses:  4.012248992919922 0.07580648362636566 0.7850908637046814
CurrentTrain: epoch  9, batch    47 | loss: 4.8731465Losses:  3.998913288116455 0.07352637499570847 0.6949590444564819
CurrentTrain: epoch  9, batch    48 | loss: 4.7673988Losses:  3.9833641052246094 0.07012347877025604 0.7404786944389343
CurrentTrain: epoch  9, batch    49 | loss: 4.7939663Losses:  3.981703281402588 0.12663021683692932 0.685861349105835
CurrentTrain: epoch  9, batch    50 | loss: 4.7941952Losses:  3.9706783294677734 0.07343640923500061 0.7839953899383545
CurrentTrain: epoch  9, batch    51 | loss: 4.8281097Losses:  4.030400276184082 0.12054973840713501 0.7301984429359436
CurrentTrain: epoch  9, batch    52 | loss: 4.8811483Losses:  4.023208141326904 0.07859396189451218 0.8129683136940002
CurrentTrain: epoch  9, batch    53 | loss: 4.9147701Losses:  4.032492637634277 0.0626801997423172 0.7628875374794006
CurrentTrain: epoch  9, batch    54 | loss: 4.8580604Losses:  4.033961296081543 0.09640146791934967 0.6786954402923584
CurrentTrain: epoch  9, batch    55 | loss: 4.8090582Losses:  4.022177696228027 0.1146252304315567 0.8129706382751465
CurrentTrain: epoch  9, batch    56 | loss: 4.9497738Losses:  4.011556148529053 0.08394565433263779 0.7937310934066772
CurrentTrain: epoch  9, batch    57 | loss: 4.8892331Losses:  3.9962520599365234 0.08874935656785965 0.7138756513595581
CurrentTrain: epoch  9, batch    58 | loss: 4.7988772Losses:  3.975283145904541 0.09859006106853485 0.7260957956314087
CurrentTrain: epoch  9, batch    59 | loss: 4.7999687Losses:  4.007483005523682 0.09531829506158829 0.7225286960601807
CurrentTrain: epoch  9, batch    60 | loss: 4.8253298Losses:  4.002233982086182 0.09540623426437378 0.7834905385971069
CurrentTrain: epoch  9, batch    61 | loss: 4.8811307Losses:  4.063932418823242 0.04701181873679161 0.7077677249908447
CurrentTrain: epoch  9, batch    62 | loss: 4.8187122
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [1 5 4 0 1 1 7 1 6 8 2 0 1 0 0 3 1 1 1 1]
Losses:  8.29641342163086 1.830413818359375 0.9967730045318604
CurrentTrain: epoch  0, batch     0 | loss: 11.1236000Losses:  7.911888599395752 1.741671085357666 0.995712161064148
CurrentTrain: epoch  0, batch     1 | loss: 10.6492720Losses:  7.2881669998168945 2.176485538482666 0.986197292804718
CurrentTrain: epoch  0, batch     2 | loss: 10.4508505Losses:  6.013547897338867 0.4879060983657837 0.9708975553512573
CurrentTrain: epoch  0, batch     3 | loss: 7.4723516Losses:  7.468885898590088 1.9821925163269043 0.9960539937019348
CurrentTrain: epoch  1, batch     0 | loss: 10.4471321Losses:  6.336933135986328 1.8931398391723633 0.9869869351387024
CurrentTrain: epoch  1, batch     1 | loss: 9.2170601Losses:  6.460944175720215 1.9235987663269043 0.9799143075942993
CurrentTrain: epoch  1, batch     2 | loss: 9.3644571Losses:  5.579006195068359 0.4966672956943512 0.9899328351020813
CurrentTrain: epoch  1, batch     3 | loss: 7.0656066Losses:  6.578547954559326 1.6821550130844116 0.989090085029602
CurrentTrain: epoch  2, batch     0 | loss: 9.2497931Losses:  5.124536514282227 1.7364494800567627 0.9768247008323669
CurrentTrain: epoch  2, batch     1 | loss: 7.8378105Losses:  4.979808330535889 1.7715705633163452 0.9801450967788696
CurrentTrain: epoch  2, batch     2 | loss: 7.7315240Losses:  7.925504684448242 0.7968945503234863 0.9480462670326233
CurrentTrain: epoch  2, batch     3 | loss: 9.6704454Losses:  5.292620658874512 1.833362340927124 0.9784679412841797
CurrentTrain: epoch  3, batch     0 | loss: 8.1044512Losses:  5.405391216278076 1.9592077732086182 0.9808306694030762
CurrentTrain: epoch  3, batch     1 | loss: 8.3454304Losses:  4.588424205780029 1.8114452362060547 0.9775654077529907
CurrentTrain: epoch  3, batch     2 | loss: 7.3774347Losses:  5.704654693603516 0.6303714513778687 0.9686013460159302
CurrentTrain: epoch  3, batch     3 | loss: 7.3036275Losses:  4.6837568283081055 1.5962443351745605 0.9674684405326843
CurrentTrain: epoch  4, batch     0 | loss: 7.2474694Losses:  5.1302337646484375 1.1556363105773926 0.9819595217704773
CurrentTrain: epoch  4, batch     1 | loss: 7.2678294Losses:  4.646258354187012 1.6570111513137817 0.9778263568878174
CurrentTrain: epoch  4, batch     2 | loss: 7.2810955Losses:  5.22264289855957 0.5558983087539673 0.9765722751617432
CurrentTrain: epoch  4, batch     3 | loss: 6.7551136Losses:  5.593606948852539 1.3294060230255127 0.9689672589302063
CurrentTrain: epoch  5, batch     0 | loss: 7.8919802Losses:  4.307877063751221 1.7826108932495117 0.968878984451294
CurrentTrain: epoch  5, batch     1 | loss: 7.0593672Losses:  3.754086494445801 1.676052451133728 0.9734984040260315
CurrentTrain: epoch  5, batch     2 | loss: 6.4036374Losses:  2.3742220401763916 0.5148099064826965 0.9388694763183594
CurrentTrain: epoch  5, batch     3 | loss: 3.8279014Losses:  4.677483558654785 1.626021385192871 0.9683757424354553
CurrentTrain: epoch  6, batch     0 | loss: 7.2718806Losses:  3.778703451156616 1.3746864795684814 0.9662982225418091
CurrentTrain: epoch  6, batch     1 | loss: 6.1196880Losses:  4.1467742919921875 1.74123215675354 0.9670987129211426
CurrentTrain: epoch  6, batch     2 | loss: 6.8551049Losses:  5.562355041503906 0.5764356851577759 0.9841451644897461
CurrentTrain: epoch  6, batch     3 | loss: 7.1229358Losses:  4.101611614227295 1.4862358570098877 0.9613023400306702
CurrentTrain: epoch  7, batch     0 | loss: 6.5491500Losses:  3.3752923011779785 1.5269734859466553 0.971686601638794
CurrentTrain: epoch  7, batch     1 | loss: 5.8739519Losses:  4.09266471862793 1.4869575500488281 0.949854850769043
CurrentTrain: epoch  7, batch     2 | loss: 6.5294771Losses:  8.025002479553223 3.5762795391747204e-07 0.9822303652763367
CurrentTrain: epoch  7, batch     3 | loss: 9.0072327Losses:  4.0844597816467285 1.6616928577423096 0.9787771105766296
CurrentTrain: epoch  8, batch     0 | loss: 6.7249298Losses:  3.9839820861816406 1.4167907238006592 0.9498653411865234
CurrentTrain: epoch  8, batch     1 | loss: 6.3506384Losses:  3.66711163520813 1.635890007019043 0.9572270512580872
CurrentTrain: epoch  8, batch     2 | loss: 6.2602286Losses:  4.186426639556885 0.36371171474456787 0.9790983200073242
CurrentTrain: epoch  8, batch     3 | loss: 5.5292368Losses:  3.3529367446899414 1.4369263648986816 0.9669492840766907
CurrentTrain: epoch  9, batch     0 | loss: 5.7568126Losses:  4.379383563995361 1.5279591083526611 0.951688289642334
CurrentTrain: epoch  9, batch     1 | loss: 6.8590312Losses:  3.150729179382324 1.4872065782546997 0.9594359993934631
CurrentTrain: epoch  9, batch     2 | loss: 5.5973716Losses:  4.6510796546936035 0.5702089667320251 0.940407395362854
CurrentTrain: epoch  9, batch     3 | loss: 6.1616960
Losses:  0.5834823250770569 0.895296037197113 0.8938833475112915
MemoryTrain:  epoch  0, batch     0 | loss: 2.3726616Losses:  0.5164769887924194 0.8305660486221313 0.9698370695114136
MemoryTrain:  epoch  0, batch     1 | loss: 2.3168802Losses:  0.8119264245033264 1.325560450553894 0.9067656993865967
MemoryTrain:  epoch  1, batch     0 | loss: 3.0442526Losses:  0.10022473335266113 0.24517938494682312 0.9232022762298584
MemoryTrain:  epoch  1, batch     1 | loss: 1.2686064Losses:  0.2777283787727356 0.9258319735527039 0.8943454027175903
MemoryTrain:  epoch  2, batch     0 | loss: 2.0979056Losses:  0.21169310808181763 0.710208535194397 0.943095326423645
MemoryTrain:  epoch  2, batch     1 | loss: 1.8649969Losses:  0.06669527292251587 1.1271789073944092 0.9221265912055969
MemoryTrain:  epoch  3, batch     0 | loss: 2.1160007Losses:  0.009687719866633415 0.2307303249835968 0.8301937580108643
MemoryTrain:  epoch  3, batch     1 | loss: 1.0706118Losses:  0.03608124703168869 1.1280535459518433 0.9453339576721191
MemoryTrain:  epoch  4, batch     0 | loss: 2.1094687Losses:  0.03635461628437042 0.16364578902721405 0.738323450088501
MemoryTrain:  epoch  4, batch     1 | loss: 0.9383239Losses:  0.04149792715907097 0.9865993857383728 0.9273117780685425
MemoryTrain:  epoch  5, batch     0 | loss: 1.9554090Losses:  0.061586230993270874 0.4376368522644043 0.8139152526855469
MemoryTrain:  epoch  5, batch     1 | loss: 1.3131384Losses:  0.025744091719388962 0.9584999680519104 0.8981316685676575
MemoryTrain:  epoch  6, batch     0 | loss: 1.8823757Losses:  0.027764813974499702 0.26892322301864624 0.9101779460906982
MemoryTrain:  epoch  6, batch     1 | loss: 1.2068660Losses:  0.020047828555107117 0.7189847826957703 0.8910905122756958
MemoryTrain:  epoch  7, batch     0 | loss: 1.6301231Losses:  0.019394099712371826 0.6901885867118835 0.9382660388946533
MemoryTrain:  epoch  7, batch     1 | loss: 1.6478487Losses:  0.01695740409195423 0.8088977932929993 0.8943057656288147
MemoryTrain:  epoch  8, batch     0 | loss: 1.7201610Losses:  0.021989230066537857 0.39812594652175903 0.9221221804618835
MemoryTrain:  epoch  8, batch     1 | loss: 1.3422374Losses:  0.022866293787956238 0.7261135578155518 0.889654278755188
MemoryTrain:  epoch  9, batch     0 | loss: 1.6386342Losses:  0.021469026803970337 0.4521210491657257 0.9176232218742371
MemoryTrain:  epoch  9, batch     1 | loss: 1.3912133
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 61.81%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 57.54%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 55.83%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 54.03%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 54.10%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 55.30%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 55.88%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 57.81%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 58.28%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 58.72%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 60.98%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 62.06%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 62.64%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 62.36%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 61.84%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 61.85%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 61.73%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 61.62%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 61.52%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 62.26%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 63.07%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 62.93%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 62.39%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 62.40%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 61.81%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.83%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 94.50%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.67%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.66%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 94.05%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 91.63%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 90.53%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 89.93%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 88.88%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.29%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.61%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.60%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 89.08%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 88.80%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.40%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 88.02%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 87.87%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 87.43%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 86.85%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 86.22%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 85.25%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 84.31%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 83.38%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 82.47%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 81.65%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 80.92%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 81.12%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 81.19%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 81.12%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 81.19%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 81.31%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 81.31%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 81.31%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 80.96%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 80.73%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 80.45%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 80.35%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 80.25%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 79.92%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 79.77%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 79.86%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 79.48%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 79.24%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 78.89%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 78.76%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 78.50%   
cur_acc:  ['0.9435', '0.6181']
his_acc:  ['0.9435', '0.7850']
Clustering into  14  clusters
Clusters:  [ 2 11  7  0  2  2 12  2 10  5  9  0  2  0  0  4  2  2  2  2  2  2  2  8
  2  3  2  1 13  6]
Losses:  6.207674503326416 1.2962918281555176 0.9728647470474243
CurrentTrain: epoch  0, batch     0 | loss: 8.4768314Losses:  6.634496688842773 1.185126781463623 0.970329999923706
CurrentTrain: epoch  0, batch     1 | loss: 8.7899532Losses:  5.737650394439697 1.4323692321777344 0.9783508777618408
CurrentTrain: epoch  0, batch     2 | loss: 8.1483707Losses:  5.446438312530518 0.32756978273391724 0.9329270124435425
CurrentTrain: epoch  0, batch     3 | loss: 6.7069354Losses:  5.040530204772949 1.3544116020202637 0.969050407409668
CurrentTrain: epoch  1, batch     0 | loss: 7.3639922Losses:  4.746586799621582 1.2801119089126587 0.9553183317184448
CurrentTrain: epoch  1, batch     1 | loss: 6.9820170Losses:  4.792764663696289 1.167776346206665 0.9760780930519104
CurrentTrain: epoch  1, batch     2 | loss: 6.9366188Losses:  4.235507965087891 0.3353472054004669 0.9827190637588501
CurrentTrain: epoch  1, batch     3 | loss: 5.5535741Losses:  4.165383815765381 1.1060175895690918 0.9620797634124756
CurrentTrain: epoch  2, batch     0 | loss: 6.2334814Losses:  4.032182693481445 1.0361027717590332 0.9819758534431458
CurrentTrain: epoch  2, batch     1 | loss: 6.0502615Losses:  4.079277992248535 1.19944167137146 0.9580868482589722
CurrentTrain: epoch  2, batch     2 | loss: 6.2368069Losses:  5.292694091796875 0.2668377161026001 0.9308660626411438
CurrentTrain: epoch  2, batch     3 | loss: 6.4903979Losses:  3.8540608882904053 1.0091725587844849 0.956688404083252
CurrentTrain: epoch  3, batch     0 | loss: 5.8199220Losses:  3.6921918392181396 1.0139681100845337 0.9641935229301453
CurrentTrain: epoch  3, batch     1 | loss: 5.6703534Losses:  3.082357406616211 1.0256474018096924 0.9571751356124878
CurrentTrain: epoch  3, batch     2 | loss: 5.0651798Losses:  4.702803134918213 0.11081264913082123 0.9645172357559204
CurrentTrain: epoch  3, batch     3 | loss: 5.7781329Losses:  2.8994812965393066 0.7765621542930603 0.958867073059082
CurrentTrain: epoch  4, batch     0 | loss: 4.6349106Losses:  3.8738512992858887 0.9837008118629456 0.9598000049591064
CurrentTrain: epoch  4, batch     1 | loss: 5.8173523Losses:  3.2119531631469727 0.8882835507392883 0.9629570841789246
CurrentTrain: epoch  4, batch     2 | loss: 5.0631938Losses:  2.6806225776672363 0.41047561168670654 0.9359063506126404
CurrentTrain: epoch  4, batch     3 | loss: 4.0270047Losses:  3.2512779235839844 0.7923362851142883 0.9520403146743774
CurrentTrain: epoch  5, batch     0 | loss: 4.9956546Losses:  2.9578261375427246 0.8306152820587158 0.9692094326019287
CurrentTrain: epoch  5, batch     1 | loss: 4.7576509Losses:  2.81282114982605 0.8958681225776672 0.9524674415588379
CurrentTrain: epoch  5, batch     2 | loss: 4.6611567Losses:  3.708085536956787 0.12016279995441437 1.0
CurrentTrain: epoch  5, batch     3 | loss: 4.8282480Losses:  2.882937431335449 0.8074742555618286 0.9552201628684998
CurrentTrain: epoch  6, batch     0 | loss: 4.6456318Losses:  2.535126209259033 0.8697284460067749 0.9765701293945312
CurrentTrain: epoch  6, batch     1 | loss: 4.3814249Losses:  3.0551419258117676 0.8125337362289429 0.9430347681045532
CurrentTrain: epoch  6, batch     2 | loss: 4.8107104Losses:  2.9471592903137207 0.0790979415178299 0.9400209784507751
CurrentTrain: epoch  6, batch     3 | loss: 3.9662783Losses:  2.474393129348755 0.788885235786438 0.9726729393005371
CurrentTrain: epoch  7, batch     0 | loss: 4.2359514Losses:  2.896169662475586 0.8753145933151245 0.9523038268089294
CurrentTrain: epoch  7, batch     1 | loss: 4.7237883Losses:  3.0491104125976562 0.743775486946106 0.9509789943695068
CurrentTrain: epoch  7, batch     2 | loss: 4.7438650Losses:  4.870007514953613 0.06419788300991058 0.9348365068435669
CurrentTrain: epoch  7, batch     3 | loss: 5.8690419Losses:  2.4735159873962402 0.7140677571296692 0.9696061015129089
CurrentTrain: epoch  8, batch     0 | loss: 4.1571898Losses:  2.8054933547973633 0.8412305116653442 0.9544713497161865
CurrentTrain: epoch  8, batch     1 | loss: 4.6011953Losses:  2.2430434226989746 0.8612035512924194 0.9481884837150574
CurrentTrain: epoch  8, batch     2 | loss: 4.0524354Losses:  3.9771766662597656 8.94069742685133e-08 0.9208009243011475
CurrentTrain: epoch  8, batch     3 | loss: 4.8979778Losses:  2.4481887817382812 0.669346809387207 0.9572001695632935
CurrentTrain: epoch  9, batch     0 | loss: 4.0747356Losses:  2.509510040283203 0.8447959423065186 0.9496197700500488
CurrentTrain: epoch  9, batch     1 | loss: 4.3039255Losses:  2.491955041885376 0.6478464007377625 0.95491623878479
CurrentTrain: epoch  9, batch     2 | loss: 4.0947180Losses:  1.9285800457000732 0.04303582385182381 0.9413745999336243
CurrentTrain: epoch  9, batch     3 | loss: 2.9129906
Losses:  1.7782447338104248 1.1388659477233887 0.9057373404502869
MemoryTrain:  epoch  0, batch     0 | loss: 3.8228481Losses:  1.0718353986740112 0.8032764196395874 0.9197840690612793
MemoryTrain:  epoch  0, batch     1 | loss: 2.7948959Losses:  2.1289243698120117 1.0177594423294067 0.9095065593719482
MemoryTrain:  epoch  1, batch     0 | loss: 4.0561905Losses:  0.6980646848678589 0.9804883599281311 0.9130162000656128
MemoryTrain:  epoch  1, batch     1 | loss: 2.5915694Losses:  0.4591802656650543 1.078874111175537 0.9262044429779053
MemoryTrain:  epoch  2, batch     0 | loss: 2.4642587Losses:  1.6113743782043457 0.8386014103889465 0.8931334614753723
MemoryTrain:  epoch  2, batch     1 | loss: 3.3431091Losses:  0.609359860420227 0.9953587055206299 0.8946728110313416
MemoryTrain:  epoch  3, batch     0 | loss: 2.4993913Losses:  0.6067811846733093 0.7955073118209839 0.9284224510192871
MemoryTrain:  epoch  3, batch     1 | loss: 2.3307109Losses:  0.8865628242492676 0.9128214716911316 0.9169787764549255
MemoryTrain:  epoch  4, batch     0 | loss: 2.7163632Losses:  0.10071047395467758 0.9253655672073364 0.902409553527832
MemoryTrain:  epoch  4, batch     1 | loss: 1.9284856Losses:  0.0975935161113739 1.0879170894622803 0.9172775149345398
MemoryTrain:  epoch  5, batch     0 | loss: 2.1027882Losses:  0.8894182443618774 0.7951133847236633 0.8956782817840576
MemoryTrain:  epoch  5, batch     1 | loss: 2.5802100Losses:  0.5620356798171997 0.9065170884132385 0.903744637966156
MemoryTrain:  epoch  6, batch     0 | loss: 2.3722975Losses:  0.2260027676820755 0.8855664730072021 0.917590320110321
MemoryTrain:  epoch  6, batch     1 | loss: 2.0291595Losses:  0.24838921427726746 1.0688848495483398 0.8763067722320557
MemoryTrain:  epoch  7, batch     0 | loss: 2.1935809Losses:  0.20894677937030792 0.7795659899711609 0.9417336583137512
MemoryTrain:  epoch  7, batch     1 | loss: 1.9302464Losses:  0.16235511004924774 1.1180144548416138 0.8981077671051025
MemoryTrain:  epoch  8, batch     0 | loss: 2.1784773Losses:  0.09943271428346634 0.5656567811965942 0.9171500205993652
MemoryTrain:  epoch  8, batch     1 | loss: 1.5822395Losses:  0.21475474536418915 0.9160102605819702 0.8823465704917908
MemoryTrain:  epoch  9, batch     0 | loss: 2.0131116Losses:  0.1096421554684639 0.6597184538841248 0.9269646406173706
MemoryTrain:  epoch  9, batch     1 | loss: 1.6963253
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 78.03%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 74.51%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 72.97%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 67.86%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.72%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.12%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.07%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.53%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.21%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.24%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.15%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 91.02%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 89.71%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 88.54%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 87.41%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 86.31%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 85.51%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 85.45%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 85.48%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.90%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.41%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 85.82%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 85.02%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 84.08%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 83.31%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 82.49%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 81.61%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 80.82%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 79.92%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 79.03%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 78.16%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 77.31%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 76.55%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 75.86%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 76.36%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 77.16%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 77.12%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 76.93%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 76.91%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 76.75%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 76.76%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 76.85%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 76.51%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 76.29%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 76.08%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 75.97%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.01%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 76.86%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 77.49%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 77.52%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 77.62%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 77.53%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 77.66%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 77.64%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 77.62%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 77.48%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 77.43%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 77.35%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 77.02%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 76.68%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 76.47%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 76.31%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 76.14%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 75.98%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.54%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 75.42%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 75.26%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 75.15%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 75.11%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 74.96%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 74.67%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 74.42%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 74.13%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 73.88%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 73.60%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 73.43%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 74.80%   
cur_acc:  ['0.9435', '0.6181', '0.7272']
his_acc:  ['0.9435', '0.7850', '0.7480']
Clustering into  19  clusters
Clusters:  [ 1 11 15  0  1  1 16  1 12 17 10  0  1  0  0  9  1  1  1  1  1  1  1  4
  1  7  1 18 13  5  8  1 14  1  6  2  3  1  1  1]
Losses:  5.7107133865356445 1.5652133226394653 0.9802483320236206
CurrentTrain: epoch  0, batch     0 | loss: 8.2561750Losses:  6.894599437713623 1.6349796056747437 0.97770756483078
CurrentTrain: epoch  0, batch     1 | loss: 9.5072870Losses:  6.899181365966797 1.4183083772659302 0.9787200689315796
CurrentTrain: epoch  0, batch     2 | loss: 9.2962093Losses:  6.005490779876709 0.40259987115859985 0.9818637371063232
CurrentTrain: epoch  0, batch     3 | loss: 7.3899546Losses:  6.36299467086792 1.581800937652588 0.9860243797302246
CurrentTrain: epoch  1, batch     0 | loss: 8.9308205Losses:  4.848484992980957 1.5270015001296997 0.9707177877426147
CurrentTrain: epoch  1, batch     1 | loss: 7.3462043Losses:  5.003911972045898 1.430609941482544 0.96919846534729
CurrentTrain: epoch  1, batch     2 | loss: 7.4037199Losses:  6.652390480041504 0.38318872451782227 0.9939483404159546
CurrentTrain: epoch  1, batch     3 | loss: 8.0295277Losses:  4.383724212646484 1.3200738430023193 0.9860618114471436
CurrentTrain: epoch  2, batch     0 | loss: 6.6898603Losses:  5.529290199279785 1.3584539890289307 0.9846451282501221
CurrentTrain: epoch  2, batch     1 | loss: 7.8723888Losses:  5.1936116218566895 1.4148608446121216 0.9605657458305359
CurrentTrain: epoch  2, batch     2 | loss: 7.5690379Losses:  2.5694289207458496 5.960464477539063e-08 0.8760048151016235
CurrentTrain: epoch  2, batch     3 | loss: 3.4454336Losses:  3.5117807388305664 0.8388567566871643 0.9646453857421875
CurrentTrain: epoch  3, batch     0 | loss: 5.3152828Losses:  5.444689750671387 1.2366706132888794 0.9770146608352661
CurrentTrain: epoch  3, batch     1 | loss: 7.6583748Losses:  5.278364181518555 1.2266216278076172 0.967795729637146
CurrentTrain: epoch  3, batch     2 | loss: 7.4727817Losses:  2.6552743911743164 0.25078848004341125 0.9835634231567383
CurrentTrain: epoch  3, batch     3 | loss: 3.8896263Losses:  4.55950927734375 1.302985668182373 0.9579925537109375
CurrentTrain: epoch  4, batch     0 | loss: 6.8204875Losses:  4.1738505363464355 0.9828829169273376 0.9689098000526428
CurrentTrain: epoch  4, batch     1 | loss: 6.1256433Losses:  4.694540977478027 1.2072571516036987 0.9791196584701538
CurrentTrain: epoch  4, batch     2 | loss: 6.8809180Losses:  4.039986610412598 0.13049817085266113 0.984864354133606
CurrentTrain: epoch  4, batch     3 | loss: 5.1553488Losses:  4.616444110870361 1.3977231979370117 0.9748691916465759
CurrentTrain: epoch  5, batch     0 | loss: 6.9890366Losses:  3.6811509132385254 1.2584693431854248 0.9654879570007324
CurrentTrain: epoch  5, batch     1 | loss: 5.9051080Losses:  4.433106899261475 1.1105542182922363 0.9576249122619629
CurrentTrain: epoch  5, batch     2 | loss: 6.5012860Losses:  1.8033111095428467 8.94069742685133e-08 1.0
CurrentTrain: epoch  5, batch     3 | loss: 2.8033113Losses:  4.140748023986816 1.37290358543396 0.9621084928512573
CurrentTrain: epoch  6, batch     0 | loss: 6.4757605Losses:  3.968132972717285 1.0106245279312134 0.950454831123352
CurrentTrain: epoch  6, batch     1 | loss: 5.9292121Losses:  3.7514610290527344 1.1841089725494385 0.9816062450408936
CurrentTrain: epoch  6, batch     2 | loss: 5.9171762Losses:  2.8452227115631104 0.1202288568019867 1.0
CurrentTrain: epoch  6, batch     3 | loss: 3.9654515Losses:  4.442786693572998 1.0721499919891357 0.9529175758361816
CurrentTrain: epoch  7, batch     0 | loss: 6.4678540Losses:  3.8433234691619873 1.3014633655548096 0.9698120355606079
CurrentTrain: epoch  7, batch     1 | loss: 6.1145988Losses:  3.0859978199005127 1.1185686588287354 0.9746792316436768
CurrentTrain: epoch  7, batch     2 | loss: 5.1792459Losses:  1.8791167736053467 0.1337783932685852 0.9093009829521179
CurrentTrain: epoch  7, batch     3 | loss: 2.9221961Losses:  3.2933387756347656 1.1344316005706787 0.9646104574203491
CurrentTrain: epoch  8, batch     0 | loss: 5.3923812Losses:  3.796412467956543 1.1483614444732666 0.9556196928024292
CurrentTrain: epoch  8, batch     1 | loss: 5.9003935Losses:  3.8268566131591797 0.928020179271698 0.9673008918762207
CurrentTrain: epoch  8, batch     2 | loss: 5.7221775Losses:  1.863033413887024 0.18495027720928192 0.961024820804596
CurrentTrain: epoch  8, batch     3 | loss: 3.0090084Losses:  3.414654016494751 1.007972240447998 0.975723147392273
CurrentTrain: epoch  9, batch     0 | loss: 5.3983498Losses:  3.717801809310913 1.1656012535095215 0.94651198387146
CurrentTrain: epoch  9, batch     1 | loss: 5.8299150Losses:  3.042842388153076 1.0472166538238525 0.9666875600814819
CurrentTrain: epoch  9, batch     2 | loss: 5.0567470Losses:  4.136915683746338 0.29866182804107666 0.8979139924049377
CurrentTrain: epoch  9, batch     3 | loss: 5.3334913
Losses:  1.0155848264694214 1.1372859477996826 0.9027541279792786
MemoryTrain:  epoch  0, batch     0 | loss: 3.0556247Losses:  0.6816022396087646 1.125105381011963 0.9290204048156738
MemoryTrain:  epoch  0, batch     1 | loss: 2.7357280Losses:  0.9665825366973877 0.5200225114822388 0.9355252385139465
MemoryTrain:  epoch  0, batch     2 | loss: 2.4221303Losses:  1.3407163619995117 1.1974360942840576 0.9287542104721069
MemoryTrain:  epoch  1, batch     0 | loss: 3.4669065Losses:  1.0416874885559082 0.8896365165710449 0.9004930257797241
MemoryTrain:  epoch  1, batch     1 | loss: 2.8318172Losses:  0.5910998582839966 0.6891043186187744 0.9325448870658875
MemoryTrain:  epoch  1, batch     2 | loss: 2.2127490Losses:  0.7459065914154053 1.1631877422332764 0.9082546234130859
MemoryTrain:  epoch  2, batch     0 | loss: 2.8173490Losses:  0.5520042777061462 0.9589734077453613 0.8980817794799805
MemoryTrain:  epoch  2, batch     1 | loss: 2.4090595Losses:  0.5907025933265686 0.4983430504798889 0.9776871800422668
MemoryTrain:  epoch  2, batch     2 | loss: 2.0667329Losses:  0.31075629591941833 1.0644993782043457 0.8974778056144714
MemoryTrain:  epoch  3, batch     0 | loss: 2.2727334Losses:  0.4811267852783203 0.8869392275810242 0.9280394315719604
MemoryTrain:  epoch  3, batch     1 | loss: 2.2961054Losses:  0.4402819275856018 0.640347957611084 0.9250004291534424
MemoryTrain:  epoch  3, batch     2 | loss: 2.0056303Losses:  0.48512187600135803 1.1047704219818115 0.9263068437576294
MemoryTrain:  epoch  4, batch     0 | loss: 2.5161991Losses:  0.11141263693571091 0.9332636594772339 0.8937680125236511
MemoryTrain:  epoch  4, batch     1 | loss: 1.9384444Losses:  0.29622745513916016 0.6345052719116211 0.939497172832489
MemoryTrain:  epoch  4, batch     2 | loss: 1.8702300Losses:  0.14435364305973053 0.7990434765815735 0.9429788589477539
MemoryTrain:  epoch  5, batch     0 | loss: 1.8863759Losses:  0.06874320656061172 1.1562849283218384 0.8768562078475952
MemoryTrain:  epoch  5, batch     1 | loss: 2.1018844Losses:  0.586546778678894 0.7173316478729248 0.9267712831497192
MemoryTrain:  epoch  5, batch     2 | loss: 2.2306497Losses:  0.19414320588111877 0.9708058834075928 0.9148668646812439
MemoryTrain:  epoch  6, batch     0 | loss: 2.0798159Losses:  0.09570209681987762 0.8285361528396606 0.9184372425079346
MemoryTrain:  epoch  6, batch     1 | loss: 1.8426754Losses:  0.07485102862119675 0.7236818075180054 0.8998286128044128
MemoryTrain:  epoch  6, batch     2 | loss: 1.6983614Losses:  0.09753961861133575 1.025163173675537 0.9098675847053528
MemoryTrain:  epoch  7, batch     0 | loss: 2.0325704Losses:  0.07841646671295166 0.789993941783905 0.9149112701416016
MemoryTrain:  epoch  7, batch     1 | loss: 1.7833216Losses:  0.06179152801632881 0.5811837315559387 0.90239417552948
MemoryTrain:  epoch  7, batch     2 | loss: 1.5453694Losses:  0.09206217527389526 0.8092083930969238 0.9202290773391724
MemoryTrain:  epoch  8, batch     0 | loss: 1.8214996Losses:  0.09815790504217148 1.0926871299743652 0.89920973777771
MemoryTrain:  epoch  8, batch     1 | loss: 2.0900548Losses:  0.07949777692556381 0.4887346923351288 0.9133439660072327
MemoryTrain:  epoch  8, batch     2 | loss: 1.4815764Losses:  0.06708063185214996 1.1328027248382568 0.9184882640838623
MemoryTrain:  epoch  9, batch     0 | loss: 2.1183715Losses:  0.03751754388213158 0.7430164813995361 0.9076616764068604
MemoryTrain:  epoch  9, batch     1 | loss: 1.6881957Losses:  0.07462012767791748 0.393127977848053 0.8993886709213257
MemoryTrain:  epoch  9, batch     2 | loss: 1.3671367
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 75.61%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 74.60%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.54%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 74.89%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 74.12%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 73.38%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 72.56%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 71.67%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 71.00%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.78%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 92.69%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.21%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.47%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.04%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 88.65%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 87.50%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 86.38%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 85.29%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 84.60%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.42%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.12%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.06%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 85.44%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 85.55%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 84.98%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 84.11%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 83.18%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 82.35%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 81.54%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 80.68%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 79.97%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 79.14%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 78.26%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.40%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 76.63%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 75.87%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 75.20%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.46%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.23%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 76.93%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 76.53%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 76.40%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 76.05%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 75.92%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 75.95%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 75.84%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.36%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 75.15%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 74.85%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 74.50%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 74.11%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 73.58%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 73.16%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 72.88%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 72.42%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.01%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 73.53%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 73.68%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  143 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 73.84%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 73.89%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 73.87%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 73.88%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 73.80%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 73.73%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 73.69%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 73.66%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 73.43%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 73.12%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 72.93%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 72.82%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 72.72%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 72.54%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 72.52%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 72.35%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 72.33%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 72.10%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 71.86%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.58%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 71.38%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 71.11%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 70.88%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 70.65%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 72.18%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 72.21%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.17%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.38%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 72.49%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 72.44%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 72.36%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 72.02%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 71.80%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 73.15%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 73.07%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 73.02%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 72.85%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 72.70%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.64%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 72.67%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 72.85%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.91%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 72.94%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 72.68%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 72.46%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 72.34%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 72.10%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 71.91%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 71.75%   
cur_acc:  ['0.9435', '0.6181', '0.7272', '0.6935']
his_acc:  ['0.9435', '0.7850', '0.7480', '0.7175']
Clustering into  24  clusters
Clusters:  [ 2  1 15  0  2  2 19  2 14 21 22  0  2  0  0 20  2  2  2  2  2  2  2  9
  2 16  2 23 13 12 18  2 11  2  7 17  8  2  2  2  1  6  2  2  2  3  2 10
  5  4]
Losses:  4.851665496826172 1.4036805629730225 0.9778307676315308
CurrentTrain: epoch  0, batch     0 | loss: 7.2331772Losses:  7.0080037117004395 1.6752221584320068 0.99228835105896
CurrentTrain: epoch  0, batch     1 | loss: 9.6755142Losses:  5.991464614868164 1.5967130661010742 0.9653987884521484
CurrentTrain: epoch  0, batch     2 | loss: 8.5535765Losses:  4.573818206787109 0.17101547122001648 1.015140414237976
CurrentTrain: epoch  0, batch     3 | loss: 5.7599740Losses:  5.422290325164795 1.1154382228851318 0.9950231909751892
CurrentTrain: epoch  1, batch     0 | loss: 7.5327516Losses:  4.906183242797852 1.5865774154663086 0.9828156232833862
CurrentTrain: epoch  1, batch     1 | loss: 7.4755764Losses:  4.381833553314209 1.3402669429779053 0.9489163160324097
CurrentTrain: epoch  1, batch     2 | loss: 6.6710167Losses:  4.263230323791504 0.43596696853637695 1.0
CurrentTrain: epoch  1, batch     3 | loss: 5.6991973Losses:  5.162349700927734 1.1632065773010254 0.9887315034866333
CurrentTrain: epoch  2, batch     0 | loss: 7.3142877Losses:  3.9872565269470215 1.2423036098480225 0.9844801425933838
CurrentTrain: epoch  2, batch     1 | loss: 6.2140398Losses:  3.5300257205963135 1.2033765316009521 0.9559608101844788
CurrentTrain: epoch  2, batch     2 | loss: 5.6893630Losses:  6.11107873916626 0.42126598954200745 0.9839946031570435
CurrentTrain: epoch  2, batch     3 | loss: 7.5163393Losses:  3.7216954231262207 1.231123685836792 0.951759397983551
CurrentTrain: epoch  3, batch     0 | loss: 5.9045782Losses:  3.988589286804199 0.9613121151924133 1.0037932395935059
CurrentTrain: epoch  3, batch     1 | loss: 5.9536948Losses:  3.5376205444335938 1.2546271085739136 0.9738941192626953
CurrentTrain: epoch  3, batch     2 | loss: 5.7661419Losses:  5.943735122680664 0.37621259689331055 0.9616448283195496
CurrentTrain: epoch  3, batch     3 | loss: 7.2815924Losses:  3.244988441467285 1.040263295173645 0.9705555438995361
CurrentTrain: epoch  4, batch     0 | loss: 5.2558069Losses:  4.2603888511657715 0.9749871492385864 0.9726650714874268
CurrentTrain: epoch  4, batch     1 | loss: 6.2080412Losses:  3.0040042400360107 1.172734260559082 0.9682977199554443
CurrentTrain: epoch  4, batch     2 | loss: 5.1450367Losses:  5.2807936668396 0.4443603754043579 0.9909912347793579
CurrentTrain: epoch  4, batch     3 | loss: 6.7161450Losses:  3.5626423358917236 1.3391814231872559 0.965370774269104
CurrentTrain: epoch  5, batch     0 | loss: 5.8671947Losses:  3.1273529529571533 1.1940739154815674 0.969653844833374
CurrentTrain: epoch  5, batch     1 | loss: 5.2910805Losses:  3.290768623352051 1.094002366065979 0.9717236757278442
CurrentTrain: epoch  5, batch     2 | loss: 5.3564944Losses:  3.0832996368408203 0.22579801082611084 0.9971580505371094
CurrentTrain: epoch  5, batch     3 | loss: 4.3062558Losses:  3.2769827842712402 1.0493955612182617 0.9622132778167725
CurrentTrain: epoch  6, batch     0 | loss: 5.2885914Losses:  3.416076183319092 1.236817717552185 0.9608807563781738
CurrentTrain: epoch  6, batch     1 | loss: 5.6137748Losses:  2.713904857635498 1.095282793045044 0.9812812805175781
CurrentTrain: epoch  6, batch     2 | loss: 4.7904692Losses:  2.0100860595703125 0.20441865921020508 0.9830871820449829
CurrentTrain: epoch  6, batch     3 | loss: 3.1975918Losses:  2.7606687545776367 0.9037598371505737 0.9899797439575195
CurrentTrain: epoch  7, batch     0 | loss: 4.6544085Losses:  2.8556909561157227 0.8487601280212402 0.9552536010742188
CurrentTrain: epoch  7, batch     1 | loss: 4.6597047Losses:  3.0361690521240234 1.1356654167175293 0.9586178660392761
CurrentTrain: epoch  7, batch     2 | loss: 5.1304522Losses:  3.262004852294922 0.5809720158576965 0.9184327721595764
CurrentTrain: epoch  7, batch     3 | loss: 4.7614098Losses:  2.736823797225952 0.9899178743362427 0.9568989276885986
CurrentTrain: epoch  8, batch     0 | loss: 4.6836405Losses:  2.6691296100616455 0.9364116191864014 0.9706612825393677
CurrentTrain: epoch  8, batch     1 | loss: 4.5762024Losses:  2.9159159660339355 0.8875390887260437 0.9574312567710876
CurrentTrain: epoch  8, batch     2 | loss: 4.7608862Losses:  1.869958758354187 0.17834903299808502 0.9117177724838257
CurrentTrain: epoch  8, batch     3 | loss: 2.9600258Losses:  2.5207395553588867 0.9349871277809143 0.9573051929473877
CurrentTrain: epoch  9, batch     0 | loss: 4.4130316Losses:  2.3879354000091553 0.7306845188140869 0.9616338014602661
CurrentTrain: epoch  9, batch     1 | loss: 4.0802536Losses:  2.7875826358795166 0.9263243675231934 0.9435751438140869
CurrentTrain: epoch  9, batch     2 | loss: 4.6574821Losses:  2.795044183731079 0.08254382759332657 1.0246055126190186
CurrentTrain: epoch  9, batch     3 | loss: 3.9021935
Losses:  0.5801810026168823 1.1188316345214844 0.9688197374343872
MemoryTrain:  epoch  0, batch     0 | loss: 2.6678324Losses:  0.6606442928314209 1.120937466621399 0.8661402463912964
MemoryTrain:  epoch  0, batch     1 | loss: 2.6477220Losses:  0.31198838353157043 0.8135641813278198 0.9295376539230347
MemoryTrain:  epoch  0, batch     2 | loss: 2.0550902Losses:  0.07499449700117111 0.20718461275100708 0.9155286550521851
MemoryTrain:  epoch  0, batch     3 | loss: 1.1977078Losses:  0.7161939144134521 1.066074013710022 0.9075635671615601
MemoryTrain:  epoch  1, batch     0 | loss: 2.6898315Losses:  0.5385081768035889 0.8821539878845215 0.9463968873023987
MemoryTrain:  epoch  1, batch     1 | loss: 2.3670590Losses:  0.5141847133636475 1.0649840831756592 0.9016678333282471
MemoryTrain:  epoch  1, batch     2 | loss: 2.4808366Losses:  0.7731531858444214 0.04485508054494858 0.8983699083328247
MemoryTrain:  epoch  1, batch     3 | loss: 1.7163782Losses:  0.2605244219303131 0.8645036220550537 0.9221302270889282
MemoryTrain:  epoch  2, batch     0 | loss: 2.0471582Losses:  0.32618269324302673 0.9119992852210999 0.9251304268836975
MemoryTrain:  epoch  2, batch     1 | loss: 2.1633124Losses:  0.293743371963501 1.267655849456787 0.9079638719558716
MemoryTrain:  epoch  2, batch     2 | loss: 2.4693632Losses:  0.01564493589103222 0.021709538996219635 0.9065410494804382
MemoryTrain:  epoch  2, batch     3 | loss: 0.9438955Losses:  0.17236265540122986 0.9638468027114868 0.8932173848152161
MemoryTrain:  epoch  3, batch     0 | loss: 2.0294268Losses:  0.2509498596191406 1.107163667678833 0.8924587368965149
MemoryTrain:  epoch  3, batch     1 | loss: 2.2505722Losses:  0.13302023708820343 0.7773594856262207 0.9475452303886414
MemoryTrain:  epoch  3, batch     2 | loss: 1.8579249Losses:  0.04922407865524292 0.08736241608858109 1.0
MemoryTrain:  epoch  3, batch     3 | loss: 1.1365864Losses:  0.12830373644828796 0.9638746976852417 0.8927414417266846
MemoryTrain:  epoch  4, batch     0 | loss: 1.9849199Losses:  0.11674702167510986 0.9014103412628174 0.9352051019668579
MemoryTrain:  epoch  4, batch     1 | loss: 1.9533625Losses:  0.07022060453891754 1.0573725700378418 0.9164332151412964
MemoryTrain:  epoch  4, batch     2 | loss: 2.0440264Losses:  0.1915830671787262 0.0057019563391804695 0.8308659791946411
MemoryTrain:  epoch  4, batch     3 | loss: 1.0281510Losses:  0.1539069563150406 1.1480896472930908 0.9446151256561279
MemoryTrain:  epoch  5, batch     0 | loss: 2.2466116Losses:  0.08260591328144073 0.8203403353691101 0.8738714456558228
MemoryTrain:  epoch  5, batch     1 | loss: 1.7768177Losses:  0.06574846059083939 0.8831444382667542 0.9053981304168701
MemoryTrain:  epoch  5, batch     2 | loss: 1.8542910Losses:  0.031148772686719894 0.02986385114490986 1.0
MemoryTrain:  epoch  5, batch     3 | loss: 1.0610126Losses:  0.06470417976379395 0.8600828051567078 0.9217754006385803
MemoryTrain:  epoch  6, batch     0 | loss: 1.8465624Losses:  0.060468390583992004 0.8082319498062134 0.8990944623947144
MemoryTrain:  epoch  6, batch     1 | loss: 1.7677948Losses:  0.1016700491309166 0.9347643852233887 0.9149812459945679
MemoryTrain:  epoch  6, batch     2 | loss: 1.9514157Losses:  0.05201389268040657 0.12610381841659546 0.8518838882446289
MemoryTrain:  epoch  6, batch     3 | loss: 1.0300016Losses:  0.06208552047610283 1.006759762763977 0.9198427200317383
MemoryTrain:  epoch  7, batch     0 | loss: 1.9886880Losses:  0.05580724775791168 0.8459588289260864 0.9549062848091125
MemoryTrain:  epoch  7, batch     1 | loss: 1.8566723Losses:  0.03839442878961563 0.7777527570724487 0.8698700666427612
MemoryTrain:  epoch  7, batch     2 | loss: 1.6860173Losses:  0.021516378968954086 0.09035927057266235 0.8163486123085022
MemoryTrain:  epoch  7, batch     3 | loss: 0.9282243Losses:  0.0978727862238884 0.7560241222381592 0.8980752229690552
MemoryTrain:  epoch  8, batch     0 | loss: 1.7519722Losses:  0.0806465744972229 0.9672278761863708 0.9162476062774658
MemoryTrain:  epoch  8, batch     1 | loss: 1.9641221Losses:  0.11585366725921631 0.9370888471603394 0.9139806032180786
MemoryTrain:  epoch  8, batch     2 | loss: 1.9669231Losses:  0.013602189719676971 0.013182207942008972 0.8879961967468262
MemoryTrain:  epoch  8, batch     3 | loss: 0.9147806Losses:  0.08375811576843262 0.6972266435623169 0.9068136215209961
MemoryTrain:  epoch  9, batch     0 | loss: 1.6877984Losses:  0.10854697972536087 0.8613519668579102 0.9070948362350464
MemoryTrain:  epoch  9, batch     1 | loss: 1.8769938Losses:  0.02889491245150566 0.8629111647605896 0.8972102403640747
MemoryTrain:  epoch  9, batch     2 | loss: 1.7890162Losses:  0.23555442690849304 0.0663643628358841 0.9496578574180603
MemoryTrain:  epoch  9, batch     3 | loss: 1.2515767
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 72.71%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 70.70%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 65.37%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 64.97%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 66.44%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.96%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.18%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.14%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.98%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.19%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 86.82%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 85.58%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 84.47%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 83.40%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 82.35%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 81.70%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.60%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 82.09%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 82.31%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 82.32%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 81.48%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 80.58%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 79.78%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 79.00%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 78.16%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 77.49%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 76.69%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 75.83%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 74.18%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 72.87%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 73.03%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.18%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 73.32%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 73.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 73.89%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 74.08%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 74.27%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.53%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 73.90%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 73.34%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 72.78%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 72.24%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 71.60%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 71.24%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 71.07%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 70.78%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 70.34%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 69.98%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 68.89%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 68.51%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.66%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 69.95%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 70.06%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 70.29%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 70.32%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 70.33%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 70.27%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 70.18%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 69.92%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 69.67%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 69.47%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 69.30%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 69.22%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 69.22%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 69.21%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 69.02%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 68.94%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 68.83%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 68.42%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 68.24%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 67.74%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 67.43%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 69.31%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 69.16%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 69.33%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 69.38%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 69.20%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 68.99%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 68.72%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 68.54%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 68.28%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 68.25%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 69.88%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 69.98%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 70.13%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 70.01%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.30%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 70.36%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 70.15%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 69.96%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 69.73%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 69.63%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 69.55%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 69.78%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 69.93%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 69.88%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.86%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 69.78%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 69.76%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  275 | acc: 25.00%,  total acc: 70.43%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 70.24%   [EVAL] batch:  277 | acc: 31.25%,  total acc: 70.10%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 70.03%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  280 | acc: 31.25%,  total acc: 69.73%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 69.66%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 69.45%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 69.32%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 68.99%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 68.92%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 68.97%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 69.18%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 69.11%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 69.02%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 68.92%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.05%   
cur_acc:  ['0.9435', '0.6181', '0.7272', '0.6935', '0.7212']
his_acc:  ['0.9435', '0.7850', '0.7480', '0.7175', '0.7005']
Clustering into  29  clusters
Clusters:  [ 0  1 21  0  0  0 25  0 15 27 16  0  0  2  2  7  0  0  0  0  0  0  0 28
  0 19  0 23 17 22 24  0 14  0 18 20  9  0  0  0  1  8  0  0  0  3  0 13
 26 11 10  4  2  0  0 12  0  6  0  5]
Losses:  5.918885231018066 1.24735426902771 0.9719935655593872
CurrentTrain: epoch  0, batch     0 | loss: 8.1382332Losses:  6.7345685958862305 1.4770861864089966 0.9779070615768433
CurrentTrain: epoch  0, batch     1 | loss: 9.1895618Losses:  6.875856399536133 1.6718038320541382 0.9820461273193359
CurrentTrain: epoch  0, batch     2 | loss: 9.5297060Losses:  8.51905632019043 0.32532018423080444 0.9806641340255737
CurrentTrain: epoch  0, batch     3 | loss: 9.8250408Losses:  4.841701984405518 1.2782847881317139 0.960680365562439
CurrentTrain: epoch  1, batch     0 | loss: 7.0806670Losses:  5.232448101043701 1.3767592906951904 0.9698449373245239
CurrentTrain: epoch  1, batch     1 | loss: 7.5790520Losses:  5.709887981414795 1.7580819129943848 0.9812874794006348
CurrentTrain: epoch  1, batch     2 | loss: 8.4492569Losses:  8.964624404907227 0.7030598521232605 0.9976577758789062
CurrentTrain: epoch  1, batch     3 | loss: 10.6653423Losses:  4.845240116119385 1.3512232303619385 0.9588215351104736
CurrentTrain: epoch  2, batch     0 | loss: 7.1552849Losses:  5.3675079345703125 1.5189694166183472 0.9832510948181152
CurrentTrain: epoch  2, batch     1 | loss: 7.8697286Losses:  4.722238063812256 1.2948646545410156 0.9607465267181396
CurrentTrain: epoch  2, batch     2 | loss: 6.9778490Losses:  2.8762292861938477 0.33657652139663696 1.0
CurrentTrain: epoch  2, batch     3 | loss: 4.2128057Losses:  4.149186611175537 1.3567228317260742 0.9631860256195068
CurrentTrain: epoch  3, batch     0 | loss: 6.4690952Losses:  5.723245620727539 1.3215875625610352 0.9718728065490723
CurrentTrain: epoch  3, batch     1 | loss: 8.0167065Losses:  3.3740134239196777 1.0024091005325317 0.9565455913543701
CurrentTrain: epoch  3, batch     2 | loss: 5.3329678Losses:  6.2229461669921875 0.8833369016647339 0.9809622168540955
CurrentTrain: epoch  3, batch     3 | loss: 8.0872450Losses:  4.745685577392578 1.3951411247253418 0.9487788677215576
CurrentTrain: epoch  4, batch     0 | loss: 7.0896053Losses:  4.093246936798096 1.1237618923187256 0.9555386900901794
CurrentTrain: epoch  4, batch     1 | loss: 6.1725473Losses:  3.3661439418792725 0.9677523374557495 0.9656000137329102
CurrentTrain: epoch  4, batch     2 | loss: 5.2994962Losses:  2.604914665222168 0.374710351228714 0.9717692136764526
CurrentTrain: epoch  4, batch     3 | loss: 3.9513941Losses:  4.058150291442871 1.0659551620483398 0.9674918055534363
CurrentTrain: epoch  5, batch     0 | loss: 6.0915971Losses:  4.428016185760498 1.3181555271148682 0.956863522529602
CurrentTrain: epoch  5, batch     1 | loss: 6.7030354Losses:  2.9992129802703857 0.7133774161338806 0.9492092728614807
CurrentTrain: epoch  5, batch     2 | loss: 4.6617999Losses:  4.305746078491211 0.3693535327911377 0.9577820301055908
CurrentTrain: epoch  5, batch     3 | loss: 5.6328812Losses:  3.4229724407196045 1.085489273071289 0.9571993350982666
CurrentTrain: epoch  6, batch     0 | loss: 5.4656610Losses:  3.5843210220336914 1.1050348281860352 0.9226862192153931
CurrentTrain: epoch  6, batch     1 | loss: 5.6120420Losses:  3.8721084594726562 0.9519352912902832 0.9696064591407776
CurrentTrain: epoch  6, batch     2 | loss: 5.7936502Losses:  5.667889595031738 0.13270346820354462 0.9246325492858887
CurrentTrain: epoch  6, batch     3 | loss: 6.7252254Losses:  3.6620607376098633 1.0611778497695923 0.9403281211853027
CurrentTrain: epoch  7, batch     0 | loss: 5.6635666Losses:  3.4180290699005127 1.009591817855835 0.960893988609314
CurrentTrain: epoch  7, batch     1 | loss: 5.3885150Losses:  3.396118402481079 1.0569273233413696 0.9496555924415588
CurrentTrain: epoch  7, batch     2 | loss: 5.4027014Losses:  2.2867441177368164 0.0869232714176178 0.9330631494522095
CurrentTrain: epoch  7, batch     3 | loss: 3.3067307Losses:  3.5123462677001953 1.024796724319458 0.935858428478241
CurrentTrain: epoch  8, batch     0 | loss: 5.4730010Losses:  3.243718147277832 0.8607305884361267 0.9456411600112915
CurrentTrain: epoch  8, batch     1 | loss: 5.0500898Losses:  3.303171157836914 1.024989366531372 0.9406464099884033
CurrentTrain: epoch  8, batch     2 | loss: 5.2688065Losses:  2.4293642044067383 8.94069742685133e-08 1.0
CurrentTrain: epoch  8, batch     3 | loss: 3.4293642Losses:  2.843984603881836 0.8016538023948669 0.9588183164596558
CurrentTrain: epoch  9, batch     0 | loss: 4.6044569Losses:  3.524683952331543 0.9813970923423767 0.9341014623641968
CurrentTrain: epoch  9, batch     1 | loss: 5.4401827Losses:  3.0237483978271484 0.9612542390823364 0.9320219159126282
CurrentTrain: epoch  9, batch     2 | loss: 4.9170246Losses:  2.1446762084960938 0.19448155164718628 1.0
CurrentTrain: epoch  9, batch     3 | loss: 3.3391578
Losses:  1.0330910682678223 0.7711948156356812 0.9137383699417114
MemoryTrain:  epoch  0, batch     0 | loss: 2.7180243Losses:  0.3923197090625763 1.1457111835479736 0.8941196203231812
MemoryTrain:  epoch  0, batch     1 | loss: 2.4321504Losses:  0.1910741627216339 0.9025901556015015 0.9065113067626953
MemoryTrain:  epoch  0, batch     2 | loss: 2.0001755Losses:  1.0645630359649658 0.934781014919281 0.913442075252533
MemoryTrain:  epoch  0, batch     3 | loss: 2.9127862Losses:  1.012677788734436 1.085598349571228 0.9194289445877075
MemoryTrain:  epoch  1, batch     0 | loss: 3.0177050Losses:  0.5695322751998901 0.8702899217605591 0.8563977479934692
MemoryTrain:  epoch  1, batch     1 | loss: 2.2962198Losses:  0.7169338464736938 1.0056695938110352 0.8802030682563782
MemoryTrain:  epoch  1, batch     2 | loss: 2.6028066Losses:  1.0296247005462646 0.659266471862793 0.9815070033073425
MemoryTrain:  epoch  1, batch     3 | loss: 2.6703982Losses:  0.31242090463638306 0.9449440836906433 0.925764799118042
MemoryTrain:  epoch  2, batch     0 | loss: 2.1831298Losses:  0.5593960285186768 0.8119494915008545 0.910457968711853
MemoryTrain:  epoch  2, batch     1 | loss: 2.2818036Losses:  0.3222871720790863 0.9342095851898193 0.9132545590400696
MemoryTrain:  epoch  2, batch     2 | loss: 2.1697514Losses:  0.23686353862285614 0.7137936353683472 0.8627728819847107
MemoryTrain:  epoch  2, batch     3 | loss: 1.8134301Losses:  0.29608970880508423 0.8627140522003174 0.9177111983299255
MemoryTrain:  epoch  3, batch     0 | loss: 2.0765150Losses:  0.20064863562583923 0.9790722131729126 0.8693516254425049
MemoryTrain:  epoch  3, batch     1 | loss: 2.0490725Losses:  0.25061869621276855 0.8887930512428284 0.9005900621414185
MemoryTrain:  epoch  3, batch     2 | loss: 2.0400019Losses:  0.2853069305419922 0.5710605382919312 0.9312442541122437
MemoryTrain:  epoch  3, batch     3 | loss: 1.7876117Losses:  0.11059481650590897 0.7345719337463379 0.8870972990989685
MemoryTrain:  epoch  4, batch     0 | loss: 1.7322640Losses:  0.1819021999835968 0.8828909993171692 0.919143795967102
MemoryTrain:  epoch  4, batch     1 | loss: 1.9839370Losses:  0.11482052505016327 0.9638110399246216 0.8741574883460999
MemoryTrain:  epoch  4, batch     2 | loss: 1.9527891Losses:  0.3670089840888977 0.7267227172851562 0.9501504898071289
MemoryTrain:  epoch  4, batch     3 | loss: 2.0438821Losses:  0.241571307182312 0.8698428869247437 0.892848789691925
MemoryTrain:  epoch  5, batch     0 | loss: 2.0042629Losses:  0.09347251057624817 0.778113842010498 0.909718930721283
MemoryTrain:  epoch  5, batch     1 | loss: 1.7813053Losses:  0.17393620312213898 0.8631555438041687 0.9253514409065247
MemoryTrain:  epoch  5, batch     2 | loss: 1.9624431Losses:  0.11395838856697083 0.6972893476486206 0.8732778429985046
MemoryTrain:  epoch  5, batch     3 | loss: 1.6845255Losses:  0.20977161824703217 0.9899088144302368 0.916922926902771
MemoryTrain:  epoch  6, batch     0 | loss: 2.1166034Losses:  0.06786023825407028 0.8833627104759216 0.8757357597351074
MemoryTrain:  epoch  6, batch     1 | loss: 1.8269587Losses:  0.08138750493526459 0.7975403070449829 0.9031175971031189
MemoryTrain:  epoch  6, batch     2 | loss: 1.7820454Losses:  0.07281269878149033 0.6123026013374329 0.9177545309066772
MemoryTrain:  epoch  6, batch     3 | loss: 1.6028697Losses:  0.2116038203239441 0.7789466977119446 0.9100340008735657
MemoryTrain:  epoch  7, batch     0 | loss: 1.9005845Losses:  0.10938222706317902 0.7472947835922241 0.9165079593658447
MemoryTrain:  epoch  7, batch     1 | loss: 1.7731850Losses:  0.1483611911535263 1.0032541751861572 0.8649813532829285
MemoryTrain:  epoch  7, batch     2 | loss: 2.0165968Losses:  0.04635262116789818 0.6377009153366089 0.9108456969261169
MemoryTrain:  epoch  7, batch     3 | loss: 1.5948992Losses:  0.13859836757183075 0.8690639138221741 0.86966872215271
MemoryTrain:  epoch  8, batch     0 | loss: 1.8773310Losses:  0.09981092810630798 0.5979738831520081 0.8932507634162903
MemoryTrain:  epoch  8, batch     1 | loss: 1.5910356Losses:  0.11571981757879257 0.8646618127822876 0.9492151737213135
MemoryTrain:  epoch  8, batch     2 | loss: 1.9295968Losses:  0.11555013060569763 0.7018283605575562 0.8713788986206055
MemoryTrain:  epoch  8, batch     3 | loss: 1.6887574Losses:  0.0676998645067215 0.7501603364944458 0.8629716634750366
MemoryTrain:  epoch  9, batch     0 | loss: 1.6808319Losses:  0.0683731734752655 0.6189737319946289 0.9073312878608704
MemoryTrain:  epoch  9, batch     1 | loss: 1.5946782Losses:  0.07249864190816879 0.85687255859375 0.9134575128555298
MemoryTrain:  epoch  9, batch     2 | loss: 1.8428288Losses:  0.10871180146932602 0.8430692553520203 0.9119381904602051
MemoryTrain:  epoch  9, batch     3 | loss: 1.8637192
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 45.59%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 42.11%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 44.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 49.72%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.90%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.91%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 55.50%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 54.33%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 53.01%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 52.46%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 51.72%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 50.42%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 49.19%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 50.20%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 51.52%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 52.76%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 53.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 55.21%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 56.42%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 57.40%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 58.01%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 58.91%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 59.23%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.23%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 60.87%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.90%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 61.07%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 61.35%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 62.00%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 60.70%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 60.26%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 59.61%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 59.55%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 59.15%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 60.49%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 61.01%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 88.41%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 88.77%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.52%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 87.94%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 87.39%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 86.97%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 86.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 85.52%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 82.88%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 80.41%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 79.23%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 78.44%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 78.52%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 79.14%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 79.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 78.95%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 78.45%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 78.32%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 78.05%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 77.78%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 76.91%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 75.98%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 75.07%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 74.26%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 73.47%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 72.63%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 71.95%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 71.14%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 70.35%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 69.57%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 68.82%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 68.08%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 67.55%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 67.76%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 69.88%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 69.86%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 69.27%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 68.24%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 67.62%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 67.02%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 66.59%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 66.47%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 66.37%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 66.24%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 66.21%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 66.20%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 65.92%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 65.65%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 65.33%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 64.97%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 64.86%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 64.50%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.11%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 66.13%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 66.41%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 66.63%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 66.19%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 65.46%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 65.23%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 64.83%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 64.71%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 66.73%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 66.72%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 66.68%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 66.60%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 66.41%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 66.09%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 65.80%   [EVAL] batch:  210 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 65.24%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 66.93%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 66.91%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 67.03%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 66.99%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 67.00%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 67.01%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 67.07%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 67.49%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 67.32%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 67.16%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  248 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 66.75%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 66.87%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 67.29%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 67.32%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 67.24%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 67.08%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 66.91%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:  275 | acc: 31.25%,  total acc: 67.48%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 67.31%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 67.20%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 67.11%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 66.94%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 66.84%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 66.51%   [EVAL] batch:  285 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 66.20%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 66.48%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 66.48%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 66.46%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 67.73%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 67.56%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 67.42%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 67.27%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 67.18%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 67.10%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.36%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 67.33%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 67.13%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 66.96%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 66.76%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 66.55%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 66.35%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.36%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 66.81%   [EVAL] batch:  338 | acc: 25.00%,  total acc: 66.69%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  340 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:  341 | acc: 18.75%,  total acc: 66.36%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 66.20%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  350 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 66.82%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 66.77%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 66.89%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  363 | acc: 18.75%,  total acc: 66.81%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:  365 | acc: 18.75%,  total acc: 66.63%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 66.62%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 66.65%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 66.75%   
cur_acc:  ['0.9435', '0.6181', '0.7272', '0.6935', '0.7212', '0.6101']
his_acc:  ['0.9435', '0.7850', '0.7480', '0.7175', '0.7005', '0.6675']
Clustering into  34  clusters
Clusters:  [ 1  2 24  1  1  1 30  1 22  0 31  1  1  3  3 19  1  1  1  1  1  1  1 32
  1  0  1 27 17 26 25  1 33  1 20 23 21  1  1  1  2 15  1  1  1  9  1 16
 29 28 14 10  3  1  1 18  1  4  1 12  8 13  1  1  6 11  7  1  1  5]
Losses:  4.392319679260254 1.5431441068649292 0.981965959072113
CurrentTrain: epoch  0, batch     0 | loss: 6.9174299Losses:  4.923162460327148 1.7631564140319824 0.983711838722229
CurrentTrain: epoch  0, batch     1 | loss: 7.6700306Losses:  4.7332916259765625 1.430182695388794 0.9821009039878845
CurrentTrain: epoch  0, batch     2 | loss: 7.1455750Losses:  6.7199788093566895 0.4774315655231476 0.9766313433647156
CurrentTrain: epoch  0, batch     3 | loss: 8.1740417Losses:  4.391284942626953 1.2323343753814697 0.9721618890762329
CurrentTrain: epoch  1, batch     0 | loss: 6.5957808Losses:  4.162116050720215 1.132185459136963 0.9926440119743347
CurrentTrain: epoch  1, batch     1 | loss: 6.2869453Losses:  3.3516154289245605 1.5666029453277588 0.9758787751197815
CurrentTrain: epoch  1, batch     2 | loss: 5.8940973Losses:  2.803511619567871 0.3051706850528717 1.0
CurrentTrain: epoch  1, batch     3 | loss: 4.1086826Losses:  3.4310760498046875 1.276512861251831 0.9625517725944519
CurrentTrain: epoch  2, batch     0 | loss: 5.6701407Losses:  3.357712984085083 1.0395865440368652 0.9773331880569458
CurrentTrain: epoch  2, batch     1 | loss: 5.3746328Losses:  2.9905333518981934 1.4500327110290527 0.9879218935966492
CurrentTrain: epoch  2, batch     2 | loss: 5.4284878Losses:  1.7389609813690186 2.9802322387695312e-08 1.0
CurrentTrain: epoch  2, batch     3 | loss: 2.7389610Losses:  2.9945573806762695 1.301543116569519 0.9807544946670532
CurrentTrain: epoch  3, batch     0 | loss: 5.2768550Losses:  2.6778180599212646 1.0746912956237793 0.9863765835762024
CurrentTrain: epoch  3, batch     1 | loss: 4.7388859Losses:  2.869804620742798 1.313693881034851 0.9556745290756226
CurrentTrain: epoch  3, batch     2 | loss: 5.1391730Losses:  2.709141254425049 0.24285006523132324 0.9352703094482422
CurrentTrain: epoch  3, batch     3 | loss: 3.8872616Losses:  3.24943208694458 0.960482656955719 0.9688246250152588
CurrentTrain: epoch  4, batch     0 | loss: 5.1787395Losses:  2.938868522644043 1.3621115684509277 0.9734483957290649
CurrentTrain: epoch  4, batch     1 | loss: 5.2744284Losses:  2.1701629161834717 0.8250039219856262 0.9665822982788086
CurrentTrain: epoch  4, batch     2 | loss: 3.9617491Losses:  2.094977378845215 0.26223620772361755 0.9431014060974121
CurrentTrain: epoch  4, batch     3 | loss: 3.3003149Losses:  2.887587785720825 0.9093854427337646 0.9515270590782166
CurrentTrain: epoch  5, batch     0 | loss: 4.7485003Losses:  2.3497018814086914 1.0664339065551758 0.9631821513175964
CurrentTrain: epoch  5, batch     1 | loss: 4.3793178Losses:  2.5118539333343506 0.666738748550415 0.9746943712234497
CurrentTrain: epoch  5, batch     2 | loss: 4.1532869Losses:  2.1350207328796387 0.2188567817211151 1.004321575164795
CurrentTrain: epoch  5, batch     3 | loss: 3.3581991Losses:  2.4279391765594482 0.9799945950508118 0.9734911918640137
CurrentTrain: epoch  6, batch     0 | loss: 4.3814249Losses:  2.798172950744629 1.0440082550048828 0.9550676941871643
CurrentTrain: epoch  6, batch     1 | loss: 4.7972488Losses:  2.246478319168091 0.8277283310890198 0.9692410230636597
CurrentTrain: epoch  6, batch     2 | loss: 4.0434475Losses:  2.0022549629211426 0.12790973484516144 0.9012277126312256
CurrentTrain: epoch  6, batch     3 | loss: 3.0313923Losses:  2.4028100967407227 1.009830117225647 0.9339110851287842
CurrentTrain: epoch  7, batch     0 | loss: 4.3465509Losses:  2.187344789505005 0.8811671137809753 0.9544236660003662
CurrentTrain: epoch  7, batch     1 | loss: 4.0229359Losses:  2.5719096660614014 0.6473769545555115 0.9935190081596375
CurrentTrain: epoch  7, batch     2 | loss: 4.2128057Losses:  1.9374617338180542 0.09198823571205139 0.9920117855072021
CurrentTrain: epoch  7, batch     3 | loss: 3.0214617Losses:  2.2861618995666504 0.7695894241333008 0.9611483812332153
CurrentTrain: epoch  8, batch     0 | loss: 4.0168996Losses:  2.429513931274414 0.7208083868026733 0.9854581952095032
CurrentTrain: epoch  8, batch     1 | loss: 4.1357808Losses:  1.9977613687515259 0.8304858803749084 0.9415549039840698
CurrentTrain: epoch  8, batch     2 | loss: 3.7698021Losses:  2.2415771484375 0.11394909024238586 0.8964850306510925
CurrentTrain: epoch  8, batch     3 | loss: 3.2520113Losses:  2.5780062675476074 0.604181170463562 0.9413277506828308
CurrentTrain: epoch  9, batch     0 | loss: 4.1235151Losses:  2.0246663093566895 0.771051824092865 0.9798840284347534
CurrentTrain: epoch  9, batch     1 | loss: 3.7756023Losses:  2.0323641300201416 0.8315544128417969 0.9505877494812012
CurrentTrain: epoch  9, batch     2 | loss: 3.8145063Losses:  1.7358683347702026 0.17299789190292358 0.9448913931846619
CurrentTrain: epoch  9, batch     3 | loss: 2.8537576
Losses:  0.6592726707458496 0.7199746370315552 0.9242545366287231
MemoryTrain:  epoch  0, batch     0 | loss: 2.3035018Losses:  0.15935823321342468 0.9791946411132812 0.9102007150650024
MemoryTrain:  epoch  0, batch     1 | loss: 2.0487537Losses:  0.11584572494029999 0.8694724440574646 0.902965247631073
MemoryTrain:  epoch  0, batch     2 | loss: 1.8882835Losses:  0.640510618686676 0.9767050743103027 0.8796465396881104
MemoryTrain:  epoch  0, batch     3 | loss: 2.4968622Losses:  0.07593251764774323 0.4520035982131958 0.9240965843200684
MemoryTrain:  epoch  0, batch     4 | loss: 1.4520327Losses:  0.8463942408561707 0.8463060855865479 0.8874066472053528
MemoryTrain:  epoch  1, batch     0 | loss: 2.5801070Losses:  0.5251776576042175 0.8104095458984375 0.9029260873794556
MemoryTrain:  epoch  1, batch     1 | loss: 2.2385135Losses:  0.39321401715278625 0.7960655689239502 0.916780948638916
MemoryTrain:  epoch  1, batch     2 | loss: 2.1060605Losses:  0.33922046422958374 0.8351435661315918 0.9258944988250732
MemoryTrain:  epoch  1, batch     3 | loss: 2.1002586Losses:  0.7962110042572021 0.5886994004249573 0.8491840362548828
MemoryTrain:  epoch  1, batch     4 | loss: 2.2340944Losses:  0.38180360198020935 0.7858652472496033 0.9093632102012634
MemoryTrain:  epoch  2, batch     0 | loss: 2.0770321Losses:  0.2545604705810547 0.7019917368888855 0.911797046661377
MemoryTrain:  epoch  2, batch     1 | loss: 1.8683493Losses:  0.2581632435321808 0.9190624952316284 0.8828753232955933
MemoryTrain:  epoch  2, batch     2 | loss: 2.0601010Losses:  0.218397855758667 0.8543038368225098 0.9060149788856506
MemoryTrain:  epoch  2, batch     3 | loss: 1.9787166Losses:  0.0454411655664444 0.3468194603919983 0.8918770551681519
MemoryTrain:  epoch  2, batch     4 | loss: 1.2841377Losses:  0.23089034855365753 0.665587842464447 0.8888107538223267
MemoryTrain:  epoch  3, batch     0 | loss: 1.7852889Losses:  0.08420420438051224 0.6717556715011597 0.9303027391433716
MemoryTrain:  epoch  3, batch     1 | loss: 1.6862626Losses:  0.20055809617042542 0.8587535619735718 0.9084174633026123
MemoryTrain:  epoch  3, batch     2 | loss: 1.9677291Losses:  0.12534120678901672 0.7619886994361877 0.8851127624511719
MemoryTrain:  epoch  3, batch     3 | loss: 1.7724427Losses:  0.19505491852760315 0.4569077491760254 0.8756231069564819
MemoryTrain:  epoch  3, batch     4 | loss: 1.5275857Losses:  0.14614138007164001 0.7117581367492676 0.9513388872146606
MemoryTrain:  epoch  4, batch     0 | loss: 1.8092384Losses:  0.11724533885717392 0.7322491407394409 0.9215570688247681
MemoryTrain:  epoch  4, batch     1 | loss: 1.7710515Losses:  0.10375188291072845 0.9199385643005371 0.8338733911514282
MemoryTrain:  epoch  4, batch     2 | loss: 1.8575639Losses:  0.07054004818201065 0.8380767107009888 0.8735213279724121
MemoryTrain:  epoch  4, batch     3 | loss: 1.7821381Losses:  0.05083167552947998 0.24333696067333221 0.9404062032699585
MemoryTrain:  epoch  4, batch     4 | loss: 1.2345748Losses:  0.07574184983968735 0.8946050405502319 0.8829120397567749
MemoryTrain:  epoch  5, batch     0 | loss: 1.8532588Losses:  0.08553437888622284 0.6456880569458008 0.9259562492370605
MemoryTrain:  epoch  5, batch     1 | loss: 1.6571786Losses:  0.15504731237888336 0.7840179204940796 0.9092540144920349
MemoryTrain:  epoch  5, batch     2 | loss: 1.8483193Losses:  0.08933646976947784 0.6558603048324585 0.8553896546363831
MemoryTrain:  epoch  5, batch     3 | loss: 1.6005864Losses:  0.06371675431728363 0.2850457429885864 0.9269680976867676
MemoryTrain:  epoch  5, batch     4 | loss: 1.2757306Losses:  0.06537704914808273 0.7316927909851074 0.9006656408309937
MemoryTrain:  epoch  6, batch     0 | loss: 1.6977355Losses:  0.10928329080343246 0.7945379614830017 0.8701831102371216
MemoryTrain:  epoch  6, batch     1 | loss: 1.7740043Losses:  0.0791085958480835 0.5845269560813904 0.9096294641494751
MemoryTrain:  epoch  6, batch     2 | loss: 1.5732651Losses:  0.07382269203662872 0.7293475866317749 0.8945211172103882
MemoryTrain:  epoch  6, batch     3 | loss: 1.6976914Losses:  0.08587681502103806 0.2979069948196411 0.9045976400375366
MemoryTrain:  epoch  6, batch     4 | loss: 1.2883815Losses:  0.06879720836877823 0.6871316432952881 0.8734506368637085
MemoryTrain:  epoch  7, batch     0 | loss: 1.6293795Losses:  0.05085096135735512 0.8355630040168762 0.8644791841506958
MemoryTrain:  epoch  7, batch     1 | loss: 1.7508931Losses:  0.069712795317173 0.7008278369903564 0.9079064726829529
MemoryTrain:  epoch  7, batch     2 | loss: 1.6784471Losses:  0.10617409646511078 0.6636979579925537 0.9272422790527344
MemoryTrain:  epoch  7, batch     3 | loss: 1.6971143Losses:  0.11352257430553436 0.31504157185554504 0.8919705152511597
MemoryTrain:  epoch  7, batch     4 | loss: 1.3205347Losses:  0.08155591785907745 0.7061887979507446 0.8530590534210205
MemoryTrain:  epoch  8, batch     0 | loss: 1.6408038Losses:  0.07992122322320938 0.7695415019989014 0.9353649616241455
MemoryTrain:  epoch  8, batch     1 | loss: 1.7848277Losses:  0.03826642036437988 0.6312066316604614 0.886917233467102
MemoryTrain:  epoch  8, batch     2 | loss: 1.5563903Losses:  0.05134333297610283 0.6596176624298096 0.9023824334144592
MemoryTrain:  epoch  8, batch     3 | loss: 1.6133435Losses:  0.05846834182739258 0.33809417486190796 0.859488844871521
MemoryTrain:  epoch  8, batch     4 | loss: 1.2560513Losses:  0.051249224692583084 0.7697830200195312 0.864039421081543
MemoryTrain:  epoch  9, batch     0 | loss: 1.6850717Losses:  0.044089093804359436 0.6445496082305908 0.9075453877449036
MemoryTrain:  epoch  9, batch     1 | loss: 1.5961840Losses:  0.09111219644546509 0.7415378093719482 0.91997230052948
MemoryTrain:  epoch  9, batch     2 | loss: 1.7526224Losses:  0.04925239831209183 0.7152999639511108 0.8769149780273438
MemoryTrain:  epoch  9, batch     3 | loss: 1.6414673Losses:  0.02319354936480522 0.28953176736831665 0.8727830648422241
MemoryTrain:  epoch  9, batch     4 | loss: 1.1855084
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 81.62%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.02%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 80.80%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.69%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 81.14%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.85%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.43%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 83.82%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 83.22%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 82.73%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 82.48%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 82.46%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 81.75%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 80.47%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 79.23%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 78.03%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 76.87%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 75.74%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 74.91%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 75.09%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 75.84%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 75.32%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 75.08%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 74.84%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 74.54%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 73.70%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 73.04%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 72.25%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 71.54%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 70.93%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 70.19%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 69.60%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 68.82%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 68.06%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.31%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 66.58%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 65.86%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 65.36%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 67.76%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 66.69%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 66.19%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 65.60%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 65.01%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 64.80%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 65.22%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 64.77%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 64.45%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 64.20%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 63.94%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 63.78%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 63.48%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 62.88%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 63.66%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 64.60%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 64.93%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 64.83%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 65.14%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 65.08%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 65.02%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 64.89%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 64.66%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 64.44%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 64.43%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 64.33%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 64.20%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 64.01%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 63.85%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 63.59%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 63.40%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 63.22%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 63.11%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 65.29%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 65.31%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 65.41%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.37%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 65.23%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 65.25%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 64.96%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 64.68%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  210 | acc: 0.00%,  total acc: 64.10%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 63.86%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 63.79%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 64.21%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 65.64%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 65.77%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 65.73%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 66.13%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 66.02%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 65.85%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 65.65%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 65.92%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 65.93%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 65.98%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 66.01%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.94%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 65.92%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 65.84%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 65.74%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.66%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.22%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  275 | acc: 31.25%,  total acc: 66.21%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 66.06%   [EVAL] batch:  277 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 65.88%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 65.71%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 65.61%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 65.40%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 65.31%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 64.96%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 65.03%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.11%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.22%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 65.20%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 65.06%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 65.01%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 64.94%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 66.21%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 66.04%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 65.89%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 65.76%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 65.60%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 65.47%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 65.36%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 65.53%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 65.57%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 65.37%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 65.21%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 65.01%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 64.81%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 64.61%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 65.09%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 64.92%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 64.80%   [EVAL] batch:  340 | acc: 31.25%,  total acc: 64.70%   [EVAL] batch:  341 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 64.43%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 64.34%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 64.85%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 64.64%   [EVAL] batch:  354 | acc: 37.50%,  total acc: 64.56%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 64.52%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 64.54%   [EVAL] batch:  359 | acc: 56.25%,  total acc: 64.51%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 64.53%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:  362 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  363 | acc: 12.50%,  total acc: 64.30%   [EVAL] batch:  364 | acc: 25.00%,  total acc: 64.20%   [EVAL] batch:  365 | acc: 18.75%,  total acc: 64.07%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 64.00%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 63.89%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 63.82%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 63.98%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 64.36%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 64.40%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  382 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 64.68%   [EVAL] batch:  385 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 64.71%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  391 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 64.69%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 64.70%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 64.76%   [EVAL] batch:  395 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:  399 | acc: 43.75%,  total acc: 64.84%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 65.39%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 65.39%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 65.39%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 65.37%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 66.31%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 66.68%   
cur_acc:  ['0.9435', '0.6181', '0.7272', '0.6935', '0.7212', '0.6101', '0.8125']
his_acc:  ['0.9435', '0.7850', '0.7480', '0.7175', '0.7005', '0.6675', '0.6668']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 35  0  0  1  1 19  0  0  0  0  0  0  0 25
  0 34  0 23 31 36 26  0 22  0 20 12 17  0  0  0  5 29  0  0  0  2  0 37
 27  9 32 28  1  0  0 18  0 30  0 16  8 10  0  0 13 15 14  0  0  7  2  3
  0  0  6  0  0  0  4 11]
Losses:  6.05393123626709 1.4339680671691895 0.9605602025985718
CurrentTrain: epoch  0, batch     0 | loss: 8.4484596Losses:  6.54103946685791 1.5039644241333008 0.9730509519577026
CurrentTrain: epoch  0, batch     1 | loss: 9.0180550Losses:  6.599666595458984 1.489781141281128 0.9720905423164368
CurrentTrain: epoch  0, batch     2 | loss: 9.0615387Losses:  3.454235076904297 0.33022668957710266 0.9692889451980591
CurrentTrain: epoch  0, batch     3 | loss: 4.7537508Losses:  5.21156120300293 1.303450584411621 0.9716883301734924
CurrentTrain: epoch  1, batch     0 | loss: 7.4867001Losses:  5.255875587463379 1.425333023071289 0.9558531045913696
CurrentTrain: epoch  1, batch     1 | loss: 7.6370616Losses:  4.927548408508301 1.2289083003997803 0.9763633012771606
CurrentTrain: epoch  1, batch     2 | loss: 7.1328201Losses:  3.651526927947998 0.2377241998910904 0.8798271417617798
CurrentTrain: epoch  1, batch     3 | loss: 4.7690783Losses:  4.257513999938965 1.2720330953598022 0.9454506635665894
CurrentTrain: epoch  2, batch     0 | loss: 6.4749980Losses:  4.701502799987793 1.0545451641082764 0.971038818359375
CurrentTrain: epoch  2, batch     1 | loss: 6.7270870Losses:  4.8369364738464355 1.3939080238342285 0.9530340433120728
CurrentTrain: epoch  2, batch     2 | loss: 7.1838784Losses:  3.9470014572143555 0.3843984007835388 1.0
CurrentTrain: epoch  2, batch     3 | loss: 5.3313999Losses:  4.118013858795166 1.2563787698745728 0.9557110667228699
CurrentTrain: epoch  3, batch     0 | loss: 6.3301034Losses:  4.44173526763916 1.1604232788085938 0.9710894823074341
CurrentTrain: epoch  3, batch     1 | loss: 6.5732479Losses:  4.197844982147217 1.2022958993911743 0.9469646215438843
CurrentTrain: epoch  3, batch     2 | loss: 6.3471055Losses:  2.660665988922119 0.5489640831947327 0.9385558366775513
CurrentTrain: epoch  3, batch     3 | loss: 4.1481857Losses:  4.085546016693115 1.2471914291381836 0.9450311660766602
CurrentTrain: epoch  4, batch     0 | loss: 6.2777686Losses:  4.152796745300293 1.2707092761993408 0.9442732334136963
CurrentTrain: epoch  4, batch     1 | loss: 6.3677788Losses:  3.3222928047180176 0.8985173106193542 0.9652973413467407
CurrentTrain: epoch  4, batch     2 | loss: 5.1861072Losses:  3.091564655303955 0.41884100437164307 1.0248684883117676
CurrentTrain: epoch  4, batch     3 | loss: 4.5352740Losses:  3.7288804054260254 0.7228511571884155 0.9486795663833618
CurrentTrain: epoch  5, batch     0 | loss: 5.4004111Losses:  3.8169121742248535 1.0622808933258057 0.9600032567977905
CurrentTrain: epoch  5, batch     1 | loss: 5.8391967Losses:  3.668119430541992 1.084208369255066 0.9512333869934082
CurrentTrain: epoch  5, batch     2 | loss: 5.7035613Losses:  3.688368320465088 0.36276668310165405 0.9589306116104126
CurrentTrain: epoch  5, batch     3 | loss: 5.0100656Losses:  3.0101232528686523 0.9536821842193604 0.9539632797241211
CurrentTrain: epoch  6, batch     0 | loss: 4.9177685Losses:  3.6551599502563477 1.0465658903121948 0.9421577453613281
CurrentTrain: epoch  6, batch     1 | loss: 5.6438837Losses:  2.938727378845215 0.8721216917037964 0.9516488313674927
CurrentTrain: epoch  6, batch     2 | loss: 4.7624979Losses:  6.288778781890869 0.7086700201034546 0.9499127864837646
CurrentTrain: epoch  6, batch     3 | loss: 7.9473619Losses:  2.972154140472412 0.902351975440979 0.9512752294540405
CurrentTrain: epoch  7, batch     0 | loss: 4.8257813Losses:  2.9455509185791016 0.986514151096344 0.9389824271202087
CurrentTrain: epoch  7, batch     1 | loss: 4.8710475Losses:  3.134036064147949 1.0276753902435303 0.9509041905403137
CurrentTrain: epoch  7, batch     2 | loss: 5.1126161Losses:  3.1820449829101562 0.1062062606215477 1.007843255996704
CurrentTrain: epoch  7, batch     3 | loss: 4.2960944Losses:  3.0977301597595215 0.9741426706314087 0.9412384033203125
CurrentTrain: epoch  8, batch     0 | loss: 5.0131111Losses:  2.6798970699310303 0.9501650929450989 0.9510511159896851
CurrentTrain: epoch  8, batch     1 | loss: 4.5811133Losses:  3.309121608734131 0.6460908055305481 0.9557170867919922
CurrentTrain: epoch  8, batch     2 | loss: 4.9109297Losses:  1.7953742742538452 0.061799149960279465 0.9284347295761108
CurrentTrain: epoch  8, batch     3 | loss: 2.7856083Losses:  2.957573890686035 0.8287127017974854 0.9475932121276855
CurrentTrain: epoch  9, batch     0 | loss: 4.7338800Losses:  2.5238420963287354 0.752731204032898 0.9414147138595581
CurrentTrain: epoch  9, batch     1 | loss: 4.2179880Losses:  2.529649257659912 0.8724699020385742 0.9323486685752869
CurrentTrain: epoch  9, batch     2 | loss: 4.3344679Losses:  3.0750837326049805 0.13481451570987701 1.0149167776107788
CurrentTrain: epoch  9, batch     3 | loss: 4.2248149
Losses:  0.36362600326538086 0.7460007071495056 0.8807600736618042
MemoryTrain:  epoch  0, batch     0 | loss: 1.9903868Losses:  1.0304456949234009 0.77723228931427 0.880154013633728
MemoryTrain:  epoch  0, batch     1 | loss: 2.6878319Losses:  0.4055269956588745 0.7963278293609619 0.9032448530197144
MemoryTrain:  epoch  0, batch     2 | loss: 2.1050997Losses:  0.2837084233760834 0.8543352484703064 0.8956253528594971
MemoryTrain:  epoch  0, batch     3 | loss: 2.0336690Losses:  1.026979684829712 0.7730157375335693 0.9463245868682861
MemoryTrain:  epoch  0, batch     4 | loss: 2.7463200Losses:  0.4458840489387512 0.7158609628677368 0.886949896812439
MemoryTrain:  epoch  1, batch     0 | loss: 2.0486951Losses:  0.7129217386245728 0.7066552639007568 0.8545921444892883
MemoryTrain:  epoch  1, batch     1 | loss: 2.2741692Losses:  0.5927242040634155 0.7220785617828369 0.9178099632263184
MemoryTrain:  epoch  1, batch     2 | loss: 2.2326126Losses:  1.0118207931518555 0.6958791613578796 0.9387918710708618
MemoryTrain:  epoch  1, batch     3 | loss: 2.6464920Losses:  0.8272241353988647 0.7899924516677856 0.8868698477745056
MemoryTrain:  epoch  1, batch     4 | loss: 2.5040865Losses:  0.6931905746459961 0.8472214937210083 0.91020268201828
MemoryTrain:  epoch  2, batch     0 | loss: 2.4506147Losses:  0.5197731256484985 0.7123062610626221 0.9480798840522766
MemoryTrain:  epoch  2, batch     1 | loss: 2.1801593Losses:  0.2995280623435974 0.7472811937332153 0.9275267124176025
MemoryTrain:  epoch  2, batch     2 | loss: 1.9743359Losses:  0.5921050906181335 0.810979962348938 0.8844712972640991
MemoryTrain:  epoch  2, batch     3 | loss: 2.2875562Losses:  0.27015435695648193 0.6508233547210693 0.8171085119247437
MemoryTrain:  epoch  2, batch     4 | loss: 1.7380862Losses:  0.16510525345802307 0.5569069385528564 0.8994690179824829
MemoryTrain:  epoch  3, batch     0 | loss: 1.6214812Losses:  0.1977793574333191 0.80607670545578 0.8570914268493652
MemoryTrain:  epoch  3, batch     1 | loss: 1.8609475Losses:  0.35132649540901184 0.7999400496482849 0.9121549725532532
MemoryTrain:  epoch  3, batch     2 | loss: 2.0634215Losses:  0.6088665127754211 0.8181602358818054 0.9172927141189575
MemoryTrain:  epoch  3, batch     3 | loss: 2.3443193Losses:  0.15066896378993988 0.6232522130012512 0.8890795707702637
MemoryTrain:  epoch  3, batch     4 | loss: 1.6630008Losses:  0.3743070960044861 0.8344457745552063 0.9536716938018799
MemoryTrain:  epoch  4, batch     0 | loss: 2.1624246Losses:  0.2536925673484802 0.8195016980171204 0.8779964447021484
MemoryTrain:  epoch  4, batch     1 | loss: 1.9511907Losses:  0.18064752221107483 0.716611385345459 0.891213059425354
MemoryTrain:  epoch  4, batch     2 | loss: 1.7884719Losses:  0.0814085453748703 0.6953035593032837 0.8636972308158875
MemoryTrain:  epoch  4, batch     3 | loss: 1.6404094Losses:  0.11819213628768921 0.6383710503578186 0.8904076814651489
MemoryTrain:  epoch  4, batch     4 | loss: 1.6469709Losses:  0.07299540936946869 0.6817541122436523 0.8905324935913086
MemoryTrain:  epoch  5, batch     0 | loss: 1.6452820Losses:  0.16508539021015167 0.8484799265861511 0.9116404056549072
MemoryTrain:  epoch  5, batch     1 | loss: 1.9252057Losses:  0.18996816873550415 0.7615726590156555 0.8951350450515747
MemoryTrain:  epoch  5, batch     2 | loss: 1.8466759Losses:  0.23758083581924438 0.6282016038894653 0.8873280882835388
MemoryTrain:  epoch  5, batch     3 | loss: 1.7531105Losses:  0.11871425807476044 0.7055919170379639 0.888019859790802
MemoryTrain:  epoch  5, batch     4 | loss: 1.7123260Losses:  0.04841780662536621 0.5713387131690979 0.9001674652099609
MemoryTrain:  epoch  6, batch     0 | loss: 1.5199239Losses:  0.12276285141706467 0.7778077721595764 0.8780267238616943
MemoryTrain:  epoch  6, batch     1 | loss: 1.7785974Losses:  0.06850714981555939 0.7027878761291504 0.8783791661262512
MemoryTrain:  epoch  6, batch     2 | loss: 1.6496742Losses:  0.15536421537399292 0.8107031583786011 0.8998459577560425
MemoryTrain:  epoch  6, batch     3 | loss: 1.8659134Losses:  0.08726540207862854 0.6889052391052246 0.9077082872390747
MemoryTrain:  epoch  6, batch     4 | loss: 1.6838789Losses:  0.19085940718650818 0.873633623123169 0.920866847038269
MemoryTrain:  epoch  7, batch     0 | loss: 1.9853599Losses:  0.06907454133033752 0.710151731967926 0.8714905381202698
MemoryTrain:  epoch  7, batch     1 | loss: 1.6507168Losses:  0.1048499047756195 0.6553757190704346 0.9169472455978394
MemoryTrain:  epoch  7, batch     2 | loss: 1.6771729Losses:  0.09622888267040253 0.652259111404419 0.9134738445281982
MemoryTrain:  epoch  7, batch     3 | loss: 1.6619618Losses:  0.06217843294143677 0.5479849576950073 0.8384619951248169
MemoryTrain:  epoch  7, batch     4 | loss: 1.4486253Losses:  0.11586583405733109 0.6740192174911499 0.9140717387199402
MemoryTrain:  epoch  8, batch     0 | loss: 1.7039568Losses:  0.06067093461751938 0.6248245239257812 0.8717280626296997
MemoryTrain:  epoch  8, batch     1 | loss: 1.5572236Losses:  0.06741410493850708 0.8133141994476318 0.8373819589614868
MemoryTrain:  epoch  8, batch     2 | loss: 1.7181103Losses:  0.12235504388809204 0.7068430185317993 0.9348629117012024
MemoryTrain:  epoch  8, batch     3 | loss: 1.7640610Losses:  0.06533438712358475 0.6393431425094604 0.8999619483947754
MemoryTrain:  epoch  8, batch     4 | loss: 1.6046395Losses:  0.06310132145881653 0.7391047477722168 0.8809289932250977
MemoryTrain:  epoch  9, batch     0 | loss: 1.6831350Losses:  0.1100851371884346 0.73309326171875 0.8956835865974426
MemoryTrain:  epoch  9, batch     1 | loss: 1.7388620Losses:  0.05576835572719574 0.6140545606613159 0.88661789894104
MemoryTrain:  epoch  9, batch     2 | loss: 1.5564408Losses:  0.0467672199010849 0.6001339554786682 0.8987077474594116
MemoryTrain:  epoch  9, batch     3 | loss: 1.5456090Losses:  0.1534736603498459 0.7044898271560669 0.8871555328369141
MemoryTrain:  epoch  9, batch     4 | loss: 1.7451191
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 0.00%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 49.63%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 47.57%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 46.05%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 48.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 51.19%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 58.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 67.99%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 67.26%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 67.84%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 67.16%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 86.01%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 84.72%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 84.09%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 83.71%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 82.00%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 81.36%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 80.94%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 80.53%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.34%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 79.66%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 78.42%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 77.21%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 76.04%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 74.91%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 73.90%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 73.19%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 73.52%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 73.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 73.54%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 73.34%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 73.12%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 72.84%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 72.03%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 71.31%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 70.46%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 69.78%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 69.04%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 68.25%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 67.68%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 66.92%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 65.45%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 64.74%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 64.05%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 64.22%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 66.12%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 65.57%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 65.08%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 64.60%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 63.45%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 63.27%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 63.38%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 63.59%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 63.91%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 63.58%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 63.32%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 63.10%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 62.90%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 62.75%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 62.26%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 62.21%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 62.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 62.59%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 62.73%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 62.91%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 63.36%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 63.31%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 63.30%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 63.43%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 63.63%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 63.62%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 63.65%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 63.62%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 63.73%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 63.92%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 63.87%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 63.70%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 63.54%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 63.49%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 63.36%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 63.07%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 63.11%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 63.06%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 62.99%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 63.06%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 62.94%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 62.76%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 62.57%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 62.14%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 61.96%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 61.82%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.04%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 62.46%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 64.00%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 63.90%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 63.92%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 64.04%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 63.92%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 63.85%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 63.87%   [EVAL] batch:  196 | acc: 31.25%,  total acc: 63.71%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 63.63%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 63.69%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 63.87%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 63.77%   [EVAL] batch:  207 | acc: 0.00%,  total acc: 63.46%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 63.19%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 62.89%   [EVAL] batch:  210 | acc: 0.00%,  total acc: 62.59%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 62.32%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.27%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 64.21%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 64.28%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 64.49%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 64.26%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 64.20%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.17%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 64.11%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 64.18%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 64.52%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 64.33%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 64.14%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 63.91%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 63.76%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 63.65%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 63.86%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 63.95%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 64.14%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 64.08%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 64.17%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 64.19%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 64.08%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 64.12%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 64.03%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 63.93%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 63.88%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 63.81%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 63.85%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 63.96%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  275 | acc: 37.50%,  total acc: 64.49%   [EVAL] batch:  276 | acc: 25.00%,  total acc: 64.35%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 64.21%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 64.14%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 64.00%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 63.90%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 63.81%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 63.74%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 63.53%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 63.36%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 63.20%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 63.00%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 62.96%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 63.27%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 63.14%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 63.05%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 62.94%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 62.90%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 62.79%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 62.69%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 62.81%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 63.60%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 63.94%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 63.75%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 63.61%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 63.49%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 63.33%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 63.19%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 63.07%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 63.12%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 63.20%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.31%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 63.29%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 63.09%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 62.94%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 62.56%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 62.37%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 62.93%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 62.93%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 62.79%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 62.67%   [EVAL] batch:  340 | acc: 50.00%,  total acc: 62.63%   [EVAL] batch:  341 | acc: 31.25%,  total acc: 62.54%   [EVAL] batch:  342 | acc: 18.75%,  total acc: 62.41%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 62.34%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 62.54%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 62.63%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 62.89%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 62.89%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 62.68%   [EVAL] batch:  354 | acc: 37.50%,  total acc: 62.61%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 62.55%   [EVAL] batch:  356 | acc: 25.00%,  total acc: 62.45%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 62.38%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 62.22%   [EVAL] batch:  359 | acc: 25.00%,  total acc: 62.12%   [EVAL] batch:  360 | acc: 18.75%,  total acc: 62.00%   [EVAL] batch:  361 | acc: 6.25%,  total acc: 61.84%   [EVAL] batch:  362 | acc: 12.50%,  total acc: 61.71%   [EVAL] batch:  363 | acc: 6.25%,  total acc: 61.56%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 61.44%   [EVAL] batch:  365 | acc: 12.50%,  total acc: 61.30%   [EVAL] batch:  366 | acc: 31.25%,  total acc: 61.22%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 61.12%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 61.08%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 61.17%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 61.32%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 61.41%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 61.60%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 61.75%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 61.82%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 61.87%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 61.96%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 62.04%   [EVAL] batch:  382 | acc: 50.00%,  total acc: 62.01%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 62.01%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 62.06%   [EVAL] batch:  385 | acc: 62.50%,  total acc: 62.06%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 62.05%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 62.07%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 62.05%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 62.04%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 62.00%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 61.99%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 61.94%   [EVAL] batch:  393 | acc: 62.50%,  total acc: 61.94%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 62.01%   [EVAL] batch:  395 | acc: 87.50%,  total acc: 62.07%   [EVAL] batch:  396 | acc: 75.00%,  total acc: 62.11%   [EVAL] batch:  397 | acc: 75.00%,  total acc: 62.14%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 62.14%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 62.08%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 62.45%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 62.55%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 62.67%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 62.70%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 62.68%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 62.71%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 62.70%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 62.68%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 62.70%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 63.13%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  425 | acc: 43.75%,  total acc: 63.67%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  428 | acc: 56.25%,  total acc: 63.68%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 63.68%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 63.70%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 63.81%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 63.99%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 64.04%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 64.03%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 63.99%   [EVAL] batch:  439 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:  440 | acc: 37.50%,  total acc: 63.90%   [EVAL] batch:  441 | acc: 43.75%,  total acc: 63.86%   [EVAL] batch:  442 | acc: 37.50%,  total acc: 63.80%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 63.78%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 64.05%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  450 | acc: 0.00%,  total acc: 63.96%   [EVAL] batch:  451 | acc: 6.25%,  total acc: 63.83%   [EVAL] batch:  452 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:  453 | acc: 6.25%,  total acc: 63.57%   [EVAL] batch:  454 | acc: 18.75%,  total acc: 63.48%   [EVAL] batch:  455 | acc: 0.00%,  total acc: 63.34%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 63.35%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 63.57%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 63.65%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  462 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:  463 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:  464 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 63.99%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  468 | acc: 87.50%,  total acc: 64.19%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 64.33%   [EVAL] batch:  471 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  475 | acc: 37.50%,  total acc: 64.54%   [EVAL] batch:  476 | acc: 25.00%,  total acc: 64.45%   [EVAL] batch:  477 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 64.38%   [EVAL] batch:  479 | acc: 18.75%,  total acc: 64.28%   [EVAL] batch:  480 | acc: 43.75%,  total acc: 64.24%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  485 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  488 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  490 | acc: 56.25%,  total acc: 64.63%   [EVAL] batch:  491 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 64.60%   [EVAL] batch:  493 | acc: 43.75%,  total acc: 64.56%   [EVAL] batch:  494 | acc: 68.75%,  total acc: 64.57%   [EVAL] batch:  495 | acc: 56.25%,  total acc: 64.55%   [EVAL] batch:  496 | acc: 43.75%,  total acc: 64.51%   [EVAL] batch:  497 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  499 | acc: 56.25%,  total acc: 64.51%   
cur_acc:  ['0.9435', '0.6181', '0.7272', '0.6935', '0.7212', '0.6101', '0.8125', '0.6716']
his_acc:  ['0.9435', '0.7850', '0.7480', '0.7175', '0.7005', '0.6675', '0.6668', '0.6451']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.023988723754883 2.1490416526794434 1.0008201599121094
CurrentTrain: epoch  0, batch     0 | loss: 13.1738510Losses:  9.52214241027832 2.0424447059631348 1.0002667903900146
CurrentTrain: epoch  0, batch     1 | loss: 12.5648537Losses:  9.476869583129883 1.872043490409851 0.9994113445281982
CurrentTrain: epoch  0, batch     2 | loss: 12.3483248Losses:  10.609551429748535 2.057371139526367 0.9928210973739624
CurrentTrain: epoch  0, batch     3 | loss: 13.6597433Losses:  9.378421783447266 1.8407210111618042 0.9905444979667664
CurrentTrain: epoch  0, batch     4 | loss: 12.2096872Losses:  9.187952041625977 1.495490312576294 1.0047663450241089
CurrentTrain: epoch  0, batch     5 | loss: 11.6882086Losses:  9.360288619995117 1.8338937759399414 0.9870826005935669
CurrentTrain: epoch  0, batch     6 | loss: 12.1812649Losses:  9.084911346435547 1.7944939136505127 0.9790508151054382
CurrentTrain: epoch  0, batch     7 | loss: 11.8584557Losses:  8.502310752868652 1.3644628524780273 0.9743878841400146
CurrentTrain: epoch  0, batch     8 | loss: 10.8411617Losses:  9.50963020324707 1.704278588294983 0.9888629913330078
CurrentTrain: epoch  0, batch     9 | loss: 12.2027721Losses:  9.013189315795898 1.404990553855896 0.9872682094573975
CurrentTrain: epoch  0, batch    10 | loss: 11.4054480Losses:  8.685025215148926 1.3883336782455444 0.984491765499115
CurrentTrain: epoch  0, batch    11 | loss: 11.0578499Losses:  8.448282241821289 1.4252676963806152 0.9856268167495728
CurrentTrain: epoch  0, batch    12 | loss: 10.8591776Losses:  8.86562728881836 1.368559718132019 0.9945635199546814
CurrentTrain: epoch  0, batch    13 | loss: 11.2287502Losses:  9.53487777709961 1.019430160522461 0.9967548251152039
CurrentTrain: epoch  0, batch    14 | loss: 11.5510626Losses:  8.088560104370117 1.4383796453475952 0.9718538522720337
CurrentTrain: epoch  0, batch    15 | loss: 10.4987936Losses:  8.321178436279297 1.5477672815322876 0.9593368768692017
CurrentTrain: epoch  0, batch    16 | loss: 10.8282833Losses:  8.83940315246582 1.6213639974594116 0.9707540273666382
CurrentTrain: epoch  0, batch    17 | loss: 11.4315205Losses:  8.515156745910645 1.5757108926773071 0.9571882486343384
CurrentTrain: epoch  0, batch    18 | loss: 11.0480566Losses:  8.955551147460938 1.558973789215088 0.9574267864227295
CurrentTrain: epoch  0, batch    19 | loss: 11.4719515Losses:  8.529094696044922 1.4103412628173828 0.968916654586792
CurrentTrain: epoch  0, batch    20 | loss: 10.9083529Losses:  8.603445053100586 1.353114128112793 0.9705899953842163
CurrentTrain: epoch  0, batch    21 | loss: 10.9271488Losses:  9.221095085144043 1.5797795057296753 0.9592223763465881
CurrentTrain: epoch  0, batch    22 | loss: 11.7600975Losses:  8.663211822509766 1.3392553329467773 0.9727615118026733
CurrentTrain: epoch  0, batch    23 | loss: 10.9752283Losses:  8.454421997070312 1.6119248867034912 0.9650987386703491
CurrentTrain: epoch  0, batch    24 | loss: 11.0314455Losses:  9.190491676330566 1.3322620391845703 0.9764158129692078
CurrentTrain: epoch  0, batch    25 | loss: 11.4991693Losses:  8.288309097290039 1.5475175380706787 0.9620156288146973
CurrentTrain: epoch  0, batch    26 | loss: 10.7978420Losses:  8.75330924987793 1.288102149963379 0.9766709208488464
CurrentTrain: epoch  0, batch    27 | loss: 11.0180826Losses:  8.435741424560547 1.490322470664978 0.9747253060340881
CurrentTrain: epoch  0, batch    28 | loss: 10.9007893Losses:  7.772529602050781 1.342677354812622 0.9557006359100342
CurrentTrain: epoch  0, batch    29 | loss: 10.0709076Losses:  8.744437217712402 1.190542459487915 0.9708729386329651
CurrentTrain: epoch  0, batch    30 | loss: 10.9058523Losses:  7.927447319030762 1.196933388710022 0.9554869532585144
CurrentTrain: epoch  0, batch    31 | loss: 10.0798683Losses:  7.302023887634277 0.9741254448890686 0.9265304207801819
CurrentTrain: epoch  0, batch    32 | loss: 9.2026806Losses:  7.965993404388428 1.0554463863372803 0.929334282875061
CurrentTrain: epoch  0, batch    33 | loss: 9.9507742Losses:  8.047270774841309 1.2320551872253418 0.958810567855835
CurrentTrain: epoch  0, batch    34 | loss: 10.2381363Losses:  7.789268493652344 1.067786455154419 0.948948860168457
CurrentTrain: epoch  0, batch    35 | loss: 9.8060036Losses:  7.49617338180542 1.1408755779266357 0.9401767253875732
CurrentTrain: epoch  0, batch    36 | loss: 9.5772257Losses:  8.063403129577637 1.2393195629119873 0.9450675845146179
CurrentTrain: epoch  0, batch    37 | loss: 10.2477903Losses:  7.4129180908203125 1.0980331897735596 0.9385284781455994
CurrentTrain: epoch  0, batch    38 | loss: 9.4494791Losses:  8.148418426513672 1.1391208171844482 0.9460613131523132
CurrentTrain: epoch  0, batch    39 | loss: 10.2336006Losses:  8.11946964263916 0.735588788986206 0.969233512878418
CurrentTrain: epoch  0, batch    40 | loss: 9.8242922Losses:  8.01159381866455 1.1272046566009521 0.9516129493713379
CurrentTrain: epoch  0, batch    41 | loss: 10.0904121Losses:  8.530975341796875 1.2554512023925781 0.9547296762466431
CurrentTrain: epoch  0, batch    42 | loss: 10.7411566Losses:  7.861965656280518 1.3421270847320557 0.9610005617141724
CurrentTrain: epoch  0, batch    43 | loss: 10.1650934Losses:  8.270898818969727 1.089722752571106 0.9508980512619019
CurrentTrain: epoch  0, batch    44 | loss: 10.3115196Losses:  7.752845764160156 1.1925902366638184 0.9504024982452393
CurrentTrain: epoch  0, batch    45 | loss: 9.8958387Losses:  7.110424995422363 0.9120712876319885 0.9259235262870789
CurrentTrain: epoch  0, batch    46 | loss: 8.9484196Losses:  7.910976409912109 1.0173077583312988 0.9499106407165527
CurrentTrain: epoch  0, batch    47 | loss: 9.8781948Losses:  8.085127830505371 1.0855473279953003 0.9514317512512207
CurrentTrain: epoch  0, batch    48 | loss: 10.1221066Losses:  7.580698013305664 1.2072150707244873 0.9312971830368042
CurrentTrain: epoch  0, batch    49 | loss: 9.7192106Losses:  7.403606414794922 1.1462361812591553 0.9448249936103821
CurrentTrain: epoch  0, batch    50 | loss: 9.4946680Losses:  7.8525190353393555 1.075724482536316 0.9402487874031067
CurrentTrain: epoch  0, batch    51 | loss: 9.8684921Losses:  7.620616912841797 1.1156338453292847 0.9511491060256958
CurrentTrain: epoch  0, batch    52 | loss: 9.6873999Losses:  6.860869884490967 0.8495974540710449 0.915313720703125
CurrentTrain: epoch  0, batch    53 | loss: 8.6257811Losses:  7.192533016204834 1.166426658630371 0.9298768639564514
CurrentTrain: epoch  0, batch    54 | loss: 9.2888365Losses:  7.490152359008789 0.9104726910591125 0.957614541053772
CurrentTrain: epoch  0, batch    55 | loss: 9.3582401Losses:  7.259286880493164 1.2506399154663086 0.9370301365852356
CurrentTrain: epoch  0, batch    56 | loss: 9.4469566Losses:  6.674478530883789 1.2526419162750244 0.9208766222000122
CurrentTrain: epoch  0, batch    57 | loss: 8.8479967Losses:  7.5906476974487305 1.1478832960128784 0.940883457660675
CurrentTrain: epoch  0, batch    58 | loss: 9.6794147Losses:  7.164485454559326 1.1288633346557617 0.9405849575996399
CurrentTrain: epoch  0, batch    59 | loss: 9.2339334Losses:  6.567675590515137 0.941024899482727 0.917508602142334
CurrentTrain: epoch  0, batch    60 | loss: 8.4262085Losses:  7.582497596740723 0.9464066028594971 0.9240099191665649
CurrentTrain: epoch  0, batch    61 | loss: 9.4529142Losses:  6.219944477081299 0.6792572736740112 0.8953293561935425
CurrentTrain: epoch  0, batch    62 | loss: 7.7945313Losses:  8.032756805419922 0.8770648241043091 0.962824285030365
CurrentTrain: epoch  1, batch     0 | loss: 9.8726454Losses:  7.118831157684326 0.8258211612701416 0.914328932762146
CurrentTrain: epoch  1, batch     1 | loss: 8.8589811Losses:  6.656926155090332 0.880486786365509 0.90912926197052
CurrentTrain: epoch  1, batch     2 | loss: 8.4465427Losses:  7.330670356750488 0.9691447615623474 0.9191340208053589
CurrentTrain: epoch  1, batch     3 | loss: 9.2189493Losses:  6.270663261413574 0.9000790119171143 0.9209567308425903
CurrentTrain: epoch  1, batch     4 | loss: 8.0916986Losses:  6.873079299926758 0.9747102856636047 0.9439283609390259
CurrentTrain: epoch  1, batch     5 | loss: 8.7917185Losses:  7.140660285949707 0.8493322134017944 0.9087969064712524
CurrentTrain: epoch  1, batch     6 | loss: 8.8987894Losses:  6.781890869140625 0.8121317625045776 0.9039461612701416
CurrentTrain: epoch  1, batch     7 | loss: 8.4979687Losses:  6.6990556716918945 0.9169817566871643 0.9404580593109131
CurrentTrain: epoch  1, batch     8 | loss: 8.5564957Losses:  6.635886192321777 0.9861046075820923 0.9198077917098999
CurrentTrain: epoch  1, batch     9 | loss: 8.5417986Losses:  6.282415390014648 0.7883692979812622 0.9102771282196045
CurrentTrain: epoch  1, batch    10 | loss: 7.9810619Losses:  7.18011474609375 0.9994780421257019 0.9263384342193604
CurrentTrain: epoch  1, batch    11 | loss: 9.1059313Losses:  7.110835075378418 0.8261174559593201 0.9247168898582458
CurrentTrain: epoch  1, batch    12 | loss: 8.8616695Losses:  6.739880561828613 0.841092050075531 0.9124490022659302
CurrentTrain: epoch  1, batch    13 | loss: 8.4934216Losses:  6.549572944641113 0.8320775032043457 0.916597843170166
CurrentTrain: epoch  1, batch    14 | loss: 8.2982483Losses:  6.1505045890808105 0.7153819799423218 0.9202011823654175
CurrentTrain: epoch  1, batch    15 | loss: 7.7860880Losses:  7.957680702209473 0.8304395079612732 0.9492284059524536
CurrentTrain: epoch  1, batch    16 | loss: 9.7373486Losses:  6.3184943199157715 0.7866885662078857 0.9087740182876587
CurrentTrain: epoch  1, batch    17 | loss: 8.0139570Losses:  7.139102935791016 0.7222452163696289 0.9670432806015015
CurrentTrain: epoch  1, batch    18 | loss: 8.8283911Losses:  7.998113632202148 0.7585690021514893 0.9223431348800659
CurrentTrain: epoch  1, batch    19 | loss: 9.6790257Losses:  6.710442543029785 0.5087101459503174 0.9179767966270447
CurrentTrain: epoch  1, batch    20 | loss: 8.1371288Losses:  5.606828689575195 0.6265976428985596 0.9029951095581055
CurrentTrain: epoch  1, batch    21 | loss: 7.1364212Losses:  6.200720310211182 0.6869642734527588 0.908698558807373
CurrentTrain: epoch  1, batch    22 | loss: 7.7963834Losses:  5.789309024810791 0.7140483260154724 0.8939005732536316
CurrentTrain: epoch  1, batch    23 | loss: 7.3972578Losses:  6.1024651527404785 0.8492291569709778 0.8798959255218506
CurrentTrain: epoch  1, batch    24 | loss: 7.8315907Losses:  6.34663200378418 0.7879166603088379 0.8879528045654297
CurrentTrain: epoch  1, batch    25 | loss: 8.0225010Losses:  7.630697250366211 0.8256111741065979 0.9169986248016357
CurrentTrain: epoch  1, batch    26 | loss: 9.3733072Losses:  6.443467140197754 0.8387280702590942 0.8892243504524231
CurrentTrain: epoch  1, batch    27 | loss: 8.1714191Losses:  6.292872905731201 0.744834303855896 0.8845279216766357
CurrentTrain: epoch  1, batch    28 | loss: 7.9222355Losses:  7.386444568634033 1.0709302425384521 0.9402106404304504
CurrentTrain: epoch  1, batch    29 | loss: 9.3975849Losses:  5.908879280090332 0.556982696056366 0.912360668182373
CurrentTrain: epoch  1, batch    30 | loss: 7.3782225Losses:  6.938157081604004 0.6733303666114807 0.9116891622543335
CurrentTrain: epoch  1, batch    31 | loss: 8.5231762Losses:  6.432284355163574 0.8234437704086304 0.9007194638252258
CurrentTrain: epoch  1, batch    32 | loss: 8.1564474Losses:  7.364646911621094 0.7632706761360168 0.9257157444953918
CurrentTrain: epoch  1, batch    33 | loss: 9.0536327Losses:  6.499590873718262 0.7344213724136353 0.9363707900047302
CurrentTrain: epoch  1, batch    34 | loss: 8.1703825Losses:  5.8679962158203125 0.3298543691635132 0.8870589733123779
CurrentTrain: epoch  1, batch    35 | loss: 7.0849094Losses:  5.912487030029297 0.7446296215057373 0.8732904195785522
CurrentTrain: epoch  1, batch    36 | loss: 7.5304074Losses:  6.954232215881348 0.9237446188926697 0.9355258941650391
CurrentTrain: epoch  1, batch    37 | loss: 8.8135033Losses:  6.213037490844727 0.7445968389511108 0.8814513683319092
CurrentTrain: epoch  1, batch    38 | loss: 7.8390856Losses:  6.8404860496521 0.8412725925445557 0.9241513013839722
CurrentTrain: epoch  1, batch    39 | loss: 8.6059103Losses:  5.926409721374512 0.723052442073822 0.9033083319664001
CurrentTrain: epoch  1, batch    40 | loss: 7.5527706Losses:  7.435965538024902 0.7106610536575317 0.9451295137405396
CurrentTrain: epoch  1, batch    41 | loss: 9.0917559Losses:  6.687081813812256 0.7667995095252991 0.90770024061203
CurrentTrain: epoch  1, batch    42 | loss: 8.3615818Losses:  6.038442611694336 0.6881121397018433 0.8939634561538696
CurrentTrain: epoch  1, batch    43 | loss: 7.6205182Losses:  5.968818664550781 0.5248830318450928 0.9078792333602905
CurrentTrain: epoch  1, batch    44 | loss: 7.4015813Losses:  7.513092041015625 0.8085728883743286 0.9120858907699585
CurrentTrain: epoch  1, batch    45 | loss: 9.2337503Losses:  5.859795570373535 0.8080372214317322 0.8815500736236572
CurrentTrain: epoch  1, batch    46 | loss: 7.5493832Losses:  6.850033760070801 0.6283382773399353 0.8828015327453613
CurrentTrain: epoch  1, batch    47 | loss: 8.3611736Losses:  6.510773658752441 0.6933389902114868 0.8920331001281738
CurrentTrain: epoch  1, batch    48 | loss: 8.0961456Losses:  5.663617134094238 0.4313080310821533 0.9664624333381653
CurrentTrain: epoch  1, batch    49 | loss: 7.0613875Losses:  5.496233940124512 0.5673980116844177 0.8993775844573975
CurrentTrain: epoch  1, batch    50 | loss: 6.9630098Losses:  6.533574104309082 0.8665097951889038 0.9117242097854614
CurrentTrain: epoch  1, batch    51 | loss: 8.3118086Losses:  5.846151828765869 0.6349805593490601 0.8688547015190125
CurrentTrain: epoch  1, batch    52 | loss: 7.3499870Losses:  5.311882019042969 0.5406562685966492 0.8800175189971924
CurrentTrain: epoch  1, batch    53 | loss: 6.7325554Losses:  5.5201263427734375 0.7001532316207886 0.892604649066925
CurrentTrain: epoch  1, batch    54 | loss: 7.1128845Losses:  6.425050735473633 0.7148876190185547 0.9036828279495239
CurrentTrain: epoch  1, batch    55 | loss: 8.0436211Losses:  5.571932792663574 0.551384449005127 0.9144191145896912
CurrentTrain: epoch  1, batch    56 | loss: 7.0377364Losses:  5.875487327575684 0.5966618061065674 0.8924592733383179
CurrentTrain: epoch  1, batch    57 | loss: 7.3646083Losses:  5.893403053283691 0.6488111615180969 0.9010100364685059
CurrentTrain: epoch  1, batch    58 | loss: 7.4432244Losses:  5.574573993682861 0.6696367263793945 0.8845982551574707
CurrentTrain: epoch  1, batch    59 | loss: 7.1288090Losses:  6.453635215759277 0.6731823682785034 0.9147486686706543
CurrentTrain: epoch  1, batch    60 | loss: 8.0415668Losses:  5.66425085067749 0.6830143928527832 0.912101149559021
CurrentTrain: epoch  1, batch    61 | loss: 7.2593665Losses:  5.168331146240234 0.42839765548706055 0.9226580262184143
CurrentTrain: epoch  1, batch    62 | loss: 6.5193868Losses:  5.879834175109863 0.5912846326828003 0.9109193086624146
CurrentTrain: epoch  2, batch     0 | loss: 7.3820381Losses:  5.77013635635376 0.5956649780273438 0.9247539639472961
CurrentTrain: epoch  2, batch     1 | loss: 7.2905555Losses:  7.354179382324219 0.6379857659339905 0.9181656837463379
CurrentTrain: epoch  2, batch     2 | loss: 8.9103308Losses:  5.3220109939575195 0.43440303206443787 0.9349299669265747
CurrentTrain: epoch  2, batch     3 | loss: 6.6913438Losses:  5.263003349304199 0.3704674243927002 0.8529690504074097
CurrentTrain: epoch  2, batch     4 | loss: 6.4864397Losses:  5.585808753967285 0.605622410774231 0.871173083782196
CurrentTrain: epoch  2, batch     5 | loss: 7.0626040Losses:  5.20527458190918 0.4956173300743103 0.8864070773124695
CurrentTrain: epoch  2, batch     6 | loss: 6.5872989Losses:  5.909708499908447 0.6023921966552734 0.9034387469291687
CurrentTrain: epoch  2, batch     7 | loss: 7.4155393Losses:  5.650218963623047 0.6834156513214111 0.8821383714675903
CurrentTrain: epoch  2, batch     8 | loss: 7.2157726Losses:  5.653886318206787 0.4887273907661438 0.8748301267623901
CurrentTrain: epoch  2, batch     9 | loss: 7.0174441Losses:  5.2555460929870605 0.471425324678421 0.868203341960907
CurrentTrain: epoch  2, batch    10 | loss: 6.5951748Losses:  5.947824478149414 0.6451187133789062 0.9076947569847107
CurrentTrain: epoch  2, batch    11 | loss: 7.5006380Losses:  5.212705612182617 0.4315680265426636 0.8811682462692261
CurrentTrain: epoch  2, batch    12 | loss: 6.5254421Losses:  4.909450531005859 0.49511751532554626 0.8496034741401672
CurrentTrain: epoch  2, batch    13 | loss: 6.2541718Losses:  4.865996837615967 0.38578081130981445 0.8630568385124207
CurrentTrain: epoch  2, batch    14 | loss: 6.1148343Losses:  5.631382942199707 0.6041628122329712 0.84581458568573
CurrentTrain: epoch  2, batch    15 | loss: 7.0813603Losses:  5.733346939086914 0.4111909866333008 0.8858852386474609
CurrentTrain: epoch  2, batch    16 | loss: 7.0304232Losses:  6.210524559020996 0.6256971955299377 0.9017950296401978
CurrentTrain: epoch  2, batch    17 | loss: 7.7380166Losses:  5.662451267242432 0.4727437496185303 0.8645389080047607
CurrentTrain: epoch  2, batch    18 | loss: 6.9997339Losses:  4.873650074005127 0.425919771194458 0.8751477003097534
CurrentTrain: epoch  2, batch    19 | loss: 6.1747179Losses:  5.023148536682129 0.4235812723636627 0.8351983428001404
CurrentTrain: epoch  2, batch    20 | loss: 6.2819281Losses:  6.082179069519043 0.5452248454093933 0.9160370826721191
CurrentTrain: epoch  2, batch    21 | loss: 7.5434408Losses:  5.615622043609619 0.4369020462036133 0.8770732879638672
CurrentTrain: epoch  2, batch    22 | loss: 6.9295974Losses:  6.185065269470215 0.5982579588890076 0.9085332751274109
CurrentTrain: epoch  2, batch    23 | loss: 7.6918564Losses:  5.800153732299805 0.5420922040939331 0.8937356472015381
CurrentTrain: epoch  2, batch    24 | loss: 7.2359819Losses:  5.670016288757324 0.6332600116729736 0.8609773516654968
CurrentTrain: epoch  2, batch    25 | loss: 7.1642532Losses:  4.851256370544434 0.4910067915916443 0.8782657980918884
CurrentTrain: epoch  2, batch    26 | loss: 6.2205291Losses:  5.525606155395508 0.4019924998283386 0.856837272644043
CurrentTrain: epoch  2, batch    27 | loss: 6.7844357Losses:  5.691309928894043 0.43140143156051636 0.8708838820457458
CurrentTrain: epoch  2, batch    28 | loss: 6.9935951Losses:  6.0112738609313965 0.5033459663391113 0.8938714265823364
CurrentTrain: epoch  2, batch    29 | loss: 7.4084911Losses:  4.865984916687012 0.39384162425994873 0.8419326543807983
CurrentTrain: epoch  2, batch    30 | loss: 6.1017594Losses:  5.032161235809326 0.4066491723060608 0.8552709817886353
CurrentTrain: epoch  2, batch    31 | loss: 6.2940812Losses:  4.750629425048828 0.3556380867958069 0.8676674365997314
CurrentTrain: epoch  2, batch    32 | loss: 5.9739351Losses:  5.848055839538574 0.3825852870941162 0.8728877902030945
CurrentTrain: epoch  2, batch    33 | loss: 7.1035290Losses:  5.035567283630371 0.5203676223754883 0.8560299873352051
CurrentTrain: epoch  2, batch    34 | loss: 6.4119649Losses:  5.2029194831848145 0.4688030481338501 0.8661984205245972
CurrentTrain: epoch  2, batch    35 | loss: 6.5379210Losses:  5.308109283447266 0.40009605884552 0.8820080757141113
CurrentTrain: epoch  2, batch    36 | loss: 6.5902133Losses:  5.3234052658081055 0.4853062629699707 0.8486872911453247
CurrentTrain: epoch  2, batch    37 | loss: 6.6573987Losses:  5.1076555252075195 0.3659605383872986 0.839103102684021
CurrentTrain: epoch  2, batch    38 | loss: 6.3127193Losses:  5.424173355102539 0.419281929731369 0.8528295755386353
CurrentTrain: epoch  2, batch    39 | loss: 6.6962848Losses:  5.329922676086426 0.3562292456626892 0.8600464463233948
CurrentTrain: epoch  2, batch    40 | loss: 6.5461984Losses:  4.742704391479492 0.4012022912502289 0.8433020114898682
CurrentTrain: epoch  2, batch    41 | loss: 5.9872084Losses:  5.096731662750244 0.3962611258029938 0.9227029085159302
CurrentTrain: epoch  2, batch    42 | loss: 6.4156957Losses:  5.1647443771362305 0.3567690849304199 0.823428213596344
CurrentTrain: epoch  2, batch    43 | loss: 6.3449416Losses:  6.00314998626709 0.5772957801818848 0.85426926612854
CurrentTrain: epoch  2, batch    44 | loss: 7.4347153Losses:  5.305285930633545 0.4761231541633606 0.8664201498031616
CurrentTrain: epoch  2, batch    45 | loss: 6.6478295Losses:  5.034975051879883 0.3789091110229492 0.8810832500457764
CurrentTrain: epoch  2, batch    46 | loss: 6.2949677Losses:  5.51230001449585 0.44356292486190796 0.8648452758789062
CurrentTrain: epoch  2, batch    47 | loss: 6.8207083Losses:  5.315169334411621 0.4488815367221832 0.8749147653579712
CurrentTrain: epoch  2, batch    48 | loss: 6.6389656Losses:  5.266275405883789 0.344276487827301 0.8792550563812256
CurrentTrain: epoch  2, batch    49 | loss: 6.4898071Losses:  5.786351680755615 0.5200064182281494 0.8667707443237305
CurrentTrain: epoch  2, batch    50 | loss: 7.1731291Losses:  4.78468132019043 0.3502041697502136 0.8688381910324097
CurrentTrain: epoch  2, batch    51 | loss: 6.0037236Losses:  5.2442474365234375 0.2631526291370392 0.82149338722229
CurrentTrain: epoch  2, batch    52 | loss: 6.3288937Losses:  4.7250895500183105 0.4543144702911377 0.8545178174972534
CurrentTrain: epoch  2, batch    53 | loss: 6.0339222Losses:  5.262409210205078 0.5271016359329224 0.8351894021034241
CurrentTrain: epoch  2, batch    54 | loss: 6.6247001Losses:  4.872147560119629 0.39129331707954407 0.8839459419250488
CurrentTrain: epoch  2, batch    55 | loss: 6.1473870Losses:  5.575827598571777 0.4460362195968628 0.8534361124038696
CurrentTrain: epoch  2, batch    56 | loss: 6.8752999Losses:  5.454215049743652 0.5144656896591187 0.8796296715736389
CurrentTrain: epoch  2, batch    57 | loss: 6.8483105Losses:  5.434884071350098 0.4125635623931885 0.8624293804168701
CurrentTrain: epoch  2, batch    58 | loss: 6.7098770Losses:  4.923591136932373 0.4214898943901062 0.8584478497505188
CurrentTrain: epoch  2, batch    59 | loss: 6.2035289Losses:  5.161266803741455 0.42569318413734436 0.8206297159194946
CurrentTrain: epoch  2, batch    60 | loss: 6.4075894Losses:  4.946098804473877 0.35849571228027344 0.8672811985015869
CurrentTrain: epoch  2, batch    61 | loss: 6.1718760Losses:  4.563118934631348 0.20572766661643982 0.853279709815979
CurrentTrain: epoch  2, batch    62 | loss: 5.6221261Losses:  4.8082990646362305 0.4157683253288269 0.8359930515289307
CurrentTrain: epoch  3, batch     0 | loss: 6.0600605Losses:  4.569054126739502 0.35071831941604614 0.8431820869445801
CurrentTrain: epoch  3, batch     1 | loss: 5.7629547Losses:  5.449639320373535 0.43987321853637695 0.8823060989379883
CurrentTrain: epoch  3, batch     2 | loss: 6.7718186Losses:  4.903735160827637 0.4686020016670227 0.8344395160675049
CurrentTrain: epoch  3, batch     3 | loss: 6.2067766Losses:  4.744668006896973 0.40166598558425903 0.847804844379425
CurrentTrain: epoch  3, batch     4 | loss: 5.9941392Losses:  4.942687034606934 0.2504153847694397 0.904057502746582
CurrentTrain: epoch  3, batch     5 | loss: 6.0971599Losses:  4.705814361572266 0.2572481632232666 0.8463566303253174
CurrentTrain: epoch  3, batch     6 | loss: 5.8094187Losses:  5.07379674911499 0.36247527599334717 0.8966145515441895
CurrentTrain: epoch  3, batch     7 | loss: 6.3328867Losses:  4.7173285484313965 0.38355886936187744 0.8657355308532715
CurrentTrain: epoch  3, batch     8 | loss: 5.9666228Losses:  4.950325965881348 0.3612198829650879 0.8906775712966919
CurrentTrain: epoch  3, batch     9 | loss: 6.2022233Losses:  4.6591477394104 0.31269359588623047 0.8114489912986755
CurrentTrain: epoch  3, batch    10 | loss: 5.7832904Losses:  4.633191108703613 0.3483639061450958 0.8121747970581055
CurrentTrain: epoch  3, batch    11 | loss: 5.7937298Losses:  5.0362701416015625 0.3719216585159302 0.8413326740264893
CurrentTrain: epoch  3, batch    12 | loss: 6.2495241Losses:  4.8817572593688965 0.411312997341156 0.8561372756958008
CurrentTrain: epoch  3, batch    13 | loss: 6.1492076Losses:  5.275396823883057 0.3648368716239929 0.8560424447059631
CurrentTrain: epoch  3, batch    14 | loss: 6.4962759Losses:  4.933103561401367 0.3071882426738739 0.922692060470581
CurrentTrain: epoch  3, batch    15 | loss: 6.1629839Losses:  5.390827178955078 0.3270340859889984 0.8484857082366943
CurrentTrain: epoch  3, batch    16 | loss: 6.5663471Losses:  5.1314897537231445 0.37115058302879333 0.8832762241363525
CurrentTrain: epoch  3, batch    17 | loss: 6.3859167Losses:  4.71064567565918 0.32898595929145813 0.8552085161209106
CurrentTrain: epoch  3, batch    18 | loss: 5.8948402Losses:  4.953105926513672 0.3811571002006531 0.8471927642822266
CurrentTrain: epoch  3, batch    19 | loss: 6.1814556Losses:  4.78229284286499 0.3683483600616455 0.870524525642395
CurrentTrain: epoch  3, batch    20 | loss: 6.0211658Losses:  4.8819732666015625 0.35102662444114685 0.8566893339157104
CurrentTrain: epoch  3, batch    21 | loss: 6.0896893Losses:  4.877410411834717 0.31986066699028015 0.8429000377655029
CurrentTrain: epoch  3, batch    22 | loss: 6.0401707Losses:  4.902665138244629 0.231858491897583 0.788857102394104
CurrentTrain: epoch  3, batch    23 | loss: 5.9233804Losses:  4.659456729888916 0.2157800793647766 0.8284440040588379
CurrentTrain: epoch  3, batch    24 | loss: 5.7036810Losses:  4.917117118835449 0.2921777069568634 0.8163905739784241
CurrentTrain: epoch  3, batch    25 | loss: 6.0256853Losses:  5.635223388671875 0.2712685465812683 0.8570656776428223
CurrentTrain: epoch  3, batch    26 | loss: 6.7635574Losses:  5.240823268890381 0.2595440745353699 0.8778623938560486
CurrentTrain: epoch  3, batch    27 | loss: 6.3782296Losses:  4.622642517089844 0.41056209802627563 0.8229250907897949
CurrentTrain: epoch  3, batch    28 | loss: 5.8561296Losses:  4.536600112915039 0.24162623286247253 0.760614275932312
CurrentTrain: epoch  3, batch    29 | loss: 5.5388408Losses:  4.746111869812012 0.2134672999382019 0.8643683195114136
CurrentTrain: epoch  3, batch    30 | loss: 5.8239474Losses:  4.583866119384766 0.2867884337902069 0.7850974798202515
CurrentTrain: epoch  3, batch    31 | loss: 5.6557522Losses:  4.801204681396484 0.32341957092285156 0.8604224920272827
CurrentTrain: epoch  3, batch    32 | loss: 5.9850469Losses:  4.745797634124756 0.3678559362888336 0.7841809988021851
CurrentTrain: epoch  3, batch    33 | loss: 5.8978348Losses:  4.464291095733643 0.34146228432655334 0.8627524375915527
CurrentTrain: epoch  3, batch    34 | loss: 5.6685057Losses:  4.931027889251709 0.3830142021179199 0.8607035875320435
CurrentTrain: epoch  3, batch    35 | loss: 6.1747456Losses:  4.88130521774292 0.38240954279899597 0.8773621916770935
CurrentTrain: epoch  3, batch    36 | loss: 6.1410770Losses:  4.391787528991699 0.31209585070610046 0.8180067539215088
CurrentTrain: epoch  3, batch    37 | loss: 5.5218897Losses:  5.713644981384277 0.47111570835113525 0.8821942806243896
CurrentTrain: epoch  3, batch    38 | loss: 7.0669546Losses:  5.619368553161621 0.3096967041492462 0.8405084609985352
CurrentTrain: epoch  3, batch    39 | loss: 6.7695737Losses:  4.9831390380859375 0.26999732851982117 0.8546218872070312
CurrentTrain: epoch  3, batch    40 | loss: 6.1077580Losses:  4.4844465255737305 0.1382085233926773 0.793942928314209
CurrentTrain: epoch  3, batch    41 | loss: 5.4165978Losses:  6.531494140625 0.31924283504486084 0.8787600994110107
CurrentTrain: epoch  3, batch    42 | loss: 7.7294970Losses:  4.707923412322998 0.29177820682525635 0.796090841293335
CurrentTrain: epoch  3, batch    43 | loss: 5.7957926Losses:  4.977612495422363 0.3192932605743408 0.8585402965545654
CurrentTrain: epoch  3, batch    44 | loss: 6.1554461Losses:  5.471564292907715 0.27824273705482483 0.79395592212677
CurrentTrain: epoch  3, batch    45 | loss: 6.5437627Losses:  4.761074066162109 0.25365617871284485 0.8794627785682678
CurrentTrain: epoch  3, batch    46 | loss: 5.8941932Losses:  4.539664268493652 0.34534937143325806 0.8376590013504028
CurrentTrain: epoch  3, batch    47 | loss: 5.7226725Losses:  4.555226802825928 0.2512558400630951 0.8456497192382812
CurrentTrain: epoch  3, batch    48 | loss: 5.6521325Losses:  4.501284599304199 0.27641934156417847 0.8225990533828735
CurrentTrain: epoch  3, batch    49 | loss: 5.6003027Losses:  4.684552192687988 0.2878705561161041 0.8297939300537109
CurrentTrain: epoch  3, batch    50 | loss: 5.8022165Losses:  4.554636478424072 0.27356553077697754 0.8977714776992798
CurrentTrain: epoch  3, batch    51 | loss: 5.7259736Losses:  4.664909362792969 0.257445752620697 0.8970282077789307
CurrentTrain: epoch  3, batch    52 | loss: 5.8193836Losses:  4.679961204528809 0.2678268551826477 0.8165619373321533
CurrentTrain: epoch  3, batch    53 | loss: 5.7643499Losses:  4.753045082092285 0.3043057918548584 0.7622938752174377
CurrentTrain: epoch  3, batch    54 | loss: 5.8196449Losses:  5.156924247741699 0.37319082021713257 0.8503386974334717
CurrentTrain: epoch  3, batch    55 | loss: 6.3804541Losses:  4.552949905395508 0.2813488245010376 0.816728413105011
CurrentTrain: epoch  3, batch    56 | loss: 5.6510272Losses:  4.631902694702148 0.27862289547920227 0.8293396234512329
CurrentTrain: epoch  3, batch    57 | loss: 5.7398653Losses:  5.38132381439209 0.4174707233905792 0.8353614807128906
CurrentTrain: epoch  3, batch    58 | loss: 6.6341562Losses:  4.499350070953369 0.28091907501220703 0.8386032581329346
CurrentTrain: epoch  3, batch    59 | loss: 5.6188726Losses:  4.932526588439941 0.29640334844589233 0.8192505836486816
CurrentTrain: epoch  3, batch    60 | loss: 6.0481806Losses:  4.654636383056641 0.33372992277145386 0.8029341697692871
CurrentTrain: epoch  3, batch    61 | loss: 5.7913003Losses:  4.768963813781738 0.10699320584535599 0.7630470991134644
CurrentTrain: epoch  3, batch    62 | loss: 5.6390042Losses:  4.5142598152160645 0.20908042788505554 0.7858412265777588
CurrentTrain: epoch  4, batch     0 | loss: 5.5091810Losses:  4.440179824829102 0.2942838668823242 0.7894397974014282
CurrentTrain: epoch  4, batch     1 | loss: 5.5239034Losses:  4.929167747497559 0.35398268699645996 0.8324772715568542
CurrentTrain: epoch  4, batch     2 | loss: 6.1156278Losses:  4.787320137023926 0.27324187755584717 0.7681683301925659
CurrentTrain: epoch  4, batch     3 | loss: 5.8287306Losses:  4.58502197265625 0.22750850021839142 0.882962703704834
CurrentTrain: epoch  4, batch     4 | loss: 5.6954932Losses:  4.633270740509033 0.27772068977355957 0.7976024150848389
CurrentTrain: epoch  4, batch     5 | loss: 5.7085943Losses:  4.353269100189209 0.16842538118362427 0.7832223773002625
CurrentTrain: epoch  4, batch     6 | loss: 5.3049169Losses:  4.9360032081604 0.257536917924881 0.8183791637420654
CurrentTrain: epoch  4, batch     7 | loss: 6.0119190Losses:  4.298887729644775 0.21639999747276306 0.8343438506126404
CurrentTrain: epoch  4, batch     8 | loss: 5.3496318Losses:  4.36422061920166 0.29066938161849976 0.7989729642868042
CurrentTrain: epoch  4, batch     9 | loss: 5.4538631Losses:  4.5405426025390625 0.29295071959495544 0.8555207252502441
CurrentTrain: epoch  4, batch    10 | loss: 5.6890140Losses:  4.4272332191467285 0.25843968987464905 0.8577181696891785
CurrentTrain: epoch  4, batch    11 | loss: 5.5433908Losses:  4.596434593200684 0.21896710991859436 0.8648866415023804
CurrentTrain: epoch  4, batch    12 | loss: 5.6802883Losses:  4.277721881866455 0.19227272272109985 0.794258713722229
CurrentTrain: epoch  4, batch    13 | loss: 5.2642531Losses:  4.715424537658691 0.2999672591686249 0.8297343850135803
CurrentTrain: epoch  4, batch    14 | loss: 5.8451262Losses:  4.448965072631836 0.22171275317668915 0.8316577672958374
CurrentTrain: epoch  4, batch    15 | loss: 5.5023355Losses:  4.56533145904541 0.18878400325775146 0.8099722266197205
CurrentTrain: epoch  4, batch    16 | loss: 5.5640879Losses:  4.6922502517700195 0.3962334394454956 0.867974579334259
CurrentTrain: epoch  4, batch    17 | loss: 5.9564586Losses:  4.970836639404297 0.21247005462646484 0.8582136631011963
CurrentTrain: epoch  4, batch    18 | loss: 6.0415201Losses:  4.414703369140625 0.28116416931152344 0.8377073407173157
CurrentTrain: epoch  4, batch    19 | loss: 5.5335751Losses:  4.634156227111816 0.32014331221580505 0.8133580684661865
CurrentTrain: epoch  4, batch    20 | loss: 5.7676573Losses:  4.407459735870361 0.31363481283187866 0.8388605117797852
CurrentTrain: epoch  4, batch    21 | loss: 5.5599551Losses:  4.487182140350342 0.1591733694076538 0.8345799446105957
CurrentTrain: epoch  4, batch    22 | loss: 5.4809356Losses:  4.457297325134277 0.282118022441864 0.8523733615875244
CurrentTrain: epoch  4, batch    23 | loss: 5.5917883Losses:  4.419312000274658 0.23060068488121033 0.8711345791816711
CurrentTrain: epoch  4, batch    24 | loss: 5.5210476Losses:  4.453636646270752 0.2768998146057129 0.7660398483276367
CurrentTrain: epoch  4, batch    25 | loss: 5.4965763Losses:  4.403589248657227 0.301291286945343 0.8563109636306763
CurrentTrain: epoch  4, batch    26 | loss: 5.5611916Losses:  4.455211639404297 0.22670811414718628 0.7814030647277832
CurrentTrain: epoch  4, batch    27 | loss: 5.4633226Losses:  4.346295356750488 0.2877312898635864 0.8071500062942505
CurrentTrain: epoch  4, batch    28 | loss: 5.4411764Losses:  4.830322265625 0.25681203603744507 0.8265411853790283
CurrentTrain: epoch  4, batch    29 | loss: 5.9136753Losses:  4.675762176513672 0.22665371000766754 0.8210830092430115
CurrentTrain: epoch  4, batch    30 | loss: 5.7234988Losses:  4.444797992706299 0.17225460708141327 0.8978893756866455
CurrentTrain: epoch  4, batch    31 | loss: 5.5149422Losses:  4.263857841491699 0.30165350437164307 0.8140963912010193
CurrentTrain: epoch  4, batch    32 | loss: 5.3796077Losses:  4.419356346130371 0.20543518662452698 0.865788996219635
CurrentTrain: epoch  4, batch    33 | loss: 5.4905806Losses:  4.353236198425293 0.26116108894348145 0.823844313621521
CurrentTrain: epoch  4, batch    34 | loss: 5.4382415Losses:  4.651760101318359 0.32073941826820374 0.856000542640686
CurrentTrain: epoch  4, batch    35 | loss: 5.8284998Losses:  4.518143653869629 0.23137173056602478 0.8588052988052368
CurrentTrain: epoch  4, batch    36 | loss: 5.6083207Losses:  4.480076789855957 0.26776111125946045 0.8232424259185791
CurrentTrain: epoch  4, batch    37 | loss: 5.5710802Losses:  4.4190568923950195 0.2205970138311386 0.7902851104736328
CurrentTrain: epoch  4, batch    38 | loss: 5.4299388Losses:  5.114774703979492 0.41255897283554077 0.821952760219574
CurrentTrain: epoch  4, batch    39 | loss: 6.3492866Losses:  4.451223373413086 0.28440147638320923 0.7748273611068726
CurrentTrain: epoch  4, batch    40 | loss: 5.5104523Losses:  4.600080966949463 0.28366538882255554 0.7683383226394653
CurrentTrain: epoch  4, batch    41 | loss: 5.6520844Losses:  4.55711555480957 0.1627979278564453 0.898247241973877
CurrentTrain: epoch  4, batch    42 | loss: 5.6181607Losses:  4.378602504730225 0.26956629753112793 0.8107690215110779
CurrentTrain: epoch  4, batch    43 | loss: 5.4589376Losses:  4.380792617797852 0.18864263594150543 0.8515397310256958
CurrentTrain: epoch  4, batch    44 | loss: 5.4209747Losses:  4.287074089050293 0.1923261284828186 0.7753528356552124
CurrentTrain: epoch  4, batch    45 | loss: 5.2547531Losses:  4.4341535568237305 0.24969983100891113 0.8029770255088806
CurrentTrain: epoch  4, batch    46 | loss: 5.4868302Losses:  4.50091552734375 0.27384477853775024 0.807917594909668
CurrentTrain: epoch  4, batch    47 | loss: 5.5826778Losses:  4.255851745605469 0.19544193148612976 0.7712643146514893
CurrentTrain: epoch  4, batch    48 | loss: 5.2225580Losses:  4.2404704093933105 0.2460935115814209 0.8279774785041809
CurrentTrain: epoch  4, batch    49 | loss: 5.3145413Losses:  4.255152225494385 0.20220059156417847 0.8319637775421143
CurrentTrain: epoch  4, batch    50 | loss: 5.2893162Losses:  4.190035820007324 0.12573972344398499 0.7837556600570679
CurrentTrain: epoch  4, batch    51 | loss: 5.0995312Losses:  4.444879055023193 0.23089341819286346 0.848029613494873
CurrentTrain: epoch  4, batch    52 | loss: 5.5238023Losses:  4.263383865356445 0.1411530077457428 0.7911974191665649
CurrentTrain: epoch  4, batch    53 | loss: 5.1957340Losses:  4.400149822235107 0.24908336997032166 0.7452765703201294
CurrentTrain: epoch  4, batch    54 | loss: 5.3945098Losses:  4.266232013702393 0.22450831532478333 0.7818379402160645
CurrentTrain: epoch  4, batch    55 | loss: 5.2725782Losses:  4.195229530334473 0.09290605783462524 0.8647983074188232
CurrentTrain: epoch  4, batch    56 | loss: 5.1529341Losses:  4.340597152709961 0.21862724423408508 0.832878828048706
CurrentTrain: epoch  4, batch    57 | loss: 5.3921032Losses:  4.264054298400879 0.24183742702007294 0.7705948352813721
CurrentTrain: epoch  4, batch    58 | loss: 5.2764864Losses:  4.3291544914245605 0.2529904246330261 0.7939379811286926
CurrentTrain: epoch  4, batch    59 | loss: 5.3760829Losses:  4.310533046722412 0.14857588708400726 0.7841832637786865
CurrentTrain: epoch  4, batch    60 | loss: 5.2432919Losses:  4.3499836921691895 0.1316525936126709 0.829939603805542
CurrentTrain: epoch  4, batch    61 | loss: 5.3115759Losses:  4.248620986938477 0.09357202053070068 0.8104702234268188
CurrentTrain: epoch  4, batch    62 | loss: 5.1526632Losses:  4.30554723739624 0.22158931195735931 0.8412767648696899
CurrentTrain: epoch  5, batch     0 | loss: 5.3684130Losses:  4.1319990158081055 0.10599325597286224 0.8133907318115234
CurrentTrain: epoch  5, batch     1 | loss: 5.0513830Losses:  4.388808250427246 0.21028250455856323 0.7894573211669922
CurrentTrain: epoch  5, batch     2 | loss: 5.3885479Losses:  4.391584396362305 0.23695258796215057 0.8273107409477234
CurrentTrain: epoch  5, batch     3 | loss: 5.4558477Losses:  4.274274826049805 0.14966779947280884 0.8327181339263916
CurrentTrain: epoch  5, batch     4 | loss: 5.2566605Losses:  4.360476016998291 0.1468656361103058 0.8894652128219604
CurrentTrain: epoch  5, batch     5 | loss: 5.3968072Losses:  4.223734378814697 0.25007331371307373 0.7767102122306824
CurrentTrain: epoch  5, batch     6 | loss: 5.2505178Losses:  4.256875038146973 0.2471524477005005 0.7980663776397705
CurrentTrain: epoch  5, batch     7 | loss: 5.3020935Losses:  4.26524019241333 0.21201731264591217 0.7853100299835205
CurrentTrain: epoch  5, batch     8 | loss: 5.2625675Losses:  4.275876998901367 0.18405503034591675 0.8075153231620789
CurrentTrain: epoch  5, batch     9 | loss: 5.2674470Losses:  4.266719818115234 0.1333075910806656 0.842686116695404
CurrentTrain: epoch  5, batch    10 | loss: 5.2427135Losses:  4.283842086791992 0.23169222474098206 0.7980332970619202
CurrentTrain: epoch  5, batch    11 | loss: 5.3135676Losses:  4.17491340637207 0.16515564918518066 0.8377367258071899
CurrentTrain: epoch  5, batch    12 | loss: 5.1778054Losses:  4.333756923675537 0.16054357588291168 0.7178804874420166
CurrentTrain: epoch  5, batch    13 | loss: 5.2121811Losses:  4.386860370635986 0.12482461333274841 0.8137807846069336
CurrentTrain: epoch  5, batch    14 | loss: 5.3254657Losses:  4.252788066864014 0.19276165962219238 0.814468264579773
CurrentTrain: epoch  5, batch    15 | loss: 5.2600183Losses:  4.1732635498046875 0.1733247935771942 0.7774358987808228
CurrentTrain: epoch  5, batch    16 | loss: 5.1240239Losses:  4.288276672363281 0.1913529932498932 0.8001531362533569
CurrentTrain: epoch  5, batch    17 | loss: 5.2797828Losses:  4.243505477905273 0.11155523359775543 0.8016669154167175
CurrentTrain: epoch  5, batch    18 | loss: 5.1567273Losses:  4.265009880065918 0.2139006406068802 0.7877452373504639
CurrentTrain: epoch  5, batch    19 | loss: 5.2666559Losses:  4.215989112854004 0.196822851896286 0.8335176706314087
CurrentTrain: epoch  5, batch    20 | loss: 5.2463293Losses:  4.280160903930664 0.24449710547924042 0.750354528427124
CurrentTrain: epoch  5, batch    21 | loss: 5.2750130Losses:  4.1644673347473145 0.1355215311050415 0.7343651652336121
CurrentTrain: epoch  5, batch    22 | loss: 5.0343537Losses:  4.247946739196777 0.18837970495224 0.8358925580978394
CurrentTrain: epoch  5, batch    23 | loss: 5.2722192Losses:  4.242855072021484 0.23414088785648346 0.7889373302459717
CurrentTrain: epoch  5, batch    24 | loss: 5.2659330Losses:  4.212921619415283 0.14715233445167542 0.7247548699378967
CurrentTrain: epoch  5, batch    25 | loss: 5.0848289Losses:  4.182342529296875 0.19100144505500793 0.8166277408599854
CurrentTrain: epoch  5, batch    26 | loss: 5.1899719Losses:  4.148770809173584 0.12633630633354187 0.7714449167251587
CurrentTrain: epoch  5, batch    27 | loss: 5.0465517Losses:  4.164455413818359 0.19044888019561768 0.8070461750030518
CurrentTrain: epoch  5, batch    28 | loss: 5.1619501Losses:  4.1526665687561035 0.20412452518939972 0.8222237229347229
CurrentTrain: epoch  5, batch    29 | loss: 5.1790147Losses:  4.193347454071045 0.1758149415254593 0.7487435936927795
CurrentTrain: epoch  5, batch    30 | loss: 5.1179061Losses:  4.171372413635254 0.13402226567268372 0.7627862095832825
CurrentTrain: epoch  5, batch    31 | loss: 5.0681810Losses:  4.140096187591553 0.1602810025215149 0.823326051235199
CurrentTrain: epoch  5, batch    32 | loss: 5.1237035Losses:  4.1951003074646 0.16844919323921204 0.7656317949295044
CurrentTrain: epoch  5, batch    33 | loss: 5.1291814Losses:  4.304856777191162 0.21845221519470215 0.8223973512649536
CurrentTrain: epoch  5, batch    34 | loss: 5.3457060Losses:  4.336456298828125 0.23087811470031738 0.7754232883453369
CurrentTrain: epoch  5, batch    35 | loss: 5.3427572Losses:  4.757451057434082 0.34002208709716797 0.8769432306289673
CurrentTrain: epoch  5, batch    36 | loss: 5.9744163Losses:  4.342426300048828 0.20361140370368958 0.8410404920578003
CurrentTrain: epoch  5, batch    37 | loss: 5.3870783Losses:  4.149826526641846 0.1709914356470108 0.7323095798492432
CurrentTrain: epoch  5, batch    38 | loss: 5.0531273Losses:  4.236603736877441 0.19927823543548584 0.7845426797866821
CurrentTrain: epoch  5, batch    39 | loss: 5.2204247Losses:  4.150608062744141 0.234485924243927 0.7440755367279053
CurrentTrain: epoch  5, batch    40 | loss: 5.1291695Losses:  4.105072021484375 0.20451077818870544 0.7832781076431274
CurrentTrain: epoch  5, batch    41 | loss: 5.0928607Losses:  4.235604286193848 0.21215537190437317 0.7798619270324707
CurrentTrain: epoch  5, batch    42 | loss: 5.2276216Losses:  4.222883701324463 0.22288981080055237 0.8362239003181458
CurrentTrain: epoch  5, batch    43 | loss: 5.2819977Losses:  4.23027229309082 0.17333997786045074 0.8257073760032654
CurrentTrain: epoch  5, batch    44 | loss: 5.2293196Losses:  4.1469268798828125 0.17541655898094177 0.6854373216629028
CurrentTrain: epoch  5, batch    45 | loss: 5.0077806Losses:  4.130075454711914 0.13974890112876892 0.8245854377746582
CurrentTrain: epoch  5, batch    46 | loss: 5.0944099Losses:  4.111185073852539 0.2064603567123413 0.7703180313110352
CurrentTrain: epoch  5, batch    47 | loss: 5.0879636Losses:  4.711461544036865 0.24261295795440674 0.7419417500495911
CurrentTrain: epoch  5, batch    48 | loss: 5.6960163Losses:  4.15212345123291 0.20005661249160767 0.8083562850952148
CurrentTrain: epoch  5, batch    49 | loss: 5.1605363Losses:  4.147424697875977 0.2256852239370346 0.7883034944534302
CurrentTrain: epoch  5, batch    50 | loss: 5.1614132Losses:  4.104156017303467 0.19050291180610657 0.7955789566040039
CurrentTrain: epoch  5, batch    51 | loss: 5.0902381Losses:  4.213618755340576 0.15514284372329712 0.7923271656036377
CurrentTrain: epoch  5, batch    52 | loss: 5.1610889Losses:  4.13932991027832 0.23022052645683289 0.8340470790863037
CurrentTrain: epoch  5, batch    53 | loss: 5.2035971Losses:  4.12010383605957 0.15407812595367432 0.8406099677085876
CurrentTrain: epoch  5, batch    54 | loss: 5.1147919Losses:  4.081942081451416 0.13915276527404785 0.7789387702941895
CurrentTrain: epoch  5, batch    55 | loss: 5.0000339Losses:  4.1285858154296875 0.1763148307800293 0.8187166452407837
CurrentTrain: epoch  5, batch    56 | loss: 5.1236172Losses:  4.249029159545898 0.15739327669143677 0.7534928321838379
CurrentTrain: epoch  5, batch    57 | loss: 5.1599154Losses:  4.226676940917969 0.13848936557769775 0.8722370266914368
CurrentTrain: epoch  5, batch    58 | loss: 5.2374034Losses:  4.096654891967773 0.14717796444892883 0.8013305068016052
CurrentTrain: epoch  5, batch    59 | loss: 5.0451636Losses:  4.104367733001709 0.1818992793560028 0.8194221258163452
CurrentTrain: epoch  5, batch    60 | loss: 5.1056890Losses:  4.173369407653809 0.18493320047855377 0.8345377445220947
CurrentTrain: epoch  5, batch    61 | loss: 5.1928406Losses:  4.102829933166504 0.12368884682655334 0.7678427696228027
CurrentTrain: epoch  5, batch    62 | loss: 4.9943614Losses:  4.179793357849121 0.1920895129442215 0.8117053508758545
CurrentTrain: epoch  6, batch     0 | loss: 5.1835880Losses:  4.118589878082275 0.17982631921768188 0.8033598065376282
CurrentTrain: epoch  6, batch     1 | loss: 5.1017761Losses:  4.104474067687988 0.15578684210777283 0.8453491926193237
CurrentTrain: epoch  6, batch     2 | loss: 5.1056104Losses:  4.113768577575684 0.17273637652397156 0.7760284543037415
CurrentTrain: epoch  6, batch     3 | loss: 5.0625334Losses:  4.13673210144043 0.16013264656066895 0.7816698551177979
CurrentTrain: epoch  6, batch     4 | loss: 5.0785341Losses:  4.1178507804870605 0.15360048413276672 0.863375186920166
CurrentTrain: epoch  6, batch     5 | loss: 5.1348267Losses:  4.1680006980896 0.12955373525619507 0.7155113816261292
CurrentTrain: epoch  6, batch     6 | loss: 5.0130658Losses:  4.128150463104248 0.1860429048538208 0.7294521331787109
CurrentTrain: epoch  6, batch     7 | loss: 5.0436454Losses:  4.2277984619140625 0.21819715201854706 0.767612636089325
CurrentTrain: epoch  6, batch     8 | loss: 5.2136083Losses:  4.237705230712891 0.13997986912727356 0.8526164889335632
CurrentTrain: epoch  6, batch     9 | loss: 5.2303014Losses:  4.079545974731445 0.12208293378353119 0.8029899001121521
CurrentTrain: epoch  6, batch    10 | loss: 5.0046186Losses:  4.245051383972168 0.15869754552841187 0.836022675037384
CurrentTrain: epoch  6, batch    11 | loss: 5.2397718Losses:  4.141237258911133 0.15955156087875366 0.8036463260650635
CurrentTrain: epoch  6, batch    12 | loss: 5.1044350Losses:  4.138507843017578 0.13490986824035645 0.8218910098075867
CurrentTrain: epoch  6, batch    13 | loss: 5.0953083Losses:  4.103953838348389 0.11188572645187378 0.780887246131897
CurrentTrain: epoch  6, batch    14 | loss: 4.9967265Losses:  4.09067440032959 0.18188738822937012 0.7708871364593506
CurrentTrain: epoch  6, batch    15 | loss: 5.0434494Losses:  4.124565601348877 0.14073309302330017 0.7542920112609863
CurrentTrain: epoch  6, batch    16 | loss: 5.0195909Losses:  4.149272441864014 0.1599564105272293 0.8200417757034302
CurrentTrain: epoch  6, batch    17 | loss: 5.1292706Losses:  4.154604434967041 0.15189799666404724 0.8080747127532959
CurrentTrain: epoch  6, batch    18 | loss: 5.1145773Losses:  4.123117446899414 0.16094550490379333 0.7954186797142029
CurrentTrain: epoch  6, batch    19 | loss: 5.0794816Losses:  4.134391784667969 0.21149882674217224 0.7377012968063354
CurrentTrain: epoch  6, batch    20 | loss: 5.0835919Losses:  4.045924186706543 0.18507100641727448 0.7830880880355835
CurrentTrain: epoch  6, batch    21 | loss: 5.0140834Losses:  4.085493087768555 0.2007622867822647 0.7813012599945068
CurrentTrain: epoch  6, batch    22 | loss: 5.0675564Losses:  4.093472480773926 0.09131421148777008 0.7669884562492371
CurrentTrain: epoch  6, batch    23 | loss: 4.9517751Losses:  4.035825729370117 0.17844432592391968 0.7685608863830566
CurrentTrain: epoch  6, batch    24 | loss: 4.9828310Losses:  4.120364665985107 0.18515095114707947 0.7801263332366943
CurrentTrain: epoch  6, batch    25 | loss: 5.0856419Losses:  4.143754959106445 0.1537407785654068 0.7743204236030579
CurrentTrain: epoch  6, batch    26 | loss: 5.0718164Losses:  4.243284702301025 0.16437092423439026 0.7312741875648499
CurrentTrain: epoch  6, batch    27 | loss: 5.1389298Losses:  4.332973480224609 0.13649609684944153 0.7325225472450256
CurrentTrain: epoch  6, batch    28 | loss: 5.2019920Losses:  4.102403163909912 0.11753863096237183 0.8200255632400513
CurrentTrain: epoch  6, batch    29 | loss: 5.0399671Losses:  4.193351745605469 0.13600268959999084 0.8067681789398193
CurrentTrain: epoch  6, batch    30 | loss: 5.1361227Losses:  4.126189231872559 0.16415944695472717 0.8088639974594116
CurrentTrain: epoch  6, batch    31 | loss: 5.0992126Losses:  4.146367073059082 0.14587625861167908 0.7614108324050903
CurrentTrain: epoch  6, batch    32 | loss: 5.0536542Losses:  4.178582191467285 0.15578067302703857 0.7633794546127319
CurrentTrain: epoch  6, batch    33 | loss: 5.0977426Losses:  4.182186126708984 0.12876173853874207 0.7608605623245239
CurrentTrain: epoch  6, batch    34 | loss: 5.0718083Losses:  4.149930953979492 0.168960303068161 0.7892481684684753
CurrentTrain: epoch  6, batch    35 | loss: 5.1081390Losses:  4.149323463439941 0.1248120367527008 0.733098030090332
CurrentTrain: epoch  6, batch    36 | loss: 5.0072336Losses:  4.12164831161499 0.17615176737308502 0.7438997030258179
CurrentTrain: epoch  6, batch    37 | loss: 5.0416999Losses:  4.078678131103516 0.17957013845443726 0.7438120245933533
CurrentTrain: epoch  6, batch    38 | loss: 5.0020604Losses:  4.135976314544678 0.19944584369659424 0.7634390592575073
CurrentTrain: epoch  6, batch    39 | loss: 5.0988612Losses:  4.1141133308410645 0.13886693120002747 0.8136426210403442
CurrentTrain: epoch  6, batch    40 | loss: 5.0666227Losses:  4.151196002960205 0.0992470234632492 0.8982087969779968
CurrentTrain: epoch  6, batch    41 | loss: 5.1486516Losses:  4.160679817199707 0.14635515213012695 0.8181374669075012
CurrentTrain: epoch  6, batch    42 | loss: 5.1251726Losses:  4.030369758605957 0.15973269939422607 0.704216718673706
CurrentTrain: epoch  6, batch    43 | loss: 4.8943195Losses:  4.104872703552246 0.09450118243694305 0.7272712588310242
CurrentTrain: epoch  6, batch    44 | loss: 4.9266448Losses:  4.111943244934082 0.1604568064212799 0.7107614278793335
CurrentTrain: epoch  6, batch    45 | loss: 4.9831614Losses:  4.106631278991699 0.1819092482328415 0.7485726475715637
CurrentTrain: epoch  6, batch    46 | loss: 5.0371132Losses:  4.1539764404296875 0.12671661376953125 0.7417701482772827
CurrentTrain: epoch  6, batch    47 | loss: 5.0224633Losses:  4.124021530151367 0.171514630317688 0.7388054132461548
CurrentTrain: epoch  6, batch    48 | loss: 5.0343413Losses:  4.107732772827148 0.16207125782966614 0.7771371603012085
CurrentTrain: epoch  6, batch    49 | loss: 5.0469413Losses:  4.094023704528809 0.11525899171829224 0.7894676923751831
CurrentTrain: epoch  6, batch    50 | loss: 4.9987507Losses:  4.067023277282715 0.14162763953208923 0.7495781183242798
CurrentTrain: epoch  6, batch    51 | loss: 4.9582291Losses:  4.160999298095703 0.1285540610551834 0.8050521612167358
CurrentTrain: epoch  6, batch    52 | loss: 5.0946054Losses:  4.089398384094238 0.15358920395374298 0.807532548904419
CurrentTrain: epoch  6, batch    53 | loss: 5.0505199Losses:  4.087184906005859 0.1725868582725525 0.7469689846038818
CurrentTrain: epoch  6, batch    54 | loss: 5.0067406Losses:  4.100685119628906 0.1508902907371521 0.7935842871665955
CurrentTrain: epoch  6, batch    55 | loss: 5.0451598Losses:  4.138558387756348 0.09303157776594162 0.8061065673828125
CurrentTrain: epoch  6, batch    56 | loss: 5.0376964Losses:  4.240873336791992 0.1804375946521759 0.7790614366531372
CurrentTrain: epoch  6, batch    57 | loss: 5.2003722Losses:  4.1411943435668945 0.147948756814003 0.7798362970352173
CurrentTrain: epoch  6, batch    58 | loss: 5.0689793Losses:  4.582813262939453 0.27296096086502075 0.8375644683837891
CurrentTrain: epoch  6, batch    59 | loss: 5.6933389Losses:  4.095831871032715 0.12391722202301025 0.847404956817627
CurrentTrain: epoch  6, batch    60 | loss: 5.0671539Losses:  4.181735992431641 0.11542715132236481 0.8245323896408081
CurrentTrain: epoch  6, batch    61 | loss: 5.1216955Losses:  4.0826191902160645 0.09202665090560913 0.7503542900085449
CurrentTrain: epoch  6, batch    62 | loss: 4.9250002Losses:  4.185535430908203 0.14612725377082825 0.794939398765564
CurrentTrain: epoch  7, batch     0 | loss: 5.1266022Losses:  4.09268856048584 0.1356322467327118 0.7719783782958984
CurrentTrain: epoch  7, batch     1 | loss: 5.0002990Losses:  4.137588977813721 0.11943367123603821 0.787021815776825
CurrentTrain: epoch  7, batch     2 | loss: 5.0440445Losses:  4.236968994140625 0.17571255564689636 0.8527888059616089
CurrentTrain: epoch  7, batch     3 | loss: 5.2654705Losses:  4.09158992767334 0.16261333227157593 0.7814041376113892
CurrentTrain: epoch  7, batch     4 | loss: 5.0356073Losses:  4.1570634841918945 0.17127659916877747 0.7684488296508789
CurrentTrain: epoch  7, batch     5 | loss: 5.0967889Losses:  4.052995681762695 0.11007195711135864 0.7385079860687256
CurrentTrain: epoch  7, batch     6 | loss: 4.9015760Losses:  4.11562967300415 0.13763470947742462 0.7709314823150635
CurrentTrain: epoch  7, batch     7 | loss: 5.0241957Losses:  4.175210952758789 0.1265708953142166 0.7904354929924011
CurrentTrain: epoch  7, batch     8 | loss: 5.0922170Losses:  4.036192893981934 0.0979270488023758 0.7973741292953491
CurrentTrain: epoch  7, batch     9 | loss: 4.9314942Losses:  4.156355381011963 0.1590404212474823 0.8352749347686768
CurrentTrain: epoch  7, batch    10 | loss: 5.1506710Losses:  4.080137252807617 0.1454961895942688 0.7399086952209473
CurrentTrain: epoch  7, batch    11 | loss: 4.9655423Losses:  4.13881778717041 0.09950282424688339 0.7666512727737427
CurrentTrain: epoch  7, batch    12 | loss: 5.0049720Losses:  4.091156005859375 0.10526221990585327 0.7938246726989746
CurrentTrain: epoch  7, batch    13 | loss: 4.9902430Losses:  4.05960750579834 0.15711762011051178 0.7350468635559082
CurrentTrain: epoch  7, batch    14 | loss: 4.9517722Losses:  4.0351996421813965 0.11810436844825745 0.725753128528595
CurrentTrain: epoch  7, batch    15 | loss: 4.8790574Losses:  4.137219429016113 0.11280234158039093 0.757777750492096
CurrentTrain: epoch  7, batch    16 | loss: 5.0077996Losses:  4.129199028015137 0.15532176196575165 0.7136354446411133
CurrentTrain: epoch  7, batch    17 | loss: 4.9981561Losses:  4.079721450805664 0.14975811541080475 0.8055402636528015
CurrentTrain: epoch  7, batch    18 | loss: 5.0350199Losses:  4.152805328369141 0.11173020303249359 0.8042048215866089
CurrentTrain: epoch  7, batch    19 | loss: 5.0687404Losses:  4.108575820922852 0.09856366366147995 0.8575060963630676
CurrentTrain: epoch  7, batch    20 | loss: 5.0646458Losses:  4.075135707855225 0.11069843918085098 0.7772319912910461
CurrentTrain: epoch  7, batch    21 | loss: 4.9630661Losses:  4.037403583526611 0.10918359458446503 0.7757583260536194
CurrentTrain: epoch  7, batch    22 | loss: 4.9223456Losses:  4.050083160400391 0.15813256800174713 0.7623260021209717
CurrentTrain: epoch  7, batch    23 | loss: 4.9705420Losses:  4.094405174255371 0.10659003257751465 0.7573049068450928
CurrentTrain: epoch  7, batch    24 | loss: 4.9583006Losses:  4.0579094886779785 0.1071000024676323 0.765530526638031
CurrentTrain: epoch  7, batch    25 | loss: 4.9305401Losses:  4.044492721557617 0.15047965943813324 0.7221640348434448
CurrentTrain: epoch  7, batch    26 | loss: 4.9171367Losses:  4.095852851867676 0.13808032870292664 0.7891457676887512
CurrentTrain: epoch  7, batch    27 | loss: 5.0230789Losses:  4.108098983764648 0.10929162055253983 0.844174861907959
CurrentTrain: epoch  7, batch    28 | loss: 5.0615654Losses:  4.104822158813477 0.12930187582969666 0.7586894035339355
CurrentTrain: epoch  7, batch    29 | loss: 4.9928136Losses:  4.041040420532227 0.1246071457862854 0.7397639751434326
CurrentTrain: epoch  7, batch    30 | loss: 4.9054117Losses:  4.067771911621094 0.13611271977424622 0.7695779800415039
CurrentTrain: epoch  7, batch    31 | loss: 4.9734626Losses:  4.067651748657227 0.14497020840644836 0.7983449697494507
CurrentTrain: epoch  7, batch    32 | loss: 5.0109673Losses:  4.043166160583496 0.13690854609012604 0.7671927213668823
CurrentTrain: epoch  7, batch    33 | loss: 4.9472675Losses:  4.0234880447387695 0.06517882645130157 0.6820403337478638
CurrentTrain: epoch  7, batch    34 | loss: 4.7707071Losses:  4.113462448120117 0.10269691050052643 0.7284213900566101
CurrentTrain: epoch  7, batch    35 | loss: 4.9445806Losses:  4.120075225830078 0.14598014950752258 0.7442466020584106
CurrentTrain: epoch  7, batch    36 | loss: 5.0103021Losses:  4.1307373046875 0.11605405807495117 0.7402197122573853
CurrentTrain: epoch  7, batch    37 | loss: 4.9870110Losses:  4.077944755554199 0.07631422579288483 0.7521493434906006
CurrentTrain: epoch  7, batch    38 | loss: 4.9064083Losses:  4.108154296875 0.11574416607618332 0.8239380121231079
CurrentTrain: epoch  7, batch    39 | loss: 5.0478363Losses:  4.148991584777832 0.10390450805425644 0.7570193409919739
CurrentTrain: epoch  7, batch    40 | loss: 5.0099158Losses:  4.060211181640625 0.12405171245336533 0.8021005392074585
CurrentTrain: epoch  7, batch    41 | loss: 4.9863634Losses:  4.071029186248779 0.1697077751159668 0.7748903036117554
CurrentTrain: epoch  7, batch    42 | loss: 5.0156274Losses:  4.074691295623779 0.11499717831611633 0.8158869743347168
CurrentTrain: epoch  7, batch    43 | loss: 5.0055757Losses:  4.054403305053711 0.1373840868473053 0.7437919974327087
CurrentTrain: epoch  7, batch    44 | loss: 4.9355793Losses:  4.0614728927612305 0.11763972043991089 0.719225287437439
CurrentTrain: epoch  7, batch    45 | loss: 4.8983378Losses:  4.103011131286621 0.09476424008607864 0.8653512001037598
CurrentTrain: epoch  7, batch    46 | loss: 5.0631266Losses:  4.132988929748535 0.0944739580154419 0.767288863658905
CurrentTrain: epoch  7, batch    47 | loss: 4.9947515Losses:  4.083597183227539 0.12169826030731201 0.819800078868866
CurrentTrain: epoch  7, batch    48 | loss: 5.0250955Losses:  3.983846664428711 0.09604033827781677 0.6864204406738281
CurrentTrain: epoch  7, batch    49 | loss: 4.7663074Losses:  4.010443210601807 0.10117151588201523 0.8224380016326904
CurrentTrain: epoch  7, batch    50 | loss: 4.9340525Losses:  4.073398590087891 0.13655225932598114 0.8066267371177673
CurrentTrain: epoch  7, batch    51 | loss: 5.0165777Losses:  4.109618186950684 0.12087631225585938 0.7574710249900818
CurrentTrain: epoch  7, batch    52 | loss: 4.9879656Losses:  4.036219596862793 0.1568198800086975 0.7679333686828613
CurrentTrain: epoch  7, batch    53 | loss: 4.9609728Losses:  4.0888872146606445 0.13815619051456451 0.7423560619354248
CurrentTrain: epoch  7, batch    54 | loss: 4.9693995Losses:  4.105450630187988 0.15466278791427612 0.8234171867370605
CurrentTrain: epoch  7, batch    55 | loss: 5.0835304Losses:  4.075448036193848 0.12877219915390015 0.702973484992981
CurrentTrain: epoch  7, batch    56 | loss: 4.9071937Losses:  4.074800968170166 0.13756391406059265 0.7266349196434021
CurrentTrain: epoch  7, batch    57 | loss: 4.9389997Losses:  4.057973861694336 0.14145907759666443 0.7506325840950012
CurrentTrain: epoch  7, batch    58 | loss: 4.9500656Losses:  3.989178419113159 0.12756630778312683 0.7368152737617493
CurrentTrain: epoch  7, batch    59 | loss: 4.8535600Losses:  4.020501136779785 0.11824598163366318 0.7036252021789551
CurrentTrain: epoch  7, batch    60 | loss: 4.8423724Losses:  4.099027633666992 0.12005778402090073 0.7525630593299866
CurrentTrain: epoch  7, batch    61 | loss: 4.9716482Losses:  3.9950735569000244 0.06382740288972855 0.6332981586456299
CurrentTrain: epoch  7, batch    62 | loss: 4.6921988Losses:  4.108467102050781 0.06867343187332153 0.7214953899383545
CurrentTrain: epoch  8, batch     0 | loss: 4.8986359Losses:  4.00782585144043 0.1253214031457901 0.7484413385391235
CurrentTrain: epoch  8, batch     1 | loss: 4.8815885Losses:  4.0177693367004395 0.13380166888237 0.7098934650421143
CurrentTrain: epoch  8, batch     2 | loss: 4.8614645Losses:  4.048363208770752 0.13383422791957855 0.7192388772964478
CurrentTrain: epoch  8, batch     3 | loss: 4.9014363Losses:  4.037046432495117 0.08408790826797485 0.8592855930328369
CurrentTrain: epoch  8, batch     4 | loss: 4.9804201Losses:  4.031400680541992 0.12532371282577515 0.7747828364372253
CurrentTrain: epoch  8, batch     5 | loss: 4.9315071Losses:  4.070520401000977 0.12148147821426392 0.7765780091285706
CurrentTrain: epoch  8, batch     6 | loss: 4.9685798Losses:  4.11705207824707 0.14290496706962585 0.7165533304214478
CurrentTrain: epoch  8, batch     7 | loss: 4.9765100Losses:  4.086445331573486 0.09275123476982117 0.7967780828475952
CurrentTrain: epoch  8, batch     8 | loss: 4.9759746Losses:  4.053102493286133 0.11429417133331299 0.7563455700874329
CurrentTrain: epoch  8, batch     9 | loss: 4.9237423Losses:  4.055527210235596 0.09806998074054718 0.8477743864059448
CurrentTrain: epoch  8, batch    10 | loss: 5.0013719Losses:  4.061532974243164 0.13568344712257385 0.7683333158493042
CurrentTrain: epoch  8, batch    11 | loss: 4.9655499Losses:  4.05473518371582 0.14529015123844147 0.7157139778137207
CurrentTrain: epoch  8, batch    12 | loss: 4.9157395Losses:  4.093735694885254 0.09516637027263641 0.7484188079833984
CurrentTrain: epoch  8, batch    13 | loss: 4.9373207Losses:  4.038378715515137 0.14044754207134247 0.7392475008964539
CurrentTrain: epoch  8, batch    14 | loss: 4.9180737Losses:  4.066808700561523 0.12374541163444519 0.7358847856521606
CurrentTrain: epoch  8, batch    15 | loss: 4.9264388Losses:  4.048878192901611 0.11512018740177155 0.7739648818969727
CurrentTrain: epoch  8, batch    16 | loss: 4.9379635Losses:  4.030868053436279 0.10267374664545059 0.7760698795318604
CurrentTrain: epoch  8, batch    17 | loss: 4.9096117Losses:  4.020866394042969 0.13968883454799652 0.7220220565795898
CurrentTrain: epoch  8, batch    18 | loss: 4.8825774Losses:  4.031676292419434 0.1506250649690628 0.7208757400512695
CurrentTrain: epoch  8, batch    19 | loss: 4.9031773Losses:  4.056654930114746 0.07334230840206146 0.7544299364089966
CurrentTrain: epoch  8, batch    20 | loss: 4.8844271Losses:  4.042913436889648 0.11543942242860794 0.8000945448875427
CurrentTrain: epoch  8, batch    21 | loss: 4.9584475Losses:  4.032120227813721 0.10623398423194885 0.7261518836021423
CurrentTrain: epoch  8, batch    22 | loss: 4.8645062Losses:  4.018126487731934 0.11346939206123352 0.6908278465270996
CurrentTrain: epoch  8, batch    23 | loss: 4.8224239Losses:  4.066741943359375 0.14269092679023743 0.7846763134002686
CurrentTrain: epoch  8, batch    24 | loss: 4.9941092Losses:  4.035269737243652 0.12552058696746826 0.7353110313415527
CurrentTrain: epoch  8, batch    25 | loss: 4.8961015Losses:  4.071043968200684 0.10039132088422775 0.8275133371353149
CurrentTrain: epoch  8, batch    26 | loss: 4.9989486Losses:  4.053083419799805 0.14114004373550415 0.7559511661529541
CurrentTrain: epoch  8, batch    27 | loss: 4.9501743Losses:  4.0358123779296875 0.09575655311346054 0.7531272172927856
CurrentTrain: epoch  8, batch    28 | loss: 4.8846960Losses:  4.042203426361084 0.06170015037059784 0.716471791267395
CurrentTrain: epoch  8, batch    29 | loss: 4.8203754Losses:  4.052879333496094 0.1148771271109581 0.7286491394042969
CurrentTrain: epoch  8, batch    30 | loss: 4.8964057Losses:  4.059462070465088 0.12718580663204193 0.8096272349357605
CurrentTrain: epoch  8, batch    31 | loss: 4.9962749Losses:  4.031314373016357 0.07845479249954224 0.7089354991912842
CurrentTrain: epoch  8, batch    32 | loss: 4.8187046Losses:  3.9682087898254395 0.1264735460281372 0.7429625988006592
CurrentTrain: epoch  8, batch    33 | loss: 4.8376446Losses:  4.034582614898682 0.13185784220695496 0.7468321323394775
CurrentTrain: epoch  8, batch    34 | loss: 4.9132729Losses:  4.056368827819824 0.08046284317970276 0.7513630986213684
CurrentTrain: epoch  8, batch    35 | loss: 4.8881950Losses:  4.036922931671143 0.13682155311107635 0.7466082572937012
CurrentTrain: epoch  8, batch    36 | loss: 4.9203529Losses:  4.083890914916992 0.13555321097373962 0.790436863899231
CurrentTrain: epoch  8, batch    37 | loss: 5.0098810Losses:  3.9518699645996094 0.09229789674282074 0.820189893245697
CurrentTrain: epoch  8, batch    38 | loss: 4.8643579Losses:  4.051225662231445 0.11745194345712662 0.6929771900177002
CurrentTrain: epoch  8, batch    39 | loss: 4.8616552Losses:  4.033835411071777 0.08270116150379181 0.75623619556427
CurrentTrain: epoch  8, batch    40 | loss: 4.8727727Losses:  4.014369010925293 0.11820625513792038 0.7808704972267151
CurrentTrain: epoch  8, batch    41 | loss: 4.9134455Losses:  4.060450553894043 0.10886358469724655 0.8091943860054016
CurrentTrain: epoch  8, batch    42 | loss: 4.9785085Losses:  4.034137725830078 0.15213002264499664 0.724602222442627
CurrentTrain: epoch  8, batch    43 | loss: 4.9108701Losses:  4.020135879516602 0.1348021924495697 0.7485796213150024
CurrentTrain: epoch  8, batch    44 | loss: 4.9035177Losses:  4.024723529815674 0.1275673806667328 0.722240686416626
CurrentTrain: epoch  8, batch    45 | loss: 4.8745317Losses:  4.022441387176514 0.11958302557468414 0.7564711570739746
CurrentTrain: epoch  8, batch    46 | loss: 4.8984957Losses:  4.024775505065918 0.0994706004858017 0.7778438925743103
CurrentTrain: epoch  8, batch    47 | loss: 4.9020901Losses:  4.053689956665039 0.10867229849100113 0.7987344264984131
CurrentTrain: epoch  8, batch    48 | loss: 4.9610968Losses:  3.987708330154419 0.11390461027622223 0.7558795213699341
CurrentTrain: epoch  8, batch    49 | loss: 4.8574924Losses:  4.015018939971924 0.11091236025094986 0.7797801494598389
CurrentTrain: epoch  8, batch    50 | loss: 4.9057112Losses:  3.9959514141082764 0.08929048478603363 0.7727809548377991
CurrentTrain: epoch  8, batch    51 | loss: 4.8580227Losses:  4.0027031898498535 0.12988120317459106 0.735742449760437
CurrentTrain: epoch  8, batch    52 | loss: 4.8683271Losses:  4.0055742263793945 0.1295875459909439 0.7243165373802185
CurrentTrain: epoch  8, batch    53 | loss: 4.8594785Losses:  4.047691345214844 0.10924478620290756 0.7348798513412476
CurrentTrain: epoch  8, batch    54 | loss: 4.8918161Losses:  4.058437347412109 0.11308075487613678 0.7812809944152832
CurrentTrain: epoch  8, batch    55 | loss: 4.9527993Losses:  4.0339579582214355 0.1311531662940979 0.8114184737205505
CurrentTrain: epoch  8, batch    56 | loss: 4.9765296Losses:  3.9850242137908936 0.12485034763813019 0.7865707278251648
CurrentTrain: epoch  8, batch    57 | loss: 4.8964453Losses:  4.01039981842041 0.11572495847940445 0.7025761604309082
CurrentTrain: epoch  8, batch    58 | loss: 4.8287010Losses:  4.040000915527344 0.1121918112039566 0.7836285829544067
CurrentTrain: epoch  8, batch    59 | loss: 4.9358211Losses:  4.027811050415039 0.11887041479349136 0.7410058975219727
CurrentTrain: epoch  8, batch    60 | loss: 4.8876872Losses:  4.031826972961426 0.12910403311252594 0.7550824880599976
CurrentTrain: epoch  8, batch    61 | loss: 4.9160137Losses:  4.1063337326049805 0.07022485136985779 0.7379469871520996
CurrentTrain: epoch  8, batch    62 | loss: 4.9145055Losses:  4.022563457489014 0.1104658991098404 0.7228248715400696
CurrentTrain: epoch  9, batch     0 | loss: 4.8558545Losses:  4.044919967651367 0.09408396482467651 0.727036714553833
CurrentTrain: epoch  9, batch     1 | loss: 4.8660402Losses:  4.003958702087402 0.07037694752216339 0.7276538610458374
CurrentTrain: epoch  9, batch     2 | loss: 4.8019896Losses:  3.9765117168426514 0.0713159367442131 0.7834067344665527
CurrentTrain: epoch  9, batch     3 | loss: 4.8312345Losses:  4.088535785675049 0.09757635742425919 0.8104137182235718
CurrentTrain: epoch  9, batch     4 | loss: 4.9965258Losses:  4.0355072021484375 0.12304028868675232 0.7902725338935852
CurrentTrain: epoch  9, batch     5 | loss: 4.9488201Losses:  4.011650085449219 0.12057732790708542 0.7638553380966187
CurrentTrain: epoch  9, batch     6 | loss: 4.8960829Losses:  4.026113033294678 0.11494186520576477 0.7783262729644775
CurrentTrain: epoch  9, batch     7 | loss: 4.9193811Losses:  4.032299041748047 0.04828549921512604 0.7029596567153931
CurrentTrain: epoch  9, batch     8 | loss: 4.7835441Losses:  3.9994122982025146 0.09640435874462128 0.7992193698883057
CurrentTrain: epoch  9, batch     9 | loss: 4.8950357Losses:  4.010605335235596 0.11877500265836716 0.7624056339263916
CurrentTrain: epoch  9, batch    10 | loss: 4.8917856Losses:  4.0209784507751465 0.08444090187549591 0.675426185131073
CurrentTrain: epoch  9, batch    11 | loss: 4.7808452Losses:  4.024715423583984 0.08054544031620026 0.7340275049209595
CurrentTrain: epoch  9, batch    12 | loss: 4.8392882Losses:  3.9964559078216553 0.1344776749610901 0.714229941368103
CurrentTrain: epoch  9, batch    13 | loss: 4.8451638Losses:  4.092680931091309 0.11224128305912018 0.846611499786377
CurrentTrain: epoch  9, batch    14 | loss: 5.0515337Losses:  4.008657455444336 0.11848732829093933 0.7686934471130371
CurrentTrain: epoch  9, batch    15 | loss: 4.8958383Losses:  4.013156414031982 0.11158834397792816 0.7220618724822998
CurrentTrain: epoch  9, batch    16 | loss: 4.8468065Losses:  4.003037452697754 0.11353781819343567 0.6958301067352295
CurrentTrain: epoch  9, batch    17 | loss: 4.8124056Losses:  4.051316261291504 0.06968092918395996 0.7138546109199524
CurrentTrain: epoch  9, batch    18 | loss: 4.8348522Losses:  4.003461837768555 0.12467175722122192 0.6876327991485596
CurrentTrain: epoch  9, batch    19 | loss: 4.8157663Losses:  4.002596855163574 0.12181076407432556 0.7505335211753845
CurrentTrain: epoch  9, batch    20 | loss: 4.8749413Losses:  4.0320940017700195 0.12396638095378876 0.7499707937240601
CurrentTrain: epoch  9, batch    21 | loss: 4.9060311Losses:  4.016119003295898 0.11002901196479797 0.6793287992477417
CurrentTrain: epoch  9, batch    22 | loss: 4.8054771Losses:  3.9850499629974365 0.0770552009344101 0.6873796582221985
CurrentTrain: epoch  9, batch    23 | loss: 4.7494850Losses:  3.9996442794799805 0.12700733542442322 0.7111122608184814
CurrentTrain: epoch  9, batch    24 | loss: 4.8377638Losses:  3.994387149810791 0.1276756078004837 0.7769292593002319
CurrentTrain: epoch  9, batch    25 | loss: 4.8989921Losses:  4.002572059631348 0.1284838318824768 0.7650567293167114
CurrentTrain: epoch  9, batch    26 | loss: 4.8961124Losses:  4.051894187927246 0.1256280392408371 0.7734386324882507
CurrentTrain: epoch  9, batch    27 | loss: 4.9509606Losses:  4.016728401184082 0.1269684135913849 0.7682955265045166
CurrentTrain: epoch  9, batch    28 | loss: 4.9119921Losses:  4.006354331970215 0.10510554909706116 0.7182022333145142
CurrentTrain: epoch  9, batch    29 | loss: 4.8296618Losses:  4.002701759338379 0.09045194834470749 0.7102741003036499
CurrentTrain: epoch  9, batch    30 | loss: 4.8034277Losses:  4.057095050811768 0.07221749424934387 0.863568902015686
CurrentTrain: epoch  9, batch    31 | loss: 4.9928813Losses:  3.992511749267578 0.07573522627353668 0.8001744747161865
CurrentTrain: epoch  9, batch    32 | loss: 4.8684216Losses:  4.013309478759766 0.06515837460756302 0.8258520364761353
CurrentTrain: epoch  9, batch    33 | loss: 4.9043198Losses:  3.998783588409424 0.09836142510175705 0.7827150821685791
CurrentTrain: epoch  9, batch    34 | loss: 4.8798599Losses:  4.010628700256348 0.11869227886199951 0.7480553984642029
CurrentTrain: epoch  9, batch    35 | loss: 4.8773766Losses:  3.989284038543701 0.11527159810066223 0.6800114512443542
CurrentTrain: epoch  9, batch    36 | loss: 4.7845669Losses:  3.992086887359619 0.07875338196754456 0.7155937552452087
CurrentTrain: epoch  9, batch    37 | loss: 4.7864342Losses:  4.045385360717773 0.09583544731140137 0.6846234202384949
CurrentTrain: epoch  9, batch    38 | loss: 4.8258443Losses:  3.9945077896118164 0.10180314630270004 0.7211238145828247
CurrentTrain: epoch  9, batch    39 | loss: 4.8174348Losses:  4.005649566650391 0.09894581884145737 0.7873443365097046
CurrentTrain: epoch  9, batch    40 | loss: 4.8919396Losses:  3.995574474334717 0.12860381603240967 0.716539740562439
CurrentTrain: epoch  9, batch    41 | loss: 4.8407183Losses:  4.0281662940979 0.08321956545114517 0.7014244794845581
CurrentTrain: epoch  9, batch    42 | loss: 4.8128104Losses:  3.9942007064819336 0.09723580628633499 0.7693456411361694
CurrentTrain: epoch  9, batch    43 | loss: 4.8607821Losses:  3.9810233116149902 0.09556270390748978 0.6786794662475586
CurrentTrain: epoch  9, batch    44 | loss: 4.7552657Losses:  4.028179168701172 0.09981128573417664 0.7146509885787964
CurrentTrain: epoch  9, batch    45 | loss: 4.8426414Losses:  3.9597113132476807 0.07008476555347443 0.700620174407959
CurrentTrain: epoch  9, batch    46 | loss: 4.7304163Losses:  4.036210060119629 0.09025315940380096 0.7892844676971436
CurrentTrain: epoch  9, batch    47 | loss: 4.9157476Losses:  4.003092288970947 0.10592783242464066 0.7698057889938354
CurrentTrain: epoch  9, batch    48 | loss: 4.8788261Losses:  4.053301811218262 0.09365632385015488 0.8117424249649048
CurrentTrain: epoch  9, batch    49 | loss: 4.9587007Losses:  4.003205299377441 0.0920974612236023 0.7046491503715515
CurrentTrain: epoch  9, batch    50 | loss: 4.7999516Losses:  3.9295494556427 0.11896974593400955 0.7261529564857483
CurrentTrain: epoch  9, batch    51 | loss: 4.7746720Losses:  4.03186559677124 0.13194702565670013 0.8039371371269226
CurrentTrain: epoch  9, batch    52 | loss: 4.9677496Losses:  3.979128360748291 0.09325512498617172 0.711732029914856
CurrentTrain: epoch  9, batch    53 | loss: 4.7841153Losses:  4.016812324523926 0.12109903246164322 0.7523520588874817
CurrentTrain: epoch  9, batch    54 | loss: 4.8902636Losses:  4.000384330749512 0.08796428143978119 0.7863736152648926
CurrentTrain: epoch  9, batch    55 | loss: 4.8747220Losses:  4.006000518798828 0.08777394890785217 0.750370442867279
CurrentTrain: epoch  9, batch    56 | loss: 4.8441448Losses:  4.011981964111328 0.05543772503733635 0.7304357886314392
CurrentTrain: epoch  9, batch    57 | loss: 4.7978554Losses:  4.0218634605407715 0.10588251054286957 0.804425835609436
CurrentTrain: epoch  9, batch    58 | loss: 4.9321718Losses:  4.0335283279418945 0.06793481111526489 0.7852692604064941
CurrentTrain: epoch  9, batch    59 | loss: 4.8867326Losses:  3.9927191734313965 0.10874281823635101 0.7824960350990295
CurrentTrain: epoch  9, batch    60 | loss: 4.8839579Losses:  4.031404495239258 0.12669029831886292 0.7069910764694214
CurrentTrain: epoch  9, batch    61 | loss: 4.8650861Losses:  3.9609599113464355 0.02515001781284809 0.6574199199676514
CurrentTrain: epoch  9, batch    62 | loss: 4.6435299
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  7.3674163818359375 1.859369158744812 0.9896218776702881
CurrentTrain: epoch  0, batch     0 | loss: 10.2164078Losses:  7.057424545288086 1.6970977783203125 0.9845744967460632
CurrentTrain: epoch  0, batch     1 | loss: 9.7390966Losses:  7.471452236175537 1.7392504215240479 0.9824632406234741
CurrentTrain: epoch  0, batch     2 | loss: 10.1931658Losses:  4.490023612976074 0.2122848629951477 0.948217511177063
CurrentTrain: epoch  0, batch     3 | loss: 5.6505260Losses:  6.604248046875 1.8515598773956299 0.9761058688163757
CurrentTrain: epoch  1, batch     0 | loss: 9.4319134Losses:  6.384050369262695 1.7453973293304443 0.9777754545211792
CurrentTrain: epoch  1, batch     1 | loss: 9.1072235Losses:  5.974947452545166 1.7730021476745605 0.9779092073440552
CurrentTrain: epoch  1, batch     2 | loss: 8.7258587Losses:  5.036570072174072 0.38992077112197876 0.9353851079940796
CurrentTrain: epoch  1, batch     3 | loss: 6.3618760Losses:  5.618014812469482 1.3323078155517578 0.9657169580459595
CurrentTrain: epoch  2, batch     0 | loss: 7.9160395Losses:  5.779566764831543 1.637147068977356 0.9775146245956421
CurrentTrain: epoch  2, batch     1 | loss: 8.3942280Losses:  5.21870756149292 1.74040949344635 0.9657585620880127
CurrentTrain: epoch  2, batch     2 | loss: 7.9248753Losses:  4.055438041687012 0.23321931064128876 0.9911177158355713
CurrentTrain: epoch  2, batch     3 | loss: 5.2797747Losses:  5.183261871337891 1.1320436000823975 0.9787557721138
CurrentTrain: epoch  3, batch     0 | loss: 7.2940617Losses:  4.2007927894592285 1.3557333946228027 0.9464277625083923
CurrentTrain: epoch  3, batch     1 | loss: 6.5029540Losses:  5.236698150634766 1.767889380455017 0.9756956100463867
CurrentTrain: epoch  3, batch     2 | loss: 7.9802833Losses:  5.2875657081604 0.2981914281845093 0.9504892826080322
CurrentTrain: epoch  3, batch     3 | loss: 6.5362463Losses:  3.5933046340942383 1.2242228984832764 0.9580167531967163
CurrentTrain: epoch  4, batch     0 | loss: 5.7755446Losses:  5.545717239379883 1.435553789138794 0.9678546190261841
CurrentTrain: epoch  4, batch     1 | loss: 7.9491253Losses:  3.8207597732543945 1.2504711151123047 0.9604026675224304
CurrentTrain: epoch  4, batch     2 | loss: 6.0316334Losses:  7.9505462646484375 8.94069742685133e-08 0.9563484191894531
CurrentTrain: epoch  4, batch     3 | loss: 8.9068947Losses:  4.376805305480957 1.3651561737060547 0.943436861038208
CurrentTrain: epoch  5, batch     0 | loss: 6.6853981Losses:  4.018585205078125 1.2545570135116577 0.9516983032226562
CurrentTrain: epoch  5, batch     1 | loss: 6.2248406Losses:  5.306675910949707 1.5777199268341064 0.9783494472503662
CurrentTrain: epoch  5, batch     2 | loss: 7.8627453Losses:  2.0980727672576904 0.12142747640609741 1.0
CurrentTrain: epoch  5, batch     3 | loss: 3.2195003Losses:  4.467605113983154 1.3762037754058838 0.9684973359107971
CurrentTrain: epoch  6, batch     0 | loss: 6.8123064Losses:  3.907458782196045 1.337781310081482 0.9389561414718628
CurrentTrain: epoch  6, batch     1 | loss: 6.1841965Losses:  3.9314839839935303 1.358264446258545 0.9605040550231934
CurrentTrain: epoch  6, batch     2 | loss: 6.2502522Losses:  4.581686973571777 2.0861628513557662e-07 0.9356288909912109
CurrentTrain: epoch  6, batch     3 | loss: 5.5173159Losses:  4.6844024658203125 1.2900238037109375 0.9534423351287842
CurrentTrain: epoch  7, batch     0 | loss: 6.9278688Losses:  3.789632558822632 1.427612543106079 0.9446501731872559
CurrentTrain: epoch  7, batch     1 | loss: 6.1618953Losses:  3.300920009613037 1.216381311416626 0.9527682065963745
CurrentTrain: epoch  7, batch     2 | loss: 5.4700699Losses:  4.757038116455078 0.36066246032714844 0.9871456623077393
CurrentTrain: epoch  7, batch     3 | loss: 6.1048460Losses:  3.6285042762756348 1.404046654701233 0.9372227191925049
CurrentTrain: epoch  8, batch     0 | loss: 5.9697733Losses:  3.7516322135925293 1.2611922025680542 0.9557898044586182
CurrentTrain: epoch  8, batch     1 | loss: 5.9686146Losses:  4.450800895690918 0.9875081777572632 0.9518600106239319
CurrentTrain: epoch  8, batch     2 | loss: 6.3901691Losses:  2.2148709297180176 0.22417610883712769 0.9400137662887573
CurrentTrain: epoch  8, batch     3 | loss: 3.3790607Losses:  3.7605204582214355 1.0744682550430298 0.9433678388595581
CurrentTrain: epoch  9, batch     0 | loss: 5.7783566Losses:  4.23952054977417 1.2723995447158813 0.9680159687995911
CurrentTrain: epoch  9, batch     1 | loss: 6.4799361Losses:  3.4085099697113037 0.8976833820343018 0.9278852939605713
CurrentTrain: epoch  9, batch     2 | loss: 5.2340784Losses:  2.0733680725097656 0.18800745904445648 0.8784019947052002
CurrentTrain: epoch  9, batch     3 | loss: 3.1397774
Losses:  1.9360854625701904 1.150177240371704 0.894851803779602
MemoryTrain:  epoch  0, batch     0 | loss: 3.9811144Losses:  0.5849481225013733 0.4014916718006134 0.864864706993103
MemoryTrain:  epoch  0, batch     1 | loss: 1.8513045Losses:  2.1774144172668457 1.1707488298416138 0.8959834575653076
MemoryTrain:  epoch  1, batch     0 | loss: 4.2441463Losses:  0.04990207403898239 0.3901670277118683 0.8660051822662354
MemoryTrain:  epoch  1, batch     1 | loss: 1.3060743Losses:  1.612438440322876 1.162457823753357 0.8777871131896973
MemoryTrain:  epoch  2, batch     0 | loss: 3.6526833Losses:  0.1855057328939438 0.17394348978996277 0.9032275080680847
MemoryTrain:  epoch  2, batch     1 | loss: 1.2626767Losses:  0.9403545260429382 1.0282139778137207 0.885046660900116
MemoryTrain:  epoch  3, batch     0 | loss: 2.8536153Losses:  0.16195514798164368 0.1681365966796875 0.8680565357208252
MemoryTrain:  epoch  3, batch     1 | loss: 1.1981483Losses:  0.3973727226257324 1.0002672672271729 0.8778812885284424
MemoryTrain:  epoch  4, batch     0 | loss: 2.2755213Losses:  1.0975587368011475 0.22984053194522858 0.8998638391494751
MemoryTrain:  epoch  4, batch     1 | loss: 2.2272630Losses:  0.2156117856502533 0.9532393217086792 0.8609722852706909
MemoryTrain:  epoch  5, batch     0 | loss: 2.0298233Losses:  0.2829890549182892 0.1905929446220398 0.9807192087173462
MemoryTrain:  epoch  5, batch     1 | loss: 1.4543012Losses:  0.1662457436323166 0.8564136028289795 0.8897439241409302
MemoryTrain:  epoch  6, batch     0 | loss: 1.9124032Losses:  0.040163010358810425 0.45609164237976074 0.8570526242256165
MemoryTrain:  epoch  6, batch     1 | loss: 1.3533072Losses:  0.11786072701215744 0.9258607029914856 0.8610345125198364
MemoryTrain:  epoch  7, batch     0 | loss: 1.9047559Losses:  0.09959838539361954 0.21831747889518738 0.9627084732055664
MemoryTrain:  epoch  7, batch     1 | loss: 1.2806244Losses:  0.13923266530036926 0.9365741014480591 0.8479295969009399
MemoryTrain:  epoch  8, batch     0 | loss: 1.9237363Losses:  0.019014937803149223 0.21140053868293762 1.0
MemoryTrain:  epoch  8, batch     1 | loss: 1.2304155Losses:  0.11090369522571564 1.0120182037353516 0.8737465739250183
MemoryTrain:  epoch  9, batch     0 | loss: 1.9966686Losses:  0.039052411913871765 0.053532227873802185 0.9217719435691833
MemoryTrain:  epoch  9, batch     1 | loss: 1.0143566
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 44.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 74.01%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 73.27%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 73.31%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 72.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 72.96%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 73.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 73.88%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 72.35%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 71.98%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 71.11%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 69.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.68%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.85%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.15%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 91.80%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 90.48%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 89.30%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 88.06%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 87.04%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 86.23%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 86.36%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 86.11%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 85.77%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 85.63%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 85.36%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 85.08%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 85.11%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 85.21%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 85.09%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 84.97%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 84.85%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 84.88%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 84.84%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.11%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 85.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.43%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 86.26%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.03%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 85.74%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 85.58%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 85.60%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.38%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 85.46%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 85.30%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 85.09%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 84.94%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 84.91%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 84.49%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 84.35%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 84.32%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 84.43%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 84.40%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 84.45%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 84.06%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 83.73%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 83.30%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 82.93%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 82.36%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 82.15%   
cur_acc:  ['0.9435', '0.6984']
his_acc:  ['0.9435', '0.8215']
Clustering into  14  clusters
Clusters:  [ 2  9 13  2  2  2  0  2 10  1  0  2  4  2 11  6  7  2  2  2  2  2  2  8
  2  1  2  3 12  5]
Losses:  7.2205939292907715 1.435057282447815 0.9931812286376953
CurrentTrain: epoch  0, batch     0 | loss: 9.6488323Losses:  6.2069854736328125 1.368781566619873 0.976954460144043
CurrentTrain: epoch  0, batch     1 | loss: 8.5527210Losses:  6.284862518310547 1.1521008014678955 0.9538578987121582
CurrentTrain: epoch  0, batch     2 | loss: 8.3908215Losses:  4.660153388977051 0.12183347344398499 0.9516741037368774
CurrentTrain: epoch  0, batch     3 | loss: 5.7336607Losses:  5.839612007141113 0.9319628477096558 0.9619084596633911
CurrentTrain: epoch  1, batch     0 | loss: 7.7334833Losses:  5.598479270935059 1.226914882659912 0.9767125844955444
CurrentTrain: epoch  1, batch     1 | loss: 7.8021069Losses:  4.135379314422607 1.1508878469467163 0.9703803062438965
CurrentTrain: epoch  1, batch     2 | loss: 6.2566476Losses:  5.132395267486572 0.22847214341163635 0.9206540584564209
CurrentTrain: epoch  1, batch     3 | loss: 6.2815218Losses:  4.129139423370361 1.1297577619552612 0.9783418774604797
CurrentTrain: epoch  2, batch     0 | loss: 6.2372394Losses:  4.1761674880981445 1.124682068824768 0.9555443525314331
CurrentTrain: epoch  2, batch     1 | loss: 6.2563939Losses:  4.188348770141602 1.0373129844665527 0.964835524559021
CurrentTrain: epoch  2, batch     2 | loss: 6.1904974Losses:  3.4918322563171387 0.32453295588493347 0.9718154072761536
CurrentTrain: epoch  2, batch     3 | loss: 4.7881808Losses:  3.8552658557891846 1.008773684501648 0.9605158567428589
CurrentTrain: epoch  3, batch     0 | loss: 5.8245554Losses:  3.463688373565674 1.1252042055130005 0.9691370725631714
CurrentTrain: epoch  3, batch     1 | loss: 5.5580297Losses:  3.7707114219665527 0.8946260809898376 0.9715617299079895
CurrentTrain: epoch  3, batch     2 | loss: 5.6368995Losses:  3.0848939418792725 0.22759000957012177 0.9300196170806885
CurrentTrain: epoch  3, batch     3 | loss: 4.2425036Losses:  3.420789957046509 0.8870320320129395 0.9603047370910645
CurrentTrain: epoch  4, batch     0 | loss: 5.2681270Losses:  3.2923526763916016 1.003345012664795 0.9462222456932068
CurrentTrain: epoch  4, batch     1 | loss: 5.2419200Losses:  3.5341274738311768 0.8524761199951172 0.9794962406158447
CurrentTrain: epoch  4, batch     2 | loss: 5.3660994Losses:  2.4329018592834473 0.08899779617786407 1.0
CurrentTrain: epoch  4, batch     3 | loss: 3.5218997Losses:  2.8888440132141113 0.9230703115463257 0.9713907241821289
CurrentTrain: epoch  5, batch     0 | loss: 4.7833052Losses:  3.6576576232910156 0.7929260730743408 0.9575026035308838
CurrentTrain: epoch  5, batch     1 | loss: 5.4080858Losses:  2.859104633331299 0.7888607978820801 0.9702879190444946
CurrentTrain: epoch  5, batch     2 | loss: 4.6182532Losses:  2.469505786895752 0.3084312677383423 0.9026839733123779
CurrentTrain: epoch  5, batch     3 | loss: 3.6806209Losses:  3.479959011077881 0.8870614767074585 0.9511412382125854
CurrentTrain: epoch  6, batch     0 | loss: 5.3181620Losses:  2.6557416915893555 0.8183708190917969 0.9662289619445801
CurrentTrain: epoch  6, batch     1 | loss: 4.4403415Losses:  2.2835917472839355 0.7365058660507202 0.9565491080284119
CurrentTrain: epoch  6, batch     2 | loss: 3.9766469Losses:  3.9705405235290527 0.4151994585990906 1.0
CurrentTrain: epoch  6, batch     3 | loss: 5.3857398Losses:  2.434873580932617 0.725459098815918 0.9589129686355591
CurrentTrain: epoch  7, batch     0 | loss: 4.1192455Losses:  3.119180917739868 0.8224400281906128 0.9594370126724243
CurrentTrain: epoch  7, batch     1 | loss: 4.9010577Losses:  3.005908489227295 0.7464168071746826 0.9509289264678955
CurrentTrain: epoch  7, batch     2 | loss: 4.7032542Losses:  2.4168639183044434 0.1387423723936081 0.9526500701904297
CurrentTrain: epoch  7, batch     3 | loss: 3.5082564Losses:  2.4840328693389893 0.6678728461265564 0.9573347568511963
CurrentTrain: epoch  8, batch     0 | loss: 4.1092405Losses:  2.519289970397949 0.723151445388794 0.9380042552947998
CurrentTrain: epoch  8, batch     1 | loss: 4.1804457Losses:  2.705244302749634 0.8976166844367981 0.971880316734314
CurrentTrain: epoch  8, batch     2 | loss: 4.5747414Losses:  2.348231315612793 0.30144062638282776 0.933330774307251
CurrentTrain: epoch  8, batch     3 | loss: 3.5830028Losses:  2.222212791442871 0.5132060050964355 0.9840531945228577
CurrentTrain: epoch  9, batch     0 | loss: 3.7194719Losses:  3.134171485900879 0.6742424368858337 0.9385701417922974
CurrentTrain: epoch  9, batch     1 | loss: 4.7469840Losses:  2.36911678314209 0.8284131288528442 0.9427112340927124
CurrentTrain: epoch  9, batch     2 | loss: 4.1402411Losses:  2.4857819080352783 0.2791152596473694 0.912935733795166
CurrentTrain: epoch  9, batch     3 | loss: 3.6778328
Losses:  1.546133279800415 0.9607012271881104 0.8562828302383423
MemoryTrain:  epoch  0, batch     0 | loss: 3.3631172Losses:  1.4221750497817993 0.9788758158683777 0.9631412029266357
MemoryTrain:  epoch  0, batch     1 | loss: 3.3641920Losses:  1.8177931308746338 0.8256910443305969 0.9039545059204102
MemoryTrain:  epoch  1, batch     0 | loss: 3.5474386Losses:  1.0749098062515259 1.0714167356491089 0.9008494019508362
MemoryTrain:  epoch  1, batch     1 | loss: 3.0471759Losses:  1.2436522245407104 1.0331697463989258 0.8836753368377686
MemoryTrain:  epoch  2, batch     0 | loss: 3.1604974Losses:  1.1057554483413696 0.7855204343795776 0.9203830361366272
MemoryTrain:  epoch  2, batch     1 | loss: 2.8116589Losses:  0.6225459575653076 0.8642657995223999 0.9050299525260925
MemoryTrain:  epoch  3, batch     0 | loss: 2.3918417Losses:  1.2533864974975586 1.0380034446716309 0.8950058817863464
MemoryTrain:  epoch  3, batch     1 | loss: 3.1863959Losses:  0.9116309881210327 1.0374369621276855 0.8782049417495728
MemoryTrain:  epoch  4, batch     0 | loss: 2.8272729Losses:  0.5991138815879822 0.714012861251831 0.9211330413818359
MemoryTrain:  epoch  4, batch     1 | loss: 2.2342598Losses:  0.7277541160583496 0.9649099111557007 0.9088624715805054
MemoryTrain:  epoch  5, batch     0 | loss: 2.6015265Losses:  0.5235356092453003 0.7741305828094482 0.8821678757667542
MemoryTrain:  epoch  5, batch     1 | loss: 2.1798341Losses:  0.795508086681366 0.8866986036300659 0.8730884790420532
MemoryTrain:  epoch  6, batch     0 | loss: 2.5552950Losses:  0.09793154895305634 0.7914783358573914 0.9247863292694092
MemoryTrain:  epoch  6, batch     1 | loss: 1.8141962Losses:  0.10859645903110504 0.8993687629699707 0.8870981335639954
MemoryTrain:  epoch  7, batch     0 | loss: 1.8950634Losses:  0.6058639287948608 0.7502234578132629 0.9050498008728027
MemoryTrain:  epoch  7, batch     1 | loss: 2.2611372Losses:  0.38808685541152954 0.6598389148712158 0.882763147354126
MemoryTrain:  epoch  8, batch     0 | loss: 1.9306889Losses:  0.13734041154384613 1.0033156871795654 0.9074639081954956
MemoryTrain:  epoch  8, batch     1 | loss: 2.0481200Losses:  0.22758281230926514 0.6646925210952759 0.9160354137420654
MemoryTrain:  epoch  9, batch     0 | loss: 1.8083107Losses:  0.16760565340518951 0.8680487275123596 0.8707576990127563
MemoryTrain:  epoch  9, batch     1 | loss: 1.9064121
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 75.49%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 74.53%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 73.78%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 73.21%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 68.88%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 67.97%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 67.09%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.12%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.46%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.80%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.73%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 89.75%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 88.46%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 87.31%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 86.10%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 85.29%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 84.60%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.64%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 84.33%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 83.65%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 83.53%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 83.39%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.28%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 82.42%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 82.38%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 81.98%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 81.97%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 82.49%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.68%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 82.98%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 83.79%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 83.70%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 83.43%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 83.29%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 83.20%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 82.70%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 82.45%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 82.10%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 81.87%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.58%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 81.47%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 81.47%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 81.63%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 81.68%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.78%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 81.78%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 81.30%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 80.73%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 80.17%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 79.67%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 79.08%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 78.80%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 78.99%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 79.02%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 79.13%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 79.20%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 79.52%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 79.54%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 79.55%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 79.77%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 79.64%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 79.58%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 79.50%   [EVAL] batch:  143 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 79.57%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 79.59%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 79.56%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 79.57%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 79.54%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 79.59%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 79.52%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 79.45%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 79.35%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 78.98%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 78.80%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 78.62%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 78.40%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 78.30%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 78.20%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 78.03%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 77.86%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 77.77%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 77.56%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 77.19%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 77.07%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 76.73%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 76.43%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 76.09%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 75.79%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 75.50%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 75.29%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 76.65%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 76.56%   
cur_acc:  ['0.9435', '0.6984', '0.7212']
his_acc:  ['0.9435', '0.8215', '0.7656']
Clustering into  19  clusters
Clusters:  [ 2  9 13  2  2  2 17  2 18  0 14  2 11  2 12 16 15  2  2  2  2  2  2 10
  2  0  2  7  5  8  2  3  2  2  4  2  2  2  1  6]
Losses:  7.408214092254639 1.6120944023132324 0.9867962598800659
CurrentTrain: epoch  0, batch     0 | loss: 10.0071049Losses:  5.860945701599121 1.6316629648208618 0.9677065014839172
CurrentTrain: epoch  0, batch     1 | loss: 8.4603148Losses:  6.049302577972412 1.5839545726776123 0.9714833498001099
CurrentTrain: epoch  0, batch     2 | loss: 8.6047401Losses:  7.751003742218018 0.7337998747825623 0.9870160818099976
CurrentTrain: epoch  0, batch     3 | loss: 9.4718189Losses:  5.75039005279541 1.7197329998016357 0.9717422723770142
CurrentTrain: epoch  1, batch     0 | loss: 8.4418659Losses:  5.636209487915039 1.6101469993591309 0.9643639922142029
CurrentTrain: epoch  1, batch     1 | loss: 8.2107201Losses:  4.317898273468018 1.47527277469635 0.9664114713668823
CurrentTrain: epoch  1, batch     2 | loss: 6.7595825Losses:  6.494714260101318 0.2680166959762573 0.9838826060295105
CurrentTrain: epoch  1, batch     3 | loss: 7.7466135Losses:  5.2754926681518555 1.4536617994308472 0.9629157781600952
CurrentTrain: epoch  2, batch     0 | loss: 7.6920705Losses:  4.583147048950195 1.314639925956726 0.9884920716285706
CurrentTrain: epoch  2, batch     1 | loss: 6.8862791Losses:  3.991748332977295 1.2253973484039307 0.9417214393615723
CurrentTrain: epoch  2, batch     2 | loss: 6.1588674Losses:  5.124920845031738 0.5781104564666748 0.9959729909896851
CurrentTrain: epoch  2, batch     3 | loss: 6.6990047Losses:  4.046472549438477 1.4479894638061523 0.9645723104476929
CurrentTrain: epoch  3, batch     0 | loss: 6.4590344Losses:  4.501652717590332 1.423259973526001 0.9623443484306335
CurrentTrain: epoch  3, batch     1 | loss: 6.8872566Losses:  4.575802803039551 1.4439328908920288 0.9613229036331177
CurrentTrain: epoch  3, batch     2 | loss: 6.9810586Losses:  5.000316619873047 0.5377696752548218 0.9994919300079346
CurrentTrain: epoch  3, batch     3 | loss: 6.5375786Losses:  3.6977310180664062 1.179244041442871 0.9499952793121338
CurrentTrain: epoch  4, batch     0 | loss: 5.8269701Losses:  4.518895149230957 1.3873897790908813 0.9764864444732666
CurrentTrain: epoch  4, batch     1 | loss: 6.8827715Losses:  4.054882526397705 1.2686145305633545 0.9630306363105774
CurrentTrain: epoch  4, batch     2 | loss: 6.2865276Losses:  5.322422027587891 0.5033541917800903 0.9690990447998047
CurrentTrain: epoch  4, batch     3 | loss: 6.7948751Losses:  3.663649082183838 1.2144283056259155 0.9645276665687561
CurrentTrain: epoch  5, batch     0 | loss: 5.8426051Losses:  3.984496593475342 1.1634794473648071 0.9591305255889893
CurrentTrain: epoch  5, batch     1 | loss: 6.1071062Losses:  4.548871994018555 1.5071451663970947 0.9540727734565735
CurrentTrain: epoch  5, batch     2 | loss: 7.0100899Losses:  2.573902130126953 0.1274624764919281 0.9957056045532227
CurrentTrain: epoch  5, batch     3 | loss: 3.6970701Losses:  4.713212966918945 1.2724864482879639 0.9486421346664429
CurrentTrain: epoch  6, batch     0 | loss: 6.9343419Losses:  3.3705835342407227 0.9983043670654297 0.959315299987793
CurrentTrain: epoch  6, batch     1 | loss: 5.3282032Losses:  3.059816837310791 1.1159908771514893 0.96063232421875
CurrentTrain: epoch  6, batch     2 | loss: 5.1364403Losses:  4.94638204574585 0.6943910121917725 0.9653466939926147
CurrentTrain: epoch  6, batch     3 | loss: 6.6061196Losses:  3.7078328132629395 1.1683839559555054 0.9552085399627686
CurrentTrain: epoch  7, batch     0 | loss: 5.8314257Losses:  4.043040752410889 1.1518452167510986 0.9565630555152893
CurrentTrain: epoch  7, batch     1 | loss: 6.1514492Losses:  2.7628097534179688 1.1773909330368042 0.9572632312774658
CurrentTrain: epoch  7, batch     2 | loss: 4.8974638Losses:  2.931298017501831 0.16380862891674042 0.9091542959213257
CurrentTrain: epoch  7, batch     3 | loss: 4.0042610Losses:  3.0293095111846924 0.9222079515457153 0.9547584056854248
CurrentTrain: epoch  8, batch     0 | loss: 4.9062757Losses:  3.7903013229370117 1.2217668294906616 0.9596840143203735
CurrentTrain: epoch  8, batch     1 | loss: 5.9717522Losses:  2.903745174407959 1.0964237451553345 0.9578784704208374
CurrentTrain: epoch  8, batch     2 | loss: 4.9580474Losses:  2.2387466430664062 5.960464477539063e-08 0.8900920152664185
CurrentTrain: epoch  8, batch     3 | loss: 3.1288385Losses:  2.1898303031921387 0.7186409831047058 0.9490798711776733
CurrentTrain: epoch  9, batch     0 | loss: 3.8575511Losses:  3.3614797592163086 1.0106960535049438 0.9631046056747437
CurrentTrain: epoch  9, batch     1 | loss: 5.3352804Losses:  3.5311644077301025 1.0651583671569824 0.9340294003486633
CurrentTrain: epoch  9, batch     2 | loss: 5.5303526Losses:  2.1169309616088867 0.041627272963523865 0.9147992730140686
CurrentTrain: epoch  9, batch     3 | loss: 3.0733573
Losses:  0.3230327367782593 0.7653193473815918 0.8623809814453125
MemoryTrain:  epoch  0, batch     0 | loss: 1.9507331Losses:  0.7899215817451477 1.4221047163009644 0.9440499544143677
MemoryTrain:  epoch  0, batch     1 | loss: 3.1560764Losses:  0.38827717304229736 0.6069672107696533 0.9671255350112915
MemoryTrain:  epoch  0, batch     2 | loss: 1.9623699Losses:  1.3277326822280884 1.215879201889038 0.9258408546447754
MemoryTrain:  epoch  1, batch     0 | loss: 3.4694529Losses:  0.5259765386581421 0.8117513656616211 0.8760574460029602
MemoryTrain:  epoch  1, batch     1 | loss: 2.2137854Losses:  1.021695852279663 0.8626455068588257 0.9647814035415649
MemoryTrain:  epoch  1, batch     2 | loss: 2.8491228Losses:  0.23632800579071045 0.8885116577148438 0.9128032326698303
MemoryTrain:  epoch  2, batch     0 | loss: 2.0376430Losses:  0.6181385517120361 0.9006116986274719 0.9038492441177368
MemoryTrain:  epoch  2, batch     1 | loss: 2.4225993Losses:  0.2876187264919281 1.1647772789001465 0.9307750463485718
MemoryTrain:  epoch  2, batch     2 | loss: 2.3831711Losses:  0.46186375617980957 0.963162899017334 0.9181658625602722
MemoryTrain:  epoch  3, batch     0 | loss: 2.3431926Losses:  0.40000349283218384 1.0934988260269165 0.9435454607009888
MemoryTrain:  epoch  3, batch     1 | loss: 2.4370480Losses:  0.228340744972229 0.5510028600692749 0.8399754762649536
MemoryTrain:  epoch  3, batch     2 | loss: 1.6193191Losses:  0.18648555874824524 0.9014831781387329 0.9205372333526611
MemoryTrain:  epoch  4, batch     0 | loss: 2.0085058Losses:  0.2844673693180084 1.0561286211013794 0.9036125540733337
MemoryTrain:  epoch  4, batch     1 | loss: 2.2442086Losses:  0.14138205349445343 0.6423206329345703 0.9193434715270996
MemoryTrain:  epoch  4, batch     2 | loss: 1.7030461Losses:  0.22074095904827118 1.226003885269165 0.9329497218132019
MemoryTrain:  epoch  5, batch     0 | loss: 2.3796945Losses:  0.3979444205760956 0.8093803524971008 0.8980398178100586
MemoryTrain:  epoch  5, batch     1 | loss: 2.1053646Losses:  0.06473459303379059 0.5870508551597595 0.8953061699867249
MemoryTrain:  epoch  5, batch     2 | loss: 1.5470916Losses:  0.10918650031089783 1.1414589881896973 0.9292044639587402
MemoryTrain:  epoch  6, batch     0 | loss: 2.1798501Losses:  0.1754131317138672 1.0270441770553589 0.9014371633529663
MemoryTrain:  epoch  6, batch     1 | loss: 2.1038945Losses:  0.09337034821510315 0.30445146560668945 0.895172119140625
MemoryTrain:  epoch  6, batch     2 | loss: 1.2929939Losses:  0.19321592152118683 0.892214834690094 0.8903408646583557
MemoryTrain:  epoch  7, batch     0 | loss: 1.9757717Losses:  0.13664786517620087 1.0720348358154297 0.9184395670890808
MemoryTrain:  epoch  7, batch     1 | loss: 2.1271222Losses:  0.12945812940597534 0.46524778008461 0.9288351535797119
MemoryTrain:  epoch  7, batch     2 | loss: 1.5235411Losses:  0.12081925570964813 0.9256442189216614 0.9310633540153503
MemoryTrain:  epoch  8, batch     0 | loss: 1.9775269Losses:  0.03681512176990509 0.9961037635803223 0.9122986793518066
MemoryTrain:  epoch  8, batch     1 | loss: 1.9452176Losses:  0.0850260853767395 0.3882708251476288 0.8596000671386719
MemoryTrain:  epoch  8, batch     2 | loss: 1.3328969Losses:  0.040447015315294266 0.9756714105606079 0.9352321028709412
MemoryTrain:  epoch  9, batch     0 | loss: 1.9513505Losses:  0.0771830677986145 0.9620294570922852 0.888399064540863
MemoryTrain:  epoch  9, batch     1 | loss: 1.9276116Losses:  0.040767114609479904 0.3709064722061157 0.8928465843200684
MemoryTrain:  epoch  9, batch     2 | loss: 1.3045201
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 44.85%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 44.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.40%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 51.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 56.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 69.48%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 70.80%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 70.26%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 70.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.09%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.14%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.19%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 91.49%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 90.10%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 89.55%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 88.39%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 87.21%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 85.96%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 84.85%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 83.68%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 82.54%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.70%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.51%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 81.16%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 80.65%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 80.41%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 80.35%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.28%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 80.38%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.92%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 80.20%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 79.99%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 79.78%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 79.80%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 79.74%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 79.83%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 81.87%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 81.86%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.61%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 81.55%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 81.60%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 81.13%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.56%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.99%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.32%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 78.89%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 78.24%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 78.10%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 78.34%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 78.41%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 77.76%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 77.12%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 76.49%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 75.86%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 75.25%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 74.65%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 74.51%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 74.32%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 74.08%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 74.00%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 74.87%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 74.78%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.82%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 74.65%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 74.74%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 74.74%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 74.79%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 74.96%   [EVAL] batch:  155 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 74.80%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 74.60%   [EVAL] batch:  158 | acc: 31.25%,  total acc: 74.33%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 74.10%   [EVAL] batch:  160 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:  161 | acc: 31.25%,  total acc: 73.65%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 73.50%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 73.40%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.33%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 73.19%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 73.09%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.03%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 72.93%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.65%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 72.37%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.06%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 71.78%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 71.36%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 72.34%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 72.41%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.46%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 72.31%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 72.12%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 71.63%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 71.32%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 71.05%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 70.81%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 70.90%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 71.10%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:  200 | acc: 25.00%,  total acc: 71.18%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 70.85%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 70.57%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 70.31%   [EVAL] batch:  204 | acc: 6.25%,  total acc: 70.00%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 69.81%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 70.63%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.90%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 72.00%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 71.92%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 72.26%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 72.29%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 72.13%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 71.93%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 71.91%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.95%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 72.00%   
cur_acc:  ['0.9435', '0.6984', '0.7212', '0.7014']
his_acc:  ['0.9435', '0.8215', '0.7656', '0.7200']
Clustering into  24  clusters
Clusters:  [ 3 23 16  4  3  3 22  3 18  1 17  3 14  3 15 21  8  3  3  3  3  3  3 12
  3  1  3 19 13  0  3 20  3  3  0  3  3  3 11  9  4 10  3  3  5  6  2  3
  3  7]
Losses:  4.319685935974121 1.3714152574539185 0.9795652627944946
CurrentTrain: epoch  0, batch     0 | loss: 6.6706662Losses:  4.791965484619141 1.9347617626190186 1.0089094638824463
CurrentTrain: epoch  0, batch     1 | loss: 7.7356367Losses:  5.061507225036621 1.4937984943389893 0.9980731010437012
CurrentTrain: epoch  0, batch     2 | loss: 7.5533786Losses:  8.058648109436035 1.0242481231689453 0.97857666015625
CurrentTrain: epoch  0, batch     3 | loss: 10.0614729Losses:  3.8048624992370605 1.44039785861969 1.001598834991455
CurrentTrain: epoch  1, batch     0 | loss: 6.2468591Losses:  3.954885482788086 1.543089509010315 0.9824182987213135
CurrentTrain: epoch  1, batch     1 | loss: 6.4803934Losses:  4.130062580108643 1.253404974937439 0.9810468554496765
CurrentTrain: epoch  1, batch     2 | loss: 6.3645144Losses:  2.0805346965789795 0.5299020409584045 0.9614138007164001
CurrentTrain: epoch  1, batch     3 | loss: 3.5718505Losses:  3.441328287124634 1.0896098613739014 1.012362003326416
CurrentTrain: epoch  2, batch     0 | loss: 5.5433002Losses:  3.708730697631836 1.2852976322174072 0.9760630130767822
CurrentTrain: epoch  2, batch     1 | loss: 5.9700909Losses:  3.7218048572540283 1.1567620038986206 0.9651519060134888
CurrentTrain: epoch  2, batch     2 | loss: 5.8437185Losses:  2.4114413261413574 0.05766698718070984 0.9922540187835693
CurrentTrain: epoch  2, batch     3 | loss: 3.4613624Losses:  3.93481183052063 1.3819398880004883 0.9877612590789795
CurrentTrain: epoch  3, batch     0 | loss: 6.3045130Losses:  2.937349557876587 1.240683913230896 0.9698233604431152
CurrentTrain: epoch  3, batch     1 | loss: 5.1478567Losses:  2.876462936401367 1.2572243213653564 0.9828181266784668
CurrentTrain: epoch  3, batch     2 | loss: 5.1165051Losses:  1.9610401391983032 0.10538136214017868 0.9851263761520386
CurrentTrain: epoch  3, batch     3 | loss: 3.0515480Losses:  2.308415174484253 0.7949278354644775 0.9831125140190125
CurrentTrain: epoch  4, batch     0 | loss: 4.0864553Losses:  2.8486862182617188 1.1272025108337402 0.9627365469932556
CurrentTrain: epoch  4, batch     1 | loss: 4.9386253Losses:  2.956484317779541 1.1768333911895752 0.9951152205467224
CurrentTrain: epoch  4, batch     2 | loss: 5.1284332Losses:  5.454651832580566 8.94069742685133e-08 0.8366664052009583
CurrentTrain: epoch  4, batch     3 | loss: 6.2913184Losses:  2.7054290771484375 1.1902499198913574 0.976380467414856
CurrentTrain: epoch  5, batch     0 | loss: 4.8720593Losses:  2.8986973762512207 1.0166242122650146 0.9672331809997559
CurrentTrain: epoch  5, batch     1 | loss: 4.8825550Losses:  2.4077606201171875 0.9145568609237671 0.9662367105484009
CurrentTrain: epoch  5, batch     2 | loss: 4.2885542Losses:  2.1527223587036133 0.4522727429866791 0.9972086548805237
CurrentTrain: epoch  5, batch     3 | loss: 3.6022036Losses:  2.885105609893799 0.9460189342498779 0.9753678441047668
CurrentTrain: epoch  6, batch     0 | loss: 4.8064923Losses:  2.3225808143615723 1.012682318687439 0.9491747617721558
CurrentTrain: epoch  6, batch     1 | loss: 4.2844381Losses:  2.1447014808654785 0.8344599008560181 0.9711371660232544
CurrentTrain: epoch  6, batch     2 | loss: 3.9502983Losses:  2.0580992698669434 0.31177809834480286 0.9573060870170593
CurrentTrain: epoch  6, batch     3 | loss: 3.3271835Losses:  2.2490782737731934 1.008411169052124 0.9524033069610596
CurrentTrain: epoch  7, batch     0 | loss: 4.2098927Losses:  2.418060302734375 1.1677234172821045 0.9633504152297974
CurrentTrain: epoch  7, batch     1 | loss: 4.5491343Losses:  2.242159605026245 0.8418080806732178 0.9714788198471069
CurrentTrain: epoch  7, batch     2 | loss: 4.0554466Losses:  2.9448490142822266 0.09281280636787415 1.0
CurrentTrain: epoch  7, batch     3 | loss: 4.0376616Losses:  2.375305652618408 1.0364071130752563 0.9692598581314087
CurrentTrain: epoch  8, batch     0 | loss: 4.3809724Losses:  2.1262922286987305 0.8039253354072571 0.9621894359588623
CurrentTrain: epoch  8, batch     1 | loss: 3.8924069Losses:  1.987620234489441 0.940340518951416 0.9514331221580505
CurrentTrain: epoch  8, batch     2 | loss: 3.8793941Losses:  1.8218567371368408 0.1890171468257904 0.9538538455963135
CurrentTrain: epoch  8, batch     3 | loss: 2.9647276Losses:  2.167978525161743 0.7914409637451172 0.958627462387085
CurrentTrain: epoch  9, batch     0 | loss: 3.9180470Losses:  2.378420352935791 0.9720679521560669 0.9577956199645996
CurrentTrain: epoch  9, batch     1 | loss: 4.3082838Losses:  2.0505452156066895 0.8677274584770203 0.951706051826477
CurrentTrain: epoch  9, batch     2 | loss: 3.8699789Losses:  1.968482255935669 0.05337533354759216 0.9770697355270386
CurrentTrain: epoch  9, batch     3 | loss: 2.9989271
Losses:  0.6430497169494629 1.0134212970733643 0.916833221912384
MemoryTrain:  epoch  0, batch     0 | loss: 2.5733042Losses:  0.927679181098938 1.0181281566619873 0.9449213743209839
MemoryTrain:  epoch  0, batch     1 | loss: 2.8907287Losses:  0.5851576328277588 1.1796913146972656 0.8958487510681152
MemoryTrain:  epoch  0, batch     2 | loss: 2.6606977Losses:  0.006798198912292719 0.02478497289121151 1.0
MemoryTrain:  epoch  0, batch     3 | loss: 1.0315832Losses:  0.6780790090560913 0.8353554010391235 0.9381445646286011
MemoryTrain:  epoch  1, batch     0 | loss: 2.4515791Losses:  1.094345211982727 0.9227566719055176 0.8928041458129883
MemoryTrain:  epoch  1, batch     1 | loss: 2.9099059Losses:  0.700999915599823 1.0696618556976318 0.9288781881332397
MemoryTrain:  epoch  1, batch     2 | loss: 2.6995401Losses:  0.16197767853736877 0.333412230014801 0.9081636071205139
MemoryTrain:  epoch  1, batch     3 | loss: 1.4035535Losses:  0.4444872736930847 0.8526498675346375 0.9481578469276428
MemoryTrain:  epoch  2, batch     0 | loss: 2.2452950Losses:  0.5886965990066528 1.0319706201553345 0.9253500699996948
MemoryTrain:  epoch  2, batch     1 | loss: 2.5460172Losses:  0.1586645096540451 0.963555634021759 0.8806158304214478
MemoryTrain:  epoch  2, batch     2 | loss: 2.0028360Losses:  0.4331735074520111 0.05161827802658081 0.8936091661453247
MemoryTrain:  epoch  2, batch     3 | loss: 1.3784009Losses:  0.33138567209243774 0.8897413015365601 0.9416640996932983
MemoryTrain:  epoch  3, batch     0 | loss: 2.1627913Losses:  0.2527303397655487 0.8270250558853149 0.8953006267547607
MemoryTrain:  epoch  3, batch     1 | loss: 1.9750561Losses:  0.2239225208759308 0.8930666446685791 0.9160188436508179
MemoryTrain:  epoch  3, batch     2 | loss: 2.0330081Losses:  0.6094634532928467 0.36266690492630005 0.874085009098053
MemoryTrain:  epoch  3, batch     3 | loss: 1.8462154Losses:  0.07421593368053436 0.7836594581604004 0.8674553632736206
MemoryTrain:  epoch  4, batch     0 | loss: 1.7253308Losses:  0.16896501183509827 1.0639421939849854 0.9552088975906372
MemoryTrain:  epoch  4, batch     1 | loss: 2.1881161Losses:  0.3286569118499756 0.9758760929107666 0.9292085766792297
MemoryTrain:  epoch  4, batch     2 | loss: 2.2337415Losses:  0.030630044639110565 0.014072814956307411 0.8975546956062317
MemoryTrain:  epoch  4, batch     3 | loss: 0.9422575Losses:  0.08798244595527649 1.0675040483474731 0.9349277019500732
MemoryTrain:  epoch  5, batch     0 | loss: 2.0904140Losses:  0.11055593192577362 0.8923032283782959 0.9136383533477783
MemoryTrain:  epoch  5, batch     1 | loss: 1.9164975Losses:  0.07515541464090347 0.6991177201271057 0.8869157433509827
MemoryTrain:  epoch  5, batch     2 | loss: 1.6611888Losses:  0.1350645273923874 0.03096437081694603 0.9810867309570312
MemoryTrain:  epoch  5, batch     3 | loss: 1.1471156Losses:  0.07641586661338806 0.738470196723938 0.9335519671440125
MemoryTrain:  epoch  6, batch     0 | loss: 1.7484381Losses:  0.04778986796736717 0.9137944579124451 0.8984463214874268
MemoryTrain:  epoch  6, batch     1 | loss: 1.8600307Losses:  0.058278948068618774 1.0353870391845703 0.9250296950340271
MemoryTrain:  epoch  6, batch     2 | loss: 2.0186956Losses:  0.05890506133437157 0.04612007364630699 0.7283964157104492
MemoryTrain:  epoch  6, batch     3 | loss: 0.8334215Losses:  0.11648357659578323 1.1312646865844727 0.8998542428016663
MemoryTrain:  epoch  7, batch     0 | loss: 2.1476026Losses:  0.14638011157512665 0.8648952841758728 0.899187445640564
MemoryTrain:  epoch  7, batch     1 | loss: 1.9104629Losses:  0.04159882664680481 0.5536444783210754 0.9300673007965088
MemoryTrain:  epoch  7, batch     2 | loss: 1.5253106Losses:  0.059777457267045975 0.09382553398609161 0.9392788410186768
MemoryTrain:  epoch  7, batch     3 | loss: 1.0928818Losses:  0.04568711295723915 0.777168333530426 0.8887262344360352
MemoryTrain:  epoch  8, batch     0 | loss: 1.7115817Losses:  0.06695827096700668 0.7124992609024048 0.9279194474220276
MemoryTrain:  epoch  8, batch     1 | loss: 1.7073770Losses:  0.2119046151638031 0.8333191275596619 0.9146036505699158
MemoryTrain:  epoch  8, batch     2 | loss: 1.9598274Losses:  0.08396334946155548 0.3163057863712311 0.8993808627128601
MemoryTrain:  epoch  8, batch     3 | loss: 1.2996500Losses:  0.04507423937320709 0.7304651737213135 0.9341350793838501
MemoryTrain:  epoch  9, batch     0 | loss: 1.7096745Losses:  0.056779708713293076 1.0519720315933228 0.9057220220565796
MemoryTrain:  epoch  9, batch     1 | loss: 2.0144739Losses:  0.050891317427158356 0.749266505241394 0.8772216439247131
MemoryTrain:  epoch  9, batch     2 | loss: 1.6773795Losses:  0.02798883616924286 0.037012502551078796 0.9748411178588867
MemoryTrain:  epoch  9, batch     3 | loss: 1.0398425
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 77.86%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.84%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 82.11%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.43%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 82.41%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 82.46%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 82.44%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.34%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.08%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.89%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 89.91%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 89.22%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 87.80%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 87.30%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 86.43%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 85.77%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 85.23%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 84.33%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 83.64%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 83.15%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 82.75%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 82.29%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 81.85%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 81.50%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 81.50%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 81.41%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 81.33%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 81.17%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 81.33%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 81.40%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 81.10%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 80.80%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 80.23%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 79.96%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 80.04%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.12%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 82.05%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 81.92%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 81.61%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.49%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 81.43%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 80.90%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.32%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 79.70%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.03%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 78.60%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 77.96%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.77%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.85%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.31%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 77.66%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 77.01%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 76.38%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 75.76%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 75.15%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 74.55%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 74.55%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 74.46%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 74.32%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 74.08%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 74.00%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 74.91%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 74.73%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 74.69%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 74.74%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 74.74%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 74.62%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 74.54%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 74.50%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 74.59%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 74.76%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 74.56%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 74.41%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 74.17%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 73.95%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 73.80%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 73.43%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.36%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 73.19%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 73.09%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.03%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 72.93%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.65%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 72.33%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.02%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 71.68%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 71.41%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 71.14%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 72.20%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 72.25%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 72.30%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 72.28%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 72.21%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 71.86%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 71.55%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 71.24%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 70.96%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 70.66%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 70.43%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 70.51%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 70.63%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.03%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 70.51%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 70.23%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 70.01%   [EVAL] batch:  204 | acc: 6.25%,  total acc: 69.70%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 69.48%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 71.68%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 71.55%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.81%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 71.90%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 71.98%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 71.86%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 71.82%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 71.59%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 71.38%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 71.37%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 71.28%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  253 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 72.30%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 72.28%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 72.19%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 71.96%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  272 | acc: 12.50%,  total acc: 71.63%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 71.56%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 71.39%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 72.11%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:  285 | acc: 56.25%,  total acc: 72.03%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 72.02%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 73.11%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 73.25%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 73.35%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 73.42%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 73.60%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 73.50%   
cur_acc:  ['0.9435', '0.6984', '0.7212', '0.7014', '0.8234']
his_acc:  ['0.9435', '0.8215', '0.7656', '0.7200', '0.7350']
Clustering into  29  clusters
Clusters:  [ 0 27 18  0  0  0 23  0 21 28 14  0 15  0 16 20 19  0  0  0  0  0  0 25
  0  9  0 24  7 26  0 22  0 13 17  0  0  0 10  4  8 11  0  0 12  6  5  0
  0  2  3  0  0  0  0  1  0  0  0  0]
Losses:  6.230437755584717 1.634997844696045 0.9613450765609741
CurrentTrain: epoch  0, batch     0 | loss: 8.8267803Losses:  6.402693748474121 1.9867268800735474 0.9780880212783813
CurrentTrain: epoch  0, batch     1 | loss: 9.3675089Losses:  6.715384483337402 1.7470511198043823 0.9654830694198608
CurrentTrain: epoch  0, batch     2 | loss: 9.4279184Losses:  8.032755851745605 0.3561352789402008 0.9737757444381714
CurrentTrain: epoch  0, batch     3 | loss: 9.3626671Losses:  5.894488334655762 1.7051182985305786 0.9848136305809021
CurrentTrain: epoch  1, batch     0 | loss: 8.5844202Losses:  5.587198257446289 1.5787110328674316 0.9589436650276184
CurrentTrain: epoch  1, batch     1 | loss: 8.1248531Losses:  4.822747230529785 1.4494131803512573 0.9359772801399231
CurrentTrain: epoch  1, batch     2 | loss: 7.2081380Losses:  4.662696838378906 0.6257949471473694 0.9916805624961853
CurrentTrain: epoch  1, batch     3 | loss: 6.2801723Losses:  4.778520107269287 1.7381460666656494 0.9520944952964783
CurrentTrain: epoch  2, batch     0 | loss: 7.4687610Losses:  4.534677505493164 1.466881513595581 0.9484658241271973
CurrentTrain: epoch  2, batch     1 | loss: 6.9500251Losses:  4.737545013427734 1.7132188081741333 0.9652494788169861
CurrentTrain: epoch  2, batch     2 | loss: 7.4160132Losses:  6.812161445617676 0.8864912986755371 1.0120460987091064
CurrentTrain: epoch  2, batch     3 | loss: 8.7106991Losses:  4.558795928955078 1.59018874168396 0.9637852907180786
CurrentTrain: epoch  3, batch     0 | loss: 7.1127701Losses:  3.7389636039733887 1.5009734630584717 0.949662446975708
CurrentTrain: epoch  3, batch     1 | loss: 6.1895990Losses:  4.988116264343262 1.3922779560089111 0.937650203704834
CurrentTrain: epoch  3, batch     2 | loss: 7.3180442Losses:  4.884312629699707 0.4051169455051422 0.9644740223884583
CurrentTrain: epoch  3, batch     3 | loss: 6.2539039Losses:  4.085630416870117 1.5095229148864746 0.9456602334976196
CurrentTrain: epoch  4, batch     0 | loss: 6.5408134Losses:  4.254579544067383 1.4189972877502441 0.9313430786132812
CurrentTrain: epoch  4, batch     1 | loss: 6.6049199Losses:  3.905754566192627 1.3152990341186523 0.9591588377952576
CurrentTrain: epoch  4, batch     2 | loss: 6.1802125Losses:  3.273970127105713 0.274139940738678 0.9199399948120117
CurrentTrain: epoch  4, batch     3 | loss: 4.4680500Losses:  3.761855363845825 1.436946153640747 0.9498386383056641
CurrentTrain: epoch  5, batch     0 | loss: 6.1486402Losses:  3.7939372062683105 1.2644314765930176 0.9220752120018005
CurrentTrain: epoch  5, batch     1 | loss: 5.9804440Losses:  3.7376644611358643 1.4806662797927856 0.9536281824111938
CurrentTrain: epoch  5, batch     2 | loss: 6.1719589Losses:  5.744071006774902 0.47126442193984985 0.9411553144454956
CurrentTrain: epoch  5, batch     3 | loss: 7.1564908Losses:  4.000876426696777 1.426446795463562 0.932380199432373
CurrentTrain: epoch  6, batch     0 | loss: 6.3597035Losses:  3.264878034591675 1.38557767868042 0.9432352781295776
CurrentTrain: epoch  6, batch     1 | loss: 5.5936909Losses:  3.3731327056884766 1.222167730331421 0.940941333770752
CurrentTrain: epoch  6, batch     2 | loss: 5.5362420Losses:  3.78033447265625 0.5019468665122986 0.9513958692550659
CurrentTrain: epoch  6, batch     3 | loss: 5.2336774Losses:  3.4178361892700195 1.032339334487915 0.9159810543060303
CurrentTrain: epoch  7, batch     0 | loss: 5.3661566Losses:  2.9942588806152344 1.1325268745422363 0.9335687160491943
CurrentTrain: epoch  7, batch     1 | loss: 5.0603542Losses:  3.290038585662842 1.3411037921905518 0.9485910534858704
CurrentTrain: epoch  7, batch     2 | loss: 5.5797338Losses:  3.086141586303711 0.27985680103302 0.8804103136062622
CurrentTrain: epoch  7, batch     3 | loss: 4.2464085Losses:  3.851724147796631 1.327864646911621 0.9287065863609314
CurrentTrain: epoch  8, batch     0 | loss: 6.1082954Losses:  2.8406319618225098 1.0286412239074707 0.9116851687431335
CurrentTrain: epoch  8, batch     1 | loss: 4.7809582Losses:  2.5820415019989014 1.1294833421707153 0.9432836771011353
CurrentTrain: epoch  8, batch     2 | loss: 4.6548085Losses:  2.5105268955230713 0.42226338386535645 0.9211257100105286
CurrentTrain: epoch  8, batch     3 | loss: 3.8539159Losses:  3.168410301208496 1.0791900157928467 0.9079849123954773
CurrentTrain: epoch  9, batch     0 | loss: 5.1555853Losses:  3.0950865745544434 1.2765295505523682 0.9267921447753906
CurrentTrain: epoch  9, batch     1 | loss: 5.2984085Losses:  2.3430228233337402 1.0824317932128906 0.9319875240325928
CurrentTrain: epoch  9, batch     2 | loss: 4.3574419Losses:  2.710921287536621 0.5794985294342041 0.9349405765533447
CurrentTrain: epoch  9, batch     3 | loss: 4.2253604
Losses:  0.17014455795288086 1.0756438970565796 0.8919683694839478
MemoryTrain:  epoch  0, batch     0 | loss: 2.1377568Losses:  0.6469095349311829 1.0174205303192139 0.9321798086166382
MemoryTrain:  epoch  0, batch     1 | loss: 2.5965099Losses:  0.533431351184845 0.9984717965126038 0.9084334373474121
MemoryTrain:  epoch  0, batch     2 | loss: 2.4403367Losses:  0.4120047092437744 0.7923545837402344 0.9267093539237976
MemoryTrain:  epoch  0, batch     3 | loss: 2.1310687Losses:  0.2321612685918808 0.8480315208435059 0.9153856635093689
MemoryTrain:  epoch  1, batch     0 | loss: 1.9955785Losses:  1.2606199979782104 1.1034862995147705 0.9106833934783936
MemoryTrain:  epoch  1, batch     1 | loss: 3.2747896Losses:  0.7315971851348877 0.7638874053955078 0.8928104639053345
MemoryTrain:  epoch  1, batch     2 | loss: 2.3882952Losses:  0.5287898182868958 0.8891793489456177 0.9223637580871582
MemoryTrain:  epoch  1, batch     3 | loss: 2.3403330Losses:  0.44426196813583374 0.9832492470741272 0.9108437895774841
MemoryTrain:  epoch  2, batch     0 | loss: 2.3383551Losses:  0.2045930027961731 0.9584876298904419 0.9169598817825317
MemoryTrain:  epoch  2, batch     1 | loss: 2.0800405Losses:  0.4709799885749817 1.042853832244873 0.9076804518699646
MemoryTrain:  epoch  2, batch     2 | loss: 2.4215143Losses:  0.1043396145105362 0.5544602870941162 0.9017330408096313
MemoryTrain:  epoch  2, batch     3 | loss: 1.5605329Losses:  0.49254554510116577 0.8561593294143677 0.9063616991043091
MemoryTrain:  epoch  3, batch     0 | loss: 2.2550664Losses:  0.36071503162384033 0.9178462028503418 0.8935946226119995
MemoryTrain:  epoch  3, batch     1 | loss: 2.1721559Losses:  0.3263716995716095 0.8201736211776733 0.932074785232544
MemoryTrain:  epoch  3, batch     2 | loss: 2.0786200Losses:  0.15803518891334534 0.953770637512207 0.9085475206375122
MemoryTrain:  epoch  3, batch     3 | loss: 2.0203533Losses:  0.2726154923439026 1.1553544998168945 0.8771575689315796
MemoryTrain:  epoch  4, batch     0 | loss: 2.3051276Losses:  0.09527254849672318 0.7114897966384888 0.9431113004684448
MemoryTrain:  epoch  4, batch     1 | loss: 1.7498736Losses:  0.11567887663841248 0.8087283968925476 0.8895279169082642
MemoryTrain:  epoch  4, batch     2 | loss: 1.8139352Losses:  0.2518009841442108 0.7217588424682617 0.9168667793273926
MemoryTrain:  epoch  4, batch     3 | loss: 1.8904266Losses:  0.1268588751554489 0.9050746560096741 0.9085683226585388
MemoryTrain:  epoch  5, batch     0 | loss: 1.9405019Losses:  0.30258965492248535 1.039154052734375 0.9091987609863281
MemoryTrain:  epoch  5, batch     1 | loss: 2.2509425Losses:  0.0632937103509903 0.7897644639015198 0.8801007270812988
MemoryTrain:  epoch  5, batch     2 | loss: 1.7331588Losses:  0.15628308057785034 0.605271577835083 0.93397057056427
MemoryTrain:  epoch  5, batch     3 | loss: 1.6955252Losses:  0.058987997472286224 0.8028222322463989 0.9302898645401001
MemoryTrain:  epoch  6, batch     0 | loss: 1.7921001Losses:  0.12795889377593994 1.0251861810684204 0.9070945382118225
MemoryTrain:  epoch  6, batch     1 | loss: 2.0602396Losses:  0.0885898619890213 0.803598940372467 0.8787453174591064
MemoryTrain:  epoch  6, batch     2 | loss: 1.7709341Losses:  0.05035429820418358 0.6306722164154053 0.9041792750358582
MemoryTrain:  epoch  6, batch     3 | loss: 1.5852058Losses:  0.1559767872095108 0.810563325881958 0.8652158975601196
MemoryTrain:  epoch  7, batch     0 | loss: 1.8317560Losses:  0.048321302980184555 0.7183098793029785 0.9211429357528687
MemoryTrain:  epoch  7, batch     1 | loss: 1.6877742Losses:  0.07154833525419235 0.9368618726730347 0.9042189121246338
MemoryTrain:  epoch  7, batch     2 | loss: 1.9126291Losses:  0.14984777569770813 0.7637755274772644 0.926217794418335
MemoryTrain:  epoch  7, batch     3 | loss: 1.8398411Losses:  0.06157035753130913 0.8980667591094971 0.9169142842292786
MemoryTrain:  epoch  8, batch     0 | loss: 1.8765514Losses:  0.07018955796957016 0.7100024223327637 0.8606389760971069
MemoryTrain:  epoch  8, batch     1 | loss: 1.6408310Losses:  0.08493499457836151 0.5836732387542725 0.9207051992416382
MemoryTrain:  epoch  8, batch     2 | loss: 1.5893135Losses:  0.12486541271209717 0.8094884753227234 0.9053257703781128
MemoryTrain:  epoch  8, batch     3 | loss: 1.8396797Losses:  0.043929435312747955 0.7502349019050598 0.8990514874458313
MemoryTrain:  epoch  9, batch     0 | loss: 1.6932158Losses:  0.07819648087024689 0.7414332628250122 0.8838810920715332
MemoryTrain:  epoch  9, batch     1 | loss: 1.7035108Losses:  0.1064378023147583 0.9450052380561829 0.915260374546051
MemoryTrain:  epoch  9, batch     2 | loss: 1.9667034Losses:  0.04257258027791977 0.6111716032028198 0.9028284549713135
MemoryTrain:  epoch  9, batch     3 | loss: 1.5565727
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 54.33%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 52.78%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 50.89%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 49.14%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 47.71%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 46.17%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 46.88%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 48.11%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 50.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 51.22%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 52.03%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 52.80%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 53.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 55.95%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 56.55%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 57.41%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 58.24%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 58.83%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 58.91%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 60.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 60.17%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 60.70%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 60.97%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 61.48%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 61.72%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 61.73%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 61.10%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 60.91%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 60.83%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 60.55%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 60.69%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 60.02%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.21%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 88.75%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 88.73%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 88.05%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 87.28%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 86.86%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 86.17%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 85.99%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 85.62%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 84.77%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 84.13%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 83.43%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.65%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 81.99%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 81.52%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 80.82%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 80.39%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 79.98%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 80.18%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 80.05%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 79.98%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 79.77%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 79.80%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 79.59%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 79.32%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 78.97%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 78.63%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 78.23%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 78.34%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.37%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.52%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 80.45%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 80.16%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.18%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 79.73%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 79.17%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 78.67%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 78.07%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 77.70%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 77.06%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.94%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 77.52%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 76.88%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 76.24%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 75.61%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 74.40%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 73.80%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 73.81%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 73.68%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 73.50%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 73.43%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 73.93%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 74.33%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 74.20%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.25%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.25%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 74.27%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 74.27%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 74.16%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 73.92%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 73.69%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 73.54%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 73.27%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 73.08%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 72.89%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 72.78%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 72.60%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 72.42%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 72.15%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 71.97%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 71.95%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 71.89%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 71.73%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.36%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 71.05%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 70.71%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 70.38%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 70.08%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 69.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 71.00%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 71.24%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 71.32%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 71.21%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 70.93%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 70.66%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 70.18%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 70.01%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 69.75%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 70.32%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 70.41%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 70.21%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 69.93%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 69.70%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 69.55%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 69.33%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.10%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 71.14%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 71.35%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 71.34%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 71.64%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 71.72%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 71.60%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 71.46%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 71.32%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 71.23%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 71.12%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 70.95%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 70.89%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 70.75%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 70.69%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 70.85%   [EVAL] batch:  253 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 71.28%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 71.37%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 71.32%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 71.24%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 71.14%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 71.06%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 70.96%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 70.68%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 70.49%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 70.25%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 71.07%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  298 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 72.09%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 72.23%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 72.51%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 72.48%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 72.27%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 72.12%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 71.93%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 71.87%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 71.72%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 71.69%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 71.94%   [EVAL] batch:  325 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  326 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:  327 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  331 | acc: 31.25%,  total acc: 72.20%   [EVAL] batch:  332 | acc: 6.25%,  total acc: 72.00%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 71.84%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  335 | acc: 18.75%,  total acc: 71.58%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 71.42%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 71.09%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 70.90%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 70.69%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 70.49%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 70.30%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 70.17%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 70.40%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 70.70%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  357 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  358 | acc: 56.25%,  total acc: 70.79%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 70.87%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 70.86%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 70.87%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 70.94%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 70.84%   [EVAL] batch:  370 | acc: 43.75%,  total acc: 70.77%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 70.60%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 70.57%   
cur_acc:  ['0.9435', '0.6984', '0.7212', '0.7014', '0.8234', '0.6002']
his_acc:  ['0.9435', '0.8215', '0.7656', '0.7200', '0.7350', '0.7057']
Clustering into  34  clusters
Clusters:  [ 0 23 24  0  0  0 28  0 19 33 26  0 20  0 17 11  9  0  0  0  0  0  0 25
  0 16  0 27 31 32  0 29  0  0 22  0  0  0 12 21 18  5  0  0 13 30 14  0
  0 15 10  0  0  2  2  8  0  0  0  0  4  7  2  0  0  3  0  6  0  1]
Losses:  6.355618476867676 1.5327935218811035 0.9636482000350952
CurrentTrain: epoch  0, batch     0 | loss: 8.8520603Losses:  6.390764236450195 1.6296765804290771 0.9892649054527283
CurrentTrain: epoch  0, batch     1 | loss: 9.0097055Losses:  7.038233280181885 1.6705071926116943 0.9787524342536926
CurrentTrain: epoch  0, batch     2 | loss: 9.6874924Losses:  7.2404351234436035 0.5470176339149475 0.9945322275161743
CurrentTrain: epoch  0, batch     3 | loss: 8.7819853Losses:  6.118498802185059 1.1020927429199219 0.9586945176124573
CurrentTrain: epoch  1, batch     0 | loss: 8.1792860Losses:  5.691556930541992 1.4002878665924072 0.9802641868591309
CurrentTrain: epoch  1, batch     1 | loss: 8.0721092Losses:  4.717893123626709 1.2542284727096558 0.9802433848381042
CurrentTrain: epoch  1, batch     2 | loss: 6.9523649Losses:  5.147226333618164 0.4345218539237976 0.9361845850944519
CurrentTrain: epoch  1, batch     3 | loss: 6.5179324Losses:  4.787181854248047 1.2573310136795044 0.9730291962623596
CurrentTrain: epoch  2, batch     0 | loss: 7.0175419Losses:  4.783059597015381 1.3739514350891113 0.977158784866333
CurrentTrain: epoch  2, batch     1 | loss: 7.1341696Losses:  5.102864742279053 1.275763988494873 0.9462578296661377
CurrentTrain: epoch  2, batch     2 | loss: 7.3248863Losses:  8.151189804077148 0.5600364804267883 0.9808759689331055
CurrentTrain: epoch  2, batch     3 | loss: 9.6921024Losses:  5.109345436096191 1.2884774208068848 0.9595043659210205
CurrentTrain: epoch  3, batch     0 | loss: 7.3573275Losses:  4.030743598937988 1.225511908531189 0.9647670388221741
CurrentTrain: epoch  3, batch     1 | loss: 6.2210226Losses:  4.858386039733887 1.0719317197799683 0.9626009464263916
CurrentTrain: epoch  3, batch     2 | loss: 6.8929186Losses:  2.940990924835205 0.1477050483226776 1.0
CurrentTrain: epoch  3, batch     3 | loss: 4.0886960Losses:  3.8428802490234375 1.1037555932998657 0.9830456376075745
CurrentTrain: epoch  4, batch     0 | loss: 5.9296813Losses:  4.734543800354004 1.2474660873413086 0.9444397687911987
CurrentTrain: epoch  4, batch     1 | loss: 6.9264498Losses:  3.647017478942871 1.0908435583114624 0.9435443878173828
CurrentTrain: epoch  4, batch     2 | loss: 5.6814055Losses:  5.08830451965332 0.2360924929380417 0.997035026550293
CurrentTrain: epoch  4, batch     3 | loss: 6.3214321Losses:  4.4495086669921875 1.231093168258667 0.9476033449172974
CurrentTrain: epoch  5, batch     0 | loss: 6.6282053Losses:  3.7150497436523438 0.9553884267807007 0.946445107460022
CurrentTrain: epoch  5, batch     1 | loss: 5.6168833Losses:  3.896610736846924 1.1197514533996582 0.9670564532279968
CurrentTrain: epoch  5, batch     2 | loss: 5.9834185Losses:  4.610581398010254 0.2913420498371124 0.97416090965271
CurrentTrain: epoch  5, batch     3 | loss: 5.8760843Losses:  3.6558449268341064 1.0199823379516602 0.9457945823669434
CurrentTrain: epoch  6, batch     0 | loss: 5.6216216Losses:  3.239638328552246 1.0657382011413574 0.9594258069992065
CurrentTrain: epoch  6, batch     1 | loss: 5.2648025Losses:  4.130294322967529 1.0900545120239258 0.9354768991470337
CurrentTrain: epoch  6, batch     2 | loss: 6.1558256Losses:  7.083662986755371 0.3944407105445862 0.9723726511001587
CurrentTrain: epoch  6, batch     3 | loss: 8.4504766Losses:  2.988938570022583 0.8353815078735352 0.9337674379348755
CurrentTrain: epoch  7, batch     0 | loss: 4.7580876Losses:  3.4861881732940674 1.1650474071502686 0.9635012149810791
CurrentTrain: epoch  7, batch     1 | loss: 5.6147366Losses:  4.267805099487305 0.9524272084236145 0.9449074268341064
CurrentTrain: epoch  7, batch     2 | loss: 6.1651402Losses:  2.8906149864196777 2.3841860752327193e-07 0.9286540150642395
CurrentTrain: epoch  7, batch     3 | loss: 3.8192692Losses:  3.751645565032959 1.0447392463684082 0.9303791522979736
CurrentTrain: epoch  8, batch     0 | loss: 5.7267637Losses:  3.0952863693237305 0.8281053304672241 0.9832186698913574
CurrentTrain: epoch  8, batch     1 | loss: 4.9066105Losses:  3.6461668014526367 0.9316360354423523 0.925934910774231
CurrentTrain: epoch  8, batch     2 | loss: 5.5037374Losses:  4.054006576538086 0.19504642486572266 0.9771377444267273
CurrentTrain: epoch  8, batch     3 | loss: 5.2261906Losses:  3.528503894805908 0.9952635765075684 0.9555211067199707
CurrentTrain: epoch  9, batch     0 | loss: 5.4792886Losses:  3.5537405014038086 1.0198111534118652 0.9532493352890015
CurrentTrain: epoch  9, batch     1 | loss: 5.5268011Losses:  3.1079893112182617 0.9369842410087585 0.9224777221679688
CurrentTrain: epoch  9, batch     2 | loss: 4.9674511Losses:  2.428859233856201 0.10342958569526672 0.9518529176712036
CurrentTrain: epoch  9, batch     3 | loss: 3.4841418
Losses:  1.4834247827529907 0.8968808650970459 0.89705491065979
MemoryTrain:  epoch  0, batch     0 | loss: 3.2773607Losses:  0.20954079926013947 1.012420654296875 0.8793738484382629
MemoryTrain:  epoch  0, batch     1 | loss: 2.1013353Losses:  0.5207528471946716 0.9385936260223389 0.9258154034614563
MemoryTrain:  epoch  0, batch     2 | loss: 2.3851619Losses:  0.19930218160152435 0.8072198629379272 0.94767165184021
MemoryTrain:  epoch  0, batch     3 | loss: 1.9541937Losses:  0.10127364099025726 0.714483380317688 0.8907544612884521
MemoryTrain:  epoch  0, batch     4 | loss: 1.7065115Losses:  0.8064202070236206 0.742098331451416 0.9055294990539551
MemoryTrain:  epoch  1, batch     0 | loss: 2.4540482Losses:  0.8335429430007935 0.937043309211731 0.8735607862472534
MemoryTrain:  epoch  1, batch     1 | loss: 2.6441469Losses:  1.0444800853729248 1.1314566135406494 0.9060044288635254
MemoryTrain:  epoch  1, batch     2 | loss: 3.0819411Losses:  0.6271237730979919 0.7971446514129639 0.9244428873062134
MemoryTrain:  epoch  1, batch     3 | loss: 2.3487115Losses:  0.8934804201126099 0.3624694049358368 0.9385461211204529
MemoryTrain:  epoch  1, batch     4 | loss: 2.1944959Losses:  0.6108016967773438 0.9593653082847595 0.9502207636833191
MemoryTrain:  epoch  2, batch     0 | loss: 2.5203879Losses:  0.40330982208251953 1.0081087350845337 0.8949322700500488
MemoryTrain:  epoch  2, batch     1 | loss: 2.3063507Losses:  0.6683973670005798 0.7186154127120972 0.8900796175003052
MemoryTrain:  epoch  2, batch     2 | loss: 2.2770925Losses:  0.3143722414970398 0.8478760123252869 0.8938536643981934
MemoryTrain:  epoch  2, batch     3 | loss: 2.0561018Losses:  0.7786387205123901 0.42027291655540466 0.8811761140823364
MemoryTrain:  epoch  2, batch     4 | loss: 2.0800877Losses:  0.6793670654296875 0.8478622436523438 0.9152836799621582
MemoryTrain:  epoch  3, batch     0 | loss: 2.4425130Losses:  0.21157774329185486 0.6121882796287537 0.9358491897583008
MemoryTrain:  epoch  3, batch     1 | loss: 1.7596152Losses:  0.18897214531898499 0.9688528776168823 0.8936707973480225
MemoryTrain:  epoch  3, batch     2 | loss: 2.0514958Losses:  0.2063077837228775 0.9681200981140137 0.860847532749176
MemoryTrain:  epoch  3, batch     3 | loss: 2.0352755Losses:  0.6077253818511963 0.4478414058685303 0.9120609164237976
MemoryTrain:  epoch  3, batch     4 | loss: 1.9676278Losses:  0.13916830718517303 0.804275393486023 0.9188367128372192
MemoryTrain:  epoch  4, batch     0 | loss: 1.8622804Losses:  0.19714179635047913 0.8270680904388428 0.8995428085327148
MemoryTrain:  epoch  4, batch     1 | loss: 1.9237527Losses:  0.564434289932251 0.8633755445480347 0.9040970206260681
MemoryTrain:  epoch  4, batch     2 | loss: 2.3319068Losses:  0.10725067555904388 0.714327335357666 0.8904992341995239
MemoryTrain:  epoch  4, batch     3 | loss: 1.7120773Losses:  0.1565805971622467 0.599223256111145 0.8650476336479187
MemoryTrain:  epoch  4, batch     4 | loss: 1.6208515Losses:  0.14943742752075195 0.748859167098999 0.9107383489608765
MemoryTrain:  epoch  5, batch     0 | loss: 1.8090349Losses:  0.13324813544750214 0.775605320930481 0.8803418278694153
MemoryTrain:  epoch  5, batch     1 | loss: 1.7891953Losses:  0.14430366456508636 0.8143047094345093 0.881600558757782
MemoryTrain:  epoch  5, batch     2 | loss: 1.8402090Losses:  0.11853725463151932 0.901996374130249 0.9329707622528076
MemoryTrain:  epoch  5, batch     3 | loss: 1.9535044Losses:  0.12404891103506088 0.5299724340438843 0.905753493309021
MemoryTrain:  epoch  5, batch     4 | loss: 1.5597749Losses:  0.17608188092708588 0.7096055746078491 0.9353833794593811
MemoryTrain:  epoch  6, batch     0 | loss: 1.8210709Losses:  0.11190634965896606 0.9048269987106323 0.8734434843063354
MemoryTrain:  epoch  6, batch     1 | loss: 1.8901769Losses:  0.16128525137901306 0.9039653539657593 0.89710932970047
MemoryTrain:  epoch  6, batch     2 | loss: 1.9623599Losses:  0.12616375088691711 0.8656823635101318 0.8786237239837646
MemoryTrain:  epoch  6, batch     3 | loss: 1.8704698Losses:  0.10393022000789642 0.3340371251106262 0.9352085590362549
MemoryTrain:  epoch  6, batch     4 | loss: 1.3731759Losses:  0.09056390076875687 0.8487663269042969 0.8653137683868408
MemoryTrain:  epoch  7, batch     0 | loss: 1.8046440Losses:  0.14445793628692627 0.8355897068977356 0.9348663687705994
MemoryTrain:  epoch  7, batch     1 | loss: 1.9149140Losses:  0.08778631687164307 0.7903389930725098 0.8907155990600586
MemoryTrain:  epoch  7, batch     2 | loss: 1.7688409Losses:  0.06260331720113754 0.799895167350769 0.9129101037979126
MemoryTrain:  epoch  7, batch     3 | loss: 1.7754085Losses:  0.1889876127243042 0.3171995282173157 0.8832623958587646
MemoryTrain:  epoch  7, batch     4 | loss: 1.3894496Losses:  0.04907655715942383 0.8833507299423218 0.8962250351905823
MemoryTrain:  epoch  8, batch     0 | loss: 1.8286524Losses:  0.10730314254760742 0.7781131267547607 0.9056515693664551
MemoryTrain:  epoch  8, batch     1 | loss: 1.7910678Losses:  0.1863320767879486 1.037703514099121 0.8669665455818176
MemoryTrain:  epoch  8, batch     2 | loss: 2.0910022Losses:  0.08972685039043427 0.6871761083602905 0.9134140014648438
MemoryTrain:  epoch  8, batch     3 | loss: 1.6903169Losses:  0.045939408242702484 0.2540915012359619 0.9438612461090088
MemoryTrain:  epoch  8, batch     4 | loss: 1.2438922Losses:  0.10924512147903442 0.6945517063140869 0.92255038022995
MemoryTrain:  epoch  9, batch     0 | loss: 1.7263472Losses:  0.0712629184126854 0.6446928977966309 0.8958524465560913
MemoryTrain:  epoch  9, batch     1 | loss: 1.6118083Losses:  0.07391497492790222 0.7548527121543884 0.9045030474662781
MemoryTrain:  epoch  9, batch     2 | loss: 1.7332706Losses:  0.0703611969947815 0.8859710097312927 0.8529281616210938
MemoryTrain:  epoch  9, batch     3 | loss: 1.8092604Losses:  0.06613457947969437 0.5481774806976318 0.9222620725631714
MemoryTrain:  epoch  9, batch     4 | loss: 1.5365741
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 47.57%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 47.04%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 49.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 54.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 58.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.75%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 58.41%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 56.68%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 55.21%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 54.44%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 55.08%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 56.06%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 57.17%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 58.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 59.20%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 60.30%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 60.10%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 60.06%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 59.97%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 59.45%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 59.09%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 58.89%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 58.83%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 58.51%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 58.20%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 57.78%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 57.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 57.60%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 57.33%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 57.43%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 57.64%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 58.07%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 58.15%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 58.55%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 58.94%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 59.58%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 60.04%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 60.38%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 60.12%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 85.29%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.59%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.70%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.79%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.03%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 88.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 88.50%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 87.83%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 86.75%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 86.02%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 85.73%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 85.25%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.98%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 83.79%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 83.27%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 82.86%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 82.09%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 81.62%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 81.07%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 80.81%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 80.47%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 79.97%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.73%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 79.77%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 79.87%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 79.73%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 79.53%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 79.63%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 79.19%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 78.31%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 77.53%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 76.62%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 75.80%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 75.07%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 74.93%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 75.55%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 77.51%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 77.25%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 77.22%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 77.42%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 77.10%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.56%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.03%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 75.40%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 75.06%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 74.44%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 74.17%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 74.84%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 74.22%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 73.61%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 73.00%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 72.41%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 71.82%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 71.25%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 71.28%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 71.31%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 71.03%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 70.99%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 71.99%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 71.92%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 72.09%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 71.98%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 71.81%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 71.69%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 71.50%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 71.52%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 71.55%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 71.33%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 71.14%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 71.04%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 70.87%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 70.65%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 70.49%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 70.30%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 70.26%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 70.21%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 70.24%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 70.16%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 69.89%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 69.59%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 69.26%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 68.93%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 68.64%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 68.36%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 69.48%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 69.24%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.98%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 68.72%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 68.52%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 68.30%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.00%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 68.81%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 68.50%   [EVAL] batch:  202 | acc: 25.00%,  total acc: 68.29%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 68.08%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 67.90%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 67.72%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.24%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 69.86%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 69.77%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 69.76%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 69.59%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 70.06%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 69.97%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 69.78%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 69.54%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 69.13%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 69.08%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 68.92%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  253 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 69.59%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 69.50%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 69.43%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 68.98%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 68.80%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 69.48%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  285 | acc: 50.00%,  total acc: 69.43%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 69.40%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 70.56%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 70.55%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.64%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 70.61%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 70.63%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 70.86%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 70.62%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 70.44%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 70.21%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 70.01%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 69.81%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 69.69%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 69.95%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 69.99%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 70.07%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 69.92%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 69.76%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 69.63%   [EVAL] batch:  334 | acc: 25.00%,  total acc: 69.50%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 69.33%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 69.06%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 68.88%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 68.69%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.49%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 68.29%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.11%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.03%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  356 | acc: 68.75%,  total acc: 68.71%   [EVAL] batch:  357 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  359 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  360 | acc: 43.75%,  total acc: 68.56%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 68.49%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  364 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  369 | acc: 25.00%,  total acc: 68.36%   [EVAL] batch:  370 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 68.20%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  373 | acc: 50.00%,  total acc: 68.08%   [EVAL] batch:  374 | acc: 43.75%,  total acc: 68.02%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 67.85%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 67.71%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 67.59%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 67.46%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 67.35%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 67.23%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  388 | acc: 25.00%,  total acc: 67.46%   [EVAL] batch:  389 | acc: 18.75%,  total acc: 67.34%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 67.20%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 67.39%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 67.30%   [EVAL] batch:  402 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 67.20%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 67.07%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 66.98%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 67.00%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  413 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 67.23%   [EVAL] batch:  416 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:  417 | acc: 37.50%,  total acc: 67.14%   [EVAL] batch:  418 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  419 | acc: 50.00%,  total acc: 67.04%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  422 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:  423 | acc: 37.50%,  total acc: 66.83%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 66.76%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  426 | acc: 43.75%,  total acc: 66.72%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  430 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  431 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.85%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 66.85%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 66.88%   
cur_acc:  ['0.9435', '0.6984', '0.7212', '0.7014', '0.8234', '0.6002', '0.6012']
his_acc:  ['0.9435', '0.8215', '0.7656', '0.7200', '0.7350', '0.7057', '0.6688']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0  0  0  0 12
  0 37  0 23 31 36  2 34  0  0 30  0  0  0 27 11 18 29  0  0 28 32 13  0
  0 17 16  0  0  1  1 20  0  0  0  0 15  9  1  0  0  8  0 14  0  7  5 10
  0  0  0  2  0  3  6  4]
Losses:  6.5468597412109375 1.3448599576950073 0.9851657152175903
CurrentTrain: epoch  0, batch     0 | loss: 8.8768854Losses:  5.380224227905273 1.2416300773620605 0.9816541075706482
CurrentTrain: epoch  0, batch     1 | loss: 7.6035085Losses:  5.553038597106934 1.4254612922668457 0.949385404586792
CurrentTrain: epoch  0, batch     2 | loss: 7.9278851Losses:  5.479622840881348 0.1334143579006195 0.9743881225585938
CurrentTrain: epoch  0, batch     3 | loss: 6.5874252Losses:  4.717451095581055 1.1044952869415283 0.9787204265594482
CurrentTrain: epoch  1, batch     0 | loss: 6.8006668Losses:  4.86830997467041 1.2417243719100952 0.9705129861831665
CurrentTrain: epoch  1, batch     1 | loss: 7.0805473Losses:  4.699314594268799 1.3373751640319824 0.960601806640625
CurrentTrain: epoch  1, batch     2 | loss: 6.9972916Losses:  4.1803364753723145 0.13648296892642975 1.0012606382369995
CurrentTrain: epoch  1, batch     3 | loss: 5.3180804Losses:  5.516026020050049 1.1486904621124268 0.9695579409599304
CurrentTrain: epoch  2, batch     0 | loss: 7.6342745Losses:  3.164348840713501 1.0315642356872559 0.954770565032959
CurrentTrain: epoch  2, batch     1 | loss: 5.1506839Losses:  3.816646099090576 1.1972631216049194 0.9735989570617676
CurrentTrain: epoch  2, batch     2 | loss: 5.9875083Losses:  2.631186008453369 0.020874980837106705 0.9398473501205444
CurrentTrain: epoch  2, batch     3 | loss: 3.5919085Losses:  3.861877202987671 0.9873453378677368 0.973383903503418
CurrentTrain: epoch  3, batch     0 | loss: 5.8226066Losses:  4.194421768188477 0.9993103742599487 0.9682447910308838
CurrentTrain: epoch  3, batch     1 | loss: 6.1619768Losses:  4.199071407318115 1.2194617986679077 0.9600311517715454
CurrentTrain: epoch  3, batch     2 | loss: 6.3785644Losses:  1.8259925842285156 0.19722212851047516 0.9199528694152832
CurrentTrain: epoch  3, batch     3 | loss: 2.9431677Losses:  4.362703323364258 0.9485054016113281 0.9743506908416748
CurrentTrain: epoch  4, batch     0 | loss: 6.2855597Losses:  3.091895818710327 0.9380326271057129 0.9263907074928284
CurrentTrain: epoch  4, batch     1 | loss: 4.9563189Losses:  3.08630108833313 1.006805658340454 0.9753441214561462
CurrentTrain: epoch  4, batch     2 | loss: 5.0684509Losses:  5.711862564086914 0.285897821187973 0.979130744934082
CurrentTrain: epoch  4, batch     3 | loss: 6.9768910Losses:  3.3210842609405518 1.045086145401001 0.9438471794128418
CurrentTrain: epoch  5, batch     0 | loss: 5.3100176Losses:  3.3311262130737305 1.1289160251617432 0.9578104019165039
CurrentTrain: epoch  5, batch     1 | loss: 5.4178524Losses:  3.1932623386383057 1.0210679769515991 0.9729387760162354
CurrentTrain: epoch  5, batch     2 | loss: 5.1872692Losses:  2.7377514839172363 0.259235143661499 0.9062457084655762
CurrentTrain: epoch  5, batch     3 | loss: 3.9032323Losses:  3.2571539878845215 1.0151088237762451 0.9575550556182861
CurrentTrain: epoch  6, batch     0 | loss: 5.2298174Losses:  2.5810251235961914 0.8864272832870483 0.9639454483985901
CurrentTrain: epoch  6, batch     1 | loss: 4.4313979Losses:  3.0314087867736816 0.9269493222236633 0.9553654789924622
CurrentTrain: epoch  6, batch     2 | loss: 4.9137235Losses:  2.821226119995117 0.1491190493106842 0.8209899663925171
CurrentTrain: epoch  6, batch     3 | loss: 3.7913351Losses:  3.200896739959717 0.6383309364318848 0.9964299201965332
CurrentTrain: epoch  7, batch     0 | loss: 4.8356576Losses:  2.514153480529785 0.8950287103652954 0.9319520592689514
CurrentTrain: epoch  7, batch     1 | loss: 4.3411341Losses:  2.5660271644592285 0.7652819156646729 0.9261701107025146
CurrentTrain: epoch  7, batch     2 | loss: 4.2574792Losses:  3.474031448364258 0.19671845436096191 0.9665971398353577
CurrentTrain: epoch  7, batch     3 | loss: 4.6373472Losses:  2.482555389404297 0.7051440477371216 0.9656491279602051
CurrentTrain: epoch  8, batch     0 | loss: 4.1533484Losses:  2.4644691944122314 0.8659050464630127 0.9455571174621582
CurrentTrain: epoch  8, batch     1 | loss: 4.2759314Losses:  2.5870447158813477 0.9227105379104614 0.9403274655342102
CurrentTrain: epoch  8, batch     2 | loss: 4.4500828Losses:  1.703244924545288 0.18088647723197937 0.9716426134109497
CurrentTrain: epoch  8, batch     3 | loss: 2.8557739Losses:  2.1683144569396973 0.7388991713523865 0.9653273224830627
CurrentTrain: epoch  9, batch     0 | loss: 3.8725410Losses:  2.4270832538604736 0.7518594264984131 0.9501190781593323
CurrentTrain: epoch  9, batch     1 | loss: 4.1290617Losses:  2.2063655853271484 0.9401082992553711 0.9393979907035828
CurrentTrain: epoch  9, batch     2 | loss: 4.0858717Losses:  2.6515989303588867 0.26164835691452026 0.914889931678772
CurrentTrain: epoch  9, batch     3 | loss: 3.8281374
Losses:  0.5366359949111938 0.931857705116272 0.9212393760681152
MemoryTrain:  epoch  0, batch     0 | loss: 2.3897331Losses:  0.9012622833251953 0.7883519530296326 0.9183938503265381
MemoryTrain:  epoch  0, batch     1 | loss: 2.6080081Losses:  0.38829559087753296 0.7472281455993652 0.8991014957427979
MemoryTrain:  epoch  0, batch     2 | loss: 2.0346253Losses:  0.44417405128479004 1.1448835134506226 0.9040259122848511
MemoryTrain:  epoch  0, batch     3 | loss: 2.4930835Losses:  0.6550312042236328 0.8311940431594849 0.8995293378829956
MemoryTrain:  epoch  0, batch     4 | loss: 2.3857546Losses:  0.8486500978469849 0.834149956703186 0.9066322445869446
MemoryTrain:  epoch  1, batch     0 | loss: 2.5894322Losses:  0.7307550311088562 0.8749034404754639 0.9098687171936035
MemoryTrain:  epoch  1, batch     1 | loss: 2.5155272Losses:  0.3303772211074829 0.5607325434684753 0.9188242554664612
MemoryTrain:  epoch  1, batch     2 | loss: 1.8099340Losses:  1.1247291564941406 1.0535945892333984 0.8657001852989197
MemoryTrain:  epoch  1, batch     3 | loss: 3.0440240Losses:  1.6603317260742188 1.0913209915161133 0.9164190888404846
MemoryTrain:  epoch  1, batch     4 | loss: 3.6680717Losses:  0.49095845222473145 0.9002195596694946 0.8615407943725586
MemoryTrain:  epoch  2, batch     0 | loss: 2.2527189Losses:  0.3812333047389984 0.9339278936386108 0.9250226020812988
MemoryTrain:  epoch  2, batch     1 | loss: 2.2401838Losses:  0.4993288516998291 0.9538581371307373 0.9247572422027588
MemoryTrain:  epoch  2, batch     2 | loss: 2.3779442Losses:  0.1712874174118042 0.6433284282684326 0.890007495880127
MemoryTrain:  epoch  2, batch     3 | loss: 1.7046233Losses:  0.5460436940193176 0.7592096328735352 0.913271427154541
MemoryTrain:  epoch  2, batch     4 | loss: 2.2185247Losses:  0.10130369663238525 0.7104730010032654 0.8905030488967896
MemoryTrain:  epoch  3, batch     0 | loss: 1.7022798Losses:  0.3332386910915375 0.8286354541778564 0.9385336637496948
MemoryTrain:  epoch  3, batch     1 | loss: 2.1004078Losses:  0.2681160867214203 0.7795443534851074 0.8812583684921265
MemoryTrain:  epoch  3, batch     2 | loss: 1.9289188Losses:  0.2840152680873871 1.0524346828460693 0.8807039260864258
MemoryTrain:  epoch  3, batch     3 | loss: 2.2171540Losses:  0.13027441501617432 0.6867167949676514 0.9300745725631714
MemoryTrain:  epoch  3, batch     4 | loss: 1.7470658Losses:  0.2130039632320404 0.9770138263702393 0.9232112169265747
MemoryTrain:  epoch  4, batch     0 | loss: 2.1132290Losses:  0.16922253370285034 0.7851213216781616 0.9077794551849365
MemoryTrain:  epoch  4, batch     1 | loss: 1.8621233Losses:  0.13238972425460815 0.6524497866630554 0.8604482412338257
MemoryTrain:  epoch  4, batch     2 | loss: 1.6452878Losses:  0.1728043556213379 0.7040932178497314 0.9145053625106812
MemoryTrain:  epoch  4, batch     3 | loss: 1.7914029Losses:  0.10920701175928116 0.890617847442627 0.9094666242599487
MemoryTrain:  epoch  4, batch     4 | loss: 1.9092915Losses:  0.1605278104543686 0.7795539498329163 0.9445288777351379
MemoryTrain:  epoch  5, batch     0 | loss: 1.8846107Losses:  0.09949083626270294 0.9536574482917786 0.8539808392524719
MemoryTrain:  epoch  5, batch     1 | loss: 1.9071290Losses:  0.06666570156812668 0.7857249975204468 0.9041650295257568
MemoryTrain:  epoch  5, batch     2 | loss: 1.7565558Losses:  0.24725127220153809 0.930575966835022 0.8846791982650757
MemoryTrain:  epoch  5, batch     3 | loss: 2.0625064Losses:  0.2521249055862427 0.6114242076873779 0.9247352480888367
MemoryTrain:  epoch  5, batch     4 | loss: 1.7882843Losses:  0.08620906621217728 0.9008553624153137 0.9256898164749146
MemoryTrain:  epoch  6, batch     0 | loss: 1.9127543Losses:  0.09880736470222473 0.6993435621261597 0.9088196158409119
MemoryTrain:  epoch  6, batch     1 | loss: 1.7069705Losses:  0.06766791641712189 0.7037458419799805 0.8724112510681152
MemoryTrain:  epoch  6, batch     2 | loss: 1.6438251Losses:  0.06296511739492416 0.7371038198471069 0.907001256942749
MemoryTrain:  epoch  6, batch     3 | loss: 1.7070701Losses:  0.07750402390956879 1.0169792175292969 0.8957539796829224
MemoryTrain:  epoch  6, batch     4 | loss: 1.9902372Losses:  0.05466458946466446 0.8433501720428467 0.8981770277023315
MemoryTrain:  epoch  7, batch     0 | loss: 1.7961918Losses:  0.08789430558681488 0.9060418605804443 0.9130898714065552
MemoryTrain:  epoch  7, batch     1 | loss: 1.9070261Losses:  0.05771371349692345 0.7476999759674072 0.8991461992263794
MemoryTrain:  epoch  7, batch     2 | loss: 1.7045598Losses:  0.21988445520401 0.6467972993850708 0.9195128679275513
MemoryTrain:  epoch  7, batch     3 | loss: 1.7861946Losses:  0.0627201646566391 0.799501359462738 0.8720271587371826
MemoryTrain:  epoch  7, batch     4 | loss: 1.7342486Losses:  0.048081837594509125 0.6464869379997253 0.8582426309585571
MemoryTrain:  epoch  8, batch     0 | loss: 1.5528114Losses:  0.07543376088142395 0.9052309989929199 0.9140063524246216
MemoryTrain:  epoch  8, batch     1 | loss: 1.8946711Losses:  0.05569779872894287 0.7187640070915222 0.9092134237289429
MemoryTrain:  epoch  8, batch     2 | loss: 1.6836753Losses:  0.19354653358459473 0.8624722957611084 0.9500941634178162
MemoryTrain:  epoch  8, batch     3 | loss: 2.0061131Losses:  0.0799376517534256 0.694453239440918 0.8682764768600464
MemoryTrain:  epoch  8, batch     4 | loss: 1.6426673Losses:  0.030764583498239517 0.9167332053184509 0.875248372554779
MemoryTrain:  epoch  9, batch     0 | loss: 1.8227462Losses:  0.13531193137168884 0.8238797783851624 0.863455057144165
MemoryTrain:  epoch  9, batch     1 | loss: 1.8226467Losses:  0.0571274571120739 0.6204362511634827 0.9280574321746826
MemoryTrain:  epoch  9, batch     2 | loss: 1.6056211Losses:  0.04035742208361626 0.6265618801116943 0.925305187702179
MemoryTrain:  epoch  9, batch     3 | loss: 1.5922245Losses:  0.1011200025677681 0.8007078170776367 0.901402473449707
MemoryTrain:  epoch  9, batch     4 | loss: 1.8032303
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 59.77%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 63.91%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 61.03%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 59.82%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 58.51%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 57.09%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 57.07%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 58.44%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 59.15%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 59.82%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 60.32%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.80%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 60.14%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 59.78%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 59.31%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 59.24%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 59.06%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 58.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 59.56%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.08%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 65.77%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 85.90%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 84.95%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 84.56%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 84.25%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 83.84%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 83.10%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 82.27%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 81.70%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 79.85%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 79.45%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 79.00%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.83%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.37%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 77.44%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 76.44%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 75.76%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 74.91%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 74.08%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 73.64%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 73.48%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 72.98%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 72.48%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 71.92%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 71.45%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 71.05%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 69.51%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 68.67%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 67.86%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 67.21%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 66.50%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 65.88%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 69.67%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 69.54%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 69.59%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 69.63%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 69.16%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 68.69%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 68.12%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 67.30%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 67.04%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 67.50%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 66.94%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 66.39%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 65.90%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 65.37%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 64.85%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 64.89%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 64.73%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 66.01%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 65.90%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  149 | acc: 43.75%,  total acc: 65.71%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 65.56%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 65.52%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 65.24%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 65.06%   [EVAL] batch:  156 | acc: 31.25%,  total acc: 64.85%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 64.52%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 64.39%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 64.30%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 64.29%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 64.28%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 64.01%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 63.74%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 63.44%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 63.15%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 62.90%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 62.64%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 64.11%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 64.35%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 64.44%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 64.39%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 64.07%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 63.93%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 63.80%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 64.21%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 64.49%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 64.20%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 64.04%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 63.82%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 63.66%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 63.53%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 65.01%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.72%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 66.01%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 65.76%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.80%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 66.29%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 66.08%   [EVAL] batch:  241 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  242 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 65.57%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 65.40%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 65.26%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 65.24%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.21%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 65.25%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 65.81%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.78%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 65.72%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 65.64%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 65.39%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 65.15%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 64.91%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 64.67%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 64.48%   [EVAL] batch:  274 | acc: 0.00%,  total acc: 64.25%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 65.04%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  285 | acc: 50.00%,  total acc: 64.99%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 66.61%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 66.72%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 66.94%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 66.76%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 66.59%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 66.38%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 66.19%   [EVAL] batch:  317 | acc: 6.25%,  total acc: 66.00%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 66.31%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 66.17%   [EVAL] batch:  332 | acc: 6.25%,  total acc: 65.99%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 65.87%   [EVAL] batch:  334 | acc: 18.75%,  total acc: 65.73%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 65.55%   [EVAL] batch:  336 | acc: 6.25%,  total acc: 65.37%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 65.26%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 65.06%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 64.89%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 64.70%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 64.51%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 64.34%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 64.23%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 64.28%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 64.56%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 64.92%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  356 | acc: 25.00%,  total acc: 64.99%   [EVAL] batch:  357 | acc: 6.25%,  total acc: 64.82%   [EVAL] batch:  358 | acc: 0.00%,  total acc: 64.64%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 64.48%   [EVAL] batch:  360 | acc: 6.25%,  total acc: 64.32%   [EVAL] batch:  361 | acc: 6.25%,  total acc: 64.16%   [EVAL] batch:  362 | acc: 18.75%,  total acc: 64.03%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 64.03%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 64.04%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 64.11%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 64.12%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 64.16%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 64.10%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 64.08%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 64.03%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 64.01%   [EVAL] batch:  373 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 63.90%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 63.74%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 63.66%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 63.54%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 63.44%   [EVAL] batch:  380 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 63.32%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 63.46%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 63.68%   [EVAL] batch:  388 | acc: 25.00%,  total acc: 63.58%   [EVAL] batch:  389 | acc: 25.00%,  total acc: 63.48%   [EVAL] batch:  390 | acc: 18.75%,  total acc: 63.36%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 63.31%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 63.26%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 63.20%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 63.72%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 63.58%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 63.50%   [EVAL] batch:  402 | acc: 37.50%,  total acc: 63.43%   [EVAL] batch:  403 | acc: 43.75%,  total acc: 63.38%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 63.24%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 63.15%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 63.19%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 63.38%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 63.56%   [EVAL] batch:  413 | acc: 50.00%,  total acc: 63.53%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:  415 | acc: 56.25%,  total acc: 63.52%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  417 | acc: 50.00%,  total acc: 63.49%   [EVAL] batch:  418 | acc: 50.00%,  total acc: 63.45%   [EVAL] batch:  419 | acc: 43.75%,  total acc: 63.41%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 63.39%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 63.33%   [EVAL] batch:  422 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 63.21%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 63.15%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 63.13%   [EVAL] batch:  426 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:  427 | acc: 56.25%,  total acc: 63.04%   [EVAL] batch:  428 | acc: 50.00%,  total acc: 63.01%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 63.02%   [EVAL] batch:  430 | acc: 50.00%,  total acc: 62.99%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 63.02%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 63.06%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 63.20%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:  438 | acc: 75.00%,  total acc: 63.27%   [EVAL] batch:  439 | acc: 43.75%,  total acc: 63.22%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  441 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 63.18%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 63.18%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 63.22%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 63.23%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  448 | acc: 37.50%,  total acc: 63.17%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 63.26%   [EVAL] batch:  451 | acc: 43.75%,  total acc: 63.22%   [EVAL] batch:  452 | acc: 50.00%,  total acc: 63.19%   [EVAL] batch:  453 | acc: 37.50%,  total acc: 63.13%   [EVAL] batch:  454 | acc: 43.75%,  total acc: 63.09%   [EVAL] batch:  455 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 63.57%   [EVAL] batch:  463 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:  464 | acc: 25.00%,  total acc: 63.45%   [EVAL] batch:  465 | acc: 43.75%,  total acc: 63.41%   [EVAL] batch:  466 | acc: 43.75%,  total acc: 63.37%   [EVAL] batch:  467 | acc: 56.25%,  total acc: 63.35%   [EVAL] batch:  468 | acc: 31.25%,  total acc: 63.29%   [EVAL] batch:  469 | acc: 43.75%,  total acc: 63.24%   [EVAL] batch:  470 | acc: 25.00%,  total acc: 63.16%   [EVAL] batch:  471 | acc: 12.50%,  total acc: 63.06%   [EVAL] batch:  472 | acc: 18.75%,  total acc: 62.96%   [EVAL] batch:  473 | acc: 6.25%,  total acc: 62.84%   [EVAL] batch:  474 | acc: 18.75%,  total acc: 62.75%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 62.79%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 62.83%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 62.88%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:  481 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 62.95%   [EVAL] batch:  483 | acc: 50.00%,  total acc: 62.93%   [EVAL] batch:  484 | acc: 43.75%,  total acc: 62.89%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 62.85%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 62.83%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 62.85%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 63.36%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 63.65%   
cur_acc:  ['0.9435', '0.6984', '0.7212', '0.7014', '0.8234', '0.6002', '0.6012', '0.6577']
his_acc:  ['0.9435', '0.8215', '0.7656', '0.7200', '0.7350', '0.7057', '0.6688', '0.6365']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.929168701171875 1.7866168022155762 1.0085493326187134
CurrentTrain: epoch  0, batch     0 | loss: 12.7243357Losses:  9.895271301269531 1.7783894538879395 0.9941726922988892
CurrentTrain: epoch  0, batch     1 | loss: 12.6678333Losses:  9.961596488952637 1.5774853229522705 0.9927022457122803
CurrentTrain: epoch  0, batch     2 | loss: 12.5317841Losses:  9.937478065490723 1.8752357959747314 0.9896246790885925
CurrentTrain: epoch  0, batch     3 | loss: 12.8023386Losses:  9.241983413696289 1.8443959951400757 0.9867941737174988
CurrentTrain: epoch  0, batch     4 | loss: 12.0731735Losses:  10.066642761230469 1.7445825338363647 0.9903761148452759
CurrentTrain: epoch  0, batch     5 | loss: 12.8016014Losses:  9.040290832519531 1.6081656217575073 0.9865531325340271
CurrentTrain: epoch  0, batch     6 | loss: 11.6350098Losses:  9.275697708129883 1.7050517797470093 0.9810538291931152
CurrentTrain: epoch  0, batch     7 | loss: 11.9618034Losses:  9.191170692443848 1.7283164262771606 0.9765948057174683
CurrentTrain: epoch  0, batch     8 | loss: 11.8960819Losses:  8.906917572021484 1.6783897876739502 0.9762576222419739
CurrentTrain: epoch  0, batch     9 | loss: 11.5615644Losses:  9.520151138305664 1.6061570644378662 0.9783293604850769
CurrentTrain: epoch  0, batch    10 | loss: 12.1046381Losses:  8.507615089416504 1.5752543210983276 0.9761068820953369
CurrentTrain: epoch  0, batch    11 | loss: 11.0589762Losses:  9.187496185302734 1.6185154914855957 0.9669106006622314
CurrentTrain: epoch  0, batch    12 | loss: 11.7729216Losses:  8.696640968322754 1.5873795747756958 0.9660022258758545
CurrentTrain: epoch  0, batch    13 | loss: 11.2500229Losses:  8.717169761657715 1.5603269338607788 0.9673197269439697
CurrentTrain: epoch  0, batch    14 | loss: 11.2448158Losses:  8.255327224731445 1.3654348850250244 0.9558935165405273
CurrentTrain: epoch  0, batch    15 | loss: 10.5766554Losses:  8.538986206054688 1.4560564756393433 0.9895375370979309
CurrentTrain: epoch  0, batch    16 | loss: 10.9845800Losses:  8.876371383666992 1.6383365392684937 0.9861652255058289
CurrentTrain: epoch  0, batch    17 | loss: 11.5008726Losses:  8.393119812011719 1.2569079399108887 0.9780831336975098
CurrentTrain: epoch  0, batch    18 | loss: 10.6281109Losses:  8.282144546508789 1.0841535329818726 0.9768836498260498
CurrentTrain: epoch  0, batch    19 | loss: 10.3431816Losses:  8.902108192443848 1.6036455631256104 1.0028612613677979
CurrentTrain: epoch  0, batch    20 | loss: 11.5086145Losses:  8.595527648925781 1.5944976806640625 0.9680077433586121
CurrentTrain: epoch  0, batch    21 | loss: 11.1580334Losses:  8.844039916992188 1.2968604564666748 0.9701043963432312
CurrentTrain: epoch  0, batch    22 | loss: 11.1110048Losses:  7.9016242027282715 1.2327594757080078 0.9682704210281372
CurrentTrain: epoch  0, batch    23 | loss: 10.1026545Losses:  9.229569435119629 1.448089599609375 0.9608578681945801
CurrentTrain: epoch  0, batch    24 | loss: 11.6385174Losses:  8.292044639587402 1.1365420818328857 0.9670091271400452
CurrentTrain: epoch  0, batch    25 | loss: 10.3955965Losses:  8.049468994140625 1.3695741891860962 0.9704776406288147
CurrentTrain: epoch  0, batch    26 | loss: 10.3895216Losses:  8.649691581726074 1.442186713218689 0.9588857889175415
CurrentTrain: epoch  0, batch    27 | loss: 11.0507641Losses:  8.378007888793945 1.4680290222167969 0.9617509245872498
CurrentTrain: epoch  0, batch    28 | loss: 10.8077879Losses:  7.579143047332764 1.1638834476470947 0.9561591148376465
CurrentTrain: epoch  0, batch    29 | loss: 9.6991863Losses:  8.857969284057617 1.323768138885498 0.9669250845909119
CurrentTrain: epoch  0, batch    30 | loss: 11.1486626Losses:  7.445685386657715 1.1051881313323975 0.9565674066543579
CurrentTrain: epoch  0, batch    31 | loss: 9.5074415Losses:  7.147438049316406 1.1546456813812256 0.973526120185852
CurrentTrain: epoch  0, batch    32 | loss: 9.2756100Losses:  8.55980396270752 1.2511820793151855 0.9594045281410217
CurrentTrain: epoch  0, batch    33 | loss: 10.7703905Losses:  7.6617631912231445 1.1113028526306152 0.9550906419754028
CurrentTrain: epoch  0, batch    34 | loss: 9.7281561Losses:  7.66114616394043 1.0959724187850952 0.9354535341262817
CurrentTrain: epoch  0, batch    35 | loss: 9.6925716Losses:  8.26042366027832 1.2877745628356934 0.9667583107948303
CurrentTrain: epoch  0, batch    36 | loss: 10.5149574Losses:  7.85081672668457 1.1337285041809082 0.9380576610565186
CurrentTrain: epoch  0, batch    37 | loss: 9.9226027Losses:  8.1246337890625 1.1656090021133423 0.9591212868690491
CurrentTrain: epoch  0, batch    38 | loss: 10.2493649Losses:  8.422806739807129 1.3740267753601074 0.9710750579833984
CurrentTrain: epoch  0, batch    39 | loss: 10.7679081Losses:  7.725426197052002 1.4156429767608643 0.9537702798843384
CurrentTrain: epoch  0, batch    40 | loss: 10.0948400Losses:  7.74045991897583 1.150663137435913 0.9588836431503296
CurrentTrain: epoch  0, batch    41 | loss: 9.8500061Losses:  7.144618511199951 0.9646358489990234 0.9245309829711914
CurrentTrain: epoch  0, batch    42 | loss: 9.0337858Losses:  7.872560024261475 1.1405158042907715 0.9494462013244629
CurrentTrain: epoch  0, batch    43 | loss: 9.9625225Losses:  8.05299186706543 1.220973253250122 0.9330084919929504
CurrentTrain: epoch  0, batch    44 | loss: 10.2069731Losses:  7.313125133514404 0.9106758832931519 0.9370334148406982
CurrentTrain: epoch  0, batch    45 | loss: 9.1608343Losses:  8.148240089416504 1.056784749031067 0.9509400129318237
CurrentTrain: epoch  0, batch    46 | loss: 10.1559649Losses:  7.658140659332275 1.121952772140503 0.9542829990386963
CurrentTrain: epoch  0, batch    47 | loss: 9.7343760Losses:  6.9559173583984375 1.2659457921981812 0.9328088164329529
CurrentTrain: epoch  0, batch    48 | loss: 9.1546717Losses:  7.92948579788208 1.1944724321365356 0.952555239200592
CurrentTrain: epoch  0, batch    49 | loss: 10.0765142Losses:  7.621322154998779 1.021552562713623 0.9477885961532593
CurrentTrain: epoch  0, batch    50 | loss: 9.5906630Losses:  7.177041530609131 1.0870516300201416 0.9456973075866699
CurrentTrain: epoch  0, batch    51 | loss: 9.2097912Losses:  7.411755561828613 1.0418164730072021 0.9346210956573486
CurrentTrain: epoch  0, batch    52 | loss: 9.3881931Losses:  7.280324935913086 0.9558470249176025 0.9148700833320618
CurrentTrain: epoch  0, batch    53 | loss: 9.1510420Losses:  7.410124778747559 0.9789443016052246 0.9432594180107117
CurrentTrain: epoch  0, batch    54 | loss: 9.3323278Losses:  7.077500820159912 1.0826728343963623 0.9268593788146973
CurrentTrain: epoch  0, batch    55 | loss: 9.0870323Losses:  7.784830570220947 1.0496712923049927 0.9383645057678223
CurrentTrain: epoch  0, batch    56 | loss: 9.7728672Losses:  7.1739044189453125 0.8277178406715393 0.935617983341217
CurrentTrain: epoch  0, batch    57 | loss: 8.9372406Losses:  7.243448734283447 1.0356099605560303 0.9307509660720825
CurrentTrain: epoch  0, batch    58 | loss: 9.2098093Losses:  7.560433387756348 1.136535882949829 0.9469789266586304
CurrentTrain: epoch  0, batch    59 | loss: 9.6439476Losses:  7.242369174957275 1.0160605907440186 0.9405275583267212
CurrentTrain: epoch  0, batch    60 | loss: 9.1989574Losses:  7.647117614746094 1.0249507427215576 0.9360668063163757
CurrentTrain: epoch  0, batch    61 | loss: 9.6081352Losses:  6.910160064697266 0.7499337196350098 0.9462990760803223
CurrentTrain: epoch  0, batch    62 | loss: 8.6063929Losses:  6.848126411437988 0.8492943048477173 0.9086445569992065
CurrentTrain: epoch  1, batch     0 | loss: 8.6060648Losses:  7.821433067321777 1.087843418121338 0.9361719489097595
CurrentTrain: epoch  1, batch     1 | loss: 9.8454485Losses:  6.663445472717285 0.9080893993377686 0.9324829578399658
CurrentTrain: epoch  1, batch     2 | loss: 8.5040178Losses:  7.286797523498535 0.975620448589325 0.930088222026825
CurrentTrain: epoch  1, batch     3 | loss: 9.1925058Losses:  6.6891584396362305 0.8765320777893066 0.9408100843429565
CurrentTrain: epoch  1, batch     4 | loss: 8.5065002Losses:  7.220120906829834 0.810930609703064 0.9160757064819336
CurrentTrain: epoch  1, batch     5 | loss: 8.9471273Losses:  7.629190921783447 0.9970981478691101 0.9516822695732117
CurrentTrain: epoch  1, batch     6 | loss: 9.5779715Losses:  6.924827575683594 0.8293527364730835 0.9371134042739868
CurrentTrain: epoch  1, batch     7 | loss: 8.6912937Losses:  7.067893981933594 0.9992414712905884 0.9315053820610046
CurrentTrain: epoch  1, batch     8 | loss: 8.9986410Losses:  6.970403671264648 0.9519577026367188 0.933100163936615
CurrentTrain: epoch  1, batch     9 | loss: 8.8554611Losses:  6.720211029052734 0.9449471831321716 0.9341433644294739
CurrentTrain: epoch  1, batch    10 | loss: 8.5993013Losses:  6.3899922370910645 0.9717940092086792 0.8979572057723999
CurrentTrain: epoch  1, batch    11 | loss: 8.2597437Losses:  7.01899528503418 0.968777060508728 0.9372400045394897
CurrentTrain: epoch  1, batch    12 | loss: 8.9250126Losses:  7.5899200439453125 1.0177037715911865 0.9237399697303772
CurrentTrain: epoch  1, batch    13 | loss: 9.5313644Losses:  6.4043779373168945 0.7326657772064209 0.9216601848602295
CurrentTrain: epoch  1, batch    14 | loss: 8.0587044Losses:  6.957779884338379 0.9602968692779541 0.9308172464370728
CurrentTrain: epoch  1, batch    15 | loss: 8.8488941Losses:  6.18089485168457 0.8187590837478638 0.9040687084197998
CurrentTrain: epoch  1, batch    16 | loss: 7.9037228Losses:  5.477372169494629 0.7196804285049438 0.8890058994293213
CurrentTrain: epoch  1, batch    17 | loss: 7.0860586Losses:  6.3043060302734375 0.8048072457313538 0.9024990797042847
CurrentTrain: epoch  1, batch    18 | loss: 8.0116119Losses:  6.492898464202881 0.9355753660202026 0.8977001905441284
CurrentTrain: epoch  1, batch    19 | loss: 8.3261738Losses:  6.413652420043945 0.785879373550415 0.9048589468002319
CurrentTrain: epoch  1, batch    20 | loss: 8.1043901Losses:  5.98234748840332 0.8976986408233643 0.9062942266464233
CurrentTrain: epoch  1, batch    21 | loss: 7.7863402Losses:  6.51878023147583 0.9675847887992859 0.9120274186134338
CurrentTrain: epoch  1, batch    22 | loss: 8.3983927Losses:  6.188654899597168 0.6440425515174866 0.8939769268035889
CurrentTrain: epoch  1, batch    23 | loss: 7.7266741Losses:  5.931394577026367 0.7295180559158325 0.8975536823272705
CurrentTrain: epoch  1, batch    24 | loss: 7.5584660Losses:  7.238770484924316 0.9235830307006836 0.9459768533706665
CurrentTrain: epoch  1, batch    25 | loss: 9.1083307Losses:  6.550728797912598 0.6715470552444458 0.9431777000427246
CurrentTrain: epoch  1, batch    26 | loss: 8.1654530Losses:  7.149726867675781 0.9520965814590454 0.9163011312484741
CurrentTrain: epoch  1, batch    27 | loss: 9.0181246Losses:  6.871026992797852 0.5824536085128784 0.9103024005889893
CurrentTrain: epoch  1, batch    28 | loss: 8.3637829Losses:  6.410116195678711 0.8320708274841309 0.9009818434715271
CurrentTrain: epoch  1, batch    29 | loss: 8.1431684Losses:  5.996001243591309 0.646526575088501 0.8826211094856262
CurrentTrain: epoch  1, batch    30 | loss: 7.5251489Losses:  6.155055522918701 0.8953843712806702 0.9136219024658203
CurrentTrain: epoch  1, batch    31 | loss: 7.9640617Losses:  5.682091236114502 0.6207592487335205 0.8947145938873291
CurrentTrain: epoch  1, batch    32 | loss: 7.1975651Losses:  6.169865608215332 0.6824512481689453 0.9246007800102234
CurrentTrain: epoch  1, batch    33 | loss: 7.7769175Losses:  6.658018112182617 0.6550227403640747 0.9227142333984375
CurrentTrain: epoch  1, batch    34 | loss: 8.2357550Losses:  6.652386665344238 0.6662778854370117 0.9054081439971924
CurrentTrain: epoch  1, batch    35 | loss: 8.2240725Losses:  5.82456636428833 0.6964476108551025 0.8845936059951782
CurrentTrain: epoch  1, batch    36 | loss: 7.4056077Losses:  6.113724708557129 0.6152863502502441 0.9012531638145447
CurrentTrain: epoch  1, batch    37 | loss: 7.6302643Losses:  6.628668308258057 0.7727845907211304 0.9246659278869629
CurrentTrain: epoch  1, batch    38 | loss: 8.3261185Losses:  6.2031755447387695 0.801555871963501 0.8975690603256226
CurrentTrain: epoch  1, batch    39 | loss: 7.9023004Losses:  6.008370399475098 0.7708462476730347 0.9035089015960693
CurrentTrain: epoch  1, batch    40 | loss: 7.6827259Losses:  5.867006778717041 0.7748311758041382 0.8896600008010864
CurrentTrain: epoch  1, batch    41 | loss: 7.5314980Losses:  5.90780782699585 0.5978947281837463 0.9069192409515381
CurrentTrain: epoch  1, batch    42 | loss: 7.4126215Losses:  5.964275360107422 0.526419460773468 0.906291663646698
CurrentTrain: epoch  1, batch    43 | loss: 7.3969865Losses:  6.74129056930542 0.5882298350334167 0.9099771976470947
CurrentTrain: epoch  1, batch    44 | loss: 8.2394972Losses:  7.042797088623047 0.8121981024742126 0.8950200080871582
CurrentTrain: epoch  1, batch    45 | loss: 8.7500153Losses:  6.716508865356445 0.7821321487426758 0.9266267418861389
CurrentTrain: epoch  1, batch    46 | loss: 8.4252682Losses:  6.5830278396606445 0.8232864737510681 0.9176560640335083
CurrentTrain: epoch  1, batch    47 | loss: 8.3239708Losses:  6.373671054840088 0.7003105878829956 0.9018940925598145
CurrentTrain: epoch  1, batch    48 | loss: 7.9758759Losses:  6.2944841384887695 0.7926559448242188 0.8602664470672607
CurrentTrain: epoch  1, batch    49 | loss: 7.9474068Losses:  6.5139594078063965 0.5423747301101685 0.9200361967086792
CurrentTrain: epoch  1, batch    50 | loss: 7.9763703Losses:  6.478799343109131 0.7681165933609009 0.9016944766044617
CurrentTrain: epoch  1, batch    51 | loss: 8.1486101Losses:  5.843998432159424 0.5641335844993591 0.8710031509399414
CurrentTrain: epoch  1, batch    52 | loss: 7.2791352Losses:  6.872278690338135 0.6590790748596191 0.9195879697799683
CurrentTrain: epoch  1, batch    53 | loss: 8.4509459Losses:  6.097174644470215 0.4979351758956909 0.9034602046012878
CurrentTrain: epoch  1, batch    54 | loss: 7.4985700Losses:  6.0776448249816895 0.5594071745872498 0.9318497180938721
CurrentTrain: epoch  1, batch    55 | loss: 7.5689020Losses:  6.138100624084473 0.7463377714157104 0.8853365182876587
CurrentTrain: epoch  1, batch    56 | loss: 7.7697749Losses:  6.552634239196777 0.644047737121582 0.910983145236969
CurrentTrain: epoch  1, batch    57 | loss: 8.1076651Losses:  5.8919878005981445 0.5783923864364624 0.8934558629989624
CurrentTrain: epoch  1, batch    58 | loss: 7.3638363Losses:  5.85522985458374 0.5551454424858093 0.8897404074668884
CurrentTrain: epoch  1, batch    59 | loss: 7.3001156Losses:  5.922700881958008 0.5386275053024292 0.8567999005317688
CurrentTrain: epoch  1, batch    60 | loss: 7.3181286Losses:  5.620510101318359 0.6023216247558594 0.8962497115135193
CurrentTrain: epoch  1, batch    61 | loss: 7.1190815Losses:  7.898811340332031 0.5797157287597656 0.956348717212677
CurrentTrain: epoch  1, batch    62 | loss: 9.4348755Losses:  5.173555374145508 0.44571417570114136 0.8699651956558228
CurrentTrain: epoch  2, batch     0 | loss: 6.4892344Losses:  5.865971565246582 0.5731467604637146 0.8929896354675293
CurrentTrain: epoch  2, batch     1 | loss: 7.3321080Losses:  5.3448052406311035 0.6131739020347595 0.9167325496673584
CurrentTrain: epoch  2, batch     2 | loss: 6.8747120Losses:  5.907198429107666 0.38849639892578125 0.879693329334259
CurrentTrain: epoch  2, batch     3 | loss: 7.1753883Losses:  5.688714981079102 0.5575830936431885 0.8847934007644653
CurrentTrain: epoch  2, batch     4 | loss: 7.1310911Losses:  5.919663429260254 0.4773268699645996 0.8862799406051636
CurrentTrain: epoch  2, batch     5 | loss: 7.2832704Losses:  5.580918312072754 0.4769275486469269 0.8682878613471985
CurrentTrain: epoch  2, batch     6 | loss: 6.9261341Losses:  5.099688529968262 0.608497679233551 0.8744120001792908
CurrentTrain: epoch  2, batch     7 | loss: 6.5825982Losses:  5.733625411987305 0.5771502256393433 0.8765285015106201
CurrentTrain: epoch  2, batch     8 | loss: 7.1873045Losses:  5.250776290893555 0.49522149562835693 0.8639562726020813
CurrentTrain: epoch  2, batch     9 | loss: 6.6099544Losses:  6.112278938293457 0.7289325594902039 0.8854807019233704
CurrentTrain: epoch  2, batch    10 | loss: 7.7266922Losses:  5.463269233703613 0.41454192996025085 0.8798669576644897
CurrentTrain: epoch  2, batch    11 | loss: 6.7576780Losses:  5.2807769775390625 0.47819778323173523 0.8742008209228516
CurrentTrain: epoch  2, batch    12 | loss: 6.6331754Losses:  5.9694600105285645 0.5569944977760315 0.9335154294967651
CurrentTrain: epoch  2, batch    13 | loss: 7.4599700Losses:  6.260865211486816 0.39722323417663574 0.9044865369796753
CurrentTrain: epoch  2, batch    14 | loss: 7.5625753Losses:  5.181081771850586 0.4203118681907654 0.8673884868621826
CurrentTrain: epoch  2, batch    15 | loss: 6.4687824Losses:  5.374252796173096 0.5419055223464966 0.8612373471260071
CurrentTrain: epoch  2, batch    16 | loss: 6.7773957Losses:  4.851156234741211 0.4268684387207031 0.8357071876525879
CurrentTrain: epoch  2, batch    17 | loss: 6.1137319Losses:  5.931874752044678 0.5380526781082153 0.8500479459762573
CurrentTrain: epoch  2, batch    18 | loss: 7.3199754Losses:  5.433228015899658 0.4931524991989136 0.8933465480804443
CurrentTrain: epoch  2, batch    19 | loss: 6.8197269Losses:  6.588602066040039 0.6873070597648621 0.8721354603767395
CurrentTrain: epoch  2, batch    20 | loss: 8.1480446Losses:  6.437145709991455 0.604582667350769 0.8572533130645752
CurrentTrain: epoch  2, batch    21 | loss: 7.8989820Losses:  6.005400657653809 0.6664193868637085 0.8867907524108887
CurrentTrain: epoch  2, batch    22 | loss: 7.5586109Losses:  5.746821880340576 0.43705207109451294 0.8760585784912109
CurrentTrain: epoch  2, batch    23 | loss: 7.0599327Losses:  5.1268205642700195 0.41682103276252747 0.8916270732879639
CurrentTrain: epoch  2, batch    24 | loss: 6.4352684Losses:  5.259500503540039 0.591601550579071 0.8679366707801819
CurrentTrain: epoch  2, batch    25 | loss: 6.7190385Losses:  5.2560882568359375 0.5842322707176208 0.8755109310150146
CurrentTrain: epoch  2, batch    26 | loss: 6.7158318Losses:  5.686285972595215 0.4855843782424927 0.869922399520874
CurrentTrain: epoch  2, batch    27 | loss: 7.0417929Losses:  5.587196350097656 0.5170309543609619 0.8466053605079651
CurrentTrain: epoch  2, batch    28 | loss: 6.9508324Losses:  5.383489608764648 0.43440330028533936 0.8651138544082642
CurrentTrain: epoch  2, batch    29 | loss: 6.6830068Losses:  5.540326118469238 0.49698424339294434 0.8508094549179077
CurrentTrain: epoch  2, batch    30 | loss: 6.8881202Losses:  6.02455472946167 0.5921302437782288 0.871811032295227
CurrentTrain: epoch  2, batch    31 | loss: 7.4884958Losses:  5.1634016036987305 0.3862949013710022 0.9099143147468567
CurrentTrain: epoch  2, batch    32 | loss: 6.4596109Losses:  6.031920433044434 0.4039798378944397 0.8672817945480347
CurrentTrain: epoch  2, batch    33 | loss: 7.3031821Losses:  6.558525085449219 0.5768755674362183 0.8579154014587402
CurrentTrain: epoch  2, batch    34 | loss: 7.9933162Losses:  5.378750801086426 0.380001962184906 0.8335061073303223
CurrentTrain: epoch  2, batch    35 | loss: 6.5922589Losses:  5.428510665893555 0.38561999797821045 0.8781707286834717
CurrentTrain: epoch  2, batch    36 | loss: 6.6923018Losses:  4.947178363800049 0.4239475131034851 0.8908374309539795
CurrentTrain: epoch  2, batch    37 | loss: 6.2619629Losses:  5.165358543395996 0.3154807686805725 0.8774155378341675
CurrentTrain: epoch  2, batch    38 | loss: 6.3582549Losses:  5.741053581237793 0.5016850233078003 0.8231481313705444
CurrentTrain: epoch  2, batch    39 | loss: 7.0658870Losses:  5.763225555419922 0.31390801072120667 0.9260307550430298
CurrentTrain: epoch  2, batch    40 | loss: 7.0031643Losses:  4.810642719268799 0.40980061888694763 0.816087007522583
CurrentTrain: epoch  2, batch    41 | loss: 6.0365305Losses:  5.5266265869140625 0.4010377526283264 0.8469682931900024
CurrentTrain: epoch  2, batch    42 | loss: 6.7746325Losses:  5.478782653808594 0.42135143280029297 0.8795422315597534
CurrentTrain: epoch  2, batch    43 | loss: 6.7796764Losses:  4.872734069824219 0.33806437253952026 0.8706485033035278
CurrentTrain: epoch  2, batch    44 | loss: 6.0814466Losses:  5.107238292694092 0.4555263817310333 0.8785211443901062
CurrentTrain: epoch  2, batch    45 | loss: 6.4412856Losses:  5.178221702575684 0.4930022358894348 0.880721926689148
CurrentTrain: epoch  2, batch    46 | loss: 6.5519462Losses:  4.977756500244141 0.35387155413627625 0.8625742197036743
CurrentTrain: epoch  2, batch    47 | loss: 6.1942019Losses:  4.403698921203613 0.22781889140605927 0.8284307718276978
CurrentTrain: epoch  2, batch    48 | loss: 5.4599485Losses:  5.043313980102539 0.3855079412460327 0.8638544082641602
CurrentTrain: epoch  2, batch    49 | loss: 6.2926764Losses:  4.972719192504883 0.37122514843940735 0.8387881517410278
CurrentTrain: epoch  2, batch    50 | loss: 6.1827326Losses:  4.693356037139893 0.3063657581806183 0.829840898513794
CurrentTrain: epoch  2, batch    51 | loss: 5.8295631Losses:  5.094335556030273 0.3612420856952667 0.8627908229827881
CurrentTrain: epoch  2, batch    52 | loss: 6.3183689Losses:  5.308350563049316 0.46011924743652344 0.8662443161010742
CurrentTrain: epoch  2, batch    53 | loss: 6.6347141Losses:  4.82463264465332 0.4238567650318146 0.886736273765564
CurrentTrain: epoch  2, batch    54 | loss: 6.1352258Losses:  5.0909223556518555 0.31148624420166016 0.8491348028182983
CurrentTrain: epoch  2, batch    55 | loss: 6.2515435Losses:  4.7247490882873535 0.3730376958847046 0.851039469242096
CurrentTrain: epoch  2, batch    56 | loss: 5.9488263Losses:  5.070278167724609 0.40817880630493164 0.8471817970275879
CurrentTrain: epoch  2, batch    57 | loss: 6.3256388Losses:  5.641408443450928 0.40674731135368347 0.8681331872940063
CurrentTrain: epoch  2, batch    58 | loss: 6.9162889Losses:  4.584556579589844 0.30454742908477783 0.8459875583648682
CurrentTrain: epoch  2, batch    59 | loss: 5.7350912Losses:  4.914025783538818 0.38816720247268677 0.8629592657089233
CurrentTrain: epoch  2, batch    60 | loss: 6.1651525Losses:  5.046131610870361 0.4083535373210907 0.8799169063568115
CurrentTrain: epoch  2, batch    61 | loss: 6.3344021Losses:  4.8107781410217285 0.3295137286186218 0.8832094669342041
CurrentTrain: epoch  2, batch    62 | loss: 6.0235014Losses:  4.73166036605835 0.2826106548309326 0.8317188620567322
CurrentTrain: epoch  3, batch     0 | loss: 5.8459897Losses:  5.0214948654174805 0.2872042953968048 0.870412290096283
CurrentTrain: epoch  3, batch     1 | loss: 6.1791115Losses:  5.804373264312744 0.46430936455726624 0.9141269326210022
CurrentTrain: epoch  3, batch     2 | loss: 7.1828094Losses:  5.097871780395508 0.37174224853515625 0.8591049313545227
CurrentTrain: epoch  3, batch     3 | loss: 6.3287191Losses:  5.136464595794678 0.36980360746383667 0.8412193059921265
CurrentTrain: epoch  3, batch     4 | loss: 6.3474874Losses:  4.719252586364746 0.25917327404022217 0.8140321969985962
CurrentTrain: epoch  3, batch     5 | loss: 5.7924581Losses:  4.719983100891113 0.41242343187332153 0.8586269021034241
CurrentTrain: epoch  3, batch     6 | loss: 5.9910336Losses:  5.449161529541016 0.38051503896713257 0.8715822100639343
CurrentTrain: epoch  3, batch     7 | loss: 6.7012587Losses:  5.0062971115112305 0.439445436000824 0.8678527474403381
CurrentTrain: epoch  3, batch     8 | loss: 6.3135953Losses:  4.780364990234375 0.3624155521392822 0.8478229641914368
CurrentTrain: epoch  3, batch     9 | loss: 5.9906034Losses:  5.254399299621582 0.4084608554840088 0.873023271560669
CurrentTrain: epoch  3, batch    10 | loss: 6.5358829Losses:  4.731679916381836 0.2818431854248047 0.8517550230026245
CurrentTrain: epoch  3, batch    11 | loss: 5.8652782Losses:  4.847159385681152 0.18410013616085052 0.8377379179000854
CurrentTrain: epoch  3, batch    12 | loss: 5.8689976Losses:  5.47158145904541 0.45789602398872375 0.8302011489868164
CurrentTrain: epoch  3, batch    13 | loss: 6.7596788Losses:  5.15099573135376 0.29423314332962036 0.8636425733566284
CurrentTrain: epoch  3, batch    14 | loss: 6.3088717Losses:  4.860198974609375 0.3490750193595886 0.8722344040870667
CurrentTrain: epoch  3, batch    15 | loss: 6.0815082Losses:  5.281578540802002 0.44315195083618164 0.8801472187042236
CurrentTrain: epoch  3, batch    16 | loss: 6.6048775Losses:  4.740599632263184 0.24135299026966095 0.84511798620224
CurrentTrain: epoch  3, batch    17 | loss: 5.8270707Losses:  4.727566719055176 0.39345118403434753 0.8415172696113586
CurrentTrain: epoch  3, batch    18 | loss: 5.9625354Losses:  4.678704261779785 0.2234194427728653 0.8847795128822327
CurrentTrain: epoch  3, batch    19 | loss: 5.7869034Losses:  4.584011077880859 0.31458574533462524 0.8256018757820129
CurrentTrain: epoch  3, batch    20 | loss: 5.7241988Losses:  4.907840251922607 0.2718487083911896 0.8545372486114502
CurrentTrain: epoch  3, batch    21 | loss: 6.0342264Losses:  4.7466840744018555 0.332161545753479 0.8399773836135864
CurrentTrain: epoch  3, batch    22 | loss: 5.9188228Losses:  4.993548393249512 0.42927804589271545 0.8604589104652405
CurrentTrain: epoch  3, batch    23 | loss: 6.2832851Losses:  4.679327487945557 0.34900009632110596 0.8660985231399536
CurrentTrain: epoch  3, batch    24 | loss: 5.8944259Losses:  4.704950332641602 0.3939342498779297 0.8215903639793396
CurrentTrain: epoch  3, batch    25 | loss: 5.9204750Losses:  4.9944047927856445 0.25024083256721497 0.8496577143669128
CurrentTrain: epoch  3, batch    26 | loss: 6.0943031Losses:  4.614322662353516 0.3596153259277344 0.8356459140777588
CurrentTrain: epoch  3, batch    27 | loss: 5.8095837Losses:  4.896815776824951 0.31124359369277954 0.8536628484725952
CurrentTrain: epoch  3, batch    28 | loss: 6.0617223Losses:  5.262818336486816 0.4086934030056 0.8225048780441284
CurrentTrain: epoch  3, batch    29 | loss: 6.4940166Losses:  4.478832244873047 0.3335902690887451 0.8572638630867004
CurrentTrain: epoch  3, batch    30 | loss: 5.6696868Losses:  4.659425258636475 0.3783376216888428 0.839293360710144
CurrentTrain: epoch  3, batch    31 | loss: 5.8770561Losses:  4.649844646453857 0.31056636571884155 0.8677123188972473
CurrentTrain: epoch  3, batch    32 | loss: 5.8281236Losses:  5.0995259284973145 0.3400052487850189 0.8772748708724976
CurrentTrain: epoch  3, batch    33 | loss: 6.3168063Losses:  4.663410663604736 0.31414568424224854 0.7741087675094604
CurrentTrain: epoch  3, batch    34 | loss: 5.7516651Losses:  5.263314247131348 0.39751216769218445 0.8354330062866211
CurrentTrain: epoch  3, batch    35 | loss: 6.4962592Losses:  5.070401191711426 0.4137963652610779 0.8099960088729858
CurrentTrain: epoch  3, batch    36 | loss: 6.2941937Losses:  4.631722450256348 0.3304504156112671 0.8123222589492798
CurrentTrain: epoch  3, batch    37 | loss: 5.7744951Losses:  4.46821403503418 0.3357847332954407 0.8191626071929932
CurrentTrain: epoch  3, batch    38 | loss: 5.6231613Losses:  5.041364669799805 0.28301820158958435 0.9107711911201477
CurrentTrain: epoch  3, batch    39 | loss: 6.2351542Losses:  4.783390522003174 0.18553340435028076 0.8263128399848938
CurrentTrain: epoch  3, batch    40 | loss: 5.7952371Losses:  4.561838626861572 0.24602901935577393 0.78769850730896
CurrentTrain: epoch  3, batch    41 | loss: 5.5955658Losses:  4.692999839782715 0.2682473063468933 0.8151404857635498
CurrentTrain: epoch  3, batch    42 | loss: 5.7763872Losses:  4.445395469665527 0.24255993962287903 0.8364326357841492
CurrentTrain: epoch  3, batch    43 | loss: 5.5243878Losses:  4.695064544677734 0.3169044256210327 0.7964613437652588
CurrentTrain: epoch  3, batch    44 | loss: 5.8084307Losses:  4.506042957305908 0.2918967008590698 0.8438361883163452
CurrentTrain: epoch  3, batch    45 | loss: 5.6417761Losses:  4.539556503295898 0.29110071063041687 0.9059967994689941
CurrentTrain: epoch  3, batch    46 | loss: 5.7366538Losses:  4.508810043334961 0.23771964013576508 0.8399078845977783
CurrentTrain: epoch  3, batch    47 | loss: 5.5864372Losses:  5.278295516967773 0.3385700583457947 0.8414851427078247
CurrentTrain: epoch  3, batch    48 | loss: 6.4583507Losses:  4.644696235656738 0.32260721921920776 0.8321107625961304
CurrentTrain: epoch  3, batch    49 | loss: 5.7994142Losses:  5.046875953674316 0.214557945728302 0.8766162395477295
CurrentTrain: epoch  3, batch    50 | loss: 6.1380501Losses:  4.472592353820801 0.29006120562553406 0.8360487222671509
CurrentTrain: epoch  3, batch    51 | loss: 5.5987020Losses:  4.557586669921875 0.27897363901138306 0.8319014310836792
CurrentTrain: epoch  3, batch    52 | loss: 5.6684618Losses:  4.764752388000488 0.158368781208992 0.8323291540145874
CurrentTrain: epoch  3, batch    53 | loss: 5.7554502Losses:  4.741920471191406 0.18253685534000397 0.8297959566116333
CurrentTrain: epoch  3, batch    54 | loss: 5.7542534Losses:  4.441378593444824 0.2998008131980896 0.7875249981880188
CurrentTrain: epoch  3, batch    55 | loss: 5.5287046Losses:  4.81258487701416 0.30508896708488464 0.844591498374939
CurrentTrain: epoch  3, batch    56 | loss: 5.9622655Losses:  4.954634666442871 0.39079713821411133 0.8401975035667419
CurrentTrain: epoch  3, batch    57 | loss: 6.1856294Losses:  4.511021614074707 0.27803748846054077 0.8005231022834778
CurrentTrain: epoch  3, batch    58 | loss: 5.5895824Losses:  4.457846641540527 0.2722608149051666 0.7870392799377441
CurrentTrain: epoch  3, batch    59 | loss: 5.5171466Losses:  4.606119155883789 0.34757381677627563 0.8060237169265747
CurrentTrain: epoch  3, batch    60 | loss: 5.7597165Losses:  4.919521808624268 0.40438395738601685 0.7709915637969971
CurrentTrain: epoch  3, batch    61 | loss: 6.0948973Losses:  4.483868598937988 0.17128148674964905 0.7886962294578552
CurrentTrain: epoch  3, batch    62 | loss: 5.4438462Losses:  4.5128679275512695 0.2513398230075836 0.8301899433135986
CurrentTrain: epoch  4, batch     0 | loss: 5.5943975Losses:  4.499974727630615 0.2873624563217163 0.8555153608322144
CurrentTrain: epoch  4, batch     1 | loss: 5.6428528Losses:  4.435812950134277 0.2662689983844757 0.8422241806983948
CurrentTrain: epoch  4, batch     2 | loss: 5.5443063Losses:  4.631375312805176 0.3140360116958618 0.8047177791595459
CurrentTrain: epoch  4, batch     3 | loss: 5.7501287Losses:  4.538216590881348 0.2765420377254486 0.8490214347839355
CurrentTrain: epoch  4, batch     4 | loss: 5.6637802Losses:  4.571105003356934 0.20584672689437866 0.8344089984893799
CurrentTrain: epoch  4, batch     5 | loss: 5.6113605Losses:  4.448813438415527 0.24576738476753235 0.8591516017913818
CurrentTrain: epoch  4, batch     6 | loss: 5.5537329Losses:  4.738825798034668 0.3239216208457947 0.7517915964126587
CurrentTrain: epoch  4, batch     7 | loss: 5.8145390Losses:  4.610539436340332 0.26227089762687683 0.8372750878334045
CurrentTrain: epoch  4, batch     8 | loss: 5.7100854Losses:  4.671753406524658 0.3753088116645813 0.8465381860733032
CurrentTrain: epoch  4, batch     9 | loss: 5.8936005Losses:  4.3932037353515625 0.2603932023048401 0.7784834504127502
CurrentTrain: epoch  4, batch    10 | loss: 5.4320803Losses:  4.475440502166748 0.19540852308273315 0.8584956526756287
CurrentTrain: epoch  4, batch    11 | loss: 5.5293446Losses:  4.373466968536377 0.2925065755844116 0.7921935319900513
CurrentTrain: epoch  4, batch    12 | loss: 5.4581671Losses:  4.432679653167725 0.2703692615032196 0.8036364316940308
CurrentTrain: epoch  4, batch    13 | loss: 5.5066853Losses:  4.4745025634765625 0.21458855271339417 0.8387455940246582
CurrentTrain: epoch  4, batch    14 | loss: 5.5278368Losses:  4.666390419006348 0.3079181909561157 0.8799014091491699
CurrentTrain: epoch  4, batch    15 | loss: 5.8542099Losses:  5.06781530380249 0.21914106607437134 0.8297842741012573
CurrentTrain: epoch  4, batch    16 | loss: 6.1167407Losses:  4.4039530754089355 0.1939578801393509 0.808847188949585
CurrentTrain: epoch  4, batch    17 | loss: 5.4067583Losses:  4.722837924957275 0.29011231660842896 0.8294011950492859
CurrentTrain: epoch  4, batch    18 | loss: 5.8423514Losses:  4.456448554992676 0.26366204023361206 0.8248274326324463
CurrentTrain: epoch  4, batch    19 | loss: 5.5449381Losses:  4.39731502532959 0.2715182304382324 0.8300493359565735
CurrentTrain: epoch  4, batch    20 | loss: 5.4988828Losses:  4.513860702514648 0.2088712900876999 0.8779909610748291
CurrentTrain: epoch  4, batch    21 | loss: 5.6007233Losses:  4.560859680175781 0.28634119033813477 0.8183771371841431
CurrentTrain: epoch  4, batch    22 | loss: 5.6655779Losses:  4.561957359313965 0.26470428705215454 0.7973810434341431
CurrentTrain: epoch  4, batch    23 | loss: 5.6240425Losses:  4.36638069152832 0.1929558515548706 0.8238568305969238
CurrentTrain: epoch  4, batch    24 | loss: 5.3831935Losses:  4.763607025146484 0.25632792711257935 0.8502281904220581
CurrentTrain: epoch  4, batch    25 | loss: 5.8701634Losses:  4.545527458190918 0.24162936210632324 0.8436222076416016
CurrentTrain: epoch  4, batch    26 | loss: 5.6307793Losses:  4.794291019439697 0.2281254678964615 0.9050956964492798
CurrentTrain: epoch  4, batch    27 | loss: 5.9275122Losses:  4.422985076904297 0.21412262320518494 0.8328675031661987
CurrentTrain: epoch  4, batch    28 | loss: 5.4699755Losses:  4.361073017120361 0.24940159916877747 0.8133079409599304
CurrentTrain: epoch  4, batch    29 | loss: 5.4237823Losses:  4.334875106811523 0.19535422325134277 0.7971706390380859
CurrentTrain: epoch  4, batch    30 | loss: 5.3274002Losses:  4.4736738204956055 0.2526259422302246 0.8258495330810547
CurrentTrain: epoch  4, batch    31 | loss: 5.5521493Losses:  4.498095512390137 0.21209394931793213 0.8099769949913025
CurrentTrain: epoch  4, batch    32 | loss: 5.5201664Losses:  4.180022239685059 0.1687413603067398 0.7990981340408325
CurrentTrain: epoch  4, batch    33 | loss: 5.1478615Losses:  4.537309646606445 0.2890792787075043 0.814223051071167
CurrentTrain: epoch  4, batch    34 | loss: 5.6406116Losses:  4.42962646484375 0.2827160954475403 0.8180361986160278
CurrentTrain: epoch  4, batch    35 | loss: 5.5303788Losses:  4.50046968460083 0.18570759892463684 0.8388582468032837
CurrentTrain: epoch  4, batch    36 | loss: 5.5250354Losses:  4.544127464294434 0.20715323090553284 0.8146044611930847
CurrentTrain: epoch  4, batch    37 | loss: 5.5658851Losses:  4.391529083251953 0.26719409227371216 0.7500176429748535
CurrentTrain: epoch  4, batch    38 | loss: 5.4087410Losses:  4.3822784423828125 0.24551430344581604 0.8364869356155396
CurrentTrain: epoch  4, batch    39 | loss: 5.4642797Losses:  4.2918853759765625 0.17167365550994873 0.8185155391693115
CurrentTrain: epoch  4, batch    40 | loss: 5.2820749Losses:  4.394895553588867 0.22921112179756165 0.7941470146179199
CurrentTrain: epoch  4, batch    41 | loss: 5.4182539Losses:  4.379204750061035 0.20409008860588074 0.7474735975265503
CurrentTrain: epoch  4, batch    42 | loss: 5.3307686Losses:  4.905203819274902 0.28195446729660034 0.8188731670379639
CurrentTrain: epoch  4, batch    43 | loss: 6.0060310Losses:  4.345398902893066 0.18681712448596954 0.8097623586654663
CurrentTrain: epoch  4, batch    44 | loss: 5.3419785Losses:  4.361596584320068 0.12053646147251129 0.7991601824760437
CurrentTrain: epoch  4, batch    45 | loss: 5.2812929Losses:  4.289073467254639 0.20699328184127808 0.8089820146560669
CurrentTrain: epoch  4, batch    46 | loss: 5.3050485Losses:  4.718503952026367 0.28502726554870605 0.8108690977096558
CurrentTrain: epoch  4, batch    47 | loss: 5.8144007Losses:  4.407919883728027 0.2138872742652893 0.8649208545684814
CurrentTrain: epoch  4, batch    48 | loss: 5.4867277Losses:  4.413108825683594 0.23709841072559357 0.8048439621925354
CurrentTrain: epoch  4, batch    49 | loss: 5.4550509Losses:  4.452797889709473 0.20202262699604034 0.7360614538192749
CurrentTrain: epoch  4, batch    50 | loss: 5.3908820Losses:  4.298198699951172 0.18160337209701538 0.8204361796379089
CurrentTrain: epoch  4, batch    51 | loss: 5.3002381Losses:  4.406002521514893 0.2121630609035492 0.7714219093322754
CurrentTrain: epoch  4, batch    52 | loss: 5.3895874Losses:  4.316230773925781 0.13314703106880188 0.7874684929847717
CurrentTrain: epoch  4, batch    53 | loss: 5.2368464Losses:  4.415244102478027 0.2746138572692871 0.8410515785217285
CurrentTrain: epoch  4, batch    54 | loss: 5.5309095Losses:  4.271511554718018 0.23133251070976257 0.8091869950294495
CurrentTrain: epoch  4, batch    55 | loss: 5.3120308Losses:  4.446255683898926 0.25486552715301514 0.8126394748687744
CurrentTrain: epoch  4, batch    56 | loss: 5.5137606Losses:  4.283555030822754 0.1906304657459259 0.8394432067871094
CurrentTrain: epoch  4, batch    57 | loss: 5.3136287Losses:  4.125677585601807 0.22203689813613892 0.7487636208534241
CurrentTrain: epoch  4, batch    58 | loss: 5.0964780Losses:  4.838017463684082 0.16346639394760132 0.8151012063026428
CurrentTrain: epoch  4, batch    59 | loss: 5.8165851Losses:  4.232396125793457 0.2453431934118271 0.790502667427063
CurrentTrain: epoch  4, batch    60 | loss: 5.2682419Losses:  4.134984016418457 0.16369593143463135 0.8218791484832764
CurrentTrain: epoch  4, batch    61 | loss: 5.1205587Losses:  4.398166656494141 0.1722661852836609 0.8476481437683105
CurrentTrain: epoch  4, batch    62 | loss: 5.4180808Losses:  4.303501129150391 0.21890178322792053 0.7936747074127197
CurrentTrain: epoch  5, batch     0 | loss: 5.3160772Losses:  4.357880592346191 0.18637627363204956 0.7833099961280823
CurrentTrain: epoch  5, batch     1 | loss: 5.3275666Losses:  4.334059715270996 0.25927233695983887 0.8142134547233582
CurrentTrain: epoch  5, batch     2 | loss: 5.4075456Losses:  4.379062652587891 0.21803505718708038 0.7560681104660034
CurrentTrain: epoch  5, batch     3 | loss: 5.3531661Losses:  4.303327560424805 0.20069067180156708 0.8474255800247192
CurrentTrain: epoch  5, batch     4 | loss: 5.3514438Losses:  4.258380889892578 0.1713964343070984 0.8272915482521057
CurrentTrain: epoch  5, batch     5 | loss: 5.2570686Losses:  4.336365222930908 0.22789190709590912 0.7929827570915222
CurrentTrain: epoch  5, batch     6 | loss: 5.3572397Losses:  4.247649192810059 0.24021786451339722 0.8022193908691406
CurrentTrain: epoch  5, batch     7 | loss: 5.2900863Losses:  4.428219795227051 0.19993117451667786 0.852243185043335
CurrentTrain: epoch  5, batch     8 | loss: 5.4803944Losses:  4.224491119384766 0.21533681452274323 0.8097614645957947
CurrentTrain: epoch  5, batch     9 | loss: 5.2495894Losses:  4.387594223022461 0.11641500890254974 0.8276526927947998
CurrentTrain: epoch  5, batch    10 | loss: 5.3316622Losses:  4.263214111328125 0.13250985741615295 0.858234167098999
CurrentTrain: epoch  5, batch    11 | loss: 5.2539577Losses:  4.206913948059082 0.19878648221492767 0.7641607522964478
CurrentTrain: epoch  5, batch    12 | loss: 5.1698608Losses:  4.210629463195801 0.17219385504722595 0.8282136917114258
CurrentTrain: epoch  5, batch    13 | loss: 5.2110372Losses:  4.236879825592041 0.17010155320167542 0.8174569606781006
CurrentTrain: epoch  5, batch    14 | loss: 5.2244387Losses:  4.292651176452637 0.2438422441482544 0.8259515762329102
CurrentTrain: epoch  5, batch    15 | loss: 5.3624449Losses:  4.222522735595703 0.21633902192115784 0.7766731381416321
CurrentTrain: epoch  5, batch    16 | loss: 5.2155352Losses:  4.13129997253418 0.2182735651731491 0.7460141777992249
CurrentTrain: epoch  5, batch    17 | loss: 5.0955877Losses:  4.247035026550293 0.18805338442325592 0.8161745071411133
CurrentTrain: epoch  5, batch    18 | loss: 5.2512631Losses:  4.321500778198242 0.18596120178699493 0.8062529563903809
CurrentTrain: epoch  5, batch    19 | loss: 5.3137150Losses:  4.172087669372559 0.17386102676391602 0.8238753080368042
CurrentTrain: epoch  5, batch    20 | loss: 5.1698241Losses:  4.38764762878418 0.13678953051567078 0.7978531122207642
CurrentTrain: epoch  5, batch    21 | loss: 5.3222899Losses:  4.175806045532227 0.24156978726387024 0.8225987553596497
CurrentTrain: epoch  5, batch    22 | loss: 5.2399750Losses:  4.307261943817139 0.14921912550926208 0.7599862813949585
CurrentTrain: epoch  5, batch    23 | loss: 5.2164674Losses:  4.320234775543213 0.20781917870044708 0.7621873021125793
CurrentTrain: epoch  5, batch    24 | loss: 5.2902412Losses:  4.261007308959961 0.1176978275179863 0.7775331735610962
CurrentTrain: epoch  5, batch    25 | loss: 5.1562381Losses:  4.312504768371582 0.23320288956165314 0.8075813055038452
CurrentTrain: epoch  5, batch    26 | loss: 5.3532891Losses:  4.164745807647705 0.2557203471660614 0.7804943919181824
CurrentTrain: epoch  5, batch    27 | loss: 5.2009602Losses:  4.1852006912231445 0.18220321834087372 0.7244913578033447
CurrentTrain: epoch  5, batch    28 | loss: 5.0918951Losses:  4.201858997344971 0.2177552580833435 0.7965790033340454
CurrentTrain: epoch  5, batch    29 | loss: 5.2161932Losses:  4.309350967407227 0.2080533355474472 0.8214467167854309
CurrentTrain: epoch  5, batch    30 | loss: 5.3388510Losses:  4.252172470092773 0.15157561004161835 0.7936888933181763
CurrentTrain: epoch  5, batch    31 | loss: 5.1974368Losses:  4.153611660003662 0.18003888428211212 0.8163872957229614
CurrentTrain: epoch  5, batch    32 | loss: 5.1500378Losses:  4.153794288635254 0.10231316089630127 0.8312875628471375
CurrentTrain: epoch  5, batch    33 | loss: 5.0873947Losses:  4.2953948974609375 0.1636822670698166 0.7652802467346191
CurrentTrain: epoch  5, batch    34 | loss: 5.2243576Losses:  4.182962417602539 0.19432903826236725 0.7823788523674011
CurrentTrain: epoch  5, batch    35 | loss: 5.1596704Losses:  4.198565483093262 0.1400619000196457 0.8927822113037109
CurrentTrain: epoch  5, batch    36 | loss: 5.2314095Losses:  4.137686252593994 0.16398334503173828 0.8061007857322693
CurrentTrain: epoch  5, batch    37 | loss: 5.1077704Losses:  4.202061653137207 0.16999241709709167 0.8082530498504639
CurrentTrain: epoch  5, batch    38 | loss: 5.1803074Losses:  4.117941856384277 0.15129446983337402 0.8490942716598511
CurrentTrain: epoch  5, batch    39 | loss: 5.1183310Losses:  4.245480537414551 0.20508244633674622 0.7696783542633057
CurrentTrain: epoch  5, batch    40 | loss: 5.2202415Losses:  4.207574844360352 0.15523648262023926 0.7220298051834106
CurrentTrain: epoch  5, batch    41 | loss: 5.0848408Losses:  4.157177448272705 0.19658207893371582 0.7535704970359802
CurrentTrain: epoch  5, batch    42 | loss: 5.1073303Losses:  4.2270612716674805 0.20594926178455353 0.7484901547431946
CurrentTrain: epoch  5, batch    43 | loss: 5.1815009Losses:  4.131611347198486 0.18638208508491516 0.7837169170379639
CurrentTrain: epoch  5, batch    44 | loss: 5.1017103Losses:  4.137591361999512 0.18769046664237976 0.7544505596160889
CurrentTrain: epoch  5, batch    45 | loss: 5.0797319Losses:  4.1923112869262695 0.18121902644634247 0.7979249954223633
CurrentTrain: epoch  5, batch    46 | loss: 5.1714554Losses:  4.217138767242432 0.14387139678001404 0.809647798538208
CurrentTrain: epoch  5, batch    47 | loss: 5.1706581Losses:  4.164894104003906 0.17091628909111023 0.8028509616851807
CurrentTrain: epoch  5, batch    48 | loss: 5.1386614Losses:  4.234255790710449 0.14633823931217194 0.8128728270530701
CurrentTrain: epoch  5, batch    49 | loss: 5.1934671Losses:  4.1226654052734375 0.19120970368385315 0.7733306884765625
CurrentTrain: epoch  5, batch    50 | loss: 5.0872059Losses:  4.2266316413879395 0.1689063310623169 0.8039251565933228
CurrentTrain: epoch  5, batch    51 | loss: 5.1994629Losses:  4.201708793640137 0.143135666847229 0.7609014511108398
CurrentTrain: epoch  5, batch    52 | loss: 5.1057458Losses:  4.234872817993164 0.23581483960151672 0.7661359310150146
CurrentTrain: epoch  5, batch    53 | loss: 5.2368240Losses:  4.112056732177734 0.1874193251132965 0.7393136024475098
CurrentTrain: epoch  5, batch    54 | loss: 5.0387897Losses:  4.117434501647949 0.1997719705104828 0.85215824842453
CurrentTrain: epoch  5, batch    55 | loss: 5.1693645Losses:  4.115864276885986 0.16953089833259583 0.7995630502700806
CurrentTrain: epoch  5, batch    56 | loss: 5.0849581Losses:  4.206432342529297 0.1272381991147995 0.8511202335357666
CurrentTrain: epoch  5, batch    57 | loss: 5.1847906Losses:  4.133816719055176 0.17082788050174713 0.835253119468689
CurrentTrain: epoch  5, batch    58 | loss: 5.1398978Losses:  4.139193534851074 0.17897731065750122 0.7636409401893616
CurrentTrain: epoch  5, batch    59 | loss: 5.0818119Losses:  4.628244876861572 0.29531556367874146 0.8608776330947876
CurrentTrain: epoch  5, batch    60 | loss: 5.7844381Losses:  4.170933723449707 0.11301714181900024 0.8033595681190491
CurrentTrain: epoch  5, batch    61 | loss: 5.0873103Losses:  4.082157135009766 0.07549464702606201 0.8376657962799072
CurrentTrain: epoch  5, batch    62 | loss: 4.9953175Losses:  4.152613639831543 0.11391392350196838 0.7700818181037903
CurrentTrain: epoch  6, batch     0 | loss: 5.0366096Losses:  4.229747772216797 0.1296171396970749 0.8439242839813232
CurrentTrain: epoch  6, batch     1 | loss: 5.2032890Losses:  4.146745204925537 0.16076651215553284 0.8060466051101685
CurrentTrain: epoch  6, batch     2 | loss: 5.1135583Losses:  4.120081424713135 0.09032337367534637 0.796538770198822
CurrentTrain: epoch  6, batch     3 | loss: 5.0069437Losses:  4.176212310791016 0.1392112821340561 0.7826268076896667
CurrentTrain: epoch  6, batch     4 | loss: 5.0980501Losses:  4.108978271484375 0.19132544100284576 0.7549397349357605
CurrentTrain: epoch  6, batch     5 | loss: 5.0552435Losses:  4.204235076904297 0.18231752514839172 0.7692520022392273
CurrentTrain: epoch  6, batch     6 | loss: 5.1558046Losses:  4.119624614715576 0.11973543465137482 0.8074473738670349
CurrentTrain: epoch  6, batch     7 | loss: 5.0468073Losses:  4.121172904968262 0.1719791442155838 0.7111363410949707
CurrentTrain: epoch  6, batch     8 | loss: 5.0042882Losses:  4.107968330383301 0.11052794754505157 0.7914696931838989
CurrentTrain: epoch  6, batch     9 | loss: 5.0099659Losses:  4.080966949462891 0.13306081295013428 0.741768479347229
CurrentTrain: epoch  6, batch    10 | loss: 4.9557962Losses:  4.112580299377441 0.10632242262363434 0.840040922164917
CurrentTrain: epoch  6, batch    11 | loss: 5.0589437Losses:  4.15922737121582 0.11044637858867645 0.8173828721046448
CurrentTrain: epoch  6, batch    12 | loss: 5.0870566Losses:  4.161350250244141 0.1243537962436676 0.7537953853607178
CurrentTrain: epoch  6, batch    13 | loss: 5.0394993Losses:  4.032014846801758 0.14774258434772491 0.8273884654045105
CurrentTrain: epoch  6, batch    14 | loss: 5.0071459Losses:  4.177976131439209 0.19408859312534332 0.813052773475647
CurrentTrain: epoch  6, batch    15 | loss: 5.1851172Losses:  4.123079299926758 0.16420498490333557 0.7658424377441406
CurrentTrain: epoch  6, batch    16 | loss: 5.0531268Losses:  4.119768142700195 0.19865788519382477 0.7480576038360596
CurrentTrain: epoch  6, batch    17 | loss: 5.0664835Losses:  4.121491432189941 0.15403905510902405 0.8136712908744812
CurrentTrain: epoch  6, batch    18 | loss: 5.0892015Losses:  4.1211090087890625 0.19840273261070251 0.7854235172271729
CurrentTrain: epoch  6, batch    19 | loss: 5.1049356Losses:  4.157668590545654 0.1892102062702179 0.7667118310928345
CurrentTrain: epoch  6, batch    20 | loss: 5.1135907Losses:  4.133447647094727 0.1593582034111023 0.8081257343292236
CurrentTrain: epoch  6, batch    21 | loss: 5.1009312Losses:  4.032388210296631 0.1975257247686386 0.8017144203186035
CurrentTrain: epoch  6, batch    22 | loss: 5.0316281Losses:  4.122859001159668 0.11403817683458328 0.8734691143035889
CurrentTrain: epoch  6, batch    23 | loss: 5.1103659Losses:  4.087454795837402 0.18588712811470032 0.7867964506149292
CurrentTrain: epoch  6, batch    24 | loss: 5.0601387Losses:  4.0770487785339355 0.1727045476436615 0.8081444501876831
CurrentTrain: epoch  6, batch    25 | loss: 5.0578980Losses:  4.099698066711426 0.1263045072555542 0.7660999298095703
CurrentTrain: epoch  6, batch    26 | loss: 4.9921026Losses:  4.044005870819092 0.10615581274032593 0.7717065215110779
CurrentTrain: epoch  6, batch    27 | loss: 4.9218683Losses:  4.1271162033081055 0.16777266561985016 0.768203854560852
CurrentTrain: epoch  6, batch    28 | loss: 5.0630927Losses:  4.564982891082764 0.2616110146045685 0.7753641605377197
CurrentTrain: epoch  6, batch    29 | loss: 5.6019583Losses:  4.08829402923584 0.14045584201812744 0.7879593968391418
CurrentTrain: epoch  6, batch    30 | loss: 5.0167093Losses:  4.1515631675720215 0.09720119833946228 0.7924708127975464
CurrentTrain: epoch  6, batch    31 | loss: 5.0412354Losses:  4.112259864807129 0.15201351046562195 0.8076782822608948
CurrentTrain: epoch  6, batch    32 | loss: 5.0719514Losses:  4.079667091369629 0.150333434343338 0.7420545816421509
CurrentTrain: epoch  6, batch    33 | loss: 4.9720550Losses:  4.1130452156066895 0.16259419918060303 0.7808841466903687
CurrentTrain: epoch  6, batch    34 | loss: 5.0565238Losses:  4.0990824699401855 0.16116118431091309 0.7783288955688477
CurrentTrain: epoch  6, batch    35 | loss: 5.0385723Losses:  4.07603645324707 0.13984999060630798 0.8175247311592102
CurrentTrain: epoch  6, batch    36 | loss: 5.0334115Losses:  4.1718597412109375 0.17261391878128052 0.7566930651664734
CurrentTrain: epoch  6, batch    37 | loss: 5.1011667Losses:  4.110104560852051 0.17658740282058716 0.7341567277908325
CurrentTrain: epoch  6, batch    38 | loss: 5.0208488Losses:  4.000693321228027 0.11493544280529022 0.7245529890060425
CurrentTrain: epoch  6, batch    39 | loss: 4.8401818Losses:  4.055822372436523 0.15189974009990692 0.7695963382720947
CurrentTrain: epoch  6, batch    40 | loss: 4.9773188Losses:  4.1365251541137695 0.14145515859127045 0.7901009321212769
CurrentTrain: epoch  6, batch    41 | loss: 5.0680814Losses:  4.113736152648926 0.12106898427009583 0.7643752694129944
CurrentTrain: epoch  6, batch    42 | loss: 4.9991803Losses:  4.10167932510376 0.11321605741977692 0.7687832713127136
CurrentTrain: epoch  6, batch    43 | loss: 4.9836783Losses:  4.11265230178833 0.16972075402736664 0.7468674182891846
CurrentTrain: epoch  6, batch    44 | loss: 5.0292406Losses:  4.069584846496582 0.128238707780838 0.8120583295822144
CurrentTrain: epoch  6, batch    45 | loss: 5.0098820Losses:  4.112824440002441 0.20258142054080963 0.7699813842773438
CurrentTrain: epoch  6, batch    46 | loss: 5.0853872Losses:  4.1392974853515625 0.10542990267276764 0.8569746017456055
CurrentTrain: epoch  6, batch    47 | loss: 5.1017022Losses:  4.044297695159912 0.14089715480804443 0.8515567779541016
CurrentTrain: epoch  6, batch    48 | loss: 5.0367517Losses:  4.04893684387207 0.136753648519516 0.7237802743911743
CurrentTrain: epoch  6, batch    49 | loss: 4.9094706Losses:  4.154848098754883 0.15302377939224243 0.7207050323486328
CurrentTrain: epoch  6, batch    50 | loss: 5.0285769Losses:  4.114175796508789 0.09984233230352402 0.7860916256904602
CurrentTrain: epoch  6, batch    51 | loss: 5.0001101Losses:  4.29615592956543 0.13402864336967468 0.8201599717140198
CurrentTrain: epoch  6, batch    52 | loss: 5.2503443Losses:  4.1125898361206055 0.16242575645446777 0.7132008075714111
CurrentTrain: epoch  6, batch    53 | loss: 4.9882164Losses:  4.0664167404174805 0.11667147278785706 0.8746825456619263
CurrentTrain: epoch  6, batch    54 | loss: 5.0577707Losses:  4.102017879486084 0.14072129130363464 0.7366756200790405
CurrentTrain: epoch  6, batch    55 | loss: 4.9794149Losses:  4.079765796661377 0.07928632944822311 0.7622687220573425
CurrentTrain: epoch  6, batch    56 | loss: 4.9213204Losses:  4.076999664306641 0.15601152181625366 0.6884942054748535
CurrentTrain: epoch  6, batch    57 | loss: 4.9215055Losses:  4.077204704284668 0.1479683220386505 0.7789266109466553
CurrentTrain: epoch  6, batch    58 | loss: 5.0040998Losses:  4.096351623535156 0.1586025357246399 0.7557286024093628
CurrentTrain: epoch  6, batch    59 | loss: 5.0106831Losses:  4.082347869873047 0.12026740610599518 0.7823659181594849
CurrentTrain: epoch  6, batch    60 | loss: 4.9849811Losses:  4.046956539154053 0.13629518449306488 0.8106428384780884
CurrentTrain: epoch  6, batch    61 | loss: 4.9938946Losses:  4.115511894226074 0.07937879115343094 0.6867212057113647
CurrentTrain: epoch  6, batch    62 | loss: 4.8816118Losses:  4.0497894287109375 0.15211670100688934 0.801754355430603
CurrentTrain: epoch  7, batch     0 | loss: 5.0036607Losses:  4.118257522583008 0.12360340356826782 0.7708551287651062
CurrentTrain: epoch  7, batch     1 | loss: 5.0127158Losses:  4.0620293617248535 0.11051543056964874 0.8392477631568909
CurrentTrain: epoch  7, batch     2 | loss: 5.0117927Losses:  4.08150577545166 0.13030359148979187 0.745943546295166
CurrentTrain: epoch  7, batch     3 | loss: 4.9577527Losses:  4.055035591125488 0.16731196641921997 0.7327257394790649
CurrentTrain: epoch  7, batch     4 | loss: 4.9550734Losses:  4.059328556060791 0.08031138777732849 0.7830359935760498
CurrentTrain: epoch  7, batch     5 | loss: 4.9226761Losses:  4.056427478790283 0.13210393488407135 0.833434522151947
CurrentTrain: epoch  7, batch     6 | loss: 5.0219660Losses:  4.066294193267822 0.1119636595249176 0.8011165857315063
CurrentTrain: epoch  7, batch     7 | loss: 4.9793744Losses:  4.066218376159668 0.12851236760616302 0.7226215600967407
CurrentTrain: epoch  7, batch     8 | loss: 4.9173522Losses:  4.162697792053223 0.1383969485759735 0.7978591918945312
CurrentTrain: epoch  7, batch     9 | loss: 5.0989537Losses:  4.092264175415039 0.17149502038955688 0.7613794803619385
CurrentTrain: epoch  7, batch    10 | loss: 5.0251389Losses:  4.095272064208984 0.11754970252513885 0.7720766067504883
CurrentTrain: epoch  7, batch    11 | loss: 4.9848986Losses:  4.147622108459473 0.1550769805908203 0.7893187999725342
CurrentTrain: epoch  7, batch    12 | loss: 5.0920181Losses:  4.148955821990967 0.17141084372997284 0.7824386358261108
CurrentTrain: epoch  7, batch    13 | loss: 5.1028056Losses:  4.065579414367676 0.14783382415771484 0.715169370174408
CurrentTrain: epoch  7, batch    14 | loss: 4.9285827Losses:  4.102995872497559 0.10963821411132812 0.7997019290924072
CurrentTrain: epoch  7, batch    15 | loss: 5.0123358Losses:  4.16678524017334 0.16851270198822021 0.7634639143943787
CurrentTrain: epoch  7, batch    16 | loss: 5.0987620Losses:  4.250572681427002 0.1334550380706787 0.7949233651161194
CurrentTrain: epoch  7, batch    17 | loss: 5.1789508Losses:  4.095603942871094 0.10938691347837448 0.7236790657043457
CurrentTrain: epoch  7, batch    18 | loss: 4.9286699Losses:  4.042236328125 0.14415109157562256 0.7799530029296875
CurrentTrain: epoch  7, batch    19 | loss: 4.9663405Losses:  4.084543228149414 0.12528778612613678 0.7853408455848694
CurrentTrain: epoch  7, batch    20 | loss: 4.9951720Losses:  4.087846755981445 0.08749543130397797 0.7883001565933228
CurrentTrain: epoch  7, batch    21 | loss: 4.9636421Losses:  4.102789878845215 0.09371378272771835 0.7614128589630127
CurrentTrain: epoch  7, batch    22 | loss: 4.9579163Losses:  4.137179374694824 0.08731895685195923 0.7655730247497559
CurrentTrain: epoch  7, batch    23 | loss: 4.9900713Losses:  4.016415596008301 0.12056003510951996 0.7257074117660522
CurrentTrain: epoch  7, batch    24 | loss: 4.8626833Losses:  4.0938310623168945 0.1710086464881897 0.7682257890701294
CurrentTrain: epoch  7, batch    25 | loss: 5.0330653Losses:  4.0062055587768555 0.04368408024311066 0.7562042474746704
CurrentTrain: epoch  7, batch    26 | loss: 4.8060937Losses:  4.101940631866455 0.15432070195674896 0.7646390199661255
CurrentTrain: epoch  7, batch    27 | loss: 5.0209002Losses:  4.037863731384277 0.14945702254772186 0.7714208364486694
CurrentTrain: epoch  7, batch    28 | loss: 4.9587417Losses:  4.072112560272217 0.1567034274339676 0.7556025981903076
CurrentTrain: epoch  7, batch    29 | loss: 4.9844189Losses:  4.074106693267822 0.1411176323890686 0.7423967123031616
CurrentTrain: epoch  7, batch    30 | loss: 4.9576211Losses:  4.138531684875488 0.09020183980464935 0.8222161531448364
CurrentTrain: epoch  7, batch    31 | loss: 5.0509496Losses:  4.122901439666748 0.1616280972957611 0.7389805316925049
CurrentTrain: epoch  7, batch    32 | loss: 5.0235100Losses:  3.976024627685547 0.08479738980531693 0.7689340710639954
CurrentTrain: epoch  7, batch    33 | loss: 4.8297563Losses:  4.087454795837402 0.12234389781951904 0.8193860054016113
CurrentTrain: epoch  7, batch    34 | loss: 5.0291848Losses:  4.0443315505981445 0.1414567232131958 0.7146838307380676
CurrentTrain: epoch  7, batch    35 | loss: 4.9004722Losses:  4.112367153167725 0.12387829273939133 0.8306644558906555
CurrentTrain: epoch  7, batch    36 | loss: 5.0669103Losses:  4.071667671203613 0.12352048605680466 0.8061761856079102
CurrentTrain: epoch  7, batch    37 | loss: 5.0013642Losses:  4.068754196166992 0.09706144034862518 0.688751220703125
CurrentTrain: epoch  7, batch    38 | loss: 4.8545671Losses:  4.0832390785217285 0.15228644013404846 0.7488985061645508
CurrentTrain: epoch  7, batch    39 | loss: 4.9844241Losses:  4.497525691986084 0.21476884186267853 0.7798932790756226
CurrentTrain: epoch  7, batch    40 | loss: 5.4921880Losses:  4.079263687133789 0.13334138691425323 0.7816965579986572
CurrentTrain: epoch  7, batch    41 | loss: 4.9943018Losses:  4.017695426940918 0.14124006032943726 0.7313554286956787
CurrentTrain: epoch  7, batch    42 | loss: 4.8902912Losses:  4.045792579650879 0.12121379375457764 0.700211226940155
CurrentTrain: epoch  7, batch    43 | loss: 4.8672175Losses:  4.0305962562561035 0.12121251225471497 0.7384123802185059
CurrentTrain: epoch  7, batch    44 | loss: 4.8902211Losses:  4.042294025421143 0.13665911555290222 0.7917249202728271
CurrentTrain: epoch  7, batch    45 | loss: 4.9706783Losses:  4.069083213806152 0.12703825533390045 0.7455800175666809
CurrentTrain: epoch  7, batch    46 | loss: 4.9417019Losses:  4.104179382324219 0.11045041680335999 0.7528635263442993
CurrentTrain: epoch  7, batch    47 | loss: 4.9674931Losses:  4.066175937652588 0.13854999840259552 0.787289023399353
CurrentTrain: epoch  7, batch    48 | loss: 4.9920149Losses:  4.031392574310303 0.14581435918807983 0.7557041645050049
CurrentTrain: epoch  7, batch    49 | loss: 4.9329109Losses:  4.104480266571045 0.13539892435073853 0.7927209138870239
CurrentTrain: epoch  7, batch    50 | loss: 5.0325999Losses:  4.142087459564209 0.1040717363357544 0.7534542083740234
CurrentTrain: epoch  7, batch    51 | loss: 4.9996133Losses:  4.027270317077637 0.07623101770877838 0.7660221457481384
CurrentTrain: epoch  7, batch    52 | loss: 4.8695235Losses:  4.052021503448486 0.10669614374637604 0.722481369972229
CurrentTrain: epoch  7, batch    53 | loss: 4.8811989Losses:  4.089542388916016 0.146408349275589 0.7462212443351746
CurrentTrain: epoch  7, batch    54 | loss: 4.9821720Losses:  4.07582950592041 0.14032645523548126 0.7794620990753174
CurrentTrain: epoch  7, batch    55 | loss: 4.9956179Losses:  4.088895797729492 0.12383922189474106 0.809707522392273
CurrentTrain: epoch  7, batch    56 | loss: 5.0224428Losses:  4.025230884552002 0.13879528641700745 0.7012978792190552
CurrentTrain: epoch  7, batch    57 | loss: 4.8653240Losses:  4.05349588394165 0.13694985210895538 0.7941867113113403
CurrentTrain: epoch  7, batch    58 | loss: 4.9846325Losses:  4.0299530029296875 0.10440205782651901 0.7328009605407715
CurrentTrain: epoch  7, batch    59 | loss: 4.8671560Losses:  4.081426620483398 0.1761648952960968 0.7819812297821045
CurrentTrain: epoch  7, batch    60 | loss: 5.0395727Losses:  4.099081039428711 0.12256000190973282 0.8121561408042908
CurrentTrain: epoch  7, batch    61 | loss: 5.0337973Losses:  4.10171365737915 0.08473662286996841 0.824134111404419
CurrentTrain: epoch  7, batch    62 | loss: 5.0105848Losses:  4.055642127990723 0.14477470517158508 0.7476661205291748
CurrentTrain: epoch  8, batch     0 | loss: 4.9480829Losses:  4.07322883605957 0.14145725965499878 0.790703535079956
CurrentTrain: epoch  8, batch     1 | loss: 5.0053892Losses:  4.07387638092041 0.10984732210636139 0.8020142912864685
CurrentTrain: epoch  8, batch     2 | loss: 4.9857383Losses:  4.107574462890625 0.1495920568704605 0.6631953120231628
CurrentTrain: epoch  8, batch     3 | loss: 4.9203615Losses:  4.156868934631348 0.11546886712312698 0.7734750509262085
CurrentTrain: epoch  8, batch     4 | loss: 5.0458131Losses:  4.039316654205322 0.12159724533557892 0.7370330691337585
CurrentTrain: epoch  8, batch     5 | loss: 4.8979468Losses:  4.106545448303223 0.0894433856010437 0.8059232234954834
CurrentTrain: epoch  8, batch     6 | loss: 5.0019121Losses:  4.118796348571777 0.14962951838970184 0.8190129995346069
CurrentTrain: epoch  8, batch     7 | loss: 5.0874391Losses:  4.045187473297119 0.15338781476020813 0.7121233940124512
CurrentTrain: epoch  8, batch     8 | loss: 4.9106989Losses:  4.054074764251709 0.1335708647966385 0.6759411692619324
CurrentTrain: epoch  8, batch     9 | loss: 4.8635864Losses:  4.0686235427856445 0.13253119587898254 0.7781080007553101
CurrentTrain: epoch  8, batch    10 | loss: 4.9792628Losses:  4.051756858825684 0.13617141544818878 0.7664471864700317
CurrentTrain: epoch  8, batch    11 | loss: 4.9543753Losses:  4.135807514190674 0.0881957933306694 0.8012572526931763
CurrentTrain: epoch  8, batch    12 | loss: 5.0252604Losses:  4.083514213562012 0.13979998230934143 0.7587655782699585
CurrentTrain: epoch  8, batch    13 | loss: 4.9820800Losses:  4.049237251281738 0.12539975345134735 0.7620453834533691
CurrentTrain: epoch  8, batch    14 | loss: 4.9366822Losses:  4.319174766540527 0.1518336832523346 0.7365705966949463
CurrentTrain: epoch  8, batch    15 | loss: 5.2075787Losses:  4.062139987945557 0.11579129099845886 0.7849880456924438
CurrentTrain: epoch  8, batch    16 | loss: 4.9629192Losses:  4.056108474731445 0.09504583477973938 0.7278528809547424
CurrentTrain: epoch  8, batch    17 | loss: 4.8790073Losses:  4.113801956176758 0.08270770311355591 0.8143324255943298
CurrentTrain: epoch  8, batch    18 | loss: 5.0108423Losses:  4.073625564575195 0.10476794838905334 0.7300437688827515
CurrentTrain: epoch  8, batch    19 | loss: 4.9084373Losses:  3.9869558811187744 0.1046266183257103 0.6777913570404053
CurrentTrain: epoch  8, batch    20 | loss: 4.7693739Losses:  4.057888984680176 0.1019740104675293 0.8438553214073181
CurrentTrain: epoch  8, batch    21 | loss: 5.0037184Losses:  4.087536334991455 0.11375692486763 0.7507208585739136
CurrentTrain: epoch  8, batch    22 | loss: 4.9520144Losses:  4.039340972900391 0.13025620579719543 0.6898083686828613
CurrentTrain: epoch  8, batch    23 | loss: 4.8594055Losses:  4.0157012939453125 0.12094570696353912 0.7690558433532715
CurrentTrain: epoch  8, batch    24 | loss: 4.9057031Losses:  4.336028099060059 0.16559621691703796 0.7890706062316895
CurrentTrain: epoch  8, batch    25 | loss: 5.2906947Losses:  4.0684967041015625 0.12209346890449524 0.7415604591369629
CurrentTrain: epoch  8, batch    26 | loss: 4.9321508Losses:  4.043872833251953 0.12845802307128906 0.789408802986145
CurrentTrain: epoch  8, batch    27 | loss: 4.9617395Losses:  4.001705646514893 0.1198076605796814 0.79217529296875
CurrentTrain: epoch  8, batch    28 | loss: 4.9136887Losses:  4.026459693908691 0.1288710981607437 0.7443006634712219
CurrentTrain: epoch  8, batch    29 | loss: 4.8996315Losses:  4.010180950164795 0.13312388956546783 0.7356035709381104
CurrentTrain: epoch  8, batch    30 | loss: 4.8789082Losses:  4.019104957580566 0.12163913249969482 0.794468879699707
CurrentTrain: epoch  8, batch    31 | loss: 4.9352131Losses:  4.031733989715576 0.13957896828651428 0.7491610646247864
CurrentTrain: epoch  8, batch    32 | loss: 4.9204741Losses:  4.049610614776611 0.13937003910541534 0.7608317732810974
CurrentTrain: epoch  8, batch    33 | loss: 4.9498124Losses:  4.063710689544678 0.14016269147396088 0.7559257745742798
CurrentTrain: epoch  8, batch    34 | loss: 4.9597988Losses:  4.021995544433594 0.0972493588924408 0.7194086313247681
CurrentTrain: epoch  8, batch    35 | loss: 4.8386536Losses:  4.034331798553467 0.06775882095098495 0.744465172290802
CurrentTrain: epoch  8, batch    36 | loss: 4.8465562Losses:  4.0307159423828125 0.1419786810874939 0.7251356244087219
CurrentTrain: epoch  8, batch    37 | loss: 4.8978305Losses:  4.222307205200195 0.13514837622642517 0.6976841688156128
CurrentTrain: epoch  8, batch    38 | loss: 5.0551400Losses:  3.992366313934326 0.11063584685325623 0.7474046945571899
CurrentTrain: epoch  8, batch    39 | loss: 4.8504066Losses:  4.06362247467041 0.12584468722343445 0.802685022354126
CurrentTrain: epoch  8, batch    40 | loss: 4.9921522Losses:  4.075356483459473 0.09245261549949646 0.7627028822898865
CurrentTrain: epoch  8, batch    41 | loss: 4.9305120Losses:  4.028512477874756 0.07616783678531647 0.8145570755004883
CurrentTrain: epoch  8, batch    42 | loss: 4.9192376Losses:  4.04033088684082 0.11735894531011581 0.7621911764144897
CurrentTrain: epoch  8, batch    43 | loss: 4.9198813Losses:  4.0770745277404785 0.12044297903776169 0.770593523979187
CurrentTrain: epoch  8, batch    44 | loss: 4.9681110Losses:  4.063453674316406 0.11134307086467743 0.7935765981674194
CurrentTrain: epoch  8, batch    45 | loss: 4.9683733Losses:  3.997807025909424 0.09571534395217896 0.7253316044807434
CurrentTrain: epoch  8, batch    46 | loss: 4.8188543Losses:  4.081961631774902 0.07920441031455994 0.7455806732177734
CurrentTrain: epoch  8, batch    47 | loss: 4.9067469Losses:  4.113290786743164 0.10053083300590515 0.6885204315185547
CurrentTrain: epoch  8, batch    48 | loss: 4.9023418Losses:  3.988560914993286 0.07392387092113495 0.7218432426452637
CurrentTrain: epoch  8, batch    49 | loss: 4.7843280Losses:  4.0559587478637695 0.12962201237678528 0.8050714135169983
CurrentTrain: epoch  8, batch    50 | loss: 4.9906521Losses:  4.058459281921387 0.07673870027065277 0.7098747491836548
CurrentTrain: epoch  8, batch    51 | loss: 4.8450727Losses:  4.067530155181885 0.12917277216911316 0.7961428761482239
CurrentTrain: epoch  8, batch    52 | loss: 4.9928460Losses:  4.019282817840576 0.08309321850538254 0.7306997776031494
CurrentTrain: epoch  8, batch    53 | loss: 4.8330755Losses:  4.083924770355225 0.1051802933216095 0.7696176171302795
CurrentTrain: epoch  8, batch    54 | loss: 4.9587226Losses:  4.037359237670898 0.10184654593467712 0.7476699948310852
CurrentTrain: epoch  8, batch    55 | loss: 4.8868761Losses:  4.049421310424805 0.12408604472875595 0.7901124954223633
CurrentTrain: epoch  8, batch    56 | loss: 4.9636197Losses:  4.081579208374023 0.11984570324420929 0.7974993586540222
CurrentTrain: epoch  8, batch    57 | loss: 4.9989243Losses:  4.025917053222656 0.1248243898153305 0.7318739891052246
CurrentTrain: epoch  8, batch    58 | loss: 4.8826156Losses:  4.143760681152344 0.06923380494117737 0.7561599016189575
CurrentTrain: epoch  8, batch    59 | loss: 4.9691544Losses:  4.035467624664307 0.08777783811092377 0.8033467531204224
CurrentTrain: epoch  8, batch    60 | loss: 4.9265919Losses:  4.008284568786621 0.1196071207523346 0.7252260446548462
CurrentTrain: epoch  8, batch    61 | loss: 4.8531175Losses:  4.053346633911133 0.06928500533103943 0.6688938736915588
CurrentTrain: epoch  8, batch    62 | loss: 4.7915254Losses:  4.006490707397461 0.06848816573619843 0.811378538608551
CurrentTrain: epoch  9, batch     0 | loss: 4.8863573Losses:  4.047786235809326 0.11408939957618713 0.7302626371383667
CurrentTrain: epoch  9, batch     1 | loss: 4.8921385Losses:  4.074410438537598 0.10235755890607834 0.7433764934539795
CurrentTrain: epoch  9, batch     2 | loss: 4.9201441Losses:  4.018642425537109 0.10891316831111908 0.7275139689445496
CurrentTrain: epoch  9, batch     3 | loss: 4.8550692Losses:  4.021744728088379 0.11360210180282593 0.7825280427932739
CurrentTrain: epoch  9, batch     4 | loss: 4.9178748Losses:  4.061379432678223 0.09400293231010437 0.7380471229553223
CurrentTrain: epoch  9, batch     5 | loss: 4.8934293Losses:  4.022760391235352 0.07923193275928497 0.7839741706848145
CurrentTrain: epoch  9, batch     6 | loss: 4.8859663Losses:  4.001847267150879 0.12404590845108032 0.7201954126358032
CurrentTrain: epoch  9, batch     7 | loss: 4.8460884Losses:  4.033102989196777 0.12752804160118103 0.7177881598472595
CurrentTrain: epoch  9, batch     8 | loss: 4.8784194Losses:  4.052446365356445 0.10396965593099594 0.7937164306640625
CurrentTrain: epoch  9, batch     9 | loss: 4.9501324Losses:  4.129521369934082 0.15925435721874237 0.700539231300354
CurrentTrain: epoch  9, batch    10 | loss: 4.9893150Losses:  4.021108627319336 0.11772755533456802 0.7059117555618286
CurrentTrain: epoch  9, batch    11 | loss: 4.8447480Losses:  4.040144920349121 0.10763508081436157 0.7989791035652161
CurrentTrain: epoch  9, batch    12 | loss: 4.9467592Losses:  4.018649101257324 0.10763636231422424 0.7563978433609009
CurrentTrain: epoch  9, batch    13 | loss: 4.8826833Losses:  4.033426284790039 0.0990791767835617 0.7439554333686829
CurrentTrain: epoch  9, batch    14 | loss: 4.8764610Losses:  4.094721794128418 0.062329042702913284 0.8418514132499695
CurrentTrain: epoch  9, batch    15 | loss: 4.9989018Losses:  4.03154182434082 0.12791143357753754 0.7681590914726257
CurrentTrain: epoch  9, batch    16 | loss: 4.9276123Losses:  4.060092449188232 0.09254610538482666 0.7264817357063293
CurrentTrain: epoch  9, batch    17 | loss: 4.8791203Losses:  4.04925537109375 0.06549500674009323 0.7095215916633606
CurrentTrain: epoch  9, batch    18 | loss: 4.8242722Losses:  4.027169704437256 0.07854160666465759 0.6935585141181946
CurrentTrain: epoch  9, batch    19 | loss: 4.7992702Losses:  4.01541805267334 0.10445946455001831 0.7838380932807922
CurrentTrain: epoch  9, batch    20 | loss: 4.9037156Losses:  4.015847206115723 0.10864652693271637 0.7507703304290771
CurrentTrain: epoch  9, batch    21 | loss: 4.8752642Losses:  4.012973308563232 0.08907830715179443 0.7650364637374878
CurrentTrain: epoch  9, batch    22 | loss: 4.8670883Losses:  4.055188179016113 0.08917468786239624 0.7496898174285889
CurrentTrain: epoch  9, batch    23 | loss: 4.8940525Losses:  3.9423828125 0.08657485246658325 0.7992236614227295
CurrentTrain: epoch  9, batch    24 | loss: 4.8281813Losses:  4.072711944580078 0.08781164884567261 0.7680376768112183
CurrentTrain: epoch  9, batch    25 | loss: 4.9285612Losses:  4.088738918304443 0.10398701578378677 0.8388161063194275
CurrentTrain: epoch  9, batch    26 | loss: 5.0315423Losses:  3.9388484954833984 0.11924333870410919 0.6962195634841919
CurrentTrain: epoch  9, batch    27 | loss: 4.7543111Losses:  4.008581161499023 0.10006275027990341 0.7193630337715149
CurrentTrain: epoch  9, batch    28 | loss: 4.8280072Losses:  4.024309158325195 0.09132476150989532 0.7118053436279297
CurrentTrain: epoch  9, batch    29 | loss: 4.8274393Losses:  4.039807319641113 0.12290797382593155 0.7692874670028687
CurrentTrain: epoch  9, batch    30 | loss: 4.9320030Losses:  4.032421112060547 0.10751408338546753 0.7075785994529724
CurrentTrain: epoch  9, batch    31 | loss: 4.8475137Losses:  4.050745010375977 0.0743761956691742 0.7423288822174072
CurrentTrain: epoch  9, batch    32 | loss: 4.8674498Losses:  4.010252952575684 0.12493016570806503 0.7179311513900757
CurrentTrain: epoch  9, batch    33 | loss: 4.8531146Losses:  4.019144058227539 0.09706124663352966 0.7564187049865723
CurrentTrain: epoch  9, batch    34 | loss: 4.8726239Losses:  4.003561973571777 0.09287898987531662 0.771314263343811
CurrentTrain: epoch  9, batch    35 | loss: 4.8677549Losses:  3.9731192588806152 0.0707148090004921 0.7750794291496277
CurrentTrain: epoch  9, batch    36 | loss: 4.8189135Losses:  4.049384593963623 0.09265455603599548 0.7238303422927856
CurrentTrain: epoch  9, batch    37 | loss: 4.8658695Losses:  4.007474422454834 0.11694624274969101 0.7849643230438232
CurrentTrain: epoch  9, batch    38 | loss: 4.9093847Losses:  4.014256954193115 0.11510084569454193 0.7896539568901062
CurrentTrain: epoch  9, batch    39 | loss: 4.9190116Losses:  4.0289082527160645 0.1139819398522377 0.7531370520591736
CurrentTrain: epoch  9, batch    40 | loss: 4.8960271Losses:  4.001143455505371 0.11387375742197037 0.6915249824523926
CurrentTrain: epoch  9, batch    41 | loss: 4.8065424Losses:  4.005214691162109 0.1310032606124878 0.716788113117218
CurrentTrain: epoch  9, batch    42 | loss: 4.8530064Losses:  3.9767069816589355 0.10910405218601227 0.7031568288803101
CurrentTrain: epoch  9, batch    43 | loss: 4.7889681Losses:  3.9939160346984863 0.11476092040538788 0.7849878072738647
CurrentTrain: epoch  9, batch    44 | loss: 4.8936648Losses:  3.9928319454193115 0.09889987111091614 0.7110518217086792
CurrentTrain: epoch  9, batch    45 | loss: 4.8027840Losses:  4.049197673797607 0.10413020849227905 0.8024757504463196
CurrentTrain: epoch  9, batch    46 | loss: 4.9558039Losses:  4.005938529968262 0.11157439649105072 0.7178692817687988
CurrentTrain: epoch  9, batch    47 | loss: 4.8353820Losses:  4.004817008972168 0.1097695454955101 0.7634152173995972
CurrentTrain: epoch  9, batch    48 | loss: 4.8780017Losses:  4.026445388793945 0.1025686115026474 0.6978795528411865
CurrentTrain: epoch  9, batch    49 | loss: 4.8268938Losses:  4.009321689605713 0.10863151401281357 0.7561032772064209
CurrentTrain: epoch  9, batch    50 | loss: 4.8740568Losses:  4.016432762145996 0.08765482157468796 0.7394335269927979
CurrentTrain: epoch  9, batch    51 | loss: 4.8435211Losses:  4.0171918869018555 0.10963620245456696 0.7129071354866028
CurrentTrain: epoch  9, batch    52 | loss: 4.8397355Losses:  3.9557766914367676 0.1346069574356079 0.6699488162994385
CurrentTrain: epoch  9, batch    53 | loss: 4.7603321Losses:  3.9827070236206055 0.12145097553730011 0.6863657236099243
CurrentTrain: epoch  9, batch    54 | loss: 4.7905235Losses:  4.025567054748535 0.08792358636856079 0.7856787443161011
CurrentTrain: epoch  9, batch    55 | loss: 4.8991694Losses:  4.024082183837891 0.09106814861297607 0.6474072933197021
CurrentTrain: epoch  9, batch    56 | loss: 4.7625580Losses:  3.9934186935424805 0.10128787159919739 0.7775793671607971
CurrentTrain: epoch  9, batch    57 | loss: 4.8722858Losses:  3.980799436569214 0.11261046677827835 0.7438086271286011
CurrentTrain: epoch  9, batch    58 | loss: 4.8372188Losses:  3.977940559387207 0.10924198478460312 0.7485936880111694
CurrentTrain: epoch  9, batch    59 | loss: 4.8357763Losses:  3.9878578186035156 0.10104387998580933 0.7539421319961548
CurrentTrain: epoch  9, batch    60 | loss: 4.8428435Losses:  3.9940760135650635 0.1006125807762146 0.7225127220153809
CurrentTrain: epoch  9, batch    61 | loss: 4.8172011Losses:  3.9840011596679688 0.040592506527900696 0.8493978977203369
CurrentTrain: epoch  9, batch    62 | loss: 4.8739920
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [0 5 4 0 0 0 0 0 8 7 0 3 0 0 0 6 0 1 0 2]
Losses:  7.0257415771484375 1.3870956897735596 0.9779404401779175
CurrentTrain: epoch  0, batch     0 | loss: 9.3907776Losses:  7.874046325683594 1.8645741939544678 0.9921177625656128
CurrentTrain: epoch  0, batch     1 | loss: 10.7307386Losses:  7.032515048980713 1.768683910369873 0.9727363586425781
CurrentTrain: epoch  0, batch     2 | loss: 9.7739353Losses:  4.826935768127441 0.30887269973754883 0.9689904451370239
CurrentTrain: epoch  0, batch     3 | loss: 6.1047988Losses:  6.691284656524658 1.8748126029968262 0.9964735507965088
CurrentTrain: epoch  1, batch     0 | loss: 9.5625706Losses:  6.529695510864258 1.2311612367630005 0.9627857208251953
CurrentTrain: epoch  1, batch     1 | loss: 8.7236423Losses:  6.110655784606934 1.5305180549621582 0.9766966104507446
CurrentTrain: epoch  1, batch     2 | loss: 8.6178703Losses:  9.39175033569336 0.3797847032546997 0.982135534286499
CurrentTrain: epoch  1, batch     3 | loss: 10.7536707Losses:  5.725251197814941 1.389758825302124 0.9827491044998169
CurrentTrain: epoch  2, batch     0 | loss: 8.0977592Losses:  4.779238224029541 1.4353445768356323 0.978237509727478
CurrentTrain: epoch  2, batch     1 | loss: 7.1928205Losses:  6.809685230255127 1.361602783203125 0.9704403877258301
CurrentTrain: epoch  2, batch     2 | loss: 9.1417274Losses:  5.112436294555664 0.41899198293685913 1.0001174211502075
CurrentTrain: epoch  2, batch     3 | loss: 6.5315456Losses:  4.730569839477539 1.4756767749786377 0.9917967319488525
CurrentTrain: epoch  3, batch     0 | loss: 7.1980429Losses:  4.8260345458984375 1.4029009342193604 0.9741003513336182
CurrentTrain: epoch  3, batch     1 | loss: 7.2030354Losses:  5.384054183959961 1.311708688735962 0.9476549625396729
CurrentTrain: epoch  3, batch     2 | loss: 7.6434174Losses:  5.086886405944824 0.5143873691558838 0.9772217273712158
CurrentTrain: epoch  3, batch     3 | loss: 6.5784950Losses:  5.00466251373291 1.4957940578460693 0.9793387651443481
CurrentTrain: epoch  4, batch     0 | loss: 7.4797955Losses:  5.314859867095947 1.5630321502685547 0.9705791473388672
CurrentTrain: epoch  4, batch     1 | loss: 7.8484712Losses:  3.9962589740753174 1.1036382913589478 0.9530428647994995
CurrentTrain: epoch  4, batch     2 | loss: 6.0529404Losses:  5.726195812225342 0.2602982521057129 0.9958347082138062
CurrentTrain: epoch  4, batch     3 | loss: 6.9823289Losses:  5.086236953735352 1.4501036405563354 0.9671611785888672
CurrentTrain: epoch  5, batch     0 | loss: 7.5035019Losses:  4.5810980796813965 1.372521996498108 0.9645143747329712
CurrentTrain: epoch  5, batch     1 | loss: 6.9181342Losses:  4.267800807952881 1.3489848375320435 0.9611905813217163
CurrentTrain: epoch  5, batch     2 | loss: 6.5779762Losses:  2.0291666984558105 8.94069742685133e-08 1.0
CurrentTrain: epoch  5, batch     3 | loss: 3.0291667Losses:  3.595104217529297 1.1608130931854248 0.9675511121749878
CurrentTrain: epoch  6, batch     0 | loss: 5.7234688Losses:  4.591063499450684 1.4307458400726318 0.972558319568634
CurrentTrain: epoch  6, batch     1 | loss: 6.9943681Losses:  4.5269317626953125 1.3388142585754395 0.9436444044113159
CurrentTrain: epoch  6, batch     2 | loss: 6.8093905Losses:  4.113429069519043 0.24757227301597595 0.9653734564781189
CurrentTrain: epoch  6, batch     3 | loss: 5.3263750Losses:  2.9429383277893066 0.7565242052078247 0.9701077342033386
CurrentTrain: epoch  7, batch     0 | loss: 4.6695700Losses:  4.466980457305908 1.2593997716903687 0.9612025618553162
CurrentTrain: epoch  7, batch     1 | loss: 6.6875830Losses:  4.6825947761535645 1.4182558059692383 0.9579291343688965
CurrentTrain: epoch  7, batch     2 | loss: 7.0587797Losses:  4.316603183746338 0.23765963315963745 0.9138951301574707
CurrentTrain: epoch  7, batch     3 | loss: 5.4681578Losses:  4.08896541595459 1.2633490562438965 0.9674877524375916
CurrentTrain: epoch  8, batch     0 | loss: 6.3198023Losses:  4.032844543457031 1.2886273860931396 0.9696017503738403
CurrentTrain: epoch  8, batch     1 | loss: 6.2910738Losses:  3.7476515769958496 1.0202200412750244 0.9295270442962646
CurrentTrain: epoch  8, batch     2 | loss: 5.6973991Losses:  1.9857276678085327 5.960464477539063e-08 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.9857278Losses:  3.090888023376465 1.1437078714370728 0.9683822393417358
CurrentTrain: epoch  9, batch     0 | loss: 5.2029781Losses:  4.098491668701172 1.303725004196167 0.9520455598831177
CurrentTrain: epoch  9, batch     1 | loss: 6.3542624Losses:  3.8663930892944336 1.1414103507995605 0.9385113716125488
CurrentTrain: epoch  9, batch     2 | loss: 5.9463148Losses:  4.062489986419678 0.17923754453659058 0.9616795778274536
CurrentTrain: epoch  9, batch     3 | loss: 5.2034068
Losses:  0.5130518674850464 1.0875694751739502 0.8844119310379028
MemoryTrain:  epoch  0, batch     0 | loss: 2.4850333Losses:  1.0213159322738647 0.19067852199077606 0.8269985914230347
MemoryTrain:  epoch  0, batch     1 | loss: 2.0389929Losses:  0.7543951272964478 1.0899890661239624 0.8954942226409912
MemoryTrain:  epoch  1, batch     0 | loss: 2.7398784Losses:  0.4905802011489868 0.261841744184494 0.764094352722168
MemoryTrain:  epoch  1, batch     1 | loss: 1.5165163Losses:  0.42251378297805786 0.8756173849105835 0.8508527278900146
MemoryTrain:  epoch  2, batch     0 | loss: 2.1489840Losses:  0.06893183290958405 0.3460760712623596 0.9368343949317932
MemoryTrain:  epoch  2, batch     1 | loss: 1.3518423Losses:  0.2878890335559845 0.9678387641906738 0.865427553653717
MemoryTrain:  epoch  3, batch     0 | loss: 2.1211553Losses:  0.01581314206123352 0.2752939462661743 0.8866426944732666
MemoryTrain:  epoch  3, batch     1 | loss: 1.1777498Losses:  0.08483940362930298 0.9192150831222534 0.8710055351257324
MemoryTrain:  epoch  4, batch     0 | loss: 1.8750601Losses:  0.04336080327630043 0.2522065043449402 0.8581979274749756
MemoryTrain:  epoch  4, batch     1 | loss: 1.1537652Losses:  0.10065451264381409 0.9073060154914856 0.8811179995536804
MemoryTrain:  epoch  5, batch     0 | loss: 1.8890786Losses:  0.02270323410630226 0.08565419167280197 0.8013573884963989
MemoryTrain:  epoch  5, batch     1 | loss: 0.9097148Losses:  0.04055904597043991 0.8828608393669128 0.8580641746520996
MemoryTrain:  epoch  6, batch     0 | loss: 1.7814841Losses:  0.025569826364517212 0.22006428241729736 0.8979585766792297
MemoryTrain:  epoch  6, batch     1 | loss: 1.1435927Losses:  0.04135997220873833 0.6620864868164062 0.8425115346908569
MemoryTrain:  epoch  7, batch     0 | loss: 1.5459580Losses:  0.04837551712989807 0.4153481423854828 0.940209686756134
MemoryTrain:  epoch  7, batch     1 | loss: 1.4039333Losses:  0.041859958320856094 0.8669761419296265 0.8510257005691528
MemoryTrain:  epoch  8, batch     0 | loss: 1.7598618Losses:  0.008267730474472046 0.17555150389671326 0.9146654009819031
MemoryTrain:  epoch  8, batch     1 | loss: 1.0984846Losses:  0.024678075686097145 0.8192627429962158 0.8520497679710388
MemoryTrain:  epoch  9, batch     0 | loss: 1.6959906Losses:  0.05366763472557068 0.17192383110523224 0.8996283411979675
MemoryTrain:  epoch  9, batch     1 | loss: 1.1252198
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 73.47%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 73.67%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 73.92%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 73.94%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.07%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.40%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.48%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.68%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.83%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 94.81%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.69%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 94.57%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 94.46%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 94.05%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 92.87%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 91.57%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 90.76%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 89.61%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.67%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 90.08%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 89.39%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 88.64%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 87.90%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 86.88%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 86.11%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 86.05%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 86.47%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 86.79%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 86.45%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 86.32%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 85.99%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 85.60%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 84.95%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 84.57%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 84.61%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 84.69%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 84.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 85.11%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 85.40%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.54%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 85.59%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 85.49%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 85.40%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 85.30%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 85.21%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 85.09%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 85.11%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 85.02%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 84.85%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 84.84%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 84.92%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 84.89%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 84.71%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 84.68%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.70%   
cur_acc:  ['0.9435', '0.7440']
his_acc:  ['0.9435', '0.8470']
Clustering into  14  clusters
Clusters:  [ 2 11  9  2  2  2  0  2  8  7  0 13 10  2  2  6  2  1  2 12  2  5  2  2
  1  2  2  2  3  4]
Losses:  5.71881103515625 1.3381109237670898 0.980391263961792
CurrentTrain: epoch  0, batch     0 | loss: 8.0373135Losses:  6.054689407348633 1.2907731533050537 0.975764274597168
CurrentTrain: epoch  0, batch     1 | loss: 8.3212271Losses:  5.993583679199219 1.4427305459976196 0.9966734647750854
CurrentTrain: epoch  0, batch     2 | loss: 8.4329872Losses:  4.867888450622559 0.6220832467079163 0.9719780087471008
CurrentTrain: epoch  0, batch     3 | loss: 6.4619498Losses:  5.99149751663208 1.3686755895614624 0.9862862825393677
CurrentTrain: epoch  1, batch     0 | loss: 8.3464594Losses:  4.993812561035156 1.2533838748931885 0.9821323752403259
CurrentTrain: epoch  1, batch     1 | loss: 7.2293286Losses:  4.787960052490234 1.202841877937317 0.9629892706871033
CurrentTrain: epoch  1, batch     2 | loss: 6.9537911Losses:  5.7490692138671875 1.4901162614933128e-07 1.0
CurrentTrain: epoch  1, batch     3 | loss: 6.7490692Losses:  4.730786323547363 1.5327852964401245 0.9658917188644409
CurrentTrain: epoch  2, batch     0 | loss: 7.2294636Losses:  4.794041633605957 1.493018627166748 0.9803181886672974
CurrentTrain: epoch  2, batch     1 | loss: 7.2673783Losses:  4.215259552001953 1.4526070356369019 0.9691162109375
CurrentTrain: epoch  2, batch     2 | loss: 6.6369829Losses:  2.2887916564941406 0.35062646865844727 0.9756645560264587
CurrentTrain: epoch  2, batch     3 | loss: 3.6150827Losses:  3.9729902744293213 1.3206485509872437 0.9780200123786926
CurrentTrain: epoch  3, batch     0 | loss: 6.2716589Losses:  4.04193115234375 1.3699777126312256 0.9640134572982788
CurrentTrain: epoch  3, batch     1 | loss: 6.3759227Losses:  4.2805914878845215 1.1998122930526733 0.9623783826828003
CurrentTrain: epoch  3, batch     2 | loss: 6.4427824Losses:  3.606578826904297 0.3780350089073181 0.9419777393341064
CurrentTrain: epoch  3, batch     3 | loss: 4.9265919Losses:  3.9933273792266846 1.187300682067871 0.9683927893638611
CurrentTrain: epoch  4, batch     0 | loss: 6.1490207Losses:  3.714890480041504 1.1346901655197144 0.9506827592849731
CurrentTrain: epoch  4, batch     1 | loss: 5.8002634Losses:  3.5185275077819824 1.2933597564697266 0.9687031507492065
CurrentTrain: epoch  4, batch     2 | loss: 5.7805905Losses:  4.667593479156494 0.46837979555130005 0.9512069225311279
CurrentTrain: epoch  4, batch     3 | loss: 6.0871801Losses:  3.4465458393096924 1.1740537881851196 0.945970892906189
CurrentTrain: epoch  5, batch     0 | loss: 5.5665708Losses:  3.6639537811279297 1.2710481882095337 0.9622170925140381
CurrentTrain: epoch  5, batch     1 | loss: 5.8972187Losses:  3.293762445449829 0.894137978553772 0.974008321762085
CurrentTrain: epoch  5, batch     2 | loss: 5.1619091Losses:  4.044152736663818 0.2435823380947113 0.925925612449646
CurrentTrain: epoch  5, batch     3 | loss: 5.2136607Losses:  3.4752631187438965 1.0107042789459229 0.9474668502807617
CurrentTrain: epoch  6, batch     0 | loss: 5.4334345Losses:  3.4941983222961426 1.2870742082595825 0.9492335319519043
CurrentTrain: epoch  6, batch     1 | loss: 5.7305059Losses:  3.478283166885376 1.1667479276657104 0.9681113958358765
CurrentTrain: epoch  6, batch     2 | loss: 5.6131425Losses:  2.4572134017944336 0.24934250116348267 1.0
CurrentTrain: epoch  6, batch     3 | loss: 3.7065558Losses:  3.608146905899048 1.2429354190826416 0.939917266368866
CurrentTrain: epoch  7, batch     0 | loss: 5.7909994Losses:  3.5949325561523438 1.1841075420379639 0.9526830911636353
CurrentTrain: epoch  7, batch     1 | loss: 5.7317233Losses:  2.252814292907715 0.8329460620880127 0.9598978757858276
CurrentTrain: epoch  7, batch     2 | loss: 4.0456581Losses:  2.854100227355957 1.1920930376163597e-07 0.9691752195358276
CurrentTrain: epoch  7, batch     3 | loss: 3.8232756Losses:  3.4068846702575684 1.015491247177124 0.9676501154899597
CurrentTrain: epoch  8, batch     0 | loss: 5.3900256Losses:  2.7369022369384766 0.872632622718811 0.9237943291664124
CurrentTrain: epoch  8, batch     1 | loss: 4.5333290Losses:  2.79831600189209 1.1352999210357666 0.9511998295783997
CurrentTrain: epoch  8, batch     2 | loss: 4.8848157Losses:  1.9023298025131226 0.22233103215694427 0.9219328165054321
CurrentTrain: epoch  8, batch     3 | loss: 3.0465937Losses:  3.6469602584838867 1.0402286052703857 0.953999936580658
CurrentTrain: epoch  9, batch     0 | loss: 5.6411891Losses:  2.401472568511963 0.909934401512146 0.9472202062606812
CurrentTrain: epoch  9, batch     1 | loss: 4.2586274Losses:  2.563194990158081 1.0350027084350586 0.9302288293838501
CurrentTrain: epoch  9, batch     2 | loss: 4.5284266Losses:  2.705565929412842 8.94069742685133e-08 0.9449864625930786
CurrentTrain: epoch  9, batch     3 | loss: 3.6505523
Losses:  1.1003100872039795 1.3331022262573242 0.9119941592216492
MemoryTrain:  epoch  0, batch     0 | loss: 3.3454065Losses:  0.43097901344299316 0.6527876853942871 0.9041941165924072
MemoryTrain:  epoch  0, batch     1 | loss: 1.9879608Losses:  0.985183596611023 1.159611463546753 0.8926175832748413
MemoryTrain:  epoch  1, batch     0 | loss: 3.0374126Losses:  0.6514387130737305 0.7288087606430054 0.9154747724533081
MemoryTrain:  epoch  1, batch     1 | loss: 2.2957222Losses:  0.3570442199707031 0.9547228813171387 0.9155614972114563
MemoryTrain:  epoch  2, batch     0 | loss: 2.2273285Losses:  0.9726622104644775 0.9341623187065125 0.8820554614067078
MemoryTrain:  epoch  2, batch     1 | loss: 2.7888801Losses:  0.47435134649276733 0.8626776933670044 0.8933541774749756
MemoryTrain:  epoch  3, batch     0 | loss: 2.2303832Losses:  0.33735597133636475 0.9379132986068726 0.9047492146492004
MemoryTrain:  epoch  3, batch     1 | loss: 2.1800184Losses:  0.28612151741981506 1.0557522773742676 0.9274422526359558
MemoryTrain:  epoch  4, batch     0 | loss: 2.2693160Losses:  0.11656159907579422 0.7340420484542847 0.859876811504364
MemoryTrain:  epoch  4, batch     1 | loss: 1.7104805Losses:  0.22448760271072388 0.735363245010376 0.8795820474624634
MemoryTrain:  epoch  5, batch     0 | loss: 1.8394330Losses:  0.3003508150577545 1.1509687900543213 0.9201357364654541
MemoryTrain:  epoch  5, batch     1 | loss: 2.3714552Losses:  0.18996784090995789 0.9051059484481812 0.9188814759254456
MemoryTrain:  epoch  6, batch     0 | loss: 2.0139554Losses:  0.10323618352413177 0.7529734373092651 0.863894522190094
MemoryTrain:  epoch  6, batch     1 | loss: 1.7201042Losses:  0.057793028652668 0.8857340216636658 0.8982419967651367
MemoryTrain:  epoch  7, batch     0 | loss: 1.8417690Losses:  0.08483567833900452 0.8057650327682495 0.8873810768127441
MemoryTrain:  epoch  7, batch     1 | loss: 1.7779818Losses:  0.04883543401956558 0.8828874230384827 0.8930572271347046
MemoryTrain:  epoch  8, batch     0 | loss: 1.8247801Losses:  0.12849901616573334 0.8291481733322144 0.8939145803451538
MemoryTrain:  epoch  8, batch     1 | loss: 1.8515618Losses:  0.14720971882343292 0.7179685831069946 0.8994011878967285
MemoryTrain:  epoch  9, batch     0 | loss: 1.7645795Losses:  0.049573127180337906 0.8821952939033508 0.8819494843482971
MemoryTrain:  epoch  9, batch     1 | loss: 1.8137178
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.29%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 76.31%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 76.42%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 77.31%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 78.07%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 77.95%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 77.57%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.19%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 77.37%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 77.66%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.48%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.93%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.21%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 92.35%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 91.95%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 91.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.50%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 90.58%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 89.55%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 88.46%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 87.97%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 87.13%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 86.21%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.00%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 86.02%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 85.77%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 85.06%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 84.21%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 83.70%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 83.20%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 82.48%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 82.47%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 82.68%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 83.01%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 83.21%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 83.45%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 83.15%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 82.92%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 82.55%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 82.27%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 81.79%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 81.38%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 81.58%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 81.57%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.17%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 82.22%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.56%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 82.36%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 81.94%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 81.02%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 80.63%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 80.08%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 79.92%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 79.82%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 79.90%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 79.87%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 79.78%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 79.84%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 79.97%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 79.98%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 79.99%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.66%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 79.38%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 78.96%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 78.73%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 78.22%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 77.86%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 77.79%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 77.91%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.03%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.35%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 78.28%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 78.21%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 78.01%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 77.90%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 77.93%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 77.86%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.17%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 78.58%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 78.84%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 78.90%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 79.05%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 79.10%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.19%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 79.60%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 79.54%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 79.47%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 79.41%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 79.13%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 79.14%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 79.47%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.75%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 79.76%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 79.70%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 79.49%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 79.40%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 79.44%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 79.32%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 79.19%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 79.23%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 79.21%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 79.26%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 79.22%   
cur_acc:  ['0.9435', '0.7440', '0.7748']
his_acc:  ['0.9435', '0.8470', '0.7922']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15 17 16  2  1  1 12  1 18  1  8  1 14  1  1
  7  1  1  1  3  6  2  1  5  1 10  0  4  1  1  1]
Losses:  5.699312210083008 1.615063190460205 0.9853920340538025
CurrentTrain: epoch  0, batch     0 | loss: 8.2997675Losses:  5.205248832702637 1.3286972045898438 0.9728861451148987
CurrentTrain: epoch  0, batch     1 | loss: 7.5068321Losses:  7.628571510314941 1.435218095779419 0.974166989326477
CurrentTrain: epoch  0, batch     2 | loss: 10.0379562Losses:  4.249266147613525 0.3639000654220581 0.945949912071228
CurrentTrain: epoch  0, batch     3 | loss: 5.5591164Losses:  5.8299455642700195 1.3133554458618164 0.9703977108001709
CurrentTrain: epoch  1, batch     0 | loss: 8.1136990Losses:  4.958235740661621 1.4573886394500732 0.962857723236084
CurrentTrain: epoch  1, batch     1 | loss: 7.3784823Losses:  5.272687911987305 1.491633653640747 0.9764769077301025
CurrentTrain: epoch  1, batch     2 | loss: 7.7407980Losses:  4.234701156616211 0.25865423679351807 0.9540168046951294
CurrentTrain: epoch  1, batch     3 | loss: 5.4473720Losses:  5.198296546936035 1.3392972946166992 0.9698473215103149
CurrentTrain: epoch  2, batch     0 | loss: 7.5074410Losses:  5.179492950439453 1.5240005254745483 0.9643568992614746
CurrentTrain: epoch  2, batch     1 | loss: 7.6678505Losses:  4.374157905578613 1.3182785511016846 0.964688777923584
CurrentTrain: epoch  2, batch     2 | loss: 6.6571250Losses:  8.035991668701172 0.2626068592071533 0.9952477216720581
CurrentTrain: epoch  2, batch     3 | loss: 9.2938461Losses:  5.730924129486084 1.2098755836486816 0.9564594030380249
CurrentTrain: epoch  3, batch     0 | loss: 7.8972592Losses:  4.55300235748291 1.3074431419372559 0.9822105169296265
CurrentTrain: epoch  3, batch     1 | loss: 6.8426561Losses:  3.969170331954956 1.098802089691162 0.9590584635734558
CurrentTrain: epoch  3, batch     2 | loss: 6.0270305Losses:  3.711970329284668 0.40112119913101196 0.9844740629196167
CurrentTrain: epoch  3, batch     3 | loss: 5.0975657Losses:  5.257479190826416 1.346071720123291 0.960111141204834
CurrentTrain: epoch  4, batch     0 | loss: 7.5636621Losses:  3.9258267879486084 1.0386362075805664 0.9549153447151184
CurrentTrain: epoch  4, batch     1 | loss: 5.9193788Losses:  4.804595947265625 1.2208609580993652 0.9729317426681519
CurrentTrain: epoch  4, batch     2 | loss: 6.9983888Losses:  1.7470521926879883 5.960464477539063e-08 1.0
CurrentTrain: epoch  4, batch     3 | loss: 2.7470522Losses:  4.412719249725342 1.012904167175293 0.9687303304672241
CurrentTrain: epoch  5, batch     0 | loss: 6.3943539Losses:  4.850990295410156 1.185868263244629 0.9505267143249512
CurrentTrain: epoch  5, batch     1 | loss: 6.9873853Losses:  3.5553245544433594 0.8526270985603333 0.9590115547180176
CurrentTrain: epoch  5, batch     2 | loss: 5.3669634Losses:  6.286462783813477 1.788139627478813e-07 0.9674776792526245
CurrentTrain: epoch  5, batch     3 | loss: 7.2539406Losses:  3.9150853157043457 1.0415773391723633 0.9496731758117676
CurrentTrain: epoch  6, batch     0 | loss: 5.9063358Losses:  3.651268482208252 1.0466041564941406 0.9565991163253784
CurrentTrain: epoch  6, batch     1 | loss: 5.6544719Losses:  4.7054524421691895 1.1784579753875732 0.9670403003692627
CurrentTrain: epoch  6, batch     2 | loss: 6.8509502Losses:  3.239299774169922 0.18616965413093567 1.0
CurrentTrain: epoch  6, batch     3 | loss: 4.4254694Losses:  3.9984841346740723 1.0746419429779053 0.9393193125724792
CurrentTrain: epoch  7, batch     0 | loss: 6.0124450Losses:  3.6164767742156982 0.957984983921051 0.9672490358352661
CurrentTrain: epoch  7, batch     1 | loss: 5.5417109Losses:  3.8500816822052 1.0803077220916748 0.9568567276000977
CurrentTrain: epoch  7, batch     2 | loss: 5.8872461Losses:  4.410905838012695 0.18912357091903687 0.9984481334686279
CurrentTrain: epoch  7, batch     3 | loss: 5.5984774Losses:  3.5654845237731934 1.147855281829834 0.9408167600631714
CurrentTrain: epoch  8, batch     0 | loss: 5.6541567Losses:  4.058955669403076 1.003273606300354 0.9699186682701111
CurrentTrain: epoch  8, batch     1 | loss: 6.0321479Losses:  3.8312487602233887 1.0736645460128784 0.9605058431625366
CurrentTrain: epoch  8, batch     2 | loss: 5.8654194Losses:  1.9426519870758057 0.14831691980361938 0.890354573726654
CurrentTrain: epoch  8, batch     3 | loss: 2.9813235Losses:  3.2855212688446045 1.1087137460708618 0.9376471042633057
CurrentTrain: epoch  9, batch     0 | loss: 5.3318825Losses:  3.361151695251465 1.0406954288482666 0.9603284597396851
CurrentTrain: epoch  9, batch     1 | loss: 5.3621755Losses:  4.052693843841553 1.1490757465362549 0.9569387435913086
CurrentTrain: epoch  9, batch     2 | loss: 6.1587086Losses:  2.2726473808288574 0.14363789558410645 1.0
CurrentTrain: epoch  9, batch     3 | loss: 3.4162853
Losses:  1.1875951290130615 1.2703906297683716 0.9274061322212219
MemoryTrain:  epoch  0, batch     0 | loss: 3.3853920Losses:  0.8424832820892334 0.9662022590637207 0.9133846759796143
MemoryTrain:  epoch  0, batch     1 | loss: 2.7220702Losses:  0.9276545643806458 0.3625144958496094 0.8598898649215698
MemoryTrain:  epoch  0, batch     2 | loss: 2.1500587Losses:  0.924451470375061 1.0192222595214844 0.9041270017623901
MemoryTrain:  epoch  1, batch     0 | loss: 2.8478007Losses:  1.5101304054260254 0.8901203870773315 0.8974372148513794
MemoryTrain:  epoch  1, batch     1 | loss: 3.2976880Losses:  1.147040843963623 0.9006202816963196 0.9422051906585693
MemoryTrain:  epoch  1, batch     2 | loss: 2.9898663Losses:  0.48719578981399536 1.099022388458252 0.8887976408004761
MemoryTrain:  epoch  2, batch     0 | loss: 2.4750156Losses:  0.6968958973884583 0.851969838142395 0.9283734560012817
MemoryTrain:  epoch  2, batch     1 | loss: 2.4772391Losses:  0.8660094738006592 0.7054972648620605 0.9009782075881958
MemoryTrain:  epoch  2, batch     2 | loss: 2.4724851Losses:  0.5385537147521973 0.9580926895141602 0.9114196300506592
MemoryTrain:  epoch  3, batch     0 | loss: 2.4080660Losses:  0.5660272240638733 1.2548856735229492 0.9326964020729065
MemoryTrain:  epoch  3, batch     1 | loss: 2.7536092Losses:  0.10638123750686646 0.3372272253036499 0.8321022391319275
MemoryTrain:  epoch  3, batch     2 | loss: 1.2757107Losses:  0.17477859556674957 0.9958275556564331 0.9091873168945312
MemoryTrain:  epoch  4, batch     0 | loss: 2.0797935Losses:  0.33829742670059204 0.9402313232421875 0.9185663461685181
MemoryTrain:  epoch  4, batch     1 | loss: 2.1970949Losses:  0.19506916403770447 0.7609148025512695 0.8680450320243835
MemoryTrain:  epoch  4, batch     2 | loss: 1.8240290Losses:  0.18052645027637482 1.0888760089874268 0.9069551825523376
MemoryTrain:  epoch  5, batch     0 | loss: 2.1763577Losses:  0.3142661154270172 0.8647365570068359 0.9306350946426392
MemoryTrain:  epoch  5, batch     1 | loss: 2.1096377Losses:  0.2402377724647522 0.557616114616394 0.8404027223587036
MemoryTrain:  epoch  5, batch     2 | loss: 1.6382565Losses:  0.12416445463895798 0.9719488024711609 0.9185917377471924
MemoryTrain:  epoch  6, batch     0 | loss: 2.0147049Losses:  0.06267555058002472 0.9622213840484619 0.8802837133407593
MemoryTrain:  epoch  6, batch     1 | loss: 1.9051807Losses:  0.08384068310260773 0.5413000583648682 0.9184235334396362
MemoryTrain:  epoch  6, batch     2 | loss: 1.5435643Losses:  0.04389536380767822 0.8509259223937988 0.8803535103797913
MemoryTrain:  epoch  7, batch     0 | loss: 1.7751749Losses:  0.055692024528980255 1.0751248598098755 0.9219763278961182
MemoryTrain:  epoch  7, batch     1 | loss: 2.0527933Losses:  0.03591100126504898 0.6236440539360046 0.9158183336257935
MemoryTrain:  epoch  7, batch     2 | loss: 1.5753734Losses:  0.03811313211917877 0.7299632430076599 0.8966989517211914
MemoryTrain:  epoch  8, batch     0 | loss: 1.6647754Losses:  0.07721024751663208 0.8121777772903442 0.8994051218032837
MemoryTrain:  epoch  8, batch     1 | loss: 1.7887931Losses:  0.07923003286123276 0.8045089840888977 0.9074516892433167
MemoryTrain:  epoch  8, batch     2 | loss: 1.7911906Losses:  0.067517951130867 0.6722557544708252 0.9160686731338501
MemoryTrain:  epoch  9, batch     0 | loss: 1.6558423Losses:  0.03896711766719818 1.1846609115600586 0.8746650218963623
MemoryTrain:  epoch  9, batch     1 | loss: 2.0982931Losses:  0.06386363506317139 0.4906069040298462 0.9247229695320129
MemoryTrain:  epoch  9, batch     2 | loss: 1.4791934
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 71.66%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 69.90%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 69.67%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 68.25%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.02%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.09%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.30%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 91.06%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.92%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 89.48%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 88.57%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 87.50%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 87.03%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 86.19%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 85.20%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 84.87%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.73%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.77%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.59%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.54%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.58%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 83.72%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 83.04%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 82.21%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 81.72%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 80.56%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 81.46%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 81.32%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 80.98%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 80.64%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 80.17%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 79.85%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 80.08%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.09%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 80.37%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.63%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 80.84%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 80.44%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 80.05%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.39%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 78.96%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 78.54%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 77.99%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 77.75%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 77.51%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 77.17%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 77.05%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.14%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 77.13%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 76.77%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 76.42%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 76.11%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 75.58%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 75.24%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 75.14%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 75.28%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 75.63%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 75.18%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 74.73%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 74.34%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 73.86%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 73.47%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.22%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 74.42%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 74.80%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.54%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 75.42%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 75.11%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 74.81%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 74.18%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 73.85%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.74%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 74.21%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 74.36%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 74.23%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 74.20%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 74.10%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 73.83%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 73.85%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 73.92%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 73.91%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 73.76%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 73.66%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 73.60%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 73.65%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.71%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 73.71%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 73.63%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 73.67%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 73.52%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 73.32%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 73.12%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 72.26%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 72.21%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 73.79%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 73.91%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 73.97%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 73.92%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 73.77%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 73.72%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 73.67%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 73.52%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.76%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 73.62%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 73.37%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 73.23%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 73.03%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 72.78%   
cur_acc:  ['0.9435', '0.7440', '0.7748', '0.6825']
his_acc:  ['0.9435', '0.8470', '0.7922', '0.7278']
Clustering into  24  clusters
Clusters:  [ 4 23 19  4  4  4  0  4 15 16  1 21  3  4  4 12  4 22  4 10  4 17  4  4
 11  4  4  4 20 18  3  4 13  4 14  0  6  4  4  4  7  8  4  4  5  1  9  4
  4  2]
Losses:  4.622554779052734 1.6817405223846436 0.9738484621047974
CurrentTrain: epoch  0, batch     0 | loss: 7.2781439Losses:  4.532463073730469 1.2637073993682861 1.0062475204467773
CurrentTrain: epoch  0, batch     1 | loss: 6.8024178Losses:  5.450043678283691 1.7793853282928467 0.9987273216247559
CurrentTrain: epoch  0, batch     2 | loss: 8.2281570Losses:  4.58187198638916 0.4385232627391815 0.9754754304885864
CurrentTrain: epoch  0, batch     3 | loss: 5.9958706Losses:  4.286197185516357 1.320011854171753 0.9778357148170471
CurrentTrain: epoch  1, batch     0 | loss: 6.5840445Losses:  3.3619790077209473 1.0146068334579468 1.005680799484253
CurrentTrain: epoch  1, batch     1 | loss: 5.3822670Losses:  3.781018018722534 1.239455223083496 0.9949427247047424
CurrentTrain: epoch  1, batch     2 | loss: 6.0154161Losses:  4.876117706298828 0.41204118728637695 0.9515575170516968
CurrentTrain: epoch  1, batch     3 | loss: 6.2397165Losses:  3.5039596557617188 1.4885692596435547 0.9903550744056702
CurrentTrain: epoch  2, batch     0 | loss: 5.9828839Losses:  3.623994827270508 1.5569536685943604 0.9881958365440369
CurrentTrain: epoch  2, batch     1 | loss: 6.1691442Losses:  2.743180751800537 1.2421388626098633 0.9837396740913391
CurrentTrain: epoch  2, batch     2 | loss: 4.9690595Losses:  2.5077457427978516 0.27640625834465027 1.0
CurrentTrain: epoch  2, batch     3 | loss: 3.7841520Losses:  3.081125259399414 1.3102461099624634 0.9847524166107178
CurrentTrain: epoch  3, batch     0 | loss: 5.3761234Losses:  3.4216408729553223 1.3074334859848022 0.9806928038597107
CurrentTrain: epoch  3, batch     1 | loss: 5.7097673Losses:  2.57761812210083 1.0222206115722656 0.9863173365592957
CurrentTrain: epoch  3, batch     2 | loss: 4.5861559Losses:  2.383012294769287 0.4231787621974945 0.9732303619384766
CurrentTrain: epoch  3, batch     3 | loss: 3.7794213Losses:  3.0584752559661865 1.216050624847412 0.9683594703674316
CurrentTrain: epoch  4, batch     0 | loss: 5.2428851Losses:  2.265932321548462 1.1904019117355347 0.9852774143218994
CurrentTrain: epoch  4, batch     1 | loss: 4.4416113Losses:  2.814492702484131 0.8902866840362549 0.9713714122772217
CurrentTrain: epoch  4, batch     2 | loss: 4.6761508Losses:  2.7031333446502686 0.28439798951148987 1.0208216905593872
CurrentTrain: epoch  4, batch     3 | loss: 4.0083532Losses:  3.1379051208496094 0.8769612908363342 0.960963249206543
CurrentTrain: epoch  5, batch     0 | loss: 4.9758296Losses:  2.2456867694854736 0.9111459255218506 0.9945953488349915
CurrentTrain: epoch  5, batch     1 | loss: 4.1514282Losses:  2.5875751972198486 1.11048424243927 0.9615898132324219
CurrentTrain: epoch  5, batch     2 | loss: 4.6596494Losses:  1.850642442703247 0.2211529016494751 0.9600632190704346
CurrentTrain: epoch  5, batch     3 | loss: 3.0318587Losses:  2.142033338546753 0.9920074939727783 0.9809643030166626
CurrentTrain: epoch  6, batch     0 | loss: 4.1150050Losses:  2.3939642906188965 1.0297750234603882 0.9592885971069336
CurrentTrain: epoch  6, batch     1 | loss: 4.3830280Losses:  2.428402900695801 0.9314342737197876 0.9622088670730591
CurrentTrain: epoch  6, batch     2 | loss: 4.3220458Losses:  3.736720561981201 0.3094640076160431 0.9990226626396179
CurrentTrain: epoch  6, batch     3 | loss: 5.0452070Losses:  2.1846795082092285 1.0219371318817139 0.9563074111938477
CurrentTrain: epoch  7, batch     0 | loss: 4.1629238Losses:  2.239576816558838 1.0528862476348877 0.9728766679763794
CurrentTrain: epoch  7, batch     1 | loss: 4.2653399Losses:  2.076369285583496 0.7767691612243652 0.9685972332954407
CurrentTrain: epoch  7, batch     2 | loss: 3.8217356Losses:  2.4368741512298584 0.32491666078567505 0.9100459218025208
CurrentTrain: epoch  7, batch     3 | loss: 3.6718366Losses:  2.089573860168457 1.0288794040679932 0.968227744102478
CurrentTrain: epoch  8, batch     0 | loss: 4.0866809Losses:  2.114100933074951 0.9294559955596924 0.9699485301971436
CurrentTrain: epoch  8, batch     1 | loss: 4.0135055Losses:  2.145554780960083 0.9038621187210083 0.9392306804656982
CurrentTrain: epoch  8, batch     2 | loss: 3.9886477Losses:  1.757973074913025 0.0 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.7579732Losses:  2.0331850051879883 0.8394540548324585 0.9454739093780518
CurrentTrain: epoch  9, batch     0 | loss: 3.8181131Losses:  1.850388765335083 0.784016489982605 0.9664357900619507
CurrentTrain: epoch  9, batch     1 | loss: 3.6008410Losses:  1.9453907012939453 0.8874573707580566 0.9580526947975159
CurrentTrain: epoch  9, batch     2 | loss: 3.7909007Losses:  1.9937106370925903 0.13358351588249207 0.9907386898994446
CurrentTrain: epoch  9, batch     3 | loss: 3.1180327
Losses:  0.3678765594959259 0.907278299331665 0.9314666986465454
MemoryTrain:  epoch  0, batch     0 | loss: 2.2066216Losses:  0.9208896160125732 1.3318572044372559 0.9084575772285461
MemoryTrain:  epoch  0, batch     1 | loss: 3.1612043Losses:  0.2960653603076935 0.9306939840316772 0.881920337677002
MemoryTrain:  epoch  0, batch     2 | loss: 2.1086798Losses:  1.8827641010284424 0.019719822332262993 1.0
MemoryTrain:  epoch  0, batch     3 | loss: 2.9024839Losses:  0.35384517908096313 0.6912363767623901 0.8773453831672668
MemoryTrain:  epoch  1, batch     0 | loss: 1.9224269Losses:  1.16463041305542 1.186935544013977 0.9223097562789917
MemoryTrain:  epoch  1, batch     1 | loss: 3.2738757Losses:  0.6905831098556519 0.9368958473205566 0.9142947793006897
MemoryTrain:  epoch  1, batch     2 | loss: 2.5417738Losses:  1.0905168056488037 0.0942520946264267 0.9283140897750854
MemoryTrain:  epoch  1, batch     3 | loss: 2.1130829Losses:  0.2771626114845276 1.074103593826294 0.9109126329421997
MemoryTrain:  epoch  2, batch     0 | loss: 2.2621789Losses:  0.4253961145877838 0.8211777210235596 0.9037246108055115
MemoryTrain:  epoch  2, batch     1 | loss: 2.1502984Losses:  0.39396315813064575 1.005115270614624 0.9228008985519409
MemoryTrain:  epoch  2, batch     2 | loss: 2.3218794Losses:  0.08273068815469742 0.10745138674974442 0.7767220735549927
MemoryTrain:  epoch  2, batch     3 | loss: 0.9669042Losses:  0.34416285157203674 0.9660831689834595 0.8669102191925049
MemoryTrain:  epoch  3, batch     0 | loss: 2.1771562Losses:  0.21454328298568726 0.8016458749771118 0.9169625043869019
MemoryTrain:  epoch  3, batch     1 | loss: 1.9331516Losses:  0.2072497010231018 0.9670885801315308 0.9243226051330566
MemoryTrain:  epoch  3, batch     2 | loss: 2.0986609Losses:  0.13862648606300354 0.07047738879919052 0.9099656343460083
MemoryTrain:  epoch  3, batch     3 | loss: 1.1190696Losses:  0.15249177813529968 1.0590499639511108 0.9122569561004639
MemoryTrain:  epoch  4, batch     0 | loss: 2.1237988Losses:  0.14973454177379608 0.879641056060791 0.9194567799568176
MemoryTrain:  epoch  4, batch     1 | loss: 1.9488323Losses:  0.09298942983150482 0.7702293395996094 0.889872670173645
MemoryTrain:  epoch  4, batch     2 | loss: 1.7530915Losses:  0.025420591235160828 0.10431678593158722 0.7852810621261597
MemoryTrain:  epoch  4, batch     3 | loss: 0.9150184Losses:  0.10830749571323395 0.839834451675415 0.9441195726394653
MemoryTrain:  epoch  5, batch     0 | loss: 1.8922615Losses:  0.10078086704015732 0.8738940954208374 0.849963903427124
MemoryTrain:  epoch  5, batch     1 | loss: 1.8246388Losses:  0.12920814752578735 0.8270848989486694 0.9175505638122559
MemoryTrain:  epoch  5, batch     2 | loss: 1.8738437Losses:  0.40286698937416077 0.31863078474998474 0.8658772706985474
MemoryTrain:  epoch  5, batch     3 | loss: 1.5873750Losses:  0.14600954949855804 0.8720289468765259 0.9553813934326172
MemoryTrain:  epoch  6, batch     0 | loss: 1.9734199Losses:  0.08122701942920685 0.7819953560829163 0.8940534591674805
MemoryTrain:  epoch  6, batch     1 | loss: 1.7572758Losses:  0.10237947106361389 0.9617976546287537 0.8711443543434143
MemoryTrain:  epoch  6, batch     2 | loss: 1.9353216Losses:  0.1466720700263977 0.2069496065378189 0.7808718085289001
MemoryTrain:  epoch  6, batch     3 | loss: 1.1344935Losses:  0.1450384557247162 0.653704047203064 0.8908083438873291
MemoryTrain:  epoch  7, batch     0 | loss: 1.6895509Losses:  0.08677993714809418 0.9737486839294434 0.9280611276626587
MemoryTrain:  epoch  7, batch     1 | loss: 1.9885898Losses:  0.1880904734134674 0.8996593952178955 0.8771783113479614
MemoryTrain:  epoch  7, batch     2 | loss: 1.9649282Losses:  0.13081094622612 0.06082834675908089 0.9462118744850159
MemoryTrain:  epoch  7, batch     3 | loss: 1.1378511Losses:  0.05896802991628647 1.0514843463897705 0.9002814292907715
MemoryTrain:  epoch  8, batch     0 | loss: 2.0107338Losses:  0.048133328557014465 0.6152337789535522 0.9045315980911255
MemoryTrain:  epoch  8, batch     1 | loss: 1.5678988Losses:  0.04901524633169174 0.8453502655029297 0.8926882147789001
MemoryTrain:  epoch  8, batch     2 | loss: 1.7870537Losses:  0.052960705012083054 0.05404684320092201 0.9320666790008545
MemoryTrain:  epoch  8, batch     3 | loss: 1.0390742Losses:  0.07637736201286316 0.8714810609817505 0.9361637830734253
MemoryTrain:  epoch  9, batch     0 | loss: 1.8840222Losses:  0.0553172342479229 0.8775407671928406 0.8685615062713623
MemoryTrain:  epoch  9, batch     1 | loss: 1.8014195Losses:  0.06270518898963928 0.8900132179260254 0.8891040682792664
MemoryTrain:  epoch  9, batch     2 | loss: 1.8418224Losses:  0.019480625167489052 0.0069335210137069225 0.8216030597686768
MemoryTrain:  epoch  9, batch     3 | loss: 0.8480172
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 76.35%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.38%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.11%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 82.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.97%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.33%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.56%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.04%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 88.36%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.03%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.60%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.30%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 86.71%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 85.84%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 83.49%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 82.81%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.38%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 82.45%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 81.91%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 81.17%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 80.37%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 79.91%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.45%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 78.78%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 79.07%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 79.80%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 79.97%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 79.56%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 79.17%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 78.85%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 78.60%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 78.02%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 77.93%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 77.99%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 78.03%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 78.03%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 77.66%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 77.21%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 76.52%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 76.02%   [EVAL] batch:  104 | acc: 25.00%,  total acc: 75.54%   [EVAL] batch:  105 | acc: 31.25%,  total acc: 75.12%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 74.82%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 74.54%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 74.25%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 73.98%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 73.76%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 72.75%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 72.39%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 72.09%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 71.90%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 71.50%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 71.32%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 72.12%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 71.65%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 71.24%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 70.98%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 70.43%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 70.04%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 70.03%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 70.88%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 70.46%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 70.09%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 69.77%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 69.32%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 68.97%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 70.58%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 71.19%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.47%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 71.51%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 71.27%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 70.98%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 70.78%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 70.47%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 70.20%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 71.04%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 70.86%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 70.80%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 70.65%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 70.43%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 70.32%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 70.06%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 70.09%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 69.91%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 69.73%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 69.62%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 69.56%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 69.73%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 69.85%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 70.02%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 70.01%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 70.07%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 70.03%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 69.90%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 69.68%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 69.47%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 69.17%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 68.90%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 68.60%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 68.52%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 70.13%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 70.18%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 70.25%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 70.14%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 70.08%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 70.05%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 69.94%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.93%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 70.18%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 70.15%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 69.94%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 69.66%   [EVAL] batch:  248 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 70.48%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 70.38%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 70.19%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 69.94%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 69.71%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 69.53%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 69.39%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 70.20%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 70.27%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 70.33%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 72.03%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 72.18%   
cur_acc:  ['0.9435', '0.7440', '0.7748', '0.6825', '0.8333']
his_acc:  ['0.9435', '0.8470', '0.7922', '0.7278', '0.7218']
Clustering into  28  clusters
Clusters:  [ 5 19 22  5  5  5 26  5 18  1  0 24  4  5  5 15  5 25  5 27  5 20  5  5
  2  5  5  5 17 21  4  5 16  5  8 14  7  5  5  5  3 10  5  5 13  0 23  5
  5  9  5  5  5 12  5  1  5  6 11  2]
Losses:  6.235323905944824 1.3606672286987305 0.9461064338684082
CurrentTrain: epoch  0, batch     0 | loss: 8.5420971Losses:  6.670644760131836 1.3831101655960083 0.9651792049407959
CurrentTrain: epoch  0, batch     1 | loss: 9.0189342Losses:  5.66147518157959 1.0821402072906494 0.9708141088485718
CurrentTrain: epoch  0, batch     2 | loss: 7.7144294Losses:  8.036079406738281 0.4633517861366272 0.9873002767562866
CurrentTrain: epoch  0, batch     3 | loss: 9.4867315Losses:  4.970648765563965 1.0920970439910889 0.9483580589294434
CurrentTrain: epoch  1, batch     0 | loss: 7.0111041Losses:  5.610040664672852 1.1728804111480713 0.9510498642921448
CurrentTrain: epoch  1, batch     1 | loss: 7.7339706Losses:  4.960341930389404 1.1106922626495361 0.963703989982605
CurrentTrain: epoch  1, batch     2 | loss: 7.0347385Losses:  3.9154551029205322 0.15690921247005463 0.9756307601928711
CurrentTrain: epoch  1, batch     3 | loss: 5.0479951Losses:  4.335837364196777 1.080556869506836 0.9426404237747192
CurrentTrain: epoch  2, batch     0 | loss: 6.3590345Losses:  4.3147478103637695 0.896306037902832 0.953203558921814
CurrentTrain: epoch  2, batch     1 | loss: 6.1642575Losses:  4.058465003967285 0.8814691305160522 0.9635398983955383
CurrentTrain: epoch  2, batch     2 | loss: 5.9034743Losses:  5.732087135314941 0.4067313075065613 0.9590756893157959
CurrentTrain: epoch  2, batch     3 | loss: 7.0978937Losses:  4.006631851196289 1.2434272766113281 0.9503637552261353
CurrentTrain: epoch  3, batch     0 | loss: 6.2004228Losses:  3.532799482345581 0.6486648321151733 0.9507634043693542
CurrentTrain: epoch  3, batch     1 | loss: 5.1322274Losses:  3.696699619293213 1.0543606281280518 0.960361897945404
CurrentTrain: epoch  3, batch     2 | loss: 5.7114224Losses:  4.399618148803711 0.2472844123840332 0.9113477468490601
CurrentTrain: epoch  3, batch     3 | loss: 5.5582504Losses:  3.9038476943969727 1.151097059249878 0.9431401491165161
CurrentTrain: epoch  4, batch     0 | loss: 5.9980850Losses:  3.790276527404785 0.7736890316009521 0.9375408291816711
CurrentTrain: epoch  4, batch     1 | loss: 5.5015068Losses:  3.3285725116729736 0.6983902454376221 0.9683219194412231
CurrentTrain: epoch  4, batch     2 | loss: 4.9952846Losses:  2.11683988571167 0.0443573072552681 1.0
CurrentTrain: epoch  4, batch     3 | loss: 3.1611972Losses:  3.651350736618042 0.7166627645492554 0.953156054019928
CurrentTrain: epoch  5, batch     0 | loss: 5.3211694Losses:  3.2237367630004883 0.9180094003677368 0.9531936645507812
CurrentTrain: epoch  5, batch     1 | loss: 5.0949397Losses:  3.234233856201172 0.8509008884429932 0.940402090549469
CurrentTrain: epoch  5, batch     2 | loss: 5.0255365Losses:  2.725296974182129 0.22433677315711975 0.9283991456031799
CurrentTrain: epoch  5, batch     3 | loss: 3.8780329Losses:  3.672590732574463 0.7765882015228271 0.9333458542823792
CurrentTrain: epoch  6, batch     0 | loss: 5.3825245Losses:  2.858685255050659 0.7309797406196594 0.957717776298523
CurrentTrain: epoch  6, batch     1 | loss: 4.5473828Losses:  2.86753511428833 0.7625110149383545 0.9485756754875183
CurrentTrain: epoch  6, batch     2 | loss: 4.5786219Losses:  2.1203269958496094 0.18308153748512268 0.9660660028457642
CurrentTrain: epoch  6, batch     3 | loss: 3.2694745Losses:  3.1109988689422607 0.8386563658714294 0.9161749482154846
CurrentTrain: epoch  7, batch     0 | loss: 4.8658304Losses:  2.718104600906372 0.4887883961200714 0.9611048102378845
CurrentTrain: epoch  7, batch     1 | loss: 4.1679978Losses:  2.4457058906555176 0.6297186017036438 0.9430447816848755
CurrentTrain: epoch  7, batch     2 | loss: 4.0184693Losses:  2.639828681945801 0.331339955329895 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.9711685Losses:  2.230151891708374 0.6350805759429932 0.942610502243042
CurrentTrain: epoch  8, batch     0 | loss: 3.8078430Losses:  2.855372667312622 0.7848146557807922 0.9438363313674927
CurrentTrain: epoch  8, batch     1 | loss: 4.5840235Losses:  2.6196179389953613 0.603238046169281 0.9414631724357605
CurrentTrain: epoch  8, batch     2 | loss: 4.1643190Losses:  2.458648204803467 0.17793533205986023 0.9238934516906738
CurrentTrain: epoch  8, batch     3 | loss: 3.5604770Losses:  2.2211813926696777 0.5723977088928223 0.9588193297386169
CurrentTrain: epoch  9, batch     0 | loss: 3.7523985Losses:  2.6336660385131836 0.6046289205551147 0.9222233295440674
CurrentTrain: epoch  9, batch     1 | loss: 4.1605186Losses:  2.5313286781311035 0.7132585644721985 0.931878924369812
CurrentTrain: epoch  9, batch     2 | loss: 4.1764660Losses:  1.906572699546814 0.03766653314232826 0.9953644275665283
CurrentTrain: epoch  9, batch     3 | loss: 2.9396038
Losses:  0.8667259812355042 0.9038740396499634 0.9114702939987183
MemoryTrain:  epoch  0, batch     0 | loss: 2.6820703Losses:  0.5830478668212891 1.0107628107070923 0.8809621334075928
MemoryTrain:  epoch  0, batch     1 | loss: 2.4747729Losses:  0.7821139097213745 0.8144849538803101 0.934329092502594
MemoryTrain:  epoch  0, batch     2 | loss: 2.5309279Losses:  0.7511982917785645 0.7921032905578613 0.8953661918640137
MemoryTrain:  epoch  0, batch     3 | loss: 2.4386678Losses:  1.6040043830871582 0.8817212581634521 0.8982717990875244
MemoryTrain:  epoch  1, batch     0 | loss: 3.3839974Losses:  0.8367907404899597 0.7917839288711548 0.8961559534072876
MemoryTrain:  epoch  1, batch     1 | loss: 2.5247307Losses:  0.22819778323173523 0.7650924324989319 0.9075184464454651
MemoryTrain:  epoch  1, batch     2 | loss: 1.9008086Losses:  0.5532474517822266 0.7647112607955933 0.8987610340118408
MemoryTrain:  epoch  1, batch     3 | loss: 2.2167196Losses:  0.33383819460868835 0.9092056751251221 0.9121475219726562
MemoryTrain:  epoch  2, batch     0 | loss: 2.1551914Losses:  0.6009273529052734 0.8971496820449829 0.9128402471542358
MemoryTrain:  epoch  2, batch     1 | loss: 2.4109173Losses:  0.6730523109436035 0.6692702174186707 0.9127534627914429
MemoryTrain:  epoch  2, batch     2 | loss: 2.2550759Losses:  0.42010697722435 0.8052675127983093 0.8623446226119995
MemoryTrain:  epoch  2, batch     3 | loss: 2.0877190Losses:  0.869001567363739 0.8005778789520264 0.8426071405410767
MemoryTrain:  epoch  3, batch     0 | loss: 2.5121865Losses:  0.13750529289245605 0.7419853806495667 0.903954267501831
MemoryTrain:  epoch  3, batch     1 | loss: 1.7834449Losses:  0.14462235569953918 0.9843437671661377 0.9216005802154541
MemoryTrain:  epoch  3, batch     2 | loss: 2.0505667Losses:  0.22334973514080048 0.6807000041007996 0.9464266300201416
MemoryTrain:  epoch  3, batch     3 | loss: 1.8504764Losses:  0.07252217829227448 0.7344799637794495 0.8886057138442993
MemoryTrain:  epoch  4, batch     0 | loss: 1.6956079Losses:  0.07987172901630402 0.7974977493286133 0.8775761723518372
MemoryTrain:  epoch  4, batch     1 | loss: 1.7549456Losses:  0.5393705368041992 1.0109777450561523 0.9057152271270752
MemoryTrain:  epoch  4, batch     2 | loss: 2.4560635Losses:  0.606399416923523 0.47651439905166626 0.9363096952438354
MemoryTrain:  epoch  4, batch     3 | loss: 2.0192237Losses:  0.4508701264858246 0.8909353613853455 0.9218916893005371
MemoryTrain:  epoch  5, batch     0 | loss: 2.2636971Losses:  0.12686803936958313 0.8799219131469727 0.8831309080123901
MemoryTrain:  epoch  5, batch     1 | loss: 1.8899208Losses:  0.3860756456851959 0.6812275648117065 0.9144694805145264
MemoryTrain:  epoch  5, batch     2 | loss: 1.9817727Losses:  0.057976432144641876 0.5772013664245605 0.8720149397850037
MemoryTrain:  epoch  5, batch     3 | loss: 1.5071927Losses:  0.13915470242500305 1.0073180198669434 0.8959412574768066
MemoryTrain:  epoch  6, batch     0 | loss: 2.0424139Losses:  0.4046839475631714 0.4982307553291321 0.9009960889816284
MemoryTrain:  epoch  6, batch     1 | loss: 1.8039107Losses:  0.39085328578948975 0.8006027936935425 0.8744114637374878
MemoryTrain:  epoch  6, batch     2 | loss: 2.0658674Losses:  0.15066149830818176 0.6376677751541138 0.9223780632019043
MemoryTrain:  epoch  6, batch     3 | loss: 1.7107073Losses:  0.3042795658111572 0.7080063819885254 0.9058383703231812
MemoryTrain:  epoch  7, batch     0 | loss: 1.9181243Losses:  0.06312239170074463 0.9338045120239258 0.8943716883659363
MemoryTrain:  epoch  7, batch     1 | loss: 1.8912985Losses:  0.27352994680404663 0.744231104850769 0.9166287183761597
MemoryTrain:  epoch  7, batch     2 | loss: 1.9343897Losses:  0.09845056384801865 0.5060980319976807 0.8614992499351501
MemoryTrain:  epoch  7, batch     3 | loss: 1.4660478Losses:  0.053813815116882324 0.6675901412963867 0.8841838836669922
MemoryTrain:  epoch  8, batch     0 | loss: 1.6055878Losses:  0.17420829832553864 0.7518216371536255 0.9245072603225708
MemoryTrain:  epoch  8, batch     1 | loss: 1.8505372Losses:  0.0992322713136673 0.9681746959686279 0.8664798140525818
MemoryTrain:  epoch  8, batch     2 | loss: 1.9338868Losses:  0.06464995443820953 0.5223736763000488 0.9121764898300171
MemoryTrain:  epoch  8, batch     3 | loss: 1.4992001Losses:  0.11056391894817352 0.7248042225837708 0.8419051766395569
MemoryTrain:  epoch  9, batch     0 | loss: 1.6772733Losses:  0.048549674451351166 0.7792239785194397 0.9241671562194824
MemoryTrain:  epoch  9, batch     1 | loss: 1.7519407Losses:  0.2780131995677948 0.8451434969902039 0.9090864062309265
MemoryTrain:  epoch  9, batch     2 | loss: 2.0322430Losses:  0.03801162540912628 0.41703543066978455 0.9002370834350586
MemoryTrain:  epoch  9, batch     3 | loss: 1.3552842
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.03%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 67.12%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 66.09%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 65.10%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 64.16%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 63.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.32%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.03%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 87.93%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 87.39%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 86.89%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.59%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 86.31%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 85.45%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 84.42%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 83.30%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 82.44%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 82.07%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.96%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 82.04%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 82.02%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 81.93%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 81.41%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 80.76%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 79.97%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 79.43%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 78.98%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 78.32%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 78.35%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 79.26%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 78.79%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 78.19%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 77.82%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 77.31%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 76.81%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 76.60%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 76.71%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 76.93%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 77.04%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 76.98%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 76.65%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 76.09%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 75.78%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 75.42%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 75.12%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 74.94%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 74.71%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 74.20%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 73.99%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.66%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 72.75%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 72.39%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 72.04%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 71.35%   [EVAL] batch:  118 | acc: 37.50%,  total acc: 71.06%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 71.57%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 71.73%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 71.26%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 70.80%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 70.49%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 69.95%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 69.56%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 69.41%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 70.15%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.65%   [EVAL] batch:  139 | acc: 0.00%,  total acc: 69.15%   [EVAL] batch:  140 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 68.22%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 67.83%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 70.32%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 70.01%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 69.70%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 69.39%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 69.05%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 68.93%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 69.35%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 69.28%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 68.99%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 68.65%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 68.44%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 68.34%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 68.20%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 68.13%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 68.07%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.11%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 68.02%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 67.93%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 68.20%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 67.88%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 67.67%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 67.38%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 67.12%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 66.83%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 68.64%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 68.72%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 68.51%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 68.32%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 68.16%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 68.09%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 67.93%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 67.94%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 68.11%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 67.86%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 67.64%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 67.14%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 66.95%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 67.99%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 67.85%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 67.70%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 67.61%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 68.31%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 68.33%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 68.34%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.85%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 70.16%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 70.26%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 70.52%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 70.72%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 70.73%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 70.74%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 70.75%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 70.66%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 70.66%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 70.57%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 70.58%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:  340 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 70.56%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 70.52%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 70.42%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 70.38%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 70.32%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 70.17%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 70.13%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 70.11%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 70.10%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 70.03%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 69.87%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 69.74%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 69.48%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 69.37%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 69.95%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.17%   
cur_acc:  ['0.9435', '0.7440', '0.7748', '0.6825', '0.8333', '0.6935']
his_acc:  ['0.9435', '0.8470', '0.7922', '0.7278', '0.7218', '0.7017']
Clustering into  34  clusters
Clusters:  [ 0  9 20  3  0  0 29  0 22  1 31 27  4  0  0 17  0 28  0 33  0 24  0  0
 16  0  0  0 21 25  4  0 18  0 19 26  8  0  0  0  3 13  0  0 30 14 23  0
  0 32  0  0  0 10  0  1  0  6 12 15  9 11  0  0  0  5  0  0  2  7]
Losses:  6.899313449859619 1.1006803512573242 0.9889532327651978
CurrentTrain: epoch  0, batch     0 | loss: 8.9889469Losses:  5.207518577575684 1.437248706817627 0.985133707523346
CurrentTrain: epoch  0, batch     1 | loss: 7.6299009Losses:  5.236598968505859 1.1157147884368896 0.9483346343040466
CurrentTrain: epoch  0, batch     2 | loss: 7.3006487Losses:  7.614532470703125 0.5332923531532288 0.9704716205596924
CurrentTrain: epoch  0, batch     3 | loss: 9.1182966Losses:  5.6120781898498535 1.2305127382278442 0.9786761403083801
CurrentTrain: epoch  1, batch     0 | loss: 7.8212671Losses:  4.575131416320801 0.9757230281829834 0.9766994714736938
CurrentTrain: epoch  1, batch     1 | loss: 6.5275540Losses:  4.369274616241455 1.2916004657745361 0.9614989757537842
CurrentTrain: epoch  1, batch     2 | loss: 6.6223745Losses:  2.6283085346221924 0.24537277221679688 0.925929069519043
CurrentTrain: epoch  1, batch     3 | loss: 3.7996104Losses:  4.361624717712402 0.9581291675567627 0.9635066986083984
CurrentTrain: epoch  2, batch     0 | loss: 6.2832603Losses:  3.6541054248809814 1.1622191667556763 0.949821949005127
CurrentTrain: epoch  2, batch     1 | loss: 5.7661467Losses:  4.076844692230225 1.176823616027832 0.9951372146606445
CurrentTrain: epoch  2, batch     2 | loss: 6.2488055Losses:  3.341594934463501 0.19524787366390228 0.9454856514930725
CurrentTrain: epoch  2, batch     3 | loss: 4.4823284Losses:  3.9178414344787598 1.2147958278656006 0.963271975517273
CurrentTrain: epoch  3, batch     0 | loss: 6.0959091Losses:  3.7647628784179688 1.0959402322769165 0.9681628346443176
CurrentTrain: epoch  3, batch     1 | loss: 5.8288660Losses:  3.5764496326446533 1.0855087041854858 0.9591073393821716
CurrentTrain: epoch  3, batch     2 | loss: 5.6210656Losses:  5.245067596435547 0.1380625069141388 0.9842268228530884
CurrentTrain: epoch  3, batch     3 | loss: 6.3673568Losses:  4.118572235107422 1.2586150169372559 0.9501024484634399
CurrentTrain: epoch  4, batch     0 | loss: 6.3272896Losses:  2.8772101402282715 0.9918838739395142 0.9654603600502014
CurrentTrain: epoch  4, batch     1 | loss: 4.8345542Losses:  3.380228042602539 0.7849987149238586 0.9779544472694397
CurrentTrain: epoch  4, batch     2 | loss: 5.1431813Losses:  3.146925449371338 0.18522290885448456 0.9379693865776062
CurrentTrain: epoch  4, batch     3 | loss: 4.2701178Losses:  3.008571147918701 0.7989919185638428 0.9822696447372437
CurrentTrain: epoch  5, batch     0 | loss: 4.7898326Losses:  2.9477033615112305 1.0153876543045044 0.954964280128479
CurrentTrain: epoch  5, batch     1 | loss: 4.9180551Losses:  3.2994964122772217 1.0123465061187744 0.9390277862548828
CurrentTrain: epoch  5, batch     2 | loss: 5.2508707Losses:  6.2043867111206055 1.1920930376163597e-07 1.0007115602493286
CurrentTrain: epoch  5, batch     3 | loss: 7.2050982Losses:  3.0168519020080566 1.0099278688430786 0.9386039972305298
CurrentTrain: epoch  6, batch     0 | loss: 4.9653835Losses:  2.9780216217041016 0.8941411375999451 0.9639482498168945
CurrentTrain: epoch  6, batch     1 | loss: 4.8361111Losses:  2.816659688949585 0.9742734432220459 0.9711766242980957
CurrentTrain: epoch  6, batch     2 | loss: 4.7621098Losses:  4.4290666580200195 0.3431926965713501 0.9715108275413513
CurrentTrain: epoch  6, batch     3 | loss: 5.7437701Losses:  2.800710439682007 0.9883841276168823 0.961738646030426
CurrentTrain: epoch  7, batch     0 | loss: 4.7508330Losses:  3.1363868713378906 0.9794148206710815 0.9499965906143188
CurrentTrain: epoch  7, batch     1 | loss: 5.0657983Losses:  2.643974542617798 0.744641900062561 0.9590145349502563
CurrentTrain: epoch  7, batch     2 | loss: 4.3476310Losses:  2.355466365814209 0.16422076523303986 0.9361579418182373
CurrentTrain: epoch  7, batch     3 | loss: 3.4558451Losses:  2.7017998695373535 0.8419994115829468 0.9430548548698425
CurrentTrain: epoch  8, batch     0 | loss: 4.4868541Losses:  2.6129283905029297 0.7059555053710938 0.9568386673927307
CurrentTrain: epoch  8, batch     1 | loss: 4.2757225Losses:  2.8209784030914307 0.9071732759475708 0.9467971324920654
CurrentTrain: epoch  8, batch     2 | loss: 4.6749487Losses:  3.743412971496582 0.15857329964637756 0.9621855020523071
CurrentTrain: epoch  8, batch     3 | loss: 4.8641720Losses:  2.6016364097595215 0.9612041115760803 0.9406904578208923
CurrentTrain: epoch  9, batch     0 | loss: 4.5035310Losses:  2.51316499710083 0.8749191761016846 0.957484781742096
CurrentTrain: epoch  9, batch     1 | loss: 4.3455691Losses:  2.6699695587158203 0.8527286052703857 0.9589414000511169
CurrentTrain: epoch  9, batch     2 | loss: 4.4816394Losses:  2.301201820373535 0.09740113466978073 0.8523616790771484
CurrentTrain: epoch  9, batch     3 | loss: 3.2509646
Losses:  0.5090279579162598 0.8879760503768921 0.8984560966491699
MemoryTrain:  epoch  0, batch     0 | loss: 2.2954602Losses:  0.6067458391189575 0.7326494455337524 0.9263261556625366
MemoryTrain:  epoch  0, batch     1 | loss: 2.2657213Losses:  0.9797346591949463 1.065843105316162 0.8647794127464294
MemoryTrain:  epoch  0, batch     2 | loss: 2.9103572Losses:  0.823009729385376 0.8301401734352112 0.9277894496917725
MemoryTrain:  epoch  0, batch     3 | loss: 2.5809393Losses:  0.03475269675254822 0.2913668751716614 0.9428513050079346
MemoryTrain:  epoch  0, batch     4 | loss: 1.2689708Losses:  0.5626716017723083 0.853447437286377 0.8995803594589233
MemoryTrain:  epoch  1, batch     0 | loss: 2.3156996Losses:  0.7695944309234619 0.7307009696960449 0.9292680025100708
MemoryTrain:  epoch  1, batch     1 | loss: 2.4295635Losses:  0.7143266201019287 0.8606849908828735 0.905092716217041
MemoryTrain:  epoch  1, batch     2 | loss: 2.4801044Losses:  1.2847946882247925 0.8518209457397461 0.8978164196014404
MemoryTrain:  epoch  1, batch     3 | loss: 3.0344322Losses:  0.6371499300003052 0.3157739043235779 0.9091156721115112
MemoryTrain:  epoch  1, batch     4 | loss: 1.8620396Losses:  0.4763035476207733 0.7181713581085205 0.9159258604049683
MemoryTrain:  epoch  2, batch     0 | loss: 2.1104007Losses:  0.3478583097457886 0.8071476817131042 0.9297908544540405
MemoryTrain:  epoch  2, batch     1 | loss: 2.0847969Losses:  0.6419830322265625 0.885323166847229 0.902435302734375
MemoryTrain:  epoch  2, batch     2 | loss: 2.4297414Losses:  0.2868613004684448 0.6635674238204956 0.869712769985199
MemoryTrain:  epoch  2, batch     3 | loss: 1.8201416Losses:  0.2619951367378235 0.7236212491989136 0.9099496603012085
MemoryTrain:  epoch  2, batch     4 | loss: 1.8955660Losses:  0.34313198924064636 0.6848690509796143 0.9040688276290894
MemoryTrain:  epoch  3, batch     0 | loss: 1.9320699Losses:  0.1282511204481125 0.8375768065452576 0.909741997718811
MemoryTrain:  epoch  3, batch     1 | loss: 1.8755699Losses:  0.36764073371887207 0.9793851375579834 0.8900946378707886
MemoryTrain:  epoch  3, batch     2 | loss: 2.2371206Losses:  0.2580112814903259 0.822237491607666 0.9151244163513184
MemoryTrain:  epoch  3, batch     3 | loss: 1.9953732Losses:  0.2031855434179306 0.28197145462036133 0.8986943960189819
MemoryTrain:  epoch  3, batch     4 | loss: 1.3838514Losses:  0.1040511280298233 0.8680911064147949 0.9350215196609497
MemoryTrain:  epoch  4, batch     0 | loss: 1.9071637Losses:  0.0895753726363182 0.9382466077804565 0.8922716975212097
MemoryTrain:  epoch  4, batch     1 | loss: 1.9200938Losses:  0.11188365519046783 0.581395149230957 0.9130522012710571
MemoryTrain:  epoch  4, batch     2 | loss: 1.6063310Losses:  0.10834314674139023 0.7130564451217651 0.9000810384750366
MemoryTrain:  epoch  4, batch     3 | loss: 1.7214806Losses:  0.12839052081108093 0.35043537616729736 0.8248071670532227
MemoryTrain:  epoch  4, batch     4 | loss: 1.3036331Losses:  0.09578189253807068 0.8168071508407593 0.9301363229751587
MemoryTrain:  epoch  5, batch     0 | loss: 1.8427254Losses:  0.09142173826694489 0.671017050743103 0.8898319602012634
MemoryTrain:  epoch  5, batch     1 | loss: 1.6522708Losses:  0.08888393640518188 0.7322734594345093 0.8957300186157227
MemoryTrain:  epoch  5, batch     2 | loss: 1.7168875Losses:  0.08790378272533417 0.723956823348999 0.9101089239120483
MemoryTrain:  epoch  5, batch     3 | loss: 1.7219696Losses:  0.06154608353972435 0.45070886611938477 0.8678270578384399
MemoryTrain:  epoch  5, batch     4 | loss: 1.3800820Losses:  0.06333506107330322 0.7892800569534302 0.8854514956474304
MemoryTrain:  epoch  6, batch     0 | loss: 1.7380667Losses:  0.0658964067697525 0.5520941019058228 0.9310973882675171
MemoryTrain:  epoch  6, batch     1 | loss: 1.5490879Losses:  0.07119692116975784 0.8039647340774536 0.911810040473938
MemoryTrain:  epoch  6, batch     2 | loss: 1.7869717Losses:  0.03777658939361572 0.7128610610961914 0.8616416454315186
MemoryTrain:  epoch  6, batch     3 | loss: 1.6122793Losses:  0.2300795614719391 0.44928738474845886 0.922605037689209
MemoryTrain:  epoch  6, batch     4 | loss: 1.6019720Losses:  0.05960647016763687 0.7110376358032227 0.9207902550697327
MemoryTrain:  epoch  7, batch     0 | loss: 1.6914344Losses:  0.08798898756504059 0.7938169240951538 0.914392352104187
MemoryTrain:  epoch  7, batch     1 | loss: 1.7961982Losses:  0.07335032522678375 0.6978281736373901 0.8857485055923462
MemoryTrain:  epoch  7, batch     2 | loss: 1.6569270Losses:  0.16888336837291718 0.6625190377235413 0.8657964468002319
MemoryTrain:  epoch  7, batch     3 | loss: 1.6971989Losses:  0.20215678215026855 0.3616269528865814 0.9287559986114502
MemoryTrain:  epoch  7, batch     4 | loss: 1.4925398Losses:  0.07425722479820251 0.7311267852783203 0.9062522649765015
MemoryTrain:  epoch  8, batch     0 | loss: 1.7116363Losses:  0.034659259021282196 0.6115962266921997 0.92679762840271
MemoryTrain:  epoch  8, batch     1 | loss: 1.5730531Losses:  0.06724651157855988 0.7424973249435425 0.8875498175621033
MemoryTrain:  epoch  8, batch     2 | loss: 1.6972936Losses:  0.0508447028696537 0.6584967970848083 0.9142128825187683
MemoryTrain:  epoch  8, batch     3 | loss: 1.6235545Losses:  0.09800869226455688 0.614815890789032 0.8076912760734558
MemoryTrain:  epoch  8, batch     4 | loss: 1.5205159Losses:  0.044990286231040955 0.7496799230575562 0.8868168592453003
MemoryTrain:  epoch  9, batch     0 | loss: 1.6814871Losses:  0.03641752153635025 0.49263954162597656 0.8992788791656494
MemoryTrain:  epoch  9, batch     1 | loss: 1.4283359Losses:  0.06283499300479889 0.8818751573562622 0.9001361131668091
MemoryTrain:  epoch  9, batch     2 | loss: 1.8448462Losses:  0.05732095614075661 0.5951193571090698 0.9028461575508118
MemoryTrain:  epoch  9, batch     3 | loss: 1.5552864Losses:  0.09305184334516525 0.5197228193283081 0.9035810232162476
MemoryTrain:  epoch  9, batch     4 | loss: 1.5163558
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 72.45%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 69.56%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 66.36%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 64.82%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 61.82%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 61.68%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 61.86%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 63.32%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.35%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.61%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 86.44%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.34%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 86.02%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 85.31%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 84.48%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 84.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 83.81%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.57%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 83.23%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 82.42%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 81.63%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 81.34%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.60%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 79.69%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 79.44%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.73%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.75%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 78.95%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 78.25%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 77.48%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 77.06%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 76.64%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 76.00%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 75.99%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 77.06%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 76.54%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 75.90%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 75.48%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 74.33%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 73.94%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.42%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 74.75%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 74.45%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 73.55%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 73.34%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 73.12%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 72.66%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 72.35%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 71.77%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 70.96%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 70.67%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 70.29%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 70.12%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 70.88%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 70.52%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 70.26%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 69.86%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 69.47%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 69.23%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 69.08%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.76%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 69.93%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.51%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.06%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 68.62%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 68.22%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 68.01%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.80%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 69.76%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 70.67%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 70.35%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 70.11%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 69.80%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 69.54%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 69.27%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 69.85%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 69.67%   [EVAL] batch:  177 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 68.92%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 68.72%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 68.55%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 68.45%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 68.38%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 68.18%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 68.10%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 67.97%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 67.79%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 67.70%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 67.68%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 67.68%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 67.72%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 67.96%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 67.75%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 67.52%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 67.25%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 66.96%   [EVAL] batch:  210 | acc: 0.00%,  total acc: 66.65%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 66.36%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 66.29%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 68.20%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 68.43%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 68.24%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 68.14%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 68.03%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 67.93%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.04%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 68.19%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 67.94%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 67.71%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 67.46%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 67.03%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 68.10%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.13%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 68.28%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.24%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 68.06%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 67.99%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 67.85%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 67.68%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 67.52%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.76%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 68.40%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 68.38%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 68.34%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 69.83%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 69.91%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 70.29%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 70.35%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 70.32%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 70.28%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 70.27%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 70.29%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 70.48%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 70.58%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 70.63%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 70.63%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 70.60%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 70.64%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 70.69%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 70.69%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 70.61%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 70.58%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 70.52%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 70.52%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 70.46%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 70.29%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 70.21%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 70.15%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 70.16%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 70.03%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 70.04%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 70.08%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 70.01%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 69.87%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 69.74%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 69.48%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 69.37%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  375 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 70.25%   [EVAL] batch:  379 | acc: 81.25%,  total acc: 70.28%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  384 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 70.16%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 70.15%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 70.13%   [EVAL] batch:  390 | acc: 37.50%,  total acc: 70.04%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 70.03%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 69.99%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 70.02%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 70.33%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 70.37%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 70.27%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  403 | acc: 56.25%,  total acc: 70.22%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 70.07%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  407 | acc: 37.50%,  total acc: 69.96%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 69.80%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 69.66%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:  411 | acc: 6.25%,  total acc: 69.37%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 69.34%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 69.40%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 69.37%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 69.30%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 69.28%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 69.24%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 69.18%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.70%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.86%   
cur_acc:  ['0.9435', '0.7440', '0.7748', '0.6825', '0.8333', '0.6935', '0.6835']
his_acc:  ['0.9435', '0.8470', '0.7922', '0.7278', '0.7218', '0.7017', '0.6986']
Clustering into  38  clusters
Clusters:  [ 1  5 24  1  1  1 33  1 21  0 32 27  3  1  1 37  1 29  1 34  2 25  1  1
 36  1  1  1 20 23 26  1 22  1 19 31 35  1  1  1 17 30  1  1 28 15 11  1
  1 18  1  1  1 12  1  0  1 13 16 14  5 10  1  1  1  2  1  8  6  9  7  1
  1  3  3  4  1  1  1  1]
Losses:  6.341399669647217 1.6532742977142334 0.951732873916626
CurrentTrain: epoch  0, batch     0 | loss: 8.9464064Losses:  6.813398361206055 1.8652015924453735 0.9626763463020325
CurrentTrain: epoch  0, batch     1 | loss: 9.6412764Losses:  6.818240165710449 1.5485255718231201 0.9614187479019165
CurrentTrain: epoch  0, batch     2 | loss: 9.3281851Losses:  7.519245147705078 0.4135139584541321 0.9562008380889893
CurrentTrain: epoch  0, batch     3 | loss: 8.8889599Losses:  6.723590850830078 1.7279378175735474 0.9609372019767761
CurrentTrain: epoch  1, batch     0 | loss: 9.4124660Losses:  4.782131195068359 1.5616116523742676 0.9341995716094971
CurrentTrain: epoch  1, batch     1 | loss: 7.2779427Losses:  4.93233585357666 1.467099905014038 0.9515205025672913
CurrentTrain: epoch  1, batch     2 | loss: 7.3509564Losses:  6.104436874389648 0.3536040186882019 0.9687772393226624
CurrentTrain: epoch  1, batch     3 | loss: 7.4268179Losses:  5.415595054626465 1.3199832439422607 0.9385844469070435
CurrentTrain: epoch  2, batch     0 | loss: 7.6741629Losses:  4.739528656005859 1.5927791595458984 0.9440921545028687
CurrentTrain: epoch  2, batch     1 | loss: 7.2764001Losses:  4.515755653381348 1.512938380241394 0.9580478668212891
CurrentTrain: epoch  2, batch     2 | loss: 6.9867420Losses:  4.160735130310059 0.35960811376571655 0.9557519555091858
CurrentTrain: epoch  2, batch     3 | loss: 5.4760952Losses:  4.06630802154541 1.2272109985351562 0.9298224449157715
CurrentTrain: epoch  3, batch     0 | loss: 6.2233415Losses:  4.587455749511719 1.5153307914733887 0.9491552114486694
CurrentTrain: epoch  3, batch     1 | loss: 7.0519419Losses:  4.5223188400268555 1.2808135747909546 0.9355306029319763
CurrentTrain: epoch  3, batch     2 | loss: 6.7386632Losses:  3.2416296005249023 0.19158928096294403 0.9724874496459961
CurrentTrain: epoch  3, batch     3 | loss: 4.4057064Losses:  3.6332223415374756 1.2818880081176758 0.9348856210708618
CurrentTrain: epoch  4, batch     0 | loss: 5.8499961Losses:  4.679948329925537 1.5200852155685425 0.945351779460907
CurrentTrain: epoch  4, batch     1 | loss: 7.1453853Losses:  4.623895168304443 1.200706958770752 0.9213145971298218
CurrentTrain: epoch  4, batch     2 | loss: 6.7459168Losses:  3.6880064010620117 0.1280808448791504 0.9801366329193115
CurrentTrain: epoch  4, batch     3 | loss: 4.7962236Losses:  3.626354932785034 1.1602773666381836 0.9439889192581177
CurrentTrain: epoch  5, batch     0 | loss: 5.7306213Losses:  3.60404634475708 1.0578948259353638 0.8983190655708313
CurrentTrain: epoch  5, batch     1 | loss: 5.5602603Losses:  4.427868843078613 1.3901386260986328 0.9558629393577576
CurrentTrain: epoch  5, batch     2 | loss: 6.7738705Losses:  2.8648171424865723 0.2788451015949249 0.8978504538536072
CurrentTrain: epoch  5, batch     3 | loss: 4.0415125Losses:  4.1315598487854 1.1688929796218872 0.9296326637268066
CurrentTrain: epoch  6, batch     0 | loss: 6.2300854Losses:  4.146543502807617 1.2249025106430054 0.9673000574111938
CurrentTrain: epoch  6, batch     1 | loss: 6.3387461Losses:  3.107335090637207 1.1545524597167969 0.8867944478988647
CurrentTrain: epoch  6, batch     2 | loss: 5.1486821Losses:  2.986365795135498 0.18072471022605896 0.9484307169914246
CurrentTrain: epoch  6, batch     3 | loss: 4.1155210Losses:  3.7199838161468506 1.1757317781448364 0.9204633235931396
CurrentTrain: epoch  7, batch     0 | loss: 5.8161793Losses:  3.2530133724212646 1.0329365730285645 0.9315913915634155
CurrentTrain: epoch  7, batch     1 | loss: 5.2175412Losses:  3.775606155395508 1.0824065208435059 0.9292507171630859
CurrentTrain: epoch  7, batch     2 | loss: 5.7872634Losses:  1.9528963565826416 0.4379408359527588 0.8952590227127075
CurrentTrain: epoch  7, batch     3 | loss: 3.2860961Losses:  3.61118745803833 1.1192424297332764 0.9269469976425171
CurrentTrain: epoch  8, batch     0 | loss: 5.6573768Losses:  2.6444458961486816 1.1117615699768066 0.9217162132263184
CurrentTrain: epoch  8, batch     1 | loss: 4.6779237Losses:  3.295229911804199 1.288248062133789 0.9268312454223633
CurrentTrain: epoch  8, batch     2 | loss: 5.5103092Losses:  3.012423276901245 0.1281818151473999 0.8099184036254883
CurrentTrain: epoch  8, batch     3 | loss: 3.9505234Losses:  3.4296393394470215 1.2039053440093994 0.9127434492111206
CurrentTrain: epoch  9, batch     0 | loss: 5.5462885Losses:  2.6516003608703613 1.1054015159606934 0.9069803953170776
CurrentTrain: epoch  9, batch     1 | loss: 4.6639824Losses:  2.969196319580078 1.077151894569397 0.921651303768158
CurrentTrain: epoch  9, batch     2 | loss: 4.9679995Losses:  5.2833404541015625 0.23926346004009247 0.9868465662002563
CurrentTrain: epoch  9, batch     3 | loss: 6.5094504
Losses:  0.12407010048627853 0.7197597026824951 0.917366623878479
MemoryTrain:  epoch  0, batch     0 | loss: 1.7611964Losses:  0.40367305278778076 1.0476655960083008 0.9156463742256165
MemoryTrain:  epoch  0, batch     1 | loss: 2.3669851Losses:  0.5404561758041382 0.8799065351486206 0.8915208578109741
MemoryTrain:  epoch  0, batch     2 | loss: 2.3118834Losses:  0.9583451747894287 0.711439847946167 0.8824957609176636
MemoryTrain:  epoch  0, batch     3 | loss: 2.5522809Losses:  0.5278869271278381 0.7059570550918579 0.8946443796157837
MemoryTrain:  epoch  0, batch     4 | loss: 2.1284885Losses:  0.5456205606460571 0.7751908302307129 0.9270689487457275
MemoryTrain:  epoch  1, batch     0 | loss: 2.2478805Losses:  0.4259089231491089 0.6551752090454102 0.9296689629554749
MemoryTrain:  epoch  1, batch     1 | loss: 2.0107532Losses:  0.5815778374671936 0.7858631610870361 0.8598238229751587
MemoryTrain:  epoch  1, batch     2 | loss: 2.2272649Losses:  0.6055811047554016 0.868048906326294 0.9061533212661743
MemoryTrain:  epoch  1, batch     3 | loss: 2.3797832Losses:  0.7407799959182739 0.8419198393821716 0.8705295920372009
MemoryTrain:  epoch  1, batch     4 | loss: 2.4532294Losses:  0.6480169296264648 0.9038704633712769 0.8723509907722473
MemoryTrain:  epoch  2, batch     0 | loss: 2.4242384Losses:  0.24231675267219543 0.6025543212890625 0.912909984588623
MemoryTrain:  epoch  2, batch     1 | loss: 1.7577810Losses:  0.08130402863025665 0.8413577079772949 0.8934068083763123
MemoryTrain:  epoch  2, batch     2 | loss: 1.8160685Losses:  0.2635006010532379 0.6611987352371216 0.8731427192687988
MemoryTrain:  epoch  2, batch     3 | loss: 1.7978420Losses:  0.2872105538845062 0.945567786693573 0.9307540655136108
MemoryTrain:  epoch  2, batch     4 | loss: 2.1635323Losses:  0.2050664871931076 0.6078380942344666 0.8816777467727661
MemoryTrain:  epoch  3, batch     0 | loss: 1.6945823Losses:  0.06811362504959106 0.7508443593978882 0.930769145488739
MemoryTrain:  epoch  3, batch     1 | loss: 1.7497271Losses:  0.46376359462738037 0.9358627200126648 0.8850763440132141
MemoryTrain:  epoch  3, batch     2 | loss: 2.2847025Losses:  0.12507423758506775 0.7950015068054199 0.8800070285797119
MemoryTrain:  epoch  3, batch     3 | loss: 1.8000828Losses:  0.30597984790802 0.8460382223129272 0.9134039878845215
MemoryTrain:  epoch  3, batch     4 | loss: 2.0654221Losses:  0.15290936827659607 0.9985384941101074 0.8738633990287781
MemoryTrain:  epoch  4, batch     0 | loss: 2.0253112Losses:  0.047729335725307465 0.6943551301956177 0.8391802906990051
MemoryTrain:  epoch  4, batch     1 | loss: 1.5812647Losses:  0.13439209759235382 0.909501850605011 0.8790669441223145
MemoryTrain:  epoch  4, batch     2 | loss: 1.9229609Losses:  0.09511172771453857 0.692895770072937 0.9411733746528625
MemoryTrain:  epoch  4, batch     3 | loss: 1.7291808Losses:  0.04650723934173584 0.5682651400566101 0.9490790963172913
MemoryTrain:  epoch  4, batch     4 | loss: 1.5638515Losses:  0.08055153489112854 0.7590820789337158 0.8944898247718811
MemoryTrain:  epoch  5, batch     0 | loss: 1.7341235Losses:  0.14250269532203674 0.7422327995300293 0.8768947124481201
MemoryTrain:  epoch  5, batch     1 | loss: 1.7616302Losses:  0.08762694895267487 0.8133921027183533 0.8990141749382019
MemoryTrain:  epoch  5, batch     2 | loss: 1.8000332Losses:  0.134217768907547 0.718483030796051 0.8914389610290527
MemoryTrain:  epoch  5, batch     3 | loss: 1.7441398Losses:  0.07391639798879623 0.7587801814079285 0.915087103843689
MemoryTrain:  epoch  5, batch     4 | loss: 1.7477837Losses:  0.12983837723731995 0.6890150308609009 0.8586877584457397
MemoryTrain:  epoch  6, batch     0 | loss: 1.6775411Losses:  0.0861978828907013 0.737214207649231 0.9490352272987366
MemoryTrain:  epoch  6, batch     1 | loss: 1.7724473Losses:  0.04809616133570671 0.8033304214477539 0.8754458427429199
MemoryTrain:  epoch  6, batch     2 | loss: 1.7268724Losses:  0.0669722780585289 0.8745929002761841 0.8761340379714966
MemoryTrain:  epoch  6, batch     3 | loss: 1.8176992Losses:  0.09785448014736176 0.684578537940979 0.9099889993667603
MemoryTrain:  epoch  6, batch     4 | loss: 1.6924220Losses:  0.05742856115102768 0.7234067916870117 0.8931304216384888
MemoryTrain:  epoch  7, batch     0 | loss: 1.6739657Losses:  0.044411782175302505 0.7778167128562927 0.9015394449234009
MemoryTrain:  epoch  7, batch     1 | loss: 1.7237680Losses:  0.08204589039087296 0.8016906380653381 0.8560793399810791
MemoryTrain:  epoch  7, batch     2 | loss: 1.7398160Losses:  0.052172355353832245 0.5874781608581543 0.9240766763687134
MemoryTrain:  epoch  7, batch     3 | loss: 1.5637271Losses:  0.17819437384605408 0.7433476448059082 0.8957170844078064
MemoryTrain:  epoch  7, batch     4 | loss: 1.8172591Losses:  0.07359309494495392 0.6844687461853027 0.8868016004562378
MemoryTrain:  epoch  8, batch     0 | loss: 1.6448634Losses:  0.06759843230247498 0.8807152509689331 0.9227865934371948
MemoryTrain:  epoch  8, batch     1 | loss: 1.8711003Losses:  0.04172435402870178 0.640853762626648 0.880695104598999
MemoryTrain:  epoch  8, batch     2 | loss: 1.5632732Losses:  0.12132221460342407 0.7525383830070496 0.8694339990615845
MemoryTrain:  epoch  8, batch     3 | loss: 1.7432946Losses:  0.04084157198667526 0.6309340596199036 0.8980394601821899
MemoryTrain:  epoch  8, batch     4 | loss: 1.5698152Losses:  0.04139811918139458 0.5939266085624695 0.9360889196395874
MemoryTrain:  epoch  9, batch     0 | loss: 1.5714136Losses:  0.06348805129528046 0.8338727951049805 0.868624210357666
MemoryTrain:  epoch  9, batch     1 | loss: 1.7659850Losses:  0.06213966757059097 0.622455894947052 0.8888571262359619
MemoryTrain:  epoch  9, batch     2 | loss: 1.5734527Losses:  0.05997547507286072 0.7781956195831299 0.8884490728378296
MemoryTrain:  epoch  9, batch     3 | loss: 1.7266202Losses:  0.04072579741477966 0.48439255356788635 0.8647006750106812
MemoryTrain:  epoch  9, batch     4 | loss: 1.3898190
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 55.53%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 53.47%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 51.56%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 49.78%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 48.12%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 46.57%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 47.07%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 48.30%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 49.08%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 50.54%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 52.53%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 53.62%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 54.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.62%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 56.55%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 59.09%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 58.19%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 57.47%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 56.78%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 55.99%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 55.10%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 54.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 53.80%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 54.45%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 54.83%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 55.44%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 55.68%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 56.14%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 56.14%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 55.71%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 55.72%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 55.83%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 55.74%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 56.05%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 55.46%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.84%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.45%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.27%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 84.87%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 84.05%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 83.79%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 83.40%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 82.13%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 81.15%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 80.87%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 80.13%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 79.23%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 78.99%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 78.84%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 78.87%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 78.97%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 78.83%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 78.04%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 77.35%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 76.52%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 76.11%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 75.70%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 75.08%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 75.08%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 76.28%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 75.70%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 75.07%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 74.73%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 74.25%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 73.66%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 73.34%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.63%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 73.78%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 74.06%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 73.51%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 73.04%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 72.45%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 71.94%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 71.37%   [EVAL] batch:  105 | acc: 18.75%,  total acc: 70.87%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 70.68%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 70.60%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 70.41%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 70.23%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 70.05%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 69.87%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 69.63%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 68.43%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 68.16%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 67.85%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 68.55%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 68.21%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 67.87%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 67.49%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 67.12%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 67.42%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 67.18%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 66.79%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 66.45%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 66.07%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 65.78%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 65.58%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 67.54%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 67.67%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 68.25%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 67.77%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 67.51%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 67.26%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 67.93%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  177 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  179 | acc: 18.75%,  total acc: 66.94%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 66.68%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 66.34%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 65.89%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 65.84%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 65.85%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 65.74%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 65.69%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.72%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 65.76%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 66.17%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 66.00%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 65.78%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 65.55%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:  210 | acc: 0.00%,  total acc: 64.99%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 64.71%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 66.89%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 66.68%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 66.56%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.47%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 66.35%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:  244 | acc: 6.25%,  total acc: 66.66%   [EVAL] batch:  245 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 66.17%   [EVAL] batch:  247 | acc: 6.25%,  total acc: 65.93%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 65.69%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 65.50%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.19%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 66.30%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 66.26%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 66.21%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 66.12%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 65.94%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 65.79%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 65.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 66.29%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 67.77%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.58%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.62%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 68.62%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 68.54%   [EVAL] batch:  333 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 68.41%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 68.32%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 68.29%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 68.22%   [EVAL] batch:  340 | acc: 56.25%,  total acc: 68.18%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 68.09%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 68.01%   [EVAL] batch:  344 | acc: 50.00%,  total acc: 67.95%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 67.92%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 67.85%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 67.85%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 67.80%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 67.83%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 67.83%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 67.83%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  356 | acc: 31.25%,  total acc: 67.80%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 67.67%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 67.55%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 67.38%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 67.18%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 67.97%   [EVAL] batch:  375 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 68.00%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.03%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 68.05%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 68.05%   [EVAL] batch:  384 | acc: 43.75%,  total acc: 67.99%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 67.92%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 67.94%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 67.88%   [EVAL] batch:  390 | acc: 31.25%,  total acc: 67.79%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 67.67%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 67.97%   [EVAL] batch:  402 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 67.78%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 67.75%   [EVAL] batch:  407 | acc: 37.50%,  total acc: 67.68%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 67.53%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 67.39%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:  411 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 67.07%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 67.09%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 67.23%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 67.13%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 67.09%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 67.02%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  423 | acc: 37.50%,  total acc: 66.92%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 66.87%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 67.64%   [EVAL] batch:  438 | acc: 12.50%,  total acc: 67.51%   [EVAL] batch:  439 | acc: 37.50%,  total acc: 67.44%   [EVAL] batch:  440 | acc: 0.00%,  total acc: 67.29%   [EVAL] batch:  441 | acc: 12.50%,  total acc: 67.17%   [EVAL] batch:  442 | acc: 0.00%,  total acc: 67.01%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 66.97%   [EVAL] batch:  445 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  446 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  449 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 67.34%   [EVAL] batch:  452 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  453 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  455 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  456 | acc: 31.25%,  total acc: 67.51%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 67.44%   [EVAL] batch:  458 | acc: 31.25%,  total acc: 67.36%   [EVAL] batch:  459 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:  460 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 67.15%   [EVAL] batch:  462 | acc: 37.50%,  total acc: 67.09%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 66.95%   [EVAL] batch:  464 | acc: 0.00%,  total acc: 66.80%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 66.66%   [EVAL] batch:  466 | acc: 0.00%,  total acc: 66.51%   [EVAL] batch:  467 | acc: 0.00%,  total acc: 66.37%   [EVAL] batch:  468 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  470 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  471 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  473 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  474 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  481 | acc: 43.75%,  total acc: 66.86%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.77%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  484 | acc: 12.50%,  total acc: 66.60%   [EVAL] batch:  485 | acc: 18.75%,  total acc: 66.50%   [EVAL] batch:  486 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:  487 | acc: 31.25%,  total acc: 66.29%   [EVAL] batch:  488 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 66.33%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 66.40%   [EVAL] batch:  494 | acc: 31.25%,  total acc: 66.33%   [EVAL] batch:  495 | acc: 50.00%,  total acc: 66.29%   [EVAL] batch:  496 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  497 | acc: 50.00%,  total acc: 66.24%   [EVAL] batch:  498 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 66.22%   
cur_acc:  ['0.9435', '0.7440', '0.7748', '0.6825', '0.8333', '0.6935', '0.6835', '0.5546']
his_acc:  ['0.9435', '0.8470', '0.7922', '0.7278', '0.7218', '0.7017', '0.6986', '0.6623']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.27536678314209 1.7023675441741943 0.9999472498893738
CurrentTrain: epoch  0, batch     0 | loss: 11.9776821Losses:  9.688996315002441 1.845266342163086 0.9921714663505554
CurrentTrain: epoch  0, batch     1 | loss: 12.5264339Losses:  9.490995407104492 2.084415912628174 0.987006664276123
CurrentTrain: epoch  0, batch     2 | loss: 12.5624180Losses:  9.904996871948242 1.6717488765716553 0.9960395097732544
CurrentTrain: epoch  0, batch     3 | loss: 12.5727854Losses:  9.482876777648926 1.5403037071228027 1.0031726360321045
CurrentTrain: epoch  0, batch     4 | loss: 12.0263529Losses:  10.691526412963867 2.034575939178467 0.9960319995880127
CurrentTrain: epoch  0, batch     5 | loss: 13.7221346Losses:  8.629640579223633 1.4554367065429688 0.9667547941207886
CurrentTrain: epoch  0, batch     6 | loss: 11.0518322Losses:  8.764314651489258 1.6624115705490112 0.9799496531486511
CurrentTrain: epoch  0, batch     7 | loss: 11.4066763Losses:  8.327323913574219 1.5780317783355713 0.9806838035583496
CurrentTrain: epoch  0, batch     8 | loss: 10.8860397Losses:  9.009676933288574 1.7961856126785278 0.9843350052833557
CurrentTrain: epoch  0, batch     9 | loss: 11.7901974Losses:  9.310457229614258 1.6090792417526245 0.9929987192153931
CurrentTrain: epoch  0, batch    10 | loss: 11.9125357Losses:  9.091215133666992 1.4367496967315674 0.9831432104110718
CurrentTrain: epoch  0, batch    11 | loss: 11.5111074Losses:  8.977813720703125 1.6228077411651611 0.9770931005477905
CurrentTrain: epoch  0, batch    12 | loss: 11.5777140Losses:  7.966645240783691 1.4566078186035156 0.9670893549919128
CurrentTrain: epoch  0, batch    13 | loss: 10.3903427Losses:  8.846673965454102 1.4967868328094482 0.964081883430481
CurrentTrain: epoch  0, batch    14 | loss: 11.3075428Losses:  8.525063514709473 1.2758760452270508 0.9795848727226257
CurrentTrain: epoch  0, batch    15 | loss: 10.7805243Losses:  8.678630828857422 1.4878640174865723 0.9660788774490356
CurrentTrain: epoch  0, batch    16 | loss: 11.1325731Losses:  9.300710678100586 1.7275830507278442 0.9876255989074707
CurrentTrain: epoch  0, batch    17 | loss: 12.0159187Losses:  8.323243141174316 1.665994644165039 0.9514453411102295
CurrentTrain: epoch  0, batch    18 | loss: 10.9406834Losses:  9.390193939208984 1.4716851711273193 0.9834335446357727
CurrentTrain: epoch  0, batch    19 | loss: 11.8453131Losses:  7.451832294464111 1.2095704078674316 0.9408471584320068
CurrentTrain: epoch  0, batch    20 | loss: 9.6022501Losses:  8.683233261108398 1.2230544090270996 0.9700936079025269
CurrentTrain: epoch  0, batch    21 | loss: 10.8763819Losses:  8.893758773803711 1.4198039770126343 0.9895718693733215
CurrentTrain: epoch  0, batch    22 | loss: 11.3031340Losses:  8.701031684875488 1.306037425994873 0.9601573944091797
CurrentTrain: epoch  0, batch    23 | loss: 10.9672260Losses:  8.588982582092285 1.332890510559082 0.9678155183792114
CurrentTrain: epoch  0, batch    24 | loss: 10.8896885Losses:  9.068855285644531 1.2883813381195068 0.9656589031219482
CurrentTrain: epoch  0, batch    25 | loss: 11.3228960Losses:  7.977911472320557 1.325118064880371 0.9648967981338501
CurrentTrain: epoch  0, batch    26 | loss: 10.2679272Losses:  8.2117919921875 1.515205979347229 0.9671441316604614
CurrentTrain: epoch  0, batch    27 | loss: 10.6941423Losses:  8.74958610534668 1.2576862573623657 0.9559255242347717
CurrentTrain: epoch  0, batch    28 | loss: 10.9631987Losses:  8.468210220336914 1.472533941268921 0.965961217880249
CurrentTrain: epoch  0, batch    29 | loss: 10.9067059Losses:  8.417919158935547 1.2824257612228394 0.9728134870529175
CurrentTrain: epoch  0, batch    30 | loss: 10.6731586Losses:  8.233938217163086 1.5420653820037842 0.9652272462844849
CurrentTrain: epoch  0, batch    31 | loss: 10.7412310Losses:  8.206823348999023 1.2311744689941406 0.9567922353744507
CurrentTrain: epoch  0, batch    32 | loss: 10.3947897Losses:  8.315467834472656 1.3859622478485107 0.945178210735321
CurrentTrain: epoch  0, batch    33 | loss: 10.6466084Losses:  7.681148052215576 1.237963080406189 0.9327105283737183
CurrentTrain: epoch  0, batch    34 | loss: 9.8518219Losses:  8.451801300048828 1.3334593772888184 0.9638134241104126
CurrentTrain: epoch  0, batch    35 | loss: 10.7490749Losses:  8.223855972290039 1.133455753326416 0.935856819152832
CurrentTrain: epoch  0, batch    36 | loss: 10.2931681Losses:  7.4076995849609375 1.3220603466033936 0.9315339922904968
CurrentTrain: epoch  0, batch    37 | loss: 9.6612940Losses:  8.368906021118164 1.299401044845581 0.9504735469818115
CurrentTrain: epoch  0, batch    38 | loss: 10.6187811Losses:  8.23063850402832 1.3644680976867676 0.952158510684967
CurrentTrain: epoch  0, batch    39 | loss: 10.5472651Losses:  7.9629669189453125 0.9855741262435913 0.9488122463226318
CurrentTrain: epoch  0, batch    40 | loss: 9.8973532Losses:  7.647119998931885 1.2377101182937622 0.9404582977294922
CurrentTrain: epoch  0, batch    41 | loss: 9.8252888Losses:  7.726475715637207 1.2891716957092285 0.9520983695983887
CurrentTrain: epoch  0, batch    42 | loss: 9.9677467Losses:  8.058503150939941 1.3521479368209839 0.9588299989700317
CurrentTrain: epoch  0, batch    43 | loss: 10.3694811Losses:  8.161531448364258 1.183973789215088 0.9534978866577148
CurrentTrain: epoch  0, batch    44 | loss: 10.2990026Losses:  7.436100482940674 1.1525678634643555 0.9576573371887207
CurrentTrain: epoch  0, batch    45 | loss: 9.5463257Losses:  7.286443710327148 1.193579912185669 0.9311596751213074
CurrentTrain: epoch  0, batch    46 | loss: 9.4111834Losses:  8.533649444580078 1.1178596019744873 0.9641090631484985
CurrentTrain: epoch  0, batch    47 | loss: 10.6156187Losses:  7.971362113952637 1.1196579933166504 0.9649808406829834
CurrentTrain: epoch  0, batch    48 | loss: 10.0560017Losses:  8.402891159057617 0.9817250967025757 0.9261587858200073
CurrentTrain: epoch  0, batch    49 | loss: 10.3107748Losses:  7.730106830596924 1.0991849899291992 0.9497658014297485
CurrentTrain: epoch  0, batch    50 | loss: 9.7790585Losses:  7.115320205688477 1.1579985618591309 0.9197704792022705
CurrentTrain: epoch  0, batch    51 | loss: 9.1930895Losses:  6.929692268371582 1.0929863452911377 0.9334118366241455
CurrentTrain: epoch  0, batch    52 | loss: 8.9560900Losses:  6.752424240112305 1.287785530090332 0.9390983581542969
CurrentTrain: epoch  0, batch    53 | loss: 8.9793081Losses:  7.936419486999512 1.1730492115020752 0.9664449691772461
CurrentTrain: epoch  0, batch    54 | loss: 10.0759134Losses:  7.413132190704346 0.9908392429351807 0.9476626515388489
CurrentTrain: epoch  0, batch    55 | loss: 9.3516340Losses:  6.5998992919921875 0.8874512910842896 0.9320858120918274
CurrentTrain: epoch  0, batch    56 | loss: 8.4194365Losses:  7.20554256439209 0.8455831408500671 0.9382761120796204
CurrentTrain: epoch  0, batch    57 | loss: 8.9894018Losses:  7.504535675048828 1.227185845375061 0.939535915851593
CurrentTrain: epoch  0, batch    58 | loss: 9.6712580Losses:  7.222436904907227 1.127866268157959 0.9249577522277832
CurrentTrain: epoch  0, batch    59 | loss: 9.2752609Losses:  7.959946632385254 1.1545227766036987 0.9515234231948853
CurrentTrain: epoch  0, batch    60 | loss: 10.0659933Losses:  6.372143745422363 0.8177120685577393 0.9427105188369751
CurrentTrain: epoch  0, batch    61 | loss: 8.1325665Losses:  7.108878135681152 0.4899342656135559 0.9060193300247192
CurrentTrain: epoch  0, batch    62 | loss: 8.5048323Losses:  7.5386433601379395 1.1380963325500488 0.9442825317382812
CurrentTrain: epoch  1, batch     0 | loss: 9.6210222Losses:  6.720400810241699 1.1075043678283691 0.9194231033325195
CurrentTrain: epoch  1, batch     1 | loss: 8.7473278Losses:  7.539363861083984 1.3036997318267822 0.9495991468429565
CurrentTrain: epoch  1, batch     2 | loss: 9.7926626Losses:  7.20859956741333 0.9438072443008423 0.9373281598091125
CurrentTrain: epoch  1, batch     3 | loss: 9.0897350Losses:  6.640956878662109 0.7237846851348877 0.9039928317070007
CurrentTrain: epoch  1, batch     4 | loss: 8.2687340Losses:  6.645918846130371 0.8234144449234009 0.9349573850631714
CurrentTrain: epoch  1, batch     5 | loss: 8.4042902Losses:  6.429175853729248 0.8852477669715881 0.9071725010871887
CurrentTrain: epoch  1, batch     6 | loss: 8.2215958Losses:  7.393243312835693 0.8258175253868103 0.928284227848053
CurrentTrain: epoch  1, batch     7 | loss: 9.1473455Losses:  6.3503618240356445 0.8896883130073547 0.9286653995513916
CurrentTrain: epoch  1, batch     8 | loss: 8.1687155Losses:  6.975632190704346 0.8391931056976318 0.9213907122612
CurrentTrain: epoch  1, batch     9 | loss: 8.7362156Losses:  7.127853870391846 0.9348368644714355 0.9375790357589722
CurrentTrain: epoch  1, batch    10 | loss: 9.0002699Losses:  6.423474311828613 0.773013174533844 0.934629499912262
CurrentTrain: epoch  1, batch    11 | loss: 8.1311169Losses:  7.120145797729492 0.9994953870773315 0.951762318611145
CurrentTrain: epoch  1, batch    12 | loss: 9.0714035Losses:  7.013843536376953 0.9332659244537354 0.9054760932922363
CurrentTrain: epoch  1, batch    13 | loss: 8.8525848Losses:  7.125709533691406 0.8493729829788208 0.9302610158920288
CurrentTrain: epoch  1, batch    14 | loss: 8.9053431Losses:  6.837385177612305 0.8164490461349487 0.9289249181747437
CurrentTrain: epoch  1, batch    15 | loss: 8.5827589Losses:  7.504787445068359 0.9034422039985657 0.9231244921684265
CurrentTrain: epoch  1, batch    16 | loss: 9.3313541Losses:  7.574280738830566 1.0622735023498535 0.934958815574646
CurrentTrain: epoch  1, batch    17 | loss: 9.5715132Losses:  7.876691818237305 1.1022701263427734 0.9557839035987854
CurrentTrain: epoch  1, batch    18 | loss: 9.9347458Losses:  7.342394828796387 0.8382377624511719 0.931580662727356
CurrentTrain: epoch  1, batch    19 | loss: 9.1122131Losses:  7.295586109161377 0.6452561020851135 0.9195162057876587
CurrentTrain: epoch  1, batch    20 | loss: 8.8603582Losses:  6.762145042419434 0.8681400418281555 0.8949439525604248
CurrentTrain: epoch  1, batch    21 | loss: 8.5252295Losses:  6.135924816131592 0.7211779356002808 0.9004344940185547
CurrentTrain: epoch  1, batch    22 | loss: 7.7575374Losses:  6.830437183380127 0.9014357924461365 0.9244135022163391
CurrentTrain: epoch  1, batch    23 | loss: 8.6562862Losses:  7.087884426116943 0.841148853302002 0.9069939851760864
CurrentTrain: epoch  1, batch    24 | loss: 8.8360271Losses:  6.546385288238525 0.6745100617408752 0.8992555141448975
CurrentTrain: epoch  1, batch    25 | loss: 8.1201506Losses:  6.887347221374512 0.7948611974716187 0.9089540839195251
CurrentTrain: epoch  1, batch    26 | loss: 8.5911627Losses:  6.760904312133789 0.817950963973999 0.8756173849105835
CurrentTrain: epoch  1, batch    27 | loss: 8.4544725Losses:  6.016658782958984 0.6218966245651245 0.9011808037757874
CurrentTrain: epoch  1, batch    28 | loss: 7.5397363Losses:  6.702214241027832 0.8022246956825256 0.904517412185669
CurrentTrain: epoch  1, batch    29 | loss: 8.4089565Losses:  6.9970903396606445 0.6312230825424194 0.8893285989761353
CurrentTrain: epoch  1, batch    30 | loss: 8.5176420Losses:  6.119782447814941 0.7458847761154175 0.9329642653465271
CurrentTrain: epoch  1, batch    31 | loss: 7.7986317Losses:  6.051089286804199 0.683768630027771 0.8965880274772644
CurrentTrain: epoch  1, batch    32 | loss: 7.6314459Losses:  6.113068580627441 0.7060803771018982 0.8904840350151062
CurrentTrain: epoch  1, batch    33 | loss: 7.7096329Losses:  5.487720489501953 0.6211467981338501 0.8758617639541626
CurrentTrain: epoch  1, batch    34 | loss: 6.9847288Losses:  6.325347423553467 0.6789469122886658 0.9042633175849915
CurrentTrain: epoch  1, batch    35 | loss: 7.9085579Losses:  6.437229156494141 0.849859356880188 0.9212302565574646
CurrentTrain: epoch  1, batch    36 | loss: 8.2083187Losses:  6.916050910949707 0.7100691795349121 0.9173442125320435
CurrentTrain: epoch  1, batch    37 | loss: 8.5434647Losses:  6.055432319641113 0.6486643552780151 0.919352650642395
CurrentTrain: epoch  1, batch    38 | loss: 7.6234493Losses:  6.734209060668945 0.6065374612808228 0.9036790132522583
CurrentTrain: epoch  1, batch    39 | loss: 8.2444258Losses:  6.332691192626953 0.8168208599090576 0.9045171141624451
CurrentTrain: epoch  1, batch    40 | loss: 8.0540295Losses:  6.926387786865234 0.7020149230957031 0.911483883857727
CurrentTrain: epoch  1, batch    41 | loss: 8.5398865Losses:  6.67434549331665 0.8135722875595093 0.869853138923645
CurrentTrain: epoch  1, batch    42 | loss: 8.3577709Losses:  6.025060653686523 0.5486370921134949 0.8868170976638794
CurrentTrain: epoch  1, batch    43 | loss: 7.4605145Losses:  6.218639850616455 0.6852564811706543 0.9018899202346802
CurrentTrain: epoch  1, batch    44 | loss: 7.8057861Losses:  5.966787338256836 0.5773286819458008 0.9200979471206665
CurrentTrain: epoch  1, batch    45 | loss: 7.4642138Losses:  6.373999118804932 0.8366157412528992 0.9005876779556274
CurrentTrain: epoch  1, batch    46 | loss: 8.1112022Losses:  5.529923439025879 0.46362102031707764 0.8634529113769531
CurrentTrain: epoch  1, batch    47 | loss: 6.8569975Losses:  5.6851701736450195 0.6944957971572876 0.8941842317581177
CurrentTrain: epoch  1, batch    48 | loss: 7.2738500Losses:  5.838323593139648 0.7592493295669556 0.8920285105705261
CurrentTrain: epoch  1, batch    49 | loss: 7.4896011Losses:  5.382798671722412 0.6099938154220581 0.8888124823570251
CurrentTrain: epoch  1, batch    50 | loss: 6.8816051Losses:  6.424202919006348 0.5822476148605347 0.9099031090736389
CurrentTrain: epoch  1, batch    51 | loss: 7.9163537Losses:  5.7711591720581055 0.6112126111984253 0.8501362800598145
CurrentTrain: epoch  1, batch    52 | loss: 7.2325082Losses:  6.021148204803467 0.6644988656044006 0.8880032300949097
CurrentTrain: epoch  1, batch    53 | loss: 7.5736504Losses:  5.801858901977539 0.5463756322860718 0.9021694660186768
CurrentTrain: epoch  1, batch    54 | loss: 7.2504044Losses:  5.226765155792236 0.5568075180053711 0.8744974136352539
CurrentTrain: epoch  1, batch    55 | loss: 6.6580701Losses:  6.314029693603516 0.6519251465797424 0.8886436223983765
CurrentTrain: epoch  1, batch    56 | loss: 7.8545985Losses:  6.103586673736572 0.7029625177383423 0.9037446975708008
CurrentTrain: epoch  1, batch    57 | loss: 7.7102938Losses:  6.4057722091674805 0.5273014307022095 0.9420310258865356
CurrentTrain: epoch  1, batch    58 | loss: 7.8751044Losses:  5.123210430145264 0.49141547083854675 0.8741357922554016
CurrentTrain: epoch  1, batch    59 | loss: 6.4887619Losses:  5.507911205291748 0.5387296676635742 0.9118531942367554
CurrentTrain: epoch  1, batch    60 | loss: 6.9584942Losses:  5.846025466918945 0.6322131752967834 0.8988889455795288
CurrentTrain: epoch  1, batch    61 | loss: 7.3771276Losses:  5.121058940887451 0.5074798464775085 0.8805732727050781
CurrentTrain: epoch  1, batch    62 | loss: 6.5091119Losses:  5.643189430236816 0.6010464429855347 0.8494421243667603
CurrentTrain: epoch  2, batch     0 | loss: 7.0936780Losses:  5.2484588623046875 0.44367656111717224 0.832189679145813
CurrentTrain: epoch  2, batch     1 | loss: 6.5243249Losses:  5.515925407409668 0.6256906390190125 0.8911212086677551
CurrentTrain: epoch  2, batch     2 | loss: 7.0327373Losses:  5.077056884765625 0.5260781049728394 0.852563202381134
CurrentTrain: epoch  2, batch     3 | loss: 6.4556985Losses:  5.676720142364502 0.5428551435470581 0.9005410671234131
CurrentTrain: epoch  2, batch     4 | loss: 7.1201162Losses:  5.25847864151001 0.5570127964019775 0.8717455863952637
CurrentTrain: epoch  2, batch     5 | loss: 6.6872373Losses:  5.461155414581299 0.5887202024459839 0.915192723274231
CurrentTrain: epoch  2, batch     6 | loss: 6.9650683Losses:  5.09414529800415 0.5697013139724731 0.8808382749557495
CurrentTrain: epoch  2, batch     7 | loss: 6.5446849Losses:  5.430149078369141 0.6010415554046631 0.856459379196167
CurrentTrain: epoch  2, batch     8 | loss: 6.8876505Losses:  5.687894344329834 0.5131841897964478 0.907976508140564
CurrentTrain: epoch  2, batch     9 | loss: 7.1090550Losses:  5.319546699523926 0.46139031648635864 0.8709908723831177
CurrentTrain: epoch  2, batch    10 | loss: 6.6519279Losses:  5.368626117706299 0.6103461384773254 0.8815178871154785
CurrentTrain: epoch  2, batch    11 | loss: 6.8604903Losses:  5.998420715332031 0.5121185779571533 0.8794984221458435
CurrentTrain: epoch  2, batch    12 | loss: 7.3900375Losses:  5.5221381187438965 0.577009916305542 0.8880130052566528
CurrentTrain: epoch  2, batch    13 | loss: 6.9871607Losses:  5.479457855224609 0.32291075587272644 0.8793449997901917
CurrentTrain: epoch  2, batch    14 | loss: 6.6817136Losses:  5.5385541915893555 0.4814431667327881 0.8838600516319275
CurrentTrain: epoch  2, batch    15 | loss: 6.9038577Losses:  5.160517692565918 0.4401450753211975 0.8863046169281006
CurrentTrain: epoch  2, batch    16 | loss: 6.4869671Losses:  5.843326568603516 0.5032256245613098 0.8808866739273071
CurrentTrain: epoch  2, batch    17 | loss: 7.2274389Losses:  5.465594291687012 0.45582324266433716 0.913435697555542
CurrentTrain: epoch  2, batch    18 | loss: 6.8348532Losses:  5.768841743469238 0.44204431772232056 0.9000431299209595
CurrentTrain: epoch  2, batch    19 | loss: 7.1109290Losses:  5.020150661468506 0.3388463258743286 0.8689045310020447
CurrentTrain: epoch  2, batch    20 | loss: 6.2279015Losses:  5.155948638916016 0.5202193260192871 0.8525837063789368
CurrentTrain: epoch  2, batch    21 | loss: 6.5287519Losses:  5.503284931182861 0.45326414704322815 0.8551572561264038
CurrentTrain: epoch  2, batch    22 | loss: 6.8117065Losses:  5.62608528137207 0.40165820717811584 0.8517658710479736
CurrentTrain: epoch  2, batch    23 | loss: 6.8795090Losses:  5.623620986938477 0.37912899255752563 0.8055282831192017
CurrentTrain: epoch  2, batch    24 | loss: 6.8082781Losses:  4.960021018981934 0.41528308391571045 0.8404574394226074
CurrentTrain: epoch  2, batch    25 | loss: 6.2157617Losses:  5.022593975067139 0.40253132581710815 0.884912371635437
CurrentTrain: epoch  2, batch    26 | loss: 6.3100376Losses:  6.048887252807617 0.5076230764389038 0.8950091004371643
CurrentTrain: epoch  2, batch    27 | loss: 7.4515195Losses:  4.882603645324707 0.4723299443721771 0.8276468515396118
CurrentTrain: epoch  2, batch    28 | loss: 6.1825805Losses:  5.058212757110596 0.463021844625473 0.8536131978034973
CurrentTrain: epoch  2, batch    29 | loss: 6.3748479Losses:  6.273042678833008 0.43461114168167114 0.880712628364563
CurrentTrain: epoch  2, batch    30 | loss: 7.5883665Losses:  5.676811695098877 0.5299651622772217 0.8649299740791321
CurrentTrain: epoch  2, batch    31 | loss: 7.0717068Losses:  5.505906105041504 0.3855511546134949 0.8476597666740417
CurrentTrain: epoch  2, batch    32 | loss: 6.7391167Losses:  4.92420768737793 0.310200035572052 0.8337825536727905
CurrentTrain: epoch  2, batch    33 | loss: 6.0681906Losses:  5.635808944702148 0.4183778166770935 0.8944017887115479
CurrentTrain: epoch  2, batch    34 | loss: 6.9485884Losses:  5.849222183227539 0.4464016556739807 0.8746931552886963
CurrentTrain: epoch  2, batch    35 | loss: 7.1703167Losses:  5.11453914642334 0.4724842607975006 0.8492439985275269
CurrentTrain: epoch  2, batch    36 | loss: 6.4362674Losses:  5.551765441894531 0.5898427367210388 0.8813645839691162
CurrentTrain: epoch  2, batch    37 | loss: 7.0229731Losses:  5.265852928161621 0.4649069607257843 0.8477076888084412
CurrentTrain: epoch  2, batch    38 | loss: 6.5784678Losses:  5.458367824554443 0.35843563079833984 0.8523611426353455
CurrentTrain: epoch  2, batch    39 | loss: 6.6691647Losses:  5.1153717041015625 0.3591887950897217 0.8518548011779785
CurrentTrain: epoch  2, batch    40 | loss: 6.3264155Losses:  6.266654968261719 0.5748260021209717 0.9041442275047302
CurrentTrain: epoch  2, batch    41 | loss: 7.7456255Losses:  4.945667743682861 0.29580390453338623 0.864859402179718
CurrentTrain: epoch  2, batch    42 | loss: 6.1063313Losses:  4.901562690734863 0.3722677230834961 0.8712447881698608
CurrentTrain: epoch  2, batch    43 | loss: 6.1450753Losses:  4.686393737792969 0.28760361671447754 0.8008550405502319
CurrentTrain: epoch  2, batch    44 | loss: 5.7748523Losses:  5.197745323181152 0.3140321969985962 0.9125805497169495
CurrentTrain: epoch  2, batch    45 | loss: 6.4243579Losses:  5.103912353515625 0.425545334815979 0.8826704621315002
CurrentTrain: epoch  2, batch    46 | loss: 6.4121280Losses:  6.535520076751709 0.6810725927352905 0.8488501310348511
CurrentTrain: epoch  2, batch    47 | loss: 8.0654430Losses:  5.145137786865234 0.43623122572898865 0.864450216293335
CurrentTrain: epoch  2, batch    48 | loss: 6.4458189Losses:  5.810878753662109 0.4052024483680725 0.901455283164978
CurrentTrain: epoch  2, batch    49 | loss: 7.1175365Losses:  6.028195858001709 0.3055201768875122 0.8984198570251465
CurrentTrain: epoch  2, batch    50 | loss: 7.2321358Losses:  5.5185866355896 0.4387384057044983 0.8624407052993774
CurrentTrain: epoch  2, batch    51 | loss: 6.8197656Losses:  5.503252029418945 0.3791017532348633 0.8293865919113159
CurrentTrain: epoch  2, batch    52 | loss: 6.7117405Losses:  5.7113237380981445 0.3014596700668335 0.889963686466217
CurrentTrain: epoch  2, batch    53 | loss: 6.9027472Losses:  4.909100532531738 0.22727878391742706 0.9017607569694519
CurrentTrain: epoch  2, batch    54 | loss: 6.0381398Losses:  5.5548014640808105 0.445715069770813 0.9044582843780518
CurrentTrain: epoch  2, batch    55 | loss: 6.9049749Losses:  4.974688529968262 0.35302048921585083 0.8306215405464172
CurrentTrain: epoch  2, batch    56 | loss: 6.1583309Losses:  5.013873100280762 0.4592786729335785 0.8351577520370483
CurrentTrain: epoch  2, batch    57 | loss: 6.3083096Losses:  6.117220401763916 0.5406649112701416 0.8480411767959595
CurrentTrain: epoch  2, batch    58 | loss: 7.5059266Losses:  5.639479637145996 0.33918875455856323 0.874509334564209
CurrentTrain: epoch  2, batch    59 | loss: 6.8531775Losses:  4.637694835662842 0.4102784991264343 0.8298212289810181
CurrentTrain: epoch  2, batch    60 | loss: 5.8777943Losses:  4.895084857940674 0.31070029735565186 0.8437124490737915
CurrentTrain: epoch  2, batch    61 | loss: 6.0494976Losses:  5.011414051055908 0.22987470030784607 0.9002195000648499
CurrentTrain: epoch  2, batch    62 | loss: 6.1415081Losses:  4.5834245681762695 0.3547142744064331 0.8635868430137634
CurrentTrain: epoch  3, batch     0 | loss: 5.8017259Losses:  5.633935451507568 0.44115957617759705 0.8686259984970093
CurrentTrain: epoch  3, batch     1 | loss: 6.9437213Losses:  5.6444621086120605 0.3970245122909546 0.8766767978668213
CurrentTrain: epoch  3, batch     2 | loss: 6.9181633Losses:  5.29244327545166 0.39505428075790405 0.8999903202056885
CurrentTrain: epoch  3, batch     3 | loss: 6.5874882Losses:  4.599979400634766 0.35153114795684814 0.8350176215171814
CurrentTrain: epoch  3, batch     4 | loss: 5.7865281Losses:  5.0316596031188965 0.4160078167915344 0.8797515630722046
CurrentTrain: epoch  3, batch     5 | loss: 6.3274193Losses:  4.745062351226807 0.3091319799423218 0.8397035598754883
CurrentTrain: epoch  3, batch     6 | loss: 5.8938980Losses:  4.952631950378418 0.3525615334510803 0.8791398406028748
CurrentTrain: epoch  3, batch     7 | loss: 6.1843333Losses:  4.740355968475342 0.33375269174575806 0.8160949945449829
CurrentTrain: epoch  3, batch     8 | loss: 5.8902035Losses:  5.11911678314209 0.3478468954563141 0.856604814529419
CurrentTrain: epoch  3, batch     9 | loss: 6.3235683Losses:  5.353071689605713 0.3493501543998718 0.8771733045578003
CurrentTrain: epoch  3, batch    10 | loss: 6.5795951Losses:  4.966686248779297 0.4040176272392273 0.8269668817520142
CurrentTrain: epoch  3, batch    11 | loss: 6.1976705Losses:  5.274643898010254 0.19066858291625977 0.8651818037033081
CurrentTrain: epoch  3, batch    12 | loss: 6.3304944Losses:  5.302151679992676 0.290089875459671 0.8881686925888062
CurrentTrain: epoch  3, batch    13 | loss: 6.4804106Losses:  4.703852653503418 0.377300500869751 0.8529075980186462
CurrentTrain: epoch  3, batch    14 | loss: 5.9340606Losses:  5.323081970214844 0.47742128372192383 0.8850631713867188
CurrentTrain: epoch  3, batch    15 | loss: 6.6855664Losses:  4.865285873413086 0.3510628938674927 0.8548476099967957
CurrentTrain: epoch  3, batch    16 | loss: 6.0711961Losses:  5.048398494720459 0.38466116786003113 0.8678964376449585
CurrentTrain: epoch  3, batch    17 | loss: 6.3009562Losses:  4.74261474609375 0.3816089332103729 0.8479568958282471
CurrentTrain: epoch  3, batch    18 | loss: 5.9721804Losses:  4.606581211090088 0.27816474437713623 0.8846611976623535
CurrentTrain: epoch  3, batch    19 | loss: 5.7694073Losses:  4.99661111831665 0.402648389339447 0.8230987787246704
CurrentTrain: epoch  3, batch    20 | loss: 6.2223582Losses:  4.633860111236572 0.3119051456451416 0.8347156643867493
CurrentTrain: epoch  3, batch    21 | loss: 5.7804813Losses:  4.675290107727051 0.36951762437820435 0.8433085680007935
CurrentTrain: epoch  3, batch    22 | loss: 5.8881164Losses:  4.5237812995910645 0.2582785487174988 0.8305624723434448
CurrentTrain: epoch  3, batch    23 | loss: 5.6126223Losses:  5.317541599273682 0.4535452425479889 0.87415611743927
CurrentTrain: epoch  3, batch    24 | loss: 6.6452427Losses:  4.710036277770996 0.17802345752716064 0.8724945783615112
CurrentTrain: epoch  3, batch    25 | loss: 5.7605543Losses:  4.713582992553711 0.307861864566803 0.8531618118286133
CurrentTrain: epoch  3, batch    26 | loss: 5.8746066Losses:  4.918262958526611 0.37103885412216187 0.8561137318611145
CurrentTrain: epoch  3, batch    27 | loss: 6.1454158Losses:  4.473330974578857 0.30125290155410767 0.7923104763031006
CurrentTrain: epoch  3, batch    28 | loss: 5.5668945Losses:  4.830479621887207 0.2690335214138031 0.816565752029419
CurrentTrain: epoch  3, batch    29 | loss: 5.9160786Losses:  4.691594123840332 0.30279287695884705 0.8219374418258667
CurrentTrain: epoch  3, batch    30 | loss: 5.8163247Losses:  4.9535675048828125 0.35024651885032654 0.8410777449607849
CurrentTrain: epoch  3, batch    31 | loss: 6.1448917Losses:  4.534332275390625 0.24380984902381897 0.8267658948898315
CurrentTrain: epoch  3, batch    32 | loss: 5.6049080Losses:  4.859377384185791 0.3456725478172302 0.8267823457717896
CurrentTrain: epoch  3, batch    33 | loss: 6.0318322Losses:  4.813793659210205 0.2532172203063965 0.8665260672569275
CurrentTrain: epoch  3, batch    34 | loss: 5.9335370Losses:  5.251957416534424 0.23475347459316254 0.8379642963409424
CurrentTrain: epoch  3, batch    35 | loss: 6.3246756Losses:  4.780183792114258 0.37708255648612976 0.8039079904556274
CurrentTrain: epoch  3, batch    36 | loss: 5.9611740Losses:  4.671224594116211 0.2654235363006592 0.8507320880889893
CurrentTrain: epoch  3, batch    37 | loss: 5.7873802Losses:  4.5462846755981445 0.35049155354499817 0.8377494812011719
CurrentTrain: epoch  3, batch    38 | loss: 5.7345257Losses:  4.768772125244141 0.3216102123260498 0.829808235168457
CurrentTrain: epoch  3, batch    39 | loss: 5.9201908Losses:  4.640111923217773 0.27802756428718567 0.8475514054298401
CurrentTrain: epoch  3, batch    40 | loss: 5.7656908Losses:  4.473957061767578 0.2675871253013611 0.8315300941467285
CurrentTrain: epoch  3, batch    41 | loss: 5.5730743Losses:  4.7041730880737305 0.23865269124507904 0.8902851343154907
CurrentTrain: epoch  3, batch    42 | loss: 5.8331108Losses:  4.55019998550415 0.16106170415878296 0.8500488996505737
CurrentTrain: epoch  3, batch    43 | loss: 5.5613108Losses:  4.665154457092285 0.22376281023025513 0.8100546598434448
CurrentTrain: epoch  3, batch    44 | loss: 5.6989722Losses:  4.881816387176514 0.2831394076347351 0.8046169281005859
CurrentTrain: epoch  3, batch    45 | loss: 5.9695725Losses:  4.649782657623291 0.2810009717941284 0.835615873336792
CurrentTrain: epoch  3, batch    46 | loss: 5.7663994Losses:  4.372036933898926 0.3167412579059601 0.8249715566635132
CurrentTrain: epoch  3, batch    47 | loss: 5.5137501Losses:  4.8737359046936035 0.2937559485435486 0.8315092325210571
CurrentTrain: epoch  3, batch    48 | loss: 5.9990010Losses:  4.701145648956299 0.1575440764427185 0.8918087482452393
CurrentTrain: epoch  3, batch    49 | loss: 5.7504988Losses:  4.328007221221924 0.1765197515487671 0.8021405935287476
CurrentTrain: epoch  3, batch    50 | loss: 5.3066678Losses:  4.443140983581543 0.2723569869995117 0.8054196834564209
CurrentTrain: epoch  3, batch    51 | loss: 5.5209179Losses:  4.450450420379639 0.2398727387189865 0.8079646229743958
CurrentTrain: epoch  3, batch    52 | loss: 5.4982882Losses:  4.408653259277344 0.27946770191192627 0.826194703578949
CurrentTrain: epoch  3, batch    53 | loss: 5.5143156Losses:  4.4514360427856445 0.3037604093551636 0.7632916569709778
CurrentTrain: epoch  3, batch    54 | loss: 5.5184884Losses:  4.910076141357422 0.38384220004081726 0.833915114402771
CurrentTrain: epoch  3, batch    55 | loss: 6.1278334Losses:  4.67737340927124 0.20183897018432617 0.8440381288528442
CurrentTrain: epoch  3, batch    56 | loss: 5.7232504Losses:  4.804006576538086 0.2852860391139984 0.8607766628265381
CurrentTrain: epoch  3, batch    57 | loss: 5.9500694Losses:  4.5573272705078125 0.3649468421936035 0.8118090629577637
CurrentTrain: epoch  3, batch    58 | loss: 5.7340832Losses:  4.797043800354004 0.32531148195266724 0.8405284285545349
CurrentTrain: epoch  3, batch    59 | loss: 5.9628839Losses:  4.535303115844727 0.34077826142311096 0.8063199520111084
CurrentTrain: epoch  3, batch    60 | loss: 5.6824017Losses:  5.043112277984619 0.2726808488368988 0.8170778751373291
CurrentTrain: epoch  3, batch    61 | loss: 6.1328707Losses:  4.324610233306885 0.13294802606105804 0.8531441688537598
CurrentTrain: epoch  3, batch    62 | loss: 5.3107023Losses:  4.649465560913086 0.28606319427490234 0.8432997465133667
CurrentTrain: epoch  4, batch     0 | loss: 5.7788286Losses:  4.552630424499512 0.27488213777542114 0.7971770763397217
CurrentTrain: epoch  4, batch     1 | loss: 5.6246901Losses:  4.969758033752441 0.4705580770969391 0.8289503455162048
CurrentTrain: epoch  4, batch     2 | loss: 6.2692666Losses:  4.572916030883789 0.25516265630722046 0.7895268201828003
CurrentTrain: epoch  4, batch     3 | loss: 5.6176057Losses:  4.831629753112793 0.37285274267196655 0.8459262847900391
CurrentTrain: epoch  4, batch     4 | loss: 6.0504088Losses:  4.4306535720825195 0.31271493434906006 0.8029471039772034
CurrentTrain: epoch  4, batch     5 | loss: 5.5463157Losses:  4.46419620513916 0.2639918923377991 0.8608835935592651
CurrentTrain: epoch  4, batch     6 | loss: 5.5890718Losses:  4.361051559448242 0.21837271749973297 0.820056676864624
CurrentTrain: epoch  4, batch     7 | loss: 5.3994808Losses:  4.6163177490234375 0.23434095084667206 0.8136561512947083
CurrentTrain: epoch  4, batch     8 | loss: 5.6643152Losses:  4.453619003295898 0.21633410453796387 0.8494728803634644
CurrentTrain: epoch  4, batch     9 | loss: 5.5194263Losses:  4.557466506958008 0.2324228584766388 0.8916947245597839
CurrentTrain: epoch  4, batch    10 | loss: 5.6815839Losses:  4.352532386779785 0.2202763706445694 0.7950338125228882
CurrentTrain: epoch  4, batch    11 | loss: 5.3678427Losses:  4.389411926269531 0.2077212929725647 0.8697390556335449
CurrentTrain: epoch  4, batch    12 | loss: 5.4668722Losses:  4.314345836639404 0.27070584893226624 0.8158582448959351
CurrentTrain: epoch  4, batch    13 | loss: 5.4009099Losses:  4.536848068237305 0.19574910402297974 0.8294388651847839
CurrentTrain: epoch  4, batch    14 | loss: 5.5620360Losses:  4.357088565826416 0.2870284914970398 0.7931996583938599
CurrentTrain: epoch  4, batch    15 | loss: 5.4373164Losses:  4.587191581726074 0.2977415919303894 0.8158326148986816
CurrentTrain: epoch  4, batch    16 | loss: 5.7007656Losses:  4.3743085861206055 0.16319730877876282 0.8617740273475647
CurrentTrain: epoch  4, batch    17 | loss: 5.3992801Losses:  4.359243392944336 0.2696313261985779 0.825331449508667
CurrentTrain: epoch  4, batch    18 | loss: 5.4542065Losses:  4.37590217590332 0.2254188060760498 0.8186643719673157
CurrentTrain: epoch  4, batch    19 | loss: 5.4199858Losses:  4.400282859802246 0.2349228709936142 0.7861673831939697
CurrentTrain: epoch  4, batch    20 | loss: 5.4213734Losses:  4.324491024017334 0.20918914675712585 0.8474948406219482
CurrentTrain: epoch  4, batch    21 | loss: 5.3811750Losses:  4.303977012634277 0.18519359827041626 0.7439494132995605
CurrentTrain: epoch  4, batch    22 | loss: 5.2331200Losses:  4.449000358581543 0.19052863121032715 0.8349781036376953
CurrentTrain: epoch  4, batch    23 | loss: 5.4745073Losses:  4.297659873962402 0.3064003884792328 0.799540102481842
CurrentTrain: epoch  4, batch    24 | loss: 5.4036002Losses:  4.337186813354492 0.17552241683006287 0.8048042058944702
CurrentTrain: epoch  4, batch    25 | loss: 5.3175135Losses:  4.474108695983887 0.1910550445318222 0.7657684683799744
CurrentTrain: epoch  4, batch    26 | loss: 5.4309320Losses:  4.445682525634766 0.2529432773590088 0.8006730079650879
CurrentTrain: epoch  4, batch    27 | loss: 5.4992986Losses:  4.426092147827148 0.17591795325279236 0.7411682605743408
CurrentTrain: epoch  4, batch    28 | loss: 5.3431787Losses:  4.4483537673950195 0.22721591591835022 0.8492323160171509
CurrentTrain: epoch  4, batch    29 | loss: 5.5248017Losses:  4.338350772857666 0.26690828800201416 0.7734967470169067
CurrentTrain: epoch  4, batch    30 | loss: 5.3787556Losses:  4.354770660400391 0.2546412944793701 0.8154808282852173
CurrentTrain: epoch  4, batch    31 | loss: 5.4248929Losses:  4.286494255065918 0.19348886609077454 0.7830678224563599
CurrentTrain: epoch  4, batch    32 | loss: 5.2630510Losses:  4.44827127456665 0.22896301746368408 0.8144283890724182
CurrentTrain: epoch  4, batch    33 | loss: 5.4916625Losses:  4.322943687438965 0.21574467420578003 0.8259320855140686
CurrentTrain: epoch  4, batch    34 | loss: 5.3646202Losses:  4.2428741455078125 0.1846102774143219 0.7873251438140869
CurrentTrain: epoch  4, batch    35 | loss: 5.2148094Losses:  4.257746696472168 0.23089633882045746 0.8170717358589172
CurrentTrain: epoch  4, batch    36 | loss: 5.3057151Losses:  4.379120826721191 0.09161119163036346 0.8560758829116821
CurrentTrain: epoch  4, batch    37 | loss: 5.3268080Losses:  4.29279899597168 0.21523818373680115 0.840114414691925
CurrentTrain: epoch  4, batch    38 | loss: 5.3481517Losses:  4.4149603843688965 0.28782549500465393 0.8336392641067505
CurrentTrain: epoch  4, batch    39 | loss: 5.5364251Losses:  4.264181137084961 0.2191825956106186 0.7802691459655762
CurrentTrain: epoch  4, batch    40 | loss: 5.2636328Losses:  4.340616226196289 0.2192012071609497 0.8267736434936523
CurrentTrain: epoch  4, batch    41 | loss: 5.3865910Losses:  4.30679988861084 0.2552427649497986 0.8127581477165222
CurrentTrain: epoch  4, batch    42 | loss: 5.3748007Losses:  4.339701175689697 0.28522375226020813 0.8097829222679138
CurrentTrain: epoch  4, batch    43 | loss: 5.4347081Losses:  4.390167236328125 0.17928007245063782 0.8347365856170654
CurrentTrain: epoch  4, batch    44 | loss: 5.4041843Losses:  4.42720890045166 0.12267965078353882 0.886982262134552
CurrentTrain: epoch  4, batch    45 | loss: 5.4368711Losses:  4.642797470092773 0.24586284160614014 0.8380053639411926
CurrentTrain: epoch  4, batch    46 | loss: 5.7266660Losses:  4.230330944061279 0.24722257256507874 0.8166738748550415
CurrentTrain: epoch  4, batch    47 | loss: 5.2942271Losses:  4.444245338439941 0.1773594319820404 0.8287734985351562
CurrentTrain: epoch  4, batch    48 | loss: 5.4503784Losses:  4.204024791717529 0.2558361291885376 0.8148733377456665
CurrentTrain: epoch  4, batch    49 | loss: 5.2747340Losses:  4.238885879516602 0.2130664587020874 0.8241573572158813
CurrentTrain: epoch  4, batch    50 | loss: 5.2761097Losses:  4.320054054260254 0.23767077922821045 0.7748881578445435
CurrentTrain: epoch  4, batch    51 | loss: 5.3326130Losses:  4.354844570159912 0.2776252031326294 0.8170379400253296
CurrentTrain: epoch  4, batch    52 | loss: 5.4495077Losses:  4.27168083190918 0.17792749404907227 0.8117331862449646
CurrentTrain: epoch  4, batch    53 | loss: 5.2613416Losses:  4.311652660369873 0.21179574728012085 0.8354904651641846
CurrentTrain: epoch  4, batch    54 | loss: 5.3589392Losses:  4.383832931518555 0.11940009891986847 0.8490043878555298
CurrentTrain: epoch  4, batch    55 | loss: 5.3522372Losses:  4.2511396408081055 0.17754094302654266 0.8412071466445923
CurrentTrain: epoch  4, batch    56 | loss: 5.2698874Losses:  4.1917219161987305 0.21984437108039856 0.7650316953659058
CurrentTrain: epoch  4, batch    57 | loss: 5.1765981Losses:  4.212222576141357 0.20306985080242157 0.7864660620689392
CurrentTrain: epoch  4, batch    58 | loss: 5.2017584Losses:  4.277597904205322 0.24887323379516602 0.7859042882919312
CurrentTrain: epoch  4, batch    59 | loss: 5.3123755Losses:  4.196435451507568 0.15986952185630798 0.8219578862190247
CurrentTrain: epoch  4, batch    60 | loss: 5.1782632Losses:  4.210944175720215 0.22868551313877106 0.8014546632766724
CurrentTrain: epoch  4, batch    61 | loss: 5.2410841Losses:  4.184953689575195 0.10189199447631836 0.8975834846496582
CurrentTrain: epoch  4, batch    62 | loss: 5.1844292Losses:  4.15391731262207 0.1461411714553833 0.7536687254905701
CurrentTrain: epoch  5, batch     0 | loss: 5.0537271Losses:  4.262046813964844 0.15825298428535461 0.828985333442688
CurrentTrain: epoch  5, batch     1 | loss: 5.2492852Losses:  4.18170166015625 0.2102881669998169 0.7979341745376587
CurrentTrain: epoch  5, batch     2 | loss: 5.1899238Losses:  4.233212471008301 0.20864781737327576 0.8076584935188293
CurrentTrain: epoch  5, batch     3 | loss: 5.2495189Losses:  4.276159286499023 0.1914644092321396 0.8093476891517639
CurrentTrain: epoch  5, batch     4 | loss: 5.2769713Losses:  4.191183090209961 0.2138318568468094 0.7900912761688232
CurrentTrain: epoch  5, batch     5 | loss: 5.1951065Losses:  4.198606014251709 0.16807010769844055 0.7685496807098389
CurrentTrain: epoch  5, batch     6 | loss: 5.1352262Losses:  4.2131547927856445 0.14779293537139893 0.816369354724884
CurrentTrain: epoch  5, batch     7 | loss: 5.1773171Losses:  4.37239408493042 0.176550030708313 0.7841875553131104
CurrentTrain: epoch  5, batch     8 | loss: 5.3331318Losses:  4.264216899871826 0.22368735074996948 0.8350481986999512
CurrentTrain: epoch  5, batch     9 | loss: 5.3229523Losses:  4.15848445892334 0.24108918011188507 0.7618032693862915
CurrentTrain: epoch  5, batch    10 | loss: 5.1613770Losses:  4.1912336349487305 0.19114410877227783 0.83980393409729
CurrentTrain: epoch  5, batch    11 | loss: 5.2221813Losses:  4.1851043701171875 0.2187831997871399 0.7521793246269226
CurrentTrain: epoch  5, batch    12 | loss: 5.1560669Losses:  4.262044906616211 0.1624847948551178 0.8090982437133789
CurrentTrain: epoch  5, batch    13 | loss: 5.2336278Losses:  4.199742317199707 0.2378196120262146 0.7443854212760925
CurrentTrain: epoch  5, batch    14 | loss: 5.1819472Losses:  4.180061340332031 0.23968049883842468 0.7773168683052063
CurrentTrain: epoch  5, batch    15 | loss: 5.1970587Losses:  4.287352085113525 0.20780694484710693 0.8284714221954346
CurrentTrain: epoch  5, batch    16 | loss: 5.3236303Losses:  4.205340385437012 0.1800800859928131 0.7568084001541138
CurrentTrain: epoch  5, batch    17 | loss: 5.1422286Losses:  4.2003936767578125 0.1708601713180542 0.810226559638977
CurrentTrain: epoch  5, batch    18 | loss: 5.1814804Losses:  4.109519958496094 0.16769546270370483 0.7640562057495117
CurrentTrain: epoch  5, batch    19 | loss: 5.0412717Losses:  4.223604202270508 0.15736941993236542 0.7975242733955383
CurrentTrain: epoch  5, batch    20 | loss: 5.1784983Losses:  4.227834224700928 0.20284855365753174 0.8090250492095947
CurrentTrain: epoch  5, batch    21 | loss: 5.2397079Losses:  4.312776565551758 0.23161917924880981 0.8480942845344543
CurrentTrain: epoch  5, batch    22 | loss: 5.3924904Losses:  4.196738243103027 0.16001491248607635 0.823752224445343
CurrentTrain: epoch  5, batch    23 | loss: 5.1805058Losses:  4.063701629638672 0.20641133189201355 0.7981357574462891
CurrentTrain: epoch  5, batch    24 | loss: 5.0682487Losses:  4.134768486022949 0.2118569314479828 0.7784180045127869
CurrentTrain: epoch  5, batch    25 | loss: 5.1250434Losses:  4.186762809753418 0.2044227123260498 0.750260055065155
CurrentTrain: epoch  5, batch    26 | loss: 5.1414456Losses:  4.1864776611328125 0.17543980479240417 0.8288741111755371
CurrentTrain: epoch  5, batch    27 | loss: 5.1907916Losses:  4.227202415466309 0.1423320472240448 0.8493093252182007
CurrentTrain: epoch  5, batch    28 | loss: 5.2188439Losses:  4.134222030639648 0.1754365712404251 0.7711184024810791
CurrentTrain: epoch  5, batch    29 | loss: 5.0807772Losses:  4.147337913513184 0.139879047870636 0.789670467376709
CurrentTrain: epoch  5, batch    30 | loss: 5.0768876Losses:  4.169472694396973 0.13480797410011292 0.8507145047187805
CurrentTrain: epoch  5, batch    31 | loss: 5.1549954Losses:  4.198489189147949 0.20480987429618835 0.8190759420394897
CurrentTrain: epoch  5, batch    32 | loss: 5.2223749Losses:  4.1656694412231445 0.1428869068622589 0.8512004613876343
CurrentTrain: epoch  5, batch    33 | loss: 5.1597571Losses:  4.251734733581543 0.21072906255722046 0.7776908874511719
CurrentTrain: epoch  5, batch    34 | loss: 5.2401547Losses:  4.3026323318481445 0.2081718146800995 0.8406046628952026
CurrentTrain: epoch  5, batch    35 | loss: 5.3514090Losses:  4.595127105712891 0.27787506580352783 0.7897589206695557
CurrentTrain: epoch  5, batch    36 | loss: 5.6627607Losses:  4.202579021453857 0.0942697748541832 0.7619373798370361
CurrentTrain: epoch  5, batch    37 | loss: 5.0587864Losses:  4.155855655670166 0.17359231412410736 0.7777359485626221
CurrentTrain: epoch  5, batch    38 | loss: 5.1071835Losses:  4.137747764587402 0.17899104952812195 0.7625788450241089
CurrentTrain: epoch  5, batch    39 | loss: 5.0793176Losses:  4.17592716217041 0.18133936822414398 0.7986749410629272
CurrentTrain: epoch  5, batch    40 | loss: 5.1559415Losses:  4.106236457824707 0.1507575362920761 0.8041255474090576
CurrentTrain: epoch  5, batch    41 | loss: 5.0611191Losses:  4.18814754486084 0.20944568514823914 0.7715560793876648
CurrentTrain: epoch  5, batch    42 | loss: 5.1691489Losses:  4.155148983001709 0.14928492903709412 0.8243489265441895
CurrentTrain: epoch  5, batch    43 | loss: 5.1287827Losses:  4.176324367523193 0.1509312391281128 0.8215463161468506
CurrentTrain: epoch  5, batch    44 | loss: 5.1488018Losses:  4.177373886108398 0.1495486944913864 0.8125574588775635
CurrentTrain: epoch  5, batch    45 | loss: 5.1394796Losses:  4.755627632141113 0.18464797735214233 0.8073970079421997
CurrentTrain: epoch  5, batch    46 | loss: 5.7476726Losses:  4.191474914550781 0.18351396918296814 0.8434041738510132
CurrentTrain: epoch  5, batch    47 | loss: 5.2183933Losses:  4.129054069519043 0.16670586168766022 0.776134729385376
CurrentTrain: epoch  5, batch    48 | loss: 5.0718946Losses:  4.286671161651611 0.19665825366973877 0.8010975122451782
CurrentTrain: epoch  5, batch    49 | loss: 5.2844267Losses:  4.168742656707764 0.14568553864955902 0.8333059549331665
CurrentTrain: epoch  5, batch    50 | loss: 5.1477342Losses:  4.316070556640625 0.1346142590045929 0.8112269639968872
CurrentTrain: epoch  5, batch    51 | loss: 5.2619119Losses:  4.240058898925781 0.19618913531303406 0.7776674032211304
CurrentTrain: epoch  5, batch    52 | loss: 5.2139153Losses:  4.193719863891602 0.19532883167266846 0.8160058856010437
CurrentTrain: epoch  5, batch    53 | loss: 5.2050543Losses:  4.11086368560791 0.14748112857341766 0.7505549192428589
CurrentTrain: epoch  5, batch    54 | loss: 5.0088997Losses:  4.161962032318115 0.1511562168598175 0.8088463544845581
CurrentTrain: epoch  5, batch    55 | loss: 5.1219649Losses:  4.135815620422363 0.13980704545974731 0.82157963514328
CurrentTrain: epoch  5, batch    56 | loss: 5.0972023Losses:  4.150853633880615 0.1387687623500824 0.7490570545196533
CurrentTrain: epoch  5, batch    57 | loss: 5.0386791Losses:  4.241296768188477 0.14246997237205505 0.7880175709724426
CurrentTrain: epoch  5, batch    58 | loss: 5.1717844Losses:  4.173495292663574 0.19573360681533813 0.7683022022247314
CurrentTrain: epoch  5, batch    59 | loss: 5.1375313Losses:  4.176095962524414 0.1572367250919342 0.8450684547424316
CurrentTrain: epoch  5, batch    60 | loss: 5.1784010Losses:  4.203365802764893 0.1483011096715927 0.787425696849823
CurrentTrain: epoch  5, batch    61 | loss: 5.1390924Losses:  4.2051615715026855 0.19564682245254517 0.7485617399215698
CurrentTrain: epoch  5, batch    62 | loss: 5.1493702Losses:  4.162175178527832 0.21652036905288696 0.7714084386825562
CurrentTrain: epoch  6, batch     0 | loss: 5.1501040Losses:  4.243821144104004 0.1471409946680069 0.787315309047699
CurrentTrain: epoch  6, batch     1 | loss: 5.1782775Losses:  4.14915657043457 0.1372731775045395 0.8217597603797913
CurrentTrain: epoch  6, batch     2 | loss: 5.1081896Losses:  4.182992458343506 0.15281154215335846 0.7886060476303101
CurrentTrain: epoch  6, batch     3 | loss: 5.1244102Losses:  4.177570343017578 0.17649510502815247 0.7996441125869751
CurrentTrain: epoch  6, batch     4 | loss: 5.1537094Losses:  4.170468330383301 0.15608593821525574 0.8035629987716675
CurrentTrain: epoch  6, batch     5 | loss: 5.1301174Losses:  4.135815620422363 0.1914513111114502 0.7226288318634033
CurrentTrain: epoch  6, batch     6 | loss: 5.0498953Losses:  4.122467041015625 0.15543904900550842 0.7081445455551147
CurrentTrain: epoch  6, batch     7 | loss: 4.9860506Losses:  4.139859676361084 0.18157371878623962 0.7483695149421692
CurrentTrain: epoch  6, batch     8 | loss: 5.0698032Losses:  4.194543361663818 0.11443275213241577 0.8878062963485718
CurrentTrain: epoch  6, batch     9 | loss: 5.1967826Losses:  4.173020362854004 0.14204856753349304 0.8164035081863403
CurrentTrain: epoch  6, batch    10 | loss: 5.1314721Losses:  4.14856481552124 0.12681052088737488 0.7689970135688782
CurrentTrain: epoch  6, batch    11 | loss: 5.0443726Losses:  4.13670539855957 0.17806914448738098 0.7122282981872559
CurrentTrain: epoch  6, batch    12 | loss: 5.0270028Losses:  4.124166965484619 0.17372842133045197 0.7960965633392334
CurrentTrain: epoch  6, batch    13 | loss: 5.0939922Losses:  4.1380181312561035 0.19156964123249054 0.791728675365448
CurrentTrain: epoch  6, batch    14 | loss: 5.1213164Losses:  4.194097995758057 0.12616659700870514 0.8148200511932373
CurrentTrain: epoch  6, batch    15 | loss: 5.1350851Losses:  4.075656890869141 0.1827695071697235 0.7626481056213379
CurrentTrain: epoch  6, batch    16 | loss: 5.0210743Losses:  4.1552734375 0.1839085966348648 0.7491661310195923
CurrentTrain: epoch  6, batch    17 | loss: 5.0883479Losses:  4.155522346496582 0.11781728267669678 0.7727851271629333
CurrentTrain: epoch  6, batch    18 | loss: 5.0461249Losses:  4.120591163635254 0.14272984862327576 0.8044200539588928
CurrentTrain: epoch  6, batch    19 | loss: 5.0677409Losses:  4.115210056304932 0.1914968639612198 0.7543802261352539
CurrentTrain: epoch  6, batch    20 | loss: 5.0610871Losses:  4.098114013671875 0.14472568035125732 0.7847118377685547
CurrentTrain: epoch  6, batch    21 | loss: 5.0275517Losses:  4.116311073303223 0.13887766003608704 0.7381118535995483
CurrentTrain: epoch  6, batch    22 | loss: 4.9933009Losses:  4.142704486846924 0.1484050750732422 0.7843053340911865
CurrentTrain: epoch  6, batch    23 | loss: 5.0754147Losses:  4.110127925872803 0.16077934205532074 0.7556251287460327
CurrentTrain: epoch  6, batch    24 | loss: 5.0265326Losses:  4.180643081665039 0.13340729475021362 0.8162192702293396
CurrentTrain: epoch  6, batch    25 | loss: 5.1302695Losses:  4.166196823120117 0.13577738404273987 0.826593279838562
CurrentTrain: epoch  6, batch    26 | loss: 5.1285677Losses:  4.138200759887695 0.19585338234901428 0.764441967010498
CurrentTrain: epoch  6, batch    27 | loss: 5.0984960Losses:  4.1585235595703125 0.10134977847337723 0.8222968578338623
CurrentTrain: epoch  6, batch    28 | loss: 5.0821705Losses:  4.1523566246032715 0.12715861201286316 0.7827272415161133
CurrentTrain: epoch  6, batch    29 | loss: 5.0622425Losses:  4.087331771850586 0.14357085525989532 0.7988903522491455
CurrentTrain: epoch  6, batch    30 | loss: 5.0297928Losses:  4.115174770355225 0.17403222620487213 0.7818389534950256
CurrentTrain: epoch  6, batch    31 | loss: 5.0710459Losses:  4.0073065757751465 0.13691525161266327 0.8377685546875
CurrentTrain: epoch  6, batch    32 | loss: 4.9819903Losses:  4.205333709716797 0.12449903041124344 0.8720577359199524
CurrentTrain: epoch  6, batch    33 | loss: 5.2018905Losses:  4.15822172164917 0.11544327437877655 0.7429193258285522
CurrentTrain: epoch  6, batch    34 | loss: 5.0165844Losses:  4.112752914428711 0.0976082980632782 0.7930135726928711
CurrentTrain: epoch  6, batch    35 | loss: 5.0033746Losses:  4.0228986740112305 0.13093404471874237 0.7295849323272705
CurrentTrain: epoch  6, batch    36 | loss: 4.8834181Losses:  4.120575904846191 0.12232714891433716 0.8511502146720886
CurrentTrain: epoch  6, batch    37 | loss: 5.0940533Losses:  4.099349498748779 0.13533669710159302 0.8235030174255371
CurrentTrain: epoch  6, batch    38 | loss: 5.0581894Losses:  4.143344879150391 0.1071716696023941 0.6800618171691895
CurrentTrain: epoch  6, batch    39 | loss: 4.9305782Losses:  4.112653732299805 0.1565351039171219 0.7434579730033875
CurrentTrain: epoch  6, batch    40 | loss: 5.0126467Losses:  4.112832546234131 0.1583627462387085 0.8254162073135376
CurrentTrain: epoch  6, batch    41 | loss: 5.0966115Losses:  4.081657409667969 0.15410619974136353 0.7796408534049988
CurrentTrain: epoch  6, batch    42 | loss: 5.0154042Losses:  4.0755815505981445 0.1602802872657776 0.7764718532562256
CurrentTrain: epoch  6, batch    43 | loss: 5.0123339Losses:  4.0833740234375 0.1168147549033165 0.754951000213623
CurrentTrain: epoch  6, batch    44 | loss: 4.9551396Losses:  4.114468097686768 0.08488723635673523 0.8717557191848755
CurrentTrain: epoch  6, batch    45 | loss: 5.0711107Losses:  4.281467437744141 0.20104524493217468 0.7744945287704468
CurrentTrain: epoch  6, batch    46 | loss: 5.2570071Losses:  4.547750473022461 0.26870742440223694 0.7737190127372742
CurrentTrain: epoch  6, batch    47 | loss: 5.5901766Losses:  4.121042728424072 0.192740797996521 0.7566927671432495
CurrentTrain: epoch  6, batch    48 | loss: 5.0704765Losses:  4.049592971801758 0.15147125720977783 0.7295358180999756
CurrentTrain: epoch  6, batch    49 | loss: 4.9306002Losses:  4.171258449554443 0.1887456327676773 0.8505297899246216
CurrentTrain: epoch  6, batch    50 | loss: 5.2105336Losses:  4.1256327629089355 0.13219860196113586 0.7920958995819092
CurrentTrain: epoch  6, batch    51 | loss: 5.0499277Losses:  4.111946105957031 0.14745762944221497 0.7269998788833618
CurrentTrain: epoch  6, batch    52 | loss: 4.9864035Losses:  4.0829010009765625 0.14416900277137756 0.7493308782577515
CurrentTrain: epoch  6, batch    53 | loss: 4.9764009Losses:  4.097024917602539 0.09396164119243622 0.6922211647033691
CurrentTrain: epoch  6, batch    54 | loss: 4.8832078Losses:  4.0307207107543945 0.14618541300296783 0.8434954881668091
CurrentTrain: epoch  6, batch    55 | loss: 5.0204015Losses:  4.042063236236572 0.11923480778932571 0.7896551489830017
CurrentTrain: epoch  6, batch    56 | loss: 4.9509535Losses:  4.0302324295043945 0.11061446368694305 0.8627195954322815
CurrentTrain: epoch  6, batch    57 | loss: 5.0035663Losses:  4.119668960571289 0.11509427428245544 0.8465035557746887
CurrentTrain: epoch  6, batch    58 | loss: 5.0812669Losses:  4.081206798553467 0.14049574732780457 0.7000809907913208
CurrentTrain: epoch  6, batch    59 | loss: 4.9217834Losses:  3.9933035373687744 0.14555099606513977 0.7170048952102661
CurrentTrain: epoch  6, batch    60 | loss: 4.8558593Losses:  4.085816383361816 0.11427778005599976 0.7389411926269531
CurrentTrain: epoch  6, batch    61 | loss: 4.9390354Losses:  3.977250099182129 0.10239751636981964 0.8451720476150513
CurrentTrain: epoch  6, batch    62 | loss: 4.9248195Losses:  4.094991683959961 0.1281893253326416 0.803954005241394
CurrentTrain: epoch  7, batch     0 | loss: 5.0271349Losses:  4.132035255432129 0.1304147094488144 0.8380166292190552
CurrentTrain: epoch  7, batch     1 | loss: 5.1004663Losses:  4.066784381866455 0.08501634001731873 0.8581244945526123
CurrentTrain: epoch  7, batch     2 | loss: 5.0099249Losses:  4.065271377563477 0.13960927724838257 0.800984263420105
CurrentTrain: epoch  7, batch     3 | loss: 5.0058651Losses:  4.081164360046387 0.18602538108825684 0.7394387125968933
CurrentTrain: epoch  7, batch     4 | loss: 5.0066285Losses:  4.059876441955566 0.141027569770813 0.7105944752693176
CurrentTrain: epoch  7, batch     5 | loss: 4.9114985Losses:  3.9409899711608887 0.12323369085788727 0.8524343967437744
CurrentTrain: epoch  7, batch     6 | loss: 4.9166584Losses:  4.123138427734375 0.12114641070365906 0.759148359298706
CurrentTrain: epoch  7, batch     7 | loss: 5.0034332Losses:  4.076302528381348 0.0897843986749649 0.7173226475715637
CurrentTrain: epoch  7, batch     8 | loss: 4.8834100Losses:  4.080837249755859 0.1557590663433075 0.7699315547943115
CurrentTrain: epoch  7, batch     9 | loss: 5.0065279Losses:  4.035435676574707 0.15737932920455933 0.7389822006225586
CurrentTrain: epoch  7, batch    10 | loss: 4.9317970Losses:  4.077129364013672 0.15995202958583832 0.8047011494636536
CurrentTrain: epoch  7, batch    11 | loss: 5.0417829Losses:  4.114641189575195 0.12449021637439728 0.730201244354248
CurrentTrain: epoch  7, batch    12 | loss: 4.9693327Losses:  4.066115379333496 0.12115976214408875 0.747816264629364
CurrentTrain: epoch  7, batch    13 | loss: 4.9350910Losses:  4.073463439941406 0.13870331645011902 0.7364262342453003
CurrentTrain: epoch  7, batch    14 | loss: 4.9485931Losses:  4.042638301849365 0.11257041245698929 0.8124840259552002
CurrentTrain: epoch  7, batch    15 | loss: 4.9676924Losses:  4.049556732177734 0.13336272537708282 0.7418652772903442
CurrentTrain: epoch  7, batch    16 | loss: 4.9247847Losses:  4.028446674346924 0.1600399911403656 0.7591078877449036
CurrentTrain: epoch  7, batch    17 | loss: 4.9475946Losses:  4.050299644470215 0.14948393404483795 0.7967153191566467
CurrentTrain: epoch  7, batch    18 | loss: 4.9964991Losses:  4.113462924957275 0.14893904328346252 0.7481286525726318
CurrentTrain: epoch  7, batch    19 | loss: 5.0105305Losses:  4.056158065795898 0.09928372502326965 0.7423231601715088
CurrentTrain: epoch  7, batch    20 | loss: 4.8977652Losses:  4.062360763549805 0.15709994733333588 0.7562224864959717
CurrentTrain: epoch  7, batch    21 | loss: 4.9756832Losses:  4.09727668762207 0.13708090782165527 0.7976024150848389
CurrentTrain: epoch  7, batch    22 | loss: 5.0319605Losses:  4.05543327331543 0.14402331411838531 0.7266083359718323
CurrentTrain: epoch  7, batch    23 | loss: 4.9260650Losses:  4.0144219398498535 0.10130128264427185 0.8016960024833679
CurrentTrain: epoch  7, batch    24 | loss: 4.9174190Losses:  4.057671070098877 0.154672309756279 0.7786665558815002
CurrentTrain: epoch  7, batch    25 | loss: 4.9910097Losses:  4.029481887817383 0.07939702272415161 0.8486993312835693
CurrentTrain: epoch  7, batch    26 | loss: 4.9575787Losses:  4.069876670837402 0.1641736924648285 0.7491405010223389
CurrentTrain: epoch  7, batch    27 | loss: 4.9831905Losses:  4.022046089172363 0.12998536229133606 0.7272621989250183
CurrentTrain: epoch  7, batch    28 | loss: 4.8792934Losses:  4.049948692321777 0.1608327180147171 0.7742428779602051
CurrentTrain: epoch  7, batch    29 | loss: 4.9850245Losses:  4.085989952087402 0.12130255997180939 0.824644923210144
CurrentTrain: epoch  7, batch    30 | loss: 5.0319376Losses:  4.04325008392334 0.09897434711456299 0.7402883768081665
CurrentTrain: epoch  7, batch    31 | loss: 4.8825126Losses:  4.037268161773682 0.15214373171329498 0.7705279588699341
CurrentTrain: epoch  7, batch    32 | loss: 4.9599400Losses:  4.022078514099121 0.09064934402704239 0.7931513786315918
CurrentTrain: epoch  7, batch    33 | loss: 4.9058790Losses:  4.068326950073242 0.1006900817155838 0.764313817024231
CurrentTrain: epoch  7, batch    34 | loss: 4.9333305Losses:  4.090096473693848 0.13826049864292145 0.76349937915802
CurrentTrain: epoch  7, batch    35 | loss: 4.9918561Losses:  4.075133323669434 0.1414080262184143 0.7543398141860962
CurrentTrain: epoch  7, batch    36 | loss: 4.9708810Losses:  4.031888961791992 0.13036274909973145 0.7990073561668396
CurrentTrain: epoch  7, batch    37 | loss: 4.9612589Losses:  4.1320037841796875 0.16193224489688873 0.7719786167144775
CurrentTrain: epoch  7, batch    38 | loss: 5.0659151Losses:  4.088250160217285 0.10087517648935318 0.8577154874801636
CurrentTrain: epoch  7, batch    39 | loss: 5.0468411Losses:  4.01833438873291 0.12781861424446106 0.7707573771476746
CurrentTrain: epoch  7, batch    40 | loss: 4.9169102Losses:  3.9874868392944336 0.14161917567253113 0.7271410226821899
CurrentTrain: epoch  7, batch    41 | loss: 4.8562469Losses:  4.050448894500732 0.13495537638664246 0.8237370252609253
CurrentTrain: epoch  7, batch    42 | loss: 5.0091414Losses:  4.037333965301514 0.10766523331403732 0.7938846945762634
CurrentTrain: epoch  7, batch    43 | loss: 4.9388838Losses:  4.0708160400390625 0.1430434286594391 0.748013973236084
CurrentTrain: epoch  7, batch    44 | loss: 4.9618735Losses:  4.028641700744629 0.08724436163902283 0.7336629629135132
CurrentTrain: epoch  7, batch    45 | loss: 4.8495493Losses:  4.068448066711426 0.12392905354499817 0.7542104721069336
CurrentTrain: epoch  7, batch    46 | loss: 4.9465876Losses:  4.01625394821167 0.14795850217342377 0.734325647354126
CurrentTrain: epoch  7, batch    47 | loss: 4.8985376Losses:  4.076082706451416 0.06777824461460114 0.7941774129867554
CurrentTrain: epoch  7, batch    48 | loss: 4.9380383Losses:  4.038695335388184 0.11601865291595459 0.754920244216919
CurrentTrain: epoch  7, batch    49 | loss: 4.9096346Losses:  4.038292407989502 0.08926396071910858 0.7796765565872192
CurrentTrain: epoch  7, batch    50 | loss: 4.9072328Losses:  4.045200347900391 0.12157266587018967 0.7537423968315125
CurrentTrain: epoch  7, batch    51 | loss: 4.9205151Losses:  4.023085117340088 0.12459567934274673 0.6716004610061646
CurrentTrain: epoch  7, batch    52 | loss: 4.8192811Losses:  4.052254676818848 0.11348605155944824 0.7393022775650024
CurrentTrain: epoch  7, batch    53 | loss: 4.9050431Losses:  4.028258323669434 0.11905668675899506 0.7560926675796509
CurrentTrain: epoch  7, batch    54 | loss: 4.9034076Losses:  4.017444133758545 0.11860332638025284 0.7516155242919922
CurrentTrain: epoch  7, batch    55 | loss: 4.8876629Losses:  4.025289535522461 0.1368085741996765 0.7275099754333496
CurrentTrain: epoch  7, batch    56 | loss: 4.8896079Losses:  3.9960649013519287 0.10907453298568726 0.7632240056991577
CurrentTrain: epoch  7, batch    57 | loss: 4.8683634Losses:  4.005809307098389 0.09559287130832672 0.7451131343841553
CurrentTrain: epoch  7, batch    58 | loss: 4.8465157Losses:  4.073910236358643 0.087760329246521 0.8092455863952637
CurrentTrain: epoch  7, batch    59 | loss: 4.9709163Losses:  4.005969047546387 0.09722604602575302 0.7820768356323242
CurrentTrain: epoch  7, batch    60 | loss: 4.8852720Losses:  4.040754318237305 0.10121119022369385 0.7363877296447754
CurrentTrain: epoch  7, batch    61 | loss: 4.8783531Losses:  4.016916751861572 0.08695290982723236 0.6829830408096313
CurrentTrain: epoch  7, batch    62 | loss: 4.7868524Losses:  3.994680643081665 0.13708975911140442 0.7452149391174316
CurrentTrain: epoch  8, batch     0 | loss: 4.8769855Losses:  4.030665397644043 0.12199550122022629 0.736900806427002
CurrentTrain: epoch  8, batch     1 | loss: 4.8895617Losses:  4.020589828491211 0.0959242731332779 0.7423561811447144
CurrentTrain: epoch  8, batch     2 | loss: 4.8588705Losses:  4.053986549377441 0.06117404252290726 0.7066619396209717
CurrentTrain: epoch  8, batch     3 | loss: 4.8218222Losses:  4.042415142059326 0.11893221735954285 0.7233868837356567
CurrentTrain: epoch  8, batch     4 | loss: 4.8847342Losses:  4.116755485534668 0.11419196426868439 0.7834933400154114
CurrentTrain: epoch  8, batch     5 | loss: 5.0144410Losses:  4.038701057434082 0.13125523924827576 0.7703635096549988
CurrentTrain: epoch  8, batch     6 | loss: 4.9403195Losses:  4.064846038818359 0.12007739394903183 0.7859572172164917
CurrentTrain: epoch  8, batch     7 | loss: 4.9708810Losses:  4.01067590713501 0.08370442688465118 0.7677953243255615
CurrentTrain: epoch  8, batch     8 | loss: 4.8621759Losses:  4.007809638977051 0.10794280469417572 0.7529276013374329
CurrentTrain: epoch  8, batch     9 | loss: 4.8686800Losses:  4.002347946166992 0.08600474894046783 0.8128468990325928
CurrentTrain: epoch  8, batch    10 | loss: 4.9011993Losses:  4.05696964263916 0.13343894481658936 0.7175250053405762
CurrentTrain: epoch  8, batch    11 | loss: 4.9079337Losses:  4.084869384765625 0.10954096168279648 0.75861656665802
CurrentTrain: epoch  8, batch    12 | loss: 4.9530268Losses:  4.071593284606934 0.09900049865245819 0.759863018989563
CurrentTrain: epoch  8, batch    13 | loss: 4.9304566Losses:  4.032547950744629 0.07657235860824585 0.7881224155426025
CurrentTrain: epoch  8, batch    14 | loss: 4.8972425Losses:  3.9876511096954346 0.09490768611431122 0.7193953990936279
CurrentTrain: epoch  8, batch    15 | loss: 4.8019543Losses:  4.042433738708496 0.10734756290912628 0.7113500833511353
CurrentTrain: epoch  8, batch    16 | loss: 4.8611312Losses:  3.9750380516052246 0.07175790518522263 0.7840613126754761
CurrentTrain: epoch  8, batch    17 | loss: 4.8308573Losses:  4.048588752746582 0.1247987300157547 0.7338034510612488
CurrentTrain: epoch  8, batch    18 | loss: 4.9071908Losses:  4.053106307983398 0.11993527412414551 0.7769564390182495
CurrentTrain: epoch  8, batch    19 | loss: 4.9499979Losses:  4.122662544250488 0.14343997836112976 0.8237898349761963
CurrentTrain: epoch  8, batch    20 | loss: 5.0898924Losses:  4.054655075073242 0.07098692655563354 0.7494267225265503
CurrentTrain: epoch  8, batch    21 | loss: 4.8750687Losses:  4.040900707244873 0.10869680345058441 0.8510414361953735
CurrentTrain: epoch  8, batch    22 | loss: 5.0006390Losses:  4.1437602043151855 0.11356464773416519 0.7564719915390015
CurrentTrain: epoch  8, batch    23 | loss: 5.0137968Losses:  4.041685581207275 0.14293839037418365 0.7489486932754517
CurrentTrain: epoch  8, batch    24 | loss: 4.9335728Losses:  4.037437438964844 0.1415870636701584 0.7023568749427795
CurrentTrain: epoch  8, batch    25 | loss: 4.8813815Losses:  4.057770729064941 0.11875800043344498 0.786817193031311
CurrentTrain: epoch  8, batch    26 | loss: 4.9633460Losses:  4.065782070159912 0.10797387361526489 0.7258251309394836
CurrentTrain: epoch  8, batch    27 | loss: 4.8995814Losses:  4.067340850830078 0.12574690580368042 0.6940357685089111
CurrentTrain: epoch  8, batch    28 | loss: 4.8871231Losses:  4.049376487731934 0.12341922521591187 0.78078693151474
CurrentTrain: epoch  8, batch    29 | loss: 4.9535828Losses:  4.041762828826904 0.11497635394334793 0.7471388578414917
CurrentTrain: epoch  8, batch    30 | loss: 4.9038782Losses:  4.021670341491699 0.10110479593276978 0.72658371925354
CurrentTrain: epoch  8, batch    31 | loss: 4.8493586Losses:  4.050732135772705 0.0903894305229187 0.7467026710510254
CurrentTrain: epoch  8, batch    32 | loss: 4.8878241Losses:  3.9204912185668945 0.1047578901052475 0.6984161138534546
CurrentTrain: epoch  8, batch    33 | loss: 4.7236652Losses:  4.057080268859863 0.10060355812311172 0.7467060685157776
CurrentTrain: epoch  8, batch    34 | loss: 4.9043899Losses:  4.00698184967041 0.11660068482160568 0.7242118120193481
CurrentTrain: epoch  8, batch    35 | loss: 4.8477941Losses:  3.999981164932251 0.12406685948371887 0.7090849876403809
CurrentTrain: epoch  8, batch    36 | loss: 4.8331332Losses:  4.01688814163208 0.11880405247211456 0.7530374526977539
CurrentTrain: epoch  8, batch    37 | loss: 4.8887296Losses:  4.039445877075195 0.1319887936115265 0.7859872579574585
CurrentTrain: epoch  8, batch    38 | loss: 4.9574223Losses:  4.028271675109863 0.10219910740852356 0.7726105451583862
CurrentTrain: epoch  8, batch    39 | loss: 4.9030814Losses:  4.063514232635498 0.0797475129365921 0.7723214030265808
CurrentTrain: epoch  8, batch    40 | loss: 4.9155831Losses:  4.002898216247559 0.0922601968050003 0.7648214101791382
CurrentTrain: epoch  8, batch    41 | loss: 4.8599801Losses:  4.027055740356445 0.08278244733810425 0.7483260631561279
CurrentTrain: epoch  8, batch    42 | loss: 4.8581638Losses:  4.0330352783203125 0.0845581442117691 0.697698712348938
CurrentTrain: epoch  8, batch    43 | loss: 4.8152919Losses:  4.071455001831055 0.11710883677005768 0.7541213035583496
CurrentTrain: epoch  8, batch    44 | loss: 4.9426851Losses:  4.173325538635254 0.16284282505512238 0.7658849954605103
CurrentTrain: epoch  8, batch    45 | loss: 5.1020532Losses:  4.064762115478516 0.12803803384304047 0.731650710105896
CurrentTrain: epoch  8, batch    46 | loss: 4.9244509Losses:  4.076854705810547 0.10848531126976013 0.7688562870025635
CurrentTrain: epoch  8, batch    47 | loss: 4.9541960Losses:  4.064215660095215 0.08242341876029968 0.7403256893157959
CurrentTrain: epoch  8, batch    48 | loss: 4.8869648Losses:  3.978097438812256 0.08633904159069061 0.8420902490615845
CurrentTrain: epoch  8, batch    49 | loss: 4.9065266Losses:  4.023163795471191 0.10186189413070679 0.7757408022880554
CurrentTrain: epoch  8, batch    50 | loss: 4.9007664Losses:  4.014899253845215 0.09501747786998749 0.8260078430175781
CurrentTrain: epoch  8, batch    51 | loss: 4.9359245Losses:  4.087931156158447 0.11323754489421844 0.8511656522750854
CurrentTrain: epoch  8, batch    52 | loss: 5.0523343Losses:  4.009890556335449 0.1122603639960289 0.7888660430908203
CurrentTrain: epoch  8, batch    53 | loss: 4.9110169Losses:  4.05648136138916 0.09610260277986526 0.7604324817657471
CurrentTrain: epoch  8, batch    54 | loss: 4.9130163Losses:  4.025603294372559 0.10448525846004486 0.7428662180900574
CurrentTrain: epoch  8, batch    55 | loss: 4.8729544Losses:  4.085532188415527 0.10656049847602844 0.6819052696228027
CurrentTrain: epoch  8, batch    56 | loss: 4.8739982Losses:  4.040657997131348 0.10446900874376297 0.749992847442627
CurrentTrain: epoch  8, batch    57 | loss: 4.8951197Losses:  4.011693000793457 0.14256590604782104 0.7493971586227417
CurrentTrain: epoch  8, batch    58 | loss: 4.9036560Losses:  4.0030670166015625 0.1343163549900055 0.7161291837692261
CurrentTrain: epoch  8, batch    59 | loss: 4.8535128Losses:  4.043831825256348 0.1066865399479866 0.7656160593032837
CurrentTrain: epoch  8, batch    60 | loss: 4.9161344Losses:  4.042574882507324 0.12372394651174545 0.7524709105491638
CurrentTrain: epoch  8, batch    61 | loss: 4.9187698Losses:  4.031939506530762 0.07046246528625488 0.7156201004981995
CurrentTrain: epoch  8, batch    62 | loss: 4.8180218Losses:  4.024440288543701 0.11611302196979523 0.7582193613052368
CurrentTrain: epoch  9, batch     0 | loss: 4.8987727Losses:  3.9989113807678223 0.12431551516056061 0.7314558029174805
CurrentTrain: epoch  9, batch     1 | loss: 4.8546829Losses:  4.010041236877441 0.0843532606959343 0.7122170925140381
CurrentTrain: epoch  9, batch     2 | loss: 4.8066120Losses:  4.031643867492676 0.1272495687007904 0.7527322769165039
CurrentTrain: epoch  9, batch     3 | loss: 4.9116259Losses:  4.056434631347656 0.1042894572019577 0.749957263469696
CurrentTrain: epoch  9, batch     4 | loss: 4.9106812Losses:  4.021895408630371 0.10426098108291626 0.709351658821106
CurrentTrain: epoch  9, batch     5 | loss: 4.8355079Losses:  4.031033515930176 0.13220900297164917 0.6979925632476807
CurrentTrain: epoch  9, batch     6 | loss: 4.8612347Losses:  4.080863952636719 0.07915498316287994 0.7541141510009766
CurrentTrain: epoch  9, batch     7 | loss: 4.9141331Losses:  3.928089141845703 0.10095075517892838 0.7847204804420471
CurrentTrain: epoch  9, batch     8 | loss: 4.8137603Losses:  4.037921905517578 0.07270467281341553 0.7683534026145935
CurrentTrain: epoch  9, batch     9 | loss: 4.8789802Losses:  4.04900598526001 0.09741005301475525 0.7961471080780029
CurrentTrain: epoch  9, batch    10 | loss: 4.9425631Losses:  4.05277156829834 0.10362569987773895 0.721484899520874
CurrentTrain: epoch  9, batch    11 | loss: 4.8778820Losses:  4.063385963439941 0.09307998418807983 0.8330833315849304
CurrentTrain: epoch  9, batch    12 | loss: 4.9895492Losses:  4.013562202453613 0.1190740242600441 0.7512145638465881
CurrentTrain: epoch  9, batch    13 | loss: 4.8838506Losses:  3.9918007850646973 0.12224374711513519 0.7234007120132446
CurrentTrain: epoch  9, batch    14 | loss: 4.8374453Losses:  4.055589199066162 0.08204665035009384 0.7467726469039917
CurrentTrain: epoch  9, batch    15 | loss: 4.8844085Losses:  3.9908108711242676 0.10915207862854004 0.7359575629234314
CurrentTrain: epoch  9, batch    16 | loss: 4.8359208Losses:  4.00443696975708 0.121515192091465 0.8119684457778931
CurrentTrain: epoch  9, batch    17 | loss: 4.9379206Losses:  4.008192539215088 0.10707981884479523 0.7509082555770874
CurrentTrain: epoch  9, batch    18 | loss: 4.8661809Losses:  4.045753002166748 0.08781899511814117 0.6862318515777588
CurrentTrain: epoch  9, batch    19 | loss: 4.8198042Losses:  3.9931182861328125 0.08826430141925812 0.73651522397995
CurrentTrain: epoch  9, batch    20 | loss: 4.8178978Losses:  4.036907196044922 0.10013067722320557 0.7732771635055542
CurrentTrain: epoch  9, batch    21 | loss: 4.9103150Losses:  4.069421291351318 0.07983461767435074 0.8219533562660217
CurrentTrain: epoch  9, batch    22 | loss: 4.9712090Losses:  4.03352689743042 0.10897029936313629 0.7530970573425293
CurrentTrain: epoch  9, batch    23 | loss: 4.8955941Losses:  3.9591000080108643 0.1128297746181488 0.7115776538848877
CurrentTrain: epoch  9, batch    24 | loss: 4.7835073Losses:  4.016741752624512 0.12390346825122833 0.7613106369972229
CurrentTrain: epoch  9, batch    25 | loss: 4.9019556Losses:  3.9632506370544434 0.11240793764591217 0.7869757413864136
CurrentTrain: epoch  9, batch    26 | loss: 4.8626347Losses:  4.031556129455566 0.11957439035177231 0.7549944519996643
CurrentTrain: epoch  9, batch    27 | loss: 4.9061251Losses:  4.004133701324463 0.10930369049310684 0.7259006500244141
CurrentTrain: epoch  9, batch    28 | loss: 4.8393378Losses:  4.033692359924316 0.11533613502979279 0.7525556087493896
CurrentTrain: epoch  9, batch    29 | loss: 4.9015837Losses:  4.032217025756836 0.1058722510933876 0.8121262192726135
CurrentTrain: epoch  9, batch    30 | loss: 4.9502153Losses:  4.023390769958496 0.13777737319469452 0.7496800422668457
CurrentTrain: epoch  9, batch    31 | loss: 4.9108481Losses:  3.9938912391662598 0.10144548863172531 0.7786778211593628
CurrentTrain: epoch  9, batch    32 | loss: 4.8740149Losses:  4.025576591491699 0.06703376024961472 0.7531930208206177
CurrentTrain: epoch  9, batch    33 | loss: 4.8458033Losses:  4.00199031829834 0.1055457592010498 0.7003121376037598
CurrentTrain: epoch  9, batch    34 | loss: 4.8078485Losses:  4.000931262969971 0.07398232072591782 0.7546685934066772
CurrentTrain: epoch  9, batch    35 | loss: 4.8295822Losses:  4.0407209396362305 0.08529792726039886 0.8078198432922363
CurrentTrain: epoch  9, batch    36 | loss: 4.9338388Losses:  3.9881582260131836 0.09320823103189468 0.7194200754165649
CurrentTrain: epoch  9, batch    37 | loss: 4.8007865Losses:  4.070033073425293 0.10605734586715698 0.6911734342575073
CurrentTrain: epoch  9, batch    38 | loss: 4.8672638Losses:  4.005748271942139 0.10845832526683807 0.6983072757720947
CurrentTrain: epoch  9, batch    39 | loss: 4.8125143Losses:  4.057098865509033 0.08423551917076111 0.7236789464950562
CurrentTrain: epoch  9, batch    40 | loss: 4.8650136Losses:  4.002875328063965 0.0834323838353157 0.8479462265968323
CurrentTrain: epoch  9, batch    41 | loss: 4.9342537Losses:  4.017293930053711 0.12145350873470306 0.7022156715393066
CurrentTrain: epoch  9, batch    42 | loss: 4.8409629Losses:  3.9667744636535645 0.07820657640695572 0.7503924369812012
CurrentTrain: epoch  9, batch    43 | loss: 4.7953734Losses:  3.971041679382324 0.10769076645374298 0.6595370769500732
CurrentTrain: epoch  9, batch    44 | loss: 4.7382698Losses:  4.0299577713012695 0.06034945696592331 0.7056306600570679
CurrentTrain: epoch  9, batch    45 | loss: 4.7959380Losses:  4.026100158691406 0.08152119815349579 0.8381588459014893
CurrentTrain: epoch  9, batch    46 | loss: 4.9457798Losses:  4.003703594207764 0.10606971383094788 0.7259031534194946
CurrentTrain: epoch  9, batch    47 | loss: 4.8356762Losses:  4.018003463745117 0.1277826428413391 0.7042722105979919
CurrentTrain: epoch  9, batch    48 | loss: 4.8500586Losses:  3.965256929397583 0.11079910397529602 0.7306942343711853
CurrentTrain: epoch  9, batch    49 | loss: 4.8067503Losses:  4.042211055755615 0.08097144961357117 0.7334542274475098
CurrentTrain: epoch  9, batch    50 | loss: 4.8566365Losses:  4.0035176277160645 0.10921289771795273 0.7503948211669922
CurrentTrain: epoch  9, batch    51 | loss: 4.8631253Losses:  4.013181686401367 0.10911495983600616 0.7430481910705566
CurrentTrain: epoch  9, batch    52 | loss: 4.8653450Losses:  3.968092441558838 0.06798911094665527 0.7323557138442993
CurrentTrain: epoch  9, batch    53 | loss: 4.7684369Losses:  4.002285957336426 0.0956106036901474 0.7258309125900269
CurrentTrain: epoch  9, batch    54 | loss: 4.8237276Losses:  4.061539649963379 0.08334492146968842 0.7956987023353577
CurrentTrain: epoch  9, batch    55 | loss: 4.9405832Losses:  3.9745564460754395 0.07274716347455978 0.6894249320030212
CurrentTrain: epoch  9, batch    56 | loss: 4.7367287Losses:  4.006848335266113 0.08120858669281006 0.7041197419166565
CurrentTrain: epoch  9, batch    57 | loss: 4.7921767Losses:  3.994812250137329 0.07831236720085144 0.7059874534606934
CurrentTrain: epoch  9, batch    58 | loss: 4.7791119Losses:  3.9742941856384277 0.0981099084019661 0.6896714568138123
CurrentTrain: epoch  9, batch    59 | loss: 4.7620754Losses:  3.989532470703125 0.10373436659574509 0.733468770980835
CurrentTrain: epoch  9, batch    60 | loss: 4.8267355Losses:  4.009572982788086 0.09795339405536652 0.7933664321899414
CurrentTrain: epoch  9, batch    61 | loss: 4.9008927Losses:  3.9930832386016846 0.04953295737504959 0.7517670392990112
CurrentTrain: epoch  9, batch    62 | loss: 4.7943835
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.44%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.23%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.44%   
cur_acc:  ['0.9444']
his_acc:  ['0.9444']
Clustering into  9  clusters
Clusters:  [2 7 8 1 2 2 2 2 0 5 1 6 2 2 2 0 4 2 2 3]
Losses:  5.406184196472168 1.5579142570495605 0.9804866909980774
CurrentTrain: epoch  0, batch     0 | loss: 7.9445853Losses:  6.969703674316406 1.664930820465088 0.9957644939422607
CurrentTrain: epoch  0, batch     1 | loss: 9.6303988Losses:  7.339301586151123 1.9308959245681763 0.9874539375305176
CurrentTrain: epoch  0, batch     2 | loss: 10.2576523Losses:  3.0829274654388428 0.4410746395587921 0.9444230794906616
CurrentTrain: epoch  0, batch     3 | loss: 4.4684253Losses:  5.6827168464660645 1.8160431385040283 0.9647049903869629
CurrentTrain: epoch  1, batch     0 | loss: 8.4634647Losses:  5.692949295043945 1.6190946102142334 0.9942229986190796
CurrentTrain: epoch  1, batch     1 | loss: 8.3062668Losses:  5.069738388061523 1.239145040512085 0.9890350699424744
CurrentTrain: epoch  1, batch     2 | loss: 7.2979188Losses:  4.5717315673828125 0.26095154881477356 0.9318076372146606
CurrentTrain: epoch  1, batch     3 | loss: 5.7644906Losses:  4.422571182250977 1.3517341613769531 0.9652280211448669
CurrentTrain: epoch  2, batch     0 | loss: 6.7395334Losses:  4.5702972412109375 1.6101016998291016 0.9858595728874207
CurrentTrain: epoch  2, batch     1 | loss: 7.1662583Losses:  3.0713605880737305 1.4543070793151855 0.9838395118713379
CurrentTrain: epoch  2, batch     2 | loss: 5.5095072Losses:  4.525270462036133 0.8656101226806641 0.9262410402297974
CurrentTrain: epoch  2, batch     3 | loss: 6.3171215Losses:  3.6622066497802734 1.2831668853759766 0.9590516090393066
CurrentTrain: epoch  3, batch     0 | loss: 5.9044251Losses:  4.345202445983887 1.51152765750885 0.9582618474960327
CurrentTrain: epoch  3, batch     1 | loss: 6.8149920Losses:  3.0098016262054443 1.2564818859100342 0.9700782299041748
CurrentTrain: epoch  3, batch     2 | loss: 5.2363615Losses:  3.7579588890075684 0.4139513373374939 1.001280665397644
CurrentTrain: epoch  3, batch     3 | loss: 5.1731911Losses:  3.84959077835083 1.6214582920074463 0.9563699960708618
CurrentTrain: epoch  4, batch     0 | loss: 6.4274192Losses:  3.1340224742889404 1.1395947933197021 0.9700819849967957
CurrentTrain: epoch  4, batch     1 | loss: 5.2436991Losses:  3.5809669494628906 1.1938234567642212 0.9519726634025574
CurrentTrain: epoch  4, batch     2 | loss: 5.7267628Losses:  3.258777618408203 0.39898914098739624 0.9917541146278381
CurrentTrain: epoch  4, batch     3 | loss: 4.6495209Losses:  3.41502046585083 1.0234347581863403 0.9610105752944946
CurrentTrain: epoch  5, batch     0 | loss: 5.3994656Losses:  2.972763776779175 1.3173763751983643 0.9582184553146362
CurrentTrain: epoch  5, batch     1 | loss: 5.2483587Losses:  3.306306838989258 1.2157407999038696 0.9473432302474976
CurrentTrain: epoch  5, batch     2 | loss: 5.4693909Losses:  4.172179222106934 0.33230695128440857 0.935273289680481
CurrentTrain: epoch  5, batch     3 | loss: 5.4397593Losses:  3.3887696266174316 1.35136079788208 0.9521676301956177
CurrentTrain: epoch  6, batch     0 | loss: 5.6922979Losses:  2.6980948448181152 1.275665283203125 0.9684398174285889
CurrentTrain: epoch  6, batch     1 | loss: 4.9421997Losses:  3.1782658100128174 0.9347786903381348 0.9381283521652222
CurrentTrain: epoch  6, batch     2 | loss: 5.0511732Losses:  5.134810924530029 0.27380168437957764 0.854303240776062
CurrentTrain: epoch  6, batch     3 | loss: 6.2629161Losses:  3.332946300506592 1.291568398475647 0.9399692416191101
CurrentTrain: epoch  7, batch     0 | loss: 5.5644836Losses:  2.5150749683380127 1.114187479019165 0.9598675966262817
CurrentTrain: epoch  7, batch     1 | loss: 4.5891299Losses:  3.2769274711608887 1.2811535596847534 0.9486516118049622
CurrentTrain: epoch  7, batch     2 | loss: 5.5067329Losses:  1.7852798700332642 0.1325186789035797 0.9042596817016602
CurrentTrain: epoch  7, batch     3 | loss: 2.8220582Losses:  2.9337944984436035 0.9858389496803284 0.94223952293396
CurrentTrain: epoch  8, batch     0 | loss: 4.8618727Losses:  2.6422743797302246 1.028775930404663 0.9380204677581787
CurrentTrain: epoch  8, batch     1 | loss: 4.6090708Losses:  2.5457546710968018 1.0407004356384277 0.9512560963630676
CurrentTrain: epoch  8, batch     2 | loss: 4.5377111Losses:  5.18556547164917 0.1478540599346161 0.9477756023406982
CurrentTrain: epoch  8, batch     3 | loss: 6.2811947Losses:  2.9721405506134033 0.9182750582695007 0.9575808048248291
CurrentTrain: epoch  9, batch     0 | loss: 4.8479967Losses:  2.500880718231201 0.9786459803581238 0.9214552640914917
CurrentTrain: epoch  9, batch     1 | loss: 4.4009819Losses:  2.5347461700439453 0.7325106859207153 0.9407998919487
CurrentTrain: epoch  9, batch     2 | loss: 4.2080564Losses:  1.678788661956787 0.23776161670684814 0.966680645942688
CurrentTrain: epoch  9, batch     3 | loss: 2.8832309
Losses:  1.9056334495544434 1.0225756168365479 0.8915771245956421
MemoryTrain:  epoch  0, batch     0 | loss: 3.8197861Losses:  2.5754380226135254 0.14327755570411682 0.8228018879890442
MemoryTrain:  epoch  0, batch     1 | loss: 3.5415175Losses:  1.8996789455413818 1.0390194654464722 0.8758956789970398
MemoryTrain:  epoch  1, batch     0 | loss: 3.8145940Losses:  1.7156028747558594 0.08302955329418182 0.8820955753326416
MemoryTrain:  epoch  1, batch     1 | loss: 2.6807280Losses:  1.0390286445617676 0.9426009058952332 0.8649272918701172
MemoryTrain:  epoch  2, batch     0 | loss: 2.8465569Losses:  0.8201866149902344 0.2866780757904053 0.911953866481781
MemoryTrain:  epoch  2, batch     1 | loss: 2.0188186Losses:  0.40527796745300293 0.8366971015930176 0.8916828632354736
MemoryTrain:  epoch  3, batch     0 | loss: 2.1336579Losses:  0.45988625288009644 0.2243143916130066 0.8107144832611084
MemoryTrain:  epoch  3, batch     1 | loss: 1.4949151Losses:  0.23039031028747559 0.9966500401496887 0.8909127712249756
MemoryTrain:  epoch  4, batch     0 | loss: 2.1179531Losses:  0.034117430448532104 0.04128066077828407 0.8093769550323486
MemoryTrain:  epoch  4, batch     1 | loss: 0.8847750Losses:  0.03680457919836044 0.7922298908233643 0.9003886580467224
MemoryTrain:  epoch  5, batch     0 | loss: 1.7294230Losses:  0.4776662588119507 0.502004861831665 0.7744912505149841
MemoryTrain:  epoch  5, batch     1 | loss: 1.7541623Losses:  0.10921607166528702 0.8175455927848816 0.8824039697647095
MemoryTrain:  epoch  6, batch     0 | loss: 1.8091657Losses:  0.02715504914522171 0.3833540380001068 0.8423541784286499
MemoryTrain:  epoch  6, batch     1 | loss: 1.2528633Losses:  0.04533278942108154 0.8208410739898682 0.8536368012428284
MemoryTrain:  epoch  7, batch     0 | loss: 1.7198107Losses:  0.021007901057600975 0.20564791560173035 0.9679814577102661
MemoryTrain:  epoch  7, batch     1 | loss: 1.1946373Losses:  0.028671694919466972 0.7135148048400879 0.858513593673706
MemoryTrain:  epoch  8, batch     0 | loss: 1.6007001Losses:  0.14026060700416565 0.3092251718044281 0.9383227825164795
MemoryTrain:  epoch  8, batch     1 | loss: 1.3878086Losses:  0.03573574870824814 0.7154921293258667 0.8693467378616333
MemoryTrain:  epoch  9, batch     0 | loss: 1.6205746Losses:  0.3087324798107147 0.22504350543022156 0.9181541204452515
MemoryTrain:  epoch  9, batch     1 | loss: 1.4519300
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 84.54%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 75.19%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 74.16%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 81.90%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 81.89%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 82.36%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 81.65%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.94%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 90.76%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.69%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.05%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.34%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 91.27%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.42%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.53%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 91.44%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.38%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 91.49%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.64%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 91.75%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.87%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.89%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.92%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 91.53%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 91.43%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 90.90%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.55%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 90.20%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 89.48%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 88.63%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 87.80%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 86.91%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 86.26%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 85.42%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 85.16%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.32%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.95%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 85.92%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 85.87%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.63%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 85.40%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 85.00%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 85.02%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 85.11%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 85.40%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 85.54%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 86.01%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 86.26%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 86.92%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 86.92%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 86.89%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 86.94%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 87.00%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 86.95%   
cur_acc:  ['0.9444', '0.8165']
his_acc:  ['0.9444', '0.8695']
Clustering into  14  clusters
Clusters:  [ 0  7  9  0  0  0 11  0 10 12  8  3  0  0  5 13  4  0  0  6  2  0  0  0
  0  1  0  0  0  0]
Losses:  6.380220890045166 1.7381467819213867 0.9633380174636841
CurrentTrain: epoch  0, batch     0 | loss: 9.0817060Losses:  6.758172988891602 1.7771693468093872 0.9552547931671143
CurrentTrain: epoch  0, batch     1 | loss: 9.4905968Losses:  6.5075531005859375 1.9818589687347412 0.9940111637115479
CurrentTrain: epoch  0, batch     2 | loss: 9.4834232Losses:  6.1967878341674805 0.4358732998371124 1.0
CurrentTrain: epoch  0, batch     3 | loss: 7.6326613Losses:  6.210775375366211 1.8267247676849365 0.968604326248169
CurrentTrain: epoch  1, batch     0 | loss: 9.0061045Losses:  5.598268508911133 1.9101648330688477 0.9650195837020874
CurrentTrain: epoch  1, batch     1 | loss: 8.4734526Losses:  5.462823867797852 1.9437472820281982 0.9709576368331909
CurrentTrain: epoch  1, batch     2 | loss: 8.3775291Losses:  5.219664573669434 0.24977275729179382 0.9712375402450562
CurrentTrain: epoch  1, batch     3 | loss: 6.4406748Losses:  5.292241096496582 1.556328296661377 0.9429104328155518
CurrentTrain: epoch  2, batch     0 | loss: 7.7914801Losses:  5.214963912963867 1.9264943599700928 0.9654552340507507
CurrentTrain: epoch  2, batch     1 | loss: 8.1069136Losses:  4.476861953735352 1.6518157720565796 0.9704990386962891
CurrentTrain: epoch  2, batch     2 | loss: 7.0991769Losses:  5.319087028503418 0.6367616653442383 1.0109400749206543
CurrentTrain: epoch  2, batch     3 | loss: 6.9667888Losses:  4.6527252197265625 1.4083962440490723 0.942756712436676
CurrentTrain: epoch  3, batch     0 | loss: 7.0038781Losses:  5.077520847320557 1.6629211902618408 0.973114013671875
CurrentTrain: epoch  3, batch     1 | loss: 7.7135563Losses:  4.200832366943359 1.7297711372375488 0.9561349153518677
CurrentTrain: epoch  3, batch     2 | loss: 6.8867383Losses:  5.615731239318848 0.581246018409729 0.9850029945373535
CurrentTrain: epoch  3, batch     3 | loss: 7.1819801Losses:  5.1543169021606445 1.6076867580413818 0.9457526206970215
CurrentTrain: epoch  4, batch     0 | loss: 7.7077565Losses:  4.103184223175049 1.6991347074508667 0.9622836112976074
CurrentTrain: epoch  4, batch     1 | loss: 6.7646027Losses:  3.9942526817321777 1.7447633743286133 0.9566471576690674
CurrentTrain: epoch  4, batch     2 | loss: 6.6956635Losses:  3.3789024353027344 0.43789374828338623 0.946092963218689
CurrentTrain: epoch  4, batch     3 | loss: 4.7628894Losses:  4.503109931945801 1.6544578075408936 0.9537900686264038
CurrentTrain: epoch  5, batch     0 | loss: 7.1113582Losses:  3.745145082473755 1.5101550817489624 0.9522566795349121
CurrentTrain: epoch  5, batch     1 | loss: 6.2075567Losses:  4.351792335510254 1.5871630907058716 0.9518417119979858
CurrentTrain: epoch  5, batch     2 | loss: 6.8907971Losses:  2.822873830795288 0.2902086675167084 0.9762890338897705
CurrentTrain: epoch  5, batch     3 | loss: 4.0893717Losses:  4.144201278686523 1.6599063873291016 0.9464271664619446
CurrentTrain: epoch  6, batch     0 | loss: 6.7505350Losses:  3.664134979248047 1.4278208017349243 0.9334691762924194
CurrentTrain: epoch  6, batch     1 | loss: 6.0254250Losses:  4.1084089279174805 1.473737120628357 0.9580793380737305
CurrentTrain: epoch  6, batch     2 | loss: 6.5402255Losses:  3.4788639545440674 0.39431193470954895 0.9600794315338135
CurrentTrain: epoch  6, batch     3 | loss: 4.8332553Losses:  3.552325963973999 1.5010802745819092 0.9379165172576904
CurrentTrain: epoch  7, batch     0 | loss: 5.9913225Losses:  3.664201498031616 1.3566155433654785 0.9366406202316284
CurrentTrain: epoch  7, batch     1 | loss: 5.9574575Losses:  3.3724303245544434 1.4904413223266602 0.9497900009155273
CurrentTrain: epoch  7, batch     2 | loss: 5.8126616Losses:  5.459212303161621 0.38801342248916626 0.961368203163147
CurrentTrain: epoch  7, batch     3 | loss: 6.8085938Losses:  3.5446953773498535 1.5911033153533936 0.9495141506195068
CurrentTrain: epoch  8, batch     0 | loss: 6.0853128Losses:  3.7068541049957275 1.2460169792175293 0.9395931959152222
CurrentTrain: epoch  8, batch     1 | loss: 5.8924646Losses:  3.3612778186798096 1.323523759841919 0.9282441139221191
CurrentTrain: epoch  8, batch     2 | loss: 5.6130457Losses:  1.8038345575332642 0.3674366772174835 0.9323279857635498
CurrentTrain: epoch  8, batch     3 | loss: 3.1035993Losses:  2.9623525142669678 1.3725184202194214 0.9315949082374573
CurrentTrain: epoch  9, batch     0 | loss: 5.2664657Losses:  3.6112661361694336 1.5544886589050293 0.9420686960220337
CurrentTrain: epoch  9, batch     1 | loss: 6.1078234Losses:  3.286965847015381 1.3757655620574951 0.9319288730621338
CurrentTrain: epoch  9, batch     2 | loss: 5.5946598Losses:  2.6274328231811523 0.20642375946044922 0.9666272401809692
CurrentTrain: epoch  9, batch     3 | loss: 3.8004837
Losses:  0.7946381568908691 1.463625192642212 0.903559684753418
MemoryTrain:  epoch  0, batch     0 | loss: 3.1618230Losses:  0.23969189822673798 0.9416139125823975 0.9191893339157104
MemoryTrain:  epoch  0, batch     1 | loss: 2.1004951Losses:  0.9625986218452454 1.1794981956481934 0.9118994474411011
MemoryTrain:  epoch  1, batch     0 | loss: 3.0539961Losses:  0.5495514869689941 1.0207586288452148 0.9083647727966309
MemoryTrain:  epoch  1, batch     1 | loss: 2.4786749Losses:  0.45417845249176025 1.020516037940979 0.8996562361717224
MemoryTrain:  epoch  2, batch     0 | loss: 2.3743508Losses:  0.3020974099636078 1.1534465551376343 0.9312045574188232
MemoryTrain:  epoch  2, batch     1 | loss: 2.3867486Losses:  0.13453270494937897 1.1965566873550415 0.9212672710418701
MemoryTrain:  epoch  3, batch     0 | loss: 2.2523565Losses:  0.19823329150676727 0.8781035542488098 0.8945401906967163
MemoryTrain:  epoch  3, batch     1 | loss: 1.9708771Losses:  0.10541942715644836 1.0340827703475952 0.9008948802947998
MemoryTrain:  epoch  4, batch     0 | loss: 2.0403972Losses:  0.06776656955480576 1.0701560974121094 0.9163365364074707
MemoryTrain:  epoch  4, batch     1 | loss: 2.0542593Losses:  0.07029423117637634 1.1779577732086182 0.9152389764785767
MemoryTrain:  epoch  5, batch     0 | loss: 2.1634910Losses:  0.028446856886148453 0.8060352206230164 0.8929412364959717
MemoryTrain:  epoch  5, batch     1 | loss: 1.7274233Losses:  0.039537325501441956 0.9959522485733032 0.9057517051696777
MemoryTrain:  epoch  6, batch     0 | loss: 1.9412413Losses:  0.03459923714399338 0.8789849281311035 0.9011741280555725
MemoryTrain:  epoch  6, batch     1 | loss: 1.8147583Losses:  0.027409009635448456 0.7926282286643982 0.8787520527839661
MemoryTrain:  epoch  7, batch     0 | loss: 1.6987894Losses:  0.046934522688388824 1.1257350444793701 0.9266085624694824
MemoryTrain:  epoch  7, batch     1 | loss: 2.0992780Losses:  0.022751400247216225 0.8996113538742065 0.8845317959785461
MemoryTrain:  epoch  8, batch     0 | loss: 1.8068945Losses:  0.021924706175923347 0.9309910535812378 0.9173116683959961
MemoryTrain:  epoch  8, batch     1 | loss: 1.8702275Losses:  0.02867160364985466 1.1287472248077393 0.8804494142532349
MemoryTrain:  epoch  9, batch     0 | loss: 2.0378683Losses:  0.02034406177699566 0.6857665777206421 0.9185006618499756
MemoryTrain:  epoch  9, batch     1 | loss: 1.6246114
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 58.96%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 57.06%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 57.23%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 59.01%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 60.18%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 60.76%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 61.32%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 61.68%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 64.89%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 65.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 63.79%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 63.24%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 62.81%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 61.90%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 89.68%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.91%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.76%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.57%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 90.35%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.19%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 90.31%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.06%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 90.02%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 90.17%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 90.13%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 89.91%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 89.88%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.81%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 89.53%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 89.42%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 88.47%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 88.14%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 87.34%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 87.11%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 86.65%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 86.05%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 85.24%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 84.38%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 83.53%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 82.85%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 82.04%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 81.82%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.22%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.80%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 82.91%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 82.76%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 82.33%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 82.20%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 81.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 82.84%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 82.94%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 83.35%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 83.74%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 83.86%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 84.32%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 84.25%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 84.53%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 84.55%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 84.03%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 83.61%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 83.20%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 82.80%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 82.55%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 82.11%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 82.05%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 82.10%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 82.44%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 82.44%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 82.47%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 82.51%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 82.78%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 82.77%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 82.58%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 82.53%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 82.47%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 82.26%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 82.17%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 81.66%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 81.13%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 80.60%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 80.07%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 79.60%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 79.09%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 79.23%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 79.24%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 79.22%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 79.27%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 79.44%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 79.41%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 79.28%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 79.18%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 79.15%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 79.00%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 78.76%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 78.54%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 78.49%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 78.30%   [EVAL] batch:  182 | acc: 18.75%,  total acc: 77.97%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 77.72%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 77.42%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 77.31%   [EVAL] batch:  187 | acc: 12.50%,  total acc: 76.96%   
cur_acc:  ['0.9444', '0.8165', '0.6190']
his_acc:  ['0.9444', '0.8695', '0.7696']
Clustering into  19  clusters
Clusters:  [ 0  3 12  1  0  0 16  0 13 11 10 17  1  0 14  6 15  0  0 18  7  1  0  0
  0  9  0  0  0  0  3  8  1  1  0  5  0  0  4  2]
Losses:  5.460136890411377 1.02728271484375 1.0224523544311523
CurrentTrain: epoch  0, batch     0 | loss: 7.5098720Losses:  6.824940204620361 1.5069739818572998 0.9660904407501221
CurrentTrain: epoch  0, batch     1 | loss: 9.2980042Losses:  6.5436201095581055 1.6312702894210815 0.976556122303009
CurrentTrain: epoch  0, batch     2 | loss: 9.1514463Losses:  6.43980073928833 0.45036885142326355 1.0280697345733643
CurrentTrain: epoch  0, batch     3 | loss: 7.9182396Losses:  5.072370529174805 1.5889359712600708 1.0082573890686035
CurrentTrain: epoch  1, batch     0 | loss: 7.6695638Losses:  6.051459789276123 1.439276933670044 0.9860837459564209
CurrentTrain: epoch  1, batch     1 | loss: 8.4768209Losses:  4.467630386352539 1.3104294538497925 0.9850362539291382
CurrentTrain: epoch  1, batch     2 | loss: 6.7630963Losses:  2.247011661529541 0.15067850053310394 0.9336287975311279
CurrentTrain: epoch  1, batch     3 | loss: 3.3313189Losses:  3.7887043952941895 1.209535837173462 0.980258584022522
CurrentTrain: epoch  2, batch     0 | loss: 5.9784989Losses:  4.72577428817749 1.0065720081329346 0.9910153150558472
CurrentTrain: epoch  2, batch     1 | loss: 6.7233620Losses:  4.686059951782227 1.3253536224365234 0.9949529767036438
CurrentTrain: epoch  2, batch     2 | loss: 7.0063667Losses:  3.4151082038879395 0.26872503757476807 0.9770612716674805
CurrentTrain: epoch  2, batch     3 | loss: 4.6608944Losses:  4.080607891082764 1.2151011228561401 1.0162707567214966
CurrentTrain: epoch  3, batch     0 | loss: 6.3119798Losses:  3.576056480407715 1.1675026416778564 0.9806697964668274
CurrentTrain: epoch  3, batch     1 | loss: 5.7242289Losses:  3.924962043762207 1.193739414215088 0.9662825465202332
CurrentTrain: epoch  3, batch     2 | loss: 6.0849838Losses:  5.9933929443359375 0.4806547164916992 1.013474464416504
CurrentTrain: epoch  3, batch     3 | loss: 7.4875221Losses:  3.47139835357666 1.0211076736450195 1.002820372581482
CurrentTrain: epoch  4, batch     0 | loss: 5.4953265Losses:  4.253684043884277 1.3329046964645386 0.9755901098251343
CurrentTrain: epoch  4, batch     1 | loss: 6.5621791Losses:  3.1460094451904297 1.2376974821090698 0.973181426525116
CurrentTrain: epoch  4, batch     2 | loss: 5.3568883Losses:  3.531261682510376 0.13096323609352112 1.038640022277832
CurrentTrain: epoch  4, batch     3 | loss: 4.7008648Losses:  2.8980233669281006 1.0979251861572266 0.9825533628463745
CurrentTrain: epoch  5, batch     0 | loss: 4.9785018Losses:  3.300971031188965 0.9573420286178589 1.005902886390686
CurrentTrain: epoch  5, batch     1 | loss: 5.2642159Losses:  3.9943108558654785 1.0569323301315308 0.9588664770126343
CurrentTrain: epoch  5, batch     2 | loss: 6.0101099Losses:  3.406491756439209 0.48892712593078613 0.9966399073600769
CurrentTrain: epoch  5, batch     3 | loss: 4.8920588Losses:  2.9212687015533447 1.07864248752594 0.9837305545806885
CurrentTrain: epoch  6, batch     0 | loss: 4.9836416Losses:  3.3189778327941895 1.118826150894165 0.9762601852416992
CurrentTrain: epoch  6, batch     1 | loss: 5.4140644Losses:  3.3069944381713867 1.2055931091308594 0.9823681712150574
CurrentTrain: epoch  6, batch     2 | loss: 5.4949555Losses:  4.182260036468506 0.44463980197906494 1.011946201324463
CurrentTrain: epoch  6, batch     3 | loss: 5.6388459Losses:  3.268449544906616 1.1146037578582764 0.9788870811462402
CurrentTrain: epoch  7, batch     0 | loss: 5.3619404Losses:  2.970855236053467 0.8031151294708252 1.0048046112060547
CurrentTrain: epoch  7, batch     1 | loss: 4.7787752Losses:  2.7141690254211426 1.0659971237182617 0.9612047672271729
CurrentTrain: epoch  7, batch     2 | loss: 4.7413712Losses:  2.872551918029785 0.3162141442298889 0.974289059638977
CurrentTrain: epoch  7, batch     3 | loss: 4.1630549Losses:  3.3284497261047363 1.0361592769622803 0.9766801595687866
CurrentTrain: epoch  8, batch     0 | loss: 5.3412890Losses:  2.665146827697754 0.6960196495056152 0.9990914463996887
CurrentTrain: epoch  8, batch     1 | loss: 4.3602581Losses:  2.2226574420928955 0.8327590227127075 0.9574863910675049
CurrentTrain: epoch  8, batch     2 | loss: 4.0129032Losses:  5.476241111755371 5.960465188081798e-08 1.0
CurrentTrain: epoch  8, batch     3 | loss: 6.4762411Losses:  2.63206148147583 0.9243595600128174 0.9873108267784119
CurrentTrain: epoch  9, batch     0 | loss: 4.5437317Losses:  2.7044239044189453 0.9876859784126282 0.9736428260803223
CurrentTrain: epoch  9, batch     1 | loss: 4.6657524Losses:  2.920670747756958 0.9230167865753174 0.9580652713775635
CurrentTrain: epoch  9, batch     2 | loss: 4.8017530Losses:  2.775129556655884 0.16154423356056213 0.9829424619674683
CurrentTrain: epoch  9, batch     3 | loss: 3.9196162
Losses:  0.8919278979301453 1.109186053276062 0.9119280576705933
MemoryTrain:  epoch  0, batch     0 | loss: 2.9130421Losses:  0.3331807851791382 0.8708661198616028 0.9156920909881592
MemoryTrain:  epoch  0, batch     1 | loss: 2.1197391Losses:  0.2079964578151703 0.7882225513458252 0.9751385450363159
MemoryTrain:  epoch  0, batch     2 | loss: 1.9713576Losses:  0.9012095332145691 1.0960283279418945 0.906608521938324
MemoryTrain:  epoch  1, batch     0 | loss: 2.9038465Losses:  1.0294363498687744 0.8535679578781128 0.9438608884811401
MemoryTrain:  epoch  1, batch     1 | loss: 2.8268652Losses:  0.20982493460178375 0.7997314929962158 0.9197068214416504
MemoryTrain:  epoch  1, batch     2 | loss: 1.9292632Losses:  0.5672174096107483 0.9128056764602661 0.942387580871582
MemoryTrain:  epoch  2, batch     0 | loss: 2.4224107Losses:  0.9840147495269775 1.0811634063720703 0.9017097353935242
MemoryTrain:  epoch  2, batch     1 | loss: 2.9668880Losses:  0.09761320799589157 0.6235650777816772 0.9360484480857849
MemoryTrain:  epoch  2, batch     2 | loss: 1.6572268Losses:  0.4554140865802765 0.8918067812919617 0.929162859916687
MemoryTrain:  epoch  3, batch     0 | loss: 2.2763839Losses:  0.45123088359832764 1.017396330833435 0.9334535598754883
MemoryTrain:  epoch  3, batch     1 | loss: 2.4020808Losses:  0.4425983726978302 0.6958744525909424 0.8892635703086853
MemoryTrain:  epoch  3, batch     2 | loss: 2.0277364Losses:  0.2272975593805313 1.0067007541656494 0.9179322719573975
MemoryTrain:  epoch  4, batch     0 | loss: 2.1519306Losses:  0.21205320954322815 0.8632584810256958 0.9263134002685547
MemoryTrain:  epoch  4, batch     1 | loss: 2.0016251Losses:  0.23885326087474823 0.7290465831756592 0.9208987355232239
MemoryTrain:  epoch  4, batch     2 | loss: 1.8887986Losses:  0.06915710121393204 0.8348557353019714 0.9217445850372314
MemoryTrain:  epoch  5, batch     0 | loss: 1.8257575Losses:  0.1381741613149643 1.0391387939453125 0.8993645310401917
MemoryTrain:  epoch  5, batch     1 | loss: 2.0766776Losses:  0.059148434549570084 0.6996418237686157 0.9541280269622803
MemoryTrain:  epoch  5, batch     2 | loss: 1.7129183Losses:  0.0765426903963089 1.0498533248901367 0.9237764477729797
MemoryTrain:  epoch  6, batch     0 | loss: 2.0501726Losses:  0.12535952031612396 1.0034981966018677 0.9127991199493408
MemoryTrain:  epoch  6, batch     1 | loss: 2.0416570Losses:  0.029066991060972214 0.46929803490638733 0.9233958125114441
MemoryTrain:  epoch  6, batch     2 | loss: 1.4217608Losses:  0.12631121277809143 1.0983881950378418 0.9446498155593872
MemoryTrain:  epoch  7, batch     0 | loss: 2.1693492Losses:  0.03233394771814346 0.5839236974716187 0.8842992782592773
MemoryTrain:  epoch  7, batch     1 | loss: 1.5005569Losses:  0.06134700030088425 0.6360749006271362 0.9193592071533203
MemoryTrain:  epoch  7, batch     2 | loss: 1.6167811Losses:  0.08027976751327515 1.1334913969039917 0.9488308429718018
MemoryTrain:  epoch  8, batch     0 | loss: 2.1626019Losses:  0.036811791360378265 0.7380549907684326 0.8991329669952393
MemoryTrain:  epoch  8, batch     1 | loss: 1.6739998Losses:  0.0469050332903862 0.5743128061294556 0.8869264125823975
MemoryTrain:  epoch  8, batch     2 | loss: 1.5081443Losses:  0.02626730501651764 1.0179120302200317 0.928571343421936
MemoryTrain:  epoch  9, batch     0 | loss: 1.9727507Losses:  0.030023518949747086 0.8582929372787476 0.9299882650375366
MemoryTrain:  epoch  9, batch     1 | loss: 1.8183048Losses:  0.03610703721642494 0.6679645776748657 0.8559869527816772
MemoryTrain:  epoch  9, batch     2 | loss: 1.5600586
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 72.99%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 67.14%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 65.20%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 66.09%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.91%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.41%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 88.45%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.16%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.15%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 88.56%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.30%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 88.06%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 88.15%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 88.35%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 88.49%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 88.38%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 88.08%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 87.95%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 87.68%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 87.59%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 87.59%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.59%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 87.33%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 86.84%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 86.44%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 86.14%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 85.60%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.47%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 85.11%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 84.53%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 83.73%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 82.96%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 82.21%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 81.54%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 80.75%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 80.54%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 81.45%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 81.45%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 81.31%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 81.12%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 80.62%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 80.69%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 81.19%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.37%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 82.05%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 82.47%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.51%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.61%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.98%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 83.09%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 83.32%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 83.25%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 82.69%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 82.33%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 81.84%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 81.49%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 80.77%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 80.68%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 80.69%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 81.02%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 81.03%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 81.12%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 81.21%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 81.29%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 81.29%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 81.08%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 80.82%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 80.78%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 80.62%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 80.37%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 80.25%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 79.76%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 79.24%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 78.72%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 78.21%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 77.70%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 77.20%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 77.15%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 77.41%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 77.45%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 77.73%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 77.61%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 77.16%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 76.85%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 76.59%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 76.19%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 75.82%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.67%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 75.67%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 75.70%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 75.52%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 75.34%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 75.10%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 74.97%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 74.83%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 74.73%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 74.53%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 74.57%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 74.61%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 74.51%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 74.45%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 74.39%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 74.36%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 74.37%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 74.40%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 74.47%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 74.42%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 74.45%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 74.64%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 74.91%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 75.03%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 74.91%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 74.77%   [EVAL] batch:  214 | acc: 18.75%,  total acc: 74.51%   [EVAL] batch:  215 | acc: 31.25%,  total acc: 74.31%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 74.16%   [EVAL] batch:  217 | acc: 25.00%,  total acc: 73.94%   [EVAL] batch:  218 | acc: 37.50%,  total acc: 73.77%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 73.61%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 73.48%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 73.29%   [EVAL] batch:  223 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 73.00%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 73.01%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 73.05%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 73.21%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 73.22%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 73.12%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 73.18%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 73.19%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 73.20%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 73.23%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.43%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.58%   
cur_acc:  ['0.9444', '0.8165', '0.6190', '0.7391']
his_acc:  ['0.9444', '0.8695', '0.7696', '0.7458']
Clustering into  24  clusters
Clusters:  [ 0  1 17  2  0  0 21  0 18 23 12 11  2  0 19 14 15  0  0 22  8  2  0  0
  0 20  0  0  0  0  1 13  2  0  0  6  0  0 10  9 16  0  4  0  5  7  3  0
  0  0]
Losses:  4.9139299392700195 1.4879741668701172 0.9861608743667603
CurrentTrain: epoch  0, batch     0 | loss: 7.3880649Losses:  6.201813697814941 1.5583738088607788 0.9603240489959717
CurrentTrain: epoch  0, batch     1 | loss: 8.7205114Losses:  6.170705318450928 1.356357455253601 0.9796966314315796
CurrentTrain: epoch  0, batch     2 | loss: 8.5067596Losses:  5.445622444152832 0.41320356726646423 0.984079122543335
CurrentTrain: epoch  0, batch     3 | loss: 6.8429050Losses:  5.806614875793457 1.3850175142288208 0.9695191383361816
CurrentTrain: epoch  1, batch     0 | loss: 8.1611519Losses:  4.390142917633057 1.2878408432006836 0.9693101048469543
CurrentTrain: epoch  1, batch     1 | loss: 6.6472940Losses:  4.961407661437988 1.4092378616333008 0.9666177034378052
CurrentTrain: epoch  1, batch     2 | loss: 7.3372631Losses:  3.576232671737671 0.17949935793876648 1.0
CurrentTrain: epoch  1, batch     3 | loss: 4.7557321Losses:  4.556851863861084 1.4181759357452393 0.969028890132904
CurrentTrain: epoch  2, batch     0 | loss: 6.9440570Losses:  4.544276237487793 1.4123749732971191 0.9638321399688721
CurrentTrain: epoch  2, batch     1 | loss: 6.9204836Losses:  4.670823097229004 1.2448298931121826 0.9683473706245422
CurrentTrain: epoch  2, batch     2 | loss: 6.8840008Losses:  2.300957202911377 0.21137025952339172 0.9744671583175659
CurrentTrain: epoch  2, batch     3 | loss: 3.4867945Losses:  3.4549739360809326 0.9681743383407593 0.9691789150238037
CurrentTrain: epoch  3, batch     0 | loss: 5.3923273Losses:  5.097434997558594 1.2427687644958496 0.9640505313873291
CurrentTrain: epoch  3, batch     1 | loss: 7.3042545Losses:  4.576261520385742 1.3772387504577637 0.9639257788658142
CurrentTrain: epoch  3, batch     2 | loss: 6.9174261Losses:  4.798793792724609 0.7085682153701782 0.9559170007705688
CurrentTrain: epoch  3, batch     3 | loss: 6.4632788Losses:  4.48488187789917 1.178736925125122 0.9654788970947266
CurrentTrain: epoch  4, batch     0 | loss: 6.6290979Losses:  4.247643947601318 1.069007158279419 0.9609626531600952
CurrentTrain: epoch  4, batch     1 | loss: 6.2776141Losses:  3.752924919128418 1.3082399368286133 0.9567296504974365
CurrentTrain: epoch  4, batch     2 | loss: 6.0178947Losses:  4.0312957763671875 0.07933111488819122 0.9503936767578125
CurrentTrain: epoch  4, batch     3 | loss: 5.0610204Losses:  4.773489952087402 1.0667788982391357 0.9510340094566345
CurrentTrain: epoch  5, batch     0 | loss: 6.7913032Losses:  3.515563726425171 1.097433090209961 0.9671906232833862
CurrentTrain: epoch  5, batch     1 | loss: 5.5801878Losses:  2.5911989212036133 1.0152242183685303 0.9660924673080444
CurrentTrain: epoch  5, batch     2 | loss: 4.5725155Losses:  3.4663522243499756 0.41285547614097595 0.8907444477081299
CurrentTrain: epoch  5, batch     3 | loss: 4.7699518Losses:  3.766484498977661 0.8738044500350952 0.96803879737854
CurrentTrain: epoch  6, batch     0 | loss: 5.6083279Losses:  2.908538818359375 0.9122148752212524 0.9557307958602905
CurrentTrain: epoch  6, batch     1 | loss: 4.7764845Losses:  3.4802563190460205 1.1335033178329468 0.9547630548477173
CurrentTrain: epoch  6, batch     2 | loss: 5.5685225Losses:  2.829468250274658 0.24544885754585266 0.9449784755706787
CurrentTrain: epoch  6, batch     3 | loss: 4.0198956Losses:  3.3305742740631104 1.0098482370376587 0.968127429485321
CurrentTrain: epoch  7, batch     0 | loss: 5.3085499Losses:  3.4713358879089355 1.1482969522476196 0.9362117648124695
CurrentTrain: epoch  7, batch     1 | loss: 5.5558443Losses:  2.930307388305664 1.1364097595214844 0.9706460237503052
CurrentTrain: epoch  7, batch     2 | loss: 5.0373631Losses:  1.8240103721618652 0.1322074830532074 0.912280797958374
CurrentTrain: epoch  7, batch     3 | loss: 2.8684988Losses:  2.8614957332611084 1.011579155921936 0.9516911506652832
CurrentTrain: epoch  8, batch     0 | loss: 4.8247662Losses:  2.881446599960327 0.8296663761138916 0.9645439982414246
CurrentTrain: epoch  8, batch     1 | loss: 4.6756568Losses:  3.160166025161743 1.060092806816101 0.958797037601471
CurrentTrain: epoch  8, batch     2 | loss: 5.1790557Losses:  2.818516731262207 0.17660477757453918 0.8461440801620483
CurrentTrain: epoch  8, batch     3 | loss: 3.8412657Losses:  2.601947069168091 0.7565294504165649 0.9369195103645325
CurrentTrain: epoch  9, batch     0 | loss: 4.2953963Losses:  3.1735751628875732 0.9066949486732483 0.9482172727584839
CurrentTrain: epoch  9, batch     1 | loss: 5.0284877Losses:  2.657857894897461 0.9290602207183838 0.9675775766372681
CurrentTrain: epoch  9, batch     2 | loss: 4.5544958Losses:  3.284543991088867 0.26112595200538635 1.0084607601165771
CurrentTrain: epoch  9, batch     3 | loss: 4.5541306
Losses:  0.2635025382041931 0.773629367351532 0.888532280921936
MemoryTrain:  epoch  0, batch     0 | loss: 1.9256642Losses:  0.5616469383239746 1.0300419330596924 0.9406492114067078
MemoryTrain:  epoch  0, batch     1 | loss: 2.5323381Losses:  0.2298274040222168 1.1380133628845215 0.9314373731613159
MemoryTrain:  epoch  0, batch     2 | loss: 2.2992783Losses:  0.599820077419281 0.31748369336128235 0.8865068554878235
MemoryTrain:  epoch  0, batch     3 | loss: 1.8038106Losses:  0.540777325630188 0.9710149168968201 0.9297930598258972
MemoryTrain:  epoch  1, batch     0 | loss: 2.4415853Losses:  0.24740368127822876 0.9711363315582275 0.9310623407363892
MemoryTrain:  epoch  1, batch     1 | loss: 2.1496024Losses:  0.46916958689689636 1.1436536312103271 0.8815847635269165
MemoryTrain:  epoch  1, batch     2 | loss: 2.4944081Losses:  0.036417778581380844 0.059836599975824356 1.0
MemoryTrain:  epoch  1, batch     3 | loss: 1.0962543Losses:  0.41856396198272705 1.054969072341919 0.9200134873390198
MemoryTrain:  epoch  2, batch     0 | loss: 2.3935466Losses:  0.1984240710735321 0.9072355628013611 0.9079534411430359
MemoryTrain:  epoch  2, batch     1 | loss: 2.0136130Losses:  0.26996055245399475 0.9270660877227783 0.9287417531013489
MemoryTrain:  epoch  2, batch     2 | loss: 2.1257684Losses:  0.3371235132217407 0.10941565036773682 0.9082250595092773
MemoryTrain:  epoch  2, batch     3 | loss: 1.3547642Losses:  0.1422644853591919 1.0139710903167725 0.8843232989311218
MemoryTrain:  epoch  3, batch     0 | loss: 2.0405588Losses:  0.1615704894065857 1.012624740600586 0.9143790006637573
MemoryTrain:  epoch  3, batch     1 | loss: 2.0885744Losses:  0.15946567058563232 0.8475255966186523 0.9634662866592407
MemoryTrain:  epoch  3, batch     2 | loss: 1.9704576Losses:  0.12569467723369598 0.075675368309021 0.832145631313324
MemoryTrain:  epoch  3, batch     3 | loss: 1.0335157Losses:  0.14111903309822083 0.9885314702987671 0.9198070168495178
MemoryTrain:  epoch  4, batch     0 | loss: 2.0494576Losses:  0.04863926023244858 0.7796626687049866 0.9131693840026855
MemoryTrain:  epoch  4, batch     1 | loss: 1.7414713Losses:  0.19941076636314392 1.135175108909607 0.9180079102516174
MemoryTrain:  epoch  4, batch     2 | loss: 2.2525938Losses:  0.04615754261612892 0.1458481252193451 0.8868215084075928
MemoryTrain:  epoch  4, batch     3 | loss: 1.0788271Losses:  0.09721460938453674 0.9695165753364563 0.9318529367446899
MemoryTrain:  epoch  5, batch     0 | loss: 1.9985842Losses:  0.07191132009029388 0.7526094913482666 0.9158456325531006
MemoryTrain:  epoch  5, batch     1 | loss: 1.7403665Losses:  0.06531678140163422 1.0233240127563477 0.8994485139846802
MemoryTrain:  epoch  5, batch     2 | loss: 1.9880893Losses:  0.020434856414794922 0.04179036617279053 0.9087128043174744
MemoryTrain:  epoch  5, batch     3 | loss: 0.9709380Losses:  0.08880867063999176 0.9967883229255676 0.9109942317008972
MemoryTrain:  epoch  6, batch     0 | loss: 1.9965913Losses:  0.08834969997406006 0.8951435089111328 0.9216189980506897
MemoryTrain:  epoch  6, batch     1 | loss: 1.9051123Losses:  0.039744339883327484 0.8394522070884705 0.9091029167175293
MemoryTrain:  epoch  6, batch     2 | loss: 1.7882994Losses:  0.03060886263847351 0.04594409465789795 0.9426301717758179
MemoryTrain:  epoch  6, batch     3 | loss: 1.0191832Losses:  0.057102546095848083 0.9491328597068787 0.9362519979476929
MemoryTrain:  epoch  7, batch     0 | loss: 1.9424874Losses:  0.09527288377285004 1.027056336402893 0.9169442653656006
MemoryTrain:  epoch  7, batch     1 | loss: 2.0392735Losses:  0.03736650571227074 0.7201508283615112 0.8755348920822144
MemoryTrain:  epoch  7, batch     2 | loss: 1.6330522Losses:  0.018812302500009537 0.02463991940021515 1.0
MemoryTrain:  epoch  7, batch     3 | loss: 1.0434523Losses:  0.036887817084789276 0.9617276191711426 0.9128995537757874
MemoryTrain:  epoch  8, batch     0 | loss: 1.9115150Losses:  0.039440952241420746 0.6516334414482117 0.9118033647537231
MemoryTrain:  epoch  8, batch     1 | loss: 1.6028777Losses:  0.09623207896947861 0.9306883215904236 0.9089921712875366
MemoryTrain:  epoch  8, batch     2 | loss: 1.9359126Losses:  0.11683132499456406 0.07387480139732361 0.9199503660202026
MemoryTrain:  epoch  8, batch     3 | loss: 1.1106565Losses:  0.06552962958812714 0.9053165912628174 0.9017997980117798
MemoryTrain:  epoch  9, batch     0 | loss: 1.8726461Losses:  0.0493687279522419 0.7044855356216431 0.8980017900466919
MemoryTrain:  epoch  9, batch     1 | loss: 1.6518561Losses:  0.045041244477033615 0.8956000804901123 0.9442297220230103
MemoryTrain:  epoch  9, batch     2 | loss: 1.8848710Losses:  0.015575363300740719 0.0823613703250885 0.8117474317550659
MemoryTrain:  epoch  9, batch     3 | loss: 0.9096842
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 70.31%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 74.09%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 73.66%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 73.84%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 73.52%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 73.02%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 72.75%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.43%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.66%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 87.23%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 86.86%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 86.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.52%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 86.32%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 85.60%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 85.53%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 85.45%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 85.70%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.73%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 85.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 85.69%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 85.64%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 85.58%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 85.61%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 85.45%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 85.62%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.70%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 85.47%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.50%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 84.79%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 84.09%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 83.49%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 82.75%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 81.87%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 81.17%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 80.50%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 79.76%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 79.04%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 78.49%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 77.73%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 77.56%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 78.71%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 78.61%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 77.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.73%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.93%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 79.59%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 79.92%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 79.71%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 79.73%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.74%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 79.54%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.56%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.52%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 79.60%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 79.61%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.89%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 79.80%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.37%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 79.04%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 78.56%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 78.20%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 77.93%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 77.53%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 77.46%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.49%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 77.92%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 78.04%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 78.15%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 78.21%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 78.19%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 77.84%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 77.53%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 77.34%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 77.20%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 76.93%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 76.79%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.32%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 75.82%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.33%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 74.84%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 74.40%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 73.92%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 74.06%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 74.43%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 74.74%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 74.85%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 74.89%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 74.63%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 74.23%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 73.91%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 73.66%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 73.28%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 72.89%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 72.59%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 72.54%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 72.49%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 72.34%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 72.15%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 71.96%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 71.77%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 71.62%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 71.47%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 71.24%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.47%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 71.45%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 71.50%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 71.49%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 71.47%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 71.46%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 71.53%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 71.33%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.57%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  211 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  212 | acc: 37.50%,  total acc: 72.12%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 71.93%   [EVAL] batch:  214 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:  215 | acc: 12.50%,  total acc: 71.38%   [EVAL] batch:  216 | acc: 6.25%,  total acc: 71.08%   [EVAL] batch:  217 | acc: 12.50%,  total acc: 70.81%   [EVAL] batch:  218 | acc: 18.75%,  total acc: 70.58%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 70.42%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 70.30%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 70.12%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 69.89%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 69.75%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 69.93%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 70.07%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 69.98%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  236 | acc: 68.75%,  total acc: 69.96%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 71.34%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 71.28%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 71.27%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 71.19%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 71.10%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 71.07%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 71.08%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 71.14%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 71.32%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 71.47%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 71.24%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 71.05%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 70.97%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 70.87%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 70.66%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 71.85%   [EVAL] batch:  288 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.81%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 71.78%   [EVAL] batch:  291 | acc: 56.25%,  total acc: 71.73%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 71.64%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 71.61%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  309 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 71.49%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 71.41%   
cur_acc:  ['0.9444', '0.8165', '0.6190', '0.7391', '0.7143']
his_acc:  ['0.9444', '0.8695', '0.7696', '0.7458', '0.7141']
Clustering into  29  clusters
Clusters:  [ 0  2 19  0  0  0 24  0 20 15 25 23  0  0 11 27  9  0  0 26 22  0  0  0
  0 17  0  0  0  0  2 21  0  0  0  1  0 28 10 12 18  0 14  0 16  8 13  0
  0  0  1  4  0  6  7  0  0  0  5  3]
Losses:  6.37637996673584 1.213817834854126 0.9779621362686157
CurrentTrain: epoch  0, batch     0 | loss: 8.5681601Losses:  6.846054553985596 1.6556471586227417 1.0028376579284668
CurrentTrain: epoch  0, batch     1 | loss: 9.5045395Losses:  6.059989929199219 1.5709973573684692 0.9929777383804321
CurrentTrain: epoch  0, batch     2 | loss: 8.6239653Losses:  4.133419990539551 0.43673908710479736 0.9699559211730957
CurrentTrain: epoch  0, batch     3 | loss: 5.5401149Losses:  6.13480281829834 1.4898109436035156 0.9920387268066406
CurrentTrain: epoch  1, batch     0 | loss: 8.6166525Losses:  4.6023054122924805 1.4644142389297485 0.9701434969902039
CurrentTrain: epoch  1, batch     1 | loss: 7.0368629Losses:  5.097918510437012 1.2338675260543823 0.9839565753936768
CurrentTrain: epoch  1, batch     2 | loss: 7.3157425Losses:  3.011439323425293 0.0 1.0
CurrentTrain: epoch  1, batch     3 | loss: 4.0114393Losses:  5.008062362670898 1.307298183441162 0.9945590496063232
CurrentTrain: epoch  2, batch     0 | loss: 7.3099194Losses:  4.654336929321289 1.4064977169036865 0.9820716381072998
CurrentTrain: epoch  2, batch     1 | loss: 7.0429068Losses:  4.539652347564697 1.2915064096450806 0.9734835624694824
CurrentTrain: epoch  2, batch     2 | loss: 6.8046422Losses:  2.7472829818725586 0.40110456943511963 0.9662962555885315
CurrentTrain: epoch  2, batch     3 | loss: 4.1146836Losses:  4.625304698944092 1.4133754968643188 0.9839275479316711
CurrentTrain: epoch  3, batch     0 | loss: 7.0226078Losses:  4.6819868087768555 1.3475141525268555 0.9805611371994019
CurrentTrain: epoch  3, batch     1 | loss: 7.0100622Losses:  4.0748491287231445 1.4067349433898926 0.9744857549667358
CurrentTrain: epoch  3, batch     2 | loss: 6.4560699Losses:  2.291447639465332 0.10073020309209824 0.9493743181228638
CurrentTrain: epoch  3, batch     3 | loss: 3.3415523Losses:  3.984717845916748 1.4210658073425293 0.9755919575691223
CurrentTrain: epoch  4, batch     0 | loss: 6.3813758Losses:  3.843827724456787 1.3171508312225342 0.9750112295150757
CurrentTrain: epoch  4, batch     1 | loss: 6.1359897Losses:  4.027364730834961 1.0357484817504883 0.9824820756912231
CurrentTrain: epoch  4, batch     2 | loss: 6.0455952Losses:  4.881534099578857 0.159601628780365 1.0
CurrentTrain: epoch  4, batch     3 | loss: 6.0411358Losses:  4.360739707946777 1.262812852859497 0.9774659872055054
CurrentTrain: epoch  5, batch     0 | loss: 6.6010184Losses:  3.513777494430542 1.059125304222107 0.9721773862838745
CurrentTrain: epoch  5, batch     1 | loss: 5.5450802Losses:  3.4190778732299805 1.1991093158721924 0.9693692922592163
CurrentTrain: epoch  5, batch     2 | loss: 5.5875564Losses:  2.16957950592041 0.09577563405036926 1.0
CurrentTrain: epoch  5, batch     3 | loss: 3.2653551Losses:  4.089069843292236 1.3271608352661133 0.9751549959182739
CurrentTrain: epoch  6, batch     0 | loss: 6.3913856Losses:  3.0071277618408203 1.1959294080734253 0.972815752029419
CurrentTrain: epoch  6, batch     1 | loss: 5.1758728Losses:  3.280001163482666 0.911605179309845 0.9801816344261169
CurrentTrain: epoch  6, batch     2 | loss: 5.1717882Losses:  3.405904531478882 0.41644224524497986 0.945547342300415
CurrentTrain: epoch  6, batch     3 | loss: 4.7678938Losses:  2.8619468212127686 1.0412192344665527 0.9808453321456909
CurrentTrain: epoch  7, batch     0 | loss: 4.8840113Losses:  3.5597152709960938 1.2249956130981445 0.9621887803077698
CurrentTrain: epoch  7, batch     1 | loss: 5.7468996Losses:  3.2247982025146484 1.0127289295196533 0.9781885743141174
CurrentTrain: epoch  7, batch     2 | loss: 5.2157154Losses:  2.2108707427978516 0.24124892055988312 0.9418476223945618
CurrentTrain: epoch  7, batch     3 | loss: 3.3939672Losses:  2.8950397968292236 1.0961709022521973 0.9764314889907837
CurrentTrain: epoch  8, batch     0 | loss: 4.9676423Losses:  3.001445770263672 1.0851892232894897 0.9790717363357544
CurrentTrain: epoch  8, batch     1 | loss: 5.0657067Losses:  3.256972074508667 1.2080979347229004 0.9506102800369263
CurrentTrain: epoch  8, batch     2 | loss: 5.4156799Losses:  2.096235752105713 0.24710968136787415 0.9501246213912964
CurrentTrain: epoch  8, batch     3 | loss: 3.2934699Losses:  3.0675885677337646 1.0416920185089111 0.9563624858856201
CurrentTrain: epoch  9, batch     0 | loss: 5.0656433Losses:  2.6089959144592285 0.8756047487258911 0.9821491241455078
CurrentTrain: epoch  9, batch     1 | loss: 4.4667497Losses:  2.927311420440674 0.9969221353530884 0.9531815648078918
CurrentTrain: epoch  9, batch     2 | loss: 4.8774152Losses:  1.799360752105713 0.33276689052581787 1.0
CurrentTrain: epoch  9, batch     3 | loss: 3.1321278
Losses:  0.42117005586624146 0.7646684646606445 0.9238676428794861
MemoryTrain:  epoch  0, batch     0 | loss: 2.1097062Losses:  0.553433895111084 0.8696144819259644 0.9363337159156799
MemoryTrain:  epoch  0, batch     1 | loss: 2.3593822Losses:  0.4014342427253723 1.0163383483886719 0.8750841617584229
MemoryTrain:  epoch  0, batch     2 | loss: 2.2928567Losses:  0.41565144062042236 1.155070424079895 0.9303416013717651
MemoryTrain:  epoch  0, batch     3 | loss: 2.5010633Losses:  0.6844162344932556 0.9316053986549377 0.9167970418930054
MemoryTrain:  epoch  1, batch     0 | loss: 2.5328188Losses:  0.8561393022537231 1.0056687593460083 0.9149600267410278
MemoryTrain:  epoch  1, batch     1 | loss: 2.7767682Losses:  0.6619775295257568 0.967832624912262 0.8999136090278625
MemoryTrain:  epoch  1, batch     2 | loss: 2.5297236Losses:  0.3954903483390808 0.6326996088027954 0.921515166759491
MemoryTrain:  epoch  1, batch     3 | loss: 1.9497051Losses:  0.4668506383895874 1.0134614706039429 0.9415242671966553
MemoryTrain:  epoch  2, batch     0 | loss: 2.4218364Losses:  0.16965505480766296 0.7478736639022827 0.9172860383987427
MemoryTrain:  epoch  2, batch     1 | loss: 1.8348148Losses:  0.27003026008605957 0.9930566549301147 0.9040619134902954
MemoryTrain:  epoch  2, batch     2 | loss: 2.1671488Losses:  0.28186142444610596 0.6656323671340942 0.8641597032546997
MemoryTrain:  epoch  2, batch     3 | loss: 1.8116535Losses:  0.19774115085601807 0.9715917706489563 0.9410390853881836
MemoryTrain:  epoch  3, batch     0 | loss: 2.1103721Losses:  0.0613008514046669 0.7332000136375427 0.8654382228851318
MemoryTrain:  epoch  3, batch     1 | loss: 1.6599391Losses:  0.10096342861652374 0.9210448265075684 0.9273364543914795
MemoryTrain:  epoch  3, batch     2 | loss: 1.9493448Losses:  0.5863121747970581 0.7670740485191345 0.9027389287948608
MemoryTrain:  epoch  3, batch     3 | loss: 2.2561250Losses:  0.18955814838409424 0.7675548791885376 0.8839472532272339
MemoryTrain:  epoch  4, batch     0 | loss: 1.8410603Losses:  0.11875581741333008 1.1851956844329834 0.9004865884780884
MemoryTrain:  epoch  4, batch     1 | loss: 2.2044382Losses:  0.0803084596991539 0.8138317465782166 0.9233449697494507
MemoryTrain:  epoch  4, batch     2 | loss: 1.8174851Losses:  0.13433560729026794 0.7324349284172058 0.9242355227470398
MemoryTrain:  epoch  4, batch     3 | loss: 1.7910061Losses:  0.17515011131763458 0.8396364450454712 0.9051334857940674
MemoryTrain:  epoch  5, batch     0 | loss: 1.9199201Losses:  0.08377522230148315 0.8810602426528931 0.9564241766929626
MemoryTrain:  epoch  5, batch     1 | loss: 1.9212596Losses:  0.17168958485126495 0.7815312743186951 0.9017225503921509
MemoryTrain:  epoch  5, batch     2 | loss: 1.8549434Losses:  0.053349804133176804 0.8421179056167603 0.8413561582565308
MemoryTrain:  epoch  5, batch     3 | loss: 1.7368238Losses:  0.04007258266210556 0.7954851984977722 0.9081869125366211
MemoryTrain:  epoch  6, batch     0 | loss: 1.7437446Losses:  0.09404085576534271 0.8768338561058044 0.9106477499008179
MemoryTrain:  epoch  6, batch     1 | loss: 1.8815224Losses:  0.0859588086605072 0.8825172781944275 0.8782715201377869
MemoryTrain:  epoch  6, batch     2 | loss: 1.8467476Losses:  0.0563199520111084 0.6671912670135498 0.9268450140953064
MemoryTrain:  epoch  6, batch     3 | loss: 1.6503563Losses:  0.06565937399864197 0.9483708739280701 0.9046041965484619
MemoryTrain:  epoch  7, batch     0 | loss: 1.9186344Losses:  0.04361069202423096 0.6836885809898376 0.8972840309143066
MemoryTrain:  epoch  7, batch     1 | loss: 1.6245832Losses:  0.04275704175233841 0.8547601699829102 0.9149376153945923
MemoryTrain:  epoch  7, batch     2 | loss: 1.8124548Losses:  0.11885782331228256 0.6344249248504639 0.8930153250694275
MemoryTrain:  epoch  7, batch     3 | loss: 1.6462981Losses:  0.047136254608631134 0.7356781363487244 0.8946099281311035
MemoryTrain:  epoch  8, batch     0 | loss: 1.6774243Losses:  0.0629986971616745 0.8488340377807617 0.9253745079040527
MemoryTrain:  epoch  8, batch     1 | loss: 1.8372073Losses:  0.035266369581222534 0.8554545640945435 0.902247428894043
MemoryTrain:  epoch  8, batch     2 | loss: 1.7929684Losses:  0.04712202399969101 0.6099403500556946 0.8759175539016724
MemoryTrain:  epoch  8, batch     3 | loss: 1.5329800Losses:  0.0362037718296051 0.7555944919586182 0.9252160787582397
MemoryTrain:  epoch  9, batch     0 | loss: 1.7170143Losses:  0.05320889502763748 0.9182600975036621 0.8695995807647705
MemoryTrain:  epoch  9, batch     1 | loss: 1.8410685Losses:  0.04649965837597847 0.7753543257713318 0.9168972969055176
MemoryTrain:  epoch  9, batch     2 | loss: 1.7387513Losses:  0.08730026334524155 0.5870631337165833 0.887603223323822
MemoryTrain:  epoch  9, batch     3 | loss: 1.5619667
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 55.08%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 74.67%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 74.20%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 73.12%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.26%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 71.82%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 70.98%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 70.29%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 70.15%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 70.02%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.05%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 87.10%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.76%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.66%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 86.14%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 85.02%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 84.75%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 84.18%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 84.13%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 84.19%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 84.05%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.20%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 83.98%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.94%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 83.61%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 83.50%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 82.81%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 81.90%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 81.33%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 80.62%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 80.31%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 79.78%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 79.19%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 78.46%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 77.90%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 76.60%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.83%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 77.10%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 77.02%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 78.67%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 78.59%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 78.61%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 78.42%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 78.41%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 78.43%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 78.56%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 78.17%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 77.81%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 77.44%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 76.83%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 76.43%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 76.37%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.62%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.77%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 76.75%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 76.94%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 76.88%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 76.74%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 76.51%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 76.16%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 76.11%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 76.01%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 75.76%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 75.17%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 74.67%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 74.18%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 73.70%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 73.23%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.76%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 73.10%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 73.27%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 73.40%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 73.68%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 73.28%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.97%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 72.69%   [EVAL] batch:  173 | acc: 12.50%,  total acc: 72.34%   [EVAL] batch:  174 | acc: 0.00%,  total acc: 71.93%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 71.73%   [EVAL] batch:  176 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 71.84%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.91%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 71.86%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:  182 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 71.40%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 71.06%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 71.12%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.26%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 71.33%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 71.33%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 71.38%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 71.37%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 71.20%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 71.34%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 71.32%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 71.42%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 71.44%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 71.92%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 71.95%   [EVAL] batch:  213 | acc: 50.00%,  total acc: 71.85%   [EVAL] batch:  214 | acc: 31.25%,  total acc: 71.66%   [EVAL] batch:  215 | acc: 12.50%,  total acc: 71.38%   [EVAL] batch:  216 | acc: 12.50%,  total acc: 71.11%   [EVAL] batch:  217 | acc: 18.75%,  total acc: 70.87%   [EVAL] batch:  218 | acc: 25.00%,  total acc: 70.66%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 70.36%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 70.07%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 69.59%   [EVAL] batch:  224 | acc: 12.50%,  total acc: 69.33%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 69.62%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 69.45%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 69.20%   [EVAL] batch:  235 | acc: 43.75%,  total acc: 69.09%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 68.93%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 69.84%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 70.14%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 70.09%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 70.11%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 69.96%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 69.88%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 69.91%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 70.09%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 70.02%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 69.81%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 69.62%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 69.39%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  288 | acc: 50.00%,  total acc: 70.59%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 70.58%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 70.58%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 70.29%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 70.27%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 70.17%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 70.38%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 70.53%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 70.40%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 70.23%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 70.11%   [EVAL] batch:  309 | acc: 18.75%,  total acc: 69.94%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 69.77%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 69.55%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 69.43%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 69.23%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 69.15%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 68.81%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 68.93%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 69.00%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 68.83%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 68.67%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 68.58%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 70.03%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 69.94%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 69.63%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  362 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 69.88%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.83%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 69.82%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.77%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 69.68%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.63%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 69.63%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.57%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 69.52%   
cur_acc:  ['0.9444', '0.8165', '0.6190', '0.7391', '0.7143', '0.6905']
his_acc:  ['0.9444', '0.8695', '0.7696', '0.7458', '0.7141', '0.6952']
Clustering into  34  clusters
Clusters:  [ 0  5 22  0  0  0 27  0 23 31 33 28  0  0 26 32 24  0  0 29 15  0  0  0
  0 19  0  0  0  0  5 21  0  0  0  2  0 16 25 13 20  0 17  0 18  9  8  0
  0  0  2 11  0  7 30  0  0  0 10  4  0  0  0 14  0 12  0  6  3  1]
Losses:  6.444737434387207 1.1477625370025635 0.9560210704803467
CurrentTrain: epoch  0, batch     0 | loss: 8.5485210Losses:  6.210624694824219 1.2359209060668945 0.9633047580718994
CurrentTrain: epoch  0, batch     1 | loss: 8.4098501Losses:  6.610128402709961 1.3414726257324219 0.9570680856704712
CurrentTrain: epoch  0, batch     2 | loss: 8.9086695Losses:  3.790482521057129 5.960465188081798e-08 1.0
CurrentTrain: epoch  0, batch     3 | loss: 4.7904825Losses:  4.838148593902588 1.026344656944275 0.9674718379974365
CurrentTrain: epoch  1, batch     0 | loss: 6.8319654Losses:  4.725541114807129 1.1907367706298828 0.9622675180435181
CurrentTrain: epoch  1, batch     1 | loss: 6.8785453Losses:  5.366355895996094 1.1500244140625 0.9383005499839783
CurrentTrain: epoch  1, batch     2 | loss: 7.4546809Losses:  4.169158935546875 0.06836482882499695 0.9899077415466309
CurrentTrain: epoch  1, batch     3 | loss: 5.2274313Losses:  4.271243095397949 1.1952588558197021 0.95477294921875
CurrentTrain: epoch  2, batch     0 | loss: 6.4212751Losses:  3.770172119140625 0.9363771677017212 0.9657279253005981
CurrentTrain: epoch  2, batch     1 | loss: 5.6722770Losses:  4.113643169403076 0.8682116866111755 0.9448097944259644
CurrentTrain: epoch  2, batch     2 | loss: 5.9266648Losses:  3.939265251159668 0.14991025626659393 0.9717828035354614
CurrentTrain: epoch  2, batch     3 | loss: 5.0609584Losses:  3.492936611175537 1.0382754802703857 0.953445553779602
CurrentTrain: epoch  3, batch     0 | loss: 5.4846573Losses:  4.197941780090332 0.7613310813903809 0.9352948665618896
CurrentTrain: epoch  3, batch     1 | loss: 5.8945675Losses:  3.1918983459472656 0.7653611898422241 0.9737251996994019
CurrentTrain: epoch  3, batch     2 | loss: 4.9309850Losses:  3.980133056640625 1.1920930376163597e-07 0.9153180718421936
CurrentTrain: epoch  3, batch     3 | loss: 4.8954515Losses:  3.427523136138916 0.752265214920044 0.956599235534668
CurrentTrain: epoch  4, batch     0 | loss: 5.1363878Losses:  3.2194788455963135 0.907600998878479 0.9399974346160889
CurrentTrain: epoch  4, batch     1 | loss: 5.0670776Losses:  2.9459495544433594 0.7185369729995728 0.9594340324401855
CurrentTrain: epoch  4, batch     2 | loss: 4.6239204Losses:  3.3234400749206543 2.9802322387695312e-08 0.8882386684417725
CurrentTrain: epoch  4, batch     3 | loss: 4.2116785Losses:  3.0627832412719727 0.6661715507507324 0.9442633986473083
CurrentTrain: epoch  5, batch     0 | loss: 4.6732183Losses:  2.637279510498047 0.7582930326461792 0.9513111114501953
CurrentTrain: epoch  5, batch     1 | loss: 4.3468838Losses:  2.981081008911133 0.8596836924552917 0.940444827079773
CurrentTrain: epoch  5, batch     2 | loss: 4.7812095Losses:  2.376892566680908 0.17173635959625244 0.9286795258522034
CurrentTrain: epoch  5, batch     3 | loss: 3.4773083Losses:  2.646388053894043 0.6791672706604004 0.9504072666168213
CurrentTrain: epoch  6, batch     0 | loss: 4.2759628Losses:  2.7362618446350098 0.5838285684585571 0.9417696595191956
CurrentTrain: epoch  6, batch     1 | loss: 4.2618599Losses:  2.876594305038452 0.4957761764526367 0.9427118897438049
CurrentTrain: epoch  6, batch     2 | loss: 4.3150826Losses:  2.154900550842285 0.06611821055412292 0.9603017568588257
CurrentTrain: epoch  6, batch     3 | loss: 3.1813207Losses:  2.6372833251953125 0.6582293510437012 0.9275753498077393
CurrentTrain: epoch  7, batch     0 | loss: 4.2230883Losses:  2.3438572883605957 0.6747404336929321 0.9327625036239624
CurrentTrain: epoch  7, batch     1 | loss: 3.9513602Losses:  2.365206718444824 0.5367269515991211 0.9649485349655151
CurrentTrain: epoch  7, batch     2 | loss: 3.8668823Losses:  2.376955986022949 0.06919777393341064 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.4461536Losses:  2.5387890338897705 0.6721400022506714 0.9255132675170898
CurrentTrain: epoch  8, batch     0 | loss: 4.1364422Losses:  2.0700509548187256 0.5520777702331543 0.9673426747322083
CurrentTrain: epoch  8, batch     1 | loss: 3.5894713Losses:  2.3340916633605957 0.6657297611236572 0.935195803642273
CurrentTrain: epoch  8, batch     2 | loss: 3.9350171Losses:  1.7322137355804443 0.058630578219890594 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.7908444Losses:  2.5400431156158447 0.5079935789108276 0.9341106414794922
CurrentTrain: epoch  9, batch     0 | loss: 3.9821472Losses:  2.3384604454040527 0.6375546455383301 0.9453128576278687
CurrentTrain: epoch  9, batch     1 | loss: 3.9213281Losses:  2.071685791015625 0.580354630947113 0.9481198191642761
CurrentTrain: epoch  9, batch     2 | loss: 3.6001604Losses:  1.9454973936080933 0.07628130912780762 0.9318915605545044
CurrentTrain: epoch  9, batch     3 | loss: 2.9536700
Losses:  0.40787017345428467 1.11249840259552 0.8948507308959961
MemoryTrain:  epoch  0, batch     0 | loss: 2.4152193Losses:  0.698236346244812 0.7130166888237 0.9250072240829468
MemoryTrain:  epoch  0, batch     1 | loss: 2.3362603Losses:  0.45770978927612305 0.941419780254364 0.9114328026771545
MemoryTrain:  epoch  0, batch     2 | loss: 2.3105624Losses:  0.93204665184021 0.7409976720809937 0.8905579447746277
MemoryTrain:  epoch  0, batch     3 | loss: 2.5636022Losses:  1.8326014280319214 0.35189253091812134 0.9254806041717529
MemoryTrain:  epoch  0, batch     4 | loss: 3.1099746Losses:  2.148070812225342 0.9801326394081116 0.907976508140564
MemoryTrain:  epoch  1, batch     0 | loss: 4.0361800Losses:  0.3154204189777374 0.8688652515411377 0.9125872850418091
MemoryTrain:  epoch  1, batch     1 | loss: 2.0968728Losses:  0.36748218536376953 0.8364827632904053 0.9315823316574097
MemoryTrain:  epoch  1, batch     2 | loss: 2.1355472Losses:  0.972350001335144 0.8511599898338318 0.8714534044265747
MemoryTrain:  epoch  1, batch     3 | loss: 2.6949635Losses:  1.063685655593872 0.27318912744522095 0.9137665629386902
MemoryTrain:  epoch  1, batch     4 | loss: 2.2506413Losses:  0.8748146295547485 0.6992098689079285 0.9214835166931152
MemoryTrain:  epoch  2, batch     0 | loss: 2.4955080Losses:  0.24567414820194244 0.8122584819793701 0.9031362533569336
MemoryTrain:  epoch  2, batch     1 | loss: 1.9610689Losses:  1.0796562433242798 0.9528567790985107 0.8510125875473022
MemoryTrain:  epoch  2, batch     2 | loss: 2.8835258Losses:  0.5362362265586853 0.8673891425132751 0.9245561361312866
MemoryTrain:  epoch  2, batch     3 | loss: 2.3281815Losses:  0.6014710664749146 0.2963063418865204 0.9477898478507996
MemoryTrain:  epoch  2, batch     4 | loss: 1.8455672Losses:  0.6189882159233093 0.9125497937202454 0.8905152678489685
MemoryTrain:  epoch  3, batch     0 | loss: 2.4220533Losses:  0.23342014849185944 0.6422004103660583 0.910821795463562
MemoryTrain:  epoch  3, batch     1 | loss: 1.7864423Losses:  0.33182114362716675 0.9279807806015015 0.8877961039543152
MemoryTrain:  epoch  3, batch     2 | loss: 2.1475980Losses:  0.7104219794273376 0.8162893056869507 0.8893747329711914
MemoryTrain:  epoch  3, batch     3 | loss: 2.4160860Losses:  0.2827639579772949 0.35785049200057983 1.0
MemoryTrain:  epoch  3, batch     4 | loss: 1.6406145Losses:  0.13134023547172546 0.7765203714370728 0.9532463550567627
MemoryTrain:  epoch  4, batch     0 | loss: 1.8611070Losses:  0.18786907196044922 0.8154370784759521 0.8624007701873779
MemoryTrain:  epoch  4, batch     1 | loss: 1.8657069Losses:  0.5276514291763306 0.6925416588783264 0.9261890649795532
MemoryTrain:  epoch  4, batch     2 | loss: 2.1463823Losses:  0.6005398035049438 0.9454364776611328 0.8557473421096802
MemoryTrain:  epoch  4, batch     3 | loss: 2.4017236Losses:  0.10832415521144867 0.4558492600917816 0.9330739974975586
MemoryTrain:  epoch  4, batch     4 | loss: 1.4972475Losses:  0.14205650985240936 0.8816733360290527 0.9042024612426758
MemoryTrain:  epoch  5, batch     0 | loss: 1.9279323Losses:  0.48577937483787537 0.8535375595092773 0.8644838929176331
MemoryTrain:  epoch  5, batch     1 | loss: 2.2038009Losses:  0.5190141201019287 0.772287130355835 0.9055973887443542
MemoryTrain:  epoch  5, batch     2 | loss: 2.1968987Losses:  0.16263583302497864 0.6962764859199524 0.9380633234977722
MemoryTrain:  epoch  5, batch     3 | loss: 1.7969756Losses:  0.05387140437960625 0.28488633036613464 0.8693841695785522
MemoryTrain:  epoch  5, batch     4 | loss: 1.2081419Losses:  0.4239261746406555 0.6847147941589355 0.886578381061554
MemoryTrain:  epoch  6, batch     0 | loss: 1.9952192Losses:  0.09351308643817902 0.9620400667190552 0.9559323787689209
MemoryTrain:  epoch  6, batch     1 | loss: 2.0114856Losses:  0.5423702597618103 0.7241277694702148 0.8508508205413818
MemoryTrain:  epoch  6, batch     2 | loss: 2.1173489Losses:  0.04635823890566826 0.7499368190765381 0.9144725799560547
MemoryTrain:  epoch  6, batch     3 | loss: 1.7107676Losses:  0.08276863396167755 0.3675325810909271 0.8863319158554077
MemoryTrain:  epoch  6, batch     4 | loss: 1.3366332Losses:  0.09187990427017212 0.8779100179672241 0.8723663091659546
MemoryTrain:  epoch  7, batch     0 | loss: 1.8421562Losses:  0.41090989112854004 0.8268239498138428 0.9117433428764343
MemoryTrain:  epoch  7, batch     1 | loss: 2.1494772Losses:  0.5001817345619202 0.6737790107727051 0.9217842817306519
MemoryTrain:  epoch  7, batch     2 | loss: 2.0957451Losses:  0.05545215308666229 0.6503078937530518 0.8916430473327637
MemoryTrain:  epoch  7, batch     3 | loss: 1.5974030Losses:  0.06914626061916351 0.24932090938091278 0.9166755080223083
MemoryTrain:  epoch  7, batch     4 | loss: 1.2351427Losses:  0.42907869815826416 0.6582496166229248 0.8616527318954468
MemoryTrain:  epoch  8, batch     0 | loss: 1.9489810Losses:  0.0697159543633461 0.8075351715087891 0.8767992258071899
MemoryTrain:  epoch  8, batch     1 | loss: 1.7540504Losses:  0.46001461148262024 0.8629589676856995 0.9003683924674988
MemoryTrain:  epoch  8, batch     2 | loss: 2.2233419Losses:  0.04863463342189789 0.7602443695068359 0.9355295896530151
MemoryTrain:  epoch  8, batch     3 | loss: 1.7444086Losses:  0.060422174632549286 0.19766616821289062 0.9523605108261108
MemoryTrain:  epoch  8, batch     4 | loss: 1.2104489Losses:  0.3576204776763916 0.768298327922821 0.8710331916809082
MemoryTrain:  epoch  9, batch     0 | loss: 1.9969521Losses:  0.03738803416490555 0.6259430646896362 0.8830188512802124
MemoryTrain:  epoch  9, batch     1 | loss: 1.5463500Losses:  0.09141874313354492 0.9981168508529663 0.909709095954895
MemoryTrain:  epoch  9, batch     2 | loss: 1.9992447Losses:  0.3015211522579193 0.5983176827430725 0.933427631855011
MemoryTrain:  epoch  9, batch     3 | loss: 1.8332665Losses:  0.036581479012966156 0.24515049159526825 0.8797730207443237
MemoryTrain:  epoch  9, batch     4 | loss: 1.1615050
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 65.89%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 63.48%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 61.98%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 61.32%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 61.18%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 61.90%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 61.11%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 59.92%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 57.55%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 56.63%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 55.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 56.74%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 57.57%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 58.37%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 61.31%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 62.29%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 63.00%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.09%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.84%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.59%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 86.79%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 86.57%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 85.24%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 85.06%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.73%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 84.57%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 84.71%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 84.70%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 84.93%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 84.96%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 84.64%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 84.29%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 83.63%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 83.12%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 82.77%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 82.12%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 81.95%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 81.40%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 80.87%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 80.12%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 79.54%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 78.90%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 78.34%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 77.87%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 77.70%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.90%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 78.95%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 78.97%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 78.70%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 78.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 80.25%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 79.99%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 79.84%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 79.74%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 79.54%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 79.40%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 79.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.48%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 79.39%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 79.74%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 79.65%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 79.02%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 78.44%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 77.93%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 77.33%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 76.83%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 76.34%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 76.23%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.48%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 76.57%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.68%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 76.75%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:  144 | acc: 0.00%,  total acc: 76.03%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 75.51%   [EVAL] batch:  146 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 74.49%   [EVAL] batch:  148 | acc: 0.00%,  total acc: 73.99%   [EVAL] batch:  149 | acc: 0.00%,  total acc: 73.50%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 73.05%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 72.57%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 72.10%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 71.63%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 71.17%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.71%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 70.70%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.85%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 71.86%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 71.45%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 71.18%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 70.92%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 70.55%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 70.18%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.03%   [EVAL] batch:  176 | acc: 93.75%,  total acc: 70.16%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 70.19%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 70.01%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 69.84%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 69.73%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 69.62%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 69.45%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 69.47%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 69.51%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 69.35%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 69.47%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 69.40%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 69.33%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 69.30%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 69.30%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  210 | acc: 81.25%,  total acc: 69.85%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:  214 | acc: 12.50%,  total acc: 69.51%   [EVAL] batch:  215 | acc: 12.50%,  total acc: 69.24%   [EVAL] batch:  216 | acc: 6.25%,  total acc: 68.95%   [EVAL] batch:  217 | acc: 6.25%,  total acc: 68.66%   [EVAL] batch:  218 | acc: 18.75%,  total acc: 68.44%   [EVAL] batch:  219 | acc: 37.50%,  total acc: 68.30%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 68.10%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 67.60%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 67.33%   [EVAL] batch:  224 | acc: 18.75%,  total acc: 67.11%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 67.48%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 67.40%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 67.07%   [EVAL] batch:  235 | acc: 37.50%,  total acc: 66.95%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.25%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 68.25%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 68.21%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 68.17%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 68.08%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 68.14%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 68.06%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 67.99%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 67.78%   [EVAL] batch:  272 | acc: 50.00%,  total acc: 67.72%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 67.66%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 67.45%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 68.77%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 68.79%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:  291 | acc: 62.50%,  total acc: 68.73%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  293 | acc: 56.25%,  total acc: 68.73%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 68.67%   [EVAL] batch:  295 | acc: 31.25%,  total acc: 68.54%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 68.48%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 68.69%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 68.83%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 68.71%   [EVAL] batch:  307 | acc: 18.75%,  total acc: 68.55%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 68.45%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 68.25%   [EVAL] batch:  310 | acc: 18.75%,  total acc: 68.09%   [EVAL] batch:  311 | acc: 6.25%,  total acc: 67.89%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 67.66%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 67.54%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 67.41%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 67.20%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 67.08%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.42%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 67.27%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 67.09%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 66.92%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 66.77%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 66.65%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 66.50%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 66.55%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 67.97%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 67.84%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 67.64%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 67.48%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 67.42%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 67.77%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 67.74%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 67.63%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 67.47%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 67.49%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 67.42%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 67.29%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 67.14%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 67.06%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  379 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 67.07%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 67.16%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  396 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 67.31%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 67.28%   [EVAL] batch:  401 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  402 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:  405 | acc: 37.50%,  total acc: 67.20%   [EVAL] batch:  406 | acc: 31.25%,  total acc: 67.11%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 67.06%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 67.01%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  411 | acc: 37.50%,  total acc: 66.87%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 66.84%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 66.86%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  418 | acc: 50.00%,  total acc: 66.86%   [EVAL] batch:  419 | acc: 18.75%,  total acc: 66.74%   [EVAL] batch:  420 | acc: 6.25%,  total acc: 66.60%   [EVAL] batch:  421 | acc: 0.00%,  total acc: 66.44%   [EVAL] batch:  422 | acc: 6.25%,  total acc: 66.30%   [EVAL] batch:  423 | acc: 12.50%,  total acc: 66.17%   [EVAL] batch:  424 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 66.71%   
cur_acc:  ['0.9444', '0.8165', '0.6190', '0.7391', '0.7143', '0.6905', '0.6250']
his_acc:  ['0.9444', '0.8695', '0.7696', '0.7458', '0.7141', '0.6952', '0.6671']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 37 29  0  0 28 32 23  0  0 35 34  0  0  1
  1 19  0  0  0  0  5 22  0  0  0  2  0 18 27 20 26  0 10  0  9 25 36  0
  0  0  2 16  0  0 17  0  0  0 13 11  0  0  0 12  0 31  0 30 15 14  7  4
  1  0  0  8  0  6  0  3]
Losses:  6.824898719787598 1.5240118503570557 0.9593261480331421
CurrentTrain: epoch  0, batch     0 | loss: 9.3082361Losses:  5.913390159606934 1.3717817068099976 0.9678522944450378
CurrentTrain: epoch  0, batch     1 | loss: 8.2530241Losses:  6.6844482421875 1.5476694107055664 0.97991943359375
CurrentTrain: epoch  0, batch     2 | loss: 9.2120371Losses:  5.46003532409668 0.3871476948261261 1.0
CurrentTrain: epoch  0, batch     3 | loss: 6.8471832Losses:  5.697463035583496 1.1612011194229126 0.9565157890319824
CurrentTrain: epoch  1, batch     0 | loss: 7.8151798Losses:  6.009969711303711 1.2201354503631592 0.9491919279098511
CurrentTrain: epoch  1, batch     1 | loss: 8.1792974Losses:  4.257513046264648 1.177577257156372 0.9779701828956604
CurrentTrain: epoch  1, batch     2 | loss: 6.4130602Losses:  4.903807163238525 0.6426677107810974 0.9940418601036072
CurrentTrain: epoch  1, batch     3 | loss: 6.5405169Losses:  4.741668701171875 1.2717158794403076 0.9405674934387207
CurrentTrain: epoch  2, batch     0 | loss: 6.9539523Losses:  4.731479644775391 1.2394790649414062 0.9925532341003418
CurrentTrain: epoch  2, batch     1 | loss: 6.9635119Losses:  5.0640411376953125 1.3393808603286743 0.9585614204406738
CurrentTrain: epoch  2, batch     2 | loss: 7.3619833Losses:  3.0751028060913086 0.19880300760269165 0.8288497924804688
CurrentTrain: epoch  2, batch     3 | loss: 4.1027555Losses:  5.260052680969238 1.1434324979782104 0.9455889463424683
CurrentTrain: epoch  3, batch     0 | loss: 7.3490744Losses:  3.858886241912842 1.0622987747192383 0.9577664732933044
CurrentTrain: epoch  3, batch     1 | loss: 5.8789515Losses:  3.984062910079956 0.9125204086303711 0.9561284780502319
CurrentTrain: epoch  3, batch     2 | loss: 5.8527122Losses:  2.476154088973999 0.11490090191364288 0.9157742261886597
CurrentTrain: epoch  3, batch     3 | loss: 3.5068293Losses:  3.978846788406372 0.9769344925880432 0.9221038818359375
CurrentTrain: epoch  4, batch     0 | loss: 5.8778853Losses:  4.000448703765869 0.9946781396865845 0.9580535888671875
CurrentTrain: epoch  4, batch     1 | loss: 5.9531803Losses:  3.3809876441955566 1.072914481163025 0.9692859053611755
CurrentTrain: epoch  4, batch     2 | loss: 5.4231882Losses:  3.937225818634033 0.10138372331857681 0.9367039203643799
CurrentTrain: epoch  4, batch     3 | loss: 4.9753132Losses:  3.450462818145752 0.8716111183166504 0.9596205949783325
CurrentTrain: epoch  5, batch     0 | loss: 5.2816944Losses:  3.566736936569214 0.9462924003601074 0.9450740814208984
CurrentTrain: epoch  5, batch     1 | loss: 5.4581032Losses:  3.730938673019409 0.9985576272010803 0.9478931427001953
CurrentTrain: epoch  5, batch     2 | loss: 5.6773896Losses:  3.5975842475891113 0.14225971698760986 0.8790100812911987
CurrentTrain: epoch  5, batch     3 | loss: 4.6188540Losses:  3.5036163330078125 0.9963027238845825 0.9422926902770996
CurrentTrain: epoch  6, batch     0 | loss: 5.4422116Losses:  4.02297830581665 0.9259364604949951 0.9579700231552124
CurrentTrain: epoch  6, batch     1 | loss: 5.9068847Losses:  3.0039291381835938 0.8416071534156799 0.9293874502182007
CurrentTrain: epoch  6, batch     2 | loss: 4.7749238Losses:  3.2004034519195557 0.29210519790649414 1.0
CurrentTrain: epoch  6, batch     3 | loss: 4.4925089Losses:  3.216571807861328 0.8689123392105103 0.9215983152389526
CurrentTrain: epoch  7, batch     0 | loss: 5.0070825Losses:  3.714979648590088 1.0304245948791504 0.9692140817642212
CurrentTrain: epoch  7, batch     1 | loss: 5.7146182Losses:  3.2783713340759277 1.0210959911346436 0.9284418225288391
CurrentTrain: epoch  7, batch     2 | loss: 5.2279091Losses:  1.944896936416626 2.9802322387695312e-08 1.0
CurrentTrain: epoch  7, batch     3 | loss: 2.9448969Losses:  2.6757678985595703 0.8252974152565002 0.9238799810409546
CurrentTrain: epoch  8, batch     0 | loss: 4.4249454Losses:  4.050110816955566 0.901881217956543 0.953754186630249
CurrentTrain: epoch  8, batch     1 | loss: 5.9057465Losses:  3.0257883071899414 0.8785232305526733 0.9340590238571167
CurrentTrain: epoch  8, batch     2 | loss: 4.8383708Losses:  1.8414610624313354 0.10647861659526825 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.9479396Losses:  2.713573932647705 0.8666931986808777 0.9427008628845215
CurrentTrain: epoch  9, batch     0 | loss: 4.5229683Losses:  2.609076499938965 0.7614524364471436 0.9074404239654541
CurrentTrain: epoch  9, batch     1 | loss: 4.2779694Losses:  3.8772993087768555 0.9423229694366455 0.9463305473327637
CurrentTrain: epoch  9, batch     2 | loss: 5.7659526Losses:  1.7924596071243286 0.0 1.0
CurrentTrain: epoch  9, batch     3 | loss: 2.7924595
Losses:  0.806439220905304 0.9677807092666626 0.888224720954895
MemoryTrain:  epoch  0, batch     0 | loss: 2.6624446Losses:  1.2185667753219604 0.988644540309906 0.8896458745002747
MemoryTrain:  epoch  0, batch     1 | loss: 3.0968571Losses:  0.5757830142974854 0.5680358409881592 0.9270744323730469
MemoryTrain:  epoch  0, batch     2 | loss: 2.0708933Losses:  0.18763116002082825 0.8912249207496643 0.9202597141265869
MemoryTrain:  epoch  0, batch     3 | loss: 1.9991158Losses:  0.2263193130493164 0.8541995882987976 0.894908607006073
MemoryTrain:  epoch  0, batch     4 | loss: 1.9754276Losses:  0.8524229526519775 0.7356176376342773 0.8884460926055908
MemoryTrain:  epoch  1, batch     0 | loss: 2.4764867Losses:  0.7064360976219177 0.7271144390106201 0.9112779498100281
MemoryTrain:  epoch  1, batch     1 | loss: 2.3448286Losses:  0.5180262327194214 0.7503795623779297 0.9059637784957886
MemoryTrain:  epoch  1, batch     2 | loss: 2.1743696Losses:  0.8648546934127808 0.991402268409729 0.8923444151878357
MemoryTrain:  epoch  1, batch     3 | loss: 2.7486014Losses:  1.304997444152832 0.8695904612541199 0.9084531664848328
MemoryTrain:  epoch  1, batch     4 | loss: 3.0830412Losses:  0.43820539116859436 0.6979916095733643 0.9189826250076294
MemoryTrain:  epoch  2, batch     0 | loss: 2.0551796Losses:  0.27555012702941895 0.7516658306121826 0.9275121092796326
MemoryTrain:  epoch  2, batch     1 | loss: 1.9547281Losses:  0.34253305196762085 0.6777431964874268 0.8530752658843994
MemoryTrain:  epoch  2, batch     2 | loss: 1.8733516Losses:  0.42898833751678467 0.9925228953361511 0.9064728021621704
MemoryTrain:  epoch  2, batch     3 | loss: 2.3279839Losses:  0.6153601408004761 0.8637072443962097 0.8999382257461548
MemoryTrain:  epoch  2, batch     4 | loss: 2.3790054Losses:  0.14840751886367798 0.7675938606262207 0.9435092210769653
MemoryTrain:  epoch  3, batch     0 | loss: 1.8595107Losses:  0.23045432567596436 0.9095458388328552 0.8754512071609497
MemoryTrain:  epoch  3, batch     1 | loss: 2.0154514Losses:  0.4463465213775635 0.8593701124191284 0.8901559114456177
MemoryTrain:  epoch  3, batch     2 | loss: 2.1958725Losses:  0.44348201155662537 0.7101453542709351 0.8931716084480286
MemoryTrain:  epoch  3, batch     3 | loss: 2.0467989Losses:  0.15559467673301697 0.8151261210441589 0.903968334197998
MemoryTrain:  epoch  3, batch     4 | loss: 1.8746891Losses:  0.11710590869188309 0.7747642993927002 0.8863599300384521
MemoryTrain:  epoch  4, batch     0 | loss: 1.7782302Losses:  0.222752183675766 0.8348444104194641 0.8973519802093506
MemoryTrain:  epoch  4, batch     1 | loss: 1.9549485Losses:  0.5301234722137451 0.8344987630844116 0.9456819295883179
MemoryTrain:  epoch  4, batch     2 | loss: 2.3103042Losses:  0.16508901119232178 0.7738244533538818 0.8613849878311157
MemoryTrain:  epoch  4, batch     3 | loss: 1.8002985Losses:  0.09869736433029175 0.6549313068389893 0.9034947156906128
MemoryTrain:  epoch  4, batch     4 | loss: 1.6571233Losses:  0.18473130464553833 0.7675988674163818 0.878523588180542
MemoryTrain:  epoch  5, batch     0 | loss: 1.8308537Losses:  0.23584872484207153 0.8929066061973572 0.9086152911186218
MemoryTrain:  epoch  5, batch     1 | loss: 2.0373707Losses:  0.12394458055496216 0.7229119539260864 0.9245536923408508
MemoryTrain:  epoch  5, batch     2 | loss: 1.7714102Losses:  0.18747541308403015 0.8989347219467163 0.8996772170066833
MemoryTrain:  epoch  5, batch     3 | loss: 1.9860873Losses:  0.08051984012126923 0.5450024008750916 0.8805987238883972
MemoryTrain:  epoch  5, batch     4 | loss: 1.5061209Losses:  0.09297271817922592 0.7851371765136719 0.890714168548584
MemoryTrain:  epoch  6, batch     0 | loss: 1.7688241Losses:  0.16766925156116486 0.6683875918388367 0.8978374004364014
MemoryTrain:  epoch  6, batch     1 | loss: 1.7338942Losses:  0.09174469858407974 0.6298900842666626 0.8837870359420776
MemoryTrain:  epoch  6, batch     2 | loss: 1.6054218Losses:  0.16145165264606476 0.8650955557823181 0.9211211204528809
MemoryTrain:  epoch  6, batch     3 | loss: 1.9476683Losses:  0.09183011949062347 0.6986606121063232 0.89311683177948
MemoryTrain:  epoch  6, batch     4 | loss: 1.6836076Losses:  0.06304232031106949 0.7197463512420654 0.869079053401947
MemoryTrain:  epoch  7, batch     0 | loss: 1.6518677Losses:  0.0730181485414505 0.7399742603302002 0.9356367588043213
MemoryTrain:  epoch  7, batch     1 | loss: 1.7486291Losses:  0.22278280556201935 0.7059323787689209 0.8761495351791382
MemoryTrain:  epoch  7, batch     2 | loss: 1.8048646Losses:  0.08611463010311127 0.7140570282936096 0.9094951748847961
MemoryTrain:  epoch  7, batch     3 | loss: 1.7096668Losses:  0.13042305409908295 0.794700026512146 0.8925834894180298
MemoryTrain:  epoch  7, batch     4 | loss: 1.8177066Losses:  0.04223442077636719 0.5364044308662415 0.8957858681678772
MemoryTrain:  epoch  8, batch     0 | loss: 1.4744247Losses:  0.07720894366502762 0.6248741149902344 0.910415768623352
MemoryTrain:  epoch  8, batch     1 | loss: 1.6124988Losses:  0.07971036434173584 0.7927635908126831 0.8827553987503052
MemoryTrain:  epoch  8, batch     2 | loss: 1.7552294Losses:  0.07029743492603302 0.7591865062713623 0.8938392400741577
MemoryTrain:  epoch  8, batch     3 | loss: 1.7233231Losses:  0.09279011189937592 0.8133599758148193 0.8962435722351074
MemoryTrain:  epoch  8, batch     4 | loss: 1.8023937Losses:  0.09258085489273071 0.6644267439842224 0.9075225591659546
MemoryTrain:  epoch  9, batch     0 | loss: 1.6645302Losses:  0.07771284878253937 0.8124486207962036 0.8696558475494385
MemoryTrain:  epoch  9, batch     1 | loss: 1.7598174Losses:  0.050934478640556335 0.6480772495269775 0.927057683467865
MemoryTrain:  epoch  9, batch     2 | loss: 1.6260694Losses:  0.057493045926094055 0.673857569694519 0.9097243547439575
MemoryTrain:  epoch  9, batch     3 | loss: 1.6410749Losses:  0.07498251646757126 0.7525472640991211 0.8525729179382324
MemoryTrain:  epoch  9, batch     4 | loss: 1.6801027
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 60.56%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 58.75%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 56.85%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 57.42%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 59.74%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 60.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 63.87%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 63.81%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.92%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 63.59%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 62.37%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 62.25%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 61.89%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 61.54%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 61.32%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 61.36%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 61.27%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 61.40%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 61.75%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 62.29%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 63.00%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.43%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.07%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.22%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.27%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.30%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 86.20%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 86.00%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.57%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 84.98%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 84.32%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 83.96%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 83.61%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.47%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 83.43%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 83.49%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 83.73%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 83.61%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 82.95%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 82.13%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 81.68%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 81.08%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 80.41%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 79.83%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 79.36%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 78.90%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 78.61%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 78.01%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 77.47%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 76.91%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 76.20%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 75.29%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 74.85%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 74.43%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 75.79%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 75.85%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 75.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 77.77%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 77.52%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 77.39%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 77.32%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 77.14%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 77.01%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.00%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 77.12%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 77.52%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 77.55%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 76.93%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 76.38%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 75.78%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 75.19%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 74.62%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 74.05%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 74.01%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 74.45%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.50%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 74.33%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 74.20%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 74.16%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 73.94%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 73.82%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 73.48%   [EVAL] batch:  144 | acc: 0.00%,  total acc: 72.97%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 72.47%   [EVAL] batch:  146 | acc: 0.00%,  total acc: 71.98%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 71.49%   [EVAL] batch:  148 | acc: 0.00%,  total acc: 71.02%   [EVAL] batch:  149 | acc: 0.00%,  total acc: 70.54%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 70.12%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.65%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 69.20%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.31%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.87%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.87%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 69.08%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 68.68%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 68.35%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 68.03%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 67.67%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 67.32%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 67.12%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 67.10%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 67.09%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:  182 | acc: 37.50%,  total acc: 66.91%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:  184 | acc: 50.00%,  total acc: 66.72%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 66.60%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 66.49%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 66.65%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 66.64%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 66.49%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 66.22%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 66.05%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 66.78%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 66.65%   [EVAL] batch:  214 | acc: 12.50%,  total acc: 66.40%   [EVAL] batch:  215 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:  216 | acc: 6.25%,  total acc: 65.87%   [EVAL] batch:  217 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:  218 | acc: 18.75%,  total acc: 65.41%   [EVAL] batch:  219 | acc: 43.75%,  total acc: 65.31%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 65.16%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 64.89%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 64.71%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 64.45%   [EVAL] batch:  224 | acc: 12.50%,  total acc: 64.22%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 64.37%   [EVAL] batch:  227 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 64.70%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 64.51%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 64.34%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 64.33%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 64.27%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 64.31%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 65.91%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  257 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 65.61%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 65.46%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 65.50%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 65.52%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 65.32%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 65.27%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 65.03%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 64.90%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 64.78%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 64.59%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  277 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  288 | acc: 43.75%,  total acc: 65.94%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  290 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  291 | acc: 37.50%,  total acc: 65.82%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  293 | acc: 43.75%,  total acc: 65.77%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 65.70%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 65.56%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 65.51%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 65.47%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 65.64%   [EVAL] batch:  307 | acc: 12.50%,  total acc: 65.46%   [EVAL] batch:  308 | acc: 12.50%,  total acc: 65.29%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 65.10%   [EVAL] batch:  310 | acc: 12.50%,  total acc: 64.93%   [EVAL] batch:  311 | acc: 6.25%,  total acc: 64.74%   [EVAL] batch:  312 | acc: 18.75%,  total acc: 64.60%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 64.47%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 64.37%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 64.22%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 64.10%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 63.95%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 63.85%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 63.91%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 63.96%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 64.23%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 63.97%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 63.81%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 63.72%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 63.58%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 63.46%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 64.09%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 64.36%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.74%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 64.87%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 64.78%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 64.63%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 64.59%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 64.72%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 64.97%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 64.94%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 64.86%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 64.74%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 64.68%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 64.61%   [EVAL] batch:  371 | acc: 31.25%,  total acc: 64.52%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 64.46%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 64.47%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 64.43%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 64.36%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 64.26%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:  380 | acc: 31.25%,  total acc: 63.98%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 64.41%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 64.41%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 64.39%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 64.43%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 64.47%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  394 | acc: 50.00%,  total acc: 64.49%   [EVAL] batch:  395 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  396 | acc: 62.50%,  total acc: 64.52%   [EVAL] batch:  397 | acc: 56.25%,  total acc: 64.49%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  399 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 64.45%   [EVAL] batch:  401 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:  402 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 64.53%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 64.49%   [EVAL] batch:  405 | acc: 50.00%,  total acc: 64.46%   [EVAL] batch:  406 | acc: 37.50%,  total acc: 64.39%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 64.37%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 64.32%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 64.27%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 64.26%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 64.21%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 64.18%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  414 | acc: 62.50%,  total acc: 64.16%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 64.18%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 64.19%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 64.22%   [EVAL] batch:  419 | acc: 6.25%,  total acc: 64.08%   [EVAL] batch:  420 | acc: 0.00%,  total acc: 63.93%   [EVAL] batch:  421 | acc: 0.00%,  total acc: 63.77%   [EVAL] batch:  422 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:  423 | acc: 0.00%,  total acc: 63.47%   [EVAL] batch:  424 | acc: 0.00%,  total acc: 63.32%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 63.83%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 63.93%   [EVAL] batch:  433 | acc: 75.00%,  total acc: 63.95%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 64.03%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  437 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 64.07%   [EVAL] batch:  439 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 64.04%   [EVAL] batch:  441 | acc: 56.25%,  total acc: 64.03%   [EVAL] batch:  442 | acc: 62.50%,  total acc: 64.02%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 64.02%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 64.21%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  450 | acc: 18.75%,  total acc: 64.30%   [EVAL] batch:  451 | acc: 18.75%,  total acc: 64.20%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 64.09%   [EVAL] batch:  453 | acc: 31.25%,  total acc: 64.01%   [EVAL] batch:  454 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:  455 | acc: 18.75%,  total acc: 63.87%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 64.10%   [EVAL] batch:  464 | acc: 12.50%,  total acc: 63.99%   [EVAL] batch:  465 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  466 | acc: 12.50%,  total acc: 63.80%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 63.68%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 63.59%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 64.04%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 64.08%   [EVAL] batch:  478 | acc: 37.50%,  total acc: 64.03%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 64.05%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.07%   [EVAL] batch:  481 | acc: 62.50%,  total acc: 64.07%   [EVAL] batch:  482 | acc: 56.25%,  total acc: 64.05%   [EVAL] batch:  483 | acc: 43.75%,  total acc: 64.01%   [EVAL] batch:  484 | acc: 56.25%,  total acc: 63.99%   [EVAL] batch:  485 | acc: 25.00%,  total acc: 63.91%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:  487 | acc: 50.00%,  total acc: 63.88%   [EVAL] batch:  488 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:  489 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:  490 | acc: 50.00%,  total acc: 63.77%   [EVAL] batch:  491 | acc: 75.00%,  total acc: 63.80%   [EVAL] batch:  492 | acc: 43.75%,  total acc: 63.76%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:  494 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 63.95%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 64.01%   
cur_acc:  ['0.9444', '0.8165', '0.6190', '0.7391', '0.7143', '0.6905', '0.6250', '0.6300']
his_acc:  ['0.9444', '0.8695', '0.7696', '0.7458', '0.7141', '0.6952', '0.6671', '0.6401']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  9.995824813842773 1.883446455001831 1.0093495845794678
CurrentTrain: epoch  0, batch     0 | loss: 12.8886213Losses:  9.994134902954102 2.0905508995056152 0.9993505477905273
CurrentTrain: epoch  0, batch     1 | loss: 13.0840368Losses:  9.362184524536133 1.8849782943725586 0.9895043969154358
CurrentTrain: epoch  0, batch     2 | loss: 12.2366676Losses:  9.57973861694336 1.5144364833831787 0.992695689201355
CurrentTrain: epoch  0, batch     3 | loss: 12.0868711Losses:  9.512935638427734 1.7860586643218994 0.9938728213310242
CurrentTrain: epoch  0, batch     4 | loss: 12.2928667Losses:  8.40866470336914 1.701129674911499 1.0003583431243896
CurrentTrain: epoch  0, batch     5 | loss: 11.1101532Losses:  9.053506851196289 1.9861235618591309 0.9849781394004822
CurrentTrain: epoch  0, batch     6 | loss: 12.0246086Losses:  9.688907623291016 1.5319772958755493 0.985080361366272
CurrentTrain: epoch  0, batch     7 | loss: 12.2059660Losses:  8.440220832824707 1.513528823852539 0.9775530099868774
CurrentTrain: epoch  0, batch     8 | loss: 10.9313030Losses:  9.88398551940918 1.942018985748291 0.9973866939544678
CurrentTrain: epoch  0, batch     9 | loss: 12.8233910Losses:  8.901634216308594 1.7641527652740479 0.9846463203430176
CurrentTrain: epoch  0, batch    10 | loss: 11.6504326Losses:  9.1781644821167 1.5349152088165283 0.9756743907928467
CurrentTrain: epoch  0, batch    11 | loss: 11.6887541Losses:  8.70612907409668 1.3569384813308716 0.9869136810302734
CurrentTrain: epoch  0, batch    12 | loss: 11.0499811Losses:  9.525888442993164 1.6894505023956299 0.9830620884895325
CurrentTrain: epoch  0, batch    13 | loss: 12.1984005Losses:  8.567495346069336 1.3790193796157837 0.966686487197876
CurrentTrain: epoch  0, batch    14 | loss: 10.9132013Losses:  9.30734634399414 1.5294743776321411 0.9778909683227539
CurrentTrain: epoch  0, batch    15 | loss: 11.8147116Losses:  8.557291030883789 1.564760684967041 0.9789420366287231
CurrentTrain: epoch  0, batch    16 | loss: 11.1009932Losses:  9.473506927490234 1.4166280031204224 0.9814759492874146
CurrentTrain: epoch  0, batch    17 | loss: 11.8716106Losses:  8.566259384155273 1.684740662574768 0.9766252040863037
CurrentTrain: epoch  0, batch    18 | loss: 11.2276258Losses:  8.755681991577148 1.524388313293457 0.9747682809829712
CurrentTrain: epoch  0, batch    19 | loss: 11.2548389Losses:  8.732067108154297 1.297424077987671 0.9643195271492004
CurrentTrain: epoch  0, batch    20 | loss: 10.9938107Losses:  8.188599586486816 1.4450063705444336 0.9619253873825073
CurrentTrain: epoch  0, batch    21 | loss: 10.5955315Losses:  8.370006561279297 1.429556965827942 0.9820756912231445
CurrentTrain: epoch  0, batch    22 | loss: 10.7816391Losses:  9.112871170043945 1.1835589408874512 0.9522330164909363
CurrentTrain: epoch  0, batch    23 | loss: 11.2486639Losses:  8.06833267211914 1.428529143333435 0.9525653123855591
CurrentTrain: epoch  0, batch    24 | loss: 10.4494267Losses:  8.517436027526855 1.4476759433746338 0.9643263816833496
CurrentTrain: epoch  0, batch    25 | loss: 10.9294376Losses:  8.56790542602539 1.4537121057510376 0.9542291164398193
CurrentTrain: epoch  0, batch    26 | loss: 10.9758472Losses:  7.657472610473633 1.0908377170562744 0.9588890671730042
CurrentTrain: epoch  0, batch    27 | loss: 9.7071991Losses:  7.976588249206543 1.283889651298523 0.9424938559532166
CurrentTrain: epoch  0, batch    28 | loss: 10.2029715Losses:  7.046536445617676 1.215514898300171 0.9440340995788574
CurrentTrain: epoch  0, batch    29 | loss: 9.2060852Losses:  7.652336120605469 1.2906019687652588 0.94426429271698
CurrentTrain: epoch  0, batch    30 | loss: 9.8872023Losses:  7.638800621032715 1.2169103622436523 0.9456262588500977
CurrentTrain: epoch  0, batch    31 | loss: 9.8013372Losses:  8.466585159301758 1.3736299276351929 0.966361939907074
CurrentTrain: epoch  0, batch    32 | loss: 10.8065767Losses:  8.22674560546875 1.309448003768921 0.960913896560669
CurrentTrain: epoch  0, batch    33 | loss: 10.4971075Losses:  8.283394813537598 1.4352102279663086 0.9725412130355835
CurrentTrain: epoch  0, batch    34 | loss: 10.6911459Losses:  7.653134346008301 1.0499463081359863 0.9475429058074951
CurrentTrain: epoch  0, batch    35 | loss: 9.6506243Losses:  8.388556480407715 1.1410104036331177 0.9560205340385437
CurrentTrain: epoch  0, batch    36 | loss: 10.4855871Losses:  8.139019966125488 1.3143341541290283 0.9544373750686646
CurrentTrain: epoch  0, batch    37 | loss: 10.4077911Losses:  7.608304023742676 1.1607340574264526 0.9292321801185608
CurrentTrain: epoch  0, batch    38 | loss: 9.6982708Losses:  7.731285095214844 0.9805474281311035 0.929479718208313
CurrentTrain: epoch  0, batch    39 | loss: 9.6413116Losses:  7.470454216003418 1.1894018650054932 0.9479852914810181
CurrentTrain: epoch  0, batch    40 | loss: 9.6078415Losses:  8.268855094909668 0.8174088001251221 0.95189368724823
CurrentTrain: epoch  0, batch    41 | loss: 10.0381575Losses:  7.351840972900391 0.9838258028030396 0.9459695816040039
CurrentTrain: epoch  0, batch    42 | loss: 9.2816362Losses:  8.411107063293457 1.2137566804885864 0.9744148254394531
CurrentTrain: epoch  0, batch    43 | loss: 10.5992785Losses:  7.8843674659729 1.3289461135864258 0.956292986869812
CurrentTrain: epoch  0, batch    44 | loss: 10.1696072Losses:  7.437117099761963 1.2976213693618774 0.9504740238189697
CurrentTrain: epoch  0, batch    45 | loss: 9.6852121Losses:  8.478521347045898 1.3321914672851562 0.9599756002426147
CurrentTrain: epoch  0, batch    46 | loss: 10.7706881Losses:  7.5583086013793945 0.9563820362091064 0.9203693270683289
CurrentTrain: epoch  0, batch    47 | loss: 9.4350595Losses:  6.492889881134033 0.7634323835372925 0.9082726240158081
CurrentTrain: epoch  0, batch    48 | loss: 8.1645947Losses:  7.512109279632568 1.144627332687378 0.947394847869873
CurrentTrain: epoch  0, batch    49 | loss: 9.6041317Losses:  7.853167533874512 0.8703908920288086 0.9466640949249268
CurrentTrain: epoch  0, batch    50 | loss: 9.6702223Losses:  7.261867523193359 1.0561555624008179 0.9346232414245605
CurrentTrain: epoch  0, batch    51 | loss: 9.2526455Losses:  7.98490047454834 1.172074556350708 0.9538143277168274
CurrentTrain: epoch  0, batch    52 | loss: 10.1107893Losses:  8.26706314086914 0.9828970432281494 0.954470157623291
CurrentTrain: epoch  0, batch    53 | loss: 10.2044296Losses:  6.901370525360107 0.9967267513275146 0.9295352101325989
CurrentTrain: epoch  0, batch    54 | loss: 8.8276320Losses:  6.924121379852295 1.1185715198516846 0.9168001413345337
CurrentTrain: epoch  0, batch    55 | loss: 8.9594936Losses:  7.608222007751465 1.2685316801071167 0.9459633827209473
CurrentTrain: epoch  0, batch    56 | loss: 9.8227177Losses:  8.184617042541504 1.2622356414794922 0.9502418041229248
CurrentTrain: epoch  0, batch    57 | loss: 10.3970947Losses:  7.468550682067871 1.006171464920044 0.9589176177978516
CurrentTrain: epoch  0, batch    58 | loss: 9.4336395Losses:  6.80524206161499 1.122125267982483 0.9204436540603638
CurrentTrain: epoch  0, batch    59 | loss: 8.8478107Losses:  7.8333587646484375 1.1147998571395874 0.9293486475944519
CurrentTrain: epoch  0, batch    60 | loss: 9.8775072Losses:  6.951028823852539 0.9514170289039612 0.9190559387207031
CurrentTrain: epoch  0, batch    61 | loss: 8.8215017Losses:  6.521936416625977 0.8553531169891357 0.9267057180404663
CurrentTrain: epoch  0, batch    62 | loss: 8.3039951Losses:  7.212856769561768 0.853264570236206 0.9156113862991333
CurrentTrain: epoch  1, batch     0 | loss: 8.9817324Losses:  6.524712562561035 1.175973892211914 0.9160006642341614
CurrentTrain: epoch  1, batch     1 | loss: 8.6166868Losses:  7.6905412673950195 0.8809412717819214 0.9288854598999023
CurrentTrain: epoch  1, batch     2 | loss: 9.5003681Losses:  7.131966590881348 0.7595599889755249 0.9167873859405518
CurrentTrain: epoch  1, batch     3 | loss: 8.8083143Losses:  7.60850191116333 1.1474522352218628 0.9289166927337646
CurrentTrain: epoch  1, batch     4 | loss: 9.6848707Losses:  7.073789596557617 0.9605448246002197 0.9415895342826843
CurrentTrain: epoch  1, batch     5 | loss: 8.9759235Losses:  7.387662887573242 0.8247101306915283 0.9006392955780029
CurrentTrain: epoch  1, batch     6 | loss: 9.1130123Losses:  7.35084342956543 0.8241480588912964 0.9342825412750244
CurrentTrain: epoch  1, batch     7 | loss: 9.1092739Losses:  7.809782981872559 1.045914888381958 0.9456545114517212
CurrentTrain: epoch  1, batch     8 | loss: 9.8013525Losses:  7.330563545227051 0.9898847341537476 0.9162770509719849
CurrentTrain: epoch  1, batch     9 | loss: 9.2367249Losses:  6.688793182373047 0.8850865960121155 0.9061099290847778
CurrentTrain: epoch  1, batch    10 | loss: 8.4799900Losses:  6.272719383239746 0.7449228167533875 0.9167313575744629
CurrentTrain: epoch  1, batch    11 | loss: 7.9343734Losses:  6.885935306549072 0.9030910134315491 0.9282085299491882
CurrentTrain: epoch  1, batch    12 | loss: 8.7172346Losses:  6.958148956298828 0.6997946500778198 0.910738468170166
CurrentTrain: epoch  1, batch    13 | loss: 8.5686817Losses:  6.951274871826172 0.8301472663879395 0.9080187082290649
CurrentTrain: epoch  1, batch    14 | loss: 8.6894407Losses:  6.910240650177002 0.6964800357818604 0.9458774328231812
CurrentTrain: epoch  1, batch    15 | loss: 8.5525980Losses:  6.8925580978393555 0.8552491664886475 0.9415092468261719
CurrentTrain: epoch  1, batch    16 | loss: 8.6893167Losses:  6.001474380493164 0.6025195121765137 0.8791771531105042
CurrentTrain: epoch  1, batch    17 | loss: 7.4831710Losses:  6.8088154792785645 1.0278980731964111 0.9126622676849365
CurrentTrain: epoch  1, batch    18 | loss: 8.7493763Losses:  6.432524681091309 0.9621894359588623 0.8995240926742554
CurrentTrain: epoch  1, batch    19 | loss: 8.2942381Losses:  6.863611221313477 0.8791465759277344 0.9361712336540222
CurrentTrain: epoch  1, batch    20 | loss: 8.6789293Losses:  5.933867454528809 0.5387415885925293 0.9281297922134399
CurrentTrain: epoch  1, batch    21 | loss: 7.4007387Losses:  6.2919111251831055 0.9724684953689575 0.9217514991760254
CurrentTrain: epoch  1, batch    22 | loss: 8.1861305Losses:  6.7294182777404785 0.7241831421852112 0.9123216271400452
CurrentTrain: epoch  1, batch    23 | loss: 8.3659229Losses:  6.693649768829346 0.9881516098976135 0.9054826498031616
CurrentTrain: epoch  1, batch    24 | loss: 8.5872841Losses:  6.089752674102783 0.7275837063789368 0.9107615947723389
CurrentTrain: epoch  1, batch    25 | loss: 7.7280979Losses:  6.422016620635986 0.658353328704834 0.8888841271400452
CurrentTrain: epoch  1, batch    26 | loss: 7.9692540Losses:  7.301061630249023 0.7090404033660889 0.9540408849716187
CurrentTrain: epoch  1, batch    27 | loss: 8.9641428Losses:  6.538609504699707 0.9311856031417847 0.9045315980911255
CurrentTrain: epoch  1, batch    28 | loss: 8.3743267Losses:  6.837315082550049 0.8807731866836548 0.9481666088104248
CurrentTrain: epoch  1, batch    29 | loss: 8.6662550Losses:  6.598084926605225 0.787335216999054 0.9384918212890625
CurrentTrain: epoch  1, batch    30 | loss: 8.3239117Losses:  5.877918243408203 0.5398750305175781 0.9166179299354553
CurrentTrain: epoch  1, batch    31 | loss: 7.3344111Losses:  6.335370063781738 0.7272921204566956 0.9523122310638428
CurrentTrain: epoch  1, batch    32 | loss: 8.0149746Losses:  6.546439170837402 0.8807545304298401 0.885177731513977
CurrentTrain: epoch  1, batch    33 | loss: 8.3123713Losses:  6.010505676269531 0.6773284673690796 0.9065443277359009
CurrentTrain: epoch  1, batch    34 | loss: 7.5943785Losses:  7.460446834564209 0.897434413433075 0.928942084312439
CurrentTrain: epoch  1, batch    35 | loss: 9.2868233Losses:  5.982887268066406 0.6686148643493652 0.919084370136261
CurrentTrain: epoch  1, batch    36 | loss: 7.5705867Losses:  7.735409259796143 0.7565473318099976 0.9032714366912842
CurrentTrain: epoch  1, batch    37 | loss: 9.3952284Losses:  6.62209415435791 0.6509996652603149 0.8992295861244202
CurrentTrain: epoch  1, batch    38 | loss: 8.1723232Losses:  6.338932037353516 0.568122923374176 0.9047116041183472
CurrentTrain: epoch  1, batch    39 | loss: 7.8117666Losses:  6.110301971435547 0.6830055713653564 0.9035930037498474
CurrentTrain: epoch  1, batch    40 | loss: 7.6969004Losses:  6.400755882263184 0.6520015001296997 0.8649771809577942
CurrentTrain: epoch  1, batch    41 | loss: 7.9177346Losses:  6.496040344238281 0.7279043793678284 0.8899498581886292
CurrentTrain: epoch  1, batch    42 | loss: 8.1138945Losses:  6.2865071296691895 0.6699146032333374 0.8968778848648071
CurrentTrain: epoch  1, batch    43 | loss: 7.8532996Losses:  6.552955627441406 0.6928231716156006 0.9387398958206177
CurrentTrain: epoch  1, batch    44 | loss: 8.1845188Losses:  6.167010307312012 0.5947281122207642 0.9399423003196716
CurrentTrain: epoch  1, batch    45 | loss: 7.7016807Losses:  5.900079727172852 0.754798412322998 0.8828157186508179
CurrentTrain: epoch  1, batch    46 | loss: 7.5376940Losses:  7.543273448944092 0.8688512444496155 0.9133384823799133
CurrentTrain: epoch  1, batch    47 | loss: 9.3254633Losses:  5.963149547576904 0.6788382530212402 0.9081665277481079
CurrentTrain: epoch  1, batch    48 | loss: 7.5501542Losses:  5.834878921508789 0.6588889956474304 0.9135614633560181
CurrentTrain: epoch  1, batch    49 | loss: 7.4073291Losses:  6.268911838531494 0.6992291808128357 0.9189330339431763
CurrentTrain: epoch  1, batch    50 | loss: 7.8870740Losses:  5.8149871826171875 0.6328049898147583 0.8665533065795898
CurrentTrain: epoch  1, batch    51 | loss: 7.3143454Losses:  5.4765424728393555 0.6006681323051453 0.8612356185913086
CurrentTrain: epoch  1, batch    52 | loss: 6.9384460Losses:  5.592423915863037 0.6228734254837036 0.8871473670005798
CurrentTrain: epoch  1, batch    53 | loss: 7.1024446Losses:  5.948128700256348 0.7229973673820496 0.8777023553848267
CurrentTrain: epoch  1, batch    54 | loss: 7.5488281Losses:  5.359769821166992 0.637519359588623 0.8944911956787109
CurrentTrain: epoch  1, batch    55 | loss: 6.8917804Losses:  5.745627403259277 0.6538456678390503 0.8934376239776611
CurrentTrain: epoch  1, batch    56 | loss: 7.2929106Losses:  5.593319892883301 0.6414162516593933 0.8953924775123596
CurrentTrain: epoch  1, batch    57 | loss: 7.1301284Losses:  6.429643630981445 0.5231707692146301 0.9320847988128662
CurrentTrain: epoch  1, batch    58 | loss: 7.8848991Losses:  6.300946235656738 0.6046428084373474 0.9023571610450745
CurrentTrain: epoch  1, batch    59 | loss: 7.8079462Losses:  4.859182357788086 0.5651000738143921 0.8359294533729553
CurrentTrain: epoch  1, batch    60 | loss: 6.2602119Losses:  5.360349655151367 0.46037590503692627 0.8819380402565002
CurrentTrain: epoch  1, batch    61 | loss: 6.7026634Losses:  4.89547061920166 0.27255138754844666 0.9166147112846375
CurrentTrain: epoch  1, batch    62 | loss: 6.0846367Losses:  6.087324142456055 0.6478715538978577 0.8976565599441528
CurrentTrain: epoch  2, batch     0 | loss: 7.6328521Losses:  5.839664459228516 0.6641960740089417 0.8673887252807617
CurrentTrain: epoch  2, batch     1 | loss: 7.3712492Losses:  5.91300630569458 0.5234004259109497 0.8804010152816772
CurrentTrain: epoch  2, batch     2 | loss: 7.3168077Losses:  5.92810583114624 0.524093508720398 0.881249189376831
CurrentTrain: epoch  2, batch     3 | loss: 7.3334484Losses:  5.144805908203125 0.5817989110946655 0.8372746109962463
CurrentTrain: epoch  2, batch     4 | loss: 6.5638795Losses:  6.491364002227783 0.5199745297431946 0.9236275553703308
CurrentTrain: epoch  2, batch     5 | loss: 7.9349661Losses:  5.870876312255859 0.6109070777893066 0.9008468389511108
CurrentTrain: epoch  2, batch     6 | loss: 7.3826303Losses:  5.203330993652344 0.5346389412879944 0.867699384689331
CurrentTrain: epoch  2, batch     7 | loss: 6.6056690Losses:  4.932651996612549 0.3739999532699585 0.8843101263046265
CurrentTrain: epoch  2, batch     8 | loss: 6.1909623Losses:  5.34152889251709 0.3584393560886383 0.866946816444397
CurrentTrain: epoch  2, batch     9 | loss: 6.5669150Losses:  5.306512355804443 0.4172999858856201 0.820641279220581
CurrentTrain: epoch  2, batch    10 | loss: 6.5444536Losses:  5.26844596862793 0.58611661195755 0.8860416412353516
CurrentTrain: epoch  2, batch    11 | loss: 6.7406044Losses:  6.049245357513428 0.46450167894363403 0.8871520757675171
CurrentTrain: epoch  2, batch    12 | loss: 7.4008994Losses:  5.750102996826172 0.5426973104476929 0.8809959888458252
CurrentTrain: epoch  2, batch    13 | loss: 7.1737967Losses:  4.997754096984863 0.3801867961883545 0.8417794704437256
CurrentTrain: epoch  2, batch    14 | loss: 6.2197208Losses:  5.3261942863464355 0.5304925441741943 0.8647575378417969
CurrentTrain: epoch  2, batch    15 | loss: 6.7214441Losses:  5.91956901550293 0.5865789651870728 0.8726074695587158
CurrentTrain: epoch  2, batch    16 | loss: 7.3787556Losses:  5.462263107299805 0.5788761377334595 0.874437689781189
CurrentTrain: epoch  2, batch    17 | loss: 6.9155769Losses:  5.197514533996582 0.5012365579605103 0.8802392482757568
CurrentTrain: epoch  2, batch    18 | loss: 6.5789900Losses:  5.49635648727417 0.5352281332015991 0.8556365370750427
CurrentTrain: epoch  2, batch    19 | loss: 6.8872213Losses:  6.129192352294922 0.6099382042884827 0.8649546504020691
CurrentTrain: epoch  2, batch    20 | loss: 7.6040850Losses:  5.310194969177246 0.404568612575531 0.866435706615448
CurrentTrain: epoch  2, batch    21 | loss: 6.5811992Losses:  5.685513496398926 0.5227389931678772 0.9145970344543457
CurrentTrain: epoch  2, batch    22 | loss: 7.1228495Losses:  5.439526557922363 0.4809671938419342 0.8568409085273743
CurrentTrain: epoch  2, batch    23 | loss: 6.7773347Losses:  5.612874507904053 0.39931124448776245 0.866565465927124
CurrentTrain: epoch  2, batch    24 | loss: 6.8787508Losses:  5.124917984008789 0.5295333862304688 0.8947583436965942
CurrentTrain: epoch  2, batch    25 | loss: 6.5492096Losses:  5.743190765380859 0.565384030342102 0.8978223204612732
CurrentTrain: epoch  2, batch    26 | loss: 7.2063971Losses:  4.972280979156494 0.42249608039855957 0.8206974864006042
CurrentTrain: epoch  2, batch    27 | loss: 6.2154746Losses:  5.715828895568848 0.4214152693748474 0.9049533605575562
CurrentTrain: epoch  2, batch    28 | loss: 7.0421977Losses:  5.83815860748291 0.45751333236694336 0.8425523042678833
CurrentTrain: epoch  2, batch    29 | loss: 7.1382241Losses:  5.295172691345215 0.4741548001766205 0.850230872631073
CurrentTrain: epoch  2, batch    30 | loss: 6.6195583Losses:  5.507665157318115 0.4418264925479889 0.8631366491317749
CurrentTrain: epoch  2, batch    31 | loss: 6.8126283Losses:  5.315904140472412 0.33602774143218994 0.8771349787712097
CurrentTrain: epoch  2, batch    32 | loss: 6.5290666Losses:  5.226112365722656 0.3844650983810425 0.8917132019996643
CurrentTrain: epoch  2, batch    33 | loss: 6.5022907Losses:  5.186078071594238 0.38273385167121887 0.8784569501876831
CurrentTrain: epoch  2, batch    34 | loss: 6.4472690Losses:  5.255958080291748 0.4543892443180084 0.8740148544311523
CurrentTrain: epoch  2, batch    35 | loss: 6.5843620Losses:  5.68145751953125 0.5955538749694824 0.8906376361846924
CurrentTrain: epoch  2, batch    36 | loss: 7.1676493Losses:  5.081243515014648 0.5068371295928955 0.8792573809623718
CurrentTrain: epoch  2, batch    37 | loss: 6.4673376Losses:  5.276127815246582 0.49597424268722534 0.8558831810951233
CurrentTrain: epoch  2, batch    38 | loss: 6.6279850Losses:  4.872220993041992 0.34405022859573364 0.8102253675460815
CurrentTrain: epoch  2, batch    39 | loss: 6.0264969Losses:  5.192560195922852 0.38896870613098145 0.8660748600959778
CurrentTrain: epoch  2, batch    40 | loss: 6.4476037Losses:  5.124571800231934 0.3717241883277893 0.8960292339324951
CurrentTrain: epoch  2, batch    41 | loss: 6.3923254Losses:  5.341967582702637 0.5332583785057068 0.8787375688552856
CurrentTrain: epoch  2, batch    42 | loss: 6.7539635Losses:  5.125889778137207 0.27799296379089355 0.8135087490081787
CurrentTrain: epoch  2, batch    43 | loss: 6.2173920Losses:  5.11591100692749 0.41018831729888916 0.8162561655044556
CurrentTrain: epoch  2, batch    44 | loss: 6.3423553Losses:  5.213034629821777 0.4176906645298004 0.9041422605514526
CurrentTrain: epoch  2, batch    45 | loss: 6.5348678Losses:  5.866390228271484 0.5260087251663208 0.9285681247711182
CurrentTrain: epoch  2, batch    46 | loss: 7.3209667Losses:  4.861884117126465 0.3804319202899933 0.8792178630828857
CurrentTrain: epoch  2, batch    47 | loss: 6.1215343Losses:  4.985334396362305 0.39132195711135864 0.8731426000595093
CurrentTrain: epoch  2, batch    48 | loss: 6.2497993Losses:  5.18402099609375 0.4020228981971741 0.8494701385498047
CurrentTrain: epoch  2, batch    49 | loss: 6.4355140Losses:  6.284798622131348 0.4517023265361786 0.8977992534637451
CurrentTrain: epoch  2, batch    50 | loss: 7.6343002Losses:  5.300913333892822 0.4024994969367981 0.9003193974494934
CurrentTrain: epoch  2, batch    51 | loss: 6.6037326Losses:  5.070894241333008 0.48854196071624756 0.8578923344612122
CurrentTrain: epoch  2, batch    52 | loss: 6.4173288Losses:  5.698481559753418 0.4139770269393921 0.8346554040908813
CurrentTrain: epoch  2, batch    53 | loss: 6.9471140Losses:  5.209575653076172 0.40645259618759155 0.8223703503608704
CurrentTrain: epoch  2, batch    54 | loss: 6.4383988Losses:  5.182392120361328 0.3497732877731323 0.8528027534484863
CurrentTrain: epoch  2, batch    55 | loss: 6.3849683Losses:  5.587084770202637 0.42371198534965515 0.9185196161270142
CurrentTrain: epoch  2, batch    56 | loss: 6.9293160Losses:  5.459595680236816 0.44850143790245056 0.8590431809425354
CurrentTrain: epoch  2, batch    57 | loss: 6.7671404Losses:  5.3965349197387695 0.262057900428772 0.8079014420509338
CurrentTrain: epoch  2, batch    58 | loss: 6.4664941Losses:  5.263745307922363 0.22420591115951538 0.8621913194656372
CurrentTrain: epoch  2, batch    59 | loss: 6.3501425Losses:  5.176924228668213 0.32639068365097046 0.8240349888801575
CurrentTrain: epoch  2, batch    60 | loss: 6.3273501Losses:  4.934138298034668 0.39287233352661133 0.8918204307556152
CurrentTrain: epoch  2, batch    61 | loss: 6.2188311Losses:  4.848813056945801 0.38625046610832214 0.8189705610275269
CurrentTrain: epoch  2, batch    62 | loss: 6.0540342Losses:  4.8172221183776855 0.422804594039917 0.8464010953903198
CurrentTrain: epoch  3, batch     0 | loss: 6.0864277Losses:  5.095364570617676 0.42180782556533813 0.8874316811561584
CurrentTrain: epoch  3, batch     1 | loss: 6.4046040Losses:  5.243391036987305 0.2659167945384979 0.8698999881744385
CurrentTrain: epoch  3, batch     2 | loss: 6.3792076Losses:  5.284201622009277 0.30640465021133423 0.8933812975883484
CurrentTrain: epoch  3, batch     3 | loss: 6.4839873Losses:  4.693880081176758 0.34586289525032043 0.8036631941795349
CurrentTrain: epoch  3, batch     4 | loss: 5.8434062Losses:  4.893683433532715 0.3451434075832367 0.7983543872833252
CurrentTrain: epoch  3, batch     5 | loss: 6.0371809Losses:  4.751987457275391 0.37333226203918457 0.855755090713501
CurrentTrain: epoch  3, batch     6 | loss: 5.9810743Losses:  5.4289631843566895 0.3648242950439453 0.830729603767395
CurrentTrain: epoch  3, batch     7 | loss: 6.6245170Losses:  4.8274383544921875 0.3572767376899719 0.8782671689987183
CurrentTrain: epoch  3, batch     8 | loss: 6.0629826Losses:  4.829428672790527 0.4069073796272278 0.8057680130004883
CurrentTrain: epoch  3, batch     9 | loss: 6.0421042Losses:  4.735891342163086 0.3627651333808899 0.8506395816802979
CurrentTrain: epoch  3, batch    10 | loss: 5.9492960Losses:  5.211734294891357 0.4658105969429016 0.871246874332428
CurrentTrain: epoch  3, batch    11 | loss: 6.5487919Losses:  4.558209419250488 0.3040790855884552 0.7962489128112793
CurrentTrain: epoch  3, batch    12 | loss: 5.6585374Losses:  4.684796333312988 0.39637166261672974 0.8472645282745361
CurrentTrain: epoch  3, batch    13 | loss: 5.9284325Losses:  4.593312740325928 0.31572097539901733 0.8722676634788513
CurrentTrain: epoch  3, batch    14 | loss: 5.7813015Losses:  4.762146949768066 0.2667232155799866 0.8515670299530029
CurrentTrain: epoch  3, batch    15 | loss: 5.8804369Losses:  4.818621635437012 0.2945585250854492 0.8906763792037964
CurrentTrain: epoch  3, batch    16 | loss: 6.0038567Losses:  5.127138137817383 0.4034195840358734 0.8462862968444824
CurrentTrain: epoch  3, batch    17 | loss: 6.3768439Losses:  5.440882682800293 0.4929661154747009 0.8243422508239746
CurrentTrain: epoch  3, batch    18 | loss: 6.7581911Losses:  4.611575603485107 0.3238043487071991 0.8213409185409546
CurrentTrain: epoch  3, batch    19 | loss: 5.7567210Losses:  4.738249778747559 0.391095906496048 0.854702353477478
CurrentTrain: epoch  3, batch    20 | loss: 5.9840484Losses:  4.605356693267822 0.38964396715164185 0.850741982460022
CurrentTrain: epoch  3, batch    21 | loss: 5.8457427Losses:  4.5626020431518555 0.3976127505302429 0.8179292678833008
CurrentTrain: epoch  3, batch    22 | loss: 5.7781439Losses:  4.700664520263672 0.22696717083454132 0.8387376666069031
CurrentTrain: epoch  3, batch    23 | loss: 5.7663693Losses:  4.998745918273926 0.35504165291786194 0.8367720246315002
CurrentTrain: epoch  3, batch    24 | loss: 6.1905594Losses:  4.364221572875977 0.18753674626350403 0.8232852220535278
CurrentTrain: epoch  3, batch    25 | loss: 5.3750434Losses:  4.641108512878418 0.3620954751968384 0.8033754825592041
CurrentTrain: epoch  3, batch    26 | loss: 5.8065796Losses:  5.001743316650391 0.4299517869949341 0.8325111865997314
CurrentTrain: epoch  3, batch    27 | loss: 6.2642059Losses:  4.517903804779053 0.3229394257068634 0.7926903963088989
CurrentTrain: epoch  3, batch    28 | loss: 5.6335335Losses:  5.0747294425964355 0.37721648812294006 0.8420330882072449
CurrentTrain: epoch  3, batch    29 | loss: 6.2939787Losses:  4.807676792144775 0.346040815114975 0.8760968446731567
CurrentTrain: epoch  3, batch    30 | loss: 6.0298142Losses:  4.513874053955078 0.2530631422996521 0.8781517744064331
CurrentTrain: epoch  3, batch    31 | loss: 5.6450891Losses:  4.8990020751953125 0.30516692996025085 0.8743598461151123
CurrentTrain: epoch  3, batch    32 | loss: 6.0785284Losses:  4.588289260864258 0.2880862057209015 0.8200889825820923
CurrentTrain: epoch  3, batch    33 | loss: 5.6964645Losses:  4.507062911987305 0.3467252850532532 0.8247716426849365
CurrentTrain: epoch  3, batch    34 | loss: 5.6785603Losses:  4.622018814086914 0.18430164456367493 0.825716495513916
CurrentTrain: epoch  3, batch    35 | loss: 5.6320372Losses:  5.204182147979736 0.34499046206474304 0.7972947359085083
CurrentTrain: epoch  3, batch    36 | loss: 6.3464670Losses:  5.226929664611816 0.3381790220737457 0.818784236907959
CurrentTrain: epoch  3, batch    37 | loss: 6.3838930Losses:  5.6548566818237305 0.4531458616256714 0.8302936553955078
CurrentTrain: epoch  3, batch    38 | loss: 6.9382963Losses:  4.530837535858154 0.33377623558044434 0.809354305267334
CurrentTrain: epoch  3, batch    39 | loss: 5.6739678Losses:  6.036733627319336 0.5321931838989258 0.8965965509414673
CurrentTrain: epoch  3, batch    40 | loss: 7.4655232Losses:  4.74783992767334 0.2443351447582245 0.8456811904907227
CurrentTrain: epoch  3, batch    41 | loss: 5.8378563Losses:  5.031474590301514 0.3006300628185272 0.8275951743125916
CurrentTrain: epoch  3, batch    42 | loss: 6.1596999Losses:  4.721272945404053 0.26392877101898193 0.874572217464447
CurrentTrain: epoch  3, batch    43 | loss: 5.8597741Losses:  4.6508378982543945 0.2539629638195038 0.8607950210571289
CurrentTrain: epoch  3, batch    44 | loss: 5.7655959Losses:  4.747507572174072 0.2121107578277588 0.8462803363800049
CurrentTrain: epoch  3, batch    45 | loss: 5.8058987Losses:  4.742520332336426 0.3022412657737732 0.8408166170120239
CurrentTrain: epoch  3, batch    46 | loss: 5.8855782Losses:  4.865321159362793 0.3108065724372864 0.8414145708084106
CurrentTrain: epoch  3, batch    47 | loss: 6.0175424Losses:  4.576318740844727 0.3208768367767334 0.8125276565551758
CurrentTrain: epoch  3, batch    48 | loss: 5.7097235Losses:  4.7965474128723145 0.30624139308929443 0.8263962268829346
CurrentTrain: epoch  3, batch    49 | loss: 5.9291849Losses:  5.294931411743164 0.2670230269432068 0.830469012260437
CurrentTrain: epoch  3, batch    50 | loss: 6.3924236Losses:  4.621828079223633 0.29644376039505005 0.8587408065795898
CurrentTrain: epoch  3, batch    51 | loss: 5.7770128Losses:  4.690184116363525 0.3286852240562439 0.7768524885177612
CurrentTrain: epoch  3, batch    52 | loss: 5.7957220Losses:  4.893192768096924 0.27953824400901794 0.8644120097160339
CurrentTrain: epoch  3, batch    53 | loss: 6.0371428Losses:  4.670323371887207 0.32562822103500366 0.8147273063659668
CurrentTrain: epoch  3, batch    54 | loss: 5.8106790Losses:  4.320132255554199 0.17817026376724243 0.8772048950195312
CurrentTrain: epoch  3, batch    55 | loss: 5.3755074Losses:  4.266949653625488 0.1949155330657959 0.8473418951034546
CurrentTrain: epoch  3, batch    56 | loss: 5.3092074Losses:  4.430841445922852 0.21901026368141174 0.8167506456375122
CurrentTrain: epoch  3, batch    57 | loss: 5.4666023Losses:  5.790607452392578 0.3191791772842407 0.7942416667938232
CurrentTrain: epoch  3, batch    58 | loss: 6.9040279Losses:  4.392092704772949 0.2722903788089752 0.8502676486968994
CurrentTrain: epoch  3, batch    59 | loss: 5.5146503Losses:  4.321506977081299 0.27531731128692627 0.8170311450958252
CurrentTrain: epoch  3, batch    60 | loss: 5.4138556Losses:  5.302968502044678 0.3611573576927185 0.844059407711029
CurrentTrain: epoch  3, batch    61 | loss: 6.5081854Losses:  4.711299896240234 0.256692111492157 0.8586550951004028
CurrentTrain: epoch  3, batch    62 | loss: 5.8266468Losses:  4.712474822998047 0.2801186740398407 0.7810896039009094
CurrentTrain: epoch  4, batch     0 | loss: 5.7736831Losses:  4.4034423828125 0.24039384722709656 0.7955708503723145
CurrentTrain: epoch  4, batch     1 | loss: 5.4394069Losses:  4.617527484893799 0.20779019594192505 0.8402413725852966
CurrentTrain: epoch  4, batch     2 | loss: 5.6655593Losses:  4.655546188354492 0.3254189193248749 0.7862787246704102
CurrentTrain: epoch  4, batch     3 | loss: 5.7672439Losses:  4.5337114334106445 0.28593510389328003 0.7751644849777222
CurrentTrain: epoch  4, batch     4 | loss: 5.5948110Losses:  4.39188814163208 0.22138164937496185 0.7367525100708008
CurrentTrain: epoch  4, batch     5 | loss: 5.3500223Losses:  4.651564598083496 0.3236122131347656 0.8438330888748169
CurrentTrain: epoch  4, batch     6 | loss: 5.8190098Losses:  4.829014778137207 0.2592109441757202 0.8283361196517944
CurrentTrain: epoch  4, batch     7 | loss: 5.9165621Losses:  4.468719005584717 0.26248568296432495 0.8309742212295532
CurrentTrain: epoch  4, batch     8 | loss: 5.5621786Losses:  4.702138423919678 0.30081498622894287 0.8332346677780151
CurrentTrain: epoch  4, batch     9 | loss: 5.8361883Losses:  4.35450553894043 0.1818402111530304 0.811786413192749
CurrentTrain: epoch  4, batch    10 | loss: 5.3481321Losses:  4.806901931762695 0.3609694242477417 0.7957944869995117
CurrentTrain: epoch  4, batch    11 | loss: 5.9636660Losses:  4.927757263183594 0.2489229291677475 0.7765834927558899
CurrentTrain: epoch  4, batch    12 | loss: 5.9532638Losses:  4.476909160614014 0.2455071657896042 0.8390119671821594
CurrentTrain: epoch  4, batch    13 | loss: 5.5614285Losses:  4.788544178009033 0.2984071969985962 0.7906094193458557
CurrentTrain: epoch  4, batch    14 | loss: 5.8775606Losses:  5.407880783081055 0.31119009852409363 0.8625211715698242
CurrentTrain: epoch  4, batch    15 | loss: 6.5815921Losses:  4.62288236618042 0.25359225273132324 0.8255143165588379
CurrentTrain: epoch  4, batch    16 | loss: 5.7019887Losses:  4.362253189086914 0.24534372985363007 0.8569624423980713
CurrentTrain: epoch  4, batch    17 | loss: 5.4645596Losses:  4.825092792510986 0.25282734632492065 0.8116586208343506
CurrentTrain: epoch  4, batch    18 | loss: 5.8895788Losses:  4.62459659576416 0.31341394782066345 0.7912206649780273
CurrentTrain: epoch  4, batch    19 | loss: 5.7292314Losses:  4.398097038269043 0.23458331823349 0.8540617823600769
CurrentTrain: epoch  4, batch    20 | loss: 5.4867420Losses:  4.468061447143555 0.18628568947315216 0.922420859336853
CurrentTrain: epoch  4, batch    21 | loss: 5.5767679Losses:  4.335969924926758 0.17962521314620972 0.8795772194862366
CurrentTrain: epoch  4, batch    22 | loss: 5.3951721Losses:  4.754878044128418 0.2497590184211731 0.8291340470314026
CurrentTrain: epoch  4, batch    23 | loss: 5.8337712Losses:  4.624090194702148 0.29382431507110596 0.7875450849533081
CurrentTrain: epoch  4, batch    24 | loss: 5.7054596Losses:  4.455794334411621 0.28772780299186707 0.8182329535484314
CurrentTrain: epoch  4, batch    25 | loss: 5.5617552Losses:  4.285823345184326 0.17388352751731873 0.801807165145874
CurrentTrain: epoch  4, batch    26 | loss: 5.2615137Losses:  4.361852645874023 0.2704014480113983 0.7972577214241028
CurrentTrain: epoch  4, batch    27 | loss: 5.4295120Losses:  4.575085639953613 0.31489673256874084 0.8223624229431152
CurrentTrain: epoch  4, batch    28 | loss: 5.7123446Losses:  4.331298828125 0.2043171375989914 0.8439373970031738
CurrentTrain: epoch  4, batch    29 | loss: 5.3795533Losses:  4.543508052825928 0.21746636927127838 0.8343568444252014
CurrentTrain: epoch  4, batch    30 | loss: 5.5953312Losses:  5.523047924041748 0.39098310470581055 0.8630998134613037
CurrentTrain: epoch  4, batch    31 | loss: 6.7771311Losses:  4.305476665496826 0.2563251852989197 0.8183708786964417
CurrentTrain: epoch  4, batch    32 | loss: 5.3801727Losses:  4.778945446014404 0.24236592650413513 0.76543128490448
CurrentTrain: epoch  4, batch    33 | loss: 5.7867427Losses:  4.72460412979126 0.2930296063423157 0.8321225047111511
CurrentTrain: epoch  4, batch    34 | loss: 5.8497562Losses:  4.456912040710449 0.17811572551727295 0.8160865306854248
CurrentTrain: epoch  4, batch    35 | loss: 5.4511147Losses:  4.354994773864746 0.2901946008205414 0.82843416929245
CurrentTrain: epoch  4, batch    36 | loss: 5.4736233Losses:  4.281649589538574 0.22620359063148499 0.8095484375953674
CurrentTrain: epoch  4, batch    37 | loss: 5.3174014Losses:  4.40709114074707 0.2511540949344635 0.8402127027511597
CurrentTrain: epoch  4, batch    38 | loss: 5.4984579Losses:  4.277237415313721 0.2752353847026825 0.8408557176589966
CurrentTrain: epoch  4, batch    39 | loss: 5.3933282Losses:  4.15043830871582 0.23339976370334625 0.7940433025360107
CurrentTrain: epoch  4, batch    40 | loss: 5.1778812Losses:  4.232946395874023 0.17473530769348145 0.8268058896064758
CurrentTrain: epoch  4, batch    41 | loss: 5.2344875Losses:  4.428609371185303 0.2332105189561844 0.8353073596954346
CurrentTrain: epoch  4, batch    42 | loss: 5.4971275Losses:  4.279261589050293 0.12313457578420639 0.7949504256248474
CurrentTrain: epoch  4, batch    43 | loss: 5.1973467Losses:  4.386075019836426 0.22895391285419464 0.8195845484733582
CurrentTrain: epoch  4, batch    44 | loss: 5.4346132Losses:  4.449939727783203 0.23376642167568207 0.7895889282226562
CurrentTrain: epoch  4, batch    45 | loss: 5.4732952Losses:  4.3005218505859375 0.196877583861351 0.8089598417282104
CurrentTrain: epoch  4, batch    46 | loss: 5.3063593Losses:  4.302061080932617 0.21530047059059143 0.8392756581306458
CurrentTrain: epoch  4, batch    47 | loss: 5.3566375Losses:  4.496243000030518 0.23449905216693878 0.8277859687805176
CurrentTrain: epoch  4, batch    48 | loss: 5.5585279Losses:  4.330610275268555 0.2558559477329254 0.8092811107635498
CurrentTrain: epoch  4, batch    49 | loss: 5.3957472Losses:  4.336638450622559 0.21464568376541138 0.8119131326675415
CurrentTrain: epoch  4, batch    50 | loss: 5.3631973Losses:  4.311485767364502 0.16606026887893677 0.7831907272338867
CurrentTrain: epoch  4, batch    51 | loss: 5.2607369Losses:  4.866835594177246 0.28013288974761963 0.8190903067588806
CurrentTrain: epoch  4, batch    52 | loss: 5.9660587Losses:  4.249229907989502 0.12483710795640945 0.797982394695282
CurrentTrain: epoch  4, batch    53 | loss: 5.1720490Losses:  4.186415195465088 0.17888009548187256 0.8674460649490356
CurrentTrain: epoch  4, batch    54 | loss: 5.2327414Losses:  4.520442008972168 0.2388589084148407 0.8391815423965454
CurrentTrain: epoch  4, batch    55 | loss: 5.5984821Losses:  4.359593391418457 0.2047637403011322 0.8379983901977539
CurrentTrain: epoch  4, batch    56 | loss: 5.4023557Losses:  4.322282314300537 0.19404728710651398 0.7846989631652832
CurrentTrain: epoch  4, batch    57 | loss: 5.3010287Losses:  4.346151828765869 0.22307461500167847 0.8491647243499756
CurrentTrain: epoch  4, batch    58 | loss: 5.4183912Losses:  4.519013404846191 0.16214919090270996 0.7704481482505798
CurrentTrain: epoch  4, batch    59 | loss: 5.4516110Losses:  4.461647987365723 0.1672123670578003 0.8016060590744019
CurrentTrain: epoch  4, batch    60 | loss: 5.4304667Losses:  4.271762847900391 0.18359805643558502 0.8257554769515991
CurrentTrain: epoch  4, batch    61 | loss: 5.2811165Losses:  4.356405258178711 0.1195027083158493 0.7811391353607178
CurrentTrain: epoch  4, batch    62 | loss: 5.2570467Losses:  4.446589469909668 0.24833689630031586 0.812861979007721
CurrentTrain: epoch  5, batch     0 | loss: 5.5077882Losses:  4.264007568359375 0.15606096386909485 0.7829761505126953
CurrentTrain: epoch  5, batch     1 | loss: 5.2030449Losses:  4.149905204772949 0.19680924713611603 0.7600421905517578
CurrentTrain: epoch  5, batch     2 | loss: 5.1067567Losses:  4.463608741760254 0.19121767580509186 0.901824414730072
CurrentTrain: epoch  5, batch     3 | loss: 5.5566511Losses:  4.319910526275635 0.23701387643814087 0.7944262027740479
CurrentTrain: epoch  5, batch     4 | loss: 5.3513508Losses:  4.141441345214844 0.16220632195472717 0.7661909461021423
CurrentTrain: epoch  5, batch     5 | loss: 5.0698385Losses:  4.151128768920898 0.19727949798107147 0.7827998399734497
CurrentTrain: epoch  5, batch     6 | loss: 5.1312079Losses:  4.388650894165039 0.12045031040906906 0.8074156641960144
CurrentTrain: epoch  5, batch     7 | loss: 5.3165169Losses:  4.189859390258789 0.26914265751838684 0.7709333896636963
CurrentTrain: epoch  5, batch     8 | loss: 5.2299356Losses:  4.165404319763184 0.12224509567022324 0.8439135551452637
CurrentTrain: epoch  5, batch     9 | loss: 5.1315632Losses:  4.237221717834473 0.1577802300453186 0.8041949272155762
CurrentTrain: epoch  5, batch    10 | loss: 5.1991968Losses:  4.410488128662109 0.1715359389781952 0.8014024496078491
CurrentTrain: epoch  5, batch    11 | loss: 5.3834267Losses:  4.29065465927124 0.156875342130661 0.8234727382659912
CurrentTrain: epoch  5, batch    12 | loss: 5.2710028Losses:  4.384726524353027 0.21120241284370422 0.8167022466659546
CurrentTrain: epoch  5, batch    13 | loss: 5.4126315Losses:  4.268375396728516 0.18096591532230377 0.7985432147979736
CurrentTrain: epoch  5, batch    14 | loss: 5.2478848Losses:  4.23149299621582 0.2358960658311844 0.798749566078186
CurrentTrain: epoch  5, batch    15 | loss: 5.2661386Losses:  4.203033447265625 0.2383735626935959 0.8277915716171265
CurrentTrain: epoch  5, batch    16 | loss: 5.2691989Losses:  4.3145341873168945 0.20923790335655212 0.7914395928382874
CurrentTrain: epoch  5, batch    17 | loss: 5.3152118Losses:  4.156170845031738 0.19752706587314606 0.7932658195495605
CurrentTrain: epoch  5, batch    18 | loss: 5.1469636Losses:  4.218585968017578 0.16577234864234924 0.8655109405517578
CurrentTrain: epoch  5, batch    19 | loss: 5.2498693Losses:  4.363210201263428 0.15659232437610626 0.758516788482666
CurrentTrain: epoch  5, batch    20 | loss: 5.2783194Losses:  4.2520647048950195 0.2236342430114746 0.8114956617355347
CurrentTrain: epoch  5, batch    21 | loss: 5.2871947Losses:  4.15798807144165 0.13226309418678284 0.8342224955558777
CurrentTrain: epoch  5, batch    22 | loss: 5.1244736Losses:  4.256044864654541 0.21783596277236938 0.8357981443405151
CurrentTrain: epoch  5, batch    23 | loss: 5.3096790Losses:  4.30539608001709 0.18403837084770203 0.8034674525260925
CurrentTrain: epoch  5, batch    24 | loss: 5.2929015Losses:  4.227025032043457 0.23854491114616394 0.8106117844581604
CurrentTrain: epoch  5, batch    25 | loss: 5.2761817Losses:  4.215633392333984 0.2181491255760193 0.8032239675521851
CurrentTrain: epoch  5, batch    26 | loss: 5.2370067Losses:  4.179048538208008 0.22461581230163574 0.7917606234550476
CurrentTrain: epoch  5, batch    27 | loss: 5.1954250Losses:  4.164202690124512 0.14640653133392334 0.8687271475791931
CurrentTrain: epoch  5, batch    28 | loss: 5.1793365Losses:  4.208089828491211 0.17840856313705444 0.7681316137313843
CurrentTrain: epoch  5, batch    29 | loss: 5.1546302Losses:  4.209417343139648 0.140180766582489 0.7664430737495422
CurrentTrain: epoch  5, batch    30 | loss: 5.1160412Losses:  4.2489728927612305 0.20373409986495972 0.7976047992706299
CurrentTrain: epoch  5, batch    31 | loss: 5.2503119Losses:  4.2170515060424805 0.23131057620048523 0.8036596179008484
CurrentTrain: epoch  5, batch    32 | loss: 5.2520213Losses:  4.164722442626953 0.21062785387039185 0.8124361038208008
CurrentTrain: epoch  5, batch    33 | loss: 5.1877866Losses:  4.292696952819824 0.1609807312488556 0.7728204727172852
CurrentTrain: epoch  5, batch    34 | loss: 5.2264981Losses:  4.161561012268066 0.12577208876609802 0.796251654624939
CurrentTrain: epoch  5, batch    35 | loss: 5.0835848Losses:  4.202605247497559 0.1688772439956665 0.8149649500846863
CurrentTrain: epoch  5, batch    36 | loss: 5.1864471Losses:  4.147287368774414 0.18399810791015625 0.8209839463233948
CurrentTrain: epoch  5, batch    37 | loss: 5.1522694Losses:  4.427462100982666 0.20802611112594604 0.8341138958930969
CurrentTrain: epoch  5, batch    38 | loss: 5.4696021Losses:  4.156680583953857 0.2257501184940338 0.7616932392120361
CurrentTrain: epoch  5, batch    39 | loss: 5.1441240Losses:  4.228120803833008 0.1596796065568924 0.7986832857131958
CurrentTrain: epoch  5, batch    40 | loss: 5.1864834Losses:  4.144564628601074 0.09457859396934509 0.7607096433639526
CurrentTrain: epoch  5, batch    41 | loss: 4.9998531Losses:  4.160581588745117 0.16869714856147766 0.7742393612861633
CurrentTrain: epoch  5, batch    42 | loss: 5.1035185Losses:  4.183117866516113 0.1439785361289978 0.7932665348052979
CurrentTrain: epoch  5, batch    43 | loss: 5.1203632Losses:  4.187235355377197 0.13061124086380005 0.7471483945846558
CurrentTrain: epoch  5, batch    44 | loss: 5.0649953Losses:  4.283108234405518 0.21064940094947815 0.7934905290603638
CurrentTrain: epoch  5, batch    45 | loss: 5.2872481Losses:  4.077764511108398 0.19220085442066193 0.7914395928382874
CurrentTrain: epoch  5, batch    46 | loss: 5.0614047Losses:  4.07263708114624 0.1207713708281517 0.7802093625068665
CurrentTrain: epoch  5, batch    47 | loss: 4.9736180Losses:  4.140849590301514 0.19054436683654785 0.8289426565170288
CurrentTrain: epoch  5, batch    48 | loss: 5.1603370Losses:  4.171402931213379 0.1797025501728058 0.7663114070892334
CurrentTrain: epoch  5, batch    49 | loss: 5.1174173Losses:  4.141091346740723 0.15905548632144928 0.7644517421722412
CurrentTrain: epoch  5, batch    50 | loss: 5.0645990Losses:  4.100711345672607 0.1463392674922943 0.8646144866943359
CurrentTrain: epoch  5, batch    51 | loss: 5.1116652Losses:  4.265191555023193 0.21156680583953857 0.7731991410255432
CurrentTrain: epoch  5, batch    52 | loss: 5.2499576Losses:  4.221654891967773 0.15738314390182495 0.8032338619232178
CurrentTrain: epoch  5, batch    53 | loss: 5.1822720Losses:  4.193953990936279 0.1959136724472046 0.8201130628585815
CurrentTrain: epoch  5, batch    54 | loss: 5.2099810Losses:  4.169976711273193 0.16679495573043823 0.7751827239990234
CurrentTrain: epoch  5, batch    55 | loss: 5.1119542Losses:  4.129904747009277 0.1301746964454651 0.7634810209274292
CurrentTrain: epoch  5, batch    56 | loss: 5.0235605Losses:  4.101914882659912 0.2242346704006195 0.7300020456314087
CurrentTrain: epoch  5, batch    57 | loss: 5.0561514Losses:  4.170620918273926 0.1956208050251007 0.8101276159286499
CurrentTrain: epoch  5, batch    58 | loss: 5.1763697Losses:  4.065587043762207 0.14316000044345856 0.8235716223716736
CurrentTrain: epoch  5, batch    59 | loss: 5.0323186Losses:  4.137907981872559 0.11737681925296783 0.821495771408081
CurrentTrain: epoch  5, batch    60 | loss: 5.0767803Losses:  4.023105144500732 0.15918922424316406 0.8249394297599792
CurrentTrain: epoch  5, batch    61 | loss: 5.0072336Losses:  4.172815322875977 0.11652130633592606 0.7951862812042236
CurrentTrain: epoch  5, batch    62 | loss: 5.0845232Losses:  4.117739677429199 0.18632321059703827 0.7391310930252075
CurrentTrain: epoch  6, batch     0 | loss: 5.0431938Losses:  4.140212059020996 0.22172601521015167 0.7994520664215088
CurrentTrain: epoch  6, batch     1 | loss: 5.1613903Losses:  4.065876483917236 0.1049221083521843 0.8263968229293823
CurrentTrain: epoch  6, batch     2 | loss: 4.9971957Losses:  4.097553253173828 0.16718918085098267 0.7722486257553101
CurrentTrain: epoch  6, batch     3 | loss: 5.0369911Losses:  4.141204833984375 0.18238255381584167 0.7440680861473083
CurrentTrain: epoch  6, batch     4 | loss: 5.0676556Losses:  4.073744773864746 0.11275584995746613 0.8411341905593872
CurrentTrain: epoch  6, batch     5 | loss: 5.0276346Losses:  4.224793434143066 0.1495673656463623 0.8294042944908142
CurrentTrain: epoch  6, batch     6 | loss: 5.2037654Losses:  4.1340742111206055 0.14964182674884796 0.7635319232940674
CurrentTrain: epoch  6, batch     7 | loss: 5.0472479Losses:  4.171360969543457 0.15019461512565613 0.8256820440292358
CurrentTrain: epoch  6, batch     8 | loss: 5.1472378Losses:  4.105923652648926 0.1562468558549881 0.7672525644302368
CurrentTrain: epoch  6, batch     9 | loss: 5.0294228Losses:  4.1084818840026855 0.12973086535930634 0.7539685964584351
CurrentTrain: epoch  6, batch    10 | loss: 4.9921813Losses:  4.127453327178955 0.17167729139328003 0.7359171509742737
CurrentTrain: epoch  6, batch    11 | loss: 5.0350475Losses:  4.12929630279541 0.15030133724212646 0.8393206596374512
CurrentTrain: epoch  6, batch    12 | loss: 5.1189184Losses:  4.097470283508301 0.16929852962493896 0.7755457162857056
CurrentTrain: epoch  6, batch    13 | loss: 5.0423145Losses:  4.107198715209961 0.1406199038028717 0.7133974432945251
CurrentTrain: epoch  6, batch    14 | loss: 4.9612160Losses:  4.1826581954956055 0.10676831752061844 0.8420287370681763
CurrentTrain: epoch  6, batch    15 | loss: 5.1314549Losses:  4.15405797958374 0.15117058157920837 0.7125360369682312
CurrentTrain: epoch  6, batch    16 | loss: 5.0177646Losses:  4.107229232788086 0.1643238365650177 0.7711987495422363
CurrentTrain: epoch  6, batch    17 | loss: 5.0427518Losses:  4.147817611694336 0.16534334421157837 0.8071858882904053
CurrentTrain: epoch  6, batch    18 | loss: 5.1203470Losses:  4.111110687255859 0.17887580394744873 0.7750626802444458
CurrentTrain: epoch  6, batch    19 | loss: 5.0650492Losses:  4.149337291717529 0.16540765762329102 0.7965180277824402
CurrentTrain: epoch  6, batch    20 | loss: 5.1112628Losses:  4.115082740783691 0.18145816028118134 0.7798178791999817
CurrentTrain: epoch  6, batch    21 | loss: 5.0763588Losses:  4.180789470672607 0.19513145089149475 0.770575761795044
CurrentTrain: epoch  6, batch    22 | loss: 5.1464968Losses:  4.140334606170654 0.14967122673988342 0.8221712112426758
CurrentTrain: epoch  6, batch    23 | loss: 5.1121769Losses:  4.1038713455200195 0.12183181941509247 0.7380439043045044
CurrentTrain: epoch  6, batch    24 | loss: 4.9637470Losses:  4.1003265380859375 0.13827091455459595 0.8029122352600098
CurrentTrain: epoch  6, batch    25 | loss: 5.0415096Losses:  4.106722831726074 0.17739048600196838 0.788739800453186
CurrentTrain: epoch  6, batch    26 | loss: 5.0728531Losses:  4.137416839599609 0.14655840396881104 0.7839018106460571
CurrentTrain: epoch  6, batch    27 | loss: 5.0678768Losses:  4.1424994468688965 0.15841642022132874 0.7502225637435913
CurrentTrain: epoch  6, batch    28 | loss: 5.0511384Losses:  4.127455234527588 0.11890494078397751 0.7363079190254211
CurrentTrain: epoch  6, batch    29 | loss: 4.9826684Losses:  4.146041393280029 0.18604257702827454 0.7951512336730957
CurrentTrain: epoch  6, batch    30 | loss: 5.1272354Losses:  4.129842758178711 0.13215477764606476 0.7755844593048096
CurrentTrain: epoch  6, batch    31 | loss: 5.0375824Losses:  4.120595455169678 0.1723955273628235 0.8285863995552063
CurrentTrain: epoch  6, batch    32 | loss: 5.1215777Losses:  4.130064964294434 0.16029781103134155 0.8523744344711304
CurrentTrain: epoch  6, batch    33 | loss: 5.1427374Losses:  4.141390323638916 0.0995173454284668 0.7268301844596863
CurrentTrain: epoch  6, batch    34 | loss: 4.9677377Losses:  4.078853130340576 0.1549021303653717 0.8211925625801086
CurrentTrain: epoch  6, batch    35 | loss: 5.0549479Losses:  4.167507648468018 0.15082566440105438 0.8115770816802979
CurrentTrain: epoch  6, batch    36 | loss: 5.1299105Losses:  4.077846050262451 0.19312727451324463 0.7578296661376953
CurrentTrain: epoch  6, batch    37 | loss: 5.0288029Losses:  4.088452339172363 0.11632799357175827 0.7312855124473572
CurrentTrain: epoch  6, batch    38 | loss: 4.9360657Losses:  4.070474624633789 0.1554332971572876 0.7686214447021484
CurrentTrain: epoch  6, batch    39 | loss: 4.9945292Losses:  4.064436912536621 0.12285619974136353 0.7947933673858643
CurrentTrain: epoch  6, batch    40 | loss: 4.9820862Losses:  4.122149467468262 0.20136123895645142 0.794277012348175
CurrentTrain: epoch  6, batch    41 | loss: 5.1177878Losses:  4.071957111358643 0.1786891222000122 0.8351547122001648
CurrentTrain: epoch  6, batch    42 | loss: 5.0858006Losses:  4.10258150100708 0.16787797212600708 0.7782237529754639
CurrentTrain: epoch  6, batch    43 | loss: 5.0486832Losses:  4.09810733795166 0.12315363436937332 0.7665637731552124
CurrentTrain: epoch  6, batch    44 | loss: 4.9878249Losses:  4.057814121246338 0.14879947900772095 0.7901959419250488
CurrentTrain: epoch  6, batch    45 | loss: 4.9968095Losses:  4.1564106941223145 0.1371905505657196 0.8244329690933228
CurrentTrain: epoch  6, batch    46 | loss: 5.1180339Losses:  4.0864973068237305 0.11918267607688904 0.784430980682373
CurrentTrain: epoch  6, batch    47 | loss: 4.9901109Losses:  4.113635063171387 0.16823634505271912 0.7590782642364502
CurrentTrain: epoch  6, batch    48 | loss: 5.0409498Losses:  4.096016883850098 0.16563457250595093 0.7862046957015991
CurrentTrain: epoch  6, batch    49 | loss: 5.0478563Losses:  4.137539863586426 0.14855535328388214 0.7658717632293701
CurrentTrain: epoch  6, batch    50 | loss: 5.0519667Losses:  4.041925430297852 0.15367098152637482 0.7821477651596069
CurrentTrain: epoch  6, batch    51 | loss: 4.9777441Losses:  4.078289985656738 0.10637957602739334 0.7924778461456299
CurrentTrain: epoch  6, batch    52 | loss: 4.9771471Losses:  4.102430820465088 0.1441449373960495 0.7710358500480652
CurrentTrain: epoch  6, batch    53 | loss: 5.0176115Losses:  4.055624008178711 0.11683168262243271 0.7918701767921448
CurrentTrain: epoch  6, batch    54 | loss: 4.9643259Losses:  4.06585168838501 0.15550199151039124 0.8251153230667114
CurrentTrain: epoch  6, batch    55 | loss: 5.0464687Losses:  4.112296104431152 0.14774000644683838 0.7534714937210083
CurrentTrain: epoch  6, batch    56 | loss: 5.0135074Losses:  4.062737941741943 0.12072956562042236 0.7566635608673096
CurrentTrain: epoch  6, batch    57 | loss: 4.9401312Losses:  4.058239936828613 0.15940043330192566 0.7289811372756958
CurrentTrain: epoch  6, batch    58 | loss: 4.9466214Losses:  4.057740688323975 0.1654263734817505 0.7986857295036316
CurrentTrain: epoch  6, batch    59 | loss: 5.0218525Losses:  4.026392936706543 0.10216476768255234 0.7463881373405457
CurrentTrain: epoch  6, batch    60 | loss: 4.8749456Losses:  4.062657356262207 0.18317589163780212 0.7680467367172241
CurrentTrain: epoch  6, batch    61 | loss: 5.0138803Losses:  4.0938720703125 0.08187681436538696 0.7729357481002808
CurrentTrain: epoch  6, batch    62 | loss: 4.9486847Losses:  4.097721099853516 0.16925349831581116 0.7302340269088745
CurrentTrain: epoch  7, batch     0 | loss: 4.9972086Losses:  4.074374198913574 0.16185913980007172 0.7780562043190002
CurrentTrain: epoch  7, batch     1 | loss: 5.0142894Losses:  4.08254337310791 0.15578821301460266 0.7833423614501953
CurrentTrain: epoch  7, batch     2 | loss: 5.0216742Losses:  4.078866004943848 0.13933438062667847 0.7534956932067871
CurrentTrain: epoch  7, batch     3 | loss: 4.9716959Losses:  4.075943470001221 0.13722285628318787 0.828637421131134
CurrentTrain: epoch  7, batch     4 | loss: 5.0418038Losses:  4.095264911651611 0.13060307502746582 0.8290797472000122
CurrentTrain: epoch  7, batch     5 | loss: 5.0549479Losses:  4.114661693572998 0.09025440365076065 0.8117102980613708
CurrentTrain: epoch  7, batch     6 | loss: 5.0166264Losses:  4.04904842376709 0.10703635215759277 0.8141502737998962
CurrentTrain: epoch  7, batch     7 | loss: 4.9702353Losses:  4.051299571990967 0.13472852110862732 0.8155834078788757
CurrentTrain: epoch  7, batch     8 | loss: 5.0016112Losses:  4.024442672729492 0.16279444098472595 0.7721264362335205
CurrentTrain: epoch  7, batch     9 | loss: 4.9593639Losses:  4.1319732666015625 0.15927162766456604 0.7937003374099731
CurrentTrain: epoch  7, batch    10 | loss: 5.0849452Losses:  4.081630706787109 0.15709653496742249 0.7693785429000854
CurrentTrain: epoch  7, batch    11 | loss: 5.0081058Losses:  4.103453159332275 0.08383272588253021 0.8112646341323853
CurrentTrain: epoch  7, batch    12 | loss: 4.9985504Losses:  4.04124116897583 0.14412494003772736 0.7145572900772095
CurrentTrain: epoch  7, batch    13 | loss: 4.8999233Losses:  4.0658369064331055 0.15321877598762512 0.754368007183075
CurrentTrain: epoch  7, batch    14 | loss: 4.9734235Losses:  4.045084476470947 0.09073685109615326 0.8045873641967773
CurrentTrain: epoch  7, batch    15 | loss: 4.9404087Losses:  4.100894927978516 0.11354830861091614 0.8012470006942749
CurrentTrain: epoch  7, batch    16 | loss: 5.0156903Losses:  4.121859550476074 0.1707521229982376 0.7714540958404541
CurrentTrain: epoch  7, batch    17 | loss: 5.0640659Losses:  4.09464168548584 0.08545257151126862 0.8295131921768188
CurrentTrain: epoch  7, batch    18 | loss: 5.0096073Losses:  4.059360504150391 0.16936272382736206 0.7349305152893066
CurrentTrain: epoch  7, batch    19 | loss: 4.9636536Losses:  4.091817378997803 0.13871870934963226 0.7430063486099243
CurrentTrain: epoch  7, batch    20 | loss: 4.9735422Losses:  4.031662940979004 0.128040611743927 0.7910795211791992
CurrentTrain: epoch  7, batch    21 | loss: 4.9507833Losses:  4.007254123687744 0.1474248468875885 0.7063523530960083
CurrentTrain: epoch  7, batch    22 | loss: 4.8610311Losses:  4.0766987800598145 0.12215077131986618 0.7788455486297607
CurrentTrain: epoch  7, batch    23 | loss: 4.9776955Losses:  4.030855178833008 0.1325329840183258 0.710159420967102
CurrentTrain: epoch  7, batch    24 | loss: 4.8735476Losses:  4.051486015319824 0.111820288002491 0.7327315807342529
CurrentTrain: epoch  7, batch    25 | loss: 4.8960381Losses:  4.084418296813965 0.1367236226797104 0.7786484956741333
CurrentTrain: epoch  7, batch    26 | loss: 4.9997902Losses:  4.056015968322754 0.10232210159301758 0.8253465890884399
CurrentTrain: epoch  7, batch    27 | loss: 4.9836845Losses:  4.0485615730285645 0.1509515345096588 0.7649887204170227
CurrentTrain: epoch  7, batch    28 | loss: 4.9645019Losses:  4.120874881744385 0.10937356948852539 0.8126895427703857
CurrentTrain: epoch  7, batch    29 | loss: 5.0429382Losses:  4.050774574279785 0.09017130732536316 0.8244308233261108
CurrentTrain: epoch  7, batch    30 | loss: 4.9653769Losses:  4.059049606323242 0.14465047419071198 0.7008922696113586
CurrentTrain: epoch  7, batch    31 | loss: 4.9045925Losses:  4.03355073928833 0.13559198379516602 0.7490053176879883
CurrentTrain: epoch  7, batch    32 | loss: 4.9181480Losses:  4.013973236083984 0.1384335160255432 0.7529542446136475
CurrentTrain: epoch  7, batch    33 | loss: 4.9053612Losses:  4.074750900268555 0.13866499066352844 0.7750576734542847
CurrentTrain: epoch  7, batch    34 | loss: 4.9884739Losses:  4.042926788330078 0.12095043063163757 0.7651439905166626
CurrentTrain: epoch  7, batch    35 | loss: 4.9290209Losses:  4.051052093505859 0.06469922512769699 0.7428364157676697
CurrentTrain: epoch  7, batch    36 | loss: 4.8585877Losses:  4.053910732269287 0.14438864588737488 0.7498981952667236
CurrentTrain: epoch  7, batch    37 | loss: 4.9481974Losses:  4.045923233032227 0.14386650919914246 0.733649730682373
CurrentTrain: epoch  7, batch    38 | loss: 4.9234395Losses:  4.010977745056152 0.15064053237438202 0.7675768136978149
CurrentTrain: epoch  7, batch    39 | loss: 4.9291949Losses:  4.012527942657471 0.1613887995481491 0.7359707355499268
CurrentTrain: epoch  7, batch    40 | loss: 4.9098873Losses:  4.02726936340332 0.1343422532081604 0.7916544675827026
CurrentTrain: epoch  7, batch    41 | loss: 4.9532661Losses:  4.060330390930176 0.11830444633960724 0.7607585787773132
CurrentTrain: epoch  7, batch    42 | loss: 4.9393930Losses:  4.032292366027832 0.10190017521381378 0.7197400331497192
CurrentTrain: epoch  7, batch    43 | loss: 4.8539324Losses:  4.020576477050781 0.13285619020462036 0.7397447824478149
CurrentTrain: epoch  7, batch    44 | loss: 4.8931775Losses:  4.048670291900635 0.09524783492088318 0.7638672590255737
CurrentTrain: epoch  7, batch    45 | loss: 4.9077854Losses:  4.121146202087402 0.0959545373916626 0.7046946287155151
CurrentTrain: epoch  7, batch    46 | loss: 4.9217954Losses:  4.052490234375 0.14644098281860352 0.7053112983703613
CurrentTrain: epoch  7, batch    47 | loss: 4.9042425Losses:  4.10349702835083 0.13530297577381134 0.7396928668022156
CurrentTrain: epoch  7, batch    48 | loss: 4.9784927Losses:  4.0470871925354 0.11522519588470459 0.8090468049049377
CurrentTrain: epoch  7, batch    49 | loss: 4.9713593Losses:  4.123927116394043 0.12014184892177582 0.7417566776275635
CurrentTrain: epoch  7, batch    50 | loss: 4.9858255Losses:  4.010129928588867 0.09585411846637726 0.7112699747085571
CurrentTrain: epoch  7, batch    51 | loss: 4.8172541Losses:  4.026959419250488 0.09709921479225159 0.8338990807533264
CurrentTrain: epoch  7, batch    52 | loss: 4.9579577Losses:  3.9863839149475098 0.1068241223692894 0.7982386350631714
CurrentTrain: epoch  7, batch    53 | loss: 4.8914466Losses:  4.068949222564697 0.07806681096553802 0.7401809692382812
CurrentTrain: epoch  7, batch    54 | loss: 4.8871970Losses:  4.147001266479492 0.14160583913326263 0.8287461996078491
CurrentTrain: epoch  7, batch    55 | loss: 5.1173534Losses:  4.113129615783691 0.0910639762878418 0.7791893482208252
CurrentTrain: epoch  7, batch    56 | loss: 4.9833832Losses:  4.033705711364746 0.1049581915140152 0.7942413687705994
CurrentTrain: epoch  7, batch    57 | loss: 4.9329052Losses:  4.043899059295654 0.1338547021150589 0.7587822675704956
CurrentTrain: epoch  7, batch    58 | loss: 4.9365363Losses:  4.010682582855225 0.14381110668182373 0.7947967052459717
CurrentTrain: epoch  7, batch    59 | loss: 4.9492903Losses:  4.060778617858887 0.16754114627838135 0.7211770415306091
CurrentTrain: epoch  7, batch    60 | loss: 4.9494967Losses:  4.032744407653809 0.10746119916439056 0.7318494915962219
CurrentTrain: epoch  7, batch    61 | loss: 4.8720551Losses:  4.0011677742004395 0.059498511254787445 0.774001955986023
CurrentTrain: epoch  7, batch    62 | loss: 4.8346682Losses:  4.042989730834961 0.1212441623210907 0.7723402976989746
CurrentTrain: epoch  8, batch     0 | loss: 4.9365740Losses:  4.060307502746582 0.10690262913703918 0.7974625825881958
CurrentTrain: epoch  8, batch     1 | loss: 4.9646726Losses:  4.033271789550781 0.11740236729383469 0.761354923248291
CurrentTrain: epoch  8, batch     2 | loss: 4.9120293Losses:  4.021303653717041 0.15513521432876587 0.7433325052261353
CurrentTrain: epoch  8, batch     3 | loss: 4.9197712Losses:  4.063592910766602 0.11911176145076752 0.8083623647689819
CurrentTrain: epoch  8, batch     4 | loss: 4.9910669Losses:  4.072021961212158 0.1319122016429901 0.7683330774307251
CurrentTrain: epoch  8, batch     5 | loss: 4.9722672Losses:  4.044410705566406 0.13600429892539978 0.7014430165290833
CurrentTrain: epoch  8, batch     6 | loss: 4.8818583Losses:  4.053999900817871 0.13306304812431335 0.7626344561576843
CurrentTrain: epoch  8, batch     7 | loss: 4.9496970Losses:  4.016122817993164 0.12850193679332733 0.7449164390563965
CurrentTrain: epoch  8, batch     8 | loss: 4.8895411Losses:  4.101204872131348 0.1204838752746582 0.7879506349563599
CurrentTrain: epoch  8, batch     9 | loss: 5.0096393Losses:  4.0950493812561035 0.1298903375864029 0.7463043332099915
CurrentTrain: epoch  8, batch    10 | loss: 4.9712443Losses:  4.056631088256836 0.1329197883605957 0.8311440944671631
CurrentTrain: epoch  8, batch    11 | loss: 5.0206947Losses:  3.9753451347351074 0.07153543084859848 0.6377768516540527
CurrentTrain: epoch  8, batch    12 | loss: 4.6846576Losses:  4.082879543304443 0.14977872371673584 0.73628830909729
CurrentTrain: epoch  8, batch    13 | loss: 4.9689465Losses:  3.9638893604278564 0.08632199466228485 0.7242323160171509
CurrentTrain: epoch  8, batch    14 | loss: 4.7744436Losses:  4.041848182678223 0.1345309019088745 0.770756721496582
CurrentTrain: epoch  8, batch    15 | loss: 4.9471359Losses:  4.00466251373291 0.13846313953399658 0.7150313258171082
CurrentTrain: epoch  8, batch    16 | loss: 4.8581567Losses:  4.00558614730835 0.08782762289047241 0.7114500999450684
CurrentTrain: epoch  8, batch    17 | loss: 4.8048639Losses:  4.006446838378906 0.11524507403373718 0.7347473502159119
CurrentTrain: epoch  8, batch    18 | loss: 4.8564391Losses:  4.008425235748291 0.13423612713813782 0.7945342659950256
CurrentTrain: epoch  8, batch    19 | loss: 4.9371958Losses:  4.028780937194824 0.10198841989040375 0.7537434101104736
CurrentTrain: epoch  8, batch    20 | loss: 4.8845129Losses:  4.0515313148498535 0.11520671844482422 0.8074573278427124
CurrentTrain: epoch  8, batch    21 | loss: 4.9741955Losses:  4.023416996002197 0.10281979292631149 0.8062140941619873
CurrentTrain: epoch  8, batch    22 | loss: 4.9324512Losses:  4.03272819519043 0.09367979317903519 0.7598286271095276
CurrentTrain: epoch  8, batch    23 | loss: 4.8862367Losses:  4.060925006866455 0.0985570028424263 0.801649808883667
CurrentTrain: epoch  8, batch    24 | loss: 4.9611320Losses:  4.039896011352539 0.08733855187892914 0.7833234071731567
CurrentTrain: epoch  8, batch    25 | loss: 4.9105577Losses:  4.091485500335693 0.0606924444437027 0.6896609663963318
CurrentTrain: epoch  8, batch    26 | loss: 4.8418388Losses:  3.9729459285736084 0.08818680047988892 0.7048487663269043
CurrentTrain: epoch  8, batch    27 | loss: 4.7659817Losses:  4.071372032165527 0.08938446640968323 0.747434675693512
CurrentTrain: epoch  8, batch    28 | loss: 4.9081912Losses:  3.9966840744018555 0.13390234112739563 0.7631433606147766
CurrentTrain: epoch  8, batch    29 | loss: 4.8937302Losses:  4.030781269073486 0.12256356328725815 0.7466771006584167
CurrentTrain: epoch  8, batch    30 | loss: 4.9000216Losses:  4.037435531616211 0.12138828635215759 0.7514487504959106
CurrentTrain: epoch  8, batch    31 | loss: 4.9102726Losses:  4.0161590576171875 0.11797771602869034 0.6995275020599365
CurrentTrain: epoch  8, batch    32 | loss: 4.8336639Losses:  4.012986183166504 0.10092546045780182 0.780773401260376
CurrentTrain: epoch  8, batch    33 | loss: 4.8946848Losses:  3.9949209690093994 0.10081246495246887 0.717850923538208
CurrentTrain: epoch  8, batch    34 | loss: 4.8135843Losses:  4.06564998626709 0.09381979703903198 0.8303567171096802
CurrentTrain: epoch  8, batch    35 | loss: 4.9898262Losses:  4.055448055267334 0.1376018226146698 0.7075701951980591
CurrentTrain: epoch  8, batch    36 | loss: 4.9006200Losses:  4.005491733551025 0.1292879432439804 0.6925420165061951
CurrentTrain: epoch  8, batch    37 | loss: 4.8273215Losses:  4.05940580368042 0.10540714114904404 0.7737163305282593
CurrentTrain: epoch  8, batch    38 | loss: 4.9385295Losses:  4.026209354400635 0.0869341716170311 0.7914808392524719
CurrentTrain: epoch  8, batch    39 | loss: 4.9046245Losses:  4.062641143798828 0.10524901747703552 0.723629891872406
CurrentTrain: epoch  8, batch    40 | loss: 4.8915200Losses:  3.9814703464508057 0.1361037939786911 0.7513065934181213
CurrentTrain: epoch  8, batch    41 | loss: 4.8688807Losses:  4.010184288024902 0.10743398219347 0.6879674196243286
CurrentTrain: epoch  8, batch    42 | loss: 4.8055854Losses:  4.037709712982178 0.11034788191318512 0.7782304883003235
CurrentTrain: epoch  8, batch    43 | loss: 4.9262881Losses:  4.016335487365723 0.06889184564352036 0.75600266456604
CurrentTrain: epoch  8, batch    44 | loss: 4.8412304Losses:  4.072906494140625 0.08774001896381378 0.7679320573806763
CurrentTrain: epoch  8, batch    45 | loss: 4.9285784Losses:  4.070353984832764 0.09687869250774384 0.7311367392539978
CurrentTrain: epoch  8, batch    46 | loss: 4.8983693Losses:  4.021028518676758 0.09356657415628433 0.76233971118927
CurrentTrain: epoch  8, batch    47 | loss: 4.8769345Losses:  4.019068241119385 0.09191039204597473 0.7229900360107422
CurrentTrain: epoch  8, batch    48 | loss: 4.8339686Losses:  3.9991841316223145 0.13861088454723358 0.7401543855667114
CurrentTrain: epoch  8, batch    49 | loss: 4.8779492Losses:  4.028326511383057 0.11034026741981506 0.7298262119293213
CurrentTrain: epoch  8, batch    50 | loss: 4.8684931Losses:  4.039231300354004 0.12291385233402252 0.7623854279518127
CurrentTrain: epoch  8, batch    51 | loss: 4.9245305Losses:  4.013065338134766 0.11337362229824066 0.7842434644699097
CurrentTrain: epoch  8, batch    52 | loss: 4.9106827Losses:  4.00282096862793 0.09289003908634186 0.7603811025619507
CurrentTrain: epoch  8, batch    53 | loss: 4.8560925Losses:  4.0229997634887695 0.09676520526409149 0.8187711834907532
CurrentTrain: epoch  8, batch    54 | loss: 4.9385362Losses:  3.9700541496276855 0.07665091753005981 0.7900347709655762
CurrentTrain: epoch  8, batch    55 | loss: 4.8367400Losses:  4.059272289276123 0.07566485553979874 0.7800018787384033
CurrentTrain: epoch  8, batch    56 | loss: 4.9149389Losses:  3.9886980056762695 0.11546167731285095 0.7335290312767029
CurrentTrain: epoch  8, batch    57 | loss: 4.8376889Losses:  4.010859966278076 0.11059261113405228 0.7232660055160522
CurrentTrain: epoch  8, batch    58 | loss: 4.8447189Losses:  3.999253273010254 0.11143698543310165 0.8317491412162781
CurrentTrain: epoch  8, batch    59 | loss: 4.9424391Losses:  4.011343955993652 0.11705118417739868 0.7608092427253723
CurrentTrain: epoch  8, batch    60 | loss: 4.8892045Losses:  3.981740951538086 0.12817589938640594 0.769770622253418
CurrentTrain: epoch  8, batch    61 | loss: 4.8796873Losses:  4.011845111846924 0.02812131866812706 0.7839537858963013
CurrentTrain: epoch  8, batch    62 | loss: 4.8239202Losses:  4.047117233276367 0.08202439546585083 0.7891941070556641
CurrentTrain: epoch  9, batch     0 | loss: 4.9183359Losses:  4.0061869621276855 0.1281653642654419 0.7308305501937866
CurrentTrain: epoch  9, batch     1 | loss: 4.8651829Losses:  4.0366411209106445 0.11213123053312302 0.8073961734771729
CurrentTrain: epoch  9, batch     2 | loss: 4.9561682Losses:  3.9951813220977783 0.10214097052812576 0.7886332869529724
CurrentTrain: epoch  9, batch     3 | loss: 4.8859558Losses:  4.002403736114502 0.11769984662532806 0.699455738067627
CurrentTrain: epoch  9, batch     4 | loss: 4.8195591Losses:  4.0253586769104 0.09065303206443787 0.761405348777771
CurrentTrain: epoch  9, batch     5 | loss: 4.8774171Losses:  3.9843192100524902 0.0924295037984848 0.7258385419845581
CurrentTrain: epoch  9, batch     6 | loss: 4.8025875Losses:  3.9866867065429688 0.0966891199350357 0.7446502447128296
CurrentTrain: epoch  9, batch     7 | loss: 4.8280263Losses:  4.030984878540039 0.13092085719108582 0.721282958984375
CurrentTrain: epoch  9, batch     8 | loss: 4.8831887Losses:  3.9829659461975098 0.10842521488666534 0.7914897203445435
CurrentTrain: epoch  9, batch     9 | loss: 4.8828807Losses:  4.04261589050293 0.11282359808683395 0.7061417102813721
CurrentTrain: epoch  9, batch    10 | loss: 4.8615808Losses:  4.020349025726318 0.07632028311491013 0.7555838227272034
CurrentTrain: epoch  9, batch    11 | loss: 4.8522530Losses:  4.017460823059082 0.097249835729599 0.6581293344497681
CurrentTrain: epoch  9, batch    12 | loss: 4.7728400Losses:  4.014615058898926 0.13177967071533203 0.7402994632720947
CurrentTrain: epoch  9, batch    13 | loss: 4.8866940Losses:  4.008969306945801 0.1475774645805359 0.7769054174423218
CurrentTrain: epoch  9, batch    14 | loss: 4.9334521Losses:  4.019311904907227 0.0820692628622055 0.6781613230705261
CurrentTrain: epoch  9, batch    15 | loss: 4.7795424Losses:  4.000848770141602 0.1161872148513794 0.7399369478225708
CurrentTrain: epoch  9, batch    16 | loss: 4.8569727Losses:  4.037637710571289 0.08091326057910919 0.8152607679367065
CurrentTrain: epoch  9, batch    17 | loss: 4.9338117Losses:  4.01094913482666 0.08177609741687775 0.717589795589447
CurrentTrain: epoch  9, batch    18 | loss: 4.8103151Losses:  3.918272018432617 0.12106441706418991 0.6982391476631165
CurrentTrain: epoch  9, batch    19 | loss: 4.7375755Losses:  4.001046657562256 0.07910218089818954 0.7032924294471741
CurrentTrain: epoch  9, batch    20 | loss: 4.7834411Losses:  3.9788155555725098 0.08924081921577454 0.7873383164405823
CurrentTrain: epoch  9, batch    21 | loss: 4.8553948Losses:  4.006623268127441 0.08751169592142105 0.7715803384780884
CurrentTrain: epoch  9, batch    22 | loss: 4.8657150Losses:  4.0743727684021 0.07006769627332687 0.8100718855857849
CurrentTrain: epoch  9, batch    23 | loss: 4.9545126Losses:  4.020137786865234 0.11397399753332138 0.7574173212051392
CurrentTrain: epoch  9, batch    24 | loss: 4.8915291Losses:  3.983402967453003 0.12108597159385681 0.7024496793746948
CurrentTrain: epoch  9, batch    25 | loss: 4.8069386Losses:  4.009245872497559 0.10074485838413239 0.7562358379364014
CurrentTrain: epoch  9, batch    26 | loss: 4.8662262Losses:  4.004349708557129 0.085635244846344 0.7431879043579102
CurrentTrain: epoch  9, batch    27 | loss: 4.8331728Losses:  4.039332389831543 0.10242322832345963 0.7727231383323669
CurrentTrain: epoch  9, batch    28 | loss: 4.9144788Losses:  4.017195701599121 0.10726355016231537 0.7657445073127747
CurrentTrain: epoch  9, batch    29 | loss: 4.8902040Losses:  3.955122709274292 0.12471948564052582 0.7447041273117065
CurrentTrain: epoch  9, batch    30 | loss: 4.8245463Losses:  3.96768856048584 0.0985066145658493 0.7640284895896912
CurrentTrain: epoch  9, batch    31 | loss: 4.8302236Losses:  3.9773147106170654 0.13301235437393188 0.7428886294364929
CurrentTrain: epoch  9, batch    32 | loss: 4.8532157Losses:  4.036276817321777 0.09701481461524963 0.7013575434684753
CurrentTrain: epoch  9, batch    33 | loss: 4.8346491Losses:  3.991816520690918 0.11392278969287872 0.7264819145202637
CurrentTrain: epoch  9, batch    34 | loss: 4.8322210Losses:  4.005832672119141 0.12636268138885498 0.7328870296478271
CurrentTrain: epoch  9, batch    35 | loss: 4.8650827Losses:  4.0052170753479 0.1133817657828331 0.7344636917114258
CurrentTrain: epoch  9, batch    36 | loss: 4.8530626Losses:  3.9968206882476807 0.09335379302501678 0.7365454435348511
CurrentTrain: epoch  9, batch    37 | loss: 4.8267202Losses:  3.9824953079223633 0.1070539653301239 0.650515079498291
CurrentTrain: epoch  9, batch    38 | loss: 4.7400641Losses:  4.036803245544434 0.12292126566171646 0.6988650560379028
CurrentTrain: epoch  9, batch    39 | loss: 4.8585896Losses:  4.010462284088135 0.11248818784952164 0.772296667098999
CurrentTrain: epoch  9, batch    40 | loss: 4.8952475Losses:  3.9888627529144287 0.11588861048221588 0.68556809425354
CurrentTrain: epoch  9, batch    41 | loss: 4.7903194Losses:  4.016246795654297 0.09465839713811874 0.7419081926345825
CurrentTrain: epoch  9, batch    42 | loss: 4.8528132Losses:  3.9604530334472656 0.08970193564891815 0.655830979347229
CurrentTrain: epoch  9, batch    43 | loss: 4.7059860Losses:  4.022603988647461 0.10862329602241516 0.7867293357849121
CurrentTrain: epoch  9, batch    44 | loss: 4.9179568Losses:  4.014983177185059 0.11235922574996948 0.6726161241531372
CurrentTrain: epoch  9, batch    45 | loss: 4.7999582Losses:  3.981560468673706 0.07955129444599152 0.7778322696685791
CurrentTrain: epoch  9, batch    46 | loss: 4.8389444Losses:  4.027632236480713 0.09026031941175461 0.7710371017456055
CurrentTrain: epoch  9, batch    47 | loss: 4.8889298Losses:  4.015753269195557 0.09256452322006226 0.6939510703086853
CurrentTrain: epoch  9, batch    48 | loss: 4.8022690Losses:  3.985447645187378 0.08755858242511749 0.769514799118042
CurrentTrain: epoch  9, batch    49 | loss: 4.8425207Losses:  4.027883052825928 0.09230267256498337 0.8170590996742249
CurrentTrain: epoch  9, batch    50 | loss: 4.9372449Losses:  4.015975475311279 0.10609054565429688 0.7703494429588318
CurrentTrain: epoch  9, batch    51 | loss: 4.8924155Losses:  4.027148723602295 0.11803388595581055 0.7248181104660034
CurrentTrain: epoch  9, batch    52 | loss: 4.8700008Losses:  4.008869171142578 0.08707443624734879 0.7844263911247253
CurrentTrain: epoch  9, batch    53 | loss: 4.8803697Losses:  4.0193867683410645 0.13178910315036774 0.7510336637496948
CurrentTrain: epoch  9, batch    54 | loss: 4.9022098Losses:  4.022935390472412 0.08796408027410507 0.822838544845581
CurrentTrain: epoch  9, batch    55 | loss: 4.9337378Losses:  3.981800079345703 0.06977052986621857 0.7463697195053101
CurrentTrain: epoch  9, batch    56 | loss: 4.7979403Losses:  3.959831476211548 0.10045446455478668 0.72723388671875
CurrentTrain: epoch  9, batch    57 | loss: 4.7875199Losses:  4.027641773223877 0.07359625399112701 0.8108618259429932
CurrentTrain: epoch  9, batch    58 | loss: 4.9120998Losses:  3.974181652069092 0.10156659781932831 0.7718433141708374
CurrentTrain: epoch  9, batch    59 | loss: 4.8475919Losses:  4.008097171783447 0.11965764313936234 0.7090117931365967
CurrentTrain: epoch  9, batch    60 | loss: 4.8367662Losses:  3.994967222213745 0.08328624069690704 0.7773855924606323
CurrentTrain: epoch  9, batch    61 | loss: 4.8556390Losses:  4.001224994659424 0.06252595782279968 0.7736916542053223
CurrentTrain: epoch  9, batch    62 | loss: 4.8374424
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.72%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.86%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.15%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.72%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.88%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.86%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.15%   
cur_acc:  ['0.9415']
his_acc:  ['0.9415']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  7.664143085479736 1.810209035873413 0.9918063282966614
CurrentTrain: epoch  0, batch     0 | loss: 10.4661579Losses:  7.9285993576049805 2.2088937759399414 0.9917111992835999
CurrentTrain: epoch  0, batch     1 | loss: 11.1292048Losses:  6.159259796142578 1.7001880407333374 0.9822769165039062
CurrentTrain: epoch  0, batch     2 | loss: 8.8417244Losses:  5.91838264465332 0.7258604168891907 0.9566534757614136
CurrentTrain: epoch  0, batch     3 | loss: 7.6008968Losses:  6.516845703125 1.6832518577575684 0.9684576392173767
CurrentTrain: epoch  1, batch     0 | loss: 9.1685553Losses:  7.007380485534668 1.9571741819381714 0.9745023250579834
CurrentTrain: epoch  1, batch     1 | loss: 9.9390574Losses:  6.433840751647949 1.5051755905151367 0.9930830597877502
CurrentTrain: epoch  1, batch     2 | loss: 8.9320993Losses:  8.639181137084961 0.7723565101623535 1.0386850833892822
CurrentTrain: epoch  1, batch     3 | loss: 10.4502220Losses:  5.831361770629883 1.5008487701416016 0.9784243106842041
CurrentTrain: epoch  2, batch     0 | loss: 8.3106346Losses:  6.1361494064331055 1.7103755474090576 0.9769387245178223
CurrentTrain: epoch  2, batch     1 | loss: 8.8234634Losses:  5.700153350830078 1.5057179927825928 0.9669070839881897
CurrentTrain: epoch  2, batch     2 | loss: 8.1727791Losses:  5.688079357147217 0.4350946843624115 0.9950276613235474
CurrentTrain: epoch  2, batch     3 | loss: 7.1182017Losses:  5.047245502471924 1.4497586488723755 0.9607187509536743
CurrentTrain: epoch  3, batch     0 | loss: 7.4577227Losses:  4.117494106292725 1.2776529788970947 0.9613228440284729
CurrentTrain: epoch  3, batch     1 | loss: 6.3564701Losses:  5.25691032409668 1.417846918106079 0.9893776178359985
CurrentTrain: epoch  3, batch     2 | loss: 7.6641345Losses:  5.410195350646973 0.4701753258705139 0.9749967455863953
CurrentTrain: epoch  3, batch     3 | loss: 6.8553672Losses:  4.557763576507568 1.552379846572876 0.967880368232727
CurrentTrain: epoch  4, batch     0 | loss: 7.0780239Losses:  4.795456886291504 1.4838694334030151 0.965107798576355
CurrentTrain: epoch  4, batch     1 | loss: 7.2444344Losses:  5.176959991455078 1.543737530708313 0.9676581025123596
CurrentTrain: epoch  4, batch     2 | loss: 7.6883554Losses:  2.265577554702759 0.45906397700309753 0.9981456995010376
CurrentTrain: epoch  4, batch     3 | loss: 3.7227874Losses:  4.8154191970825195 1.4820665121078491 0.9516326189041138
CurrentTrain: epoch  5, batch     0 | loss: 7.2491183Losses:  5.081977844238281 1.3467074632644653 0.9713039398193359
CurrentTrain: epoch  5, batch     1 | loss: 7.3999891Losses:  4.286670684814453 1.3363265991210938 0.9673576354980469
CurrentTrain: epoch  5, batch     2 | loss: 6.5903549Losses:  1.7844926118850708 0.25007304549217224 0.9645273685455322
CurrentTrain: epoch  5, batch     3 | loss: 2.9990931Losses:  3.9829297065734863 1.294114589691162 0.9663873314857483
CurrentTrain: epoch  6, batch     0 | loss: 6.2434316Losses:  4.48506498336792 1.210202932357788 0.9653204679489136
CurrentTrain: epoch  6, batch     1 | loss: 6.6605883Losses:  4.461783409118652 1.3246957063674927 0.9361792206764221
CurrentTrain: epoch  6, batch     2 | loss: 6.7226582Losses:  4.9374237060546875 0.21366259455680847 0.9486023783683777
CurrentTrain: epoch  6, batch     3 | loss: 6.0996885Losses:  4.545414447784424 1.3985544443130493 0.9646161794662476
CurrentTrain: epoch  7, batch     0 | loss: 6.9085851Losses:  4.533294677734375 1.1562981605529785 0.9536008834838867
CurrentTrain: epoch  7, batch     1 | loss: 6.6431937Losses:  3.877274990081787 1.2783743143081665 0.9455268383026123
CurrentTrain: epoch  7, batch     2 | loss: 6.1011763Losses:  3.1102590560913086 0.14847075939178467 0.9031757712364197
CurrentTrain: epoch  7, batch     3 | loss: 4.1619058Losses:  4.624000549316406 1.432217001914978 0.9720925092697144
CurrentTrain: epoch  8, batch     0 | loss: 7.0283103Losses:  4.041985988616943 1.2696850299835205 0.9674809575080872
CurrentTrain: epoch  8, batch     1 | loss: 6.2791524Losses:  3.9104175567626953 1.006838083267212 0.9108069539070129
CurrentTrain: epoch  8, batch     2 | loss: 5.8280625Losses:  1.7448158264160156 8.94069742685133e-08 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.7448158Losses:  3.96435809135437 1.063699722290039 0.9479870796203613
CurrentTrain: epoch  9, batch     0 | loss: 5.9760451Losses:  3.9728875160217285 1.2568418979644775 0.9578291177749634
CurrentTrain: epoch  9, batch     1 | loss: 6.1875587Losses:  3.618713140487671 1.0778992176055908 0.9326403737068176
CurrentTrain: epoch  9, batch     2 | loss: 5.6292529Losses:  4.874977111816406 0.5330692529678345 0.9606653451919556
CurrentTrain: epoch  9, batch     3 | loss: 6.3687115
Losses:  1.6106488704681396 1.2657232284545898 0.8967162370681763
MemoryTrain:  epoch  0, batch     0 | loss: 3.7730885Losses:  0.6518364548683167 0.20666621625423431 0.853357195854187
MemoryTrain:  epoch  0, batch     1 | loss: 1.7118599Losses:  1.2461167573928833 1.1544971466064453 0.8789535760879517
MemoryTrain:  epoch  1, batch     0 | loss: 3.2795672Losses:  1.7109448909759521 0.1727234572172165 0.9230799674987793
MemoryTrain:  epoch  1, batch     1 | loss: 2.8067484Losses:  0.7350625991821289 1.060065746307373 0.8653604984283447
MemoryTrain:  epoch  2, batch     0 | loss: 2.6604888Losses:  0.7146317958831787 0.4226619601249695 0.9642243385314941
MemoryTrain:  epoch  2, batch     1 | loss: 2.1015182Losses:  0.18386784195899963 0.7727758884429932 0.8900508880615234
MemoryTrain:  epoch  3, batch     0 | loss: 1.8466946Losses:  0.29403430223464966 0.5630300641059875 0.8902005553245544
MemoryTrain:  epoch  3, batch     1 | loss: 1.7472649Losses:  0.1409977823495865 1.0067856311798096 0.9083777070045471
MemoryTrain:  epoch  4, batch     0 | loss: 2.0561612Losses:  0.02673598751425743 0.21343323588371277 0.7892688512802124
MemoryTrain:  epoch  4, batch     1 | loss: 1.0294380Losses:  0.07524002343416214 1.005228042602539 0.888890266418457
MemoryTrain:  epoch  5, batch     0 | loss: 1.9693583Losses:  0.040805816650390625 0.4231109619140625 0.8644108176231384
MemoryTrain:  epoch  5, batch     1 | loss: 1.3283277Losses:  0.06396068632602692 0.9198112487792969 0.9021964073181152
MemoryTrain:  epoch  6, batch     0 | loss: 1.8859683Losses:  0.059380121529102325 0.34151679277420044 0.8101538419723511
MemoryTrain:  epoch  6, batch     1 | loss: 1.2110507Losses:  0.0318666473031044 0.795256495475769 0.8745354413986206
MemoryTrain:  epoch  7, batch     0 | loss: 1.7016586Losses:  0.023488927632570267 0.4418751299381256 0.9220866560935974
MemoryTrain:  epoch  7, batch     1 | loss: 1.3874507Losses:  0.03018367849290371 0.8273005485534668 0.8739067316055298
MemoryTrain:  epoch  8, batch     0 | loss: 1.7313910Losses:  0.016771212220191956 0.2508472204208374 0.9058514833450317
MemoryTrain:  epoch  8, batch     1 | loss: 1.1734699Losses:  0.0476149320602417 0.9398118853569031 0.856240451335907
MemoryTrain:  epoch  9, batch     0 | loss: 1.8436673Losses:  0.027287034317851067 0.16486896574497223 0.9825954437255859
MemoryTrain:  epoch  9, batch     1 | loss: 1.1747514
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 77.36%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 76.63%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 76.33%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 76.43%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 76.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.68%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.85%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 76.54%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 76.08%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 75.42%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 74.18%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 73.19%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.62%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.81%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 94.41%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.29%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.27%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 94.06%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.05%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 93.55%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 92.87%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 91.83%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 90.53%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 89.74%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 89.06%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 88.32%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 88.20%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 87.85%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 87.59%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 87.25%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 87.10%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 86.95%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 86.80%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 86.97%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.76%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 86.62%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 86.70%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 86.64%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.87%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.01%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.43%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.82%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.94%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.06%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 87.87%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 87.68%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 87.56%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 87.44%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 87.44%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 87.32%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 87.21%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 86.81%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 86.58%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 86.48%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 86.26%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 86.10%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 86.17%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 86.13%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.16%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.23%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 85.57%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 85.18%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 84.89%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 84.35%   [EVAL] batch:  123 | acc: 31.25%,  total acc: 83.92%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 83.65%   
cur_acc:  ['0.9415', '0.7262']
his_acc:  ['0.9415', '0.8365']
Clustering into  14  clusters
Clusters:  [ 2 11 13  2  2  2  0  2  9  7  6  2  8  2 10  0  4  2  2  2  0 12  6  2
  2  3  2  1  2  5]
Losses:  7.041440010070801 1.6720445156097412 0.9913709163665771
CurrentTrain: epoch  0, batch     0 | loss: 9.7048559Losses:  6.650213241577148 1.7880618572235107 0.9818679690361023
CurrentTrain: epoch  0, batch     1 | loss: 9.4201431Losses:  6.457646369934082 1.6897385120391846 0.9867573976516724
CurrentTrain: epoch  0, batch     2 | loss: 9.1341419Losses:  7.02951192855835 0.500003457069397 0.977573812007904
CurrentTrain: epoch  0, batch     3 | loss: 8.5070887Losses:  6.08823299407959 1.8241064548492432 0.992389440536499
CurrentTrain: epoch  1, batch     0 | loss: 8.9047289Losses:  5.474778175354004 1.6074637174606323 0.9667161703109741
CurrentTrain: epoch  1, batch     1 | loss: 8.0489578Losses:  5.459680557250977 1.5120553970336914 0.9889324307441711
CurrentTrain: epoch  1, batch     2 | loss: 7.9606686Losses:  5.053457260131836 0.4638594090938568 0.9345471262931824
CurrentTrain: epoch  1, batch     3 | loss: 6.4518638Losses:  5.002195835113525 1.5952308177947998 0.9724628329277039
CurrentTrain: epoch  2, batch     0 | loss: 7.5698891Losses:  5.029514312744141 1.4990043640136719 0.9738855361938477
CurrentTrain: epoch  2, batch     1 | loss: 7.5024042Losses:  5.173039436340332 1.5507125854492188 0.9830379486083984
CurrentTrain: epoch  2, batch     2 | loss: 7.7067900Losses:  3.113097906112671 0.24371740221977234 1.0
CurrentTrain: epoch  2, batch     3 | loss: 4.3568153Losses:  3.6922130584716797 1.1579062938690186 0.9649009704589844
CurrentTrain: epoch  3, batch     0 | loss: 5.8150206Losses:  4.807419776916504 1.4484100341796875 0.9714536666870117
CurrentTrain: epoch  3, batch     1 | loss: 7.2272835Losses:  5.7825775146484375 1.6657884120941162 0.9923542737960815
CurrentTrain: epoch  3, batch     2 | loss: 8.4407206Losses:  4.7829999923706055 0.33813080191612244 0.9112930297851562
CurrentTrain: epoch  3, batch     3 | loss: 6.0324240Losses:  4.829843521118164 1.435340166091919 0.9783360958099365
CurrentTrain: epoch  4, batch     0 | loss: 7.2435198Losses:  4.199710845947266 1.291552186012268 0.9465423822402954
CurrentTrain: epoch  4, batch     1 | loss: 6.4378052Losses:  3.725374698638916 1.0991370677947998 0.9786075353622437
CurrentTrain: epoch  4, batch     2 | loss: 5.8031192Losses:  5.885080337524414 0.35691604018211365 0.9952021837234497
CurrentTrain: epoch  4, batch     3 | loss: 7.2371984Losses:  3.9613595008850098 1.2049188613891602 0.9590268135070801
CurrentTrain: epoch  5, batch     0 | loss: 6.1253052Losses:  4.1964874267578125 1.0924073457717896 0.9554702639579773
CurrentTrain: epoch  5, batch     1 | loss: 6.2443647Losses:  4.0052008628845215 1.3585271835327148 0.974891185760498
CurrentTrain: epoch  5, batch     2 | loss: 6.3386192Losses:  4.587834358215332 0.3509226441383362 0.9895275235176086
CurrentTrain: epoch  5, batch     3 | loss: 5.9282846Losses:  4.496919631958008 1.377892017364502 0.9743790626525879
CurrentTrain: epoch  6, batch     0 | loss: 6.8491907Losses:  3.67586088180542 1.2327210903167725 0.9438390135765076
CurrentTrain: epoch  6, batch     1 | loss: 5.8524208Losses:  3.4461114406585693 1.259647250175476 0.9663186073303223
CurrentTrain: epoch  6, batch     2 | loss: 5.6720772Losses:  5.360097885131836 0.483898401260376 1.0107908248901367
CurrentTrain: epoch  6, batch     3 | loss: 6.8547869Losses:  3.580376386642456 1.1147441864013672 0.9639192819595337
CurrentTrain: epoch  7, batch     0 | loss: 5.6590400Losses:  4.058006763458252 1.2068878412246704 0.9711568355560303
CurrentTrain: epoch  7, batch     1 | loss: 6.2360516Losses:  3.8035168647766113 0.9470676183700562 0.9370824098587036
CurrentTrain: epoch  7, batch     2 | loss: 5.6876669Losses:  2.615250825881958 0.1843208223581314 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.7995718Losses:  3.27785062789917 1.04295015335083 0.9466842412948608
CurrentTrain: epoch  8, batch     0 | loss: 5.2674851Losses:  3.8250784873962402 1.1454417705535889 0.9750953912734985
CurrentTrain: epoch  8, batch     1 | loss: 5.9456153Losses:  3.396275520324707 0.9673754572868347 0.951454758644104
CurrentTrain: epoch  8, batch     2 | loss: 5.3151054Losses:  5.651928424835205 0.33406639099121094 0.9284465909004211
CurrentTrain: epoch  8, batch     3 | loss: 6.9144416Losses:  4.076600074768066 1.2781047821044922 0.9575608372688293
CurrentTrain: epoch  9, batch     0 | loss: 6.3122659Losses:  3.079653263092041 0.9496878385543823 0.9439705610275269
CurrentTrain: epoch  9, batch     1 | loss: 4.9733119Losses:  3.2363057136535645 1.102383017539978 0.9522200226783752
CurrentTrain: epoch  9, batch     2 | loss: 5.2909088Losses:  2.3288416862487793 0.3780324459075928 0.9982549548149109
CurrentTrain: epoch  9, batch     3 | loss: 3.7051291
Losses:  1.0976744890213013 1.2996995449066162 0.9114199280738831
MemoryTrain:  epoch  0, batch     0 | loss: 3.3087940Losses:  0.38459354639053345 0.7955122590065002 0.9074702262878418
MemoryTrain:  epoch  0, batch     1 | loss: 2.0875759Losses:  1.3712252378463745 1.210892677307129 0.8909826278686523
MemoryTrain:  epoch  1, batch     0 | loss: 3.4731007Losses:  0.844351053237915 0.8164989352226257 0.9253585338592529
MemoryTrain:  epoch  1, batch     1 | loss: 2.5862086Losses:  1.105669379234314 1.1616249084472656 0.9039221405982971
MemoryTrain:  epoch  2, batch     0 | loss: 3.1712165Losses:  0.23518916964530945 0.8429582118988037 0.9043265581130981
MemoryTrain:  epoch  2, batch     1 | loss: 1.9824740Losses:  0.18375736474990845 0.891464114189148 0.8995660543441772
MemoryTrain:  epoch  3, batch     0 | loss: 1.9747876Losses:  0.39329084753990173 1.1318731307983398 0.9057601094245911
MemoryTrain:  epoch  3, batch     1 | loss: 2.4309242Losses:  0.24473558366298676 1.0799510478973389 0.8921587467193604
MemoryTrain:  epoch  4, batch     0 | loss: 2.2168455Losses:  0.0711367055773735 0.895116925239563 0.9160898327827454
MemoryTrain:  epoch  4, batch     1 | loss: 1.8823435Losses:  0.11408640444278717 1.0303606986999512 0.9125158786773682
MemoryTrain:  epoch  5, batch     0 | loss: 2.0569630Losses:  0.0646950900554657 0.8416447043418884 0.883222758769989
MemoryTrain:  epoch  5, batch     1 | loss: 1.7895625Losses:  0.10641410946846008 0.9384861588478088 0.9042642712593079
MemoryTrain:  epoch  6, batch     0 | loss: 1.9491646Losses:  0.1281772255897522 0.9257580637931824 0.8944615125656128
MemoryTrain:  epoch  6, batch     1 | loss: 1.9483968Losses:  0.0941953957080841 0.9675542116165161 0.9125190377235413
MemoryTrain:  epoch  7, batch     0 | loss: 1.9742687Losses:  0.07034264504909515 0.8614833950996399 0.8832300901412964
MemoryTrain:  epoch  7, batch     1 | loss: 1.8150561Losses:  0.03971753641963005 1.0435622930526733 0.8928570747375488
MemoryTrain:  epoch  8, batch     0 | loss: 1.9761369Losses:  0.0567961260676384 0.7519450783729553 0.9041638374328613
MemoryTrain:  epoch  8, batch     1 | loss: 1.7129050Losses:  0.026878835633397102 0.8669601678848267 0.8728265762329102
MemoryTrain:  epoch  9, batch     0 | loss: 1.7666656Losses:  0.08523319661617279 0.9476302266120911 0.9206572771072388
MemoryTrain:  epoch  9, batch     1 | loss: 1.9535207
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 50.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 61.21%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 60.62%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 59.48%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 60.35%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 61.17%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 70.08%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.15%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.32%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.22%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.83%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.84%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 91.80%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 90.25%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 89.46%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 88.97%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 88.41%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 88.12%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 87.85%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.42%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.42%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 87.01%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 86.77%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 86.54%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 86.08%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 85.31%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 84.53%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 83.58%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 82.59%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 81.69%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 80.10%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 79.83%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 82.05%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 81.98%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 81.97%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 82.13%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 81.95%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 81.60%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 81.08%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 80.97%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 80.64%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 80.59%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.88%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 80.47%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 80.27%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 80.02%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 79.52%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 79.18%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 78.95%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 78.52%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 78.05%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 77.59%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 77.28%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 76.88%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 76.57%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.66%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 76.82%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 76.85%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.10%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 77.04%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 76.66%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 76.34%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.02%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 75.79%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.30%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 76.12%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 75.95%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 75.61%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 75.40%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.08%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.73%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 76.10%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.20%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 76.58%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 76.53%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 76.34%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 76.29%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 76.29%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 76.07%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 75.88%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 75.87%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 75.80%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 75.79%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.82%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 75.81%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.87%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 75.66%   
cur_acc:  ['0.9415', '0.7262', '0.6915']
his_acc:  ['0.9415', '0.8365', '0.7566']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15  2  1 12  1 10  0  4  1  1  1 17 16  2  1
  1  5  1 18  1  8  1 14  1  1  7  1  1  1  3  6]
Losses:  5.423678398132324 1.5082149505615234 0.9704418182373047
CurrentTrain: epoch  0, batch     0 | loss: 7.9023352Losses:  5.867494583129883 1.3512810468673706 0.9748744964599609
CurrentTrain: epoch  0, batch     1 | loss: 8.1936502Losses:  6.010179042816162 1.525993824005127 0.987398624420166
CurrentTrain: epoch  0, batch     2 | loss: 8.5235710Losses:  7.019036293029785 0.13928619027137756 0.9815310835838318
CurrentTrain: epoch  0, batch     3 | loss: 8.1398535Losses:  6.1103692054748535 1.4446402788162231 0.9808641672134399
CurrentTrain: epoch  1, batch     0 | loss: 8.5358734Losses:  5.042895317077637 1.4501322507858276 0.9815514087677002
CurrentTrain: epoch  1, batch     1 | loss: 7.4745789Losses:  3.943164825439453 1.3269810676574707 0.9554910659790039
CurrentTrain: epoch  1, batch     2 | loss: 6.2256370Losses:  5.460253715515137 0.3517051935195923 0.949519157409668
CurrentTrain: epoch  1, batch     3 | loss: 6.7614779Losses:  4.568998336791992 1.3106133937835693 0.9691572189331055
CurrentTrain: epoch  2, batch     0 | loss: 6.8487692Losses:  4.58999490737915 1.1977300643920898 0.9647408723831177
CurrentTrain: epoch  2, batch     1 | loss: 6.7524657Losses:  4.549411296844482 1.5369868278503418 0.9720737934112549
CurrentTrain: epoch  2, batch     2 | loss: 7.0584717Losses:  3.7489068508148193 0.3633289337158203 1.0
CurrentTrain: epoch  2, batch     3 | loss: 5.1122360Losses:  4.54541015625 1.3706345558166504 0.9627103209495544
CurrentTrain: epoch  3, batch     0 | loss: 6.8787551Losses:  4.409835338592529 1.1730314493179321 0.9647501707077026
CurrentTrain: epoch  3, batch     1 | loss: 6.5476170Losses:  3.922574520111084 1.3622140884399414 0.9798948764801025
CurrentTrain: epoch  3, batch     2 | loss: 6.2646837Losses:  2.8922996520996094 0.42935609817504883 0.9595060348510742
CurrentTrain: epoch  3, batch     3 | loss: 4.2811618Losses:  3.6995677947998047 1.3736788034439087 0.9755493402481079
CurrentTrain: epoch  4, batch     0 | loss: 6.0487957Losses:  4.472857475280762 1.2799038887023926 0.9647014141082764
CurrentTrain: epoch  4, batch     1 | loss: 6.7174625Losses:  3.9825377464294434 1.314044713973999 0.9607638716697693
CurrentTrain: epoch  4, batch     2 | loss: 6.2573462Losses:  2.7064549922943115 0.32216715812683105 0.9229174852371216
CurrentTrain: epoch  4, batch     3 | loss: 3.9515395Losses:  4.51065731048584 1.2866568565368652 0.9490046501159668
CurrentTrain: epoch  5, batch     0 | loss: 6.7463188Losses:  3.152162551879883 1.1086137294769287 0.9754341840744019
CurrentTrain: epoch  5, batch     1 | loss: 5.2362108Losses:  3.714400053024292 1.0117902755737305 0.9632664918899536
CurrentTrain: epoch  5, batch     2 | loss: 5.6894569Losses:  4.44536018371582 0.3147355914115906 1.0
CurrentTrain: epoch  5, batch     3 | loss: 5.7600956Losses:  3.2926578521728516 1.2724452018737793 0.9567227363586426
CurrentTrain: epoch  6, batch     0 | loss: 5.5218258Losses:  3.636143207550049 1.2588322162628174 0.9574475288391113
CurrentTrain: epoch  6, batch     1 | loss: 5.8524232Losses:  4.031328201293945 1.1990797519683838 0.9621012210845947
CurrentTrain: epoch  6, batch     2 | loss: 6.1925087Losses:  1.7562029361724854 0.06014694273471832 0.9590095281600952
CurrentTrain: epoch  6, batch     3 | loss: 2.7753594Losses:  2.9058279991149902 0.9941369891166687 0.9563721418380737
CurrentTrain: epoch  7, batch     0 | loss: 4.8563371Losses:  3.206333637237549 0.8458206057548523 0.9523718357086182
CurrentTrain: epoch  7, batch     1 | loss: 5.0045261Losses:  4.069762229919434 1.2481777667999268 0.9579424262046814
CurrentTrain: epoch  7, batch     2 | loss: 6.2758822Losses:  3.7622342109680176 0.32266706228256226 0.9583302140235901
CurrentTrain: epoch  7, batch     3 | loss: 5.0432315Losses:  3.8132429122924805 1.0078459978103638 0.9564683437347412
CurrentTrain: epoch  8, batch     0 | loss: 5.7775574Losses:  3.130506992340088 1.1328320503234863 0.9515771865844727
CurrentTrain: epoch  8, batch     1 | loss: 5.2149162Losses:  2.856266736984253 0.907792329788208 0.9378315806388855
CurrentTrain: epoch  8, batch     2 | loss: 4.7018905Losses:  1.9526934623718262 0.20371223986148834 0.9554880261421204
CurrentTrain: epoch  8, batch     3 | loss: 3.1118937Losses:  3.3477425575256348 0.9038518667221069 0.9495009183883667
CurrentTrain: epoch  9, batch     0 | loss: 5.2010956Losses:  3.0219173431396484 1.116353154182434 0.9663494229316711
CurrentTrain: epoch  9, batch     1 | loss: 5.1046200Losses:  3.0266928672790527 1.1517552137374878 0.9341820478439331
CurrentTrain: epoch  9, batch     2 | loss: 5.1126304Losses:  1.9688671827316284 0.0577722042798996 0.9412044286727905
CurrentTrain: epoch  9, batch     3 | loss: 2.9678440
Losses:  0.7461066246032715 1.104708194732666 0.9120104312896729
MemoryTrain:  epoch  0, batch     0 | loss: 2.7628253Losses:  1.2600446939468384 0.9581716656684875 0.911470890045166
MemoryTrain:  epoch  0, batch     1 | loss: 3.1296873Losses:  0.2742972671985626 0.6488126516342163 0.8995139598846436
MemoryTrain:  epoch  0, batch     2 | loss: 1.8226238Losses:  1.488467812538147 1.2022727727890015 0.8992865085601807
MemoryTrain:  epoch  1, batch     0 | loss: 3.5900271Losses:  0.44522321224212646 1.0263766050338745 0.9159855842590332
MemoryTrain:  epoch  1, batch     1 | loss: 2.3875854Losses:  0.9935131669044495 0.6083462834358215 0.9375395774841309
MemoryTrain:  epoch  1, batch     2 | loss: 2.5393991Losses:  0.7648802399635315 0.9032077789306641 0.9106628894805908
MemoryTrain:  epoch  2, batch     0 | loss: 2.5787508Losses:  0.5269163250923157 1.184943437576294 0.9214993715286255
MemoryTrain:  epoch  2, batch     1 | loss: 2.6333590Losses:  0.21369293332099915 0.5764814615249634 0.8708568215370178
MemoryTrain:  epoch  2, batch     2 | loss: 1.6610312Losses:  0.8465049266815186 1.1183658838272095 0.9010839462280273
MemoryTrain:  epoch  3, batch     0 | loss: 2.8659549Losses:  0.33970534801483154 1.0795708894729614 0.9267247915267944
MemoryTrain:  epoch  3, batch     1 | loss: 2.3460011Losses:  0.0693584680557251 0.4018661081790924 0.8727605938911438
MemoryTrain:  epoch  3, batch     2 | loss: 1.3439852Losses:  0.14983031153678894 1.1155714988708496 0.9128233790397644
MemoryTrain:  epoch  4, batch     0 | loss: 2.1782253Losses:  0.24241049587726593 0.8599724769592285 0.872480571269989
MemoryTrain:  epoch  4, batch     1 | loss: 1.9748635Losses:  1.2001268863677979 0.6033363342285156 0.952682614326477
MemoryTrain:  epoch  4, batch     2 | loss: 2.7561460Losses:  0.20347274839878082 1.1883349418640137 0.8970509767532349
MemoryTrain:  epoch  5, batch     0 | loss: 2.2888587Losses:  0.15010854601860046 0.6900838613510132 0.9091436266899109
MemoryTrain:  epoch  5, batch     1 | loss: 1.7493360Losses:  0.20786839723587036 0.8983817100524902 0.9260947108268738
MemoryTrain:  epoch  5, batch     2 | loss: 2.0323448Losses:  0.08222578465938568 0.9773596525192261 0.9365649223327637
MemoryTrain:  epoch  6, batch     0 | loss: 1.9961504Losses:  0.13645216822624207 1.0525559186935425 0.8944047093391418
MemoryTrain:  epoch  6, batch     1 | loss: 2.0834129Losses:  0.39948180317878723 0.4615360498428345 0.8619053363800049
MemoryTrain:  epoch  6, batch     2 | loss: 1.7229232Losses:  0.12431580573320389 1.0126789808273315 0.9073499441146851
MemoryTrain:  epoch  7, batch     0 | loss: 2.0443449Losses:  0.13991865515708923 0.9442434906959534 0.9208114147186279
MemoryTrain:  epoch  7, batch     1 | loss: 2.0049734Losses:  0.10993384569883347 0.4477187395095825 0.864700436592102
MemoryTrain:  epoch  7, batch     2 | loss: 1.4223530Losses:  0.08627389371395111 1.0600380897521973 0.9078265428543091
MemoryTrain:  epoch  8, batch     0 | loss: 2.0541387Losses:  0.1185922622680664 0.9139640927314758 0.909548282623291
MemoryTrain:  epoch  8, batch     1 | loss: 1.9421046Losses:  0.03459712117910385 0.4879305064678192 0.8770521879196167
MemoryTrain:  epoch  8, batch     2 | loss: 1.3995798Losses:  0.03658520057797432 0.7512561082839966 0.8735769987106323
MemoryTrain:  epoch  9, batch     0 | loss: 1.6614183Losses:  0.11746702343225479 0.9725019931793213 0.9248778820037842
MemoryTrain:  epoch  9, batch     1 | loss: 2.0148468Losses:  0.049860864877700806 0.7220752239227295 0.9006264805793762
MemoryTrain:  epoch  9, batch     2 | loss: 1.6725626
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 48.61%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 48.68%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 51.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.40%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 65.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 65.41%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 67.11%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 66.53%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.87%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 88.35%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 87.77%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.92%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.57%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 92.21%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 90.06%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.72%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 89.29%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 88.57%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 88.17%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 87.31%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 86.57%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 85.94%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 85.33%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.36%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 85.21%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 84.90%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.85%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.80%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.62%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 84.50%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 84.29%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 83.94%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 83.20%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 83.18%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 82.55%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 81.63%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 80.65%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 79.78%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 79.00%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 78.23%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.19%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 80.52%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 80.53%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 80.43%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 79.86%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.42%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 78.86%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 78.49%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.01%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 77.88%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.18%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.36%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 77.76%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 77.22%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 76.74%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 76.22%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 75.81%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 75.50%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 75.10%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 74.66%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 74.22%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 73.93%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 73.61%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 73.33%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.48%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 74.05%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 73.65%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 73.30%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 72.96%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 72.76%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.55%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.31%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 73.19%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 73.04%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 72.82%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 72.52%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 72.61%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 73.14%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 73.16%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 73.02%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 72.80%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 72.55%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 72.34%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 72.10%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 71.86%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.58%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 71.49%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 71.22%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 71.06%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 70.91%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 70.68%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.53%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 70.44%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 70.33%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 70.25%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 70.10%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 70.52%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 70.48%   [EVAL] batch:  188 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 69.97%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 69.73%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 69.47%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 69.24%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 69.04%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 69.37%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 68.84%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 68.69%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 68.54%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 69.76%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 70.27%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  225 | acc: 18.75%,  total acc: 70.38%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 70.13%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 69.70%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 69.51%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 69.59%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 69.99%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 69.85%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 69.74%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.56%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 69.51%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 69.53%   
cur_acc:  ['0.9415', '0.7262', '0.6915', '0.6587']
his_acc:  ['0.9415', '0.8365', '0.7566', '0.6953']
Clustering into  24  clusters
Clusters:  [ 0 23 13  0  0  0 21  0 12 15 20  0 14  0 22 17 11  0  0  0 19 18  1  0
  0  6  0  9  0 10  0 16  0  5  8  0  0  0  2  7  3  0  0  1  1  4  0  0
  0  0]
Losses:  7.194277763366699 1.885480523109436 0.9772132039070129
CurrentTrain: epoch  0, batch     0 | loss: 10.0569715Losses:  6.768627643585205 1.836075782775879 0.9671245813369751
CurrentTrain: epoch  0, batch     1 | loss: 9.5718288Losses:  6.643131732940674 1.8461270332336426 0.9727026224136353
CurrentTrain: epoch  0, batch     2 | loss: 9.4619617Losses:  6.9316253662109375 0.6202077269554138 0.9893842339515686
CurrentTrain: epoch  0, batch     3 | loss: 8.5412178Losses:  6.2545576095581055 2.0315561294555664 0.9793297052383423
CurrentTrain: epoch  1, batch     0 | loss: 9.2654438Losses:  6.145771026611328 1.6537357568740845 0.9641731381416321
CurrentTrain: epoch  1, batch     1 | loss: 8.7636795Losses:  5.173429489135742 1.6252272129058838 0.9468851089477539
CurrentTrain: epoch  1, batch     2 | loss: 7.7455416Losses:  5.2502007484436035 0.44744163751602173 0.9810056686401367
CurrentTrain: epoch  1, batch     3 | loss: 6.6786480Losses:  4.693613052368164 1.5105252265930176 0.952824592590332
CurrentTrain: epoch  2, batch     0 | loss: 7.1569629Losses:  5.340671062469482 1.6832828521728516 0.9793957471847534
CurrentTrain: epoch  2, batch     1 | loss: 8.0033493Losses:  4.911713600158691 1.5706729888916016 0.9538557529449463
CurrentTrain: epoch  2, batch     2 | loss: 7.4362421Losses:  7.466182708740234 0.5594689249992371 0.955615222454071
CurrentTrain: epoch  2, batch     3 | loss: 8.9812670Losses:  4.879744052886963 1.5031979084014893 0.9601156115531921
CurrentTrain: epoch  3, batch     0 | loss: 7.3430576Losses:  5.011153697967529 1.395735263824463 0.9490032196044922
CurrentTrain: epoch  3, batch     1 | loss: 7.3558922Losses:  3.99017596244812 1.5385921001434326 0.9573438167572021
CurrentTrain: epoch  3, batch     2 | loss: 6.4861116Losses:  3.7032947540283203 0.2601029872894287 0.9127360582351685
CurrentTrain: epoch  3, batch     3 | loss: 4.8761339Losses:  4.581798553466797 1.7186003923416138 0.9507985711097717
CurrentTrain: epoch  4, batch     0 | loss: 7.2511973Losses:  4.204706192016602 1.5984941720962524 0.952041506767273
CurrentTrain: epoch  4, batch     1 | loss: 6.7552419Losses:  4.129932880401611 1.6006832122802734 0.9615726470947266
CurrentTrain: epoch  4, batch     2 | loss: 6.6921887Losses:  3.0077402591705322 5.960464477539063e-08 0.8114235401153564
CurrentTrain: epoch  4, batch     3 | loss: 3.8191638Losses:  3.868502378463745 1.6189539432525635 0.9378407001495361
CurrentTrain: epoch  5, batch     0 | loss: 6.4252968Losses:  3.2067089080810547 1.2899214029312134 0.942196249961853
CurrentTrain: epoch  5, batch     1 | loss: 5.4388266Losses:  4.062105178833008 1.4432042837142944 0.950437068939209
CurrentTrain: epoch  5, batch     2 | loss: 6.4557467Losses:  6.876459121704102 0.4544076919555664 0.9346359968185425
CurrentTrain: epoch  5, batch     3 | loss: 8.2655029Losses:  3.5712056159973145 1.457625389099121 0.9386987686157227
CurrentTrain: epoch  6, batch     0 | loss: 5.9675298Losses:  4.115176200866699 1.491761326789856 0.9382060766220093
CurrentTrain: epoch  6, batch     1 | loss: 6.5451436Losses:  3.4649808406829834 1.535538911819458 0.9472451210021973
CurrentTrain: epoch  6, batch     2 | loss: 5.9477649Losses:  4.3735127449035645 0.33673393726348877 0.9215800762176514
CurrentTrain: epoch  6, batch     3 | loss: 5.6318264Losses:  4.089165210723877 1.418144941329956 0.9432539939880371
CurrentTrain: epoch  7, batch     0 | loss: 6.4505639Losses:  3.6732218265533447 1.3520971536636353 0.9319978952407837
CurrentTrain: epoch  7, batch     1 | loss: 5.9573169Losses:  2.8903894424438477 1.2089356184005737 0.9387781620025635
CurrentTrain: epoch  7, batch     2 | loss: 5.0381031Losses:  1.953465461730957 0.12241898477077484 0.842802882194519
CurrentTrain: epoch  7, batch     3 | loss: 2.9186873Losses:  3.4319329261779785 1.3181053400039673 0.9296993017196655
CurrentTrain: epoch  8, batch     0 | loss: 5.6797376Losses:  3.2445597648620605 1.396437644958496 0.9224176406860352
CurrentTrain: epoch  8, batch     1 | loss: 5.5634151Losses:  2.993669033050537 1.1747586727142334 0.945713996887207
CurrentTrain: epoch  8, batch     2 | loss: 5.1141415Losses:  2.2497341632843018 0.395426869392395 0.9034537076950073
CurrentTrain: epoch  8, batch     3 | loss: 3.5486150Losses:  3.4489078521728516 1.2772189378738403 0.9184417724609375
CurrentTrain: epoch  9, batch     0 | loss: 5.6445684Losses:  2.8938660621643066 1.3328081369400024 0.9401019811630249
CurrentTrain: epoch  9, batch     1 | loss: 5.1667762Losses:  2.6021437644958496 1.1298034191131592 0.9326961040496826
CurrentTrain: epoch  9, batch     2 | loss: 4.6646433Losses:  3.0912117958068848 0.2740001380443573 0.8900158405303955
CurrentTrain: epoch  9, batch     3 | loss: 4.2552280
Losses:  0.6183077096939087 1.02980375289917 0.9132081270217896
MemoryTrain:  epoch  0, batch     0 | loss: 2.5613196Losses:  0.2812173366546631 1.0784597396850586 0.9215899705886841
MemoryTrain:  epoch  0, batch     1 | loss: 2.2812672Losses:  0.689569890499115 1.2844009399414062 0.9214769601821899
MemoryTrain:  epoch  0, batch     2 | loss: 2.8954477Losses:  0.08169373869895935 0.045364003628492355 0.8527144193649292
MemoryTrain:  epoch  0, batch     3 | loss: 0.9797722Losses:  0.8252736330032349 1.2894985675811768 0.9022233486175537
MemoryTrain:  epoch  1, batch     0 | loss: 3.0169957Losses:  0.5849115252494812 1.1823396682739258 0.9248420000076294
MemoryTrain:  epoch  1, batch     1 | loss: 2.6920934Losses:  0.5084228515625 0.8733500242233276 0.8945674300193787
MemoryTrain:  epoch  1, batch     2 | loss: 2.2763402Losses:  0.2978273034095764 0.04981987550854683 1.0
MemoryTrain:  epoch  1, batch     3 | loss: 1.3476472Losses:  0.300625205039978 1.0471744537353516 0.8967124223709106
MemoryTrain:  epoch  2, batch     0 | loss: 2.2445121Losses:  0.6116821765899658 1.0943491458892822 0.9108926057815552
MemoryTrain:  epoch  2, batch     1 | loss: 2.6169238Losses:  0.18112784624099731 1.0661066770553589 0.9285767078399658
MemoryTrain:  epoch  2, batch     2 | loss: 2.1758113Losses:  0.11990906298160553 0.20240247249603271 0.9254486560821533
MemoryTrain:  epoch  2, batch     3 | loss: 1.2477602Losses:  0.07564424723386765 0.8641358613967896 0.9499623775482178
MemoryTrain:  epoch  3, batch     0 | loss: 1.8897425Losses:  0.35962411761283875 1.1298459768295288 0.8787837624549866
MemoryTrain:  epoch  3, batch     1 | loss: 2.3682539Losses:  0.28930121660232544 1.13771390914917 0.9018369913101196
MemoryTrain:  epoch  3, batch     2 | loss: 2.3288522Losses:  0.07563371956348419 0.05189307779073715 0.963275134563446
MemoryTrain:  epoch  3, batch     3 | loss: 1.0908020Losses:  0.2219127118587494 1.0968501567840576 0.9147710800170898
MemoryTrain:  epoch  4, batch     0 | loss: 2.2335339Losses:  0.07914234697818756 1.0164432525634766 0.8925631046295166
MemoryTrain:  epoch  4, batch     1 | loss: 1.9881487Losses:  0.14537104964256287 0.8587959408760071 0.9209280014038086
MemoryTrain:  epoch  4, batch     2 | loss: 1.9250950Losses:  0.3771388828754425 0.4302690029144287 0.9290152788162231
MemoryTrain:  epoch  4, batch     3 | loss: 1.7364231Losses:  0.14852480590343475 1.2030913829803467 0.9293750524520874
MemoryTrain:  epoch  5, batch     0 | loss: 2.2809911Losses:  0.057766206562519073 0.7462338209152222 0.8713818192481995
MemoryTrain:  epoch  5, batch     1 | loss: 1.6753819Losses:  0.10935130715370178 0.9776824712753296 0.9369239211082458
MemoryTrain:  epoch  5, batch     2 | loss: 2.0239577Losses:  0.06478153169155121 0.1501736342906952 0.7754871249198914
MemoryTrain:  epoch  5, batch     3 | loss: 0.9904423Losses:  0.04680517315864563 0.9850987195968628 0.8987644910812378
MemoryTrain:  epoch  6, batch     0 | loss: 1.9306684Losses:  0.08434653282165527 1.0646346807479858 0.8997836112976074
MemoryTrain:  epoch  6, batch     1 | loss: 2.0487647Losses:  0.054118767380714417 0.8544566631317139 0.9163495302200317
MemoryTrain:  epoch  6, batch     2 | loss: 1.8249249Losses:  0.04327584430575371 0.036094844341278076 0.9207841753959656
MemoryTrain:  epoch  6, batch     3 | loss: 1.0001549Losses:  0.0763358548283577 1.0787023305892944 0.9103683829307556
MemoryTrain:  epoch  7, batch     0 | loss: 2.0654066Losses:  0.044431962072849274 0.8396656513214111 0.9393858909606934
MemoryTrain:  epoch  7, batch     1 | loss: 1.8234835Losses:  0.06383293867111206 0.9263378381729126 0.8735566139221191
MemoryTrain:  epoch  7, batch     2 | loss: 1.8637273Losses:  0.06185115873813629 0.08869028091430664 0.8093180656433105
MemoryTrain:  epoch  7, batch     3 | loss: 0.9598595Losses:  0.05416083708405495 0.966350793838501 0.9337116479873657
MemoryTrain:  epoch  8, batch     0 | loss: 1.9542233Losses:  0.0477498322725296 1.0176327228546143 0.8763183355331421
MemoryTrain:  epoch  8, batch     1 | loss: 1.9417009Losses:  0.04188557341694832 0.7741925716400146 0.8957091569900513
MemoryTrain:  epoch  8, batch     2 | loss: 1.7117872Losses:  0.038601335138082504 0.07190737873315811 0.9606961607933044
MemoryTrain:  epoch  8, batch     3 | loss: 1.0712049Losses:  0.07841897010803223 1.0302485227584839 0.8915501832962036
MemoryTrain:  epoch  9, batch     0 | loss: 2.0002177Losses:  0.03982628881931305 0.8012523651123047 0.8965129256248474
MemoryTrain:  epoch  9, batch     1 | loss: 1.7375915Losses:  0.07517209649085999 0.775835394859314 0.905401349067688
MemoryTrain:  epoch  9, batch     2 | loss: 1.7564088Losses:  0.026765186339616776 0.08778892457485199 0.9183741807937622
MemoryTrain:  epoch  9, batch     3 | loss: 1.0329282
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.82%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 54.36%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 55.15%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 57.29%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 57.94%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 58.55%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 59.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 63.61%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 63.59%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.70%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 63.60%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 64.51%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 64.14%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 63.79%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 63.45%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 63.42%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 63.51%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.80%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.69%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.99%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.09%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.80%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 89.44%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 89.09%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 88.75%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.52%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 88.21%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 87.70%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 87.01%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 86.63%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 86.17%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 85.45%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 84.74%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 84.24%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 84.20%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 83.98%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 83.73%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 83.61%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 83.63%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 83.17%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 82.83%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 82.27%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 82.25%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 81.63%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 80.72%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 79.84%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 78.90%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 77.98%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 77.23%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 76.92%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.43%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.19%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 79.60%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 79.61%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 79.63%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 79.76%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 79.83%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 79.50%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.88%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 78.38%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 77.73%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 77.36%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 76.84%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 76.66%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.15%   [EVAL] batch:  119 | acc: 25.00%,  total acc: 76.72%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 76.24%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 75.77%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 75.36%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 74.95%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 74.65%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 74.31%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 73.92%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 73.54%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 73.26%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 72.93%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 72.71%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.09%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.40%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.98%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 72.68%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 72.30%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 71.90%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.66%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.57%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 72.49%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 72.34%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 72.18%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 71.92%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 72.05%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 72.20%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 71.99%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 71.78%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 71.54%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 71.21%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 71.08%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 70.88%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 70.80%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 70.64%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 70.48%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 70.33%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 69.96%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 69.84%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 69.70%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.73%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 69.83%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 69.88%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 69.21%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 68.95%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 68.72%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 68.52%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 68.66%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 68.48%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 68.29%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.81%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 69.47%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 70.08%   [EVAL] batch:  226 | acc: 18.75%,  total acc: 69.85%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.82%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:  230 | acc: 18.75%,  total acc: 69.24%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 69.73%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 69.74%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:  240 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 69.58%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 69.55%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 69.34%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 69.23%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 69.16%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 69.05%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 68.92%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 68.82%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 68.68%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  254 | acc: 12.50%,  total acc: 68.36%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 68.21%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 68.73%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 68.98%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 68.94%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 68.82%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 68.77%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 68.52%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 68.28%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 68.03%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 67.79%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 67.54%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 67.30%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 67.29%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 68.20%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 68.17%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 68.24%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 68.10%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 68.02%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 68.02%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  312 | acc: 18.75%,  total acc: 67.83%   
cur_acc:  ['0.9415', '0.7262', '0.6915', '0.6587', '0.6280']
his_acc:  ['0.9415', '0.8365', '0.7566', '0.6953', '0.6783']
Clustering into  29  clusters
Clusters:  [ 0 19 15  0  0  0 24  0 27 28 20  0 17  0 14  9 18  0  0  0 23 21  2  0
  0 16  0 22  0 25  0 11  0 13 26  0  0  0  6  8 10  0  0  2  2  7  0  0
  0  0  0  0  0  4  0  3  0 12  5  1]
Losses:  6.114057540893555 0.8944039940834045 0.9650376439094543
CurrentTrain: epoch  0, batch     0 | loss: 7.9734993Losses:  6.217146873474121 1.2331737279891968 0.9456494450569153
CurrentTrain: epoch  0, batch     1 | loss: 8.3959703Losses:  6.835458755493164 1.2299607992172241 0.9638835787773132
CurrentTrain: epoch  0, batch     2 | loss: 9.0293026Losses:  6.574808120727539 0.6906847953796387 0.9832009077072144
CurrentTrain: epoch  0, batch     3 | loss: 8.2486935Losses:  5.718181610107422 1.11775541305542 0.9565250873565674
CurrentTrain: epoch  1, batch     0 | loss: 7.7924623Losses:  5.532970428466797 1.0093932151794434 0.9724903106689453
CurrentTrain: epoch  1, batch     1 | loss: 7.5148540Losses:  5.124490737915039 1.125316858291626 0.9565491080284119
CurrentTrain: epoch  1, batch     2 | loss: 7.2063565Losses:  4.573331356048584 0.3648855686187744 0.8971359133720398
CurrentTrain: epoch  1, batch     3 | loss: 5.8353529Losses:  4.443810939788818 0.7990412712097168 0.9549587368965149
CurrentTrain: epoch  2, batch     0 | loss: 6.1978111Losses:  5.326413154602051 0.9814055562019348 0.955755889415741
CurrentTrain: epoch  2, batch     1 | loss: 7.2635746Losses:  3.634744644165039 0.8741723299026489 0.9716484546661377
CurrentTrain: epoch  2, batch     2 | loss: 5.4805651Losses:  4.4124250411987305 0.22909730672836304 0.9141219854354858
CurrentTrain: epoch  2, batch     3 | loss: 5.5556445Losses:  4.180810928344727 1.0751159191131592 0.9533185958862305
CurrentTrain: epoch  3, batch     0 | loss: 6.2092457Losses:  4.180203914642334 0.8299990892410278 0.9699984192848206
CurrentTrain: epoch  3, batch     1 | loss: 5.9802012Losses:  3.5506091117858887 0.8360559940338135 0.9383131861686707
CurrentTrain: epoch  3, batch     2 | loss: 5.3249784Losses:  5.1272172927856445 0.08328810334205627 0.9772915840148926
CurrentTrain: epoch  3, batch     3 | loss: 6.1877971Losses:  3.8480021953582764 1.160423994064331 0.9710351824760437
CurrentTrain: epoch  4, batch     0 | loss: 5.9794612Losses:  3.4840188026428223 0.8404240608215332 0.9441773891448975
CurrentTrain: epoch  4, batch     1 | loss: 5.2686205Losses:  3.17695951461792 0.8307719230651855 0.9510554671287537
CurrentTrain: epoch  4, batch     2 | loss: 4.9587870Losses:  3.104898452758789 8.94069742685133e-08 0.9152703881263733
CurrentTrain: epoch  4, batch     3 | loss: 4.0201688Losses:  2.8237667083740234 0.7481144666671753 0.9436039328575134
CurrentTrain: epoch  5, batch     0 | loss: 4.5154853Losses:  3.586426019668579 0.9258424639701843 0.9395091533660889
CurrentTrain: epoch  5, batch     1 | loss: 5.4517775Losses:  2.815180540084839 0.6729030609130859 0.9619048833847046
CurrentTrain: epoch  5, batch     2 | loss: 4.4499884Losses:  3.001041889190674 0.0 1.0
CurrentTrain: epoch  5, batch     3 | loss: 4.0010419Losses:  2.612273693084717 0.8056687712669373 0.9630780220031738
CurrentTrain: epoch  6, batch     0 | loss: 4.3810205Losses:  2.8657469749450684 0.6672861576080322 0.9555812478065491
CurrentTrain: epoch  6, batch     1 | loss: 4.4886146Losses:  3.2788338661193848 0.7612656354904175 0.9354119896888733
CurrentTrain: epoch  6, batch     2 | loss: 4.9755116Losses:  2.8900580406188965 0.297254741191864 0.9039459228515625
CurrentTrain: epoch  6, batch     3 | loss: 4.0912590Losses:  2.9916300773620605 0.7978814840316772 0.9613871574401855
CurrentTrain: epoch  7, batch     0 | loss: 4.7508988Losses:  2.9699325561523438 0.7006235718727112 0.9394060373306274
CurrentTrain: epoch  7, batch     1 | loss: 4.6099620Losses:  2.6307451725006104 0.555884838104248 0.9404585361480713
CurrentTrain: epoch  7, batch     2 | loss: 4.1270885Losses:  2.1879525184631348 0.21073807775974274 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.3986907Losses:  3.015380859375 0.6886191368103027 0.9385503530502319
CurrentTrain: epoch  8, batch     0 | loss: 4.6425505Losses:  2.3890113830566406 0.48785150051116943 0.9367969036102295
CurrentTrain: epoch  8, batch     1 | loss: 3.8136599Losses:  2.163022041320801 0.6564913988113403 0.9704717397689819
CurrentTrain: epoch  8, batch     2 | loss: 3.7899852Losses:  2.9647271633148193 0.029365111142396927 0.9019672870635986
CurrentTrain: epoch  8, batch     3 | loss: 3.8960595Losses:  2.3434603214263916 0.6720075607299805 0.9580979347229004
CurrentTrain: epoch  9, batch     0 | loss: 3.9735658Losses:  2.169402599334717 0.6383333206176758 0.9394277334213257
CurrentTrain: epoch  9, batch     1 | loss: 3.7471638Losses:  2.3605098724365234 0.5637410879135132 0.9282801747322083
CurrentTrain: epoch  9, batch     2 | loss: 3.8525312Losses:  2.1079883575439453 0.2255019098520279 0.8998420238494873
CurrentTrain: epoch  9, batch     3 | loss: 3.2333324
Losses:  0.6638270616531372 0.8689049482345581 0.9233497977256775
MemoryTrain:  epoch  0, batch     0 | loss: 2.4560819Losses:  1.084291696548462 0.9672797322273254 0.91157466173172
MemoryTrain:  epoch  0, batch     1 | loss: 2.9631460Losses:  0.6091751456260681 1.062619686126709 0.9069329500198364
MemoryTrain:  epoch  0, batch     2 | loss: 2.5787277Losses:  1.0147569179534912 0.644039511680603 0.8765561580657959
MemoryTrain:  epoch  0, batch     3 | loss: 2.5353527Losses:  1.5913666486740112 0.8850809931755066 0.9121391773223877
MemoryTrain:  epoch  1, batch     0 | loss: 3.3885868Losses:  0.550028383731842 0.9253960847854614 0.9182779788970947
MemoryTrain:  epoch  1, batch     1 | loss: 2.3937025Losses:  1.0452289581298828 0.9644408822059631 0.8969178199768066
MemoryTrain:  epoch  1, batch     2 | loss: 2.9065876Losses:  0.6913112998008728 0.71160888671875 0.8914092779159546
MemoryTrain:  epoch  1, batch     3 | loss: 2.2943296Losses:  0.9778953194618225 1.0095592737197876 0.8760614395141602
MemoryTrain:  epoch  2, batch     0 | loss: 2.8635161Losses:  0.4279167056083679 1.117607831954956 0.8920422792434692
MemoryTrain:  epoch  2, batch     1 | loss: 2.4375668Losses:  0.46188071370124817 0.845357358455658 0.9498310089111328
MemoryTrain:  epoch  2, batch     2 | loss: 2.2570691Losses:  0.36300426721572876 0.533759355545044 0.9068925380706787
MemoryTrain:  epoch  2, batch     3 | loss: 1.8036561Losses:  0.5297520160675049 0.9548364877700806 0.8657743334770203
MemoryTrain:  epoch  3, batch     0 | loss: 2.3503628Losses:  0.21023769676685333 0.8965652585029602 0.9212584495544434
MemoryTrain:  epoch  3, batch     1 | loss: 2.0280614Losses:  0.24278749525547028 0.9501746296882629 0.9381059408187866
MemoryTrain:  epoch  3, batch     2 | loss: 2.1310682Losses:  0.4123665690422058 0.5236004590988159 0.8908601999282837
MemoryTrain:  epoch  3, batch     3 | loss: 1.8268273Losses:  0.2469862848520279 1.0493383407592773 0.8692424893379211
MemoryTrain:  epoch  4, batch     0 | loss: 2.1655672Losses:  0.38117191195487976 0.9687210321426392 0.9005744457244873
MemoryTrain:  epoch  4, batch     1 | loss: 2.2504673Losses:  0.1933881640434265 0.9088641405105591 0.9503473043441772
MemoryTrain:  epoch  4, batch     2 | loss: 2.0525994Losses:  0.06516016274690628 0.5265141129493713 0.8927438259124756
MemoryTrain:  epoch  4, batch     3 | loss: 1.4844182Losses:  0.3560730218887329 1.0030913352966309 0.9273928999900818
MemoryTrain:  epoch  5, batch     0 | loss: 2.2865572Losses:  0.081759974360466 0.8813544511795044 0.9103080630302429
MemoryTrain:  epoch  5, batch     1 | loss: 1.8734225Losses:  0.18966753780841827 0.8699477314949036 0.9161851406097412
MemoryTrain:  epoch  5, batch     2 | loss: 1.9758004Losses:  0.0730409026145935 0.6586935520172119 0.8515520095825195
MemoryTrain:  epoch  5, batch     3 | loss: 1.5832865Losses:  0.10223200172185898 1.0368200540542603 0.9150924682617188
MemoryTrain:  epoch  6, batch     0 | loss: 2.0541444Losses:  0.12518873810768127 0.7713183164596558 0.8822177648544312
MemoryTrain:  epoch  6, batch     1 | loss: 1.7787248Losses:  0.07541367411613464 0.7396916151046753 0.9087580442428589
MemoryTrain:  epoch  6, batch     2 | loss: 1.7238634Losses:  0.1794564425945282 0.7573418617248535 0.9025750160217285
MemoryTrain:  epoch  6, batch     3 | loss: 1.8393734Losses:  0.16451945900917053 0.7588921189308167 0.9296040534973145
MemoryTrain:  epoch  7, batch     0 | loss: 1.8530157Losses:  0.09543028473854065 1.0617554187774658 0.8574783205986023
MemoryTrain:  epoch  7, batch     1 | loss: 2.0146639Losses:  0.07509571313858032 0.8970818519592285 0.9020919799804688
MemoryTrain:  epoch  7, batch     2 | loss: 1.8742695Losses:  0.05003129690885544 0.6002198457717896 0.9219309091567993
MemoryTrain:  epoch  7, batch     3 | loss: 1.5721821Losses:  0.11931518465280533 0.9590960741043091 0.8999456763267517
MemoryTrain:  epoch  8, batch     0 | loss: 1.9783568Losses:  0.05047577992081642 0.6272841095924377 0.9184896945953369
MemoryTrain:  epoch  8, batch     1 | loss: 1.5962496Losses:  0.18460166454315186 0.8291446566581726 0.9049404859542847
MemoryTrain:  epoch  8, batch     2 | loss: 1.9186867Losses:  0.03354161977767944 0.8397727012634277 0.8617172241210938
MemoryTrain:  epoch  8, batch     3 | loss: 1.7350316Losses:  0.04015407711267471 0.8573505878448486 0.9171701073646545
MemoryTrain:  epoch  9, batch     0 | loss: 1.8146749Losses:  0.06663987040519714 0.728333592414856 0.9031956195831299
MemoryTrain:  epoch  9, batch     1 | loss: 1.6981691Losses:  0.08609740436077118 0.7392125725746155 0.863129198551178
MemoryTrain:  epoch  9, batch     2 | loss: 1.6884391Losses:  0.19487135112285614 0.7772268056869507 0.9123040437698364
MemoryTrain:  epoch  9, batch     3 | loss: 1.8844023
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 62.66%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 62.20%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 62.22%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 61.67%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 61.01%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 60.03%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 59.44%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 59.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.18%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.98%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.05%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 89.84%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 89.36%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 88.69%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.20%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 86.90%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 86.43%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 86.15%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 85.70%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 85.26%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 84.74%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.20%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 83.89%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 83.33%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.22%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 83.02%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 82.47%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 82.21%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 81.96%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 81.09%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 80.34%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 79.37%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 78.50%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 77.57%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 76.74%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 78.37%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 78.28%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 78.25%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 78.42%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 78.10%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 77.55%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.01%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.42%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 76.18%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.78%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.61%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 75.71%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 75.97%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.22%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.16%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 75.57%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 75.05%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 74.49%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 73.93%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 73.39%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 72.95%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 72.52%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 72.15%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 71.78%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 71.41%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 71.11%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 70.85%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 71.27%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.46%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 71.27%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 70.61%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 70.42%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 70.01%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 70.94%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 70.81%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 70.59%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 70.36%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 70.07%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 70.50%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 70.33%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 70.16%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 70.04%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 69.69%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 69.58%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 69.25%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 68.82%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 68.44%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 68.51%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 68.95%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 68.62%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 68.39%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.13%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 67.87%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 67.62%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 67.36%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  200 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 67.21%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 67.00%   [EVAL] batch:  204 | acc: 12.50%,  total acc: 66.74%   [EVAL] batch:  205 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  225 | acc: 12.50%,  total acc: 68.34%   [EVAL] batch:  226 | acc: 0.00%,  total acc: 68.03%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 67.90%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 67.74%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  230 | acc: 6.25%,  total acc: 67.23%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 67.89%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 67.84%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 67.69%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 67.64%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 67.35%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 67.28%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 67.21%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 67.33%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 66.89%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 66.65%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 66.15%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 65.89%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 66.64%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 66.41%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 66.17%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 65.95%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 65.71%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 65.49%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 65.27%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 64.82%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 64.59%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 64.36%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 64.13%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 63.90%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 63.92%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 65.06%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.04%   [EVAL] batch:  298 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 65.10%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 65.27%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 65.20%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 65.13%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 65.14%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 65.09%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 65.10%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 64.99%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 64.92%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 64.85%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 64.79%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 65.33%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  335 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 65.23%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 65.21%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 65.16%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 65.08%   [EVAL] batch:  344 | acc: 43.75%,  total acc: 65.02%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 64.93%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 64.87%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  349 | acc: 37.50%,  total acc: 64.80%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 64.78%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 64.77%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 64.71%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 64.73%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 64.69%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 64.50%   [EVAL] batch:  359 | acc: 31.25%,  total acc: 64.41%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 64.34%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 64.30%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 64.31%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 65.17%   
cur_acc:  ['0.9415', '0.7262', '0.6915', '0.6587', '0.6280', '0.6518']
his_acc:  ['0.9415', '0.8365', '0.7566', '0.6953', '0.6783', '0.6517']
Clustering into  34  clusters
Clusters:  [ 0  1 17  0  0  0 29  0 31 32 26  0 20  0 33 23 19  0  0  0 21 24  2  0
  0 18  0 25  0 30  0 12  0 16 14  0  0  0 11 22 27  0  0  2  2 15  0  0
  0  0  0  0  0 10  0  9  0  5 28 13  1  8  0  0  0  4  0  7  6  3]
Losses:  5.74208927154541 1.4660108089447021 0.9675360918045044
CurrentTrain: epoch  0, batch     0 | loss: 8.1756363Losses:  5.221297264099121 1.3630415201187134 0.9727603197097778
CurrentTrain: epoch  0, batch     1 | loss: 7.5570989Losses:  6.665645122528076 1.4807322025299072 0.9770904779434204
CurrentTrain: epoch  0, batch     2 | loss: 9.1234684Losses:  5.191424369812012 0.16867214441299438 0.9797952175140381
CurrentTrain: epoch  0, batch     3 | loss: 6.3398914Losses:  4.236819744110107 1.154661774635315 0.951683521270752
CurrentTrain: epoch  1, batch     0 | loss: 6.3431649Losses:  4.476498126983643 1.3742979764938354 0.9811263680458069
CurrentTrain: epoch  1, batch     1 | loss: 6.8319225Losses:  4.816667556762695 1.0589011907577515 0.9745876789093018
CurrentTrain: epoch  1, batch     2 | loss: 6.8501568Losses:  7.279753684997559 0.4220927655696869 0.9715052843093872
CurrentTrain: epoch  1, batch     3 | loss: 8.6733522Losses:  4.016942977905273 1.2820236682891846 0.9621502161026001
CurrentTrain: epoch  2, batch     0 | loss: 6.2611165Losses:  4.061179161071777 0.993972659111023 0.970432460308075
CurrentTrain: epoch  2, batch     1 | loss: 6.0255842Losses:  4.266767978668213 1.1623848676681519 0.9713207483291626
CurrentTrain: epoch  2, batch     2 | loss: 6.4004736Losses:  2.580734968185425 0.2531961500644684 0.9294223189353943
CurrentTrain: epoch  2, batch     3 | loss: 3.7633536Losses:  4.377450466156006 1.0989391803741455 0.9726413488388062
CurrentTrain: epoch  3, batch     0 | loss: 6.4490314Losses:  4.02236270904541 1.1372689008712769 0.9637477397918701
CurrentTrain: epoch  3, batch     1 | loss: 6.1233797Losses:  2.671968460083008 0.9997002482414246 0.9507398009300232
CurrentTrain: epoch  3, batch     2 | loss: 4.6224084Losses:  2.4726414680480957 0.10562421381473541 1.02787446975708
CurrentTrain: epoch  3, batch     3 | loss: 3.6061401Losses:  2.9028849601745605 0.9618008136749268 0.9574989080429077
CurrentTrain: epoch  4, batch     0 | loss: 4.8221846Losses:  3.7178237438201904 0.9794464111328125 0.9792054891586304
CurrentTrain: epoch  4, batch     1 | loss: 5.6764760Losses:  3.339080333709717 1.0671800374984741 0.962939977645874
CurrentTrain: epoch  4, batch     2 | loss: 5.3692007Losses:  3.820950746536255 0.3102334141731262 0.9212567806243896
CurrentTrain: epoch  4, batch     3 | loss: 5.0524406Losses:  3.1525166034698486 0.8664226531982422 0.9824272990226746
CurrentTrain: epoch  5, batch     0 | loss: 5.0013661Losses:  3.474761724472046 1.0847899913787842 0.9500154256820679
CurrentTrain: epoch  5, batch     1 | loss: 5.5095673Losses:  2.6391947269439697 0.775793194770813 0.9424874782562256
CurrentTrain: epoch  5, batch     2 | loss: 4.3574753Losses:  2.3984479904174805 0.12857265770435333 1.0060213804244995
CurrentTrain: epoch  5, batch     3 | loss: 3.5330420Losses:  3.3419361114501953 0.9643993377685547 0.9585926532745361
CurrentTrain: epoch  6, batch     0 | loss: 5.2649279Losses:  2.8307533264160156 0.8639852404594421 0.9490234851837158
CurrentTrain: epoch  6, batch     1 | loss: 4.6437621Losses:  2.4703497886657715 0.8006407022476196 0.9559817314147949
CurrentTrain: epoch  6, batch     2 | loss: 4.2269721Losses:  2.8546152114868164 0.09897803515195847 1.0
CurrentTrain: epoch  6, batch     3 | loss: 3.9535933Losses:  3.00669002532959 0.912541389465332 0.9630593061447144
CurrentTrain: epoch  7, batch     0 | loss: 4.8822908Losses:  2.8518905639648438 0.9340990781784058 0.9540407657623291
CurrentTrain: epoch  7, batch     1 | loss: 4.7400303Losses:  2.5395240783691406 0.893683671951294 0.9421498775482178
CurrentTrain: epoch  7, batch     2 | loss: 4.3753576Losses:  2.2889962196350098 0.08912736922502518 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.3781235Losses:  2.249117851257324 0.780276358127594 0.9402986764907837
CurrentTrain: epoch  8, batch     0 | loss: 3.9696927Losses:  2.639768123626709 0.8777835965156555 0.9305111169815063
CurrentTrain: epoch  8, batch     1 | loss: 4.4480629Losses:  2.947507381439209 0.818315863609314 0.9841418266296387
CurrentTrain: epoch  8, batch     2 | loss: 4.7499652Losses:  1.7827285528182983 0.27326059341430664 1.0
CurrentTrain: epoch  8, batch     3 | loss: 3.0559893Losses:  2.3204569816589355 0.6911743879318237 0.9816408753395081
CurrentTrain: epoch  9, batch     0 | loss: 3.9932723Losses:  2.3533620834350586 0.566863477230072 0.9468944668769836
CurrentTrain: epoch  9, batch     1 | loss: 3.8671200Losses:  2.7436084747314453 0.8809062242507935 0.9259634017944336
CurrentTrain: epoch  9, batch     2 | loss: 4.5504780Losses:  2.2795615196228027 8.94069742685133e-08 0.9012869000434875
CurrentTrain: epoch  9, batch     3 | loss: 3.1808484
Losses:  0.8900680541992188 0.7336551547050476 0.9113824367523193
MemoryTrain:  epoch  0, batch     0 | loss: 2.5351057Losses:  0.35564228892326355 0.9549407958984375 0.9049378633499146
MemoryTrain:  epoch  0, batch     1 | loss: 2.2155209Losses:  0.7418993711471558 0.8875452876091003 0.9140883684158325
MemoryTrain:  epoch  0, batch     2 | loss: 2.5435328Losses:  0.7707295417785645 0.9610897898674011 0.9073577523231506
MemoryTrain:  epoch  0, batch     3 | loss: 2.6391771Losses:  0.1137731522321701 0.4925233721733093 0.8892521262168884
MemoryTrain:  epoch  0, batch     4 | loss: 1.4955487Losses:  1.0900959968566895 1.2168020009994507 0.8755302429199219
MemoryTrain:  epoch  1, batch     0 | loss: 3.1824284Losses:  0.5357189178466797 0.9053881168365479 0.9186481833457947
MemoryTrain:  epoch  1, batch     1 | loss: 2.3597553Losses:  0.6583583354949951 0.615237832069397 0.908405065536499
MemoryTrain:  epoch  1, batch     2 | loss: 2.1820011Losses:  1.0644062757492065 0.7751446962356567 0.9078712463378906
MemoryTrain:  epoch  1, batch     3 | loss: 2.7474222Losses:  0.49147412180900574 0.401804655790329 0.9163421392440796
MemoryTrain:  epoch  1, batch     4 | loss: 1.8096209Losses:  0.6768327951431274 0.7685582041740417 0.8791949152946472
MemoryTrain:  epoch  2, batch     0 | loss: 2.3245859Losses:  0.4333866238594055 0.7797706723213196 0.912805438041687
MemoryTrain:  epoch  2, batch     1 | loss: 2.1259627Losses:  0.15806397795677185 0.8771398663520813 0.8980401158332825
MemoryTrain:  epoch  2, batch     2 | loss: 1.9332440Losses:  0.386244535446167 0.9914822578430176 0.8918831944465637
MemoryTrain:  epoch  2, batch     3 | loss: 2.2696099Losses:  0.1321285367012024 0.27867379784584045 0.9830991625785828
MemoryTrain:  epoch  2, batch     4 | loss: 1.3939015Losses:  0.4322354793548584 0.9016873836517334 0.9163321256637573
MemoryTrain:  epoch  3, batch     0 | loss: 2.2502551Losses:  0.14820048213005066 0.7543587684631348 0.9101942181587219
MemoryTrain:  epoch  3, batch     1 | loss: 1.8127534Losses:  0.1605157107114792 0.8918696045875549 0.899208664894104
MemoryTrain:  epoch  3, batch     2 | loss: 1.9515940Losses:  0.2680284082889557 0.8169674873352051 0.8964869976043701
MemoryTrain:  epoch  3, batch     3 | loss: 1.9814829Losses:  0.40078720450401306 0.3004177212715149 0.8613072633743286
MemoryTrain:  epoch  3, batch     4 | loss: 1.5625122Losses:  0.1662592887878418 0.9345346689224243 0.8921024203300476
MemoryTrain:  epoch  4, batch     0 | loss: 1.9928963Losses:  0.10747883468866348 1.0861799716949463 0.8546615839004517
MemoryTrain:  epoch  4, batch     1 | loss: 2.0483203Losses:  0.3324021100997925 0.6812653541564941 0.9214092493057251
MemoryTrain:  epoch  4, batch     2 | loss: 1.9350767Losses:  0.2564292252063751 0.8493868112564087 0.9266691207885742
MemoryTrain:  epoch  4, batch     3 | loss: 2.0324850Losses:  0.061125557869672775 0.1494007110595703 0.9255775213241577
MemoryTrain:  epoch  4, batch     4 | loss: 1.1361037Losses:  0.07291059195995331 0.6940792202949524 0.9232692718505859
MemoryTrain:  epoch  5, batch     0 | loss: 1.6902591Losses:  0.08681610971689224 0.9774125218391418 0.891006588935852
MemoryTrain:  epoch  5, batch     1 | loss: 1.9552352Losses:  0.15365949273109436 0.7687504291534424 0.8995314836502075
MemoryTrain:  epoch  5, batch     2 | loss: 1.8219414Losses:  0.24562247097492218 0.7841494083404541 0.8881693482398987
MemoryTrain:  epoch  5, batch     3 | loss: 1.9179413Losses:  0.23829948902130127 0.4270060658454895 0.8781419992446899
MemoryTrain:  epoch  5, batch     4 | loss: 1.5434475Losses:  0.09237654507160187 0.8218275308609009 0.8812216520309448
MemoryTrain:  epoch  6, batch     0 | loss: 1.7954257Losses:  0.18952366709709167 0.820210337638855 0.9426267147064209
MemoryTrain:  epoch  6, batch     1 | loss: 1.9523607Losses:  0.10657122731208801 0.691473126411438 0.9021255373954773
MemoryTrain:  epoch  6, batch     2 | loss: 1.7001698Losses:  0.14945334196090698 0.8151073455810547 0.8629387617111206
MemoryTrain:  epoch  6, batch     3 | loss: 1.8274994Losses:  0.05950696021318436 0.4711260199546814 0.8942767977714539
MemoryTrain:  epoch  6, batch     4 | loss: 1.4249098Losses:  0.1975541114807129 1.0519237518310547 0.8803854584693909
MemoryTrain:  epoch  7, batch     0 | loss: 2.1298633Losses:  0.0866083949804306 0.7871191501617432 0.8672925233840942
MemoryTrain:  epoch  7, batch     1 | loss: 1.7410201Losses:  0.046586744487285614 0.5348172783851624 0.9016426801681519
MemoryTrain:  epoch  7, batch     2 | loss: 1.4830468Losses:  0.07989877462387085 0.8468278050422668 0.9086868166923523
MemoryTrain:  epoch  7, batch     3 | loss: 1.8354135Losses:  0.12854605913162231 0.3895770311355591 0.9712492227554321
MemoryTrain:  epoch  7, batch     4 | loss: 1.4893723Losses:  0.07956211268901825 0.6916414499282837 0.9068601131439209
MemoryTrain:  epoch  8, batch     0 | loss: 1.6780636Losses:  0.09263911843299866 0.7355465292930603 0.9179980158805847
MemoryTrain:  epoch  8, batch     1 | loss: 1.7461836Losses:  0.06419265270233154 0.7160477042198181 0.8954451084136963
MemoryTrain:  epoch  8, batch     2 | loss: 1.6756854Losses:  0.07645794004201889 0.7063084840774536 0.8566651344299316
MemoryTrain:  epoch  8, batch     3 | loss: 1.6394315Losses:  0.16454294323921204 0.5189430713653564 0.8896176218986511
MemoryTrain:  epoch  8, batch     4 | loss: 1.5731037Losses:  0.0701519101858139 0.7047302722930908 0.9448225498199463
MemoryTrain:  epoch  9, batch     0 | loss: 1.7197047Losses:  0.1484554558992386 0.8072652816772461 0.881767749786377
MemoryTrain:  epoch  9, batch     1 | loss: 1.8374884Losses:  0.06142903491854668 0.6594892740249634 0.8340915441513062
MemoryTrain:  epoch  9, batch     2 | loss: 1.5550098Losses:  0.06359092146158218 0.8365083336830139 0.8914908170700073
MemoryTrain:  epoch  9, batch     3 | loss: 1.7915901Losses:  0.06153356283903122 0.4538300931453705 0.9595748782157898
MemoryTrain:  epoch  9, batch     4 | loss: 1.4749385
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 60.81%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 60.69%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 61.22%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 62.76%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 87.96%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 87.61%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 87.28%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 86.73%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 85.78%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 85.38%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 84.90%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 84.43%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.73%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 83.11%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 82.60%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 81.82%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 81.16%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 80.61%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 80.07%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 79.49%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 78.99%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 78.38%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 78.33%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 78.04%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 77.93%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 77.73%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 76.98%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 76.05%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 75.22%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 74.34%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 73.47%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 72.70%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 75.48%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.71%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 75.41%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 74.88%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 74.43%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 73.81%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 73.54%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.16%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 73.55%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.78%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 73.23%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 72.73%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 72.18%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 71.65%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 71.17%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 70.75%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 70.34%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 69.93%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 69.53%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 69.19%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 68.89%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 68.56%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.12%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 69.24%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 68.57%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.01%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 68.87%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 68.59%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 68.27%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.16%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 68.60%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 68.71%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 68.28%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 68.17%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 67.83%   [EVAL] batch:  176 | acc: 12.50%,  total acc: 67.51%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 67.31%   [EVAL] batch:  178 | acc: 12.50%,  total acc: 67.00%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 66.84%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 66.57%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.89%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 66.87%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 66.64%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 66.18%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 66.00%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 65.93%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 65.84%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 65.64%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 65.41%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  225 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  226 | acc: 6.25%,  total acc: 67.18%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 66.92%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 66.68%   [EVAL] batch:  230 | acc: 6.25%,  total acc: 66.42%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 67.12%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 66.90%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 66.70%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 66.54%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 66.47%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 66.33%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 65.75%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 65.64%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 65.79%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 65.76%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 65.28%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 65.04%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 64.83%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 64.61%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 64.17%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 63.94%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 63.71%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 63.48%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 63.26%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 63.38%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 63.47%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 64.43%   [EVAL] batch:  294 | acc: 12.50%,  total acc: 64.26%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 64.04%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:  297 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:  298 | acc: 0.00%,  total acc: 63.48%   [EVAL] batch:  299 | acc: 0.00%,  total acc: 63.27%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 63.29%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 63.51%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 63.58%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 63.53%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 63.47%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 63.47%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 63.42%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 63.36%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 63.30%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 63.09%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 63.03%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 62.99%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 62.95%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 63.47%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 63.48%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 63.40%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:  336 | acc: 56.25%,  total acc: 63.37%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 63.37%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 63.40%   [EVAL] batch:  340 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  342 | acc: 56.25%,  total acc: 63.37%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 63.30%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 63.24%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 63.26%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 63.20%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 63.22%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 63.18%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 63.10%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 63.10%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 63.13%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 62.99%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 62.88%   [EVAL] batch:  359 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 62.71%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 62.67%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 62.69%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 63.38%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 63.47%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 63.51%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 63.57%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 63.58%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 63.58%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 63.63%   [EVAL] batch:  377 | acc: 75.00%,  total acc: 63.66%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 63.67%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 63.73%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 63.73%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.77%   [EVAL] batch:  384 | acc: 50.00%,  total acc: 63.73%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 63.77%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:  390 | acc: 31.25%,  total acc: 63.67%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 63.66%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 63.67%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 64.15%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 64.09%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 64.12%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 64.04%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 63.99%   [EVAL] batch:  406 | acc: 43.75%,  total acc: 63.94%   [EVAL] batch:  407 | acc: 37.50%,  total acc: 63.88%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 63.74%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 63.61%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 63.49%   [EVAL] batch:  411 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 63.32%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 63.36%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 63.42%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 63.53%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 63.58%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 63.63%   [EVAL] batch:  420 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 63.52%   [EVAL] batch:  422 | acc: 50.00%,  total acc: 63.49%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 63.47%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 64.36%   
cur_acc:  ['0.9415', '0.7262', '0.6915', '0.6587', '0.6280', '0.6518', '0.6895']
his_acc:  ['0.9415', '0.8365', '0.7566', '0.6953', '0.6783', '0.6517', '0.6436']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0 32 27  1  0
  0 37  0 29  0 34  2 31  0  0 36  0  0  0 20 23 17  0  0  1  1  9  0  0
  0  0  0  0  0 12  0 16  0 11 15 30  5 14  0  0  0  2  0 18 28 10  8  4
  0  0 13  7  6  0  0  3]
Losses:  4.248300552368164 1.3863879442214966 1.0032694339752197
CurrentTrain: epoch  0, batch     0 | loss: 6.6379576Losses:  4.924890518188477 1.4877097606658936 0.9806889891624451
CurrentTrain: epoch  0, batch     1 | loss: 7.3932896Losses:  5.430840492248535 1.5725046396255493 0.9738889932632446
CurrentTrain: epoch  0, batch     2 | loss: 7.9772339Losses:  6.104358673095703 0.534982442855835 0.9555798172950745
CurrentTrain: epoch  0, batch     3 | loss: 7.5949211Losses:  4.701200485229492 1.4672213792800903 0.9816941618919373
CurrentTrain: epoch  1, batch     0 | loss: 7.1501160Losses:  3.6528310775756836 1.4810473918914795 0.9852771759033203
CurrentTrain: epoch  1, batch     1 | loss: 6.1191559Losses:  3.1240124702453613 1.275216817855835 0.9792866706848145
CurrentTrain: epoch  1, batch     2 | loss: 5.3785157Losses:  4.249567985534668 0.20131166279315948 1.0
CurrentTrain: epoch  1, batch     3 | loss: 5.4508796Losses:  3.2956371307373047 1.2163219451904297 0.990017831325531
CurrentTrain: epoch  2, batch     0 | loss: 5.5019770Losses:  3.0113577842712402 1.202538013458252 0.9801995754241943
CurrentTrain: epoch  2, batch     1 | loss: 5.1940956Losses:  3.569237470626831 1.3140190839767456 0.9607031345367432
CurrentTrain: epoch  2, batch     2 | loss: 5.8439598Losses:  1.8724236488342285 0.0606340728700161 1.0
CurrentTrain: epoch  2, batch     3 | loss: 2.9330578Losses:  2.9028818607330322 1.0303248167037964 0.9964406490325928
CurrentTrain: epoch  3, batch     0 | loss: 4.9296474Losses:  3.3112995624542236 1.2115521430969238 0.9572056531906128
CurrentTrain: epoch  3, batch     1 | loss: 5.4800577Losses:  2.554807186126709 1.1593575477600098 0.9637463092803955
CurrentTrain: epoch  3, batch     2 | loss: 4.6779108Losses:  2.1551568508148193 3.278256031080673e-07 0.9634517431259155
CurrentTrain: epoch  3, batch     3 | loss: 3.1186090Losses:  2.362917423248291 0.8623636364936829 0.9502946138381958
CurrentTrain: epoch  4, batch     0 | loss: 4.1755757Losses:  2.9697329998016357 0.9671963453292847 0.9759441614151001
CurrentTrain: epoch  4, batch     1 | loss: 4.9128733Losses:  2.526815414428711 0.9751144647598267 0.992244303226471
CurrentTrain: epoch  4, batch     2 | loss: 4.4941740Losses:  3.2366104125976562 0.11159509420394897 0.91480553150177
CurrentTrain: epoch  4, batch     3 | loss: 4.2630110Losses:  2.830705165863037 1.0237032175064087 0.9529276490211487
CurrentTrain: epoch  5, batch     0 | loss: 4.8073359Losses:  2.7281103134155273 1.015250563621521 0.9774475693702698
CurrentTrain: epoch  5, batch     1 | loss: 4.7208085Losses:  2.325242042541504 1.041783094406128 0.9726871252059937
CurrentTrain: epoch  5, batch     2 | loss: 4.3397121Losses:  1.8940973281860352 0.054382845759391785 0.9354522824287415
CurrentTrain: epoch  5, batch     3 | loss: 2.8839324Losses:  2.609570026397705 1.0418777465820312 0.9834383726119995
CurrentTrain: epoch  6, batch     0 | loss: 4.6348863Losses:  2.5157470703125 0.8430815935134888 0.9394822120666504
CurrentTrain: epoch  6, batch     1 | loss: 4.2983108Losses:  2.242769718170166 0.893610417842865 0.971450686454773
CurrentTrain: epoch  6, batch     2 | loss: 4.1078310Losses:  2.3150978088378906 0.20635706186294556 0.9384156465530396
CurrentTrain: epoch  6, batch     3 | loss: 3.4598703Losses:  2.1111748218536377 0.8209872841835022 0.9630175828933716
CurrentTrain: epoch  7, batch     0 | loss: 3.8951797Losses:  2.1721460819244385 1.0269443988800049 0.966952919960022
CurrentTrain: epoch  7, batch     1 | loss: 4.1660433Losses:  2.6331984996795654 0.8643691539764404 0.944679856300354
CurrentTrain: epoch  7, batch     2 | loss: 4.4422474Losses:  2.8101775646209717 0.06848789006471634 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.8786654Losses:  2.3475794792175293 0.8453131914138794 0.9661380052566528
CurrentTrain: epoch  8, batch     0 | loss: 4.1590304Losses:  2.0445706844329834 0.6298798322677612 0.9443490505218506
CurrentTrain: epoch  8, batch     1 | loss: 3.6187994Losses:  2.2038450241088867 0.8385493755340576 0.9698019027709961
CurrentTrain: epoch  8, batch     2 | loss: 4.0121965Losses:  2.8166420459747314 0.12604331970214844 0.8913054466247559
CurrentTrain: epoch  8, batch     3 | loss: 3.8339908Losses:  2.425922393798828 0.8542510271072388 0.9580328464508057
CurrentTrain: epoch  9, batch     0 | loss: 4.2382059Losses:  1.8790497779846191 0.567537248134613 0.9537771940231323
CurrentTrain: epoch  9, batch     1 | loss: 3.4003644Losses:  2.314608097076416 0.6981637477874756 0.9589303731918335
CurrentTrain: epoch  9, batch     2 | loss: 3.9717021Losses:  2.1933369636535645 0.06785929203033447 0.8985605239868164
CurrentTrain: epoch  9, batch     3 | loss: 3.1597567
Losses:  0.17264163494110107 0.8749281167984009 0.8798620700836182
MemoryTrain:  epoch  0, batch     0 | loss: 1.9274318Losses:  0.16872766613960266 0.8124380111694336 0.9095961451530457
MemoryTrain:  epoch  0, batch     1 | loss: 1.8907619Losses:  0.3108617663383484 0.7981399893760681 0.8925213813781738
MemoryTrain:  epoch  0, batch     2 | loss: 2.0015230Losses:  0.27122020721435547 0.9545902013778687 0.9398291110992432
MemoryTrain:  epoch  0, batch     3 | loss: 2.1656394Losses:  0.7751166820526123 0.8896729946136475 0.8966201543807983
MemoryTrain:  epoch  0, batch     4 | loss: 2.5614100Losses:  0.6893647313117981 0.7065243721008301 0.8870891332626343
MemoryTrain:  epoch  1, batch     0 | loss: 2.2829781Losses:  0.5514063835144043 0.9046915769577026 0.9325544238090515
MemoryTrain:  epoch  1, batch     1 | loss: 2.3886523Losses:  0.5287432670593262 0.9474676251411438 0.8896791934967041
MemoryTrain:  epoch  1, batch     2 | loss: 2.3658900Losses:  0.3264947831630707 1.0180014371871948 0.8656015396118164
MemoryTrain:  epoch  1, batch     3 | loss: 2.2100978Losses:  0.320686936378479 0.6012891530990601 0.9216916561126709
MemoryTrain:  epoch  1, batch     4 | loss: 1.8436677Losses:  0.2038414478302002 0.9127064943313599 0.8721882104873657
MemoryTrain:  epoch  2, batch     0 | loss: 1.9887362Losses:  0.112764373421669 0.704913318157196 0.8799751996994019
MemoryTrain:  epoch  2, batch     1 | loss: 1.6976528Losses:  0.24288409948349 0.6678566932678223 0.9231424331665039
MemoryTrain:  epoch  2, batch     2 | loss: 1.8338833Losses:  0.2424585223197937 0.8275452256202698 0.8945092558860779
MemoryTrain:  epoch  2, batch     3 | loss: 1.9645131Losses:  0.28967979550361633 0.8508437871932983 0.9246151447296143
MemoryTrain:  epoch  2, batch     4 | loss: 2.0651388Losses:  0.11471030861139297 0.7152838706970215 0.8984286785125732
MemoryTrain:  epoch  3, batch     0 | loss: 1.7284229Losses:  0.18732258677482605 0.8567350506782532 0.8751734495162964
MemoryTrain:  epoch  3, batch     1 | loss: 1.9192311Losses:  0.10282085835933685 0.8464246392250061 0.9187031984329224
MemoryTrain:  epoch  3, batch     2 | loss: 1.8679488Losses:  0.10991868376731873 0.6667505502700806 0.9109336137771606
MemoryTrain:  epoch  3, batch     3 | loss: 1.6876029Losses:  0.12270054221153259 0.8608756065368652 0.8873099684715271
MemoryTrain:  epoch  3, batch     4 | loss: 1.8708861Losses:  0.15936486423015594 0.6295557022094727 0.9196139574050903
MemoryTrain:  epoch  4, batch     0 | loss: 1.7085345Losses:  0.0819152221083641 0.7690051198005676 0.9304119348526001
MemoryTrain:  epoch  4, batch     1 | loss: 1.7813323Losses:  0.06840062886476517 0.6740041971206665 0.8701527714729309
MemoryTrain:  epoch  4, batch     2 | loss: 1.6125576Losses:  0.19260838627815247 0.9175638556480408 0.8851829767227173
MemoryTrain:  epoch  4, batch     3 | loss: 1.9953552Losses:  0.1740894615650177 0.7639870643615723 0.877623438835144
MemoryTrain:  epoch  4, batch     4 | loss: 1.8156999Losses:  0.06065996363759041 0.6239895820617676 0.8827582597732544
MemoryTrain:  epoch  5, batch     0 | loss: 1.5674078Losses:  0.06317653506994247 0.7179675102233887 0.8827643394470215
MemoryTrain:  epoch  5, batch     1 | loss: 1.6639084Losses:  0.08798928558826447 0.7517578601837158 0.9199075698852539
MemoryTrain:  epoch  5, batch     2 | loss: 1.7596548Losses:  0.177930548787117 0.9422385096549988 0.9082725048065186
MemoryTrain:  epoch  5, batch     3 | loss: 2.0284414Losses:  0.0752653107047081 0.6571854948997498 0.887836217880249
MemoryTrain:  epoch  5, batch     4 | loss: 1.6202869Losses:  0.09857973456382751 0.8383500576019287 0.881277322769165
MemoryTrain:  epoch  6, batch     0 | loss: 1.8182071Losses:  0.03959207981824875 0.5693662166595459 0.8778809905052185
MemoryTrain:  epoch  6, batch     1 | loss: 1.4868393Losses:  0.0919618308544159 0.7655069828033447 0.8881962895393372
MemoryTrain:  epoch  6, batch     2 | loss: 1.7456651Losses:  0.06797382235527039 0.8067808151245117 0.9104695320129395
MemoryTrain:  epoch  6, batch     3 | loss: 1.7852242Losses:  0.050550270825624466 0.6416431665420532 0.9142767190933228
MemoryTrain:  epoch  6, batch     4 | loss: 1.6064701Losses:  0.05555913597345352 0.7687473893165588 0.8844212293624878
MemoryTrain:  epoch  7, batch     0 | loss: 1.7087278Losses:  0.10492436587810516 0.925678014755249 0.9118322134017944
MemoryTrain:  epoch  7, batch     1 | loss: 1.9424345Losses:  0.04645539075136185 0.590296745300293 0.8872609734535217
MemoryTrain:  epoch  7, batch     2 | loss: 1.5240130Losses:  0.0535036064684391 0.6093846559524536 0.8906508088111877
MemoryTrain:  epoch  7, batch     3 | loss: 1.5535390Losses:  0.06496469676494598 0.7243965864181519 0.8881381154060364
MemoryTrain:  epoch  7, batch     4 | loss: 1.6774994Losses:  0.0824267566204071 0.5954676866531372 0.9193076491355896
MemoryTrain:  epoch  8, batch     0 | loss: 1.5972021Losses:  0.036148279905319214 0.5428223013877869 0.9051048755645752
MemoryTrain:  epoch  8, batch     1 | loss: 1.4840754Losses:  0.047959104180336 0.7897865772247314 0.8908926248550415
MemoryTrain:  epoch  8, batch     2 | loss: 1.7286383Losses:  0.09049246460199356 0.9714861512184143 0.8291070461273193
MemoryTrain:  epoch  8, batch     3 | loss: 1.8910856Losses:  0.059360988438129425 0.6405996084213257 0.9177061915397644
MemoryTrain:  epoch  8, batch     4 | loss: 1.6176667Losses:  0.05630733072757721 0.6174942851066589 0.886676549911499
MemoryTrain:  epoch  9, batch     0 | loss: 1.5604782Losses:  0.05070227384567261 0.5961200594902039 0.9107418060302734
MemoryTrain:  epoch  9, batch     1 | loss: 1.5575641Losses:  0.057912930846214294 0.7439479231834412 0.8512392044067383
MemoryTrain:  epoch  9, batch     2 | loss: 1.6531000Losses:  0.06091703847050667 0.656419038772583 0.9567759037017822
MemoryTrain:  epoch  9, batch     3 | loss: 1.6741120Losses:  0.0647580623626709 0.6706303358078003 0.8479599356651306
MemoryTrain:  epoch  9, batch     4 | loss: 1.5833483
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 80.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.74%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 85.24%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 85.28%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 85.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 85.76%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.32%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.30%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.07%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.22%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.15%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 85.97%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 85.30%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 84.66%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 84.04%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 83.44%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 82.65%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 82.31%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 82.08%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 81.76%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 81.15%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 80.66%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 80.38%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 79.92%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 79.38%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 78.86%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 78.53%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.48%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 78.26%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 77.86%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 77.83%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 77.62%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 77.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.76%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.56%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 77.61%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 77.58%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 77.62%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 77.06%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 76.13%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 75.30%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 74.41%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 73.55%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 72.77%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.51%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.79%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.61%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 75.55%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.77%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.47%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 74.94%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 74.60%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 74.09%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 73.82%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.34%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 73.95%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 73.39%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 72.88%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 72.34%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 71.80%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 71.27%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 70.80%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 70.44%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 70.13%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 69.78%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 69.23%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 68.94%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 69.88%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.56%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 69.29%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 68.93%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 69.16%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 68.39%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 68.15%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 68.11%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 68.56%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 68.19%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 68.04%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 67.82%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 67.64%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 67.37%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 67.24%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  176 | acc: 12.50%,  total acc: 66.63%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 66.40%   [EVAL] batch:  178 | acc: 18.75%,  total acc: 66.13%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 65.81%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 66.27%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 65.84%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 65.59%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 65.45%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 65.27%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 65.72%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 65.66%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 65.52%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 65.35%   [EVAL] batch:  204 | acc: 18.75%,  total acc: 65.12%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 64.90%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 65.87%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 66.74%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 66.78%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 66.71%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 66.61%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 66.28%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 66.10%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 66.54%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 66.44%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 66.14%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 66.05%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 65.81%   [EVAL] batch:  247 | acc: 43.75%,  total acc: 65.73%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 65.58%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 65.36%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 65.20%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 65.00%   [EVAL] batch:  253 | acc: 18.75%,  total acc: 64.81%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 64.56%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 64.36%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 64.32%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 64.91%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 64.93%   [EVAL] batch:  268 | acc: 31.25%,  total acc: 64.80%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 64.39%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 64.18%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 63.94%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 63.73%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 63.52%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 63.32%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 63.09%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 62.86%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 62.63%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 62.41%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 62.19%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 62.21%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 62.32%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 62.37%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 62.48%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 62.54%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 62.80%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:  294 | acc: 6.25%,  total acc: 63.11%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 62.90%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 62.73%   [EVAL] batch:  297 | acc: 6.25%,  total acc: 62.54%   [EVAL] batch:  298 | acc: 0.00%,  total acc: 62.33%   [EVAL] batch:  299 | acc: 0.00%,  total acc: 62.12%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 62.11%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 62.21%   [EVAL] batch:  302 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 62.31%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 62.34%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 62.34%   [EVAL] batch:  307 | acc: 12.50%,  total acc: 62.18%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 62.08%   [EVAL] batch:  309 | acc: 43.75%,  total acc: 62.02%   [EVAL] batch:  310 | acc: 31.25%,  total acc: 61.92%   [EVAL] batch:  311 | acc: 56.25%,  total acc: 61.90%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 61.78%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 61.74%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 61.67%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 61.57%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 61.51%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 61.50%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 61.48%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 61.60%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 61.78%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 61.98%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 62.06%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 62.06%   [EVAL] batch:  326 | acc: 43.75%,  total acc: 62.00%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 62.04%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 62.06%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 62.10%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 62.05%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 62.05%   [EVAL] batch:  334 | acc: 50.00%,  total acc: 62.01%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 61.95%   [EVAL] batch:  338 | acc: 50.00%,  total acc: 61.91%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 61.93%   [EVAL] batch:  340 | acc: 56.25%,  total acc: 61.91%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 61.90%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 61.86%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 61.77%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 61.76%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 61.74%   [EVAL] batch:  347 | acc: 43.75%,  total acc: 61.69%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 61.71%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 61.68%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 61.70%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 61.68%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 61.69%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 61.69%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 61.73%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 61.78%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 61.73%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 61.63%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 61.53%   [EVAL] batch:  359 | acc: 31.25%,  total acc: 61.44%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 61.36%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 61.33%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 61.35%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 61.45%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 61.66%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 61.87%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 62.06%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 62.25%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 62.35%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 62.38%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 62.43%   [EVAL] batch:  377 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 62.48%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 62.48%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 62.58%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 62.58%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 62.61%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 62.63%   [EVAL] batch:  384 | acc: 43.75%,  total acc: 62.58%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 62.53%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 62.55%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 62.60%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 62.55%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 62.53%   [EVAL] batch:  390 | acc: 25.00%,  total acc: 62.44%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 62.39%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 62.34%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 62.39%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 62.47%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 62.66%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 62.89%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 62.83%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 62.79%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 62.79%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 62.73%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 62.68%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 62.65%   [EVAL] batch:  407 | acc: 43.75%,  total acc: 62.61%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 62.47%   [EVAL] batch:  409 | acc: 12.50%,  total acc: 62.35%   [EVAL] batch:  410 | acc: 12.50%,  total acc: 62.23%   [EVAL] batch:  411 | acc: 0.00%,  total acc: 62.08%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 62.06%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 62.17%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 62.23%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 62.32%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 62.37%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 62.31%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 62.26%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 62.22%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 62.20%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 62.19%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 62.15%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 62.75%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 62.80%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 62.86%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 63.00%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  438 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  439 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  440 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 63.61%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 63.68%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 63.73%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 63.93%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 63.93%   [EVAL] batch:  453 | acc: 37.50%,  total acc: 63.88%   [EVAL] batch:  454 | acc: 62.50%,  total acc: 63.87%   [EVAL] batch:  455 | acc: 50.00%,  total acc: 63.84%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  457 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  459 | acc: 56.25%,  total acc: 63.94%   [EVAL] batch:  460 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 63.92%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:  469 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 64.49%   [EVAL] batch:  471 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  472 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  473 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  474 | acc: 43.75%,  total acc: 64.46%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  488 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 65.55%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  493 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 65.99%   
cur_acc:  ['0.9415', '0.7262', '0.6915', '0.6587', '0.6280', '0.6518', '0.6895', '0.8532']
his_acc:  ['0.9415', '0.8365', '0.7566', '0.6953', '0.6783', '0.6517', '0.6436', '0.6599']
----------END
his_acc mean:  [0.9436 0.8305 0.7698 0.7285 0.7178 0.6908 0.6696 0.6486]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.027637481689453 1.8446838855743408 0.9525788426399231
CurrentTrain: epoch  0, batch     0 | loss: 12.8248997Losses:  9.965803146362305 2.194183111190796 0.9756466746330261
CurrentTrain: epoch  0, batch     1 | loss: 13.1356335Losses:  10.063091278076172 1.7253367900848389 0.9309903383255005
CurrentTrain: epoch  0, batch     2 | loss: 12.7194185Losses:  10.009361267089844 1.7325854301452637 0.9528564810752869
CurrentTrain: epoch  0, batch     3 | loss: 12.6948032Losses:  9.247632026672363 1.6062676906585693 0.9246972799301147
CurrentTrain: epoch  0, batch     4 | loss: 11.7785969Losses:  10.409693717956543 1.561234951019287 0.9499995708465576
CurrentTrain: epoch  0, batch     5 | loss: 12.9209280Losses:  9.541285514831543 1.8051633834838867 0.9324369430541992
CurrentTrain: epoch  0, batch     6 | loss: 12.2788858Losses:  8.973109245300293 1.4958511590957642 0.8851103186607361
CurrentTrain: epoch  0, batch     7 | loss: 11.3540707Losses:  9.1409273147583 1.5235402584075928 0.9006975889205933
CurrentTrain: epoch  0, batch     8 | loss: 11.5651655Losses:  9.717533111572266 1.7448663711547852 0.9287407994270325
CurrentTrain: epoch  0, batch     9 | loss: 12.3911400Losses:  9.020872116088867 1.4534356594085693 0.9106189012527466
CurrentTrain: epoch  0, batch    10 | loss: 11.3849268Losses:  8.4821195602417 1.3133976459503174 0.9143908023834229
CurrentTrain: epoch  0, batch    11 | loss: 10.7099075Losses:  9.162933349609375 1.5534172058105469 0.9028114676475525
CurrentTrain: epoch  0, batch    12 | loss: 11.6191616Losses:  9.031879425048828 1.4751474857330322 0.8827075958251953
CurrentTrain: epoch  0, batch    13 | loss: 11.3897343Losses:  8.20500373840332 1.5347697734832764 0.876811146736145
CurrentTrain: epoch  0, batch    14 | loss: 10.6165848Losses:  8.84937858581543 1.7073673009872437 0.91942298412323
CurrentTrain: epoch  0, batch    15 | loss: 11.4761686Losses:  9.314208984375 1.3092129230499268 0.8750736713409424
CurrentTrain: epoch  0, batch    16 | loss: 11.4984951Losses:  9.103401184082031 1.7267431020736694 0.9026888608932495
CurrentTrain: epoch  0, batch    17 | loss: 11.7328329Losses:  8.967742919921875 1.4045565128326416 0.8555381298065186
CurrentTrain: epoch  0, batch    18 | loss: 11.2278376Losses:  8.722370147705078 1.682065486907959 0.9225032925605774
CurrentTrain: epoch  0, batch    19 | loss: 11.3269396Losses:  8.101913452148438 1.3146898746490479 0.856037437915802
CurrentTrain: epoch  0, batch    20 | loss: 10.2726402Losses:  8.2227783203125 1.4822702407836914 0.8865231871604919
CurrentTrain: epoch  0, batch    21 | loss: 10.5915718Losses:  8.756954193115234 1.4431037902832031 0.87433922290802
CurrentTrain: epoch  0, batch    22 | loss: 11.0743971Losses:  8.134103775024414 1.1773182153701782 0.8495680689811707
CurrentTrain: epoch  0, batch    23 | loss: 10.1609907Losses:  7.831748008728027 1.4227570295333862 0.8684377670288086
CurrentTrain: epoch  0, batch    24 | loss: 10.1229429Losses:  8.046960830688477 1.1922941207885742 0.8349202275276184
CurrentTrain: epoch  0, batch    25 | loss: 10.0741749Losses:  8.256089210510254 1.1963022947311401 0.8497837781906128
CurrentTrain: epoch  0, batch    26 | loss: 10.3021755Losses:  7.70709228515625 1.369907259941101 0.84943687915802
CurrentTrain: epoch  0, batch    27 | loss: 9.9264364Losses:  8.237741470336914 1.258052110671997 0.8454645872116089
CurrentTrain: epoch  0, batch    28 | loss: 10.3412580Losses:  8.088417053222656 1.273146152496338 0.8350956439971924
CurrentTrain: epoch  0, batch    29 | loss: 10.1966581Losses:  8.584667205810547 1.4652818441390991 0.8425407409667969
CurrentTrain: epoch  0, batch    30 | loss: 10.8924894Losses:  8.013579368591309 1.07220458984375 0.8493167161941528
CurrentTrain: epoch  0, batch    31 | loss: 9.9351006Losses:  8.0584716796875 1.017490029335022 0.8794862031936646
CurrentTrain: epoch  0, batch    32 | loss: 9.9554482Losses:  7.5241899490356445 0.9676612019538879 0.8580470681190491
CurrentTrain: epoch  0, batch    33 | loss: 9.3498983Losses:  7.905702590942383 1.085278034210205 0.8394633531570435
CurrentTrain: epoch  0, batch    34 | loss: 9.8304434Losses:  7.886101722717285 1.1114850044250488 0.8224287033081055
CurrentTrain: epoch  0, batch    35 | loss: 9.8200159Losses:  7.766352653503418 1.3450160026550293 0.8740348219871521
CurrentTrain: epoch  0, batch    36 | loss: 9.9854031Losses:  7.838717460632324 1.1260433197021484 0.8336496353149414
CurrentTrain: epoch  0, batch    37 | loss: 9.7984104Losses:  8.508161544799805 1.3459951877593994 0.864778459072113
CurrentTrain: epoch  0, batch    38 | loss: 10.7189350Losses:  8.162741661071777 1.2422397136688232 0.8130165338516235
CurrentTrain: epoch  0, batch    39 | loss: 10.2179985Losses:  8.516192436218262 1.2518256902694702 0.855075478553772
CurrentTrain: epoch  0, batch    40 | loss: 10.6230936Losses:  7.486090183258057 1.0976862907409668 0.8063055276870728
CurrentTrain: epoch  0, batch    41 | loss: 9.3900824Losses:  7.321561813354492 1.1287562847137451 0.8284544348716736
CurrentTrain: epoch  0, batch    42 | loss: 9.2787724Losses:  7.027963638305664 0.8512051105499268 0.8283288478851318
CurrentTrain: epoch  0, batch    43 | loss: 8.7074976Losses:  7.4491286277771 1.034698247909546 0.8073301315307617
CurrentTrain: epoch  0, batch    44 | loss: 9.2911568Losses:  7.376736640930176 1.1745617389678955 0.8414034843444824
CurrentTrain: epoch  0, batch    45 | loss: 9.3927021Losses:  8.165268898010254 0.9405325651168823 0.8130350708961487
CurrentTrain: epoch  0, batch    46 | loss: 9.9188366Losses:  6.70048189163208 1.1338707208633423 0.8328501582145691
CurrentTrain: epoch  0, batch    47 | loss: 8.6672029Losses:  7.671633720397949 1.338337779045105 0.8724108338356018
CurrentTrain: epoch  0, batch    48 | loss: 9.8823824Losses:  7.293811798095703 1.2156552076339722 0.8651678562164307
CurrentTrain: epoch  0, batch    49 | loss: 9.3746347Losses:  7.672031879425049 1.1682002544403076 0.8329956531524658
CurrentTrain: epoch  0, batch    50 | loss: 9.6732273Losses:  7.248935699462891 1.121227741241455 0.8235994577407837
CurrentTrain: epoch  0, batch    51 | loss: 9.1937628Losses:  7.064741134643555 0.9336361885070801 0.8208733797073364
CurrentTrain: epoch  0, batch    52 | loss: 8.8192511Losses:  7.205226898193359 1.1457860469818115 0.8478405475616455
CurrentTrain: epoch  0, batch    53 | loss: 9.1988535Losses:  7.804207801818848 1.3366121053695679 0.8591371774673462
CurrentTrain: epoch  0, batch    54 | loss: 9.9999571Losses:  6.906512260437012 0.9768181443214417 0.827486515045166
CurrentTrain: epoch  0, batch    55 | loss: 8.7108173Losses:  6.907964706420898 0.9177498817443848 0.7622621655464172
CurrentTrain: epoch  0, batch    56 | loss: 8.5879765Losses:  7.580048561096191 1.032289981842041 0.8130556344985962
CurrentTrain: epoch  0, batch    57 | loss: 9.4253950Losses:  7.541520595550537 0.8433716297149658 0.7854478359222412
CurrentTrain: epoch  0, batch    58 | loss: 9.1703405Losses:  7.117746353149414 0.7791969180107117 0.77778160572052
CurrentTrain: epoch  0, batch    59 | loss: 8.6747246Losses:  6.652664661407471 0.8772773742675781 0.7906274795532227
CurrentTrain: epoch  0, batch    60 | loss: 8.3205700Losses:  7.638004302978516 0.7208238244056702 0.7767521142959595
CurrentTrain: epoch  0, batch    61 | loss: 9.1355810Losses:  9.135408401489258 0.767449140548706 0.8481937646865845
CurrentTrain: epoch  0, batch    62 | loss: 10.7510519Losses:  7.49288272857666 0.8910950422286987 0.8144201636314392
CurrentTrain: epoch  1, batch     0 | loss: 9.1983976Losses:  7.039545059204102 0.7669596672058105 0.7730489373207092
CurrentTrain: epoch  1, batch     1 | loss: 8.5795536Losses:  6.809209823608398 0.9915875196456909 0.8367359638214111
CurrentTrain: epoch  1, batch     2 | loss: 8.6375332Losses:  7.1764936447143555 0.9027323722839355 0.7847849130630493
CurrentTrain: epoch  1, batch     3 | loss: 8.8640108Losses:  7.390933990478516 0.9655585289001465 0.8048989176750183
CurrentTrain: epoch  1, batch     4 | loss: 9.1613922Losses:  6.956557273864746 0.7211182117462158 0.7690302133560181
CurrentTrain: epoch  1, batch     5 | loss: 8.4467058Losses:  6.841618537902832 0.8582346439361572 0.7831032276153564
CurrentTrain: epoch  1, batch     6 | loss: 8.4829559Losses:  6.270252704620361 0.831902027130127 0.7626175284385681
CurrentTrain: epoch  1, batch     7 | loss: 7.8647723Losses:  6.5045390129089355 0.6800674200057983 0.7674949169158936
CurrentTrain: epoch  1, batch     8 | loss: 7.9521017Losses:  7.137781143188477 0.9411698579788208 0.8051087856292725
CurrentTrain: epoch  1, batch     9 | loss: 8.8840599Losses:  7.044149875640869 0.8375691175460815 0.7871435880661011
CurrentTrain: epoch  1, batch    10 | loss: 8.6688623Losses:  6.6271796226501465 1.031410813331604 0.8041487336158752
CurrentTrain: epoch  1, batch    11 | loss: 8.4627390Losses:  6.918183326721191 0.926256537437439 0.7826305627822876
CurrentTrain: epoch  1, batch    12 | loss: 8.6270704Losses:  7.259688377380371 0.6801532506942749 0.7905226349830627
CurrentTrain: epoch  1, batch    13 | loss: 8.7303648Losses:  6.271247863769531 0.7965189814567566 0.7687114477157593
CurrentTrain: epoch  1, batch    14 | loss: 7.8364782Losses:  7.738436698913574 0.823062539100647 0.7761707901954651
CurrentTrain: epoch  1, batch    15 | loss: 9.3376703Losses:  6.633089542388916 0.7583467960357666 0.7817219495773315
CurrentTrain: epoch  1, batch    16 | loss: 8.1731586Losses:  6.076658248901367 0.5520047545433044 0.7471544742584229
CurrentTrain: epoch  1, batch    17 | loss: 7.3758173Losses:  6.524425506591797 0.647108793258667 0.7435238361358643
CurrentTrain: epoch  1, batch    18 | loss: 7.9150581Losses:  6.715588569641113 0.7594764232635498 0.739223301410675
CurrentTrain: epoch  1, batch    19 | loss: 8.2142887Losses:  7.1103949546813965 0.8770580887794495 0.7631344199180603
CurrentTrain: epoch  1, batch    20 | loss: 8.7505875Losses:  7.104523658752441 0.8978715538978577 0.7758175134658813
CurrentTrain: epoch  1, batch    21 | loss: 8.7782135Losses:  6.708820819854736 0.8465331196784973 0.7811182737350464
CurrentTrain: epoch  1, batch    22 | loss: 8.3364725Losses:  6.890023231506348 0.5744974613189697 0.7447523474693298
CurrentTrain: epoch  1, batch    23 | loss: 8.2092724Losses:  6.6108527183532715 0.758515477180481 0.7559999227523804
CurrentTrain: epoch  1, batch    24 | loss: 8.1253681Losses:  5.643161773681641 0.650927722454071 0.753919780254364
CurrentTrain: epoch  1, batch    25 | loss: 7.0480089Losses:  6.207433223724365 0.829613983631134 0.7694105505943298
CurrentTrain: epoch  1, batch    26 | loss: 7.8064580Losses:  6.083014488220215 0.8076635599136353 0.7386000752449036
CurrentTrain: epoch  1, batch    27 | loss: 7.6292782Losses:  6.666765213012695 0.7029348611831665 0.745132327079773
CurrentTrain: epoch  1, batch    28 | loss: 8.1148319Losses:  6.007271766662598 0.7805003523826599 0.7423802018165588
CurrentTrain: epoch  1, batch    29 | loss: 7.5301523Losses:  6.509784698486328 0.5300605297088623 0.6943292617797852
CurrentTrain: epoch  1, batch    30 | loss: 7.7341747Losses:  5.680410385131836 0.7487699389457703 0.7329199314117432
CurrentTrain: epoch  1, batch    31 | loss: 7.1620998Losses:  5.630980491638184 0.7664356231689453 0.7247240543365479
CurrentTrain: epoch  1, batch    32 | loss: 7.1221399Losses:  6.009710311889648 0.5198888182640076 0.7111724615097046
CurrentTrain: epoch  1, batch    33 | loss: 7.2407718Losses:  6.717139720916748 0.8559927940368652 0.7423727512359619
CurrentTrain: epoch  1, batch    34 | loss: 8.3155050Losses:  5.681591033935547 0.7302689552307129 0.7415154576301575
CurrentTrain: epoch  1, batch    35 | loss: 7.1533756Losses:  6.969794273376465 0.5030390620231628 0.7276239395141602
CurrentTrain: epoch  1, batch    36 | loss: 8.2004566Losses:  6.5999531745910645 0.7730128765106201 0.7473955750465393
CurrentTrain: epoch  1, batch    37 | loss: 8.1203613Losses:  7.066043853759766 0.8674692511558533 0.7351107597351074
CurrentTrain: epoch  1, batch    38 | loss: 8.6686239Losses:  5.635915756225586 0.5699483156204224 0.6912709474563599
CurrentTrain: epoch  1, batch    39 | loss: 6.8971348Losses:  6.6079864501953125 0.5495861172676086 0.6948806047439575
CurrentTrain: epoch  1, batch    40 | loss: 7.8524532Losses:  6.849061489105225 0.9135007858276367 0.7729021906852722
CurrentTrain: epoch  1, batch    41 | loss: 8.5354643Losses:  7.060637474060059 0.7256198525428772 0.7334737181663513
CurrentTrain: epoch  1, batch    42 | loss: 8.5197306Losses:  7.2664642333984375 0.8041969537734985 0.7433711290359497
CurrentTrain: epoch  1, batch    43 | loss: 8.8140326Losses:  7.424322128295898 0.6511698961257935 0.7146644592285156
CurrentTrain: epoch  1, batch    44 | loss: 8.7901564Losses:  5.953329086303711 0.6228447556495667 0.7013762593269348
CurrentTrain: epoch  1, batch    45 | loss: 7.2775502Losses:  5.4817399978637695 0.6754299402236938 0.7283042669296265
CurrentTrain: epoch  1, batch    46 | loss: 6.8854742Losses:  6.475553512573242 0.8386140465736389 0.7305736541748047
CurrentTrain: epoch  1, batch    47 | loss: 8.0447407Losses:  6.3193678855896 0.7507057189941406 0.7612018585205078
CurrentTrain: epoch  1, batch    48 | loss: 7.8312755Losses:  5.818434715270996 0.619674801826477 0.7085427641868591
CurrentTrain: epoch  1, batch    49 | loss: 7.1466522Losses:  6.22281551361084 0.6089375019073486 0.6976583003997803
CurrentTrain: epoch  1, batch    50 | loss: 7.5294113Losses:  6.994623184204102 0.8071976900100708 0.752626895904541
CurrentTrain: epoch  1, batch    51 | loss: 8.5544472Losses:  6.268974304199219 0.6651315093040466 0.7137219905853271
CurrentTrain: epoch  1, batch    52 | loss: 7.6478281Losses:  6.091329574584961 0.5751908421516418 0.6632817983627319
CurrentTrain: epoch  1, batch    53 | loss: 7.3298025Losses:  6.363192081451416 0.7084999680519104 0.7344096899032593
CurrentTrain: epoch  1, batch    54 | loss: 7.8061018Losses:  5.180538177490234 0.35753151774406433 0.6437209844589233
CurrentTrain: epoch  1, batch    55 | loss: 6.1817908Losses:  5.393435478210449 0.513757050037384 0.6815969347953796
CurrentTrain: epoch  1, batch    56 | loss: 6.5887895Losses:  6.132998943328857 0.6779894232749939 0.6919535994529724
CurrentTrain: epoch  1, batch    57 | loss: 7.5029421Losses:  5.623137474060059 0.4797345995903015 0.702567994594574
CurrentTrain: epoch  1, batch    58 | loss: 6.8054399Losses:  6.686871528625488 0.7895256280899048 0.7019227743148804
CurrentTrain: epoch  1, batch    59 | loss: 8.1783199Losses:  6.361960411071777 0.784343957901001 0.7345386743545532
CurrentTrain: epoch  1, batch    60 | loss: 7.8808427Losses:  5.590246200561523 0.6368856430053711 0.6897344589233398
CurrentTrain: epoch  1, batch    61 | loss: 6.9168663Losses:  4.8243207931518555 0.24671116471290588 0.646148681640625
CurrentTrain: epoch  1, batch    62 | loss: 5.7171807Losses:  5.886567115783691 0.5382825136184692 0.7222051620483398
CurrentTrain: epoch  2, batch     0 | loss: 7.1470547Losses:  5.7856645584106445 0.48822885751724243 0.6314526796340942
CurrentTrain: epoch  2, batch     1 | loss: 6.9053459Losses:  6.172752380371094 0.6198575496673584 0.6527853012084961
CurrentTrain: epoch  2, batch     2 | loss: 7.4453955Losses:  5.2085113525390625 0.5806355476379395 0.6730514764785767
CurrentTrain: epoch  2, batch     3 | loss: 6.4621983Losses:  5.800744533538818 0.5370079874992371 0.6530638337135315
CurrentTrain: epoch  2, batch     4 | loss: 6.9908161Losses:  5.41335391998291 0.6303585767745972 0.6689014434814453
CurrentTrain: epoch  2, batch     5 | loss: 6.7126141Losses:  5.5800323486328125 0.4353824555873871 0.6652252674102783
CurrentTrain: epoch  2, batch     6 | loss: 6.6806402Losses:  6.001426696777344 0.49156898260116577 0.6807383894920349
CurrentTrain: epoch  2, batch     7 | loss: 7.1737342Losses:  6.62026309967041 0.5347249507904053 0.6663230061531067
CurrentTrain: epoch  2, batch     8 | loss: 7.8213115Losses:  5.255171775817871 0.35811564326286316 0.6309388279914856
CurrentTrain: epoch  2, batch     9 | loss: 6.2442265Losses:  5.211682319641113 0.5511292219161987 0.6770086884498596
CurrentTrain: epoch  2, batch    10 | loss: 6.4398203Losses:  5.875162124633789 0.5141971707344055 0.6709433794021606
CurrentTrain: epoch  2, batch    11 | loss: 7.0603027Losses:  6.118735313415527 0.5131833553314209 0.6638502478599548
CurrentTrain: epoch  2, batch    12 | loss: 7.2957692Losses:  5.832659721374512 0.46233266592025757 0.6480517387390137
CurrentTrain: epoch  2, batch    13 | loss: 6.9430442Losses:  5.614241123199463 0.48835617303848267 0.6463757753372192
CurrentTrain: epoch  2, batch    14 | loss: 6.7489729Losses:  5.440060615539551 0.442386269569397 0.6388991475105286
CurrentTrain: epoch  2, batch    15 | loss: 6.5213461Losses:  5.002713203430176 0.4035044312477112 0.6198366284370422
CurrentTrain: epoch  2, batch    16 | loss: 6.0260544Losses:  4.766149997711182 0.40026333928108215 0.6425809264183044
CurrentTrain: epoch  2, batch    17 | loss: 5.8089943Losses:  6.026812553405762 0.4866451621055603 0.6538792252540588
CurrentTrain: epoch  2, batch    18 | loss: 7.1673369Losses:  5.531291961669922 0.3816055357456207 0.5994000434875488
CurrentTrain: epoch  2, batch    19 | loss: 6.5122976Losses:  5.496072292327881 0.5105441808700562 0.6699408292770386
CurrentTrain: epoch  2, batch    20 | loss: 6.6765575Losses:  5.262063503265381 0.3732595443725586 0.6260409355163574
CurrentTrain: epoch  2, batch    21 | loss: 6.2613640Losses:  5.057302474975586 0.473206102848053 0.6256520748138428
CurrentTrain: epoch  2, batch    22 | loss: 6.1561604Losses:  5.66102933883667 0.388679563999176 0.6605063676834106
CurrentTrain: epoch  2, batch    23 | loss: 6.7102151Losses:  5.629877090454102 0.5990397930145264 0.6693891882896423
CurrentTrain: epoch  2, batch    24 | loss: 6.8983064Losses:  5.246982574462891 0.39456385374069214 0.6311036944389343
CurrentTrain: epoch  2, batch    25 | loss: 6.2726498Losses:  5.088956832885742 0.36803996562957764 0.6495825052261353
CurrentTrain: epoch  2, batch    26 | loss: 6.1065793Losses:  5.31981086730957 0.38218167424201965 0.6364905834197998
CurrentTrain: epoch  2, batch    27 | loss: 6.3384829Losses:  4.9701995849609375 0.43521320819854736 0.6450847387313843
CurrentTrain: epoch  2, batch    28 | loss: 6.0504975Losses:  5.336188316345215 0.4761781692504883 0.6270742416381836
CurrentTrain: epoch  2, batch    29 | loss: 6.4394407Losses:  5.619959354400635 0.6015995740890503 0.671894371509552
CurrentTrain: epoch  2, batch    30 | loss: 6.8934536Losses:  5.155182838439941 0.44766926765441895 0.6068108677864075
CurrentTrain: epoch  2, batch    31 | loss: 6.2096629Losses:  6.4172210693359375 0.5336540341377258 0.6763624548912048
CurrentTrain: epoch  2, batch    32 | loss: 7.6272378Losses:  4.8884687423706055 0.4914207458496094 0.6319984197616577
CurrentTrain: epoch  2, batch    33 | loss: 6.0118880Losses:  5.569903373718262 0.5397394895553589 0.6475766897201538
CurrentTrain: epoch  2, batch    34 | loss: 6.7572198Losses:  5.542533874511719 0.3703571557998657 0.6099722981452942
CurrentTrain: epoch  2, batch    35 | loss: 6.5228634Losses:  5.021390914916992 0.38402169942855835 0.6021988391876221
CurrentTrain: epoch  2, batch    36 | loss: 6.0076113Losses:  5.363813400268555 0.398881196975708 0.6451493501663208
CurrentTrain: epoch  2, batch    37 | loss: 6.4078436Losses:  5.31563663482666 0.2819240987300873 0.6084890365600586
CurrentTrain: epoch  2, batch    38 | loss: 6.2060499Losses:  4.688484191894531 0.38909414410591125 0.6388152837753296
CurrentTrain: epoch  2, batch    39 | loss: 5.7163939Losses:  5.542454719543457 0.4503898620605469 0.6116551160812378
CurrentTrain: epoch  2, batch    40 | loss: 6.6044998Losses:  5.883438587188721 0.3986026346683502 0.6828900575637817
CurrentTrain: epoch  2, batch    41 | loss: 6.9649310Losses:  5.190442085266113 0.44409048557281494 0.6295206546783447
CurrentTrain: epoch  2, batch    42 | loss: 6.2640533Losses:  5.313067436218262 0.3838132917881012 0.6602346897125244
CurrentTrain: epoch  2, batch    43 | loss: 6.3571157Losses:  5.16387939453125 0.4496472477912903 0.623956561088562
CurrentTrain: epoch  2, batch    44 | loss: 6.2374835Losses:  5.507838249206543 0.34773173928260803 0.6171467900276184
CurrentTrain: epoch  2, batch    45 | loss: 6.4727168Losses:  4.896636962890625 0.2926957607269287 0.5702054500579834
CurrentTrain: epoch  2, batch    46 | loss: 5.7595387Losses:  5.984580039978027 0.4757969081401825 0.6045030951499939
CurrentTrain: epoch  2, batch    47 | loss: 7.0648799Losses:  4.787164688110352 0.34141814708709717 0.6042431592941284
CurrentTrain: epoch  2, batch    48 | loss: 5.7328262Losses:  5.31762170791626 0.46090811491012573 0.6301964521408081
CurrentTrain: epoch  2, batch    49 | loss: 6.4087262Losses:  5.391380310058594 0.2666541039943695 0.6048924326896667
CurrentTrain: epoch  2, batch    50 | loss: 6.2629266Losses:  4.838171482086182 0.3320460915565491 0.6020702123641968
CurrentTrain: epoch  2, batch    51 | loss: 5.7722878Losses:  5.1974897384643555 0.2539644241333008 0.5752231478691101
CurrentTrain: epoch  2, batch    52 | loss: 6.0266771Losses:  5.183856010437012 0.3836544156074524 0.6122392416000366
CurrentTrain: epoch  2, batch    53 | loss: 6.1797500Losses:  4.9318461418151855 0.3746497333049774 0.614756166934967
CurrentTrain: epoch  2, batch    54 | loss: 5.9212518Losses:  4.987495422363281 0.3577437996864319 0.5994367599487305
CurrentTrain: epoch  2, batch    55 | loss: 5.9446759Losses:  5.428820610046387 0.5875340700149536 0.6098229885101318
CurrentTrain: epoch  2, batch    56 | loss: 6.6261778Losses:  5.147286415100098 0.3719406723976135 0.6236404180526733
CurrentTrain: epoch  2, batch    57 | loss: 6.1428676Losses:  4.936147689819336 0.3428035378456116 0.59636390209198
CurrentTrain: epoch  2, batch    58 | loss: 5.8753152Losses:  4.951108932495117 0.3263946771621704 0.6246777772903442
CurrentTrain: epoch  2, batch    59 | loss: 5.9021811Losses:  4.675654411315918 0.4371856451034546 0.6091761589050293
CurrentTrain: epoch  2, batch    60 | loss: 5.7220163Losses:  4.8836669921875 0.3992953896522522 0.6056947708129883
CurrentTrain: epoch  2, batch    61 | loss: 5.8886571Losses:  5.093136787414551 0.12037179619073868 0.5395337343215942
CurrentTrain: epoch  2, batch    62 | loss: 5.7530422Losses:  5.645393371582031 0.36521050333976746 0.5966686010360718
CurrentTrain: epoch  3, batch     0 | loss: 6.6072726Losses:  5.975554943084717 0.4293428063392639 0.6349064707756042
CurrentTrain: epoch  3, batch     1 | loss: 7.0398040Losses:  4.862429618835449 0.3613440990447998 0.6072146892547607
CurrentTrain: epoch  3, batch     2 | loss: 5.8309889Losses:  5.525772571563721 0.2951333820819855 0.5861320495605469
CurrentTrain: epoch  3, batch     3 | loss: 6.4070382Losses:  5.2300848960876465 0.3383222818374634 0.5857992172241211
CurrentTrain: epoch  3, batch     4 | loss: 6.1542063Losses:  5.211937427520752 0.25801438093185425 0.5729203820228577
CurrentTrain: epoch  3, batch     5 | loss: 6.0428720Losses:  4.903670310974121 0.35865241289138794 0.5770342946052551
CurrentTrain: epoch  3, batch     6 | loss: 5.8393574Losses:  4.954779624938965 0.3005967140197754 0.55794757604599
CurrentTrain: epoch  3, batch     7 | loss: 5.8133240Losses:  5.5215253829956055 0.45904505252838135 0.574786901473999
CurrentTrain: epoch  3, batch     8 | loss: 6.5553570Losses:  5.210842609405518 0.35970553755760193 0.5905868411064148
CurrentTrain: epoch  3, batch     9 | loss: 6.1611347Losses:  5.09890079498291 0.42767441272735596 0.5631116032600403
CurrentTrain: epoch  3, batch    10 | loss: 6.0896869Losses:  5.027088165283203 0.3280753493309021 0.6224520206451416
CurrentTrain: epoch  3, batch    11 | loss: 5.9776154Losses:  4.526576042175293 0.23789842426776886 0.5630015134811401
CurrentTrain: epoch  3, batch    12 | loss: 5.3274760Losses:  5.055991172790527 0.4780505299568176 0.5939382314682007
CurrentTrain: epoch  3, batch    13 | loss: 6.1279802Losses:  4.685686111450195 0.33193209767341614 0.564580500125885
CurrentTrain: epoch  3, batch    14 | loss: 5.5821986Losses:  4.66588830947876 0.3530550003051758 0.5603919625282288
CurrentTrain: epoch  3, batch    15 | loss: 5.5793352Losses:  4.975407600402832 0.40297749638557434 0.5746932029724121
CurrentTrain: epoch  3, batch    16 | loss: 5.9530783Losses:  5.217375755310059 0.3095357418060303 0.6105199456214905
CurrentTrain: epoch  3, batch    17 | loss: 6.1374316Losses:  5.133203983306885 0.28495121002197266 0.5532865524291992
CurrentTrain: epoch  3, batch    18 | loss: 5.9714417Losses:  4.966226100921631 0.35668689012527466 0.58827805519104
CurrentTrain: epoch  3, batch    19 | loss: 5.9111910Losses:  4.756828308105469 0.2706465721130371 0.5494083166122437
CurrentTrain: epoch  3, batch    20 | loss: 5.5768833Losses:  4.793365478515625 0.31477874517440796 0.5588498115539551
CurrentTrain: epoch  3, batch    21 | loss: 5.6669941Losses:  4.855411529541016 0.3914978504180908 0.5965220928192139
CurrentTrain: epoch  3, batch    22 | loss: 5.8434315Losses:  4.695594787597656 0.3731226325035095 0.5918610095977783
CurrentTrain: epoch  3, batch    23 | loss: 5.6605787Losses:  4.707558631896973 0.22969327867031097 0.5694413781166077
CurrentTrain: epoch  3, batch    24 | loss: 5.5066934Losses:  4.599934101104736 0.32958802580833435 0.5555274486541748
CurrentTrain: epoch  3, batch    25 | loss: 5.4850492Losses:  4.6662116050720215 0.35513558983802795 0.5735723376274109
CurrentTrain: epoch  3, batch    26 | loss: 5.5949192Losses:  4.751579761505127 0.3464936316013336 0.6144689917564392
CurrentTrain: epoch  3, batch    27 | loss: 5.7125425Losses:  4.853172779083252 0.37218236923217773 0.5630364418029785
CurrentTrain: epoch  3, batch    28 | loss: 5.7883916Losses:  5.196310043334961 0.3491027057170868 0.5729399919509888
CurrentTrain: epoch  3, batch    29 | loss: 6.1183524Losses:  5.389261245727539 0.33098775148391724 0.5736486911773682
CurrentTrain: epoch  3, batch    30 | loss: 6.2938976Losses:  4.377295017242432 0.2427537739276886 0.5318608283996582
CurrentTrain: epoch  3, batch    31 | loss: 5.1519098Losses:  4.794902801513672 0.27498435974121094 0.5812535881996155
CurrentTrain: epoch  3, batch    32 | loss: 5.6511407Losses:  4.796463966369629 0.2737188935279846 0.5909645557403564
CurrentTrain: epoch  3, batch    33 | loss: 5.6611471Losses:  4.553318977355957 0.3456053137779236 0.5456247329711914
CurrentTrain: epoch  3, batch    34 | loss: 5.4445491Losses:  4.616562366485596 0.36914896965026855 0.5581218004226685
CurrentTrain: epoch  3, batch    35 | loss: 5.5438328Losses:  5.49949836730957 0.5121983885765076 0.5790913105010986
CurrentTrain: epoch  3, batch    36 | loss: 6.5907879Losses:  4.670247554779053 0.21133793890476227 0.5315364003181458
CurrentTrain: epoch  3, batch    37 | loss: 5.4131222Losses:  4.486532688140869 0.2348634898662567 0.5291948318481445
CurrentTrain: epoch  3, batch    38 | loss: 5.2505908Losses:  5.634585380554199 0.2563968598842621 0.5148003101348877
CurrentTrain: epoch  3, batch    39 | loss: 6.4057827Losses:  4.630927562713623 0.30058273673057556 0.5456825494766235
CurrentTrain: epoch  3, batch    40 | loss: 5.4771929Losses:  4.545807361602783 0.26522135734558105 0.5439848303794861
CurrentTrain: epoch  3, batch    41 | loss: 5.3550134Losses:  4.976880073547363 0.24145202338695526 0.5883083343505859
CurrentTrain: epoch  3, batch    42 | loss: 5.8066406Losses:  4.681652069091797 0.20245638489723206 0.5433210730552673
CurrentTrain: epoch  3, batch    43 | loss: 5.4274297Losses:  4.931514739990234 0.3746730387210846 0.5639184713363647
CurrentTrain: epoch  3, batch    44 | loss: 5.8701062Losses:  5.126526832580566 0.2266685515642166 0.5360390543937683
CurrentTrain: epoch  3, batch    45 | loss: 5.8892341Losses:  4.420012474060059 0.24199843406677246 0.5310056209564209
CurrentTrain: epoch  3, batch    46 | loss: 5.1930170Losses:  4.51011323928833 0.23229792714118958 0.5722411870956421
CurrentTrain: epoch  3, batch    47 | loss: 5.3146524Losses:  4.475129127502441 0.2519107162952423 0.516007661819458
CurrentTrain: epoch  3, batch    48 | loss: 5.2430477Losses:  4.54444694519043 0.261052668094635 0.5624243021011353
CurrentTrain: epoch  3, batch    49 | loss: 5.3679237Losses:  4.530583381652832 0.29651010036468506 0.5502796173095703
CurrentTrain: epoch  3, batch    50 | loss: 5.3773732Losses:  4.734668731689453 0.20468640327453613 0.5847231149673462
CurrentTrain: epoch  3, batch    51 | loss: 5.5240779Losses:  4.494271278381348 0.32049018144607544 0.5767172574996948
CurrentTrain: epoch  3, batch    52 | loss: 5.3914790Losses:  5.220279693603516 0.3295344412326813 0.5486352443695068
CurrentTrain: epoch  3, batch    53 | loss: 6.0984497Losses:  4.538992881774902 0.31535911560058594 0.531324028968811
CurrentTrain: epoch  3, batch    54 | loss: 5.3856759Losses:  4.693282127380371 0.30856168270111084 0.5378273725509644
CurrentTrain: epoch  3, batch    55 | loss: 5.5396714Losses:  4.388463020324707 0.30247485637664795 0.5469412803649902
CurrentTrain: epoch  3, batch    56 | loss: 5.2378793Losses:  4.863085746765137 0.3055112957954407 0.5699626803398132
CurrentTrain: epoch  3, batch    57 | loss: 5.7385597Losses:  4.71800422668457 0.1829959899187088 0.5450512170791626
CurrentTrain: epoch  3, batch    58 | loss: 5.4460511Losses:  4.63916015625 0.18773184716701508 0.49474963545799255
CurrentTrain: epoch  3, batch    59 | loss: 5.3216414Losses:  4.514815807342529 0.34768056869506836 0.5349596738815308
CurrentTrain: epoch  3, batch    60 | loss: 5.3974562Losses:  4.417307376861572 0.20106223225593567 0.5184750556945801
CurrentTrain: epoch  3, batch    61 | loss: 5.1368446Losses:  4.399479866027832 0.06835459172725677 0.47538870573043823
CurrentTrain: epoch  3, batch    62 | loss: 4.9432230Losses:  4.375870704650879 0.22835877537727356 0.5176772475242615
CurrentTrain: epoch  4, batch     0 | loss: 5.1219068Losses:  4.421479225158691 0.24871201813220978 0.5451855659484863
CurrentTrain: epoch  4, batch     1 | loss: 5.2153769Losses:  4.506319999694824 0.1934220790863037 0.538388192653656
CurrentTrain: epoch  4, batch     2 | loss: 5.2381306Losses:  4.335294723510742 0.1991300880908966 0.46896934509277344
CurrentTrain: epoch  4, batch     3 | loss: 5.0033941Losses:  4.517033576965332 0.2591754198074341 0.5120829343795776
CurrentTrain: epoch  4, batch     4 | loss: 5.2882919Losses:  4.617038726806641 0.2855738401412964 0.5283920168876648
CurrentTrain: epoch  4, batch     5 | loss: 5.4310045Losses:  4.528881072998047 0.25923702120780945 0.501451849937439
CurrentTrain: epoch  4, batch     6 | loss: 5.2895699Losses:  4.372406005859375 0.2200099229812622 0.4901888370513916
CurrentTrain: epoch  4, batch     7 | loss: 5.0826044Losses:  4.379451274871826 0.25834712386131287 0.466797798871994
CurrentTrain: epoch  4, batch     8 | loss: 5.1045961Losses:  4.424868583679199 0.24561981856822968 0.46989041566848755
CurrentTrain: epoch  4, batch     9 | loss: 5.1403790Losses:  4.482234954833984 0.1676415055990219 0.5138555765151978
CurrentTrain: epoch  4, batch    10 | loss: 5.1637321Losses:  4.776924133300781 0.3132826089859009 0.5480519533157349
CurrentTrain: epoch  4, batch    11 | loss: 5.6382585Losses:  4.409487724304199 0.20903164148330688 0.5293954610824585
CurrentTrain: epoch  4, batch    12 | loss: 5.1479149Losses:  4.5844621658325195 0.3155366778373718 0.5278443098068237
CurrentTrain: epoch  4, batch    13 | loss: 5.4278431Losses:  4.218786239624023 0.17361769080162048 0.4969474673271179
CurrentTrain: epoch  4, batch    14 | loss: 4.8893514Losses:  4.3881306648254395 0.17128875851631165 0.5273920297622681
CurrentTrain: epoch  4, batch    15 | loss: 5.0868115Losses:  4.472318649291992 0.2928048372268677 0.5411593914031982
CurrentTrain: epoch  4, batch    16 | loss: 5.3062830Losses:  4.385105133056641 0.1261279284954071 0.44090867042541504
CurrentTrain: epoch  4, batch    17 | loss: 4.9521418Losses:  4.4194560050964355 0.26267534494400024 0.5082426071166992
CurrentTrain: epoch  4, batch    18 | loss: 5.1903739Losses:  4.400993347167969 0.32559216022491455 0.5172719955444336
CurrentTrain: epoch  4, batch    19 | loss: 5.2438574Losses:  4.938326835632324 0.2730787694454193 0.5151515007019043
CurrentTrain: epoch  4, batch    20 | loss: 5.7265573Losses:  4.630828857421875 0.19662919640541077 0.4695636034011841
CurrentTrain: epoch  4, batch    21 | loss: 5.2970214Losses:  4.235400199890137 0.2755560278892517 0.49309659004211426
CurrentTrain: epoch  4, batch    22 | loss: 5.0040531Losses:  4.219453811645508 0.07603535056114197 0.4683976173400879
CurrentTrain: epoch  4, batch    23 | loss: 4.7638869Losses:  4.597692012786865 0.23292122781276703 0.5751544833183289
CurrentTrain: epoch  4, batch    24 | loss: 5.4057674Losses:  4.398225784301758 0.23213982582092285 0.5109339952468872
CurrentTrain: epoch  4, batch    25 | loss: 5.1412992Losses:  4.304660797119141 0.24450604617595673 0.49127230048179626
CurrentTrain: epoch  4, batch    26 | loss: 5.0404391Losses:  4.221639633178711 0.13585646450519562 0.47707247734069824
CurrentTrain: epoch  4, batch    27 | loss: 4.8345690Losses:  4.390110015869141 0.25833091139793396 0.48974165320396423
CurrentTrain: epoch  4, batch    28 | loss: 5.1381826Losses:  4.351856231689453 0.15837298333644867 0.4969322979450226
CurrentTrain: epoch  4, batch    29 | loss: 5.0071616Losses:  4.261303901672363 0.1278187781572342 0.45922762155532837
CurrentTrain: epoch  4, batch    30 | loss: 4.8483500Losses:  4.431009769439697 0.21761029958724976 0.5010444521903992
CurrentTrain: epoch  4, batch    31 | loss: 5.1496644Losses:  4.611896514892578 0.32421064376831055 0.4762708842754364
CurrentTrain: epoch  4, batch    32 | loss: 5.4123778Losses:  4.397503852844238 0.3041514754295349 0.5304208993911743
CurrentTrain: epoch  4, batch    33 | loss: 5.2320762Losses:  4.32014799118042 0.11497075855731964 0.48887163400650024
CurrentTrain: epoch  4, batch    34 | loss: 4.9239902Losses:  4.761026382446289 0.23892201483249664 0.5115448832511902
CurrentTrain: epoch  4, batch    35 | loss: 5.5114932Losses:  4.376415729522705 0.24621383845806122 0.5297563672065735
CurrentTrain: epoch  4, batch    36 | loss: 5.1523862Losses:  4.634496688842773 0.29111745953559875 0.5057554244995117
CurrentTrain: epoch  4, batch    37 | loss: 5.4313698Losses:  4.317324161529541 0.1538621187210083 0.5097166299819946
CurrentTrain: epoch  4, batch    38 | loss: 4.9809027Losses:  4.27902889251709 0.25526857376098633 0.493163138628006
CurrentTrain: epoch  4, batch    39 | loss: 5.0274606Losses:  4.2313032150268555 0.14783990383148193 0.4650937020778656
CurrentTrain: epoch  4, batch    40 | loss: 4.8442369Losses:  4.171048641204834 0.2363947331905365 0.5049762725830078
CurrentTrain: epoch  4, batch    41 | loss: 4.9124198Losses:  5.346323013305664 0.3119293749332428 0.5233744978904724
CurrentTrain: epoch  4, batch    42 | loss: 6.1816268Losses:  4.812114715576172 0.15724852681159973 0.48219960927963257
CurrentTrain: epoch  4, batch    43 | loss: 5.4515629Losses:  4.1726531982421875 0.14441455900669098 0.43541550636291504
CurrentTrain: epoch  4, batch    44 | loss: 4.7524834Losses:  4.5420355796813965 0.24303385615348816 0.49446961283683777
CurrentTrain: epoch  4, batch    45 | loss: 5.2795391Losses:  5.018216609954834 0.21427395939826965 0.4584601819515228
CurrentTrain: epoch  4, batch    46 | loss: 5.6909509Losses:  4.813243389129639 0.2287604957818985 0.4408460557460785
CurrentTrain: epoch  4, batch    47 | loss: 5.4828501Losses:  4.451244354248047 0.22325661778450012 0.4789889454841614
CurrentTrain: epoch  4, batch    48 | loss: 5.1534901Losses:  4.2756524085998535 0.18735799193382263 0.4749518036842346
CurrentTrain: epoch  4, batch    49 | loss: 4.9379621Losses:  4.666172504425049 0.30250778794288635 0.49699175357818604
CurrentTrain: epoch  4, batch    50 | loss: 5.4656720Losses:  4.295466899871826 0.12452614307403564 0.4720041751861572
CurrentTrain: epoch  4, batch    51 | loss: 4.8919973Losses:  4.213879585266113 0.2603531777858734 0.47678810358047485
CurrentTrain: epoch  4, batch    52 | loss: 4.9510207Losses:  4.361052513122559 0.21364960074424744 0.5106867551803589
CurrentTrain: epoch  4, batch    53 | loss: 5.0853891Losses:  4.83418607711792 0.1991620659828186 0.47842997312545776
CurrentTrain: epoch  4, batch    54 | loss: 5.5117779Losses:  4.20815372467041 0.23380529880523682 0.4553637206554413
CurrentTrain: epoch  4, batch    55 | loss: 4.8973227Losses:  4.478848457336426 0.1635042428970337 0.45849883556365967
CurrentTrain: epoch  4, batch    56 | loss: 5.1008515Losses:  4.3610758781433105 0.2141980081796646 0.472009539604187
CurrentTrain: epoch  4, batch    57 | loss: 5.0472836Losses:  4.19335412979126 0.14050474762916565 0.4664161801338196
CurrentTrain: epoch  4, batch    58 | loss: 4.8002753Losses:  4.279941558837891 0.14943259954452515 0.42371731996536255
CurrentTrain: epoch  4, batch    59 | loss: 4.8530917Losses:  4.206081867218018 0.17276914417743683 0.4744994044303894
CurrentTrain: epoch  4, batch    60 | loss: 4.8533502Losses:  4.553497791290283 0.1722041219472885 0.43000566959381104
CurrentTrain: epoch  4, batch    61 | loss: 5.1557074Losses:  4.224339485168457 0.06478101015090942 0.4588686227798462
CurrentTrain: epoch  4, batch    62 | loss: 4.7479892Losses:  4.338183403015137 0.22152118384838104 0.4831393361091614
CurrentTrain: epoch  5, batch     0 | loss: 5.0428443Losses:  4.277795791625977 0.2503453493118286 0.4978177845478058
CurrentTrain: epoch  5, batch     1 | loss: 5.0259590Losses:  4.2351179122924805 0.12802238762378693 0.4477044343948364
CurrentTrain: epoch  5, batch     2 | loss: 4.8108444Losses:  4.180665969848633 0.13919320702552795 0.46780532598495483
CurrentTrain: epoch  5, batch     3 | loss: 4.7876644Losses:  4.311482906341553 0.13048267364501953 0.47306036949157715
CurrentTrain: epoch  5, batch     4 | loss: 4.9150257Losses:  4.494862079620361 0.20895478129386902 0.474225252866745
CurrentTrain: epoch  5, batch     5 | loss: 5.1780419Losses:  4.324899196624756 0.1698850840330124 0.4734809994697571
CurrentTrain: epoch  5, batch     6 | loss: 4.9682655Losses:  4.2072343826293945 0.2048763632774353 0.4863104224205017
CurrentTrain: epoch  5, batch     7 | loss: 4.8984213Losses:  4.283298492431641 0.16295504570007324 0.44504231214523315
CurrentTrain: epoch  5, batch     8 | loss: 4.8912959Losses:  4.1974945068359375 0.17011743783950806 0.4522930383682251
CurrentTrain: epoch  5, batch     9 | loss: 4.8199048Losses:  4.520406723022461 0.31412607431411743 0.48867255449295044
CurrentTrain: epoch  5, batch    10 | loss: 5.3232055Losses:  4.223118782043457 0.16162419319152832 0.4650363326072693
CurrentTrain: epoch  5, batch    11 | loss: 4.8497791Losses:  4.3165130615234375 0.23171144723892212 0.4409291446208954
CurrentTrain: epoch  5, batch    12 | loss: 4.9891534Losses:  4.203127861022949 0.1627616286277771 0.43604034185409546
CurrentTrain: epoch  5, batch    13 | loss: 4.8019300Losses:  4.119772911071777 0.22060830891132355 0.4415586590766907
CurrentTrain: epoch  5, batch    14 | loss: 4.7819400Losses:  4.259857177734375 0.13759876787662506 0.4629337191581726
CurrentTrain: epoch  5, batch    15 | loss: 4.8603897Losses:  4.26608943939209 0.1982923448085785 0.4451242685317993
CurrentTrain: epoch  5, batch    16 | loss: 4.9095058Losses:  4.147911071777344 0.20868057012557983 0.4317203164100647
CurrentTrain: epoch  5, batch    17 | loss: 4.7883120Losses:  4.29209566116333 0.20350459218025208 0.45404595136642456
CurrentTrain: epoch  5, batch    18 | loss: 4.9496460Losses:  4.225187301635742 0.13143324851989746 0.4402589201927185
CurrentTrain: epoch  5, batch    19 | loss: 4.7968798Losses:  4.292067527770996 0.17841657996177673 0.41958528757095337
CurrentTrain: epoch  5, batch    20 | loss: 4.8900695Losses:  4.398880958557129 0.1835322082042694 0.44101282954216003
CurrentTrain: epoch  5, batch    21 | loss: 5.0234261Losses:  4.229687690734863 0.20037513971328735 0.39326733350753784
CurrentTrain: epoch  5, batch    22 | loss: 4.8233299Losses:  4.239388942718506 0.1722877323627472 0.4572209119796753
CurrentTrain: epoch  5, batch    23 | loss: 4.8688979Losses:  4.18963623046875 0.11576077342033386 0.4016782343387604
CurrentTrain: epoch  5, batch    24 | loss: 4.7070751Losses:  4.353086471557617 0.19196118414402008 0.4502694308757782
CurrentTrain: epoch  5, batch    25 | loss: 4.9953170Losses:  4.246445178985596 0.19669850170612335 0.46355974674224854
CurrentTrain: epoch  5, batch    26 | loss: 4.9067035Losses:  4.189853668212891 0.16634848713874817 0.4574735164642334
CurrentTrain: epoch  5, batch    27 | loss: 4.8136759Losses:  4.18427038192749 0.17854538559913635 0.42544201016426086
CurrentTrain: epoch  5, batch    28 | loss: 4.7882581Losses:  4.14457368850708 0.20933672785758972 0.4737170338630676
CurrentTrain: epoch  5, batch    29 | loss: 4.8276277Losses:  4.157930374145508 0.10745738446712494 0.4053031802177429
CurrentTrain: epoch  5, batch    30 | loss: 4.6706905Losses:  4.182016849517822 0.1411689966917038 0.4183022975921631
CurrentTrain: epoch  5, batch    31 | loss: 4.7414885Losses:  4.254710674285889 0.1206139549612999 0.4238899350166321
CurrentTrain: epoch  5, batch    32 | loss: 4.7992148Losses:  4.107986927032471 0.21327300369739532 0.41111576557159424
CurrentTrain: epoch  5, batch    33 | loss: 4.7323756Losses:  4.28691291809082 0.2088492065668106 0.4283146858215332
CurrentTrain: epoch  5, batch    34 | loss: 4.9240770Losses:  4.149591445922852 0.15513724088668823 0.4385298192501068
CurrentTrain: epoch  5, batch    35 | loss: 4.7432585Losses:  4.201362609863281 0.1669887900352478 0.44212472438812256
CurrentTrain: epoch  5, batch    36 | loss: 4.8104763Losses:  4.130409240722656 0.20504939556121826 0.42189520597457886
CurrentTrain: epoch  5, batch    37 | loss: 4.7573538Losses:  4.156956672668457 0.1445806324481964 0.4556918740272522
CurrentTrain: epoch  5, batch    38 | loss: 4.7572293Losses:  4.304455757141113 0.1923556923866272 0.4531146287918091
CurrentTrain: epoch  5, batch    39 | loss: 4.9499259Losses:  4.105932235717773 0.18384036421775818 0.4249878525733948
CurrentTrain: epoch  5, batch    40 | loss: 4.7147603Losses:  4.160670280456543 0.17518766224384308 0.45347607135772705
CurrentTrain: epoch  5, batch    41 | loss: 4.7893338Losses:  4.286844730377197 0.23464953899383545 0.4390515387058258
CurrentTrain: epoch  5, batch    42 | loss: 4.9605460Losses:  4.15187931060791 0.1734326183795929 0.4731280207633972
CurrentTrain: epoch  5, batch    43 | loss: 4.7984400Losses:  4.153680324554443 0.2036803960800171 0.41556257009506226
CurrentTrain: epoch  5, batch    44 | loss: 4.7729235Losses:  4.098671913146973 0.24065789580345154 0.4352779686450958
CurrentTrain: epoch  5, batch    45 | loss: 4.7746077Losses:  4.133826732635498 0.218390554189682 0.43986693024635315
CurrentTrain: epoch  5, batch    46 | loss: 4.7920842Losses:  4.132817268371582 0.22109806537628174 0.4070570170879364
CurrentTrain: epoch  5, batch    47 | loss: 4.7609720Losses:  4.105175018310547 0.19868667423725128 0.45669853687286377
CurrentTrain: epoch  5, batch    48 | loss: 4.7605600Losses:  4.150506973266602 0.19262322783470154 0.4338485598564148
CurrentTrain: epoch  5, batch    49 | loss: 4.7769785Losses:  4.113851070404053 0.1693088710308075 0.4071308672428131
CurrentTrain: epoch  5, batch    50 | loss: 4.6902905Losses:  4.111261367797852 0.15331614017486572 0.4474141597747803
CurrentTrain: epoch  5, batch    51 | loss: 4.7119913Losses:  4.194480895996094 0.23513928055763245 0.43617910146713257
CurrentTrain: epoch  5, batch    52 | loss: 4.8657994Losses:  4.160126209259033 0.1828770935535431 0.44111987948417664
CurrentTrain: epoch  5, batch    53 | loss: 4.7841229Losses:  4.073151588439941 0.23162642121315002 0.42931219935417175
CurrentTrain: epoch  5, batch    54 | loss: 4.7340903Losses:  4.1538190841674805 0.23585166037082672 0.43060338497161865
CurrentTrain: epoch  5, batch    55 | loss: 4.8202744Losses:  4.14040470123291 0.20223647356033325 0.3957880437374115
CurrentTrain: epoch  5, batch    56 | loss: 4.7384295Losses:  4.228211402893066 0.1558992862701416 0.44906336069107056
CurrentTrain: epoch  5, batch    57 | loss: 4.8331738Losses:  4.069893836975098 0.14959657192230225 0.3891381025314331
CurrentTrain: epoch  5, batch    58 | loss: 4.6086287Losses:  4.14394474029541 0.19661150872707367 0.456798255443573
CurrentTrain: epoch  5, batch    59 | loss: 4.7973542Losses:  4.094078063964844 0.13006502389907837 0.4243309795856476
CurrentTrain: epoch  5, batch    60 | loss: 4.6484742Losses:  4.1008148193359375 0.156889408826828 0.45115816593170166
CurrentTrain: epoch  5, batch    61 | loss: 4.7088623Losses:  4.20487642288208 0.07002747058868408 0.38957422971725464
CurrentTrain: epoch  5, batch    62 | loss: 4.6644778Losses:  4.138408184051514 0.19001460075378418 0.40714144706726074
CurrentTrain: epoch  6, batch     0 | loss: 4.7355642Losses:  4.13633918762207 0.12316069006919861 0.378214955329895
CurrentTrain: epoch  6, batch     1 | loss: 4.6377149Losses:  4.057223320007324 0.13943161070346832 0.39222586154937744
CurrentTrain: epoch  6, batch     2 | loss: 4.5888805Losses:  4.243095397949219 0.2009960114955902 0.41287747025489807
CurrentTrain: epoch  6, batch     3 | loss: 4.8569689Losses:  4.123645782470703 0.14688175916671753 0.3838290572166443
CurrentTrain: epoch  6, batch     4 | loss: 4.6543565Losses:  4.132471084594727 0.11475566029548645 0.38881075382232666
CurrentTrain: epoch  6, batch     5 | loss: 4.6360373Losses:  4.113900184631348 0.18781724572181702 0.4186832308769226
CurrentTrain: epoch  6, batch     6 | loss: 4.7204003Losses:  4.140408515930176 0.12055782973766327 0.38671398162841797
CurrentTrain: epoch  6, batch     7 | loss: 4.6476803Losses:  4.075799942016602 0.20232552289962769 0.4056718945503235
CurrentTrain: epoch  6, batch     8 | loss: 4.6837974Losses:  4.142702579498291 0.13708271086215973 0.41201847791671753
CurrentTrain: epoch  6, batch     9 | loss: 4.6918035Losses:  4.11958122253418 0.17383913695812225 0.41657477617263794
CurrentTrain: epoch  6, batch    10 | loss: 4.7099953Losses:  4.182415962219238 0.10559546947479248 0.38220810890197754
CurrentTrain: epoch  6, batch    11 | loss: 4.6702194Losses:  4.149525165557861 0.17928504943847656 0.4364985227584839
CurrentTrain: epoch  6, batch    12 | loss: 4.7653089Losses:  4.113683700561523 0.10644678771495819 0.39801090955734253
CurrentTrain: epoch  6, batch    13 | loss: 4.6181412Losses:  4.118260383605957 0.18173031508922577 0.38370946049690247
CurrentTrain: epoch  6, batch    14 | loss: 4.6837001Losses:  4.174267768859863 0.1773020178079605 0.41626793146133423
CurrentTrain: epoch  6, batch    15 | loss: 4.7678375Losses:  4.094174385070801 0.18684956431388855 0.3736962676048279
CurrentTrain: epoch  6, batch    16 | loss: 4.6547203Losses:  4.0983428955078125 0.15756157040596008 0.4080001711845398
CurrentTrain: epoch  6, batch    17 | loss: 4.6639047Losses:  4.139286041259766 0.05556110292673111 0.38443928956985474
CurrentTrain: epoch  6, batch    18 | loss: 4.5792866Losses:  4.090469837188721 0.18938800692558289 0.3942238390445709
CurrentTrain: epoch  6, batch    19 | loss: 4.6740813Losses:  4.0763654708862305 0.21581541001796722 0.38654419779777527
CurrentTrain: epoch  6, batch    20 | loss: 4.6787252Losses:  4.203009605407715 0.14072176814079285 0.36043789982795715
CurrentTrain: epoch  6, batch    21 | loss: 4.7041693Losses:  4.150275230407715 0.15332308411598206 0.38493967056274414
CurrentTrain: epoch  6, batch    22 | loss: 4.6885381Losses:  4.281088829040527 0.13308633863925934 0.4393025040626526
CurrentTrain: epoch  6, batch    23 | loss: 4.8534775Losses:  4.144403457641602 0.15306656062602997 0.40910691022872925
CurrentTrain: epoch  6, batch    24 | loss: 4.7065768Losses:  4.143691539764404 0.1262444704771042 0.40950241684913635
CurrentTrain: epoch  6, batch    25 | loss: 4.6794386Losses:  4.1371002197265625 0.17432080209255219 0.3835746645927429
CurrentTrain: epoch  6, batch    26 | loss: 4.6949954Losses:  4.110198020935059 0.15888530015945435 0.34623152017593384
CurrentTrain: epoch  6, batch    27 | loss: 4.6153150Losses:  4.069416522979736 0.1800781786441803 0.3949347138404846
CurrentTrain: epoch  6, batch    28 | loss: 4.6444292Losses:  4.0922956466674805 0.18141746520996094 0.3650131821632385
CurrentTrain: epoch  6, batch    29 | loss: 4.6387262Losses:  4.09238338470459 0.16529330611228943 0.3611065149307251
CurrentTrain: epoch  6, batch    30 | loss: 4.6187830Losses:  4.039943695068359 0.10861524939537048 0.38816601037979126
CurrentTrain: epoch  6, batch    31 | loss: 4.5367250Losses:  4.134847640991211 0.11988270282745361 0.4151654839515686
CurrentTrain: epoch  6, batch    32 | loss: 4.6698956Losses:  4.094543933868408 0.19102010130882263 0.422618567943573
CurrentTrain: epoch  6, batch    33 | loss: 4.7081823Losses:  4.120569705963135 0.21902859210968018 0.38079333305358887
CurrentTrain: epoch  6, batch    34 | loss: 4.7203913Losses:  4.064611911773682 0.10885953903198242 0.43210485577583313
CurrentTrain: epoch  6, batch    35 | loss: 4.6055765Losses:  4.170212745666504 0.10960197448730469 0.3461519479751587
CurrentTrain: epoch  6, batch    36 | loss: 4.6259665Losses:  4.1095991134643555 0.14263218641281128 0.408060222864151
CurrentTrain: epoch  6, batch    37 | loss: 4.6602912Losses:  4.106026649475098 0.19137932360172272 0.38046127557754517
CurrentTrain: epoch  6, batch    38 | loss: 4.6778674Losses:  4.067174911499023 0.09931444376707077 0.3536311089992523
CurrentTrain: epoch  6, batch    39 | loss: 4.5201201Losses:  4.1324462890625 0.15117576718330383 0.41006529331207275
CurrentTrain: epoch  6, batch    40 | loss: 4.6936874Losses:  4.098010063171387 0.18131816387176514 0.38737165927886963
CurrentTrain: epoch  6, batch    41 | loss: 4.6666999Losses:  4.066479682922363 0.18252363801002502 0.3691585958003998
CurrentTrain: epoch  6, batch    42 | loss: 4.6181622Losses:  4.026758193969727 0.15481075644493103 0.3579205274581909
CurrentTrain: epoch  6, batch    43 | loss: 4.5394897Losses:  4.062603950500488 0.12786898016929626 0.3923022449016571
CurrentTrain: epoch  6, batch    44 | loss: 4.5827751Losses:  4.096031665802002 0.12437999248504639 0.3585183620452881
CurrentTrain: epoch  6, batch    45 | loss: 4.5789299Losses:  4.0978546142578125 0.18338826298713684 0.3429994285106659
CurrentTrain: epoch  6, batch    46 | loss: 4.6242423Losses:  4.061728477478027 0.1176939457654953 0.3240630030632019
CurrentTrain: epoch  6, batch    47 | loss: 4.5034852Losses:  4.055192470550537 0.15807166695594788 0.3822365999221802
CurrentTrain: epoch  6, batch    48 | loss: 4.5955005Losses:  4.083573818206787 0.08914899080991745 0.3135158121585846
CurrentTrain: epoch  6, batch    49 | loss: 4.4862385Losses:  4.108009338378906 0.11741796880960464 0.36758339405059814
CurrentTrain: epoch  6, batch    50 | loss: 4.5930104Losses:  4.0350799560546875 0.12529659271240234 0.3745476305484772
CurrentTrain: epoch  6, batch    51 | loss: 4.5349240Losses:  4.088027000427246 0.13483744859695435 0.3998538851737976
CurrentTrain: epoch  6, batch    52 | loss: 4.6227183Losses:  4.083500385284424 0.04428558051586151 0.38114672899246216
CurrentTrain: epoch  6, batch    53 | loss: 4.5089331Losses:  4.056594371795654 0.1517755091190338 0.36245250701904297
CurrentTrain: epoch  6, batch    54 | loss: 4.5708222Losses:  4.026848793029785 0.1645733267068863 0.41620051860809326
CurrentTrain: epoch  6, batch    55 | loss: 4.6076226Losses:  4.118608474731445 0.09640905261039734 0.4004748463630676
CurrentTrain: epoch  6, batch    56 | loss: 4.6154923Losses:  4.038684368133545 0.12153717130422592 0.37964940071105957
CurrentTrain: epoch  6, batch    57 | loss: 4.5398712Losses:  4.0523223876953125 0.13155704736709595 0.35285836458206177
CurrentTrain: epoch  6, batch    58 | loss: 4.5367379Losses:  4.0642805099487305 0.18480947613716125 0.37996894121170044
CurrentTrain: epoch  6, batch    59 | loss: 4.6290593Losses:  4.116541385650635 0.10789520293474197 0.36815333366394043
CurrentTrain: epoch  6, batch    60 | loss: 4.5925903Losses:  4.069602012634277 0.13505177199840546 0.3929192125797272
CurrentTrain: epoch  6, batch    61 | loss: 4.5975728Losses:  4.010422706604004 0.18652796745300293 0.4134465754032135
CurrentTrain: epoch  6, batch    62 | loss: 4.6103973Losses:  4.098689079284668 0.1069202795624733 0.3252969980239868
CurrentTrain: epoch  7, batch     0 | loss: 4.5309062Losses:  4.088520050048828 0.12420289218425751 0.38309136033058167
CurrentTrain: epoch  7, batch     1 | loss: 4.5958142Losses:  4.065545082092285 0.12654660642147064 0.37319833040237427
CurrentTrain: epoch  7, batch     2 | loss: 4.5652900Losses:  4.052112579345703 0.1250191330909729 0.3536156415939331
CurrentTrain: epoch  7, batch     3 | loss: 4.5307474Losses:  4.053569316864014 0.10400880873203278 0.3904799222946167
CurrentTrain: epoch  7, batch     4 | loss: 4.5480580Losses:  4.024600505828857 0.09442435204982758 0.3194354176521301
CurrentTrain: epoch  7, batch     5 | loss: 4.4384604Losses:  4.041665077209473 0.12029224634170532 0.36893439292907715
CurrentTrain: epoch  7, batch     6 | loss: 4.5308914Losses:  4.102113723754883 0.15973183512687683 0.3848973512649536
CurrentTrain: epoch  7, batch     7 | loss: 4.6467428Losses:  3.983231544494629 0.08417951315641403 0.3652753531932831
CurrentTrain: epoch  7, batch     8 | loss: 4.4326863Losses:  4.023714065551758 0.17908629775047302 0.3778623342514038
CurrentTrain: epoch  7, batch     9 | loss: 4.5806627Losses:  4.019536972045898 0.2019471526145935 0.3520672917366028
CurrentTrain: epoch  7, batch    10 | loss: 4.5735517Losses:  3.998004198074341 0.124381884932518 0.34003499150276184
CurrentTrain: epoch  7, batch    11 | loss: 4.4624209Losses:  4.0088653564453125 0.15873821079730988 0.28690940141677856
CurrentTrain: epoch  7, batch    12 | loss: 4.4545131Losses:  4.045056343078613 0.10358672589063644 0.3512577712535858
CurrentTrain: epoch  7, batch    13 | loss: 4.4999008Losses:  4.010824203491211 0.14426851272583008 0.35460618138313293
CurrentTrain: epoch  7, batch    14 | loss: 4.5096989Losses:  3.9960556030273438 0.20010794699192047 0.34531188011169434
CurrentTrain: epoch  7, batch    15 | loss: 4.5414753Losses:  4.072465419769287 0.2232644110918045 0.36890149116516113
CurrentTrain: epoch  7, batch    16 | loss: 4.6646309Losses:  4.229650497436523 0.27520671486854553 0.3580949008464813
CurrentTrain: epoch  7, batch    17 | loss: 4.8629518Losses:  4.047152519226074 0.1137574091553688 0.3500668406486511
CurrentTrain: epoch  7, batch    18 | loss: 4.5109768Losses:  4.098891258239746 0.11396212875843048 0.37378066778182983
CurrentTrain: epoch  7, batch    19 | loss: 4.5866342Losses:  4.06995964050293 0.08176769316196442 0.3795675039291382
CurrentTrain: epoch  7, batch    20 | loss: 4.5312948Losses:  3.9994397163391113 0.12269321829080582 0.35502639412879944
CurrentTrain: epoch  7, batch    21 | loss: 4.4771590Losses:  4.034317493438721 0.11895299702882767 0.2967231571674347
CurrentTrain: epoch  7, batch    22 | loss: 4.4499941Losses:  4.04436731338501 0.09534962475299835 0.4021684527397156
CurrentTrain: epoch  7, batch    23 | loss: 4.5418854Losses:  4.022171974182129 0.1738949418067932 0.37517207860946655
CurrentTrain: epoch  7, batch    24 | loss: 4.5712390Losses:  4.16792106628418 0.13558748364448547 0.3721972107887268
CurrentTrain: epoch  7, batch    25 | loss: 4.6757059Losses:  4.013744831085205 0.08642110228538513 0.3774072229862213
CurrentTrain: epoch  7, batch    26 | loss: 4.4775729Losses:  4.042132377624512 0.12601366639137268 0.3427639603614807
CurrentTrain: epoch  7, batch    27 | loss: 4.5109100Losses:  4.041435241699219 0.15610408782958984 0.3582864999771118
CurrentTrain: epoch  7, batch    28 | loss: 4.5558257Losses:  4.059267997741699 0.07747746258974075 0.3335646986961365
CurrentTrain: epoch  7, batch    29 | loss: 4.4703102Losses:  4.0737104415893555 0.09297404438257217 0.33121076226234436
CurrentTrain: epoch  7, batch    30 | loss: 4.4978952Losses:  4.071752548217773 0.13008925318717957 0.38421130180358887
CurrentTrain: epoch  7, batch    31 | loss: 4.5860529Losses:  4.070126056671143 0.07330922037363052 0.2740001678466797
CurrentTrain: epoch  7, batch    32 | loss: 4.4174356Losses:  4.1382551193237305 0.12818484008312225 0.37921974062919617
CurrentTrain: epoch  7, batch    33 | loss: 4.6456594Losses:  4.088715553283691 0.07933486253023148 0.29786401987075806
CurrentTrain: epoch  7, batch    34 | loss: 4.4659142Losses:  4.031274318695068 0.09198407828807831 0.34608837962150574
CurrentTrain: epoch  7, batch    35 | loss: 4.4693470Losses:  4.1166510581970215 0.11152122169733047 0.3286452293395996
CurrentTrain: epoch  7, batch    36 | loss: 4.5568175Losses:  4.010273456573486 0.10591703653335571 0.32866448163986206
CurrentTrain: epoch  7, batch    37 | loss: 4.4448547Losses:  4.030797481536865 0.12678682804107666 0.34883198142051697
CurrentTrain: epoch  7, batch    38 | loss: 4.5064163Losses:  4.002745628356934 0.09740188717842102 0.33937716484069824
CurrentTrain: epoch  7, batch    39 | loss: 4.4395247Losses:  4.039578914642334 0.16934463381767273 0.3072929084300995
CurrentTrain: epoch  7, batch    40 | loss: 4.5162163Losses:  4.016761779785156 0.11559305340051651 0.33187928795814514
CurrentTrain: epoch  7, batch    41 | loss: 4.4642339Losses:  4.023253917694092 0.10390442609786987 0.38680604100227356
CurrentTrain: epoch  7, batch    42 | loss: 4.5139642Losses:  4.061748504638672 0.12246472388505936 0.3104148507118225
CurrentTrain: epoch  7, batch    43 | loss: 4.4946280Losses:  4.028574466705322 0.12015154957771301 0.3304241895675659
CurrentTrain: epoch  7, batch    44 | loss: 4.4791503Losses:  3.9759809970855713 0.10615913569927216 0.3325875699520111
CurrentTrain: epoch  7, batch    45 | loss: 4.4147277Losses:  4.03513240814209 0.10024948418140411 0.31838324666023254
CurrentTrain: epoch  7, batch    46 | loss: 4.4537649Losses:  4.022865295410156 0.11158742755651474 0.3275947570800781
CurrentTrain: epoch  7, batch    47 | loss: 4.4620476Losses:  4.030450344085693 0.191676527261734 0.33818352222442627
CurrentTrain: epoch  7, batch    48 | loss: 4.5603104Losses:  3.9848830699920654 0.19505351781845093 0.33272746205329895
CurrentTrain: epoch  7, batch    49 | loss: 4.5126638Losses:  3.967867851257324 0.08496229350566864 0.3808073401451111
CurrentTrain: epoch  7, batch    50 | loss: 4.4336376Losses:  4.013779640197754 0.10628004372119904 0.3637283444404602
CurrentTrain: epoch  7, batch    51 | loss: 4.4837880Losses:  4.072902679443359 0.21089772880077362 0.3476483225822449
CurrentTrain: epoch  7, batch    52 | loss: 4.6314487Losses:  4.062657356262207 0.1176200807094574 0.3369218409061432
CurrentTrain: epoch  7, batch    53 | loss: 4.5171990Losses:  4.065520286560059 0.09787245094776154 0.3902358412742615
CurrentTrain: epoch  7, batch    54 | loss: 4.5536284Losses:  4.026556968688965 0.128436878323555 0.3344416618347168
CurrentTrain: epoch  7, batch    55 | loss: 4.4894357Losses:  4.0256195068359375 0.09230820834636688 0.30693483352661133
CurrentTrain: epoch  7, batch    56 | loss: 4.4248624Losses:  4.013326644897461 0.11928834021091461 0.28901922702789307
CurrentTrain: epoch  7, batch    57 | loss: 4.4216342Losses:  4.007926940917969 0.16311228275299072 0.31645631790161133
CurrentTrain: epoch  7, batch    58 | loss: 4.4874954Losses:  4.036210060119629 0.08081074059009552 0.31781846284866333
CurrentTrain: epoch  7, batch    59 | loss: 4.4348392Losses:  4.036168098449707 0.13120634853839874 0.3211691677570343
CurrentTrain: epoch  7, batch    60 | loss: 4.4885440Losses:  4.020959854125977 0.08202485740184784 0.36738094687461853
CurrentTrain: epoch  7, batch    61 | loss: 4.4703660Losses:  4.054150581359863 0.07006782293319702 0.28472405672073364
CurrentTrain: epoch  7, batch    62 | loss: 4.4089427Losses:  4.032989501953125 0.15848281979560852 0.3320758044719696
CurrentTrain: epoch  8, batch     0 | loss: 4.5235481Losses:  4.050510406494141 0.13656380772590637 0.3606492280960083
CurrentTrain: epoch  8, batch     1 | loss: 4.5477233Losses:  4.018649101257324 0.14846199750900269 0.3035464584827423
CurrentTrain: epoch  8, batch     2 | loss: 4.4706573Losses:  4.03038215637207 0.078261598944664 0.3081705868244171
CurrentTrain: epoch  8, batch     3 | loss: 4.4168143Losses:  3.9751884937286377 0.10416543483734131 0.33057573437690735
CurrentTrain: epoch  8, batch     4 | loss: 4.4099298Losses:  4.030122756958008 0.13438576459884644 0.2721903622150421
CurrentTrain: epoch  8, batch     5 | loss: 4.4366989Losses:  4.022347450256348 0.10101582854986191 0.3539671003818512
CurrentTrain: epoch  8, batch     6 | loss: 4.4773307Losses:  4.042693138122559 0.09044337272644043 0.3496227264404297
CurrentTrain: epoch  8, batch     7 | loss: 4.4827595Losses:  4.011934757232666 0.09909483045339584 0.3189845085144043
CurrentTrain: epoch  8, batch     8 | loss: 4.4300141Losses:  4.030902862548828 0.11757603287696838 0.3391163945198059
CurrentTrain: epoch  8, batch     9 | loss: 4.4875956Losses:  3.9870970249176025 0.08155927062034607 0.34148913621902466
CurrentTrain: epoch  8, batch    10 | loss: 4.4101458Losses:  4.007779121398926 0.11130791157484055 0.2869155704975128
CurrentTrain: epoch  8, batch    11 | loss: 4.4060030Losses:  4.061583995819092 0.09834638237953186 0.31802546977996826
CurrentTrain: epoch  8, batch    12 | loss: 4.4779558Losses:  4.021244049072266 0.09720811247825623 0.2925374507904053
CurrentTrain: epoch  8, batch    13 | loss: 4.4109898Losses:  3.979865074157715 0.15884676575660706 0.3241068422794342
CurrentTrain: epoch  8, batch    14 | loss: 4.4628186Losses:  4.00183629989624 0.13562804460525513 0.32301583886146545
CurrentTrain: epoch  8, batch    15 | loss: 4.4604802Losses:  3.9810948371887207 0.07137805223464966 0.2917431890964508
CurrentTrain: epoch  8, batch    16 | loss: 4.3442163Losses:  4.0183515548706055 0.12378144264221191 0.3016234338283539
CurrentTrain: epoch  8, batch    17 | loss: 4.4437561Losses:  4.032224655151367 0.12277991324663162 0.33575260639190674
CurrentTrain: epoch  8, batch    18 | loss: 4.4907570Losses:  4.029291152954102 0.09869826585054398 0.32499629259109497
CurrentTrain: epoch  8, batch    19 | loss: 4.4529858Losses:  3.9397225379943848 0.1156139224767685 0.24885529279708862
CurrentTrain: epoch  8, batch    20 | loss: 4.3041916Losses:  4.000813961029053 0.09490598738193512 0.3004126250743866
CurrentTrain: epoch  8, batch    21 | loss: 4.3961325Losses:  4.000194549560547 0.13557493686676025 0.3048948049545288
CurrentTrain: epoch  8, batch    22 | loss: 4.4406643Losses:  4.017942428588867 0.1368097960948944 0.3220420479774475
CurrentTrain: epoch  8, batch    23 | loss: 4.4767942Losses:  4.0293169021606445 0.1585502177476883 0.30563968420028687
CurrentTrain: epoch  8, batch    24 | loss: 4.4935069Losses:  3.9955921173095703 0.0780969187617302 0.2936779260635376
CurrentTrain: epoch  8, batch    25 | loss: 4.3673668Losses:  4.020623683929443 0.1288691908121109 0.3239322304725647
CurrentTrain: epoch  8, batch    26 | loss: 4.4734249Losses:  4.048983573913574 0.1248343288898468 0.315771222114563
CurrentTrain: epoch  8, batch    27 | loss: 4.4895892Losses:  4.005338191986084 0.16654984652996063 0.3011462092399597
CurrentTrain: epoch  8, batch    28 | loss: 4.4730339Losses:  4.010586261749268 0.11234631389379501 0.3134816288948059
CurrentTrain: epoch  8, batch    29 | loss: 4.4364142Losses:  3.9982597827911377 0.09813466668128967 0.32710298895835876
CurrentTrain: epoch  8, batch    30 | loss: 4.4234977Losses:  4.0071892738342285 0.1188814714550972 0.2743871212005615
CurrentTrain: epoch  8, batch    31 | loss: 4.4004583Losses:  4.0283122062683105 0.12056748569011688 0.30919989943504333
CurrentTrain: epoch  8, batch    32 | loss: 4.4580793Losses:  4.0225300788879395 0.0879107341170311 0.31141722202301025
CurrentTrain: epoch  8, batch    33 | loss: 4.4218578Losses:  4.019608974456787 0.1489035189151764 0.3090571463108063
CurrentTrain: epoch  8, batch    34 | loss: 4.4775696Losses:  4.004219055175781 0.13489195704460144 0.2898973822593689
CurrentTrain: epoch  8, batch    35 | loss: 4.4290085Losses:  4.0173444747924805 0.07780347019433975 0.2774655222892761
CurrentTrain: epoch  8, batch    36 | loss: 4.3726134Losses:  4.044379711151123 0.12041953206062317 0.3316873610019684
CurrentTrain: epoch  8, batch    37 | loss: 4.4964867Losses:  3.9762563705444336 0.13291914761066437 0.3282627761363983
CurrentTrain: epoch  8, batch    38 | loss: 4.4374385Losses:  4.043463706970215 0.13422055542469025 0.32343748211860657
CurrentTrain: epoch  8, batch    39 | loss: 4.5011220Losses:  4.172957420349121 0.14505787193775177 0.33488771319389343
CurrentTrain: epoch  8, batch    40 | loss: 4.6529026Losses:  4.025758743286133 0.11092665791511536 0.3042925000190735
CurrentTrain: epoch  8, batch    41 | loss: 4.4409781Losses:  4.025852203369141 0.12283718585968018 0.2792113125324249
CurrentTrain: epoch  8, batch    42 | loss: 4.4279008Losses:  4.052463531494141 0.151931494474411 0.3009779453277588
CurrentTrain: epoch  8, batch    43 | loss: 4.5053730Losses:  4.02436637878418 0.12619169056415558 0.2919038236141205
CurrentTrain: epoch  8, batch    44 | loss: 4.4424620Losses:  4.141024589538574 0.07487881183624268 0.32678794860839844
CurrentTrain: epoch  8, batch    45 | loss: 4.5426912Losses:  3.9717721939086914 0.10166735202074051 0.29024526476860046
CurrentTrain: epoch  8, batch    46 | loss: 4.3636847Losses:  4.0112457275390625 0.14922359585762024 0.29944705963134766
CurrentTrain: epoch  8, batch    47 | loss: 4.4599166Losses:  4.030052185058594 0.08590424060821533 0.2945282459259033
CurrentTrain: epoch  8, batch    48 | loss: 4.4104843Losses:  3.992645740509033 0.09473779797554016 0.34396877884864807
CurrentTrain: epoch  8, batch    49 | loss: 4.4313526Losses:  3.9938454627990723 0.08602709323167801 0.2908531129360199
CurrentTrain: epoch  8, batch    50 | loss: 4.3707256Losses:  4.024418830871582 0.09730886667966843 0.30257195234298706
CurrentTrain: epoch  8, batch    51 | loss: 4.4242992Losses:  4.017777442932129 0.19232362508773804 0.3030233085155487
CurrentTrain: epoch  8, batch    52 | loss: 4.5131245Losses:  4.02471923828125 0.14976339042186737 0.31216198205947876
CurrentTrain: epoch  8, batch    53 | loss: 4.4866447Losses:  3.985203504562378 0.12934967875480652 0.3009829521179199
CurrentTrain: epoch  8, batch    54 | loss: 4.4155359Losses:  3.9895169734954834 0.1271895170211792 0.2885361909866333
CurrentTrain: epoch  8, batch    55 | loss: 4.4052424Losses:  4.012080192565918 0.13078990578651428 0.25335830450057983
CurrentTrain: epoch  8, batch    56 | loss: 4.3962283Losses:  3.9938340187072754 0.12154261767864227 0.2690136432647705
CurrentTrain: epoch  8, batch    57 | loss: 4.3843899Losses:  4.021962642669678 0.12199005484580994 0.26220202445983887
CurrentTrain: epoch  8, batch    58 | loss: 4.4061546Losses:  4.022932052612305 0.1612473428249359 0.29535597562789917
CurrentTrain: epoch  8, batch    59 | loss: 4.4795351Losses:  3.9491171836853027 0.17253488302230835 0.2779320776462555
CurrentTrain: epoch  8, batch    60 | loss: 4.3995843Losses:  4.016311168670654 0.13281740248203278 0.25264450907707214
CurrentTrain: epoch  8, batch    61 | loss: 4.4017730Losses:  3.974515438079834 0.024593394249677658 0.2809019684791565
CurrentTrain: epoch  8, batch    62 | loss: 4.2800107Losses:  3.995906352996826 0.087783582508564 0.27557292580604553
CurrentTrain: epoch  9, batch     0 | loss: 4.3592629Losses:  4.0735182762146 0.10280279815196991 0.3234829604625702
CurrentTrain: epoch  9, batch     1 | loss: 4.4998040Losses:  4.030087471008301 0.09715075045824051 0.3186379075050354
CurrentTrain: epoch  9, batch     2 | loss: 4.4458761Losses:  4.04321813583374 0.10164286196231842 0.2946489453315735
CurrentTrain: epoch  9, batch     3 | loss: 4.4395103Losses:  3.9987058639526367 0.08375862240791321 0.30614984035491943
CurrentTrain: epoch  9, batch     4 | loss: 4.3886147Losses:  4.0000224113464355 0.07772265374660492 0.29476138949394226
CurrentTrain: epoch  9, batch     5 | loss: 4.3725061Losses:  4.017295837402344 0.11468057334423065 0.28436189889907837
CurrentTrain: epoch  9, batch     6 | loss: 4.4163384Losses:  3.9932503700256348 0.08653204143047333 0.26563531160354614
CurrentTrain: epoch  9, batch     7 | loss: 4.3454180Losses:  3.9678421020507812 0.09880882501602173 0.2730438709259033
CurrentTrain: epoch  9, batch     8 | loss: 4.3396950Losses:  3.997157335281372 0.14756393432617188 0.2711286246776581
CurrentTrain: epoch  9, batch     9 | loss: 4.4158497Losses:  4.030348300933838 0.1177300214767456 0.27829283475875854
CurrentTrain: epoch  9, batch    10 | loss: 4.4263711Losses:  4.025308609008789 0.14775216579437256 0.27631163597106934
CurrentTrain: epoch  9, batch    11 | loss: 4.4493723Losses:  4.002545356750488 0.13208898901939392 0.3054164946079254
CurrentTrain: epoch  9, batch    12 | loss: 4.4400511Losses:  4.005389213562012 0.10533902049064636 0.25093740224838257
CurrentTrain: epoch  9, batch    13 | loss: 4.3616657Losses:  4.009958267211914 0.16987326741218567 0.30153486132621765
CurrentTrain: epoch  9, batch    14 | loss: 4.4813662Losses:  4.0299272537231445 0.11499608308076859 0.2818293273448944
CurrentTrain: epoch  9, batch    15 | loss: 4.4267526Losses:  3.9888997077941895 0.1272979974746704 0.2757105231285095
CurrentTrain: epoch  9, batch    16 | loss: 4.3919082Losses:  4.028057098388672 0.12645386159420013 0.27244922518730164
CurrentTrain: epoch  9, batch    17 | loss: 4.4269600Losses:  3.9942116737365723 0.07207480072975159 0.31503844261169434
CurrentTrain: epoch  9, batch    18 | loss: 4.3813248Losses:  3.942312479019165 0.0456690788269043 0.3303297758102417
CurrentTrain: epoch  9, batch    19 | loss: 4.3183112Losses:  4.05220365524292 0.09862236678600311 0.26403141021728516
CurrentTrain: epoch  9, batch    20 | loss: 4.4148574Losses:  3.9886631965637207 0.11234402656555176 0.30214208364486694
CurrentTrain: epoch  9, batch    21 | loss: 4.4031496Losses:  4.036719799041748 0.1198253259062767 0.2868485748767853
CurrentTrain: epoch  9, batch    22 | loss: 4.4433937Losses:  3.9852585792541504 0.09612791240215302 0.28786757588386536
CurrentTrain: epoch  9, batch    23 | loss: 4.3692541Losses:  3.9893109798431396 0.09055079519748688 0.24573247134685516
CurrentTrain: epoch  9, batch    24 | loss: 4.3255939Losses:  4.008395671844482 0.10028421878814697 0.3109997808933258
CurrentTrain: epoch  9, batch    25 | loss: 4.4196796Losses:  3.956789970397949 0.14094191789627075 0.33534425497055054
CurrentTrain: epoch  9, batch    26 | loss: 4.4330764Losses:  3.9869723320007324 0.08994516730308533 0.2595294713973999
CurrentTrain: epoch  9, batch    27 | loss: 4.3364472Losses:  3.980510711669922 0.1357516348361969 0.24392324686050415
CurrentTrain: epoch  9, batch    28 | loss: 4.3601856Losses:  3.9845528602600098 0.12661944329738617 0.2690237760543823
CurrentTrain: epoch  9, batch    29 | loss: 4.3801961Losses:  4.021627426147461 0.13667243719100952 0.23806729912757874
CurrentTrain: epoch  9, batch    30 | loss: 4.3963671Losses:  3.97694730758667 0.17027083039283752 0.2910007834434509
CurrentTrain: epoch  9, batch    31 | loss: 4.4382191Losses:  3.954866409301758 0.1063138097524643 0.2633622884750366
CurrentTrain: epoch  9, batch    32 | loss: 4.3245425Losses:  3.9584665298461914 0.033859219402074814 0.27228134870529175
CurrentTrain: epoch  9, batch    33 | loss: 4.2646070Losses:  4.001665115356445 0.12752056121826172 0.2532539963722229
CurrentTrain: epoch  9, batch    34 | loss: 4.3824396Losses:  3.988032817840576 0.15261580049991608 0.29786765575408936
CurrentTrain: epoch  9, batch    35 | loss: 4.4385166Losses:  3.984253168106079 0.05714108794927597 0.27823519706726074
CurrentTrain: epoch  9, batch    36 | loss: 4.3196297Losses:  3.984711170196533 0.0919400006532669 0.306801438331604
CurrentTrain: epoch  9, batch    37 | loss: 4.3834524Losses:  4.024816036224365 0.06564800441265106 0.27951717376708984
CurrentTrain: epoch  9, batch    38 | loss: 4.3699813Losses:  3.9834177494049072 0.15629643201828003 0.2656899690628052
CurrentTrain: epoch  9, batch    39 | loss: 4.4054041Losses:  4.072945594787598 0.08284035325050354 0.26284337043762207
CurrentTrain: epoch  9, batch    40 | loss: 4.4186296Losses:  3.979146957397461 0.05683211237192154 0.2990146577358246
CurrentTrain: epoch  9, batch    41 | loss: 4.3349938Losses:  3.998361587524414 0.06429289281368256 0.24730290472507477
CurrentTrain: epoch  9, batch    42 | loss: 4.3099575Losses:  3.970547676086426 0.05810634791851044 0.2715393900871277
CurrentTrain: epoch  9, batch    43 | loss: 4.3001933Losses:  3.996753692626953 0.07040965557098389 0.251554399728775
CurrentTrain: epoch  9, batch    44 | loss: 4.3187180Losses:  3.9627091884613037 0.11707708984613419 0.27387282252311707
CurrentTrain: epoch  9, batch    45 | loss: 4.3536592Losses:  3.9568371772766113 0.07173354923725128 0.26599767804145813
CurrentTrain: epoch  9, batch    46 | loss: 4.2945685Losses:  4.011849403381348 0.09298065304756165 0.2843782603740692
CurrentTrain: epoch  9, batch    47 | loss: 4.3892083Losses:  3.9924142360687256 0.09571385383605957 0.26999253034591675
CurrentTrain: epoch  9, batch    48 | loss: 4.3581204Losses:  4.045255661010742 0.16527338325977325 0.24890905618667603
CurrentTrain: epoch  9, batch    49 | loss: 4.4594378Losses:  4.021726608276367 0.14206397533416748 0.25427743792533875
CurrentTrain: epoch  9, batch    50 | loss: 4.4180679Losses:  4.01492977142334 0.11630921065807343 0.26255476474761963
CurrentTrain: epoch  9, batch    51 | loss: 4.3937936Losses:  3.985173225402832 0.07309897243976593 0.26870062947273254
CurrentTrain: epoch  9, batch    52 | loss: 4.3269730Losses:  3.9588661193847656 0.1306467354297638 0.27418699860572815
CurrentTrain: epoch  9, batch    53 | loss: 4.3636999Losses:  3.9861087799072266 0.11471083760261536 0.2543526291847229
CurrentTrain: epoch  9, batch    54 | loss: 4.3551722Losses:  3.9831972122192383 0.07508432865142822 0.24988819658756256
CurrentTrain: epoch  9, batch    55 | loss: 4.3081698Losses:  3.9431509971618652 0.1141887903213501 0.27506494522094727
CurrentTrain: epoch  9, batch    56 | loss: 4.3324046Losses:  3.9834718704223633 0.15637485682964325 0.24902145564556122
CurrentTrain: epoch  9, batch    57 | loss: 4.3888683Losses:  3.9571752548217773 0.07835293561220169 0.26803135871887207
CurrentTrain: epoch  9, batch    58 | loss: 4.3035593Losses:  3.953571081161499 0.1286822408437729 0.2612198293209076
CurrentTrain: epoch  9, batch    59 | loss: 4.3434734Losses:  3.987635612487793 0.11483711004257202 0.2510678768157959
CurrentTrain: epoch  9, batch    60 | loss: 4.3535404Losses:  3.9865670204162598 0.10361598432064056 0.2837742567062378
CurrentTrain: epoch  9, batch    61 | loss: 4.3739572Losses:  3.975890874862671 0.04278503358364105 0.2665497362613678
CurrentTrain: epoch  9, batch    62 | loss: 4.2852254
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.83%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.98%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.25%   
cur_acc:  ['0.9425']
his_acc:  ['0.9425']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  7.286175727844238 1.4143257141113281 0.5774798393249512
CurrentTrain: epoch  0, batch     0 | loss: 9.2779808Losses:  6.898527145385742 1.1968860626220703 0.653249204158783
CurrentTrain: epoch  0, batch     1 | loss: 8.7486620Losses:  7.256796836853027 1.428987741470337 0.6579432487487793
CurrentTrain: epoch  0, batch     2 | loss: 9.3437271Losses:  10.526520729064941 2.0861628513557662e-07 0.8985718488693237
CurrentTrain: epoch  0, batch     3 | loss: 11.4250927Losses:  6.9989471435546875 1.166380524635315 0.6431558132171631
CurrentTrain: epoch  1, batch     0 | loss: 8.8084841Losses:  6.598505973815918 1.2656279802322388 0.65883868932724
CurrentTrain: epoch  1, batch     1 | loss: 8.5229721Losses:  5.993201732635498 1.2637685537338257 0.6031606793403625
CurrentTrain: epoch  1, batch     2 | loss: 7.8601313Losses:  3.7655794620513916 0.33502888679504395 0.6874082088470459
CurrentTrain: epoch  1, batch     3 | loss: 4.7880163Losses:  5.783875465393066 1.0165269374847412 0.6359236836433411
CurrentTrain: epoch  2, batch     0 | loss: 7.4363265Losses:  4.934927940368652 1.1230268478393555 0.6592485904693604
CurrentTrain: epoch  2, batch     1 | loss: 6.7172031Losses:  4.611732006072998 0.936569094657898 0.5972678661346436
CurrentTrain: epoch  2, batch     2 | loss: 6.1455688Losses:  5.737277030944824 0.021566303446888924 0.5820838809013367
CurrentTrain: epoch  2, batch     3 | loss: 6.3409271Losses:  5.522770881652832 1.1320782899856567 0.6050933003425598
CurrentTrain: epoch  3, batch     0 | loss: 7.2599425Losses:  4.12155818939209 0.8644582033157349 0.6433129906654358
CurrentTrain: epoch  3, batch     1 | loss: 5.6293292Losses:  4.469577789306641 1.17252516746521 0.6430469751358032
CurrentTrain: epoch  3, batch     2 | loss: 6.2851501Losses:  2.786921977996826 0.36257433891296387 0.5773738622665405
CurrentTrain: epoch  3, batch     3 | loss: 3.7268701Losses:  3.846825122833252 0.8875440955162048 0.6042364835739136
CurrentTrain: epoch  4, batch     0 | loss: 5.3386059Losses:  3.6459240913391113 0.7610945701599121 0.6271451711654663
CurrentTrain: epoch  4, batch     1 | loss: 5.0341640Losses:  4.882004261016846 1.0307626724243164 0.6733331680297852
CurrentTrain: epoch  4, batch     2 | loss: 6.5861001Losses:  4.21653938293457 0.02041139453649521 0.579054057598114
CurrentTrain: epoch  4, batch     3 | loss: 4.8160048Losses:  4.537472724914551 0.8522312641143799 0.64841628074646
CurrentTrain: epoch  5, batch     0 | loss: 6.0381203Losses:  3.33774471282959 0.8402793407440186 0.6775230169296265
CurrentTrain: epoch  5, batch     1 | loss: 4.8555474Losses:  3.3420755863189697 1.0827717781066895 0.6633726358413696
CurrentTrain: epoch  5, batch     2 | loss: 5.0882201Losses:  6.717128753662109 0.017539115622639656 0.567322850227356
CurrentTrain: epoch  5, batch     3 | loss: 7.3019905Losses:  3.421050786972046 0.8923167586326599 0.6224425435066223
CurrentTrain: epoch  6, batch     0 | loss: 4.9358101Losses:  3.6587605476379395 0.8879284858703613 0.6523497104644775
CurrentTrain: epoch  6, batch     1 | loss: 5.1990385Losses:  3.7699952125549316 0.9289201498031616 0.7072532773017883
CurrentTrain: epoch  6, batch     2 | loss: 5.4061689Losses:  2.932774066925049 1.1920930376163597e-07 0.8663135170936584
CurrentTrain: epoch  6, batch     3 | loss: 3.7990878Losses:  3.4734246730804443 0.8613463640213013 0.6168131828308105
CurrentTrain: epoch  7, batch     0 | loss: 4.9515843Losses:  3.381880760192871 0.9914697408676147 0.6263980269432068
CurrentTrain: epoch  7, batch     1 | loss: 4.9997487Losses:  3.108130931854248 0.8859472870826721 0.6859772801399231
CurrentTrain: epoch  7, batch     2 | loss: 4.6800556Losses:  2.246089458465576 8.94069742685133e-08 0.6449652910232544
CurrentTrain: epoch  7, batch     3 | loss: 2.8910546Losses:  3.1434364318847656 0.872981071472168 0.6506812572479248
CurrentTrain: epoch  8, batch     0 | loss: 4.6670990Losses:  2.7656760215759277 0.7142379283905029 0.6124693155288696
CurrentTrain: epoch  8, batch     1 | loss: 4.0923834Losses:  2.8425559997558594 0.8787018060684204 0.644174337387085
CurrentTrain: epoch  8, batch     2 | loss: 4.3654318Losses:  2.2585864067077637 0.050385236740112305 0.5330830812454224
CurrentTrain: epoch  8, batch     3 | loss: 2.8420548Losses:  2.640165328979492 0.9271963834762573 0.7033207416534424
CurrentTrain: epoch  9, batch     0 | loss: 4.2706823Losses:  3.015073537826538 0.658771276473999 0.5862838625907898
CurrentTrain: epoch  9, batch     1 | loss: 4.2601285Losses:  2.2936253547668457 0.7767198085784912 0.6349422931671143
CurrentTrain: epoch  9, batch     2 | loss: 3.7052875Losses:  2.2118606567382812 0.11315038800239563 0.6727514266967773
CurrentTrain: epoch  9, batch     3 | loss: 2.9977624
Losses:  2.523423194885254 1.0051451921463013 0.46868032217025757
MemoryTrain:  epoch  0, batch     0 | loss: 3.9972486Losses:  1.1592847108840942 0.06886005401611328 0.46606573462486267
MemoryTrain:  epoch  0, batch     1 | loss: 1.6942105Losses:  2.1038084030151367 0.8568037748336792 0.47277358174324036
MemoryTrain:  epoch  1, batch     0 | loss: 3.4333858Losses:  1.5879331827163696 0.5351913571357727 0.5014763474464417
MemoryTrain:  epoch  1, batch     1 | loss: 2.6246009Losses:  1.5422364473342896 0.841929018497467 0.48115137219429016
MemoryTrain:  epoch  2, batch     0 | loss: 2.8653169Losses:  0.40370067954063416 0.2351573407649994 0.5567739009857178
MemoryTrain:  epoch  2, batch     1 | loss: 1.1956320Losses:  0.7227073907852173 0.745556116104126 0.4638698399066925
MemoryTrain:  epoch  3, batch     0 | loss: 1.9321333Losses:  1.857662558555603 0.35886743664741516 0.5578131675720215
MemoryTrain:  epoch  3, batch     1 | loss: 2.7743433Losses:  0.9504980444908142 0.9001635909080505 0.5508381128311157
MemoryTrain:  epoch  4, batch     0 | loss: 2.4014997Losses:  0.02825980633497238 0.2331603467464447 0.36314353346824646
MemoryTrain:  epoch  4, batch     1 | loss: 0.6245637Losses:  0.8077710866928101 0.8433467149734497 0.503778338432312
MemoryTrain:  epoch  5, batch     0 | loss: 2.1548963Losses:  0.14123958349227905 0.08085518330335617 0.44437408447265625
MemoryTrain:  epoch  5, batch     1 | loss: 0.6664689Losses:  0.7025560140609741 0.8031858801841736 0.47889572381973267
MemoryTrain:  epoch  6, batch     0 | loss: 1.9846375Losses:  0.047669440507888794 0.22163580358028412 0.5407190322875977
MemoryTrain:  epoch  6, batch     1 | loss: 0.8100243Losses:  0.41016796231269836 0.7934976816177368 0.5065057873725891
MemoryTrain:  epoch  7, batch     0 | loss: 1.7101715Losses:  1.3875607252120972 0.15292489528656006 0.4150729775428772
MemoryTrain:  epoch  7, batch     1 | loss: 1.9555585Losses:  0.545957088470459 0.7717471122741699 0.498839795589447
MemoryTrain:  epoch  8, batch     0 | loss: 1.8165441Losses:  0.02657807618379593 0.29068005084991455 0.5251487493515015
MemoryTrain:  epoch  8, batch     1 | loss: 0.8424069Losses:  0.550518274307251 0.7748353481292725 0.5004923343658447
MemoryTrain:  epoch  9, batch     0 | loss: 1.8258460Losses:  0.8340999484062195 0.08801402151584625 0.43636786937713623
MemoryTrain:  epoch  9, batch     1 | loss: 1.3584819
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 71.18%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 70.10%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 65.14%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 63.99%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 62.77%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 61.85%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 60.84%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 60.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 61.66%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 63.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.77%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.54%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 93.24%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.15%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.16%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 93.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 93.32%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.24%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 93.07%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.01%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.86%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 92.39%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 92.01%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.64%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 91.39%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 90.89%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 90.48%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 90.07%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 89.90%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 89.58%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 89.20%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.97%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 88.47%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 88.11%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 87.57%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 87.03%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 86.58%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 86.07%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 85.76%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 85.27%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 84.97%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 84.38%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 83.97%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 83.58%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 83.01%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 82.69%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 82.44%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 82.13%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 81.72%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 81.08%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 80.45%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 79.83%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.34%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.85%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.65%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.97%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.40%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.75%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.25%   
cur_acc:  ['0.9425', '0.6677']
his_acc:  ['0.9425', '0.8025']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  7.23210334777832 1.6332736015319824 0.7283932566642761
CurrentTrain: epoch  0, batch     0 | loss: 9.5937710Losses:  6.251345157623291 1.5944616794586182 0.7668344378471375
CurrentTrain: epoch  0, batch     1 | loss: 8.6126413Losses:  6.729242324829102 1.830124855041504 0.8114925622940063
CurrentTrain: epoch  0, batch     2 | loss: 9.3708601Losses:  8.15958023071289 0.4319261312484741 0.7792884111404419
CurrentTrain: epoch  0, batch     3 | loss: 9.3707943Losses:  5.386255741119385 1.5632466077804565 0.7377976179122925
CurrentTrain: epoch  1, batch     0 | loss: 7.6873002Losses:  6.497217178344727 1.6331406831741333 0.7591364979743958
CurrentTrain: epoch  1, batch     1 | loss: 8.8894939Losses:  4.91554069519043 1.489553451538086 0.7476930022239685
CurrentTrain: epoch  1, batch     2 | loss: 7.1527872Losses:  5.863844871520996 0.3231905996799469 0.6171392798423767
CurrentTrain: epoch  1, batch     3 | loss: 6.8041749Losses:  4.438382148742676 1.1262004375457764 0.7626611590385437
CurrentTrain: epoch  2, batch     0 | loss: 6.3272438Losses:  4.9394450187683105 1.4750652313232422 0.7159025073051453
CurrentTrain: epoch  2, batch     1 | loss: 7.1304126Losses:  5.968428134918213 1.6335384845733643 0.7224019765853882
CurrentTrain: epoch  2, batch     2 | loss: 8.3243685Losses:  2.5667314529418945 0.09613613784313202 0.7154949903488159
CurrentTrain: epoch  2, batch     3 | loss: 3.3783627Losses:  4.796627998352051 1.3825666904449463 0.731804370880127
CurrentTrain: epoch  3, batch     0 | loss: 6.9109988Losses:  4.275774955749512 1.1716337203979492 0.6981225609779358
CurrentTrain: epoch  3, batch     1 | loss: 6.1455312#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.027637481689453 1.8446838855743408 0.993945837020874
CurrentTrain: epoch  0, batch     0 | loss: 12.3692942Losses:  9.964725494384766 2.1947412490844727 1.0028235912322998
CurrentTrain: epoch  0, batch     1 | loss: 12.6608782Losses:  10.061779022216797 1.7269822359085083 0.9862359166145325
CurrentTrain: epoch  0, batch     2 | loss: 12.2818794Losses:  10.003937721252441 1.7333552837371826 0.9978234767913818
CurrentTrain: epoch  0, batch     3 | loss: 12.2362051Losses:  9.243915557861328 1.6074559688568115 0.9957940578460693
CurrentTrain: epoch  0, batch     4 | loss: 11.3492689Losses:  10.405978202819824 1.5617876052856445 0.9906721115112305
CurrentTrain: epoch  0, batch     5 | loss: 12.4631023Losses:  9.537890434265137 1.8096911907196045 0.9735417366027832
CurrentTrain: epoch  0, batch     6 | loss: 11.8343525Losses:  8.967214584350586 1.4989781379699707 0.9890275001525879
CurrentTrain: epoch  0, batch     7 | loss: 10.9607058Losses:  9.131591796875 1.528984785079956 0.9792283773422241
CurrentTrain: epoch  0, batch     8 | loss: 11.1501913Losses:  9.711952209472656 1.7486108541488647 0.9926708936691284
CurrentTrain: epoch  0, batch     9 | loss: 11.9568977Losses:  9.00284481048584 1.4582654237747192 0.9895837306976318
CurrentTrain: epoch  0, batch    10 | loss: 10.9559021Losses:  8.470197677612305 1.3161513805389404 0.9707358479499817
CurrentTrain: epoch  0, batch    11 | loss: 10.2717171Losses:  9.149148941040039 1.5646827220916748 0.9835063219070435
CurrentTrain: epoch  0, batch    12 | loss: 11.2055855Losses:  9.028322219848633 1.4840339422225952 0.975482165813446
CurrentTrain: epoch  0, batch    13 | loss: 11.0000973Losses:  8.188961029052734 1.5554008483886719 0.9768858551979065
CurrentTrain: epoch  0, batch    14 | loss: 10.2328053Losses:  8.844189643859863 1.7206522226333618 0.9747852683067322
CurrentTrain: epoch  0, batch    15 | loss: 11.0522346Losses:  9.297234535217285 1.3307045698165894 0.9739718437194824
CurrentTrain: epoch  0, batch    16 | loss: 11.1149254Losses:  9.097733497619629 1.7395302057266235 0.9763646125793457
CurrentTrain: epoch  0, batch    17 | loss: 11.3254461Losses:  8.978212356567383 1.4128742218017578 0.9746174812316895
CurrentTrain: epoch  0, batch    18 | loss: 10.8783951Losses:  8.712364196777344 1.6998682022094727 0.9862276911735535
CurrentTrain: epoch  0, batch    19 | loss: 10.9053459Losses:  8.102032661437988 1.3264224529266357 0.9559445381164551
CurrentTrain: epoch  0, batch    20 | loss: 9.9064274Losses:  8.222564697265625 1.4926493167877197 0.9717939496040344
CurrentTrain: epoch  0, batch    21 | loss: 10.2011108Losses:  8.749086380004883 1.4615800380706787 0.9604471921920776
CurrentTrain: epoch  0, batch    22 | loss: 10.6908903Losses:  8.13443374633789 1.1936135292053223 0.9410363435745239
CurrentTrain: epoch  0, batch    23 | loss: 9.7985649Losses:  7.831385612487793 1.4391682147979736 0.9563221335411072
CurrentTrain: epoch  0, batch    24 | loss: 9.7487144Losses:  8.036055564880371 1.218957781791687 0.9660471677780151
CurrentTrain: epoch  0, batch    25 | loss: 9.7380371Losses:  8.253091812133789 1.2293071746826172 0.9664218425750732
CurrentTrain: epoch  0, batch    26 | loss: 9.9656096Losses:  7.698612213134766 1.3949360847473145 0.9696030616760254
CurrentTrain: epoch  0, batch    27 | loss: 9.5783491Losses:  8.223679542541504 1.2817293405532837 0.9628804922103882
CurrentTrain: epoch  0, batch    28 | loss: 9.9868498Losses:  8.079949378967285 1.2866432666778564 0.9409960508346558
CurrentTrain: epoch  0, batch    29 | loss: 9.8370905Losses:  8.570510864257812 1.4909683465957642 0.9672616124153137
CurrentTrain: epoch  0, batch    30 | loss: 10.5451107Losses:  8.00694465637207 1.0843098163604736 0.9413934946060181
CurrentTrain: epoch  0, batch    31 | loss: 9.5619507Losses:  8.039913177490234 1.0363123416900635 0.9712473750114441
CurrentTrain: epoch  0, batch    32 | loss: 9.5618486Losses:  7.512750625610352 0.9890937805175781 0.9666740894317627
CurrentTrain: epoch  0, batch    33 | loss: 8.9851818Losses:  7.8970465660095215 1.103309154510498 0.9290941953659058
CurrentTrain: epoch  0, batch    34 | loss: 9.4649029Losses:  7.8607354164123535 1.1264780759811401 0.966656506061554
CurrentTrain: epoch  0, batch    35 | loss: 9.4705410Losses:  7.751188278198242 1.361135482788086 0.9596283435821533
CurrentTrain: epoch  0, batch    36 | loss: 9.5921383Losses:  7.819258689880371 1.140519380569458 0.9405050873756409
CurrentTrain: epoch  0, batch    37 | loss: 9.4300308Losses:  8.499539375305176 1.3722872734069824 0.9693797826766968
CurrentTrain: epoch  0, batch    38 | loss: 10.3565159Losses:  8.140396118164062 1.2613914012908936 0.949826717376709
CurrentTrain: epoch  0, batch    39 | loss: 9.8767014Losses:  8.505571365356445 1.2818896770477295 0.9686025977134705
CurrentTrain: epoch  0, batch    40 | loss: 10.2717628Losses:  7.460101127624512 1.1278456449508667 0.9656335115432739
CurrentTrain: epoch  0, batch    41 | loss: 9.0707636Losses:  7.316168785095215 1.1577657461166382 0.9460036158561707
CurrentTrain: epoch  0, batch    42 | loss: 8.9469357Losses:  7.036617279052734 0.8766752481460571 0.9493658542633057
CurrentTrain: epoch  0, batch    43 | loss: 8.3879757Losses:  7.430561065673828 1.052514672279358 0.9475241899490356
CurrentTrain: epoch  0, batch    44 | loss: 8.9568386Losses:  7.373027801513672 1.207373023033142 0.9541827440261841
CurrentTrain: epoch  0, batch    45 | loss: 9.0574923Losses:  8.178058624267578 0.9650403261184692 0.962702751159668
CurrentTrain: epoch  0, batch    46 | loss: 9.6244507Losses:  6.694906234741211 1.1678682565689087 0.9487249255180359
CurrentTrain: epoch  0, batch    47 | loss: 8.3371372Losses:  7.683621406555176 1.3601899147033691 0.9485692977905273
CurrentTrain: epoch  0, batch    48 | loss: 9.5180969Losses:  7.30507230758667 1.2446434497833252 0.9507101774215698
CurrentTrain: epoch  0, batch    49 | loss: 9.0250711Losses:  7.6659836769104 1.1927011013031006 0.9410600662231445
CurrentTrain: epoch  0, batch    50 | loss: 9.3292141Losses:  7.228784561157227 1.1572859287261963 0.9435508251190186
CurrentTrain: epoch  0, batch    51 | loss: 8.8578453Losses:  7.055690288543701 0.9650509357452393 0.9239088296890259
CurrentTrain: epoch  0, batch    52 | loss: 8.4826956Losses:  7.192636013031006 1.179776906967163 0.9490771889686584
CurrentTrain: epoch  0, batch    53 | loss: 8.8469515Losses:  7.777133941650391 1.3653303384780884 0.9631079435348511
CurrentTrain: epoch  0, batch    54 | loss: 9.6240187Losses:  6.918211936950684 1.0134413242340088 0.9580327272415161
CurrentTrain: epoch  0, batch    55 | loss: 8.4106693Losses:  6.87558650970459 0.920504093170166 0.8979831337928772
CurrentTrain: epoch  0, batch    56 | loss: 8.2450819Losses:  7.557569980621338 1.0737550258636475 0.9605778455734253
CurrentTrain: epoch  0, batch    57 | loss: 9.1116133Losses:  7.516936302185059 0.8688486814498901 0.9291656017303467
CurrentTrain: epoch  0, batch    58 | loss: 8.8503675Losses:  7.1002044677734375 0.8020650744438171 0.9402952194213867
CurrentTrain: epoch  0, batch    59 | loss: 8.3724174Losses:  6.633211135864258 0.915118396282196 0.9212297797203064
CurrentTrain: epoch  0, batch    60 | loss: 8.0089445Losses:  7.602592468261719 0.7496799230575562 0.9271071553230286
CurrentTrain: epoch  0, batch    61 | loss: 8.8158255Losses:  9.150208473205566 0.7868963479995728 0.9551596641540527
CurrentTrain: epoch  0, batch    62 | loss: 10.4146852Losses:  7.476035118103027 0.9267264604568481 0.9440174102783203
CurrentTrain: epoch  1, batch     0 | loss: 8.8747702Losses:  7.0339508056640625 0.8062391877174377 0.9279231429100037
CurrentTrain: epoch  1, batch     1 | loss: 8.3041515Losses:  6.792009353637695 1.0220446586608887 0.9290876388549805
CurrentTrain: epoch  1, batch     2 | loss: 8.2785978Losses:  7.142288684844971 0.9534267783164978 0.9318234920501709
CurrentTrain: epoch  1, batch     3 | loss: 8.5616274Losses:  7.368525505065918 1.0024231672286987 0.9279212355613708
CurrentTrain: epoch  1, batch     4 | loss: 8.8349094Losses:  6.935711860656738 0.7751153707504272 0.9287866353988647
CurrentTrain: epoch  1, batch     5 | loss: 8.1752205Losses:  6.8638505935668945 0.8933089971542358 0.9156326055526733
CurrentTrain: epoch  1, batch     6 | loss: 8.2149763Losses:  6.270302772521973 0.8702315092086792 0.9182717800140381
CurrentTrain: epoch  1, batch     7 | loss: 7.5996704Losses:  6.450139045715332 0.7086125612258911 0.9049821496009827
CurrentTrain: epoch  1, batch     8 | loss: 7.6112428Losses:  7.106417179107666 0.993851363658905 0.9378624558448792
CurrentTrain: epoch  1, batch     9 | loss: 8.5691996Losses:  7.025550842285156 0.8936896324157715 0.9494768381118774
CurrentTrain: epoch  1, batch    10 | loss: 8.3939791Losses:  6.620182037353516 1.0815908908843994 0.9396896958351135
CurrentTrain: epoch  1, batch    11 | loss: 8.1716175Losses:  6.902884483337402 0.964529275894165 0.937874436378479
CurrentTrain: epoch  1, batch    12 | loss: 8.3363504Losses:  7.222836971282959 0.7119815349578857 0.9399446249008179
CurrentTrain: epoch  1, batch    13 | loss: 8.4047909Losses:  6.258025169372559 0.8399545550346375 0.915784478187561
CurrentTrain: epoch  1, batch    14 | loss: 7.5558720Losses:  7.779256343841553 0.8788232803344727 0.9575262069702148
CurrentTrain: epoch  1, batch    15 | loss: 9.1368427Losses:  6.6443586349487305 0.7871299982070923 0.8951376080513
CurrentTrain: epoch  1, batch    16 | loss: 7.8790574Losses:  6.028813362121582 0.5797302722930908 0.9163040518760681
CurrentTrain: epoch  1, batch    17 | loss: 7.0666952Losses:  6.480945587158203 0.6774108409881592 0.9086569547653198
CurrentTrain: epoch  1, batch    18 | loss: 7.6126852Losses:  6.699506759643555 0.8133456707000732 0.9060904383659363
CurrentTrain: epoch  1, batch    19 | loss: 7.9658980Losses:  7.06708288192749 0.9191718101501465 0.9305991530418396
CurrentTrain: epoch  1, batch    20 | loss: 8.4515543Losses:  7.089842319488525 0.9421021342277527 0.928062379360199
CurrentTrain: epoch  1, batch    21 | loss: 8.4959755Losses:  6.708620071411133 0.8927921652793884 0.9100725650787354
CurrentTrain: epoch  1, batch    22 | loss: 8.0564489Losses:  6.890402793884277 0.6234678626060486 0.906602144241333
CurrentTrain: epoch  1, batch    23 | loss: 7.9671717Losses:  6.609273433685303 0.7941306829452515 0.8918498158454895
CurrentTrain: epoch  1, batch    24 | loss: 7.8493290Losses:  5.603211879730225 0.6847723722457886 0.8824784755706787
CurrentTrain: epoch  1, batch    25 | loss: 6.7292237Losses:  6.194411754608154 0.8731206655502319 0.9172019958496094
CurrentTrain: epoch  1, batch    26 | loss: 7.5261335Losses:  6.086644172668457 0.8495886325836182 0.9180516004562378
CurrentTrain: epoch  1, batch    27 | loss: 7.3952584Losses:  6.616288185119629 0.739091694355011 0.9099400043487549
CurrentTrain: epoch  1, batch    28 | loss: 7.8103499Losses:  6.000723838806152 0.8243415951728821 0.9091448187828064
CurrentTrain: epoch  1, batch    29 | loss: 7.2796378Losses:  6.469818592071533 0.5831167101860046 0.9258977770805359
CurrentTrain: epoch  1, batch    30 | loss: 7.5158839Losses:  5.668313980102539 0.8029995560646057 0.8961243629455566
CurrentTrain: epoch  1, batch    31 | loss: 6.9193754Losses:  5.6090989112854 0.7971020936965942 0.8807352185249329
CurrentTrain: epoch  1, batch    32 | loss: 6.8465686Losses:  5.998152732849121 0.565069317817688 0.8985607624053955
CurrentTrain: epoch  1, batch    33 | loss: 7.0125022Losses:  6.680883884429932 0.8933325409889221 0.9169420599937439
CurrentTrain: epoch  1, batch    34 | loss: 8.0326872Losses:  5.65573787689209 0.7608540058135986 0.8791807293891907
CurrentTrain: epoch  1, batch    35 | loss: 6.8561821Losses:  6.979945182800293 0.5738259553909302 0.9518510699272156
CurrentTrain: epoch  1, batch    36 | loss: 8.0296965Losses:  6.619627952575684 0.8535832166671753 0.9335405826568604
CurrentTrain: epoch  1, batch    37 | loss: 7.9399815Losses:  7.047601699829102 0.8949538469314575 0.9105895757675171
CurrentTrain: epoch  1, batch    38 | loss: 8.3978500Losses:  5.660762310028076 0.6311131715774536 0.9027588367462158
CurrentTrain: epoch  1, batch    39 | loss: 6.7432547Losses:  6.6008453369140625 0.5984848737716675 0.9036197066307068
CurrentTrain: epoch  1, batch    40 | loss: 7.6511402Losses:  6.904239654541016 0.9624282717704773 0.9220444560050964
CurrentTrain: epoch  1, batch    41 | loss: 8.3276901Losses:  7.150447368621826 0.7877727150917053 0.8860453963279724
CurrentTrain: epoch  1, batch    42 | loss: 8.3812428Losses:  7.324671745300293 0.8761587738990784 0.9165506362915039
CurrentTrain: epoch  1, batch    43 | loss: 8.6591053Losses:  7.400862216949463 0.700670063495636 0.942503035068512
CurrentTrain: epoch  1, batch    44 | loss: 8.5727835Losses:  5.941607475280762 0.6881608366966248 0.9060944318771362
CurrentTrain: epoch  1, batch    45 | loss: 7.0828156Losses:  5.491157531738281 0.7317417860031128 0.8912227153778076
CurrentTrain: epoch  1, batch    46 | loss: 6.6685109Losses:  6.474820137023926 0.8689975738525391 0.8960058689117432
CurrentTrain: epoch  1, batch    47 | loss: 7.7918205Losses:  6.317039966583252 0.8018733263015747 0.9055927991867065
CurrentTrain: epoch  1, batch    48 | loss: 7.5717096Losses:  5.840490818023682 0.6813404560089111 0.9182101488113403
CurrentTrain: epoch  1, batch    49 | loss: 6.9809365Losses:  6.210055828094482 0.6434001922607422 0.8737671971321106
CurrentTrain: epoch  1, batch    50 | loss: 7.2903395Losses:  7.052611351013184 0.8775126338005066 0.9318219423294067
CurrentTrain: epoch  1, batch    51 | loss: 8.3960352Losses:  6.298609733581543 0.7130718231201172 0.9060050249099731
CurrentTrain: epoch  1, batch    52 | loss: 7.4646840Losses:  6.102541923522949 0.6306567192077637 0.9268475770950317
CurrentTrain: epoch  1, batch    53 | loss: 7.1966224Losses:  6.356836318969727 0.7476221919059753 0.8870711326599121
CurrentTrain: epoch  1, batch    54 | loss: 7.5479937Losses:  5.169668197631836 0.39859819412231445 0.9019010066986084
CurrentTrain: epoch  1, batch    55 | loss: 6.0192170Losses:  5.336940765380859 0.5494393110275269 0.9132674336433411
CurrentTrain: epoch  1, batch    56 | loss: 6.3430138Losses:  6.108943939208984 0.7217345833778381 0.9024530649185181
CurrentTrain: epoch  1, batch    57 | loss: 7.2819052Losses:  5.6104736328125 0.5364698767662048 0.8862521648406982
CurrentTrain: epoch  1, batch    58 | loss: 6.5900698Losses:  6.664895057678223 0.8579812049865723 0.9168559908866882
CurrentTrain: epoch  1, batch    59 | loss: 7.9813042Losses:  6.250033378601074 0.8137304782867432 0.8915761709213257
CurrentTrain: epoch  1, batch    60 | loss: 7.5095515Losses:  5.581831932067871 0.7130081653594971 0.914024829864502
CurrentTrain: epoch  1, batch    61 | loss: 6.7518520Losses:  4.853222846984863 0.27081847190856934 0.8526018261909485
CurrentTrain: epoch  1, batch    62 | loss: 5.5503426Losses:  5.91912841796875 0.5753778219223022 0.8778977394104004
CurrentTrain: epoch  2, batch     0 | loss: 6.9334555Losses:  5.812858581542969 0.5411378741264343 0.9128018021583557
CurrentTrain: epoch  2, batch     1 | loss: 6.8103971Losses:  6.120789051055908 0.6661427021026611 0.9056695699691772
CurrentTrain: epoch  2, batch     2 | loss: 7.2397666Losses:  5.2218828201293945 0.6091837882995605 0.8535737991333008
CurrentTrain: epoch  2, batch     3 | loss: 6.2578535Losses:  5.742202281951904 0.548548698425293 0.8762575387954712
CurrentTrain: epoch  2, batch     4 | loss: 6.7288799Losses:  5.408384323120117 0.6700578927993774 0.8672051429748535
CurrentTrain: epoch  2, batch     5 | loss: 6.5120449Losses:  5.549321174621582 0.46765103936195374 0.9033477902412415
CurrentTrain: epoch  2, batch     6 | loss: 6.4686460Losses:  6.0302934646606445 0.5598502159118652 0.9078550934791565
CurrentTrain: epoch  2, batch     7 | loss: 7.0440712Losses:  6.637285232543945 0.5741546154022217 0.8858035802841187
CurrentTrain: epoch  2, batch     8 | loss: 7.6543417Losses:  5.244818687438965 0.41155606508255005 0.9126072525978088
CurrentTrain: epoch  2, batch     9 | loss: 6.1126785Losses:  5.219864845275879 0.5806453227996826 0.8628783226013184
CurrentTrain: epoch  2, batch    10 | loss: 6.2319498Losses:  5.889780044555664 0.5510100722312927 0.8846687078475952
CurrentTrain: epoch  2, batch    11 | loss: 6.8831244Losses:  6.15181827545166 0.5825198292732239 0.9439373016357422
CurrentTrain: epoch  2, batch    12 | loss: 7.2063069Losses:  5.82218599319458 0.4959314465522766 0.8926540017127991
CurrentTrain: epoch  2, batch    13 | loss: 6.7644448Losses:  5.635320663452148 0.5448353886604309 0.9066653847694397
CurrentTrain: epoch  2, batch    14 | loss: 6.6334891Losses:  5.392688751220703 0.5016288757324219 0.9134896397590637
CurrentTrain: epoch  2, batch    15 | loss: 6.3510623Losses:  4.997637748718262 0.4607938230037689 0.9120638370513916
CurrentTrain: epoch  2, batch    16 | loss: 5.9144635Losses:  4.771458625793457 0.4202699661254883 0.846176266670227
CurrentTrain: epoch  2, batch    17 | loss: 5.6148167Losses:  6.030320167541504 0.5307785272598267 0.8899593353271484
CurrentTrain: epoch  2, batch    18 | loss: 7.0060782Losses:  5.527865409851074 0.42968228459358215 0.9258066415786743
CurrentTrain: epoch  2, batch    19 | loss: 6.4204512Losses:  5.534613132476807 0.559604287147522 0.8736540079116821
CurrentTrain: epoch  2, batch    20 | loss: 6.5310445Losses:  5.248103618621826 0.420871376991272 0.8868066668510437
CurrentTrain: epoch  2, batch    21 | loss: 6.1123781Losses:  5.041821479797363 0.5154143571853638 0.890129804611206
CurrentTrain: epoch  2, batch    22 | loss: 6.0023007Losses:  5.675857067108154 0.462249755859375 0.8990631699562073
CurrentTrain: epoch  2, batch    23 | loss: 6.5876384Losses:  5.6444244384765625 0.629027247428894 0.8486077785491943
CurrentTrain: epoch  2, batch    24 | loss: 6.6977558Losses:  5.228950023651123 0.4332072138786316 0.8647346496582031
CurrentTrain: epoch  2, batch    25 | loss: 6.0945244Losses:  5.069774150848389 0.40599536895751953 0.8478599786758423
CurrentTrain: epoch  2, batch    26 | loss: 5.8996997Losses:  5.298949241638184 0.4217074513435364 0.8650102615356445
CurrentTrain: epoch  2, batch    27 | loss: 6.1531620Losses:  5.014443874359131 0.46086418628692627 0.8295713663101196
CurrentTrain: epoch  2, batch    28 | loss: 5.8900938Losses:  5.343397617340088 0.5332420468330383 0.8789098858833313
CurrentTrain: epoch  2, batch    29 | loss: 6.3160949Losses:  5.6339569091796875 0.6604797840118408 0.8640094995498657
CurrentTrain: epoch  2, batch    30 | loss: 6.7264414Losses:  5.177698135375977 0.5091433525085449 0.8834004998207092
CurrentTrain: epoch  2, batch    31 | loss: 6.1285419Losses:  6.401632308959961 0.5369908213615417 0.8680448532104492
CurrentTrain: epoch  2, batch    32 | loss: 7.3726454Losses:  4.902920246124268 0.5206964015960693 0.8591742515563965
CurrentTrain: epoch  2, batch    33 | loss: 5.8532038Losses:  5.5495758056640625 0.5789546966552734 0.8440176248550415
CurrentTrain: epoch  2, batch    34 | loss: 6.5505395Losses:  5.528275489807129 0.42809727787971497 0.8945068120956421
CurrentTrain: epoch  2, batch    35 | loss: 6.4036260Losses:  5.010618209838867 0.4340671896934509 0.8718706965446472
CurrentTrain: epoch  2, batch    36 | loss: 5.8806210Losses:  5.373321533203125 0.4215822219848633 0.8295798301696777
CurrentTrain: epoch  2, batch    37 | loss: 6.2096939Losses:  5.265875339508057 0.34579864144325256 0.8825075626373291
CurrentTrain: epoch  2, batch    38 | loss: 6.0529275Losses:  4.704074382781982 0.4149073660373688 0.8460323810577393
CurrentTrain: epoch  2, batch    39 | loss: 5.5419979Losses:  5.481816291809082 0.46927666664123535 0.8420150876045227
CurrentTrain: epoch  2, batch    40 | loss: 6.3721004Losses:  5.903252124786377 0.42239832878112793 0.8457998633384705
CurrentTrain: epoch  2, batch    41 | loss: 6.7485499Losses:  5.206770420074463 0.47757044434547424 0.8349460363388062
CurrentTrain: epoch  2, batch    42 | loss: 6.1018138Losses:  5.341837406158447 0.4469662308692932 0.8516339063644409
CurrentTrain: epoch  2, batch    43 | loss: 6.2146206Losses:  5.1763458251953125 0.508149266242981 0.8507381677627563
CurrentTrain: epoch  2, batch    44 | loss: 6.1098642Losses:  5.513789176940918 0.3990243673324585 0.8467428684234619
CurrentTrain: epoch  2, batch    45 | loss: 6.3361850Losses:  4.8932271003723145 0.3389459252357483 0.8862035870552063
CurrentTrain: epoch  2, batch    46 | loss: 5.6752748Losses:  5.893815040588379 0.5197067260742188 0.8800130486488342
CurrentTrain: epoch  2, batch    47 | loss: 6.8535285Losses:  4.789736747741699 0.3758375644683838 0.8450666666030884
CurrentTrain: epoch  2, batch    48 | loss: 5.5881076Losses:  5.354745864868164 0.5175763368606567 0.8715376853942871
CurrentTrain: epoch  2, batch    49 | loss: 6.3080912Losses:  5.31624698638916 0.30838677287101746 0.8792796730995178
CurrentTrain: epoch  2, batch    50 | loss: 6.0642738Losses:  4.832154273986816 0.38243234157562256 0.8551316261291504
CurrentTrain: epoch  2, batch    51 | loss: 5.6421528Losses:  5.204323768615723 0.311231791973114 0.9157041907310486
CurrentTrain: epoch  2, batch    52 | loss: 5.9734073Losses:  5.163358211517334 0.4457605481147766 0.8761833310127258
CurrentTrain: epoch  2, batch    53 | loss: 6.0472107Losses:  4.928062438964844 0.42129582166671753 0.8646661639213562
CurrentTrain: epoch  2, batch    54 | loss: 5.7816911Losses:  5.035388946533203 0.4219176173210144 0.8758533000946045
CurrentTrain: epoch  2, batch    55 | loss: 5.8952332Losses:  5.354072570800781 0.6025434136390686 0.850925087928772
CurrentTrain: epoch  2, batch    56 | loss: 6.3820786Losses:  5.160216331481934 0.4563429355621338 0.9165664315223694
CurrentTrain: epoch  2, batch    57 | loss: 6.0748425Losses:  4.939879417419434 0.3756250739097595 0.8661522269248962
CurrentTrain: epoch  2, batch    58 | loss: 5.7485805Losses:  4.95109748840332 0.3727405071258545 0.8542722463607788
CurrentTrain: epoch  2, batch    59 | loss: 5.7509742Losses:  4.688446521759033 0.4704657196998596 0.8161461353302002
CurrentTrain: epoch  2, batch    60 | loss: 5.5669851Losses:  4.908199310302734 0.43037816882133484 0.8685175180435181
CurrentTrain: epoch  2, batch    61 | loss: 5.7728362Losses:  5.008613586425781 0.1495666205883026 0.846733570098877
CurrentTrain: epoch  2, batch    62 | loss: 5.5815468Losses:  5.554469108581543 0.41216108202934265 0.8763478994369507
CurrentTrain: epoch  3, batch     0 | loss: 6.4048038Losses:  6.017951011657715 0.4801066517829895 0.9082134366035461
CurrentTrain: epoch  3, batch     1 | loss: 6.9521646Losses:  4.89179801940918 0.40365687012672424 0.845136284828186
CurrentTrain: epoch  3, batch     2 | loss: 5.7180233Losses:  5.469786167144775 0.3330751657485962 0.870500922203064
CurrentTrain: epoch  3, batch     3 | loss: 6.2381115Losses:  5.224125862121582 0.35955145955085754 0.8228929042816162
CurrentTrain: epoch  3, batch     4 | loss: 5.9951239Losses:  5.24755334854126 0.289803147315979 0.823321521282196
CurrentTrain: epoch  3, batch     5 | loss: 5.9490170Losses:  4.8843674659729 0.3902395963668823 0.8932552337646484
CurrentTrain: epoch  3, batch     6 | loss: 5.7212348Losses:  4.986108779907227 0.33880165219306946 0.8762367963790894
CurrentTrain: epoch  3, batch     7 | loss: 5.7630291Losses:  5.398615837097168 0.45709407329559326 0.8151315450668335
CurrentTrain: epoch  3, batch     8 | loss: 6.2632756Losses:  5.2548370361328125 0.39885658025741577 0.832634449005127
CurrentTrain: epoch  3, batch     9 | loss: 6.0700111Losses:  5.063071250915527 0.462046355009079 0.8498968482017517
CurrentTrain: epoch  3, batch    10 | loss: 5.9500656Losses:  5.036150932312012 0.36364611983299255 0.8405336141586304
CurrentTrain: epoch  3, batch    11 | loss: 5.8200636Losses:  4.546993255615234 0.26602667570114136 0.8399066925048828
CurrentTrain: epoch  3, batch    12 | loss: 5.2329731Losses:  4.975163459777832 0.507628321647644 0.8359159827232361
CurrentTrain: epoch  3, batch    13 | loss: 5.9007497Losses:  4.723026275634766 0.3700422942638397 0.8323761224746704
CurrentTrain: epoch  3, batch    14 | loss: 5.5092568Losses:  4.683032989501953 0.3838097155094147 0.8307611346244812
CurrentTrain: epoch  3, batch    15 | loss: 5.4822230Losses:  4.967768669128418 0.43346673250198364 0.8632486462593079
CurrentTrain: epoch  3, batch    16 | loss: 5.8328600Losses:  5.1788482666015625 0.3317658603191376 0.8771188855171204
CurrentTrain: epoch  3, batch    17 | loss: 5.9491735Losses:  5.124372959136963 0.3077012300491333 0.843977689743042
CurrentTrain: epoch  3, batch    18 | loss: 5.8540630Losses:  4.9640793800354 0.3899051547050476 0.8327305316925049
CurrentTrain: epoch  3, batch    19 | loss: 5.7703495Losses:  4.78113317489624 0.3166615664958954 0.8642498254776001
CurrentTrain: epoch  3, batch    20 | loss: 5.5299196Losses:  4.8097028732299805 0.3528609871864319 0.8665100336074829
CurrentTrain: epoch  3, batch    21 | loss: 5.5958190Losses:  4.842568397521973 0.43844082951545715 0.9036202430725098
CurrentTrain: epoch  3, batch    22 | loss: 5.7328196Losses:  4.674853324890137 0.40655219554901123 0.8316512107849121
CurrentTrain: epoch  3, batch    23 | loss: 5.4972315Losses:  4.708865165710449 0.24136455357074738 0.789064884185791
CurrentTrain: epoch  3, batch    24 | loss: 5.3447618Losses:  4.582390308380127 0.3621777296066284 0.8254880309104919
CurrentTrain: epoch  3, batch    25 | loss: 5.3573122Losses:  4.723695755004883 0.42423418164253235 0.8655728697776794
CurrentTrain: epoch  3, batch    26 | loss: 5.5807166Losses:  4.752509593963623 0.40113064646720886 0.863948404788971
CurrentTrain: epoch  3, batch    27 | loss: 5.5856147Losses:  4.862583160400391 0.40764498710632324 0.8546522855758667
CurrentTrain: epoch  3, batch    28 | loss: 5.6975546Losses:  5.160925388336182 0.37622106075286865 0.8491231799125671
CurrentTrain: epoch  3, batch    29 | loss: 5.9617081Losses:  5.398226737976074 0.37879854440689087 0.8719197511672974
CurrentTrain: epoch  3, batch    30 | loss: 6.2129850Losses:  4.392309188842773 0.282112181186676 0.8504409790039062
CurrentTrain: epoch  3, batch    31 | loss: 5.0996418Losses:  4.82317590713501 0.3084198236465454 0.8289017081260681
CurrentTrain: epoch  3, batch    32 | loss: 5.5460463Losses:  4.798223495483398 0.31188374757766724 0.793034017086029
CurrentTrain: epoch  3, batch    33 | loss: 5.5066242Losses:  4.532247543334961 0.36986830830574036 0.8037592172622681
CurrentTrain: epoch  3, batch    34 | loss: 5.3039956Losses:  4.571752548217773 0.3965081572532654 0.8235571980476379
CurrentTrain: epoch  3, batch    35 | loss: 5.3800392Losses:  5.488354682922363 0.5437754392623901 0.8478432297706604
CurrentTrain: epoch  3, batch    36 | loss: 6.4560518Losses:  4.6806535720825195 0.25437992811203003 0.8529015779495239
CurrentTrain: epoch  3, batch    37 | loss: 5.3614841Losses:  4.485062599182129 0.2775546908378601 0.8469996452331543
CurrentTrain: epoch  3, batch    38 | loss: 5.1861172Losses:  5.655350685119629 0.2985009551048279 0.9142417907714844
CurrentTrain: epoch  3, batch    39 | loss: 6.4109726Losses:  4.643318176269531 0.34119701385498047 0.8370150327682495
CurrentTrain: epoch  3, batch    40 | loss: 5.4030228Losses:  4.519634246826172 0.27174457907676697 0.7761406898498535
CurrentTrain: epoch  3, batch    41 | loss: 5.1794491Losses:  5.049130439758301 0.29635071754455566 0.8565822243690491
CurrentTrain: epoch  3, batch    42 | loss: 5.7737722Losses:  4.673400402069092 0.22968991100788116 0.802150547504425
CurrentTrain: epoch  3, batch    43 | loss: 5.3041658Losses:  4.906423568725586 0.4101789593696594 0.8526448011398315
CurrentTrain: epoch  3, batch    44 | loss: 5.7429252Losses:  5.108110427856445 0.26778218150138855 0.8627240061759949
CurrentTrain: epoch  3, batch    45 | loss: 5.8072548Losses:  4.413294792175293 0.2873893976211548 0.8657248616218567
CurrentTrain: epoch  3, batch    46 | loss: 5.1335464Losses:  4.72822904586792 0.2898198962211609 0.913108766078949
CurrentTrain: epoch  3, batch    47 | loss: 5.4746032Losses:  4.478489875793457 0.2649712562561035 0.8568261861801147
CurrentTrain: epoch  3, batch    48 | loss: 5.1718740Losses:  4.638193130493164 0.2960514724254608 0.8347765207290649
CurrentTrain: epoch  3, batch    49 | loss: 5.3516331Losses:  4.544206619262695 0.34482553601264954 0.8583236932754517
CurrentTrain: epoch  3, batch    50 | loss: 5.3181944Losses:  4.7442169189453125 0.2531840205192566 0.8905298709869385
CurrentTrain: epoch  3, batch    51 | loss: 5.4426656Losses:  4.502323150634766 0.3549814522266388 0.8412216901779175
CurrentTrain: epoch  3, batch    52 | loss: 5.2779155Losses:  5.143468379974365 0.35402804613113403 0.8378725647926331
CurrentTrain: epoch  3, batch    53 | loss: 5.9164329Losses:  4.526383399963379 0.3362833261489868 0.8035019636154175
CurrentTrain: epoch  3, batch    54 | loss: 5.2644176Losses:  4.6872406005859375 0.35724398493766785 0.8628334999084473
CurrentTrain: epoch  3, batch    55 | loss: 5.4759016Losses:  4.405270576477051 0.33597421646118164 0.8290828466415405
CurrentTrain: epoch  3, batch    56 | loss: 5.1557860Losses:  4.867990493774414 0.36207449436187744 0.8672487735748291
CurrentTrain: epoch  3, batch    57 | loss: 5.6636891Losses:  4.778500556945801 0.23811568319797516 0.8575550317764282
CurrentTrain: epoch  3, batch    58 | loss: 5.4453940Losses:  4.5998430252075195 0.21321678161621094 0.8870975971221924
CurrentTrain: epoch  3, batch    59 | loss: 5.2566085Losses:  4.56788444519043 0.38650810718536377 0.8196382522583008
CurrentTrain: epoch  3, batch    60 | loss: 5.3642116Losses:  4.432560443878174 0.24173381924629211 0.8501350283622742
CurrentTrain: epoch  3, batch    61 | loss: 5.0993619Losses:  4.4231276512146 0.08579017966985703 0.8021293878555298
CurrentTrain: epoch  3, batch    62 | loss: 4.9099827Losses:  4.356290817260742 0.25284820795059204 0.8001145124435425
CurrentTrain: epoch  4, batch     0 | loss: 5.0091963Losses:  4.4237446784973145 0.2709656059741974 0.8322187662124634
CurrentTrain: epoch  4, batch     1 | loss: 5.1108198Losses:  4.521964073181152 0.2274954617023468 0.8304529786109924
CurrentTrain: epoch  4, batch     2 | loss: 5.1646862Losses:  4.314570426940918 0.20940953493118286 0.8247978687286377
CurrentTrain: epoch  4, batch     3 | loss: 4.9363790Losses:  4.498559951782227 0.31456029415130615 0.862922728061676
CurrentTrain: epoch  4, batch     4 | loss: 5.2445817Losses:  4.615674018859863 0.32648298144340515 0.8370956778526306
CurrentTrain: epoch  4, batch     5 | loss: 5.3607044Losses:  4.509817123413086 0.2850072383880615 0.8048308491706848
CurrentTrain: epoch  4, batch     6 | loss: 5.1972399Losses:  4.384636878967285 0.23517253994941711 0.8287003040313721
CurrentTrain: epoch  4, batch     7 | loss: 5.0341597Losses:  4.375504493713379 0.2836846113204956 0.8462877869606018
CurrentTrain: epoch  4, batch     8 | loss: 5.0823331Losses:  4.439250469207764 0.2719666361808777 0.8506484627723694
CurrentTrain: epoch  4, batch     9 | loss: 5.1365414Losses:  4.490296363830566 0.19871768355369568 0.838266134262085
CurrentTrain: epoch  4, batch    10 | loss: 5.1081471Losses:  4.808506965637207 0.3787268400192261 0.860968291759491
CurrentTrain: epoch  4, batch    11 | loss: 5.6177182Losses:  4.43113374710083 0.2616903483867645 0.8852891325950623
CurrentTrain: epoch  4, batch    12 | loss: 5.1354685Losses:  4.589219570159912 0.3420252799987793 0.8113073706626892
CurrentTrain: epoch  4, batch    13 | loss: 5.3368983Losses:  4.212696075439453 0.1806815266609192 0.7795881032943726
CurrentTrain: epoch  4, batch    14 | loss: 4.7831717Losses:  4.405599117279053 0.2056051343679428 0.8103557825088501
CurrentTrain: epoch  4, batch    15 | loss: 5.0163822Losses:  4.451461315155029 0.29829809069633484 0.8280358910560608
CurrentTrain: epoch  4, batch    16 | loss: 5.1637774Losses:  4.351304054260254 0.15723933279514313 0.8821392059326172
CurrentTrain: epoch  4, batch    17 | loss: 4.9496131Losses:  4.425151348114014 0.2938570976257324 0.8187462091445923
CurrentTrain: epoch  4, batch    18 | loss: 5.1283817Losses:  4.405316352844238 0.34176287055015564 0.8167052268981934
CurrentTrain: epoch  4, batch    19 | loss: 5.1554317Losses:  4.927670478820801 0.3070217967033386 0.88289475440979
CurrentTrain: epoch  4, batch    20 | loss: 5.6761394Losses:  4.635167121887207 0.2134336531162262 0.8753609657287598
CurrentTrain: epoch  4, batch    21 | loss: 5.2862816Losses:  4.239264488220215 0.2804989218711853 0.782202959060669
CurrentTrain: epoch  4, batch    22 | loss: 4.9108648Losses:  4.225550651550293 0.10018190741539001 0.8568108081817627
CurrentTrain: epoch  4, batch    23 | loss: 4.7541380Losses:  4.594563007354736 0.2463752031326294 0.8044173717498779
CurrentTrain: epoch  4, batch    24 | loss: 5.2431469Losses:  4.40056848526001 0.26390576362609863 0.8408570885658264
CurrentTrain: epoch  4, batch    25 | loss: 5.0849032Losses:  4.2890400886535645 0.2572367787361145 0.7921675443649292
CurrentTrain: epoch  4, batch    26 | loss: 4.9423609Losses:  4.257599830627441 0.1672484278678894 0.8236875534057617
CurrentTrain: epoch  4, batch    27 | loss: 4.8366919Losses:  4.396625518798828 0.267069935798645 0.7882245182991028
CurrentTrain: epoch  4, batch    28 | loss: 5.0578074Losses:  4.365903377532959 0.19070354104042053 0.8154474496841431
CurrentTrain: epoch  4, batch    29 | loss: 4.9643307Losses:  4.241298198699951 0.15363569557666779 0.8281274437904358
CurrentTrain: epoch  4, batch    30 | loss: 4.8089976Losses:  4.417667865753174 0.24372676014900208 0.8232503533363342
CurrentTrain: epoch  4, batch    31 | loss: 5.0730200Losses:  4.717401504516602 0.3605029582977295 0.8119913339614868
CurrentTrain: epoch  4, batch    32 | loss: 5.4839005Losses:  4.423162460327148 0.3260602653026581 0.7923163771629333
CurrentTrain: epoch  4, batch    33 | loss: 5.1453810Losses:  4.323646545410156 0.15367639064788818 0.8748162388801575
CurrentTrain: epoch  4, batch    34 | loss: 4.9147310Losses:  4.748778343200684 0.2819661498069763 0.8465467691421509
CurrentTrain: epoch  4, batch    35 | loss: 5.4540181Losses:  4.39760684967041 0.27946537733078003 0.8332861661911011
CurrentTrain: epoch  4, batch    36 | loss: 5.0937152Losses:  4.631131649017334 0.3096265494823456 0.8305963277816772
CurrentTrain: epoch  4, batch    37 | loss: 5.3560562Losses:  4.328222274780273 0.19388322532176971 0.8803302049636841
CurrentTrain: epoch  4, batch    38 | loss: 4.9622707Losses:  4.29242467880249 0.27154767513275146 0.8154199123382568
CurrentTrain: epoch  4, batch    39 | loss: 4.9716825Losses:  4.253118515014648 0.17817829549312592 0.8285579681396484
CurrentTrain: epoch  4, batch    40 | loss: 4.8455758Losses:  4.201395034790039 0.2776035964488983 0.837040364742279
CurrentTrain: epoch  4, batch    41 | loss: 4.8975186Losses:  5.362738132476807 0.33695608377456665 0.8723236322402954
CurrentTrain: epoch  4, batch    42 | loss: 6.1358562Losses:  4.800559043884277 0.164466992020607 0.7512081861495972
CurrentTrain: epoch  4, batch    43 | loss: 5.3406301Losses:  4.174776077270508 0.17303751409053802 0.8592013120651245
CurrentTrain: epoch  4, batch    44 | loss: 4.7774143Losses:  4.645152568817139 0.25054749846458435 0.8099896907806396
CurrentTrain: epoch  4, batch    45 | loss: 5.3006949Losses:  5.02903413772583 0.22205042839050293 0.8046554327011108
CurrentTrain: epoch  4, batch    46 | loss: 5.6534119Losses:  4.848156452178955 0.26206889748573303 0.8376623392105103
CurrentTrain: epoch  4, batch    47 | loss: 5.5290565Losses:  4.472831726074219 0.24836963415145874 0.8005908727645874
CurrentTrain: epoch  4, batch    48 | loss: 5.1214967Losses:  4.298956871032715 0.20671775937080383 0.7836195826530457
CurrentTrain: epoch  4, batch    49 | loss: 4.8974848Losses:  4.713309288024902 0.33021292090415955 0.8023738861083984
CurrentTrain: epoch  4, batch    50 | loss: 5.4447093Losses:  4.350864410400391 0.1505831778049469 0.8023331165313721
CurrentTrain: epoch  4, batch    51 | loss: 4.9026141Losses:  4.2316484451293945 0.2604179084300995 0.8170974254608154
CurrentTrain: epoch  4, batch    52 | loss: 4.9006152Losses:  4.47573184967041 0.247999906539917 0.8104742765426636
CurrentTrain: epoch  4, batch    53 | loss: 5.1289692Losses:  4.876993179321289 0.2258913367986679 0.850914716720581
CurrentTrain: epoch  4, batch    54 | loss: 5.5283418Losses:  4.231998920440674 0.248163640499115 0.7699823975563049
CurrentTrain: epoch  4, batch    55 | loss: 4.8651538Losses:  4.5096845626831055 0.17758598923683167 0.834536075592041
CurrentTrain: epoch  4, batch    56 | loss: 5.1045389Losses:  4.380603790283203 0.255666583776474 0.8338392972946167
CurrentTrain: epoch  4, batch    57 | loss: 5.0531902Losses:  4.2286481857299805 0.17110876739025116 0.8260084390640259
CurrentTrain: epoch  4, batch    58 | loss: 4.8127613Losses:  4.275468826293945 0.17389923334121704 0.8323279619216919
CurrentTrain: epoch  4, batch    59 | loss: 4.8655319Losses:  4.223300933837891 0.21005436778068542 0.8584620952606201
CurrentTrain: epoch  4, batch    60 | loss: 4.8625865Losses:  4.497560024261475 0.20027567446231842 0.8336259126663208
CurrentTrain: epoch  4, batch    61 | loss: 5.1146488Losses:  4.24049186706543 0.10812968760728836 0.8889822959899902
CurrentTrain: epoch  4, batch    62 | loss: 4.7931128Losses:  4.369057655334473 0.2560998797416687 0.8428424000740051
CurrentTrain: epoch  5, batch     0 | loss: 5.0465784Losses:  4.302298545837402 0.2719261646270752 0.7737070918083191
CurrentTrain: epoch  5, batch     1 | loss: 4.9610782Losses:  4.247655868530273 0.15038467943668365 0.7919684052467346
CurrentTrain: epoch  5, batch     2 | loss: 4.7940249Losses:  4.202908039093018 0.17444917559623718 0.8400396108627319
CurrentTrain: epoch  5, batch     3 | loss: 4.7973766Losses:  4.342551231384277 0.17044702172279358 0.8520321249961853
CurrentTrain: epoch  5, batch     4 | loss: 4.9390140Losses:  4.499218940734863 0.22899296879768372 0.8107150197029114
CurrentTrain: epoch  5, batch     5 | loss: 5.1335692Losses:  4.345956802368164 0.21265694499015808 0.8379368782043457
CurrentTrain: epoch  5, batch     6 | loss: 4.9775820Losses:  4.208462715148926 0.2394481748342514 0.7900550365447998
CurrentTrain: epoch  5, batch     7 | loss: 4.8429384Losses:  4.29211950302124 0.21456468105316162 0.867140531539917
CurrentTrain: epoch  5, batch     8 | loss: 4.9402547Losses:  4.2174482345581055 0.18291664123535156 0.7540304660797119
CurrentTrain: epoch  5, batch     9 | loss: 4.7773800Losses:  4.552764892578125 0.33662673830986023 0.794080913066864
CurrentTrain: epoch  5, batch    10 | loss: 5.2864318Losses:  4.236377239227295 0.2070595771074295 0.8443824052810669
CurrentTrain: epoch  5, batch    11 | loss: 4.8656278Losses:  4.331525802612305 0.2370913028717041 0.761265754699707
CurrentTrain: epoch  5, batch    12 | loss: 4.9492497Losses:  4.211559295654297 0.19509534537792206 0.8312803506851196
CurrentTrain: epoch  5, batch    13 | loss: 4.8222952Losses:  4.134273529052734 0.24685271084308624 0.8151715397834778
CurrentTrain: epoch  5, batch    14 | loss: 4.7887120Losses:  4.271451950073242 0.1881042718887329 0.868357241153717
CurrentTrain: epoch  5, batch    15 | loss: 4.8937349Losses:  4.281157493591309 0.2389657348394394 0.8318817615509033
CurrentTrain: epoch  5, batch    16 | loss: 4.9360638Losses:  4.1529107093811035 0.23277528584003448 0.8135014176368713
CurrentTrain: epoch  5, batch    17 | loss: 4.7924366Losses:  4.307766914367676 0.26528286933898926 0.8792711496353149
CurrentTrain: epoch  5, batch    18 | loss: 5.0126853Losses:  4.238924980163574 0.15508422255516052 0.7855735421180725
CurrentTrain: epoch  5, batch    19 | loss: 4.7867961Losses:  4.162342071533203 0.18253126740455627 0.8149012923240662
CurrentTrain: epoch  5, batch    20 | loss: 4.7523241Losses:  4.4021782875061035 0.21734973788261414 0.8466227054595947
CurrentTrain: epoch  5, batch    21 | loss: 5.0428391Losses:  4.245933532714844 0.2029050588607788 0.7985934019088745
CurrentTrain: epoch  5, batch    22 | loss: 4.8481355Losses:  4.262255668640137 0.2077450007200241 0.8497565984725952
CurrentTrain: epoch  5, batch    23 | loss: 4.8948789Losses:  4.192456245422363 0.13024988770484924 0.7736170291900635
CurrentTrain: epoch  5, batch    24 | loss: 4.7095146Losses:  4.308379173278809 0.23393157124519348 0.8613393306732178
CurrentTrain: epoch  5, batch    25 | loss: 4.9729805Losses:  4.2530107498168945 0.2201838195323944 0.7958515882492065
CurrentTrain: epoch  5, batch    26 | loss: 4.8711205Losses:  4.179374694824219 0.2090505063533783 0.869134783744812
CurrentTrain: epoch  5, batch    27 | loss: 4.8229928Losses:  4.203775882720947 0.21948203444480896 0.8288437128067017
CurrentTrain: epoch  5, batch    28 | loss: 4.8376799Losses:  4.159578800201416 0.2389720380306244 0.8010442852973938
CurrentTrain: epoch  5, batch    29 | loss: 4.7990732Losses:  4.150912284851074 0.1304646134376526 0.8006699085235596
CurrentTrain: epoch  5, batch    30 | loss: 4.6817117Losses:  4.180746555328369 0.16240054368972778 0.7826934456825256
CurrentTrain: epoch  5, batch    31 | loss: 4.7344942Losses:  4.261022567749023 0.15165650844573975 0.8130612373352051
CurrentTrain: epoch  5, batch    32 | loss: 4.8192101Losses:  4.117679595947266 0.21734699606895447 0.7642509937286377
CurrentTrain: epoch  5, batch    33 | loss: 4.7171521Losses:  4.279757976531982 0.2146456241607666 0.7833143472671509
CurrentTrain: epoch  5, batch    34 | loss: 4.8860612Losses:  4.228283882141113 0.18486645817756653 0.8210046887397766
CurrentTrain: epoch  5, batch    35 | loss: 4.8236527Losses:  4.199804782867432 0.18586844205856323 0.7752605676651001
CurrentTrain: epoch  5, batch    36 | loss: 4.7733035Losses:  4.12746524810791 0.21230487525463104 0.7512446045875549
CurrentTrain: epoch  5, batch    37 | loss: 4.7153926Losses:  4.168509483337402 0.18625149130821228 0.8180294036865234
CurrentTrain: epoch  5, batch    38 | loss: 4.7637758Losses:  4.267823219299316 0.21483783423900604 0.7671960592269897
CurrentTrain: epoch  5, batch    39 | loss: 4.8662591Losses:  4.113628387451172 0.21381106972694397 0.7995033264160156
CurrentTrain: epoch  5, batch    40 | loss: 4.7271910Losses:  4.180700302124023 0.18581922352313995 0.7822785377502441
CurrentTrain: epoch  5, batch    41 | loss: 4.7576590Losses:  4.320098400115967 0.29062747955322266 0.8433710336685181
CurrentTrain: epoch  5, batch    42 | loss: 5.0324116Losses:  4.198003768920898 0.20366953313350677 0.7935861349105835
CurrentTrain: epoch  5, batch    43 | loss: 4.7984662Losses:  4.1499528884887695 0.2257283478975296 0.7956061363220215
CurrentTrain: epoch  5, batch    44 | loss: 4.7734842Losses:  4.108687400817871 0.2555201053619385 0.8002336621284485
CurrentTrain: epoch  5, batch    45 | loss: 4.7643242Losses:  4.142343521118164 0.23216353356838226 0.7913708686828613
CurrentTrain: epoch  5, batch    46 | loss: 4.7701921Losses:  4.137362957000732 0.2385450154542923 0.7991449236869812
CurrentTrain: epoch  5, batch    47 | loss: 4.7754803Losses:  4.100679874420166 0.23607757687568665 0.8329553604125977
CurrentTrain: epoch  5, batch    48 | loss: 4.7532353Losses:  4.1606974601745605 0.2136044204235077 0.8282333612442017
CurrentTrain: epoch  5, batch    49 | loss: 4.7884188Losses:  4.126302719116211 0.1862819492816925 0.8095941543579102
CurrentTrain: epoch  5, batch    50 | loss: 4.7173820Losses:  4.149551868438721 0.1961137056350708 0.8936858177185059
CurrentTrain: epoch  5, batch    51 | loss: 4.7925081Losses:  4.21708869934082 0.2700727581977844 0.771061897277832
CurrentTrain: epoch  5, batch    52 | loss: 4.8726926Losses:  4.170621871948242 0.19662833213806152 0.8283472061157227
CurrentTrain: epoch  5, batch    53 | loss: 4.7814240Losses:  4.087320804595947 0.24420224130153656 0.8176639676094055
CurrentTrain: epoch  5, batch    54 | loss: 4.7403550Losses:  4.2227783203125 0.24178823828697205 0.7621670365333557
CurrentTrain: epoch  5, batch    55 | loss: 4.8456502Losses:  4.14547872543335 0.2168245017528534 0.7783564329147339
CurrentTrain: epoch  5, batch    56 | loss: 4.7514815Losses:  4.225317478179932 0.17202797532081604 0.7725124359130859
CurrentTrain: epoch  5, batch    57 | loss: 4.7836018Losses:  4.081191539764404 0.16728951036930084 0.7796640992164612
CurrentTrain: epoch  5, batch    58 | loss: 4.6383133Losses:  4.17694091796875 0.20439359545707703 0.7717422246932983
CurrentTrain: epoch  5, batch    59 | loss: 4.7672052Losses:  4.105095863342285 0.14979439973831177 0.7751607298851013
CurrentTrain: epoch  5, batch    60 | loss: 4.6424708Losses:  4.127584457397461 0.16492170095443726 0.8349660038948059
CurrentTrain: epoch  5, batch    61 | loss: 4.7099891Losses:  4.211185455322266 0.10598234832286835 0.887286365032196
CurrentTrain: epoch  5, batch    62 | loss: 4.7608109Losses:  4.150941848754883 0.2015269547700882 0.7807663679122925
CurrentTrain: epoch  6, batch     0 | loss: 4.7428522Losses:  4.121020317077637 0.1587221324443817 0.870428204536438
CurrentTrain: epoch  6, batch     1 | loss: 4.7149563Losses:  4.074437141418457 0.1521727442741394 0.7712688446044922
CurrentTrain: epoch  6, batch     2 | loss: 4.6122441Losses:  4.2216715812683105 0.2129155993461609 0.8033818006515503
CurrentTrain: epoch  6, batch     3 | loss: 4.8362780Losses:  4.14561653137207 0.14911511540412903 0.7597298622131348
CurrentTrain: epoch  6, batch     4 | loss: 4.6745968Losses:  4.133797645568848 0.1427646279335022 0.8323621153831482
CurrentTrain: epoch  6, batch     5 | loss: 4.6927433Losses:  4.131992340087891 0.20201903581619263 0.8023651838302612
CurrentTrain: epoch  6, batch     6 | loss: 4.7351942Losses:  4.164327144622803 0.15136060118675232 0.8760189414024353
CurrentTrain: epoch  6, batch     7 | loss: 4.7536969Losses:  4.094094276428223 0.20755720138549805 0.7862467765808105
CurrentTrain: epoch  6, batch     8 | loss: 4.6947746Losses:  4.132081031799316 0.1451576054096222 0.8576430678367615
CurrentTrain: epoch  6, batch     9 | loss: 4.7060604Losses:  4.125558376312256 0.1899915337562561 0.793005645275116
CurrentTrain: epoch  6, batch    10 | loss: 4.7120528Losses:  4.1921234130859375 0.12988576292991638 0.8877921104431152
CurrentTrain: epoch  6, batch    11 | loss: 4.7659054Losses:  4.171696662902832 0.20498009026050568 0.780512809753418
CurrentTrain: epoch  6, batch    12 | loss: 4.7669330Losses:  4.119771480560303 0.1178017258644104 0.7849735021591187
CurrentTrain: epoch  6, batch    13 | loss: 4.6300597Losses:  4.126406669616699 0.19669894874095917 0.8007632493972778
CurrentTrain: epoch  6, batch    14 | loss: 4.7234874Losses:  4.153411865234375 0.18898499011993408 0.8513597249984741
CurrentTrain: epoch  6, batch    15 | loss: 4.7680764Losses:  4.106618881225586 0.1903907060623169 0.7640115022659302
CurrentTrain: epoch  6, batch    16 | loss: 4.6790152Losses:  4.1063103675842285 0.18124184012413025 0.819024920463562
CurrentTrain: epoch  6, batch    17 | loss: 4.6970649Losses:  4.146624565124512 0.07829064130783081 0.8559848666191101
CurrentTrain: epoch  6, batch    18 | loss: 4.6529074Losses:  4.101773738861084 0.20018374919891357 0.7898004055023193
CurrentTrain: epoch  6, batch    19 | loss: 4.6968579Losses:  4.080713272094727 0.22315654158592224 0.7702649831771851
CurrentTrain: epoch  6, batch    20 | loss: 4.6890020Losses:  4.197375297546387 0.16054901480674744 0.8096680641174316
CurrentTrain: epoch  6, batch    21 | loss: 4.7627583Losses:  4.163366794586182 0.17198167741298676 0.8169400691986084
CurrentTrain: epoch  6, batch    22 | loss: 4.7438188Losses:  4.350107669830322 0.1525591015815735 0.7836285829544067
CurrentTrain: epoch  6, batch    23 | loss: 4.8944812Losses:  4.160139560699463 0.16031378507614136 0.7791973948478699
CurrentTrain: epoch  6, batch    24 | loss: 4.7100520Losses:  4.153930187225342 0.15974733233451843 0.8287194967269897
CurrentTrain: epoch  6, batch    25 | loss: 4.7280369Losses:  4.150999069213867 0.1861477792263031 0.8103781938552856
CurrentTrain: epoch  6, batch    26 | loss: 4.7423358Losses:  4.114631652832031 0.15842360258102417 0.7857590913772583
CurrentTrain: epoch  6, batch    27 | loss: 4.6659346Losses:  4.068528652191162 0.1858399212360382 0.7555108070373535
CurrentTrain: epoch  6, batch    28 | loss: 4.6321239Losses:  4.10888147354126 0.17839613556861877 0.7620257139205933
CurrentTrain: epoch  6, batch    29 | loss: 4.6682906Losses:  4.100489616394043 0.17952419817447662 0.8011239171028137
CurrentTrain: epoch  6, batch    30 | loss: 4.6805758Losses:  4.053943634033203 0.12080275267362595 0.8403083682060242
CurrentTrain: epoch  6, batch    31 | loss: 4.5949006Losses:  4.144186496734619 0.1296650767326355 0.7616121172904968
CurrentTrain: epoch  6, batch    32 | loss: 4.6546574Losses:  4.10577392578125 0.2109193205833435 0.7846440076828003
CurrentTrain: epoch  6, batch    33 | loss: 4.7090154Losses:  4.117369174957275 0.21099722385406494 0.7230518460273743
CurrentTrain: epoch  6, batch    34 | loss: 4.6898923Losses:  4.061919212341309 0.11384674906730652 0.7356880903244019
CurrentTrain: epoch  6, batch    35 | loss: 4.5436101Losses:  4.173944473266602 0.10365274548530579 0.7658137083053589
CurrentTrain: epoch  6, batch    36 | loss: 4.6605043Losses:  4.11799430847168 0.16345319151878357 0.825370192527771
CurrentTrain: epoch  6, batch    37 | loss: 4.6941323Losses:  4.110825538635254 0.20782789587974548 0.8094415664672852
CurrentTrain: epoch  6, batch    38 | loss: 4.7233744Losses:  4.066610336303711 0.12510493397712708 0.8363707065582275
CurrentTrain: epoch  6, batch    39 | loss: 4.6099005Losses:  4.134564399719238 0.1729581356048584 0.7904779314994812
CurrentTrain: epoch  6, batch    40 | loss: 4.7027617Losses:  4.098021984100342 0.1965770423412323 0.8011414408683777
CurrentTrain: epoch  6, batch    41 | loss: 4.6951699Losses:  4.06406307220459 0.1842040717601776 0.7823230028152466
CurrentTrain: epoch  6, batch    42 | loss: 4.6394286Losses:  4.034875869750977 0.1631971299648285 0.7802259922027588
CurrentTrain: epoch  6, batch    43 | loss: 4.5881858Losses:  4.067030906677246 0.15408487617969513 0.8558661341667175
CurrentTrain: epoch  6, batch    44 | loss: 4.6490488Losses:  4.097382545471191 0.1432059109210968 0.7933599948883057
CurrentTrain: epoch  6, batch    45 | loss: 4.6372685Losses:  4.102164268493652 0.17994818091392517 0.7347456216812134
CurrentTrain: epoch  6, batch    46 | loss: 4.6494856Losses:  4.078069686889648 0.13193397223949432 0.8543993830680847
CurrentTrain: epoch  6, batch    47 | loss: 4.6372037Losses:  4.06389856338501 0.18441416323184967 0.807131290435791
CurrentTrain: epoch  6, batch    48 | loss: 4.6518784Losses:  4.086464881896973 0.09728200733661652 0.7809505462646484
CurrentTrain: epoch  6, batch    49 | loss: 4.5742221Losses:  4.114253997802734 0.14282339811325073 0.8016518354415894
CurrentTrain: epoch  6, batch    50 | loss: 4.6579032Losses:  4.040846824645996 0.14582513272762299 0.8290424942970276
CurrentTrain: epoch  6, batch    51 | loss: 4.6011930Losses:  4.09464168548584 0.1615675389766693 0.8260490894317627
CurrentTrain: epoch  6, batch    52 | loss: 4.6692338Losses:  4.102693557739258 0.07337410748004913 0.76129150390625
CurrentTrain: epoch  6, batch    53 | loss: 4.5567136Losses:  4.069188117980957 0.1507434844970703 0.7419548630714417
CurrentTrain: epoch  6, batch    54 | loss: 4.5909090Losses:  4.045548439025879 0.1601092517375946 0.7766530513763428
CurrentTrain: epoch  6, batch    55 | loss: 4.5939841Losses:  4.116037368774414 0.10462549328804016 0.7091224193572998
CurrentTrain: epoch  6, batch    56 | loss: 4.5752244Losses:  4.048147201538086 0.1367853730916977 0.7630534172058105
CurrentTrain: epoch  6, batch    57 | loss: 4.5664597Losses:  4.072355270385742 0.1458171010017395 0.8142356872558594
CurrentTrain: epoch  6, batch    58 | loss: 4.6252904Losses:  4.075438976287842 0.1733073741197586 0.7289973497390747
CurrentTrain: epoch  6, batch    59 | loss: 4.6132450Losses:  4.1344990730285645 0.12019313126802444 0.7890719175338745
CurrentTrain: epoch  6, batch    60 | loss: 4.6492281Losses:  4.077763557434082 0.13920456171035767 0.7237432599067688
CurrentTrain: epoch  6, batch    61 | loss: 4.5788398Losses:  4.023237705230713 0.18898265063762665 0.7246318459510803
CurrentTrain: epoch  6, batch    62 | loss: 4.5745363Losses:  4.090626239776611 0.10487274080514908 0.7681523561477661
CurrentTrain: epoch  7, batch     0 | loss: 4.5795751Losses:  4.094619274139404 0.12662728130817413 0.7681025266647339
CurrentTrain: epoch  7, batch     1 | loss: 4.6052980Losses:  4.077700614929199 0.13255953788757324 0.7483290433883667
CurrentTrain: epoch  7, batch     2 | loss: 4.5844250Losses:  4.062516689300537 0.1454353630542755 0.7859418988227844
CurrentTrain: epoch  7, batch     3 | loss: 4.6009231Losses:  4.069608688354492 0.12250436842441559 0.7817896604537964
CurrentTrain: epoch  7, batch     4 | loss: 4.5830078Losses:  4.04403018951416 0.1141754761338234 0.8202019929885864
CurrentTrain: epoch  7, batch     5 | loss: 4.5683064Losses:  4.045443058013916 0.13598217070102692 0.8087687492370605
CurrentTrain: epoch  7, batch     6 | loss: 4.5858097Losses:  4.12409782409668 0.1750016212463379 0.8241938352584839
CurrentTrain: epoch  7, batch     7 | loss: 4.7111964Losses:  4.000585556030273 0.09936629235744476 0.8296362161636353
CurrentTrain: epoch  7, batch     8 | loss: 4.5147700Losses:  4.026882648468018 0.16798758506774902 0.7100018858909607
CurrentTrain: epoch  7, batch     9 | loss: 4.5498710Losses:  4.033797264099121 0.19170066714286804 0.7698279619216919
CurrentTrain: epoch  7, batch    10 | loss: 4.6104116Losses:  4.007181167602539 0.1252036690711975 0.7689626216888428
CurrentTrain: epoch  7, batch    11 | loss: 4.5168662Losses:  4.019659042358398 0.13587656617164612 0.7289469242095947
CurrentTrain: epoch  7, batch    12 | loss: 4.5200090Losses:  4.0593342781066895 0.1113838478922844 0.7367391586303711
CurrentTrain: epoch  7, batch    13 | loss: 4.5390878Losses:  4.0184431076049805 0.15067830681800842 0.7701374292373657
CurrentTrain: epoch  7, batch    14 | loss: 4.5541902Losses:  4.011410713195801 0.1859043538570404 0.7697117328643799
CurrentTrain: epoch  7, batch    15 | loss: 4.5821710Losses:  4.0864763259887695 0.20036651194095612 0.7456898093223572
CurrentTrain: epoch  7, batch    16 | loss: 4.6596875Losses:  4.15526008605957 0.2296973466873169 0.7166758179664612
CurrentTrain: epoch  7, batch    17 | loss: 4.7432952Losses:  4.055184841156006 0.13090863823890686 0.799213707447052
CurrentTrain: epoch  7, batch    18 | loss: 4.5857000Losses:  4.107874870300293 0.13961215317249298 0.8242807984352112
CurrentTrain: epoch  7, batch    19 | loss: 4.6596274Losses:  4.0783586502075195 0.09651277214288712 0.7716248035430908
CurrentTrain: epoch  7, batch    20 | loss: 4.5606837Losses:  4.015879154205322 0.13035261631011963 0.7952494621276855
CurrentTrain: epoch  7, batch    21 | loss: 4.5438566Losses:  4.050375938415527 0.12529239058494568 0.8411585092544556
CurrentTrain: epoch  7, batch    22 | loss: 4.5962477Losses:  4.044328212738037 0.11749737709760666 0.7281385660171509
CurrentTrain: epoch  7, batch    23 | loss: 4.5258951Losses:  4.027220726013184 0.16943323612213135 0.7335798740386963
CurrentTrain: epoch  7, batch    24 | loss: 4.5634437Losses:  4.169948101043701 0.14903624355793 0.7981635332107544
CurrentTrain: epoch  7, batch    25 | loss: 4.7180662Losses:  4.018857955932617 0.10020119696855545 0.7656598687171936
CurrentTrain: epoch  7, batch    26 | loss: 4.5018892Losses:  4.049479007720947 0.13343775272369385 0.7831228971481323
CurrentTrain: epoch  7, batch    27 | loss: 4.5744781Losses:  4.054130554199219 0.14785948395729065 0.7160286903381348
CurrentTrain: epoch  7, batch    28 | loss: 4.5600042Losses:  4.0612688064575195 0.09606915712356567 0.8087479472160339
CurrentTrain: epoch  7, batch    29 | loss: 4.5617123Losses:  4.080110549926758 0.12262488901615143 0.8252339959144592
CurrentTrain: epoch  7, batch    30 | loss: 4.6153526Losses:  4.071714401245117 0.13478003442287445 0.7538840770721436
CurrentTrain: epoch  7, batch    31 | loss: 4.5834365Losses:  4.0675859451293945 0.08876289427280426 0.8141271471977234
CurrentTrain: epoch  7, batch    32 | loss: 4.5634122Losses:  4.155928611755371 0.13433223962783813 0.8132534027099609
CurrentTrain: epoch  7, batch    33 | loss: 4.6968875Losses:  4.096402645111084 0.10033832490444183 0.888403058052063
CurrentTrain: epoch  7, batch    34 | loss: 4.6409426Losses:  4.045797824859619 0.13223765790462494 0.8566024303436279
CurrentTrain: epoch  7, batch    35 | loss: 4.6063366Losses:  4.118777751922607 0.13127654790878296 0.8381186723709106
CurrentTrain: epoch  7, batch    36 | loss: 4.6691136Losses:  4.015730857849121 0.10615037381649017 0.7337741851806641
CurrentTrain: epoch  7, batch    37 | loss: 4.4887681Losses:  4.033356666564941 0.13529866933822632 0.7702635526657104
CurrentTrain: epoch  7, batch    38 | loss: 4.5537872Losses:  4.012308120727539 0.10187530517578125 0.7738122940063477
CurrentTrain: epoch  7, batch    39 | loss: 4.5010896Losses:  4.0444769859313965 0.16993899643421173 0.8032229542732239
CurrentTrain: epoch  7, batch    40 | loss: 4.6160274Losses:  4.026024341583252 0.11632358282804489 0.7568258047103882
CurrentTrain: epoch  7, batch    41 | loss: 4.5207605Losses:  4.024621486663818 0.11839717626571655 0.7762913703918457
CurrentTrain: epoch  7, batch    42 | loss: 4.5311642Losses:  4.060920715332031 0.13208387792110443 0.829982578754425
CurrentTrain: epoch  7, batch    43 | loss: 4.6079960Losses:  4.035991668701172 0.12689140439033508 0.8192796111106873
CurrentTrain: epoch  7, batch    44 | loss: 4.5725231Losses:  3.980017900466919 0.11223739385604858 0.7459226250648499
CurrentTrain: epoch  7, batch    45 | loss: 4.4652166Losses:  4.0431084632873535 0.11711817979812622 0.8042216300964355
CurrentTrain: epoch  7, batch    46 | loss: 4.5623379Losses:  4.032899379730225 0.1356770098209381 0.7954145073890686
CurrentTrain: epoch  7, batch    47 | loss: 4.5662837Losses:  4.030364036560059 0.179080992937088 0.7597149610519409
CurrentTrain: epoch  7, batch    48 | loss: 4.5893025Losses:  3.990760087966919 0.186739981174469 0.7918428778648376
CurrentTrain: epoch  7, batch    49 | loss: 4.5734215Losses:  3.96901798248291 0.10772301256656647 0.7859867811203003
CurrentTrain: epoch  7, batch    50 | loss: 4.4697347Losses:  4.020537853240967 0.11673791706562042 0.7232909202575684
CurrentTrain: epoch  7, batch    51 | loss: 4.4989214Losses:  4.0854668617248535 0.17857053875923157 0.7033452391624451
CurrentTrain: epoch  7, batch    52 | loss: 4.6157103Losses:  4.077762603759766 0.13480684161186218 0.7771548628807068
CurrentTrain: epoch  7, batch    53 | loss: 4.6011467Losses:  4.075006008148193 0.11819528043270111 0.8180760145187378
CurrentTrain: epoch  7, batch    54 | loss: 4.6022391Losses:  4.0345778465271 0.1283569037914276 0.7612977623939514
CurrentTrain: epoch  7, batch    55 | loss: 4.5435839Losses:  4.052526473999023 0.10634644329547882 0.8572030067443848
CurrentTrain: epoch  7, batch    56 | loss: 4.5874748Losses:  4.026262283325195 0.13519471883773804 0.8230417370796204
CurrentTrain: epoch  7, batch    57 | loss: 4.5729780Losses:  4.014054775238037 0.16512379050254822 0.7608187198638916
CurrentTrain: epoch  7, batch    58 | loss: 4.5595880Losses:  4.050271511077881 0.09056739509105682 0.8089646100997925
CurrentTrain: epoch  7, batch    59 | loss: 4.5453215Losses:  4.052055358886719 0.14426475763320923 0.8028438091278076
CurrentTrain: epoch  7, batch    60 | loss: 4.5977421Losses:  4.042806148529053 0.09375052154064178 0.7163033485412598
CurrentTrain: epoch  7, batch    61 | loss: 4.4947081Losses:  4.0642924308776855 0.07122837007045746 0.7491549253463745
CurrentTrain: epoch  7, batch    62 | loss: 4.5100985Losses:  4.032317161560059 0.15879584848880768 0.7886446118354797
CurrentTrain: epoch  8, batch     0 | loss: 4.5854354Losses:  4.068184852600098 0.13807745277881622 0.7764773368835449
CurrentTrain: epoch  8, batch     1 | loss: 4.5945005Losses:  4.038231372833252 0.16670265793800354 0.7816322445869446
CurrentTrain: epoch  8, batch     2 | loss: 4.5957503Losses:  4.050661563873291 0.07705548405647278 0.7193057537078857
CurrentTrain: epoch  8, batch     3 | loss: 4.4873700Losses:  3.9877185821533203 0.097288578748703 0.7280392050743103
CurrentTrain: epoch  8, batch     4 | loss: 4.4490266Losses:  4.048062324523926 0.13394176959991455 0.7841209769248962
CurrentTrain: epoch  8, batch     5 | loss: 4.5740643Losses:  4.026329040527344 0.1120404377579689 0.7480014562606812
CurrentTrain: epoch  8, batch     6 | loss: 4.5123701Losses:  4.061150074005127 0.10948452353477478 0.7769030332565308
CurrentTrain: epoch  8, batch     7 | loss: 4.5590863Losses:  4.02935266494751 0.12496358156204224 0.8255792856216431
CurrentTrain: epoch  8, batch     8 | loss: 4.5671062Losses:  4.049100875854492 0.12648959457874298 0.7213089466094971
CurrentTrain: epoch  8, batch     9 | loss: 4.5362449Losses:  3.9999444484710693 0.10446362942457199 0.8364098072052002
CurrentTrain: epoch  8, batch    10 | loss: 4.5226130Losses:  4.0217814445495605 0.12789084017276764 0.805069088935852
CurrentTrain: epoch  8, batch    11 | loss: 4.5522070Losses:  4.06235408782959 0.11927942931652069 0.8424267172813416
CurrentTrain: epoch  8, batch    12 | loss: 4.6028466Losses:  4.039654731750488 0.10428860783576965 0.8117669224739075
CurrentTrain: epoch  8, batch    13 | loss: 4.5498266Losses:  3.992358684539795 0.14891009032726288 0.7521008253097534
CurrentTrain: epoch  8, batch    14 | loss: 4.5173192Losses:  4.018831253051758 0.12667107582092285 0.7894507646560669
CurrentTrain: epoch  8, batch    15 | loss: 4.5402274Losses:  3.993145704269409 0.08030165731906891 0.7567664384841919
CurrentTrain: epoch  8, batch    16 | loss: 4.4518304Losses:  4.027061462402344 0.12521636486053467 0.8003429770469666
CurrentTrain: epoch  8, batch    17 | loss: 4.5524492Losses:  4.043070316314697 0.12405019253492355 0.7920227646827698
CurrentTrain: epoch  8, batch    18 | loss: 4.5631318Losses:  4.038144588470459 0.10747191309928894 0.8059041500091553
CurrentTrain: epoch  8, batch    19 | loss: 4.5485687Losses:  3.947702646255493 0.12476180493831635 0.7898198366165161
CurrentTrain: epoch  8, batch    20 | loss: 4.4673743Losses:  4.012760639190674 0.11380122601985931 0.7938986420631409
CurrentTrain: epoch  8, batch    21 | loss: 4.5235109Losses:  4.0008320808410645 0.12297451496124268 0.7594350576400757
CurrentTrain: epoch  8, batch    22 | loss: 4.5035238Losses:  4.030536651611328 0.14673112332820892 0.7925844788551331
CurrentTrain: epoch  8, batch    23 | loss: 4.5735598Losses:  4.032732009887695 0.15713034570217133 0.781308114528656
CurrentTrain: epoch  8, batch    24 | loss: 4.5805163Losses:  4.009769916534424 0.08489148318767548 0.7833174467086792
CurrentTrain: epoch  8, batch    25 | loss: 4.4863200Losses:  4.026076316833496 0.11501294374465942 0.679885745048523
CurrentTrain: epoch  8, batch    26 | loss: 4.4810324Losses:  4.0686235427856445 0.13317903876304626 0.806182861328125
CurrentTrain: epoch  8, batch    27 | loss: 4.6048942Losses:  4.001952171325684 0.15851473808288574 0.7575462460517883
CurrentTrain: epoch  8, batch    28 | loss: 4.5392404Losses:  4.022279739379883 0.12152992188930511 0.7960284352302551
CurrentTrain: epoch  8, batch    29 | loss: 4.5418239Losses:  4.014155387878418 0.12062761187553406 0.7596193552017212
CurrentTrain: epoch  8, batch    30 | loss: 4.5145926Losses:  4.013058185577393 0.11998654901981354 0.7779110074043274
CurrentTrain: epoch  8, batch    31 | loss: 4.5220003Losses:  4.04098653793335 0.11096958070993423 0.7521933317184448
CurrentTrain: epoch  8, batch    32 | loss: 4.5280528Losses:  4.028828144073486 0.10870318114757538 0.7794263362884521
CurrentTrain: epoch  8, batch    33 | loss: 4.5272446Losses:  4.016864776611328 0.1410696655511856 0.7508221864700317
CurrentTrain: epoch  8, batch    34 | loss: 4.5333457Losses:  4.013129234313965 0.1229228526353836 0.700698733329773
CurrentTrain: epoch  8, batch    35 | loss: 4.4864016Losses:  4.023805618286133 0.0950351357460022 0.8591034412384033
CurrentTrain: epoch  8, batch    36 | loss: 4.5483923Losses:  4.0586676597595215 0.12006326019763947 0.7151361703872681
CurrentTrain: epoch  8, batch    37 | loss: 4.5362992Losses:  3.972611427307129 0.12385280430316925 0.7160903811454773
CurrentTrain: epoch  8, batch    38 | loss: 4.4545093Losses:  4.056357383728027 0.13247978687286377 0.7516975402832031
CurrentTrain: epoch  8, batch    39 | loss: 4.5646858Losses:  4.144481658935547 0.15082533657550812 0.7535988092422485
CurrentTrain: epoch  8, batch    40 | loss: 4.6721067Losses:  4.026979446411133 0.11716160178184509 0.7914928197860718
CurrentTrain: epoch  8, batch    41 | loss: 4.5398874Losses:  4.025712490081787 0.1259538233280182 0.7960948944091797
CurrentTrain: epoch  8, batch    42 | loss: 4.5497136Losses:  4.052466869354248 0.14915373921394348 0.7885047197341919
CurrentTrain: epoch  8, batch    43 | loss: 4.5958729Losses:  4.0359110832214355 0.1279333233833313 0.7799062132835388
CurrentTrain: epoch  8, batch    44 | loss: 4.5537977Losses:  4.18735408782959 0.08830452710390091 0.7664920091629028
CurrentTrain: epoch  8, batch    45 | loss: 4.6589046Losses:  3.9815833568573 0.10042784363031387 0.7279225587844849
CurrentTrain: epoch  8, batch    46 | loss: 4.4459724Losses:  4.019966125488281 0.12548133730888367 0.6644282341003418
CurrentTrain: epoch  8, batch    47 | loss: 4.4776611Losses:  4.044329643249512 0.09108652174472809 0.8297806978225708
CurrentTrain: epoch  8, batch    48 | loss: 4.5503063Losses:  4.002111911773682 0.09955037385225296 0.7272506952285767
CurrentTrain: epoch  8, batch    49 | loss: 4.4652877Losses:  3.9968881607055664 0.09820660948753357 0.7747412919998169
CurrentTrain: epoch  8, batch    50 | loss: 4.4824653Losses:  4.039159774780273 0.10113036632537842 0.7665019035339355
CurrentTrain: epoch  8, batch    51 | loss: 4.5235415Losses:  4.019545078277588 0.17301714420318604 0.7494856119155884
CurrentTrain: epoch  8, batch    52 | loss: 4.5673051Losses:  4.035072326660156 0.13005919754505157 0.7700451016426086
CurrentTrain: epoch  8, batch    53 | loss: 4.5501542Losses:  4.001601219177246 0.1326994001865387 0.7687327861785889
CurrentTrain: epoch  8, batch    54 | loss: 4.5186672Losses:  3.9923105239868164 0.1114424616098404 0.7762433290481567
CurrentTrain: epoch  8, batch    55 | loss: 4.4918747Losses:  4.016507148742676 0.12743093073368073 0.8063799738883972
CurrentTrain: epoch  8, batch    56 | loss: 4.5471282Losses:  4.0076446533203125 0.13626736402511597 0.7906204462051392
CurrentTrain: epoch  8, batch    57 | loss: 4.5392222Losses:  4.032588005065918 0.11829372495412827 0.794819712638855
CurrentTrain: epoch  8, batch    58 | loss: 4.5482917Losses:  4.0284905433654785 0.1334834098815918 0.6979306936264038
CurrentTrain: epoch  8, batch    59 | loss: 4.5109391Losses:  3.967978000640869 0.15360212326049805 0.7258780598640442
CurrentTrain: epoch  8, batch    60 | loss: 4.4845190Losses:  4.032906532287598 0.13056498765945435 0.7933149933815002
CurrentTrain: epoch  8, batch    61 | loss: 4.5601292Losses:  3.989595890045166 0.046179741621017456 0.9187653064727783
CurrentTrain: epoch  8, batch    62 | loss: 4.4951582Losses:  4.006617546081543 0.08290935307741165 0.7044892311096191
CurrentTrain: epoch  9, batch     0 | loss: 4.4417715Losses:  4.0712080001831055 0.10150517523288727 0.7235736846923828
CurrentTrain: epoch  9, batch     1 | loss: 4.5345001Losses:  4.042102336883545 0.11196483671665192 0.7529462575912476
CurrentTrain: epoch  9, batch     2 | loss: 4.5305400Losses:  4.055919647216797 0.10188959538936615 0.8054845333099365
CurrentTrain: epoch  9, batch     3 | loss: 4.5605516Losses:  4.015348434448242 0.08415595442056656 0.7269381880760193
CurrentTrain: epoch  9, batch     4 | loss: 4.4629736Losses:  4.015547752380371 0.09164778143167496 0.7575222253799438
CurrentTrain: epoch  9, batch     5 | loss: 4.4859567Losses:  4.032637119293213 0.11108949780464172 0.766937255859375
CurrentTrain: epoch  9, batch     6 | loss: 4.5271955Losses:  4.008204460144043 0.09838484227657318 0.8551194071769714
CurrentTrain: epoch  9, batch     7 | loss: 4.5341492Losses:  3.982738733291626 0.09269006550312042 0.7571837306022644
CurrentTrain: epoch  9, batch     8 | loss: 4.4540210Losses:  4.004122734069824 0.13111624121665955 0.7416086792945862
CurrentTrain: epoch  9, batch     9 | loss: 4.5060434Losses:  4.009537696838379 0.11908094584941864 0.7452725768089294
CurrentTrain: epoch  9, batch    10 | loss: 4.5012550Losses:  4.032075881958008 0.13121934235095978 0.7029906511306763
CurrentTrain: epoch  9, batch    11 | loss: 4.5147905Losses:  4.009548187255859 0.1210048496723175 0.7105158567428589
CurrentTrain: epoch  9, batch    12 | loss: 4.4858112Losses:  4.013298511505127 0.12202976644039154 0.810820996761322
CurrentTrain: epoch  9, batch    13 | loss: 4.5407386Losses:  4.006064414978027 0.15503595769405365 0.7356446385383606
CurrentTrain: epoch  9, batch    14 | loss: 4.5289226Losses:  4.043846130371094 0.1182292103767395 0.7664825320243835
CurrentTrain: epoch  9, batch    15 | loss: 4.5453167Losses:  3.997981548309326 0.1231149211525917 0.731414258480072
CurrentTrain: epoch  9, batch    16 | loss: 4.4868035Losses:  4.039506912231445 0.12700839340686798 0.7792006731033325
CurrentTrain: epoch  9, batch    17 | loss: 4.5561156Losses:  4.0039777755737305 0.08838970214128494 0.8014010787010193
CurrentTrain: epoch  9, batch    18 | loss: 4.4930682Losses:  3.955188751220703 0.061064768582582474 0.7389109134674072
CurrentTrain: epoch  9, batch    19 | loss: 4.3857088Losses:  4.054256439208984 0.10303203016519547 0.7956007719039917
CurrentTrain: epoch  9, batch    20 | loss: 4.5550890Losses:  4.001158237457275 0.10472851991653442 0.741381049156189
CurrentTrain: epoch  9, batch    21 | loss: 4.4765773Losses:  4.04757022857666 0.1157447099685669 0.7670257091522217
CurrentTrain: epoch  9, batch    22 | loss: 4.5468278Losses:  3.9941246509552 0.1006195992231369 0.7655891180038452
CurrentTrain: epoch  9, batch    23 | loss: 4.4775386Losses:  4.002384185791016 0.08454611897468567 0.7694994211196899
CurrentTrain: epoch  9, batch    24 | loss: 4.4716802Losses:  4.015517234802246 0.09698071330785751 0.7058603763580322
CurrentTrain: epoch  9, batch    25 | loss: 4.4654279Losses:  3.9641969203948975 0.13108643889427185 0.7186998724937439
CurrentTrain: epoch  9, batch    26 | loss: 4.4546332Losses:  3.9993247985839844 0.10020547360181808 0.8272043466567993
CurrentTrain: epoch  9, batch    27 | loss: 4.5131326Losses:  3.9888546466827393 0.13177147507667542 0.745466947555542
CurrentTrain: epoch  9, batch    28 | loss: 4.4933596Losses:  3.997312545776367 0.11650385707616806 0.7687186002731323
CurrentTrain: epoch  9, batch    29 | loss: 4.4981756Losses:  4.0191497802734375 0.12746693193912506 0.7758647203445435
CurrentTrain: epoch  9, batch    30 | loss: 4.5345492Losses:  3.982513427734375 0.14547111093997955 0.7330116033554077
CurrentTrain: epoch  9, batch    31 | loss: 4.4944901Losses:  3.9693644046783447 0.103139728307724 0.7938216328620911
CurrentTrain: epoch  9, batch    32 | loss: 4.4694147Losses:  3.9686203002929688 0.05772123485803604 0.8612478375434875
CurrentTrain: epoch  9, batch    33 | loss: 4.4569654Losses:  4.003772735595703 0.12023363262414932 0.7630936503410339
CurrentTrain: epoch  9, batch    34 | loss: 4.5055532Losses:  3.9930124282836914 0.1433403640985489 0.756877601146698
CurrentTrain: epoch  9, batch    35 | loss: 4.5147920Losses:  3.997445821762085 0.0718013346195221 0.8590637445449829
CurrentTrain: epoch  9, batch    36 | loss: 4.4987793Losses:  3.991971015930176 0.10970230400562286 0.8095937967300415
CurrentTrain: epoch  9, batch    37 | loss: 4.5064702Losses:  4.029150009155273 0.08000944554805756 0.7726748585700989
CurrentTrain: epoch  9, batch    38 | loss: 4.4954967Losses:  3.985793113708496 0.1355893909931183 0.7591931819915771
CurrentTrain: epoch  9, batch    39 | loss: 4.5009794Losses:  4.077439308166504 0.08697783201932907 0.7577317357063293
CurrentTrain: epoch  9, batch    40 | loss: 4.5432830Losses:  3.9927608966827393 0.0640951544046402 0.7927711009979248
CurrentTrain: epoch  9, batch    41 | loss: 4.4532418Losses:  4.003827095031738 0.06479213386774063 0.7371174693107605
CurrentTrain: epoch  9, batch    42 | loss: 4.4371781Losses:  3.9819960594177246 0.06394907832145691 0.8061546087265015
CurrentTrain: epoch  9, batch    43 | loss: 4.4490223Losses:  4.004123687744141 0.09044038504362106 0.8038753271102905
CurrentTrain: epoch  9, batch    44 | loss: 4.4965014Losses:  3.9778149127960205 0.10776922106742859 0.7178068161010742
CurrentTrain: epoch  9, batch    45 | loss: 4.4444876Losses:  3.9701337814331055 0.07103881239891052 0.7565916776657104
CurrentTrain: epoch  9, batch    46 | loss: 4.4194684Losses:  4.020319938659668 0.08401718735694885 0.7535810470581055
CurrentTrain: epoch  9, batch    47 | loss: 4.4811277Losses:  3.995051145553589 0.09425894170999527 0.733363151550293
CurrentTrain: epoch  9, batch    48 | loss: 4.4559917Losses:  4.056514739990234 0.13134436309337616 0.7221020460128784
CurrentTrain: epoch  9, batch    49 | loss: 4.5489101Losses:  4.022833347320557 0.12690597772598267 0.7798134684562683
CurrentTrain: epoch  9, batch    50 | loss: 4.5396461Losses:  4.032771110534668 0.11948636174201965 0.7894542217254639
CurrentTrain: epoch  9, batch    51 | loss: 4.5469847Losses:  3.992952346801758 0.0808940976858139 0.7848716378211975
CurrentTrain: epoch  9, batch    52 | loss: 4.4662824Losses:  3.9740078449249268 0.11821190267801285 0.7469570636749268
CurrentTrain: epoch  9, batch    53 | loss: 4.4656982Losses:  3.9859836101531982 0.10934130847454071 0.7965929508209229
CurrentTrain: epoch  9, batch    54 | loss: 4.4936213Losses:  3.9876675605773926 0.08232226222753525 0.761917233467102
CurrentTrain: epoch  9, batch    55 | loss: 4.4509482Losses:  3.9561901092529297 0.11808228492736816 0.784928560256958
CurrentTrain: epoch  9, batch    56 | loss: 4.4667363Losses:  3.9909400939941406 0.1277325451374054 0.6981396675109863
CurrentTrain: epoch  9, batch    57 | loss: 4.4677429Losses:  3.9715657234191895 0.0879167914390564 0.7368706464767456
CurrentTrain: epoch  9, batch    58 | loss: 4.4279180Losses:  3.9606566429138184 0.11803647130727768 0.768493115901947
CurrentTrain: epoch  9, batch    59 | loss: 4.4629393Losses:  3.9987053871154785 0.10807330906391144 0.709261953830719
CurrentTrain: epoch  9, batch    60 | loss: 4.4614096Losses:  3.9921538829803467 0.10340776294469833 0.7189853191375732
CurrentTrain: epoch  9, batch    61 | loss: 4.4550543Losses:  3.988679885864258 0.04127095267176628 0.7182728052139282
CurrentTrain: epoch  9, batch    62 | loss: 4.3890872
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.85%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  7.055784225463867 1.278787612915039 0.9817715883255005
CurrentTrain: epoch  0, batch     0 | loss: 8.8254576Losses:  6.4046173095703125 1.036469578742981 0.997948169708252
CurrentTrain: epoch  0, batch     1 | loss: 7.9400606Losses:  6.6467156410217285 1.3306005001068115 0.9807372093200684
CurrentTrain: epoch  0, batch     2 | loss: 8.4676847Losses:  7.697811603546143 1.788139627478813e-07 0.9855589270591736
CurrentTrain: epoch  0, batch     3 | loss: 8.1905909Losses:  6.306767463684082 1.2297061681747437 1.0015015602111816
CurrentTrain: epoch  1, batch     0 | loss: 8.0372248Losses:  5.947414875030518 1.2940309047698975 0.9751861095428467
CurrentTrain: epoch  1, batch     1 | loss: 7.7290387Losses:  5.262706756591797 1.2544034719467163 0.975654661655426
CurrentTrain: epoch  1, batch     2 | loss: 7.0049376Losses:  3.394963264465332 0.381244957447052 0.9801478981971741
CurrentTrain: epoch  1, batch     3 | loss: 4.2662821Losses:  5.265565872192383 1.1196858882904053 0.9821195006370544
CurrentTrain: epoch  2, batch     0 | loss: 6.8763118Losses:  4.465237140655518 1.079696774482727 0.9655736684799194
CurrentTrain: epoch  2, batch     1 | loss: 6.0277205Losses:  4.310721397399902 1.0453500747680664 0.9949597120285034
CurrentTrain: epoch  2, batch     2 | loss: 5.8535514Losses:  5.879876136779785 0.055790841579437256 1.0079659223556519
CurrentTrain: epoch  2, batch     3 | loss: 6.4396501Losses:  5.048508644104004 1.1346927881240845 0.9741177558898926
CurrentTrain: epoch  3, batch     0 | loss: 6.6702604Losses:  3.8374509811401367 0.9408058524131775 0.9877573251724243
CurrentTrain: epoch  3, batch     1 | loss: 5.2721357Losses:  4.120697021484375 1.2438029050827026 0.9666873216629028
CurrentTrain: epoch  3, batch     2 | loss: 5.8478436Losses:  2.742215156555176 0.3362343907356262 1.0487933158874512
CurrentTrain: epoch  3, batch     3 | loss: 3.6028461Losses:  3.6309022903442383 0.902363657951355 0.9618836641311646
CurrentTrain: epoch  4, batch     0 | loss: 5.0142078Losses:  3.4233498573303223 0.9161438345909119 1.0053529739379883
CurrentTrain: epoch  4, batch     1 | loss: 4.8421702Losses:  4.3273210525512695 1.025837779045105 0.9663220643997192
CurrentTrain: epoch  4, batch     2 | loss: 5.8363199Losses:  3.380884885787964 0.07813608646392822 0.9504980444908142
CurrentTrain: epoch  4, batch     3 | loss: 3.9342701Losses:  4.073171615600586 0.876629650592804 0.9756854176521301
CurrentTrain: epoch  5, batch     0 | loss: 5.4376440Losses:  2.897754192352295 0.9582377672195435 0.9699389338493347
CurrentTrain: epoch  5, batch     1 | loss: 4.3409615Losses:  3.201507329940796 1.0694937705993652 0.9744374752044678
CurrentTrain: epoch  5, batch     2 | loss: 4.7582197Losses:  4.615493297576904 0.09336360543966293 0.9800049066543579
CurrentTrain: epoch  5, batch     3 | loss: 5.1988597Losses:  3.2567784786224365 0.9433642625808716 0.9574580788612366
CurrentTrain: epoch  6, batch     0 | loss: 4.6788721Losses:  3.396117687225342 0.8586356043815613 0.9725571274757385
CurrentTrain: epoch  6, batch     1 | loss: 4.7410316Losses:  3.135361671447754 1.0773637294769287 0.9901087284088135
CurrentTrain: epoch  6, batch     2 | loss: 4.7077799Losses:  2.9436826705932617 1.1920930376163597e-07 1.0
CurrentTrain: epoch  6, batch     3 | loss: 3.4436829Losses:  3.118217945098877 0.90521240234375 0.9831778407096863
CurrentTrain: epoch  7, batch     0 | loss: 4.5150194Losses:  2.801682710647583 1.0668082237243652 0.9891622066497803
CurrentTrain: epoch  7, batch     1 | loss: 4.3630719Losses:  2.6727793216705322 0.8610483407974243 0.9529761075973511
CurrentTrain: epoch  7, batch     2 | loss: 4.0103159Losses:  2.2661566734313965 8.94069742685133e-08 0.9227645993232727
CurrentTrain: epoch  7, batch     3 | loss: 2.7275391Losses:  2.566279888153076 0.9329795241355896 0.9755527973175049
CurrentTrain: epoch  8, batch     0 | loss: 3.9870358Losses:  2.567762851715088 0.7871360182762146 0.9573246240615845
CurrentTrain: epoch  8, batch     1 | loss: 3.8335612Losses:  2.6358964443206787 0.9212143421173096 0.970537006855011
CurrentTrain: epoch  8, batch     2 | loss: 4.0423794Losses:  1.9114632606506348 0.06361454725265503 1.0579029321670532
CurrentTrain: epoch  8, batch     3 | loss: 2.5040293Losses:  2.3526272773742676 0.989557147026062 0.9673095345497131
CurrentTrain: epoch  9, batch     0 | loss: 3.8258393Losses:  2.734876871109009 0.7348376512527466 0.9786014556884766
CurrentTrain: epoch  9, batch     1 | loss: 3.9590154Losses:  2.2317235469818115 0.8145911693572998 0.9575203657150269
CurrentTrain: epoch  9, batch     2 | loss: 3.5250750Losses:  1.908379316329956 0.08522151410579681 0.9493595361709595
CurrentTrain: epoch  9, batch     3 | loss: 2.4682806
Losses:  2.5452136993408203 1.005371332168579 0.8624069690704346
MemoryTrain:  epoch  0, batch     0 | loss: 3.9817886Losses:  1.0512460470199585 0.08384162187576294 0.9486885070800781
MemoryTrain:  epoch  0, batch     1 | loss: 1.6094320Losses:  1.9029290676116943 0.9329221844673157 0.8961867094039917
MemoryTrain:  epoch  1, batch     0 | loss: 3.2839446Losses:  1.6930817365646362 0.5380717515945435 0.791506290435791
MemoryTrain:  epoch  1, batch     1 | loss: 2.6269066Losses:  1.549662709236145 0.9266217350959778 0.8570396900177002
MemoryTrain:  epoch  2, batch     0 | loss: 2.9048042Losses:  0.17081820964813232 0.2709287106990814 0.9509848952293396
MemoryTrain:  epoch  2, batch     1 | loss: 0.9172394Losses:  0.670508623123169 0.8300930261611938 0.8708304166793823
MemoryTrain:  epoch  3, batch     0 | loss: 1.9360168Losses:  1.8567090034484863 0.49962058663368225 0.8996753096580505
MemoryTrain:  epoch  3, batch     1 | loss: 2.8061674Losses:  0.8314087986946106 0.9758895635604858 0.8939130306243896
MemoryTrain:  epoch  4, batch     0 | loss: 2.2542548Losses:  0.015345158986747265 0.22788965702056885 0.804924488067627
MemoryTrain:  epoch  4, batch     1 | loss: 0.6456971Losses:  0.5760592222213745 0.9339964985847473 0.8717631101608276
MemoryTrain:  epoch  5, batch     0 | loss: 1.9459374Losses:  0.07742582261562347 0.1389518827199936 0.8935812711715698
MemoryTrain:  epoch  5, batch     1 | loss: 0.6631683Losses:  0.49679914116859436 0.8401274085044861 0.8909177780151367
MemoryTrain:  epoch  6, batch     0 | loss: 1.7823855Losses:  0.13239876925945282 0.2682611048221588 0.8184061050415039
MemoryTrain:  epoch  6, batch     1 | loss: 0.8098629Losses:  0.23864580690860748 0.8556119203567505 0.873565673828125
MemoryTrain:  epoch  7, batch     0 | loss: 1.5310405Losses:  1.0374298095703125 0.14857718348503113 0.8779032230377197
MemoryTrain:  epoch  7, batch     1 | loss: 1.6249586Losses:  0.3310810327529907 0.8191359639167786 0.8690603971481323
MemoryTrain:  epoch  8, batch     0 | loss: 1.5847473Losses:  0.025532696396112442 0.3208049535751343 0.8966102600097656
MemoryTrain:  epoch  8, batch     1 | loss: 0.7946428Losses:  0.1858222782611847 0.8283832669258118 0.8773468732833862
MemoryTrain:  epoch  9, batch     0 | loss: 1.4528790Losses:  0.6474776864051819 0.11684109270572662 0.8696633577346802
MemoryTrain:  epoch  9, batch     1 | loss: 1.1991504
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 78.03%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 73.82%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 73.03%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 69.18%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 66.62%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 64.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.93%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.58%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.99%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.85%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.85%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.85%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 94.11%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 94.19%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 94.10%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.92%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.92%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.83%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.67%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 93.19%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 92.80%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 92.34%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 92.30%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 92.02%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 91.69%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.57%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 91.24%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 91.05%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 90.80%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 90.35%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 90.08%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 89.58%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 89.10%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 88.62%   [EVAL] batch:   95 | acc: 25.00%,  total acc: 87.96%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.63%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 87.05%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.74%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 86.19%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 85.71%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 85.36%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 84.77%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.50%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 84.17%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 83.79%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 83.41%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 82.87%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.34%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 81.93%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 81.48%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.19%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.97%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.14%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 81.83%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 81.98%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.56%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.65%   
cur_acc:  ['0.9435', '0.7093']
his_acc:  ['0.9435', '0.8265']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  6.955957412719727 1.6348928213119507 0.9850648641586304
CurrentTrain: epoch  0, batch     0 | loss: 9.0833826Losses:  6.010058403015137 1.5889620780944824 0.9874417781829834
CurrentTrain: epoch  0, batch     1 | loss: 8.0927410Losses:  6.746856212615967 1.8114556074142456 0.9901292324066162
CurrentTrain: epoch  0, batch     2 | loss: 9.0533762Losses:  7.791628837585449 0.41907453536987305 0.9663126468658447
CurrentTrain: epoch  0, batch     3 | loss: 8.6938591Losses:  5.330340385437012 1.601395606994629 0.9772152304649353
CurrentTrain: epoch  1, batch     0 | loss: 7.4203434Losses:  6.457982540130615 1.6354734897613525 0.9940943121910095
CurrentTrain: epoch  1, batch     1 | loss: 8.5905037Losses:  4.584226131439209 1.5264514684677124 0.9833298921585083
CurrentTrain: epoch  1, batch     2 | loss: 6.6023426Losses:  5.959055423736572 0.26801028847694397 0.9480897188186646
CurrentTrain: epoch  1, batch     3 | loss: 6.7011104Losses:  4.100390911102295 1.2018153667449951 0.9798270463943481
CurrentTrain: epoch  2, batch     0 | loss: 5.7921195Losses:  4.853476524353027 1.5015835762023926 0.974572479724884
CurrentTrain: epoch  2, batch     1 | loss: 6.8423462Losses:  5.924900531768799 1.6221555471420288 0.9878690242767334
CurrentTrain: epoch  2, batch     2 | loss: 8.0409908Losses:  2.348356246948242 0.1795700192451477 0.9195245504379272
CurrentTrain: epoch  2, batch     3 | loss: 2.9876885Losses:  4.661391258239746 1.4721161127090454 0.9664139151573181
CurrentTrain: epoch  3, batch     0 | loss: 6.6167140Losses:  4.0817975997924805 1.256552815437317 0.9771471619606018
CurrentTrain: epoch  3, batch     1 | loss: 5.8269238Losses:  4.982169151306152 1.4248136281967163 0.9747562408447266
CurrentTrain: epoch  3, batch     2 | loss: 6.8943610Losses:  5.316536903381348 0.18321281671524048 0.9732164740562439
CurrentTrain: epoch  3, batch     3 | loss: 5.9863577Losses:  5.049744606018066 1.3623080253601074 0.9743512868881226
CurrentTrain: epoch  4, batch     0 | loss: 6.8992281Losses:  4.297504425048828 1.3977258205413818 0.9766013622283936
CurrentTrain: epoch  4, batch     1 | loss: 6.1835313Losses:  3.1419191360473633 1.1349844932556152 0.9604014754295349
CurrentTrain: epoch  4, batch     2 | loss: 4.7571044Losses:  4.176070213317871 0.2669793963432312 1.0
CurrentTrain: epoch  4, batch     3 | loss: 4.9430494Losses:  4.760313034057617 1.2884737253189087 0.9670897722244263
CurrentTrain: epoch  5, batch     0 | loss: 6.5323315Losses:  3.456948757171631 1.044257640838623 0.9764895439147949
CurrentTrain: epoch  5, batch     1 | loss: 4.9894514Losses:  3.5571022033691406 0.982491135597229 0.9628010392189026
CurrentTrain: epoch  5, batch     2 | loss: 5.0209937Losses:  7.3241071701049805 0.6031002998352051 0.9962674379348755
CurrentTrain: epoch  5, batch     3 | loss: 8.4253416Losses:  4.253609657287598 1.4392226934432983 0.9771119952201843
CurrentTrain: epoch  6, batch     0 | loss: 6.1813884Losses:  4.096027851104736 1.2487359046936035 0.9573755264282227
CurrentTrain: epoch  6, batch     1 | loss: 5.8234515Losses:  3.2248287200927734 1.0487148761749268 0.9694332480430603
CurrentTrain: epoch  6, batch     2 | loss: 4.7582598Losses:  2.723484516143799 0.19619587063789368 0.8899703025817871
CurrentTrain: epoch  6, batch     3 | loss: 3.3646655Losses:  3.3686463832855225 1.0970150232315063 0.9770262241363525
CurrentTrain: epoch  7, batch     0 | loss: 4.9541745Losses:  3.6998696327209473 1.0775344371795654 0.9654432535171509
CurrentTrain: epoch  7, batch     1 | loss: 5.2601256Losses:  3.5905394554138184 1.1033276319503784 0.9384844899177551
CurrentTrain: epoch  7, batch     2 | loss: 5.1631093Losses:  5.080680847167969 0.5330368280410767 0.9485539197921753
CurrentTrain: epoch  7, batch     3 | loss: 6.0879946Losses:  3.38846492767334 1.176421880722046 0.9478611946105957
CurrentTrain: epoch  8, batch     0 | loss: 5.0388174Losses:  3.440725803375244 1.1790928840637207 0.961410641670227
CurrentTrain: epoch  8, batch     1 | loss: 5.1005239Losses:  3.3953311443328857 0.8516086339950562 0.9509632587432861
CurrentTrain: epoch  8, batch     2 | loss: 4.7224212Losses:  2.3707962036132812 0.06024640053510666 0.9626180529594421
CurrentTrain: epoch  8, batch     3 | loss: 2.9123516Losses:  2.9625284671783447 0.9652554392814636 0.9258760213851929
CurrentTrain: epoch  9, batch     0 | loss: 4.3907218Losses:  2.6221652030944824 0.8045262098312378 0.9731190204620361
CurrentTrain: epoch  9, batch     1 | loss: 3.9132509Losses:  3.6329519748687744 0.9864057302474976 0.9563369750976562
CurrentTrain: epoch  9, batch     2 | loss: 5.0975261Losses:  4.807207107543945 0.5455621480941772 0.9224084615707397
CurrentTrain: epoch  9, batch     3 | loss: 5.8139734
Losses:  1.1493215560913086 1.1700928211212158 0.9136365652084351
MemoryTrain:  epoch  0, batch     0 | loss: 2.7762327Losses:  0.6379122138023376 1.0298733711242676 0.8761457204818726
MemoryTrain:  epoch  0, batch     1 | loss: 2.1058586Losses:  1.0164883136749268 1.1588455438613892 0.9355824589729309
MemoryTrain:  epoch  1, batch     0 | loss: 2.6431253Losses:  0.8875074982643127 0.8239734172821045 0.8539532423019409
MemoryTrain:  epoch  1, batch     1 | loss: 2.1384575Losses:  0.4175693094730377 0.9120783805847168 0.8854447603225708
MemoryTrain:  epoch  2, batch     0 | loss: 1.7723701Losses:  0.8979552388191223 0.9650368690490723 0.9059087634086609
MemoryTrain:  epoch  2, batch     1 | loss: 2.3159463Losses:  0.31562912464141846 0.9568694829940796 0.8629299402236938
MemoryTrain:  epoch  3, batch     0 | loss: 1.7039635Losses:  0.28880634903907776 0.8673384189605713 0.925574541091919
MemoryTrain:  epoch  3, batch     1 | loss: 1.6189320Losses:  0.25916436314582825 1.0910924673080444 0.8643069863319397
MemoryTrain:  epoch  4, batch     0 | loss: 1.7824103Losses:  0.24934470653533936 0.7573475241661072 0.9199290871620178
MemoryTrain:  epoch  4, batch     1 | loss: 1.4666567Losses:  0.1661393940448761 1.0460171699523926 0.9255659580230713
MemoryTrain:  epoch  5, batch     0 | loss: 1.6749395Losses:  0.10569056123495102 0.7917982935905457 0.8558446168899536
MemoryTrain:  epoch  5, batch     1 | loss: 1.3254111Losses:  0.15156219899654388 0.8918050527572632 0.8897691965103149
MemoryTrain:  epoch  6, batch     0 | loss: 1.4882519Losses:  0.04602072760462761 0.8585066795349121 0.8977830410003662
MemoryTrain:  epoch  6, batch     1 | loss: 1.3534189Losses:  0.23307763040065765 0.8296745419502258 0.8743608593940735
MemoryTrain:  epoch  7, batch     0 | loss: 1.4999325Losses:  0.10664324462413788 0.9223011136054993 0.9082074165344238
MemoryTrain:  epoch  7, batch     1 | loss: 1.4830481Losses:  0.06568734347820282 1.0869009494781494 0.9172027111053467
MemoryTrain:  epoch  8, batch     0 | loss: 1.6111896Losses:  0.06907609105110168 0.5936249494552612 0.8578495979309082
MemoryTrain:  epoch  8, batch     1 | loss: 1.0916258Losses:  0.043207455426454544 1.0550131797790527 0.9023752808570862
MemoryTrain:  epoch  9, batch     0 | loss: 1.5494082Losses:  0.04337896779179573 0.6106026768684387 0.8748142123222351
MemoryTrain:  epoch  9, batch     1 | loss: 1.0913887
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 18.75%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 62.10%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 71.43%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 71.55%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.93%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 71.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.53%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.46%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.64%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.44%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 92.94%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 93.07%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.57%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 93.66%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 93.66%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.58%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 93.42%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 93.26%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 93.11%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.88%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.58%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 92.30%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 91.94%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 91.74%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 91.40%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.28%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.95%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 90.84%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 90.59%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 90.35%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 90.01%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 89.58%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 89.16%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 88.75%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 88.28%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.95%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 87.44%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 87.18%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 86.39%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.15%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 85.86%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.64%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 85.48%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.26%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.81%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 84.26%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 83.72%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 83.24%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.77%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.48%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.25%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.94%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 83.18%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 83.16%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 83.50%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 83.12%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 82.91%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 82.90%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 82.74%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 82.54%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.80%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 82.88%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 82.93%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 82.42%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 81.92%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 81.52%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 81.07%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 80.64%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.34%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.96%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 80.75%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 80.47%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 80.19%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 80.03%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 79.64%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 79.25%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 79.26%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 79.67%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 80.48%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 80.41%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 80.27%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 80.24%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 80.32%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 80.12%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 79.92%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 79.79%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.77%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 79.74%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 79.71%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 79.79%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 79.70%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 79.70%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 79.71%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 79.49%   
cur_acc:  ['0.9435', '0.7093', '0.7153']
his_acc:  ['0.9435', '0.8265', '0.7949']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  5.944109916687012 1.5935401916503906 0.9698348045349121
CurrentTrain: epoch  0, batch     0 | loss: 8.0225677Losses:  4.248978614807129 1.5158448219299316 0.9961916208267212
CurrentTrain: epoch  0, batch     1 | loss: 6.2629194Losses:  5.0258378982543945 1.639284610748291 0.9910619258880615
CurrentTrain: epoch  0, batch     2 | loss: 7.1606536Losses:  3.273947238922119 0.4651198387145996 0.9521967172622681
CurrentTrain: epoch  0, batch     3 | loss: 4.2151656Losses:  4.506061553955078 1.4683518409729004 0.9806509017944336
CurrentTrain: epoch  1, batch     0 | loss: 6.4647388Losses:  3.6299195289611816 1.2867577075958252 0.9559929370880127
CurrentTrain: epoch  1, batch     1 | loss: 5.3946738Losses:  3.8168673515319824 1.8312724828720093 0.9973094463348389
CurrentTrain: epoch  1, batch     2 | loss: 6.1467948Losses:  2.833494186401367 0.17647424340248108 0.9665217399597168
CurrentTrain: epoch  1, batch     3 | loss: 3.4932294Losses:  4.554873943328857 1.5854079723358154 0.9815315008163452
CurrentTrain: epoch  2, batch     0 | loss: 6.6310472Losses:  3.0604026317596436 1.3772714138031006 0.9664767980575562
CurrentTrain: epoch  2, batch     1 | loss: 4.9209123Losses:  2.4053778648376465 1.1958630084991455 0.9850825071334839
CurrentTrain: epoch  2, batch     2 | loss: 4.0937819Losses:  1.927844524383545 0.12716759741306305 0.9207068681716919
CurrentTrain: epoch  2, batch     3 | loss: 2.5153656Losses:  3.1426444053649902 1.1942036151885986 0.9781809449195862
CurrentTrain: epoch  3, batch     0 | loss: 4.8259387Losses:  3.15438175201416 1.4284237623214722 0.9519952535629272
CurrentTrain: epoch  3, batch     1 | loss: 5.0588031Losses:  3.0590710639953613 1.3681838512420654 0.9726947546005249
CurrentTrain: epoch  3, batch     2 | loss: 4.9136019Losses:  2.1997482776641846 0.1280784159898758 1.0
CurrentTrain: epoch  3, batch     3 | loss: 2.8278267Losses:  2.9101576805114746 1.0004427433013916 0.9433803558349609
CurrentTrain: epoch  4, batch     0 | loss: 4.3822908Losses:  2.738588809967041 1.081270456314087 0.9440146684646606
CurrentTrain: epoch  4, batch     1 | loss: 4.2918668Losses:  3.109917402267456 0.8990254402160645 1.002748966217041
CurrentTrain: epoch  4, batch     2 | loss: 4.5103168Losses:  2.1225247383117676 0.2179221212863922 0.9321891069412231
CurrentTrain: epoch  4, batch     3 | loss: 2.8065414Losses:  3.0994746685028076 1.1070849895477295 0.9643543362617493
CurrentTrain: epoch  5, batch     0 | loss: 4.6887369Losses:  3.1849117279052734 1.0347273349761963 0.9325022101402283
CurrentTrain: epoch  5, batch     1 | loss: 4.6858897Losses:  2.4756011962890625 1.2334976196289062 0.9687633514404297
CurrentTrain: epoch  5, batch     2 | loss: 4.1934805Losses:  2.123995304107666 8.94069742685133e-08 1.0
CurrentTrain: epoch  5, batch     3 | loss: 2.6239953Losses:  2.1627092361450195 0.8915741443634033 0.9815691709518433
CurrentTrain: epoch  6, batch     0 | loss: 3.5450680Losses:  2.844066619873047 0.8721296787261963 0.9484536647796631
CurrentTrain: epoch  6, batch     1 | loss: 4.1904230Losses:  3.1265571117401123 1.1658262014389038 0.9322207570075989
CurrentTrain: epoch  6, batch     2 | loss: 4.7584934Losses:  2.5111167430877686 0.26546478271484375 0.9226231575012207
CurrentTrain: epoch  6, batch     3 | loss: 3.2378931Losses:  2.316983222961426 0.8676912784576416 0.9593002796173096
CurrentTrain: epoch  7, batch     0 | loss: 3.6643248Losses:  2.39517879486084 1.1249605417251587 0.9571884274482727
CurrentTrain: epoch  7, batch     1 | loss: 3.9987335Losses:  2.92057466506958 1.1409368515014648 0.9198974967002869
CurrentTrain: epoch  7, batch     2 | loss: 4.5214601Losses:  2.5551490783691406 0.34891408681869507 0.9854950308799744
CurrentTrain: epoch  7, batch     3 | loss: 3.3968108Losses:  2.478630781173706 1.007689118385315 0.9535818099975586
CurrentTrain: epoch  8, batch     0 | loss: 3.9631109Losses:  2.4631595611572266 0.8616760969161987 0.9335846900939941
CurrentTrain: epoch  8, batch     1 | loss: 3.7916281Losses:  2.1195566654205322 0.841537356376648 0.9481192231178284
CurrentTrain: epoch  8, batch     2 | loss: 3.4351535Losses:  2.693140983581543 0.18496674299240112 0.8892278671264648
CurrentTrain: epoch  8, batch     3 | loss: 3.3227217Losses:  2.2816483974456787 0.915140688419342 0.9416192173957825
CurrentTrain: epoch  9, batch     0 | loss: 3.6675987Losses:  2.795825719833374 0.8080185651779175 0.9269742369651794
CurrentTrain: epoch  9, batch     1 | loss: 4.0673313Losses:  2.108582019805908 0.834074854850769 0.942287802696228
CurrentTrain: epoch  9, batch     2 | loss: 3.4138010Losses:  2.082240343093872 0.4242803156375885 0.9346210360527039
CurrentTrain: epoch  9, batch     3 | loss: 2.9738312
Losses:  0.42148375511169434 1.1556243896484375 0.9155528545379639
MemoryTrain:  epoch  0, batch     0 | loss: 2.0348845Losses:  1.141808032989502 0.8630254864692688 0.8840121030807495
MemoryTrain:  epoch  0, batch     1 | loss: 2.4468396Losses:  0.9564970135688782 0.5487275123596191 0.9165608882904053
MemoryTrain:  epoch  0, batch     2 | loss: 1.9635049Losses:  0.996657133102417 0.8805834054946899 0.9364018440246582
MemoryTrain:  epoch  1, batch     0 | loss: 2.3454413Losses:  0.7339105606079102 0.7680515050888062 0.8645864725112915
MemoryTrain:  epoch  1, batch     1 | loss: 1.9342554Losses:  1.4015069007873535 0.8214914798736572 0.9028338193893433
MemoryTrain:  epoch  1, batch     2 | loss: 2.6744153Losses:  0.5784846544265747 1.090679407119751 0.9034774303436279
MemoryTrain:  epoch  2, batch     0 | loss: 2.1209028Losses:  0.7259427905082703 0.8545942902565002 0.9352281093597412
MemoryTrain:  epoch  2, batch     1 | loss: 2.0481510Losses:  0.5392619371414185 0.5008710026741028 0.8303985595703125
MemoryTrain:  epoch  2, batch     2 | loss: 1.4553323Losses:  0.49959856271743774 1.0885589122772217 0.9102917909622192
MemoryTrain:  epoch  3, batch     0 | loss: 2.0433033Losses:  0.3773398697376251 0.6750808358192444 0.8974804282188416
MemoryTrain:  epoch  3, batch     1 | loss: 1.5011610Losses:  0.22197303175926208 0.5715717673301697 0.8734880089759827
MemoryTrain:  epoch  3, batch     2 | loss: 1.2302887Losses:  0.13221023976802826 0.9881870746612549 0.9112027883529663
MemoryTrain:  epoch  4, batch     0 | loss: 1.5759988Losses:  0.6691980361938477 0.8302128314971924 0.8800173997879028
MemoryTrain:  epoch  4, batch     1 | loss: 1.9394195Losses:  0.35616445541381836 0.5673648715019226 0.907597541809082
MemoryTrain:  epoch  4, batch     2 | loss: 1.3773282Losses:  0.1953570544719696 0.7976201772689819 0.9025483131408691
MemoryTrain:  epoch  5, batch     0 | loss: 1.4442514Losses:  0.09019941091537476 1.070612907409668 0.8600287437438965
MemoryTrain:  epoch  5, batch     1 | loss: 1.5908267Losses:  0.06999775767326355 0.4357742369174957 0.9577537775039673
MemoryTrain:  epoch  5, batch     2 | loss: 0.9846489Losses:  0.0999421775341034 0.7228355407714844 0.868942141532898
MemoryTrain:  epoch  6, batch     0 | loss: 1.2572489Losses:  0.06794975697994232 0.8713483214378357 0.9192094802856445
MemoryTrain:  epoch  6, batch     1 | loss: 1.3989029Losses:  0.0723569393157959 0.7128615975379944 0.8975675106048584
MemoryTrain:  epoch  6, batch     2 | loss: 1.2340024Losses:  0.07172593474388123 0.7472319602966309 0.8858226537704468
MemoryTrain:  epoch  7, batch     0 | loss: 1.2618692Losses:  0.07800190150737762 0.7993800640106201 0.9031729698181152
MemoryTrain:  epoch  7, batch     1 | loss: 1.3289685Losses:  0.06273896992206573 0.5694980621337891 0.8917474746704102
MemoryTrain:  epoch  7, batch     2 | loss: 1.0781107Losses:  0.053357042372226715 0.6103386878967285 0.9091395139694214
MemoryTrain:  epoch  8, batch     0 | loss: 1.1182655Losses:  0.09004662930965424 0.9896631836891174 0.8977969884872437
MemoryTrain:  epoch  8, batch     1 | loss: 1.5286083Losses:  0.05888235941529274 0.49499014019966125 0.8508374094963074
MemoryTrain:  epoch  8, batch     2 | loss: 0.9792912Losses:  0.04496578127145767 0.9541416168212891 0.8997474908828735
MemoryTrain:  epoch  9, batch     0 | loss: 1.4489812Losses:  0.07572746276855469 0.6618731021881104 0.8648701310157776
MemoryTrain:  epoch  9, batch     1 | loss: 1.1700356Losses:  0.028646493330597878 0.42388245463371277 0.9299397468566895
MemoryTrain:  epoch  9, batch     2 | loss: 0.9174988
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 12.50%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.66%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 78.34%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.20%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 78.67%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 88.42%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 90.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 90.90%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.73%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.37%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 90.22%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 90.76%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 90.81%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.98%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 90.89%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 90.75%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.54%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.42%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.30%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.11%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 89.84%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.81%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 89.41%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 89.08%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 88.38%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 88.15%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 87.43%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 87.22%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 86.87%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 86.60%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 86.40%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 86.14%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 85.55%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.04%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 84.61%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 84.05%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 83.76%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 83.35%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 83.08%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 82.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 82.24%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 82.05%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.80%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 81.55%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 81.37%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 80.78%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.21%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.64%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.60%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.24%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.04%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.22%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.34%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.79%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 79.80%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 79.28%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 79.05%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 78.97%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 78.75%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 78.48%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 78.46%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 78.04%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 77.57%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 77.07%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 76.43%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 77.03%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 76.77%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 76.51%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 76.42%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 76.01%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 75.76%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 75.80%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.20%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.48%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 76.51%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 76.57%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 76.65%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 76.55%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 76.54%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 76.45%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 76.26%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 76.22%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 76.22%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 76.24%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 76.30%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 76.39%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 76.73%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.76%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 76.87%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 76.73%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 76.63%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 76.48%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 76.32%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 76.19%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 76.12%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 76.02%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 75.83%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.79%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.73%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 75.63%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 75.48%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 75.30%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 75.06%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.82%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 74.59%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.91%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 75.28%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  221 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 75.34%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 76.58%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 76.60%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 76.71%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.73%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 76.82%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 77.15%   
cur_acc:  ['0.9435', '0.7093', '0.7153', '0.7867']
his_acc:  ['0.9435', '0.8265', '0.7949', '0.7715']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  5.881065845489502 1.393949031829834 0.9717339873313904
CurrentTrain: epoch  0, batch     0 | loss: 7.7608819Losses:  6.375132083892822 1.6577708721160889 0.9621191620826721
CurrentTrain: epoch  0, batch     1 | loss: 8.5139627Losses:  6.002503395080566 1.3234513998031616 0.9865312576293945
CurrentTrain: epoch  0, batch     2 | loss: 7.8192205Losses:  4.013055801391602 8.94069742685133e-08 1.0
CurrentTrain: epoch  0, batch     3 | loss: 4.5130558Losses:  5.689345359802246 1.411017894744873 0.9511043429374695
CurrentTrain: epoch  1, batch     0 | loss: 7.5759153Losses:  4.739008903503418 1.4499708414077759 0.9708269238471985
CurrentTrain: epoch  1, batch     1 | loss: 6.6743932Losses:  4.743014335632324 1.3121864795684814 0.9933491945266724
CurrentTrain: epoch  1, batch     2 | loss: 6.5518751Losses:  4.166090488433838 0.46575015783309937 0.9049213528633118
CurrentTrain: epoch  1, batch     3 | loss: 5.0843015Losses:  4.3675994873046875 1.560561180114746 0.951782763004303
CurrentTrain: epoch  2, batch     0 | loss: 6.4040523Losses:  5.329590320587158 1.4455041885375977 0.9699796438217163
CurrentTrain: epoch  2, batch     1 | loss: 7.2600842Losses:  4.565826416015625 1.2209874391555786 0.9608322978019714
CurrentTrain: epoch  2, batch     2 | loss: 6.2672300Losses:  2.9943482875823975 0.06666448712348938 1.0240274667739868
CurrentTrain: epoch  2, batch     3 | loss: 3.5730264Losses:  4.834560394287109 1.326858401298523 0.9653632044792175
CurrentTrain: epoch  3, batch     0 | loss: 6.6441007Losses:  3.9956247806549072 1.0524815320968628 0.960533618927002
CurrentTrain: epoch  3, batch     1 | loss: 5.5283728Losses:  3.7667832374572754 1.1493773460388184 0.9564762115478516
CurrentTrain: epoch  3, batch     2 | loss: 5.3943987Losses:  4.964376926422119 0.5105803608894348 0.9396622180938721
CurrentTrain: epoch  3, batch     3 | loss: 5.9447885Losses:  3.6963822841644287 1.1020900011062622 0.9624970555305481
CurrentTrain: epoch  4, batch     0 | loss: 5.2797208Losses:  4.278509616851807 0.9880495667457581 0.9433329105377197
CurrentTrain: epoch  4, batch     1 | loss: 5.7382255Losses:  3.8169808387756348 1.2209924459457397 0.9636130928993225
CurrentTrain: epoch  4, batch     2 | loss: 5.5197802Losses:  1.7333860397338867 0.20578335225582123 0.8633757829666138
CurrentTrain: epoch  4, batch     3 | loss: 2.3708572Losses:  3.402651786804199 1.3063879013061523 0.9307661652565002
CurrentTrain: epoch  5, batch     0 | loss: 5.1744227Losses:  3.3790643215179443 0.9798155426979065 0.9474338293075562
CurrentTrain: epoch  5, batch     1 | loss: 4.8325968Losses:  3.804630756378174 1.0695586204528809 0.9651432037353516
CurrentTrain: epoch  5, batch     2 | loss: 5.3567610Losses:  5.455275058746338 0.543809175491333 0.9371211528778076
CurrentTrain: epoch  5, batch     3 | loss: 6.4676452Losses:  3.445298910140991 0.9787271618843079 0.9722428321838379
CurrentTrain: epoch  6, batch     0 | loss: 4.9101477Losses:  3.378755569458008 0.9573022723197937 0.9444093704223633
CurrentTrain: epoch  6, batch     1 | loss: 4.8082623Losses:  3.454385757446289 1.136168122291565 0.9330904483795166
CurrentTrain: epoch  6, batch     2 | loss: 5.0570989Losses:  2.5713536739349365 0.24792352318763733 0.8473498821258545
CurrentTrain: epoch  6, batch     3 | loss: 3.2429523Losses:  3.0372934341430664 1.0843307971954346 0.9537380337715149
CurrentTrain: epoch  7, batch     0 | loss: 4.5984931Losses:  3.3189961910247803 1.0620301961898804 0.9601075053215027
CurrentTrain: epoch  7, batch     1 | loss: 4.8610802Losses:  3.226719856262207 0.9674276113510132 0.9177689552307129
CurrentTrain: epoch  7, batch     2 | loss: 4.6530323Losses:  2.977491617202759 0.4356796443462372 0.900901198387146
CurrentTrain: epoch  7, batch     3 | loss: 3.8636220Losses:  2.679795265197754 1.0656201839447021 0.922001302242279
CurrentTrain: epoch  8, batch     0 | loss: 4.2064161Losses:  3.5707762241363525 1.002339482307434 0.918468713760376
CurrentTrain: epoch  8, batch     1 | loss: 5.0323501Losses:  3.0331804752349854 0.9189846515655518 0.9636030793190002
CurrentTrain: epoch  8, batch     2 | loss: 4.4339666Losses:  1.7869702577590942 0.12621252238750458 1.0
CurrentTrain: epoch  8, batch     3 | loss: 2.4131827Losses:  2.977478504180908 1.0258023738861084 0.9251126050949097
CurrentTrain: epoch  9, batch     0 | loss: 4.4658370Losses:  2.2565207481384277 0.8048591613769531 0.9124693870544434
CurrentTrain: epoch  9, batch     1 | loss: 3.5176146Losses:  3.2824599742889404 0.8982840776443481 0.964659571647644
CurrentTrain: epoch  9, batch     2 | loss: 4.6630740Losses:  2.230205535888672 0.21801449358463287 0.9623799920082092
CurrentTrain: epoch  9, batch     3 | loss: 2.9294100
Losses:  0.6709452867507935 0.7629533410072327 0.9071739912033081
MemoryTrain:  epoch  0, batch     0 | loss: 1.8874857Losses:  0.5042186975479126 1.1766142845153809 0.9455052614212036
MemoryTrain:  epoch  0, batch     1 | loss: 2.1535857Losses:  0.4440772235393524 0.8075370788574219 0.9138723015785217
MemoryTrain:  epoch  0, batch     2 | loss: 1.7085505Losses:  1.6620815992355347 0.08579052984714508 0.8167214393615723
MemoryTrain:  epoch  0, batch     3 | loss: 2.1562328Losses:  0.9791598320007324 0.9053005576133728 0.9287198781967163
MemoryTrain:  epoch  1, batch     0 | loss: 2.3488204Losses:  0.5211597681045532 1.0306978225708008 0.9156942963600159
MemoryTrain:  epoch  1, batch     1 | loss: 2.0097048Losses:  0.33922749757766724 0.8803507089614868 0.8860817551612854
MemoryTrain:  epoch  1, batch     2 | loss: 1.6626191Losses:  0.04213157296180725 0.029484692960977554 1.0
MemoryTrain:  epoch  1, batch     3 | loss: 0.5716163Losses:  0.5710029602050781 0.8850895166397095 0.8898684978485107
MemoryTrain:  epoch  2, batch     0 | loss: 1.9010267Losses:  0.18056608736515045 0.8385714292526245 0.886612594127655
MemoryTrain:  epoch  2, batch     1 | loss: 1.4624438Losses:  0.5172621011734009 0.7992572784423828 0.9715893268585205
MemoryTrain:  epoch  2, batch     2 | loss: 1.8023140Losses:  0.05623254179954529 0.48549598455429077 0.8429491519927979
MemoryTrain:  epoch  2, batch     3 | loss: 0.9632031Losses:  0.40327346324920654 0.7055003643035889 0.9485653638839722
MemoryTrain:  epoch  3, batch     0 | loss: 1.5830564Losses:  0.140335351228714 1.1061818599700928 0.8865626454353333
MemoryTrain:  epoch  3, batch     1 | loss: 1.6897985Losses:  0.4307868182659149 0.922478437423706 0.8985104560852051
MemoryTrain:  epoch  3, batch     2 | loss: 1.8025205Losses:  0.12837530672550201 0.18857534229755402 0.9432240128517151
MemoryTrain:  epoch  3, batch     3 | loss: 0.7885627Losses:  0.31639641523361206 0.7798503637313843 0.9249362349510193
MemoryTrain:  epoch  4, batch     0 | loss: 1.5587149Losses:  0.12961536645889282 0.957324743270874 0.9134032130241394
MemoryTrain:  epoch  4, batch     1 | loss: 1.5436417Losses:  0.1062232181429863 0.8572646975517273 0.8803662657737732
MemoryTrain:  epoch  4, batch     2 | loss: 1.4036710Losses:  0.21877767145633698 0.053355857729911804 1.0
MemoryTrain:  epoch  4, batch     3 | loss: 0.7721335Losses:  0.17446832358837128 0.650380551815033 0.9004489183425903
MemoryTrain:  epoch  5, batch     0 | loss: 1.2750733Losses:  0.09395672380924225 0.8966925144195557 0.9244185090065002
MemoryTrain:  epoch  5, batch     1 | loss: 1.4528584Losses:  0.09116272628307343 1.0600805282592773 0.9051773548126221
MemoryTrain:  epoch  5, batch     2 | loss: 1.6038319Losses:  0.014773646369576454 0.01701023429632187 0.9241083860397339
MemoryTrain:  epoch  5, batch     3 | loss: 0.4938381Losses:  0.15716949105262756 0.7743111848831177 0.920809268951416
MemoryTrain:  epoch  6, batch     0 | loss: 1.3918853Losses:  0.09292373061180115 0.8744788765907288 0.8669149875640869
MemoryTrain:  epoch  6, batch     1 | loss: 1.4008601Losses:  0.08238452672958374 0.8924698829650879 0.9317624568939209
MemoryTrain:  epoch  6, batch     2 | loss: 1.4407356Losses:  0.018754351884126663 0.04197746515274048 0.9449574947357178
MemoryTrain:  epoch  6, batch     3 | loss: 0.5332106Losses:  0.14211657643318176 0.9973488450050354 0.929011344909668
MemoryTrain:  epoch  7, batch     0 | loss: 1.6039711Losses:  0.05105415731668472 0.6798114776611328 0.8853233456611633
MemoryTrain:  epoch  7, batch     1 | loss: 1.1735274Losses:  0.08034618943929672 0.7937518358230591 0.9100797772407532
MemoryTrain:  epoch  7, batch     2 | loss: 1.3291379Losses:  0.018684817478060722 0.009974168613553047 0.9078760743141174
MemoryTrain:  epoch  7, batch     3 | loss: 0.4825970Losses:  0.0725448876619339 0.9289267063140869 0.8657703399658203
MemoryTrain:  epoch  8, batch     0 | loss: 1.4343568Losses:  0.06540657579898834 0.6825324296951294 0.9459749460220337
MemoryTrain:  epoch  8, batch     1 | loss: 1.2209265Losses:  0.07136517763137817 0.766682505607605 0.9036989212036133
MemoryTrain:  epoch  8, batch     2 | loss: 1.2898972Losses:  0.0503518283367157 0.1551852971315384 0.935193657875061
MemoryTrain:  epoch  8, batch     3 | loss: 0.6731340Losses:  0.06462061405181885 0.7726731300354004 0.8664765357971191
MemoryTrain:  epoch  9, batch     0 | loss: 1.2705320Losses:  0.05370311811566353 0.8542454838752747 0.8931639790534973
MemoryTrain:  epoch  9, batch     1 | loss: 1.3545306Losses:  0.09450329840183258 0.7277991771697998 0.958549439907074
MemoryTrain:  epoch  9, batch     2 | loss: 1.3015772Losses:  0.016126560047268867 0.005175500176846981 0.8835386037826538
MemoryTrain:  epoch  9, batch     3 | loss: 0.4630713
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 61.67%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 60.80%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 58.57%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 57.47%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 56.08%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 56.09%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 56.73%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 57.50%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 59.08%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 59.59%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 60.09%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 60.28%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 60.46%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 60.77%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 61.20%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.15%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.16%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.36%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 89.12%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 88.98%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 88.73%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 88.61%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.69%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.85%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 89.09%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 89.25%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.55%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 89.55%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.61%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.53%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 89.24%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 88.97%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.64%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.25%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 88.02%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 87.57%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 87.21%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 86.49%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 86.22%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 86.03%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 85.46%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 85.08%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 84.57%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 84.08%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 83.59%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.38%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 82.91%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 82.64%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 82.12%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 81.81%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 81.56%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.31%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 81.07%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 80.89%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 80.78%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 80.32%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 79.75%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.19%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 78.64%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.15%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 77.85%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.60%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.73%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 79.27%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.94%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 78.71%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 78.59%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 78.32%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 78.15%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.22%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 78.67%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 78.24%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 77.81%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 77.35%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 76.89%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 76.44%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 76.22%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 76.70%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 76.40%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 76.14%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.52%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.20%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 75.16%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.28%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 75.90%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 76.24%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 76.26%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 76.28%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 76.17%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 76.05%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 75.93%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.93%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 76.05%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 76.38%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 76.47%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 76.31%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 76.10%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 75.91%   [EVAL] batch:  199 | acc: 43.75%,  total acc: 75.75%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 75.68%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.68%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.65%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 75.46%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.43%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 75.36%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 75.27%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 75.15%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 74.97%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 74.82%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 74.65%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 75.25%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 75.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.68%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 76.58%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.63%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 77.15%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 77.02%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 76.96%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 76.90%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 76.77%   [EVAL] batch:  254 | acc: 37.50%,  total acc: 76.62%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 76.51%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 76.39%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 76.38%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 76.35%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 76.27%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 76.12%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 76.02%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 75.97%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 75.80%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 75.75%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 75.65%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 75.67%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 76.03%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 76.00%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 75.83%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 75.74%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 75.65%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 75.49%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 75.38%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 75.31%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 75.24%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 75.02%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 74.87%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 74.67%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 74.43%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 74.37%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 74.44%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 74.56%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.58%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.54%   
cur_acc:  ['0.9435', '0.7093', '0.7153', '0.7867', '0.6915']
his_acc:  ['0.9435', '0.8265', '0.7949', '0.7715', '0.7554']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  5.902064323425293 1.4159454107284546 0.9726297855377197
CurrentTrain: epoch  0, batch     0 | loss: 7.8043246Losses:  5.8186492919921875 1.2106194496154785 0.9679540395736694
CurrentTrain: epoch  0, batch     1 | loss: 7.5132456Losses:  6.79454231262207 1.4291489124298096 0.9545746445655823
CurrentTrain: epoch  0, batch     2 | loss: 8.7009783Losses:  4.9416422843933105 0.0 0.9041657447814941
CurrentTrain: epoch  0, batch     3 | loss: 5.3937254Losses:  5.908755302429199 1.5374610424041748 0.9657487869262695
CurrentTrain: epoch  1, batch     0 | loss: 7.9290910Losses:  4.99092435836792 1.2263636589050293 0.9496320486068726
CurrentTrain: epoch  1, batch     1 | loss: 6.6921039Losses:  4.14724588394165 1.2380366325378418 0.9559216499328613
CurrentTrain: epoch  1, batch     2 | loss: 5.8632431Losses:  4.850349426269531 0.35851460695266724 0.9434744119644165
CurrentTrain: epoch  1, batch     3 | loss: 5.6806016Losses:  4.409745216369629 1.1654274463653564 0.9393914937973022
CurrentTrain: epoch  2, batch     0 | loss: 6.0448680Losses:  5.037312984466553 1.2635607719421387 0.9525097012519836
CurrentTrain: epoch  2, batch     1 | loss: 6.7771287Losses:  3.9390926361083984 1.209040880203247 0.969286322593689
CurrentTrain: epoch  2, batch     2 | loss: 5.6327763Losses:  4.628225326538086 0.3651277720928192 0.9258750677108765
CurrentTrain: epoch  2, batch     3 | loss: 5.4562902Losses:  3.4177963733673096 1.1095490455627441 0.9658101797103882
CurrentTrain: epoch  3, batch     0 | loss: 5.0102506Losses:  4.598421096801758 1.2833218574523926 0.9500145316123962
CurrentTrain: epoch  3, batch     1 | loss: 6.3567500Losses:  4.2281365394592285 1.3653182983398438 0.945009171962738
CurrentTrain: epoch  3, batch     2 | loss: 6.0659595Losses:  5.253152847290039 0.2209654152393341 0.8982031345367432
CurrentTrain: epoch  3, batch     3 | loss: 5.9232197Losses:  3.234795093536377 0.9599993228912354 0.9366384744644165
CurrentTrain: epoch  4, batch     0 | loss: 4.6631141Losses:  4.7639055252075195 1.046868085861206 0.9477880597114563
CurrentTrain: epoch  4, batch     1 | loss: 6.2846680Losses:  3.8795361518859863 1.1652278900146484 0.9572699069976807
CurrentTrain: epoch  4, batch     2 | loss: 5.5233989Losses:  2.6279194355010986 0.3336963355541229 0.9723248481750488
CurrentTrain: epoch  4, batch     3 | loss: 3.4477782Losses:  3.393759250640869 1.1298691034317017 0.9444013237953186
CurrentTrain: epoch  5, batch     0 | loss: 4.9958291Losses:  4.118198871612549 0.8772373795509338 0.9670082330703735
CurrentTrain: epoch  5, batch     1 | loss: 5.4789405Losses:  3.195075273513794 1.1084437370300293 0.9315704107284546
CurrentTrain: epoch  5, batch     2 | loss: 4.7693043Losses:  3.420760154724121 0.14819470047950745 0.8697413206100464
CurrentTrain: epoch  5, batch     3 | loss: 4.0038257Losses:  2.6062071323394775 0.9633781909942627 0.9423275589942932
CurrentTrain: epoch  6, batch     0 | loss: 4.0407491Losses:  3.5890913009643555 1.1167199611663818 0.9357463121414185
CurrentTrain: epoch  6, batch     1 | loss: 5.1736846Losses:  3.4898440837860107 1.1570873260498047 0.9429135918617249
CurrentTrain: epoch  6, batch     2 | loss: 5.1183887Losses:  4.081385612487793 0.025901131331920624 0.9413389563560486
CurrentTrain: epoch  6, batch     3 | loss: 4.5779562Losses:  3.153172731399536 0.9740511178970337 0.930945634841919
CurrentTrain: epoch  7, batch     0 | loss: 4.5926967Losses:  3.220816135406494 1.0119919776916504 0.9447791576385498
CurrentTrain: epoch  7, batch     1 | loss: 4.7051978Losses:  2.6299753189086914 0.8068720102310181 0.9261917471885681
CurrentTrain: epoch  7, batch     2 | loss: 3.8999431Losses:  3.837362289428711 0.15655112266540527 0.969218909740448
CurrentTrain: epoch  7, batch     3 | loss: 4.4785228Losses:  2.602700710296631 0.6765859127044678 0.9306588172912598
CurrentTrain: epoch  8, batch     0 | loss: 3.7446160Losses:  2.415839672088623 0.8116323947906494 0.925655722618103
CurrentTrain: epoch  8, batch     1 | loss: 3.6903000Losses:  3.5868935585021973 1.0197618007659912 0.9423409104347229
CurrentTrain: epoch  8, batch     2 | loss: 5.0778255Losses:  1.8952832221984863 0.06800231337547302 0.9360601902008057
CurrentTrain: epoch  8, batch     3 | loss: 2.4313157Losses:  2.3794913291931152 0.7964431047439575 0.9365444183349609
CurrentTrain: epoch  9, batch     0 | loss: 3.6442065Losses:  2.3155059814453125 0.7824255228042603 0.9337942600250244
CurrentTrain: epoch  9, batch     1 | loss: 3.5648284Losses:  2.9722533226013184 0.8871649503707886 0.9302989840507507
CurrentTrain: epoch  9, batch     2 | loss: 4.3245678Losses:  5.484675884246826 0.43484997749328613 0.8979359865188599
CurrentTrain: epoch  9, batch     3 | loss: 6.3684940
Losses:  0.6018393039703369 0.8799519538879395 0.9230901598930359
MemoryTrain:  epoch  0, batch     0 | loss: 1.9433364Losses:  1.044086217880249 1.0956671237945557 0.9349339008331299
MemoryTrain:  epoch  0, batch     1 | loss: 2.6072202Losses:  0.476035475730896 0.8556486368179321 0.9201585650444031
MemoryTrain:  epoch  0, batch     2 | loss: 1.7917634Losses:  0.7123947143554688 0.738587498664856 0.8775633573532104
MemoryTrain:  epoch  0, batch     3 | loss: 1.8897638Losses:  1.2948338985443115 0.8444216847419739 0.9073036909103394
MemoryTrain:  epoch  1, batch     0 | loss: 2.5929074Losses:  0.5857466459274292 0.8713259100914001 0.915793776512146
MemoryTrain:  epoch  1, batch     1 | loss: 1.9149694Losses:  0.8690423965454102 0.9947303533554077 0.9284359812736511
MemoryTrain:  epoch  1, batch     2 | loss: 2.3279908Losses:  0.4129297733306885 0.7935508489608765 0.9102389216423035
MemoryTrain:  epoch  1, batch     3 | loss: 1.6616001Losses:  0.47298476099967957 0.9408265352249146 0.9075692892074585
MemoryTrain:  epoch  2, batch     0 | loss: 1.8675959Losses:  0.16186770796775818 0.8451299667358398 0.9188371896743774
MemoryTrain:  epoch  2, batch     1 | loss: 1.4664164Losses:  0.7540505528450012 0.789240837097168 0.9188452959060669
MemoryTrain:  epoch  2, batch     2 | loss: 2.0027139Losses:  0.7195400595664978 0.6479117274284363 0.9022607207298279
MemoryTrain:  epoch  2, batch     3 | loss: 1.8185822Losses:  0.41848069429397583 0.7648061513900757 0.8585754632949829
MemoryTrain:  epoch  3, batch     0 | loss: 1.6125746Losses:  0.2089734971523285 0.7964945435523987 0.9302523136138916
MemoryTrain:  epoch  3, batch     1 | loss: 1.4705942Losses:  0.4306192994117737 0.6681713461875916 0.9377689361572266
MemoryTrain:  epoch  3, batch     2 | loss: 1.5676751Losses:  0.33055365085601807 0.9657090902328491 0.9211020469665527
MemoryTrain:  epoch  3, batch     3 | loss: 1.7568138Losses:  0.4220491349697113 1.0980260372161865 0.9048275947570801
MemoryTrain:  epoch  4, batch     0 | loss: 1.9724890Losses:  0.19689789414405823 0.6458584666252136 0.9353244304656982
MemoryTrain:  epoch  4, batch     1 | loss: 1.3104186Losses:  0.10135560482740402 0.7177992463111877 0.904996395111084
MemoryTrain:  epoch  4, batch     2 | loss: 1.2716531Losses:  0.15916728973388672 0.6140903234481812 0.8916038274765015
MemoryTrain:  epoch  4, batch     3 | loss: 1.2190595Losses:  0.18180540204048157 0.6497892737388611 0.9213725328445435
MemoryTrain:  epoch  5, batch     0 | loss: 1.2922809Losses:  0.2145390510559082 0.9730304479598999 0.9051218032836914
MemoryTrain:  epoch  5, batch     1 | loss: 1.6401304Losses:  0.10198582708835602 0.7923503518104553 0.8837893009185791
MemoryTrain:  epoch  5, batch     2 | loss: 1.3362308Losses:  0.2099590301513672 0.6807788610458374 0.9345008730888367
MemoryTrain:  epoch  5, batch     3 | loss: 1.3579884Losses:  0.0899861678481102 0.7461921572685242 0.9038179516792297
MemoryTrain:  epoch  6, batch     0 | loss: 1.2880872Losses:  0.16594690084457397 0.8102909326553345 0.9139114618301392
MemoryTrain:  epoch  6, batch     1 | loss: 1.4331936Losses:  0.08279823511838913 0.8483116626739502 0.9044317007064819
MemoryTrain:  epoch  6, batch     2 | loss: 1.3833258Losses:  0.09332604706287384 0.6511389017105103 0.9097113609313965
MemoryTrain:  epoch  6, batch     3 | loss: 1.1993206Losses:  0.12992003560066223 0.72852623462677 0.8871657848358154
MemoryTrain:  epoch  7, batch     0 | loss: 1.3020291Losses:  0.09727954864501953 0.905809760093689 0.9131722450256348
MemoryTrain:  epoch  7, batch     1 | loss: 1.4596754Losses:  0.054549168795347214 0.7446911334991455 0.9137924909591675
MemoryTrain:  epoch  7, batch     2 | loss: 1.2561365Losses:  0.061919670552015305 0.6237937808036804 0.917773962020874
MemoryTrain:  epoch  7, batch     3 | loss: 1.1446004Losses:  0.04916729778051376 0.7544201016426086 0.8884193897247314
MemoryTrain:  epoch  8, batch     0 | loss: 1.2477970Losses:  0.06878200173377991 0.6825894117355347 0.9211663007736206
MemoryTrain:  epoch  8, batch     1 | loss: 1.2119546Losses:  0.055072054266929626 0.8861053586006165 0.8837271928787231
MemoryTrain:  epoch  8, batch     2 | loss: 1.3830410Losses:  0.1297205239534378 0.6668753623962402 0.9461294412612915
MemoryTrain:  epoch  8, batch     3 | loss: 1.2696606Losses:  0.0672484040260315 0.6845855712890625 0.8840435743331909
MemoryTrain:  epoch  9, batch     0 | loss: 1.1938558Losses:  0.0619061142206192 0.8894289135932922 0.8789851665496826
MemoryTrain:  epoch  9, batch     1 | loss: 1.3908277Losses:  0.04791487753391266 0.8308371901512146 0.9506964683532715
MemoryTrain:  epoch  9, batch     2 | loss: 1.3541002Losses:  0.05169379711151123 0.5873663425445557 0.9121350049972534
MemoryTrain:  epoch  9, batch     3 | loss: 1.0951276
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 39.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 50.35%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 68.90%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 69.87%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.56%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.36%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.78%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.47%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.69%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 88.14%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.02%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.60%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 87.40%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 86.62%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 86.35%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 85.82%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 85.05%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.53%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.64%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 85.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 85.61%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 85.31%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.26%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.13%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 84.80%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 84.53%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 84.11%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 83.78%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 83.31%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 82.92%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 82.18%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 81.96%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 81.81%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 81.46%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 81.18%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 80.98%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 80.52%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 80.13%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 79.69%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 79.08%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 78.79%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 78.38%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 78.09%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 77.88%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 77.67%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 77.52%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 77.38%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 76.99%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 76.45%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 75.92%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 75.40%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 74.94%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 74.61%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 74.39%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 75.15%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 74.95%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 74.61%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.32%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 73.99%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 73.76%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 73.82%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.97%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.07%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.21%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 74.37%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 73.83%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 73.35%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 72.87%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 72.36%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 71.90%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 72.31%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 72.04%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 71.73%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 71.51%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 71.17%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 70.83%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.97%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 71.68%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 71.82%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 71.69%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 71.52%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 71.22%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 71.10%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 70.97%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 70.98%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 72.04%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 71.83%   [EVAL] batch:  195 | acc: 25.00%,  total acc: 71.59%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 71.51%   [EVAL] batch:  197 | acc: 31.25%,  total acc: 71.31%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 71.11%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 70.91%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 70.93%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 70.80%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 70.79%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:  206 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 70.55%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 70.36%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 70.20%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 70.08%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 71.01%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 71.06%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 70.86%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 72.82%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 72.91%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 72.94%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 73.22%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 73.01%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 72.80%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 72.64%   [EVAL] batch:  254 | acc: 31.25%,  total acc: 72.48%   [EVAL] batch:  255 | acc: 37.50%,  total acc: 72.34%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 72.25%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.25%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 72.19%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 72.09%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 71.80%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 71.64%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:  275 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 72.00%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 71.86%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 71.74%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 71.64%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 71.59%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 71.49%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.26%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 70.85%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 70.60%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 70.53%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 70.75%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 70.64%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 70.32%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 71.11%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 71.16%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 71.14%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 70.97%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 70.81%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 70.64%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 70.52%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 70.40%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:  326 | acc: 12.50%,  total acc: 70.51%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 70.41%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 70.29%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 70.19%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 70.13%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 70.78%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 70.91%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 71.24%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 71.13%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 71.05%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 70.99%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 70.86%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 70.79%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 70.90%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 71.21%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 71.22%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 71.16%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.11%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 71.02%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  371 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 71.07%   
cur_acc:  ['0.9435', '0.7093', '0.7153', '0.7867', '0.6915', '0.6935']
his_acc:  ['0.9435', '0.8265', '0.7949', '0.7715', '0.7554', '0.7107']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  5.756369590759277 1.358425498008728 0.9682382345199585
CurrentTrain: epoch  0, batch     0 | loss: 7.5989141Losses:  7.305622100830078 1.460073471069336 0.9667872786521912
CurrentTrain: epoch  0, batch     1 | loss: 9.2490892Losses:  6.574646949768066 1.7066407203674316 0.9515484571456909
CurrentTrain: epoch  0, batch     2 | loss: 8.7570620Losses:  8.282754898071289 0.39473849534988403 0.9575833082199097
CurrentTrain: epoch  0, batch     3 | loss: 9.1562843Losses:  5.888683319091797 1.7397236824035645 0.9627482891082764
CurrentTrain: epoch  1, batch     0 | loss: 8.1097813Losses:  5.829074859619141 1.3956012725830078 0.9503840208053589
CurrentTrain: epoch  1, batch     1 | loss: 7.6998682Losses:  5.085357666015625 1.6882582902908325 0.9625355005264282
CurrentTrain: epoch  1, batch     2 | loss: 7.2548838Losses:  6.149416923522949 0.34085553884506226 0.9632887840270996
CurrentTrain: epoch  1, batch     3 | loss: 6.9719172Losses:  4.966832160949707 1.5347237586975098 0.9487496018409729
CurrentTrain: epoch  2, batch     0 | loss: 6.9759307Losses:  4.655478477478027 1.3432897329330444 0.9476034641265869
CurrentTrain: epoch  2, batch     1 | loss: 6.4725699Losses:  4.924310684204102 1.746894359588623 0.9551087617874146
CurrentTrain: epoch  2, batch     2 | loss: 7.1487594Losses:  6.2588300704956055 0.2943969964981079 0.9790218472480774
CurrentTrain: epoch  2, batch     3 | loss: 7.0427380Losses:  4.764837741851807 1.3677903413772583 0.9449673295021057
CurrentTrain: epoch  3, batch     0 | loss: 6.6051116Losses:  4.498317241668701 1.5198594331741333 0.9559450149536133
CurrentTrain: epoch  3, batch     1 | loss: 6.4961491Losses:  3.8921470642089844 1.3599928617477417 0.9434382915496826
CurrentTrain: epoch  3, batch     2 | loss: 5.7238593Losses:  4.633459091186523 0.40082696080207825 0.9068504571914673
CurrentTrain: epoch  3, batch     3 | loss: 5.4877114Losses:  4.616140365600586 1.530941367149353 0.9343158006668091
CurrentTrain: epoch  4, batch     0 | loss: 6.6142397Losses:  3.4127025604248047 1.20015549659729 0.9451370239257812
CurrentTrain: epoch  4, batch     1 | loss: 5.0854263Losses:  3.9775428771972656 1.4422833919525146 0.9424355030059814
CurrentTrain: epoch  4, batch     2 | loss: 5.8910441Losses:  5.989447593688965 0.46855008602142334 0.9934062957763672
CurrentTrain: epoch  4, batch     3 | loss: 6.9547009Losses:  3.899190664291382 1.1752548217773438 0.9637053608894348
CurrentTrain: epoch  5, batch     0 | loss: 5.5562983Losses:  3.703005790710449 1.3088202476501465 0.920066237449646
CurrentTrain: epoch  5, batch     1 | loss: 5.4718590Losses:  3.758385181427002 1.43082594871521 0.9322601556777954
CurrentTrain: epoch  5, batch     2 | loss: 5.6553411Losses:  3.731133460998535 0.4422936737537384 0.9662914276123047
CurrentTrain: epoch  5, batch     3 | loss: 4.6565728Losses:  3.883269786834717 1.2893180847167969 0.9360760450363159
CurrentTrain: epoch  6, batch     0 | loss: 5.6406260Losses:  4.09404182434082 1.1320865154266357 0.9412376284599304
CurrentTrain: epoch  6, batch     1 | loss: 5.6967473Losses:  2.693978786468506 0.9773821234703064 0.9293668270111084
CurrentTrain: epoch  6, batch     2 | loss: 4.1360445Losses:  4.924232482910156 0.34872299432754517 0.9484225511550903
CurrentTrain: epoch  6, batch     3 | loss: 5.7471666Losses:  2.8253469467163086 1.1189870834350586 0.9375973343849182
CurrentTrain: epoch  7, batch     0 | loss: 4.4131327Losses:  3.9048099517822266 1.1981396675109863 0.9362354278564453
CurrentTrain: epoch  7, batch     1 | loss: 5.5710673Losses:  3.185141086578369 1.3296897411346436 0.9269335269927979
CurrentTrain: epoch  7, batch     2 | loss: 4.9782972Losses:  4.6508073806762695 0.9244215488433838 0.9703840017318726
CurrentTrain: epoch  7, batch     3 | loss: 6.0604205Losses:  3.3342647552490234 1.2606441974639893 0.9171479940414429
CurrentTrain: epoch  8, batch     0 | loss: 5.0534825Losses:  3.155891180038452 0.995928168296814 0.938010573387146
CurrentTrain: epoch  8, batch     1 | loss: 4.6208243Losses:  3.227297067642212 1.3489582538604736 0.938590407371521
CurrentTrain: epoch  8, batch     2 | loss: 5.0455503Losses:  2.6248714923858643 0.3117848336696625 0.9795423746109009
CurrentTrain: epoch  8, batch     3 | loss: 3.4264274Losses:  2.818676710128784 1.2852481603622437 0.9130620956420898
CurrentTrain: epoch  9, batch     0 | loss: 4.5604558Losses:  3.4474902153015137 1.2294588088989258 0.9201157689094543
CurrentTrain: epoch  9, batch     1 | loss: 5.1370068Losses:  2.826897144317627 1.1262140274047852 0.9494656324386597
CurrentTrain: epoch  9, batch     2 | loss: 4.4278440Losses:  2.0871150493621826 0.5698921084403992 0.946365237236023
CurrentTrain: epoch  9, batch     3 | loss: 3.1301899
Losses:  0.6946066617965698 0.8900625109672546 0.9230313301086426
MemoryTrain:  epoch  0, batch     0 | loss: 2.0461848Losses:  0.31242284178733826 1.075119972229004 0.8932880163192749
MemoryTrain:  epoch  0, batch     1 | loss: 1.8341868Losses:  0.8957566022872925 1.067229151725769 0.9500573873519897
MemoryTrain:  epoch  0, batch     2 | loss: 2.4380145Losses:  0.2723286747932434 0.8932111859321594 0.9251639246940613
MemoryTrain:  epoch  0, batch     3 | loss: 1.6281219Losses:  0.06656840443611145 0.24298810958862305 0.7999841570854187
MemoryTrain:  epoch  0, batch     4 | loss: 0.7095486Losses:  0.6249378323554993 0.6953608989715576 0.9456532001495361
MemoryTrain:  epoch  1, batch     0 | loss: 1.7931253Losses:  0.8051395416259766 0.8090053796768188 0.9347652196884155
MemoryTrain:  epoch  1, batch     1 | loss: 2.0815275Losses:  0.34080225229263306 0.9103219509124756 0.9020493030548096
MemoryTrain:  epoch  1, batch     2 | loss: 1.7021488Losses:  0.9456255435943604 1.011093020439148 0.8642665147781372
MemoryTrain:  epoch  1, batch     3 | loss: 2.3888519Losses:  0.1747276782989502 0.5644868016242981 0.8688108325004578
MemoryTrain:  epoch  1, batch     4 | loss: 1.1736199Losses:  0.40585970878601074 0.7577716112136841 0.8763924241065979
MemoryTrain:  epoch  2, batch     0 | loss: 1.6018275Losses:  0.22928215563297272 0.8074071407318115 0.923599898815155
MemoryTrain:  epoch  2, batch     1 | loss: 1.4984893Losses:  0.15225066244602203 0.9376345872879028 0.8932802677154541
MemoryTrain:  epoch  2, batch     2 | loss: 1.5365254Losses:  0.4648900628089905 1.0165190696716309 0.9303482174873352
MemoryTrain:  epoch  2, batch     3 | loss: 1.9465832Losses:  0.8146259188652039 0.4089641273021698 0.929661750793457
MemoryTrain:  epoch  2, batch     4 | loss: 1.6884209Losses:  0.08572888374328613 0.9354314804077148 0.8778855800628662
MemoryTrain:  epoch  3, batch     0 | loss: 1.4601032Losses:  0.41807821393013 0.8036564588546753 0.8934099078178406
MemoryTrain:  epoch  3, batch     1 | loss: 1.6684396Losses:  0.10455284267663956 0.7645785808563232 0.9317003488540649
MemoryTrain:  epoch  3, batch     2 | loss: 1.3349817Losses:  0.2833040654659271 0.9522243738174438 0.9075834155082703
MemoryTrain:  epoch  3, batch     3 | loss: 1.6893202Losses:  0.21168506145477295 0.4205951690673828 0.9842015504837036
MemoryTrain:  epoch  3, batch     4 | loss: 1.1243811Losses:  0.06282038986682892 0.7378172278404236 0.9155738353729248
MemoryTrain:  epoch  4, batch     0 | loss: 1.2584245Losses:  0.26543641090393066 0.7669997811317444 0.9073635339736938
MemoryTrain:  epoch  4, batch     1 | loss: 1.4861178Losses:  0.15922830998897552 0.7380633354187012 0.9243643283843994
MemoryTrain:  epoch  4, batch     2 | loss: 1.3594738Losses:  0.13223493099212646 1.086306095123291 0.8958407640457153
MemoryTrain:  epoch  4, batch     3 | loss: 1.6664615Losses:  0.10516884177923203 0.5216754078865051 0.8542839884757996
MemoryTrain:  epoch  4, batch     4 | loss: 1.0539862Losses:  0.0935143530368805 0.7860920429229736 0.9024393558502197
MemoryTrain:  epoch  5, batch     0 | loss: 1.3308260Losses:  0.09877099096775055 0.9101768732070923 0.890053927898407
MemoryTrain:  epoch  5, batch     1 | loss: 1.4539748Losses:  0.0480409637093544 0.8429179787635803 0.9409548044204712
MemoryTrain:  epoch  5, batch     2 | loss: 1.3614364Losses:  0.05912763625383377 0.9038421511650085 0.9170886278152466
MemoryTrain:  epoch  5, batch     3 | loss: 1.4215140Losses:  0.029984910041093826 0.20228883624076843 0.8259072303771973
MemoryTrain:  epoch  5, batch     4 | loss: 0.6452274Losses:  0.057621754705905914 0.842958390712738 0.9063223600387573
MemoryTrain:  epoch  6, batch     0 | loss: 1.3537414Losses:  0.05922951549291611 0.6952699422836304 0.906990647315979
MemoryTrain:  epoch  6, batch     1 | loss: 1.2079947Losses:  0.13829249143600464 0.8290400505065918 0.8723044991493225
MemoryTrain:  epoch  6, batch     2 | loss: 1.4034848Losses:  0.07786145806312561 0.8125614523887634 0.9171004891395569
MemoryTrain:  epoch  6, batch     3 | loss: 1.3489732Losses:  0.04298434406518936 0.4781246781349182 0.9522806406021118
MemoryTrain:  epoch  6, batch     4 | loss: 0.9972494Losses:  0.05141931772232056 0.7722787857055664 0.902289867401123
MemoryTrain:  epoch  7, batch     0 | loss: 1.2748430Losses:  0.06779856979846954 0.9309984445571899 0.8689208626747131
MemoryTrain:  epoch  7, batch     1 | loss: 1.4332575Losses:  0.06569014489650726 0.7337958216667175 0.9126001596450806
MemoryTrain:  epoch  7, batch     2 | loss: 1.2557861Losses:  0.10361503809690475 0.6973609924316406 0.9289116859436035
MemoryTrain:  epoch  7, batch     3 | loss: 1.2654319Losses:  0.042763493955135345 0.21256765723228455 0.9019941687583923
MemoryTrain:  epoch  7, batch     4 | loss: 0.7063283Losses:  0.04640677943825722 0.662861168384552 0.9372529983520508
MemoryTrain:  epoch  8, batch     0 | loss: 1.1778945Losses:  0.08701427280902863 0.9414273500442505 0.8646173477172852
MemoryTrain:  epoch  8, batch     1 | loss: 1.4607503Losses:  0.07893657684326172 0.8679620027542114 0.8963927030563354
MemoryTrain:  epoch  8, batch     2 | loss: 1.3950949Losses:  0.1120942085981369 0.6493992805480957 0.8976171016693115
MemoryTrain:  epoch  8, batch     3 | loss: 1.2103021Losses:  0.09403064846992493 0.323478639125824 0.9123830795288086
MemoryTrain:  epoch  8, batch     4 | loss: 0.8737009Losses:  0.12859204411506653 0.8868722915649414 0.8743486404418945
MemoryTrain:  epoch  9, batch     0 | loss: 1.4526386Losses:  0.05414312332868576 0.6869298815727234 0.9258156418800354
MemoryTrain:  epoch  9, batch     1 | loss: 1.2039808Losses:  0.08313065767288208 0.8004279136657715 0.8796225786209106
MemoryTrain:  epoch  9, batch     2 | loss: 1.3233699Losses:  0.06306946277618408 0.7111545205116272 0.8957661390304565
MemoryTrain:  epoch  9, batch     3 | loss: 1.2221071Losses:  0.0402139388024807 0.27844226360321045 0.9680352210998535
MemoryTrain:  epoch  9, batch     4 | loss: 0.8026738
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 44.89%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 54.55%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 53.53%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 51.82%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 50.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 49.04%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 47.22%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 45.54%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 43.97%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 42.50%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 41.13%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.80%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 43.18%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 43.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 45.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 46.35%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 46.96%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 48.19%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 49.20%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 51.52%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 52.23%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 53.20%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 53.98%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 54.58%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 54.89%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 55.45%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 56.12%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 56.89%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 57.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 57.48%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 58.05%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 58.49%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.03%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 59.20%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 59.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 59.76%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 59.48%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 59.69%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.53%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 59.68%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.13%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.30%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.11%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.22%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 88.21%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 86.82%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 86.18%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 85.45%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 85.06%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.68%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 83.89%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 83.46%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 82.95%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 82.46%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.88%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.38%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 82.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 82.65%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 82.22%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.05%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 81.72%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 81.48%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 80.87%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 80.73%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 80.29%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 80.01%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 79.31%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 79.14%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 78.74%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 78.49%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 77.63%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 77.21%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 76.72%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 76.58%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 76.06%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 75.74%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 75.30%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 75.24%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 74.65%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 74.13%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 73.62%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 73.12%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 72.69%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 72.32%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 73.54%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 73.45%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 73.46%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 73.39%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 73.25%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 72.87%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 72.49%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 72.17%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 71.78%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 71.52%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 71.90%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 72.06%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 71.54%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 71.12%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 70.70%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 70.20%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 69.76%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 69.49%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 70.28%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 69.98%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 69.65%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 69.48%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 68.83%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 68.98%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 68.56%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 68.15%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 67.78%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 67.37%   [EVAL] batch:  168 | acc: 18.75%,  total acc: 67.09%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 67.02%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 67.00%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.69%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 66.61%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 66.51%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 66.36%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 66.32%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  194 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 67.28%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 67.08%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 66.87%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 66.72%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 66.64%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 65.92%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 65.76%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 65.57%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 66.72%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 66.73%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 66.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 68.31%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 68.31%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.88%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 68.85%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 68.68%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.56%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.44%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 68.23%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 68.64%   [EVAL] batch:  274 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 68.66%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 68.52%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 68.35%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 68.26%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 68.20%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 67.89%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 67.70%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 67.27%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 67.08%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 66.89%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 66.66%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 66.43%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 66.20%   [EVAL] batch:  292 | acc: 0.00%,  total acc: 65.98%   [EVAL] batch:  293 | acc: 25.00%,  total acc: 65.84%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 65.74%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 65.61%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 65.51%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 65.52%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 66.36%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 66.40%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 66.49%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 66.69%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 66.60%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 66.34%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 66.21%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 66.10%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 66.40%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 66.39%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 66.11%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 66.66%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 66.99%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 67.38%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 67.26%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 67.11%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 66.99%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 66.89%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 67.32%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 67.29%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 67.20%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 67.17%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 67.04%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 66.97%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 67.02%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 66.87%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 66.74%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 66.58%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 66.46%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 66.16%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 66.68%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  394 | acc: 12.50%,  total acc: 66.63%   [EVAL] batch:  395 | acc: 6.25%,  total acc: 66.48%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 66.33%   [EVAL] batch:  397 | acc: 31.25%,  total acc: 66.24%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 66.10%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 66.00%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.85%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 65.69%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 65.36%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 65.20%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.04%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 65.03%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 65.10%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.34%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.53%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 65.68%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 65.69%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  422 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  425 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 66.01%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 65.97%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 65.88%   
cur_acc:  ['0.9435', '0.7093', '0.7153', '0.7867', '0.6915', '0.6935', '0.5913']
his_acc:  ['0.9435', '0.8265', '0.7949', '0.7715', '0.7554', '0.7107', '0.6588']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  6.085197448730469 1.5093917846679688 0.9808337688446045
CurrentTrain: epoch  0, batch     0 | loss: 8.0850058Losses:  6.217804908752441 1.4766831398010254 0.9584991335868835
CurrentTrain: epoch  0, batch     1 | loss: 8.1737375Losses:  5.776707649230957 1.2701590061187744 0.9767528772354126
CurrentTrain: epoch  0, batch     2 | loss: 7.5352430Losses:  5.921726226806641 0.233134463429451 1.0
CurrentTrain: epoch  0, batch     3 | loss: 6.6548605Losses:  5.383118152618408 1.0447025299072266 0.9943904280662537
CurrentTrain: epoch  1, batch     0 | loss: 6.9250159Losses:  5.188260078430176 1.3461486101150513 0.9659081697463989
CurrentTrain: epoch  1, batch     1 | loss: 7.0173626Losses:  5.325039386749268 1.2014496326446533 0.9411313533782959
CurrentTrain: epoch  1, batch     2 | loss: 6.9970551Losses:  2.795062303543091 5.960464477539063e-08 1.0
CurrentTrain: epoch  1, batch     3 | loss: 3.2950623Losses:  4.710409641265869 1.194989562034607 0.9483773112297058
CurrentTrain: epoch  2, batch     0 | loss: 6.3795881Losses:  5.058696746826172 1.0753670930862427 0.986006498336792
CurrentTrain: epoch  2, batch     1 | loss: 6.6270671Losses:  4.6957268714904785 1.054754376411438 0.9556422829627991
CurrentTrain: epoch  2, batch     2 | loss: 6.2283025Losses:  3.5083489418029785 0.3284347653388977 1.0
CurrentTrain: epoch  2, batch     3 | loss: 4.3367834Losses:  4.666051864624023 1.2145953178405762 0.9569050073623657
CurrentTrain: epoch  3, batch     0 | loss: 6.3590999Losses:  4.289487361907959 1.0168232917785645 0.9567439556121826
CurrentTrain: epoch  3, batch     1 | loss: 5.7846828Losses:  4.119327545166016 1.0581905841827393 0.9732864499092102
CurrentTrain: epoch  3, batch     2 | loss: 5.6641612Losses:  5.687938690185547 0.92097407579422 0.9960049390792847
CurrentTrain: epoch  3, batch     3 | loss: 7.1069155Losses:  4.047593116760254 0.982299268245697 0.9643211960792542
CurrentTrain: epoch  4, batch     0 | loss: 5.5120530Losses:  3.530184268951416 1.0522329807281494 0.9573829174041748
CurrentTrain: epoch  4, batch     1 | loss: 5.0611091Losses:  4.548501968383789 1.2876464128494263 0.963675856590271
CurrentTrain: epoch  4, batch     2 | loss: 6.3179860Losses:  6.284275054931641 0.1910124123096466 0.9636596441268921
CurrentTrain: epoch  4, batch     3 | loss: 6.9571171Losses:  3.9926514625549316 1.212733507156372 0.9698768854141235
CurrentTrain: epoch  5, batch     0 | loss: 5.6903238Losses:  4.750658988952637 0.8417731523513794 0.9475366473197937
CurrentTrain: epoch  5, batch     1 | loss: 6.0662003Losses:  3.716977119445801 0.8138456344604492 0.9630025625228882
CurrentTrain: epoch  5, batch     2 | loss: 5.0123239Losses:  2.060074806213379 0.1630602478981018 0.9627143144607544
CurrentTrain: epoch  5, batch     3 | loss: 2.7044921Losses:  4.244973182678223 1.2482961416244507 0.968314528465271
CurrentTrain: epoch  6, batch     0 | loss: 5.9774265Losses:  3.6766610145568848 0.9327139258384705 0.9555308818817139
CurrentTrain: epoch  6, batch     1 | loss: 5.0871406Losses:  3.1821539402008057 0.6953079104423523 0.9539183974266052
CurrentTrain: epoch  6, batch     2 | loss: 4.3544211Losses:  2.6854662895202637 0.3576914072036743 0.9675483703613281
CurrentTrain: epoch  6, batch     3 | loss: 3.5269318Losses:  3.5759623050689697 1.0011520385742188 0.9503233432769775
CurrentTrain: epoch  7, batch     0 | loss: 5.0522757Losses:  3.2161245346069336 0.7761522531509399 0.9593944549560547
CurrentTrain: epoch  7, batch     1 | loss: 4.4719739Losses:  3.2234249114990234 1.0384173393249512 0.958946943283081
CurrentTrain: epoch  7, batch     2 | loss: 4.7413158Losses:  2.7819314002990723 0.09909480810165405 1.0
CurrentTrain: epoch  7, batch     3 | loss: 3.3810263Losses:  3.4348583221435547 0.954227089881897 0.9321931004524231
CurrentTrain: epoch  8, batch     0 | loss: 4.8551817Losses:  3.1684868335723877 0.7719072103500366 0.9823020696640015
CurrentTrain: epoch  8, batch     1 | loss: 4.4315448Losses:  2.626499652862549 0.8728571534156799 0.9496301412582397
CurrentTrain: epoch  8, batch     2 | loss: 3.9741719Losses:  3.884704113006592 0.1701195240020752 0.9397818446159363
CurrentTrain: epoch  8, batch     3 | loss: 4.5247149Losses:  2.3996331691741943 0.6920473575592041 0.9525938034057617
CurrentTrain: epoch  9, batch     0 | loss: 3.5679774Losses:  2.904237747192383 1.067519187927246 0.9466478824615479
CurrentTrain: epoch  9, batch     1 | loss: 4.4450808Losses:  2.7709546089172363 0.8614164590835571 0.957634687423706
CurrentTrain: epoch  9, batch     2 | loss: 4.1111884Losses:  2.52809476852417 0.15941625833511353 0.9608180522918701
CurrentTrain: epoch  9, batch     3 | loss: 3.1679201
Losses:  0.26215770840644836 0.759696900844574 0.8536571264266968
MemoryTrain:  epoch  0, batch     0 | loss: 1.4486833Losses:  0.5130917429924011 0.9063071012496948 0.9670171737670898
MemoryTrain:  epoch  0, batch     1 | loss: 1.9029074Losses:  0.4747921824455261 1.111999273300171 0.8857289552688599
MemoryTrain:  epoch  0, batch     2 | loss: 2.0296559Losses:  0.16473710536956787 0.8680337071418762 0.8694462776184082
MemoryTrain:  epoch  0, batch     3 | loss: 1.4674940Losses:  0.5970690250396729 0.7951838374137878 0.9461297988891602
MemoryTrain:  epoch  0, batch     4 | loss: 1.8653178Losses:  1.0391082763671875 0.742314338684082 0.9189896583557129
MemoryTrain:  epoch  1, batch     0 | loss: 2.2409174Losses:  0.41527992486953735 0.6978128552436829 0.9516435861587524
MemoryTrain:  epoch  1, batch     1 | loss: 1.5889146Losses:  0.3513900339603424 0.7776401042938232 0.9276564717292786
MemoryTrain:  epoch  1, batch     2 | loss: 1.5928583Losses:  0.5161491632461548 1.0172390937805176 0.88707435131073
MemoryTrain:  epoch  1, batch     3 | loss: 1.9769254Losses:  0.3504085838794708 0.8056115508079529 0.8434041738510132
MemoryTrain:  epoch  1, batch     4 | loss: 1.5777223Losses:  0.38092678785324097 0.7952768206596375 0.9063966274261475
MemoryTrain:  epoch  2, batch     0 | loss: 1.6294019Losses:  0.4749920070171356 1.0036972761154175 0.9060254693031311
MemoryTrain:  epoch  2, batch     1 | loss: 1.9317020Losses:  0.24246785044670105 0.800858736038208 0.9183712005615234
MemoryTrain:  epoch  2, batch     2 | loss: 1.5025122Losses:  0.14487425982952118 0.8677040338516235 0.8782912492752075
MemoryTrain:  epoch  2, batch     3 | loss: 1.4517238Losses:  0.12926211953163147 0.6099846363067627 0.9161281585693359
MemoryTrain:  epoch  2, batch     4 | loss: 1.1973108Losses:  0.21333542466163635 0.8644334673881531 0.8912545442581177
MemoryTrain:  epoch  3, batch     0 | loss: 1.5233963Losses:  0.11487266421318054 0.9127798080444336 0.8570769429206848
MemoryTrain:  epoch  3, batch     1 | loss: 1.4561909Losses:  0.11475399136543274 0.6681434512138367 0.9135746955871582
MemoryTrain:  epoch  3, batch     2 | loss: 1.2396848Losses:  0.25819000601768494 0.7797129154205322 0.9253535270690918
MemoryTrain:  epoch  3, batch     3 | loss: 1.5005797Losses:  0.14615659415721893 0.7249702215194702 0.9342387914657593
MemoryTrain:  epoch  3, batch     4 | loss: 1.3382462Losses:  0.10684240609407425 0.7445574998855591 0.889400839805603
MemoryTrain:  epoch  4, batch     0 | loss: 1.2961004Losses:  0.12023237347602844 0.7240549325942993 0.8548173904418945
MemoryTrain:  epoch  4, batch     1 | loss: 1.2716960Losses:  0.1893610954284668 0.6488932371139526 0.9084591865539551
MemoryTrain:  epoch  4, batch     2 | loss: 1.2924839Losses:  0.1200772151350975 0.8104461431503296 0.9479681849479675
MemoryTrain:  epoch  4, batch     3 | loss: 1.4045074Losses:  0.15086454153060913 0.9083870649337769 0.9065250158309937
MemoryTrain:  epoch  4, batch     4 | loss: 1.5125141Losses:  0.09803634881973267 0.5360265374183655 0.9101227521896362
MemoryTrain:  epoch  5, batch     0 | loss: 1.0891242Losses:  0.049864500761032104 0.8351930379867554 0.8669917583465576
MemoryTrain:  epoch  5, batch     1 | loss: 1.3185534Losses:  0.06291551887989044 0.8067798614501953 0.8897380232810974
MemoryTrain:  epoch  5, batch     2 | loss: 1.3145643Losses:  0.13974878191947937 0.8497035503387451 0.9089910984039307
MemoryTrain:  epoch  5, batch     3 | loss: 1.4439479Losses:  0.1297934353351593 0.7450253963470459 0.9333117008209229
MemoryTrain:  epoch  5, batch     4 | loss: 1.3414747Losses:  0.10265372693538666 0.6706709861755371 0.9238129258155823
MemoryTrain:  epoch  6, batch     0 | loss: 1.2352312Losses:  0.05099954828619957 0.5831936597824097 0.8774331212043762
MemoryTrain:  epoch  6, batch     1 | loss: 1.0729097Losses:  0.08261753618717194 0.7334963083267212 0.9407578706741333
MemoryTrain:  epoch  6, batch     2 | loss: 1.2864928Losses:  0.10755002498626709 0.9357638955116272 0.8667144179344177
MemoryTrain:  epoch  6, batch     3 | loss: 1.4766712Losses:  0.056353580206632614 0.7716675996780396 0.8965970277786255
MemoryTrain:  epoch  6, batch     4 | loss: 1.2763197Losses:  0.08046577870845795 0.8662282228469849 0.8425477743148804
MemoryTrain:  epoch  7, batch     0 | loss: 1.3679678Losses:  0.07959163933992386 0.7352832555770874 0.92006516456604
MemoryTrain:  epoch  7, batch     1 | loss: 1.2749075Losses:  0.05287850275635719 0.6968451142311096 0.9182027578353882
MemoryTrain:  epoch  7, batch     2 | loss: 1.2088250Losses:  0.06863658875226974 0.8477929830551147 0.907603919506073
MemoryTrain:  epoch  7, batch     3 | loss: 1.3702315Losses:  0.07639217376708984 0.5453280210494995 0.9104538559913635
MemoryTrain:  epoch  7, batch     4 | loss: 1.0769471Losses:  0.08067646622657776 0.7349148392677307 0.9587432742118835
MemoryTrain:  epoch  8, batch     0 | loss: 1.2949630Losses:  0.04862261191010475 0.741714358329773 0.8328422904014587
MemoryTrain:  epoch  8, batch     1 | loss: 1.2067581Losses:  0.18307894468307495 0.6645117998123169 0.9516587257385254
MemoryTrain:  epoch  8, batch     2 | loss: 1.3234200Losses:  0.04258223623037338 0.7470383048057556 0.8843405842781067
MemoryTrain:  epoch  8, batch     3 | loss: 1.2317908Losses:  0.057659368962049484 0.7261650562286377 0.8706125020980835
MemoryTrain:  epoch  8, batch     4 | loss: 1.2191308Losses:  0.07257861644029617 0.6773620843887329 0.8980860710144043
MemoryTrain:  epoch  9, batch     0 | loss: 1.1989837Losses:  0.10954587161540985 0.649869978427887 0.8903077244758606
MemoryTrain:  epoch  9, batch     1 | loss: 1.2045697Losses:  0.07563786208629608 1.0020179748535156 0.8919731378555298
MemoryTrain:  epoch  9, batch     2 | loss: 1.5236423Losses:  0.05193405598402023 0.5668421387672424 0.929957389831543
MemoryTrain:  epoch  9, batch     3 | loss: 1.0837549Losses:  0.05831675976514816 0.7200183868408203 0.8718727827072144
MemoryTrain:  epoch  9, batch     4 | loss: 1.2142715
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 66.85%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.35%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.64%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 75.74%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 75.28%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 74.05%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 73.14%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 72.66%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 73.36%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 73.52%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 72.92%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 72.85%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 72.38%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 71.73%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 86.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.91%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 85.31%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 84.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 83.91%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.67%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.04%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 82.13%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 81.06%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 80.02%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 79.38%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 78.49%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 77.90%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 78.56%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 78.77%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 78.97%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 78.73%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 78.59%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 78.35%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 78.01%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 77.75%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 76.96%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 76.29%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 76.12%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 75.75%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 75.40%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 74.54%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 74.15%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 73.72%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 73.19%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 72.96%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 72.57%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 72.54%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 72.38%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 72.41%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 72.02%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 71.04%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 70.57%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 70.16%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 71.33%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 71.60%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 71.50%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 71.23%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 70.56%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 70.35%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 70.10%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 69.85%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 70.02%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 70.01%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 70.09%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 69.74%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.29%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 68.88%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.40%   [EVAL] batch:  142 | acc: 0.00%,  total acc: 67.92%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 67.62%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 68.46%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 68.13%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 67.81%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 67.30%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 66.91%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 66.92%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 67.56%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 67.15%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 66.74%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 66.34%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 65.98%   [EVAL] batch:  167 | acc: 0.00%,  total acc: 65.59%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 65.35%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 65.19%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 65.07%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 65.09%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 65.07%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 64.77%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 64.36%   [EVAL] batch:  178 | acc: 25.00%,  total acc: 64.14%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 63.92%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 63.98%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 64.07%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.23%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 64.54%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 65.29%   [EVAL] batch:  195 | acc: 18.75%,  total acc: 65.05%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 64.94%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 64.80%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 64.60%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 64.47%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 64.43%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 64.33%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 63.95%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 63.76%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 63.70%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 63.57%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 63.42%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 63.24%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 64.41%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.41%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 64.43%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 64.31%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 64.59%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  238 | acc: 37.50%,  total acc: 65.87%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 65.69%   [EVAL] batch:  241 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 65.53%   [EVAL] batch:  243 | acc: 56.25%,  total acc: 65.50%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.81%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 66.19%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 65.87%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 65.72%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 65.61%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 65.49%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 65.50%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 65.96%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 66.00%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 65.91%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 65.80%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 65.73%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  282 | acc: 43.75%,  total acc: 65.59%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 65.38%   [EVAL] batch:  284 | acc: 12.50%,  total acc: 65.20%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 65.01%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 64.79%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 64.61%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 64.20%   [EVAL] batch:  290 | acc: 6.25%,  total acc: 64.00%   [EVAL] batch:  291 | acc: 6.25%,  total acc: 63.81%   [EVAL] batch:  292 | acc: 12.50%,  total acc: 63.63%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 63.52%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 63.43%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 63.39%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 63.32%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 64.01%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 64.08%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 64.23%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 64.35%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 64.23%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 64.08%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 63.94%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 63.84%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 63.73%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 63.79%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 64.08%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 64.15%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 64.00%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 63.82%   [EVAL] batch:  327 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:  328 | acc: 6.25%,  total acc: 63.45%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:  330 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 63.29%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 63.49%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 63.86%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 63.90%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 63.98%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 64.00%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 64.15%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 64.34%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 64.39%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  350 | acc: 0.00%,  total acc: 64.48%   [EVAL] batch:  351 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:  352 | acc: 6.25%,  total acc: 64.13%   [EVAL] batch:  353 | acc: 0.00%,  total acc: 63.95%   [EVAL] batch:  354 | acc: 0.00%,  total acc: 63.77%   [EVAL] batch:  355 | acc: 0.00%,  total acc: 63.59%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 63.70%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 63.95%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 64.05%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 63.99%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 63.94%   [EVAL] batch:  365 | acc: 31.25%,  total acc: 63.85%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 63.83%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 63.81%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 63.70%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 63.70%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 63.68%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 63.72%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 63.60%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 63.49%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 63.36%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 63.28%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 63.16%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 63.01%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 63.01%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 63.15%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 63.34%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 63.45%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 63.66%   [EVAL] batch:  392 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 63.77%   [EVAL] batch:  394 | acc: 18.75%,  total acc: 63.66%   [EVAL] batch:  395 | acc: 6.25%,  total acc: 63.51%   [EVAL] batch:  396 | acc: 0.00%,  total acc: 63.35%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 63.25%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 63.13%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 63.03%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 62.89%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 62.73%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 62.58%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 62.42%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 62.27%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 62.12%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 62.12%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 62.18%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 62.44%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.59%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:  418 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 62.93%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 62.96%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 62.99%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 63.06%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 63.09%   [EVAL] batch:  425 | acc: 37.50%,  total acc: 63.03%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 63.09%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 63.17%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 63.23%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 63.18%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 63.15%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 63.17%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:  438 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:  439 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  440 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:  441 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:  442 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 63.22%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 63.24%   [EVAL] batch:  446 | acc: 25.00%,  total acc: 63.16%   [EVAL] batch:  447 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:  448 | acc: 43.75%,  total acc: 63.10%   [EVAL] batch:  449 | acc: 62.50%,  total acc: 63.10%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 63.17%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 63.25%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 63.37%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 63.43%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 63.43%   [EVAL] batch:  458 | acc: 50.00%,  total acc: 63.40%   [EVAL] batch:  459 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 63.31%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 63.34%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:  465 | acc: 87.50%,  total acc: 63.55%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 64.07%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  475 | acc: 68.75%,  total acc: 64.23%   [EVAL] batch:  476 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  478 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 64.30%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 64.23%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 64.18%   [EVAL] batch:  484 | acc: 31.25%,  total acc: 64.11%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 64.08%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 64.08%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 64.37%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 64.32%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:  496 | acc: 68.75%,  total acc: 64.37%   [EVAL] batch:  497 | acc: 37.50%,  total acc: 64.32%   [EVAL] batch:  498 | acc: 62.50%,  total acc: 64.32%   [EVAL] batch:  499 | acc: 56.25%,  total acc: 64.30%   
cur_acc:  ['0.9435', '0.7093', '0.7153', '0.7867', '0.6915', '0.6935', '0.5913', '0.7173']
his_acc:  ['0.9435', '0.8265', '0.7949', '0.7715', '0.7554', '0.7107', '0.6588', '0.6430']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  10.395576477050781 2.1285247802734375 1.0004911422729492
CurrentTrain: epoch  0, batch     0 | loss: 13.0243473Losses:  9.956295013427734 2.196631669998169 1.0014135837554932
CurrentTrain: epoch  0, batch     1 | loss: 12.6536331Losses:  9.733768463134766 1.714848279953003 0.9798377752304077
CurrentTrain: epoch  0, batch     2 | loss: 11.9385357Losses:  10.035991668701172 2.023122787475586 1.0016679763793945
CurrentTrain: epoch  0, batch     3 | loss: 12.5599480Losses:  9.93391227722168 1.7209677696228027 0.9913440346717834
CurrentTrain: epoch  0, batch     4 | loss: 12.1505527Losses:  9.193528175354004 1.8677618503570557 0.9909446239471436
CurrentTrain: epoch  0, batch     5 | loss: 11.5567617Losses:  9.599172592163086 1.6330907344818115 0.996560275554657
CurrentTrain: epoch  0, batch     6 | loss: 11.7305441Losses:  9.25622844696045 1.5588054656982422 0.9826902151107788
CurrentTrain: epoch  0, batch     7 | loss: 11.3063793Losses:  9.522754669189453 1.6419860124588013 0.980804443359375
CurrentTrain: epoch  0, batch     8 | loss: 11.6551428Losses:  8.963064193725586 1.7727783918380737 0.9728052616119385
CurrentTrain: epoch  0, batch     9 | loss: 11.2222452Losses:  9.48396110534668 1.6594125032424927 0.98051917552948
CurrentTrain: epoch  0, batch    10 | loss: 11.6336327Losses:  8.662195205688477 1.6269757747650146 0.97948157787323
CurrentTrain: epoch  0, batch    11 | loss: 10.7789116Losses:  8.811336517333984 1.6300653219223022 0.9919081926345825
CurrentTrain: epoch  0, batch    12 | loss: 10.9373560Losses:  8.714810371398926 1.637284278869629 0.9675820469856262
CurrentTrain: epoch  0, batch    13 | loss: 10.8358860Losses:  9.109260559082031 1.3083549737930298 0.9721593856811523
CurrentTrain: epoch  0, batch    14 | loss: 10.9036961Losses:  8.216911315917969 1.4264278411865234 0.9557162523269653
CurrentTrain: epoch  0, batch    15 | loss: 10.1211977Losses:  8.614368438720703 1.6105189323425293 0.9758702516555786
CurrentTrain: epoch  0, batch    16 | loss: 10.7128229Losses:  8.471302032470703 1.42792809009552 0.9650842547416687
CurrentTrain: epoch  0, batch    17 | loss: 10.3817720Losses:  8.849342346191406 1.4632508754730225 0.9706050157546997
CurrentTrain: epoch  0, batch    18 | loss: 10.7978964Losses:  7.970617771148682 1.4948920011520386 0.961665153503418
CurrentTrain: epoch  0, batch    19 | loss: 9.9463425Losses:  8.095954895019531 1.4218392372131348 0.9551914930343628
CurrentTrain: epoch  0, batch    20 | loss: 9.9953890Losses:  8.5332612991333 1.3091588020324707 0.9778730273246765
CurrentTrain: epoch  0, batch    21 | loss: 10.3313570Losses:  7.897628307342529 1.1510591506958008 0.9537907242774963
CurrentTrain: epoch  0, batch    22 | loss: 9.5255823Losses:  8.031171798706055 1.3329284191131592 0.9729868173599243
CurrentTrain: epoch  0, batch    23 | loss: 9.8505936Losses:  8.603229522705078 1.4911311864852905 0.9527376890182495
CurrentTrain: epoch  0, batch    24 | loss: 10.5707293Losses:  7.969921112060547 1.1942025423049927 0.9379714131355286
CurrentTrain: epoch  0, batch    25 | loss: 9.6331091Losses:  8.609895706176758 1.450042963027954 0.9588826894760132
CurrentTrain: epoch  0, batch    26 | loss: 10.5393801Losses:  8.579766273498535 1.6625497341156006 0.9750248193740845
CurrentTrain: epoch  0, batch    27 | loss: 10.7298288Losses:  8.303727149963379 1.3617823123931885 0.9669525623321533
CurrentTrain: epoch  0, batch    28 | loss: 10.1489859Losses:  8.015462875366211 1.1770384311676025 0.9586131572723389
CurrentTrain: epoch  0, batch    29 | loss: 9.6718073Losses:  7.986114978790283 1.3279690742492676 0.9639291763305664
CurrentTrain: epoch  0, batch    30 | loss: 9.7960491Losses:  8.186838150024414 1.707817554473877 0.9585804343223572
CurrentTrain: epoch  0, batch    31 | loss: 10.3739452Losses:  8.504924774169922 1.4626266956329346 0.9720290899276733
CurrentTrain: epoch  0, batch    32 | loss: 10.4535656Losses:  8.01252555847168 1.361076831817627 0.9645092487335205
CurrentTrain: epoch  0, batch    33 | loss: 9.8558569Losses:  7.753860950469971 1.2570216655731201 0.9372999668121338
CurrentTrain: epoch  0, batch    34 | loss: 9.4795322Losses:  7.371417045593262 1.1759750843048096 0.9558454751968384
CurrentTrain: epoch  0, batch    35 | loss: 9.0253143Losses:  8.245129585266113 1.2088382244110107 0.9710783958435059
CurrentTrain: epoch  0, batch    36 | loss: 9.9395075Losses:  7.5501790046691895 1.3434898853302002 0.938705563545227
CurrentTrain: epoch  0, batch    37 | loss: 9.3630219Losses:  7.933022499084473 1.224899172782898 0.9450951814651489
CurrentTrain: epoch  0, batch    38 | loss: 9.6304693Losses:  8.088287353515625 1.3161277770996094 0.9539105892181396
CurrentTrain: epoch  0, batch    39 | loss: 9.8813705Losses:  7.723760604858398 1.3154163360595703 0.9391725063323975
CurrentTrain: epoch  0, batch    40 | loss: 9.5087633Losses:  7.696824550628662 1.255016565322876 0.9574072360992432
CurrentTrain: epoch  0, batch    41 | loss: 9.4305449Losses:  7.488297462463379 1.056232213973999 0.9307019710540771
CurrentTrain: epoch  0, batch    42 | loss: 9.0098810Losses:  7.777862071990967 1.0298467874526978 0.9636728763580322
CurrentTrain: epoch  0, batch    43 | loss: 9.2895451Losses:  7.45727014541626 1.1381498575210571 0.9405835866928101
CurrentTrain: epoch  0, batch    44 | loss: 9.0657120Losses:  7.77041482925415 1.1131699085235596 0.9463266134262085
CurrentTrain: epoch  0, batch    45 | loss: 9.3567486Losses:  7.881616592407227 1.2511725425720215 0.9721101522445679
CurrentTrain: epoch  0, batch    46 | loss: 9.6188450Losses:  7.281313896179199 1.2313584089279175 0.9646830558776855
CurrentTrain: epoch  0, batch    47 | loss: 8.9950142Losses:  7.868410587310791 1.19565749168396 0.9386030435562134
CurrentTrain: epoch  0, batch    48 | loss: 9.5333691Losses:  7.111825466156006 1.2642521858215332 0.9421678185462952
CurrentTrain: epoch  0, batch    49 | loss: 8.8471613Losses:  8.648975372314453 0.8608611822128296 0.9442942142486572
CurrentTrain: epoch  0, batch    50 | loss: 9.9819832Losses:  6.720367431640625 1.005657434463501 0.9309804439544678
CurrentTrain: epoch  0, batch    51 | loss: 8.1915150Losses:  7.510862350463867 1.1512987613677979 0.9225152730941772
CurrentTrain: epoch  0, batch    52 | loss: 9.1234188Losses:  7.647546768188477 0.914623498916626 0.9698739051818848
CurrentTrain: epoch  0, batch    53 | loss: 9.0471067Losses:  8.80441665649414 1.112855315208435 0.9692977666854858
CurrentTrain: epoch  0, batch    54 | loss: 10.4019203Losses:  8.353352546691895 1.2665942907333374 0.9402439594268799
CurrentTrain: epoch  0, batch    55 | loss: 10.0900688Losses:  6.694027423858643 1.053596019744873 0.9294184446334839
CurrentTrain: epoch  0, batch    56 | loss: 8.2123327Losses:  6.804671287536621 0.7991502285003662 0.9421205520629883
CurrentTrain: epoch  0, batch    57 | loss: 8.0748825Losses:  7.3323259353637695 1.0527801513671875 0.9482380747795105
CurrentTrain: epoch  0, batch    58 | loss: 8.8592253Losses:  7.845747947692871 0.9523980021476746 0.9432827234268188
CurrentTrain: epoch  0, batch    59 | loss: 9.2697878Losses:  7.362375259399414 1.0102663040161133 0.9176987409591675
CurrentTrain: epoch  0, batch    60 | loss: 8.8314905Losses:  7.0743184089660645 0.9809228181838989 0.9486452341079712
CurrentTrain: epoch  0, batch    61 | loss: 8.5295639Losses:  6.59975528717041 0.9702650904655457 0.9369115829467773
CurrentTrain: epoch  0, batch    62 | loss: 8.0384760Losses:  8.483438491821289 0.9707497358322144 0.9751793146133423
CurrentTrain: epoch  1, batch     0 | loss: 9.9417782Losses:  6.506619453430176 0.8609622716903687 0.9101889133453369
CurrentTrain: epoch  1, batch     1 | loss: 7.8226762Losses:  7.237802982330322 1.1243362426757812 0.9291965365409851
CurrentTrain: epoch  1, batch     2 | loss: 8.8267374Losses:  6.846732139587402 0.9176317453384399 0.9019299149513245
CurrentTrain: epoch  1, batch     3 | loss: 8.2153292Losses:  6.297430038452148 0.9362758994102478 0.9275553226470947
CurrentTrain: epoch  1, batch     4 | loss: 7.6974835Losses:  6.372276306152344 0.6811521053314209 0.9306496381759644
CurrentTrain: epoch  1, batch     5 | loss: 7.5187535Losses:  7.381667137145996 0.7700167298316956 0.9512805938720703
CurrentTrain: epoch  1, batch     6 | loss: 8.6273241Losses:  7.160677433013916 1.0511107444763184 0.9222623109817505
CurrentTrain: epoch  1, batch     7 | loss: 8.6729193Losses:  6.9965057373046875 1.0160207748413086 0.9446250796318054
CurrentTrain: epoch  1, batch     8 | loss: 8.4848394Losses:  6.534479141235352 0.7329460382461548 0.8900800943374634
CurrentTrain: epoch  1, batch     9 | loss: 7.7124653Losses:  6.917651653289795 0.9527125358581543 0.9196111559867859
CurrentTrain: epoch  1, batch    10 | loss: 8.3301697Losses:  7.776548385620117 0.9550841450691223 0.9435266256332397
CurrentTrain: epoch  1, batch    11 | loss: 9.2033958Losses:  6.898684024810791 0.9463140964508057 0.9232213497161865
CurrentTrain: epoch  1, batch    12 | loss: 8.3066092Losses:  7.093448638916016 1.0542314052581787 0.9193298816680908
CurrentTrain: epoch  1, batch    13 | loss: 8.6073456Losses:  7.381043910980225 1.0680922269821167 0.9254077672958374
CurrentTrain: epoch  1, batch    14 | loss: 8.9118395Losses:  7.0030927658081055 0.8581008911132812 0.9116337299346924
CurrentTrain: epoch  1, batch    15 | loss: 8.3170109Losses:  6.439888000488281 0.8234904408454895 0.9223252534866333
CurrentTrain: epoch  1, batch    16 | loss: 7.7245412Losses:  7.93159294128418 0.831445574760437 0.9372859597206116
CurrentTrain: epoch  1, batch    17 | loss: 9.2316818Losses:  6.582958221435547 0.9066317677497864 0.9250167608261108
CurrentTrain: epoch  1, batch    18 | loss: 7.9520984Losses:  6.648865699768066 0.9237319827079773 0.9054718613624573
CurrentTrain: epoch  1, batch    19 | loss: 8.0253334Losses:  6.843535423278809 0.917310357093811 0.9230241179466248
CurrentTrain: epoch  1, batch    20 | loss: 8.2223577Losses:  6.717116355895996 0.8227362632751465 0.9293554425239563
CurrentTrain: epoch  1, batch    21 | loss: 8.0045300Losses:  6.4252448081970215 0.8876355886459351 0.9250243902206421
CurrentTrain: epoch  1, batch    22 | loss: 7.7753925Losses:  6.814266204833984 0.9096473455429077 0.9067484736442566
CurrentTrain: epoch  1, batch    23 | loss: 8.1772881Losses:  6.967436790466309 0.8617055416107178 0.8979514837265015
CurrentTrain: epoch  1, batch    24 | loss: 8.2781181Losses:  6.201052188873291 0.8867079615592957 0.9121948480606079
CurrentTrain: epoch  1, batch    25 | loss: 7.5438576Losses:  7.27573823928833 0.9270386695861816 0.9276912212371826
CurrentTrain: epoch  1, batch    26 | loss: 8.6666222Losses:  5.98994779586792 0.5495513081550598 0.8937246203422546
CurrentTrain: epoch  1, batch    27 | loss: 6.9863615Losses:  5.922732830047607 0.6052827835083008 0.9364078640937805
CurrentTrain: epoch  1, batch    28 | loss: 6.9962196Losses:  6.415187358856201 0.8354181051254272 0.9061740636825562
CurrentTrain: epoch  1, batch    29 | loss: 7.7036924Losses:  6.593944549560547 0.8310534954071045 0.9192733764648438
CurrentTrain: epoch  1, batch    30 | loss: 7.8846350Losses:  6.754461765289307 0.7665611505508423 0.9202708601951599
CurrentTrain: epoch  1, batch    31 | loss: 7.9811583Losses:  6.177227020263672 0.5240609645843506 0.9296470284461975
CurrentTrain: epoch  1, batch    32 | loss: 7.1661119Losses:  6.2892866134643555 0.864133358001709 0.9119490385055542
CurrentTrain: epoch  1, batch    33 | loss: 7.6093946Losses:  6.454769134521484 0.6470091342926025 0.9011072516441345
CurrentTrain: epoch  1, batch    34 | loss: 7.5523314Losses:  6.557541847229004 0.8253799080848694 0.9161554574966431
CurrentTrain: epoch  1, batch    35 | loss: 7.8409996Losses:  6.2153496742248535 0.8468919992446899 0.9022285342216492
CurrentTrain: epoch  1, batch    36 | loss: 7.5133557Losses:  6.24945068359375 0.876773476600647 0.8992077708244324
CurrentTrain: epoch  1, batch    37 | loss: 7.5758281Losses:  6.739728927612305 0.8370493054389954 0.909461498260498
CurrentTrain: epoch  1, batch    38 | loss: 8.0315094Losses:  5.407341957092285 0.5193385481834412 0.8664202690124512
CurrentTrain: epoch  1, batch    39 | loss: 6.3598909Losses:  6.71258020401001 0.730182945728302 0.9317935705184937
CurrentTrain: epoch  1, batch    40 | loss: 7.9086599Losses:  6.9837327003479 0.9290650486946106 0.9278742074966431
CurrentTrain: epoch  1, batch    41 | loss: 8.3767347Losses:  5.772021293640137 0.8586716651916504 0.8939284086227417
CurrentTrain: epoch  1, batch    42 | loss: 7.0776572Losses:  6.440361976623535 0.7690423727035522 0.9162471890449524
CurrentTrain: epoch  1, batch    43 | loss: 7.6675282Losses:  7.248335838317871 0.9039015173912048 0.9203181266784668
CurrentTrain: epoch  1, batch    44 | loss: 8.6123962Losses:  6.006805419921875 0.7168766260147095 0.8832874894142151
CurrentTrain: epoch  1, batch    45 | loss: 7.1653256Losses:  6.011445999145508 0.6215905547142029 0.9116595983505249
CurrentTrain: epoch  1, batch    46 | loss: 7.0888662Losses:  6.107175350189209 0.6470917463302612 0.8837624788284302
CurrentTrain: epoch  1, batch    47 | loss: 7.1961484Losses:  7.066557884216309 0.6737805008888245 0.9226852655410767
CurrentTrain: epoch  1, batch    48 | loss: 8.2016811Losses:  5.76593542098999 0.5806014537811279 0.8981344699859619
CurrentTrain: epoch  1, batch    49 | loss: 6.7956038Losses:  5.836276054382324 0.6277580261230469 0.9111160039901733
CurrentTrain: epoch  1, batch    50 | loss: 6.9195919Losses:  5.209636211395264 0.35216978192329407 0.8879952430725098
CurrentTrain: epoch  1, batch    51 | loss: 6.0058041Losses:  6.053499698638916 0.5442192554473877 0.8830476403236389
CurrentTrain: epoch  1, batch    52 | loss: 7.0392432Losses:  5.268803119659424 0.6696498990058899 0.8657437562942505
CurrentTrain: epoch  1, batch    53 | loss: 6.3713250Losses:  6.696516036987305 0.6674988269805908 0.9167253971099854
CurrentTrain: epoch  1, batch    54 | loss: 7.8223772Losses:  5.564553260803223 0.34609872102737427 0.9029695987701416
CurrentTrain: epoch  1, batch    55 | loss: 6.3621368Losses:  6.421627044677734 0.7326548099517822 0.9012603759765625
CurrentTrain: epoch  1, batch    56 | loss: 7.6049118Losses:  6.470904350280762 0.696124792098999 0.944167971611023
CurrentTrain: epoch  1, batch    57 | loss: 7.6391134Losses:  6.061582565307617 0.5907578468322754 0.9146766662597656
CurrentTrain: epoch  1, batch    58 | loss: 7.1096787Losses:  5.622199058532715 0.711536169052124 0.8850948810577393
CurrentTrain: epoch  1, batch    59 | loss: 6.7762828Losses:  5.427472114562988 0.49900954961776733 0.9013757705688477
CurrentTrain: epoch  1, batch    60 | loss: 6.3771696Losses:  6.323031425476074 0.5738234519958496 0.9225501418113708
CurrentTrain: epoch  1, batch    61 | loss: 7.3581300Losses:  6.72506046295166 0.4023074209690094 0.9233485460281372
CurrentTrain: epoch  1, batch    62 | loss: 7.5890422Losses:  5.749985694885254 0.5861328840255737 0.8961688280105591
CurrentTrain: epoch  2, batch     0 | loss: 6.7842031Losses:  5.146641731262207 0.4710049033164978 0.8726276159286499
CurrentTrain: epoch  2, batch     1 | loss: 6.0539603Losses:  5.112138271331787 0.562719464302063 0.8879916667938232
CurrentTrain: epoch  2, batch     2 | loss: 6.1188536Losses:  5.294794082641602 0.5402210354804993 0.8545352220535278
CurrentTrain: epoch  2, batch     3 | loss: 6.2622828Losses:  5.310130596160889 0.6123437285423279 0.8571981191635132
CurrentTrain: epoch  2, batch     4 | loss: 6.3510733Losses:  5.942549228668213 0.5353970527648926 0.8744083642959595
CurrentTrain: epoch  2, batch     5 | loss: 6.9151506Losses:  5.359192848205566 0.5659377574920654 0.8663250207901001
CurrentTrain: epoch  2, batch     6 | loss: 6.3582935Losses:  5.964541435241699 0.6819593906402588 0.9025092124938965
CurrentTrain: epoch  2, batch     7 | loss: 7.0977554Losses:  5.853972434997559 0.6251755952835083 0.8946293592453003
CurrentTrain: epoch  2, batch     8 | loss: 6.9264627Losses:  5.010812282562256 0.47946256399154663 0.8712056875228882
CurrentTrain: epoch  2, batch     9 | loss: 5.9258776Losses:  5.505913734436035 0.5009992122650146 0.8702779412269592
CurrentTrain: epoch  2, batch    10 | loss: 6.4420524Losses:  5.29263973236084 0.566548228263855 0.8840657472610474
CurrentTrain: epoch  2, batch    11 | loss: 6.3012209Losses:  5.388172626495361 0.3862890899181366 0.859245240688324
CurrentTrain: epoch  2, batch    12 | loss: 6.2040844Losses:  5.448194980621338 0.5974831581115723 0.8719314336776733
CurrentTrain: epoch  2, batch    13 | loss: 6.4816437Losses:  5.788797378540039 0.432279109954834 0.9040369391441345
CurrentTrain: epoch  2, batch    14 | loss: 6.6730947Losses:  5.192596435546875 0.4567076563835144 0.8854329586029053
CurrentTrain: epoch  2, batch    15 | loss: 6.0920205Losses:  5.434683322906494 0.4441375732421875 0.9075926542282104
CurrentTrain: epoch  2, batch    16 | loss: 6.3326173Losses:  6.67899227142334 0.6316201686859131 0.9162285327911377
CurrentTrain: epoch  2, batch    17 | loss: 7.7687268Losses:  5.305064678192139 0.5637784004211426 0.8440440893173218
CurrentTrain: epoch  2, batch    18 | loss: 6.2908649Losses:  5.539777755737305 0.4661054313182831 0.8724367618560791
CurrentTrain: epoch  2, batch    19 | loss: 6.4421015Losses:  6.729139804840088 0.32542362809181213 0.9196245670318604
CurrentTrain: epoch  2, batch    20 | loss: 7.5143757Losses:  5.766158103942871 0.6210747957229614 0.8879778385162354
CurrentTrain: epoch  2, batch    21 | loss: 6.8312216Losses:  4.948728561401367 0.45010048151016235 0.8610263466835022
CurrentTrain: epoch  2, batch    22 | loss: 5.8293424Losses:  5.479404926300049 0.4439648687839508 0.8914177417755127
CurrentTrain: epoch  2, batch    23 | loss: 6.3690786Losses:  4.956991195678711 0.5291856527328491 0.840537965297699
CurrentTrain: epoch  2, batch    24 | loss: 5.9064460Losses:  5.302425384521484 0.29882529377937317 0.8800404071807861
CurrentTrain: epoch  2, batch    25 | loss: 6.0412707#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  0.13006459176540375 1.8446838855743408
CurrentTrain: epoch  0, batch     0 | loss: 1.9747485Losses:  0.10661881417036057 2.1760687828063965
CurrentTrain: epoch  0, batch     1 | loss: 2.2826877Losses:  0.0797070562839508 1.6884669065475464
CurrentTrain: epoch  0, batch     2 | loss: 1.7681739Losses:  0.06376776099205017 1.7019027471542358
CurrentTrain: epoch  0, batch     3 | loss: 1.7656705Losses:  0.04586523398756981 1.593049168586731
CurrentTrain: epoch  0, batch     4 | loss: 1.6389143Losses:  0.05787397921085358 1.490599513053894
CurrentTrain: epoch  0, batch     5 | loss: 1.5484735Losses:  0.05406584590673447 1.709665060043335
CurrentTrain: epoch  0, batch     6 | loss: 1.7637309Losses:  0.02093050442636013 1.4969063997268677
CurrentTrain: epoch  0, batch     7 | loss: 1.5178369Losses:  0.02971249260008335 1.4722394943237305
CurrentTrain: epoch  0, batch     8 | loss: 1.5019519Losses:  0.026485685259103775 1.7265595197677612
CurrentTrain: epoch  0, batch     9 | loss: 1.7530452Losses:  0.0226155873388052 1.442582130432129
CurrentTrain: epoch  0, batch    10 | loss: 1.4651977Losses:  0.020182181149721146 1.2952886819839478
CurrentTrain: epoch  0, batch    11 | loss: 1.3154708Losses:  0.019742727279663086 1.43180513381958
CurrentTrain: epoch  0, batch    12 | loss: 1.4515479Losses:  0.020532716065645218 1.3811551332473755
CurrentTrain: epoch  0, batch    13 | loss: 1.4016879Losses:  0.010062525048851967 1.4613633155822754
CurrentTrain: epoch  0, batch    14 | loss: 1.4714259Losses:  0.015905681997537613 1.5699561834335327
CurrentTrain: epoch  0, batch    15 | loss: 1.5858619Losses:  0.013663719408214092 1.1749612092971802
CurrentTrain: epoch  0, batch    16 | loss: 1.1886250Losses:  0.013044960796833038 1.6136016845703125
CurrentTrain: epoch  0, batch    17 | loss: 1.6266466Losses:  0.01003931649029255 1.2304760217666626
CurrentTrain: epoch  0, batch    18 | loss: 1.2405154Losses:  0.014225641265511513 1.5491737127304077
CurrentTrain: epoch  0, batch    19 | loss: 1.5633993Losses:  0.011988982558250427 1.1588871479034424
CurrentTrain: epoch  0, batch    20 | loss: 1.1708761Losses:  0.00971890613436699 1.3737907409667969
CurrentTrain: epoch  0, batch    21 | loss: 1.3835096Losses:  0.02115912362933159 1.2258398532867432
CurrentTrain: epoch  0, batch    22 | loss: 1.2469990Losses:  0.008226102218031883 1.0055387020111084
CurrentTrain: epoch  0, batch    23 | loss: 1.0137649Losses:  0.008666486479341984 1.2416903972625732
CurrentTrain: epoch  0, batch    24 | loss: 1.2503569Losses:  0.01095208153128624 0.9823071956634521
CurrentTrain: epoch  0, batch    25 | loss: 0.9932593Losses:  0.013864153996109962 1.0201354026794434
CurrentTrain: epoch  0, batch    26 | loss: 1.0339996Losses:  0.0069681499153375626 1.1338577270507812
CurrentTrain: epoch  0, batch    27 | loss: 1.1408259Losses:  0.009097469039261341 1.0245490074157715
CurrentTrain: epoch  0, batch    28 | loss: 1.0336465Losses:  0.009259413927793503 1.1947338581085205
CurrentTrain: epoch  0, batch    29 | loss: 1.2039933Losses:  0.007821732200682163 1.231715202331543
CurrentTrain: epoch  0, batch    30 | loss: 1.2395369Losses:  0.01233496144413948 0.9422398209571838
CurrentTrain: epoch  0, batch    31 | loss: 0.9545748Losses:  0.007784591056406498 0.8794095516204834
CurrentTrain: epoch  0, batch    32 | loss: 0.8871942Losses:  0.009638169780373573 0.78520667552948
CurrentTrain: epoch  0, batch    33 | loss: 0.7948449Losses:  0.008917724713683128 0.8916498422622681
CurrentTrain: epoch  0, batch    34 | loss: 0.9005676Losses:  0.009917879477143288 0.9721863865852356
CurrentTrain: epoch  0, batch    35 | loss: 0.9821042Losses:  0.009603340178728104 1.1919162273406982
CurrentTrain: epoch  0, batch    36 | loss: 1.2015196Losses:  0.00966061931103468 0.9834752678871155
CurrentTrain: epoch  0, batch    37 | loss: 0.9931359Losses:  0.009706664830446243 1.1329410076141357
CurrentTrain: epoch  0, batch    38 | loss: 1.1426476Losses:  0.015295126475393772 1.0428025722503662
CurrentTrain: epoch  0, batch    39 | loss: 1.0580977Losses:  0.012554651126265526 1.0576438903808594
CurrentTrain: epoch  0, batch    40 | loss: 1.0701985Losses:  0.006999783683568239 0.8336316347122192
CurrentTrain: epoch  0, batch    41 | loss: 0.8406314Losses:  0.006811520550400019 0.8522801399230957
CurrentTrain: epoch  0, batch    42 | loss: 0.8590916Losses:  0.006019962951540947 0.6343535780906677
CurrentTrain: epoch  0, batch    43 | loss: 0.6403735Losses:  0.010175308212637901 0.8640674948692322
CurrentTrain: epoch  0, batch    44 | loss: 0.8742428Losses:  0.006902883294969797 0.8876429796218872
CurrentTrain: epoch  0, batch    45 | loss: 0.8945459Losses:  0.005286567844450474 0.7504754066467285
CurrentTrain: epoch  0, batch    46 | loss: 0.7557620Losses:  0.007646175101399422 0.865254819393158
CurrentTrain: epoch  0, batch    47 | loss: 0.8729010Losses:  0.010105551220476627 1.0558862686157227
CurrentTrain: epoch  0, batch    48 | loss: 1.0659919Losses:  0.007473908830434084 0.95970219373703
CurrentTrain: epoch  0, batch    49 | loss: 0.9671761Losses:  0.007785293739289045 0.8909978270530701
CurrentTrain: epoch  0, batch    50 | loss: 0.8987831Losses:  0.006536601111292839 0.8167421221733093
CurrentTrain: epoch  0, batch    51 | loss: 0.8232787Losses:  0.005777780432254076 0.7145870923995972
CurrentTrain: epoch  0, batch    52 | loss: 0.7203649Losses:  0.00834125466644764 0.8327347040176392
CurrentTrain: epoch  0, batch    53 | loss: 0.8410760Losses:  0.011129503138363361 0.9939479827880859
CurrentTrain: epoch  0, batch    54 | loss: 1.0050775Losses:  0.0052351877093315125 0.7132323384284973
CurrentTrain: epoch  0, batch    55 | loss: 0.7184675Losses:  0.008671658113598824 0.8553885221481323
CurrentTrain: epoch  0, batch    56 | loss: 0.8640602Losses:  0.006871853023767471 0.793196976184845
CurrentTrain: epoch  0, batch    57 | loss: 0.8000689Losses:  0.006746385246515274 0.6433338522911072
CurrentTrain: epoch  0, batch    58 | loss: 0.6500803Losses:  0.004542102105915546 0.5773427486419678
CurrentTrain: epoch  0, batch    59 | loss: 0.5818849Losses:  0.005561213009059429 0.6087040901184082
CurrentTrain: epoch  0, batch    60 | loss: 0.6142653Losses:  0.00993246491998434 0.5354233980178833
CurrentTrain: epoch  0, batch    61 | loss: 0.5453559Losses:  0.0043467190116643906 0.621925950050354
CurrentTrain: epoch  0, batch    62 | loss: 0.6262727Losses:  0.007446860894560814 0.6891421675682068
CurrentTrain: epoch  1, batch     0 | loss: 0.6965891Losses:  0.005836674012243748 0.5630279183387756
CurrentTrain: epoch  1, batch     1 | loss: 0.5688646Losses:  0.0054592592641711235 0.6769859194755554
CurrentTrain: epoch  1, batch     2 | loss: 0.6824452Losses:  0.0055039310827851295 0.6313305497169495
CurrentTrain: epoch  1, batch     3 | loss: 0.6368345Losses:  0.006900917738676071 0.6269914507865906
CurrentTrain: epoch  1, batch     4 | loss: 0.6338924Losses:  0.006553770508617163 0.4661521911621094
CurrentTrain: epoch  1, batch     5 | loss: 0.4727060Losses:  0.006605734117329121 0.6245503425598145
CurrentTrain: epoch  1, batch     6 | loss: 0.6311561Losses:  0.0040893275290727615 0.5434808135032654
CurrentTrain: epoch  1, batch     7 | loss: 0.5475702Losses:  0.0042792088352143764 0.4464394450187683
CurrentTrain: epoch  1, batch     8 | loss: 0.4507186Losses:  0.005962382070720196 0.6548773050308228
CurrentTrain: epoch  1, batch     9 | loss: 0.6608397Losses:  0.005378987640142441 0.5341834425926208
CurrentTrain: epoch  1, batch    10 | loss: 0.5395624Losses:  0.01024944894015789 0.6891758441925049
CurrentTrain: epoch  1, batch    11 | loss: 0.6994253Losses:  0.004590331111103296 0.6096911430358887
CurrentTrain: epoch  1, batch    12 | loss: 0.6142815Losses:  0.015433719381690025 0.4867405891418457
CurrentTrain: epoch  1, batch    13 | loss: 0.5021743Losses:  0.006380088627338409 0.5239048004150391
CurrentTrain: epoch  1, batch    14 | loss: 0.5302849Losses:  0.009953530505299568 0.5691506862640381
CurrentTrain: epoch  1, batch    15 | loss: 0.5791042Losses:  0.005912357941269875 0.5371803045272827
CurrentTrain: epoch  1, batch    16 | loss: 0.5430927Losses:  0.006419433280825615 0.3746055066585541
CurrentTrain: epoch  1, batch    17 | loss: 0.3810249Losses:  0.0025642812252044678 0.3974034786224365
CurrentTrain: epoch  1, batch    18 | loss: 0.3999678Losses:  0.0037208241410553455 0.4358135759830475
CurrentTrain: epoch  1, batch    19 | loss: 0.4395344Losses:  0.01811082288622856 0.5764981508255005
CurrentTrain: epoch  1, batch    20 | loss: 0.5946090Losses:  0.005363892763853073 0.6132022142410278
CurrentTrain: epoch  1, batch    21 | loss: 0.6185661Losses:  0.00451684882864356 0.5847547054290771
CurrentTrain: epoch  1, batch    22 | loss: 0.5892715Losses:  0.0025474028661847115 0.3486327528953552
CurrentTrain: epoch  1, batch    23 | loss: 0.3511802Losses:  0.0060593923553824425 0.5150957107543945
CurrentTrain: epoch  1, batch    24 | loss: 0.5211551Losses:  0.0026062263641506433 0.4017990231513977
CurrentTrain: epoch  1, batch    25 | loss: 0.4044052Losses:  0.009352179244160652 0.5356746315956116
CurrentTrain: epoch  1, batch    26 | loss: 0.5450268Losses:  0.00509794894605875 0.5678800344467163
CurrentTrain: epoch  1, batch    27 | loss: 0.5729780Losses:  0.0035411405842751265 0.413906991481781
CurrentTrain: epoch  1, batch    28 | loss: 0.4174481Losses:  0.0035818754695355892 0.4757647216320038
CurrentTrain: epoch  1, batch    29 | loss: 0.4793466Losses:  0.0027613171841949224 0.321084201335907
CurrentTrain: epoch  1, batch    30 | loss: 0.3238455Losses:  0.0027348999865353107 0.4532553553581238
CurrentTrain: epoch  1, batch    31 | loss: 0.4559903Losses:  0.00669813621789217 0.5041347742080688
CurrentTrain: epoch  1, batch    32 | loss: 0.5108329Losses:  0.003294101683422923 0.3110106885433197
CurrentTrain: epoch  1, batch    33 | loss: 0.3143048Losses:  0.00387552916072309 0.6264019012451172
CurrentTrain: epoch  1, batch    34 | loss: 0.6302775Losses:  0.004675688222050667 0.44835126399993896
CurrentTrain: epoch  1, batch    35 | loss: 0.4530270Losses:  0.0024325160775333643 0.25180649757385254
CurrentTrain: epoch  1, batch    36 | loss: 0.2542390Losses:  0.0033886590972542763 0.5250828862190247
CurrentTrain: epoch  1, batch    37 | loss: 0.5284715Losses:  0.01924191787838936 0.6045494079589844
CurrentTrain: epoch  1, batch    38 | loss: 0.6237913Losses:  0.0025065201334655285 0.30797481536865234
CurrentTrain: epoch  1, batch    39 | loss: 0.3104813Losses:  0.002773893531411886 0.35617509484291077
CurrentTrain: epoch  1, batch    40 | loss: 0.3589490Losses:  0.011181075125932693 0.6701604723930359
CurrentTrain: epoch  1, batch    41 | loss: 0.6813415Losses:  0.010670185089111328 0.5260401964187622
CurrentTrain: epoch  1, batch    42 | loss: 0.5367104Losses:  0.007086656987667084 0.4591687321662903
CurrentTrain: epoch  1, batch    43 | loss: 0.4662554Losses:  0.0046044206246733665 0.37079352140426636
CurrentTrain: epoch  1, batch    44 | loss: 0.3753980Losses:  0.0029001208022236824 0.33560121059417725
CurrentTrain: epoch  1, batch    45 | loss: 0.3385013Losses:  0.002681299112737179 0.3534619212150574
CurrentTrain: epoch  1, batch    46 | loss: 0.3561432Losses:  0.0047637540847063065 0.5141501426696777
CurrentTrain: epoch  1, batch    47 | loss: 0.5189139Losses:  0.003419073298573494 0.45301371812820435
CurrentTrain: epoch  1, batch    48 | loss: 0.4564328Losses:  0.0026756655424833298 0.39093223214149475
CurrentTrain: epoch  1, batch    49 | loss: 0.3936079Losses:  0.008339452557265759 0.3541145324707031
CurrentTrain: epoch  1, batch    50 | loss: 0.3624540Losses:  0.008305692113935947 0.6128571033477783
CurrentTrain: epoch  1, batch    51 | loss: 0.6211628Losses:  0.005741257220506668 0.40872177481651306
CurrentTrain: epoch  1, batch    52 | loss: 0.4144630Losses:  0.0018300870433449745 0.3128596544265747
CurrentTrain: epoch  1, batch    53 | loss: 0.3146898Losses:  0.004536785185337067 0.40037310123443604
CurrentTrain: epoch  1, batch    54 | loss: 0.4049099Losses:  0.0015513596590608358 0.1524195373058319
CurrentTrain: epoch  1, batch    55 | loss: 0.1539709Losses:  0.0019315590616315603 0.24225886166095734
CurrentTrain: epoch  1, batch    56 | loss: 0.2441904Losses:  0.005832388997077942 0.4568594694137573
CurrentTrain: epoch  1, batch    57 | loss: 0.4626918Losses:  0.0038275206461548805 0.227024644613266
CurrentTrain: epoch  1, batch    58 | loss: 0.2308522Losses:  0.00361582450568676 0.4999296963214874
CurrentTrain: epoch  1, batch    59 | loss: 0.5035455Losses:  0.0038675484247505665 0.5133291482925415
CurrentTrain: epoch  1, batch    60 | loss: 0.5171967Losses:  0.0035035484470427036 0.3971055746078491
CurrentTrain: epoch  1, batch    61 | loss: 0.4006091Losses:  0.0002760165953077376 0.1101190447807312
CurrentTrain: epoch  1, batch    62 | loss: 0.1103951Losses:  0.0026815070305019617 0.28460532426834106
CurrentTrain: epoch  2, batch     0 | loss: 0.2872868Losses:  0.002942923456430435 0.3102155327796936
CurrentTrain: epoch  2, batch     1 | loss: 0.3131585Losses:  0.006707593332976103 0.422451913356781
CurrentTrain: epoch  2, batch     2 | loss: 0.4291595Losses:  0.0022151381708681583 0.3267560303211212
CurrentTrain: epoch  2, batch     3 | loss: 0.3289712Losses:  0.0029408452101051807 0.3045623302459717
CurrentTrain: epoch  2, batch     4 | loss: 0.3075032Losses:  0.0040521095506846905 0.37575653195381165
CurrentTrain: epoch  2, batch     5 | loss: 0.3798086Losses:  0.0013388388324528933 0.2110486626625061
CurrentTrain: epoch  2, batch     6 | loss: 0.2123875Losses:  0.003455718047916889 0.2716134488582611
CurrentTrain: epoch  2, batch     7 | loss: 0.2750692Losses:  0.0024674232117831707 0.3664013743400574
CurrentTrain: epoch  2, batch     8 | loss: 0.3688688Losses:  0.0017397819319739938 0.1895044893026352
CurrentTrain: epoch  2, batch     9 | loss: 0.1912443Losses:  0.0017810002900660038 0.2768416106700897
CurrentTrain: epoch  2, batch    10 | loss: 0.2786226Losses:  0.0029794559814035892 0.2813510596752167
CurrentTrain: epoch  2, batch    11 | loss: 0.2843305Losses:  0.0026860651560127735 0.2749052941799164
CurrentTrain: epoch  2, batch    12 | loss: 0.2775913Losses:  0.0020655691623687744 0.24335819482803345
CurrentTrain: epoch  2, batch    13 | loss: 0.2454238Losses:  0.004000166431069374 0.25300097465515137
CurrentTrain: epoch  2, batch    14 | loss: 0.2570011Losses:  0.0018361289985477924 0.243830606341362
CurrentTrain: epoch  2, batch    15 | loss: 0.2456667Losses:  0.0016818674048408866 0.20170184969902039
CurrentTrain: epoch  2, batch    16 | loss: 0.2033837Losses:  0.0035668218042701483 0.22179244458675385
CurrentTrain: epoch  2, batch    17 | loss: 0.2253593Losses:  0.004593796096742153 0.3263549506664276
CurrentTrain: epoch  2, batch    18 | loss: 0.3309487Losses:  0.004924147389829159 0.23498626053333282
CurrentTrain: epoch  2, batch    19 | loss: 0.2399104Losses:  0.001858311239629984 0.253222793340683
CurrentTrain: epoch  2, batch    20 | loss: 0.2550811Losses:  0.0014706653309985995 0.21527230739593506
CurrentTrain: epoch  2, batch    21 | loss: 0.2167430Losses:  0.0023149210028350353 0.2526950538158417
CurrentTrain: epoch  2, batch    22 | loss: 0.2550100Losses:  0.001826322404667735 0.2229609489440918
CurrentTrain: epoch  2, batch    23 | loss: 0.2247873Losses:  0.001176559366285801 0.26593899726867676
CurrentTrain: epoch  2, batch    24 | loss: 0.2671156Losses:  0.0010734102688729763 0.21356269717216492
CurrentTrain: epoch  2, batch    25 | loss: 0.2146361Losses:  0.0024903854355216026 0.18576869368553162
CurrentTrain: epoch  2, batch    26 | loss: 0.1882591Losses:  0.0014286490622907877 0.20484918355941772
CurrentTrain: epoch  2, batch    27 | loss: 0.2062778Losses:  0.0011416575871407986 0.22576764225959778
CurrentTrain: epoch  2, batch    28 | loss: 0.2269093Losses:  0.0015638158656656742 0.23641285300254822
CurrentTrain: epoch  2, batch    29 | loss: 0.2379767Losses:  0.002916058525443077 0.37015077471733093
CurrentTrain: epoch  2, batch    30 | loss: 0.3730668Losses:  0.0029018339700996876 0.2741701602935791
CurrentTrain: epoch  2, batch    31 | loss: 0.2770720Losses:  0.014210989698767662 0.413130521774292
CurrentTrain: epoch  2, batch    32 | loss: 0.4273415Losses:  0.002757106442004442 0.2716147303581238
CurrentTrain: epoch  2, batch    33 | loss: 0.2743718Losses:  0.0014289824757725 0.31140273809432983
CurrentTrain: epoch  2, batch    34 | loss: 0.3128317Losses:  0.0030600372701883316 0.1893291473388672
CurrentTrain: epoch  2, batch    35 | loss: 0.1923892Losses:  0.000987095758318901 0.17433348298072815
CurrentTrain: epoch  2, batch    36 | loss: 0.1753206Losses:  0.0023412236478179693 0.21244671940803528
CurrentTrain: epoch  2, batch    37 | loss: 0.2147879Losses:  0.0028491008561104536 0.15219402313232422
CurrentTrain: epoch  2, batch    38 | loss: 0.1550431Losses:  0.0012589506804943085 0.18127146363258362
CurrentTrain: epoch  2, batch    39 | loss: 0.1825304Losses:  0.0013045906089246273 0.24236725270748138
CurrentTrain: epoch  2, batch    40 | loss: 0.2436718Losses:  0.001998311374336481 0.25065192580223083
CurrentTrain: epoch  2, batch    41 | loss: 0.2526502Losses:  0.0008969013579189777 0.20823442935943604
CurrentTrain: epoch  2, batch    42 | loss: 0.2091313Losses:  0.0016423497581854463 0.20169639587402344
CurrentTrain: epoch  2, batch    43 | loss: 0.2033387Losses:  0.00503784092143178 0.25738754868507385
CurrentTrain: epoch  2, batch    44 | loss: 0.2624254Losses:  0.001857327064499259 0.18409565091133118
CurrentTrain: epoch  2, batch    45 | loss: 0.1859530Losses:  0.0015854028752073646 0.1474655121564865
CurrentTrain: epoch  2, batch    46 | loss: 0.1490509Losses:  0.0036635599099099636 0.3205301761627197
CurrentTrain: epoch  2, batch    47 | loss: 0.3241937Losses:  0.001158586353994906 0.17002809047698975
CurrentTrain: epoch  2, batch    48 | loss: 0.1711867Losses:  0.002007095143198967 0.2342146635055542
CurrentTrain: epoch  2, batch    49 | loss: 0.2362218Losses:  0.0009302191901952028 0.12462620437145233
CurrentTrain: epoch  2, batch    50 | loss: 0.1255564Losses:  0.0011532315984368324 0.1827969253063202
CurrentTrain: epoch  2, batch    51 | loss: 0.1839502Losses:  0.0022364668548107147 0.18727508187294006
CurrentTrain: epoch  2, batch    52 | loss: 0.1895116Losses:  0.0013872823910787702 0.17677339911460876
CurrentTrain: epoch  2, batch    53 | loss: 0.1781607Losses:  0.0013498803600668907 0.18074654042720795
CurrentTrain: epoch  2, batch    54 | loss: 0.1820964Losses:  0.002140539698302746 0.1718902885913849
CurrentTrain: epoch  2, batch    55 | loss: 0.1740308Losses:  0.0056227161549031734 0.2910879850387573
CurrentTrain: epoch  2, batch    56 | loss: 0.2967107Losses:  0.003192701144143939 0.20556247234344482
CurrentTrain: epoch  2, batch    57 | loss: 0.2087552Losses:  0.001954294042661786 0.20042112469673157
CurrentTrain: epoch  2, batch    58 | loss: 0.2023754Losses:  0.0009317206568084657 0.13386601209640503
CurrentTrain: epoch  2, batch    59 | loss: 0.1347977Losses:  0.0010054741287603974 0.2308664321899414
CurrentTrain: epoch  2, batch    60 | loss: 0.2318719Losses:  0.002103939652442932 0.19590863585472107
CurrentTrain: epoch  2, batch    61 | loss: 0.1980126Losses:  0.0005711028934456408 0.058296285569667816
CurrentTrain: epoch  2, batch    62 | loss: 0.0588674Losses:  0.0013139742659404874 0.1714826077222824
CurrentTrain: epoch  3, batch     0 | loss: 0.1727966Losses:  0.0392804816365242 0.34666508436203003
CurrentTrain: epoch  3, batch     1 | loss: 0.3859456Losses:  0.0011730875121429563 0.19995878636837006
CurrentTrain: epoch  3, batch     2 | loss: 0.2011319Losses:  0.0019755964167416096 0.152326762676239
CurrentTrain: epoch  3, batch     3 | loss: 0.1543024Losses:  0.0008094178047031164 0.14881247282028198
CurrentTrain: epoch  3, batch     4 | loss: 0.1496219Losses:  0.0009622147190384567 0.14239202439785004
CurrentTrain: epoch  3, batch     5 | loss: 0.1433542Losses:  0.0017454377375543118 0.21269863843917847
CurrentTrain: epoch  3, batch     6 | loss: 0.2144441Losses:  0.0012273309985175729 0.14510515332221985
CurrentTrain: epoch  3, batch     7 | loss: 0.1463325Losses:  0.002769181039184332 0.3301997184753418
CurrentTrain: epoch  3, batch     8 | loss: 0.3329689Losses:  0.0010374016128480434 0.1658414602279663
CurrentTrain: epoch  3, batch     9 | loss: 0.1668789Losses:  0.0018547491636127234 0.22144407033920288
CurrentTrain: epoch  3, batch    10 | loss: 0.2232988Losses:  0.0018171491101384163 0.16220906376838684
CurrentTrain: epoch  3, batch    11 | loss: 0.1640262Losses:  0.0007291326764971018 0.10142019391059875
CurrentTrain: epoch  3, batch    12 | loss: 0.1021493Losses:  0.0021234326995909214 0.2610093653202057
CurrentTrain: epoch  3, batch    13 | loss: 0.2631328Losses:  0.0015153249260038137 0.18851220607757568
CurrentTrain: epoch  3, batch    14 | loss: 0.1900275Losses:  0.000943543273024261 0.1609155535697937
CurrentTrain: epoch  3, batch    15 | loss: 0.1618591Losses:  0.0010569890728220344 0.1599787473678589
CurrentTrain: epoch  3, batch    16 | loss: 0.1610357Losses:  0.0012923396425321698 0.13904324173927307
CurrentTrain: epoch  3, batch    17 | loss: 0.1403356Losses:  0.0007686907192692161 0.13137102127075195
CurrentTrain: epoch  3, batch    18 | loss: 0.1321397Losses:  0.0021277419291436672 0.20836839079856873
CurrentTrain: epoch  3, batch    19 | loss: 0.2104961Losses:  0.0010894634760916233 0.1553908884525299
CurrentTrain: epoch  3, batch    20 | loss: 0.1564804Losses:  0.0012524583144113421 0.14908164739608765
CurrentTrain: epoch  3, batch    21 | loss: 0.1503341Losses:  0.0011517966631799936 0.19879212975502014
CurrentTrain: epoch  3, batch    22 | loss: 0.1999439Losses:  0.0011477069929242134 0.1771601140499115
CurrentTrain: epoch  3, batch    23 | loss: 0.1783078Losses:  0.0004473594017326832 0.09638041257858276
CurrentTrain: epoch  3, batch    24 | loss: 0.0968278Losses:  0.0007567860884591937 0.15301287174224854
CurrentTrain: epoch  3, batch    25 | loss: 0.1537697Losses:  0.000977504183538258 0.14407482743263245
CurrentTrain: epoch  3, batch    26 | loss: 0.1450523Losses:  0.0010734994430094957 0.14948967099189758
CurrentTrain: epoch  3, batch    27 | loss: 0.1505632Losses:  0.0009700336959213018 0.17604337632656097
CurrentTrain: epoch  3, batch    28 | loss: 0.1770134Losses:  0.008652089163661003 0.2026773989200592
CurrentTrain: epoch  3, batch    29 | loss: 0.2113295Losses:  0.002609917428344488 0.17611448466777802
CurrentTrain: epoch  3, batch    30 | loss: 0.1787244Losses:  0.001960133668035269 0.13685286045074463
CurrentTrain: epoch  3, batch    31 | loss: 0.1388130Losses:  0.001054343767464161 0.17351911962032318
CurrentTrain: epoch  3, batch    32 | loss: 0.1745735Losses:  0.0014008386060595512 0.13125289976596832
CurrentTrain: epoch  3, batch    33 | loss: 0.1326537Losses:  0.001036749454215169 0.1672380119562149
CurrentTrain: epoch  3, batch    34 | loss: 0.1682748Losses:  0.0011245120549574494 0.18568024039268494
CurrentTrain: epoch  3, batch    35 | loss: 0.1868048Losses:  0.004327400587499142 0.36358073353767395
CurrentTrain: epoch  3, batch    36 | loss: 0.3679081Losses:  0.0009052841924130917 0.10880842804908752
CurrentTrain: epoch  3, batch    37 | loss: 0.1097137Losses:  0.001223737490363419 0.11454431712627411
CurrentTrain: epoch  3, batch    38 | loss: 0.1157681Losses:  0.0008821389637887478 0.11654439568519592
CurrentTrain: epoch  3, batch    39 | loss: 0.1174265Losses:  0.000805050425697118 0.1409217119216919
CurrentTrain: epoch  3, batch    40 | loss: 0.1417268Losses:  0.0007190210744738579 0.10404300689697266
CurrentTrain: epoch  3, batch    41 | loss: 0.1047620Losses:  0.0016432792181149125 0.11136777698993683
CurrentTrain: epoch  3, batch    42 | loss: 0.1130111Losses:  0.000761619652621448 0.09816981106996536
CurrentTrain: epoch  3, batch    43 | loss: 0.0989314Losses:  0.0026675963308662176 0.19951751828193665
CurrentTrain: epoch  3, batch    44 | loss: 0.2021851Losses:  0.001082687871530652 0.09119097888469696
CurrentTrain: epoch  3, batch    45 | loss: 0.0922737Losses:  0.0007932177395559847 0.10603897273540497
CurrentTrain: epoch  3, batch    46 | loss: 0.1068322Losses:  0.001113067613914609 0.11235363781452179
CurrentTrain: epoch  3, batch    47 | loss: 0.1134667Losses:  0.0005106322932988405 0.11173565685749054
CurrentTrain: epoch  3, batch    48 | loss: 0.1122463Losses:  0.0008301778580062091 0.1626356989145279
CurrentTrain: epoch  3, batch    49 | loss: 0.1634659Losses:  0.0010779108852148056 0.13126543164253235
CurrentTrain: epoch  3, batch    50 | loss: 0.1323433Losses:  0.0009646336548030376 0.08549055457115173
CurrentTrain: epoch  3, batch    51 | loss: 0.0864552Losses:  0.0010168962180614471 0.14125679433345795
CurrentTrain: epoch  3, batch    52 | loss: 0.1422737Losses:  0.0026329245883971453 0.17598125338554382
CurrentTrain: epoch  3, batch    53 | loss: 0.1786142Losses:  0.000996037502773106 0.1529281586408615
CurrentTrain: epoch  3, batch    54 | loss: 0.1539242Losses:  0.0010102484375238419 0.13245251774787903
CurrentTrain: epoch  3, batch    55 | loss: 0.1334628Losses:  0.000748461636248976 0.14462925493717194
CurrentTrain: epoch  3, batch    56 | loss: 0.1453777Losses:  0.0014188410714268684 0.21644604206085205
CurrentTrain: epoch  3, batch    57 | loss: 0.2178649Losses:  0.0012955148704349995 0.09364750981330872
CurrentTrain: epoch  3, batch    58 | loss: 0.0949430Losses:  0.0005138474516570568 0.0769357979297638
CurrentTrain: epoch  3, batch    59 | loss: 0.0774496Losses:  0.0013568385038524866 0.17335036396980286
CurrentTrain: epoch  3, batch    60 | loss: 0.1747072Losses:  0.0005572088994085789 0.08804692327976227
CurrentTrain: epoch  3, batch    61 | loss: 0.0886041Losses:  0.00026186872855760157 0.03483167290687561
CurrentTrain: epoch  3, batch    62 | loss: 0.0350935Losses:  0.00041838124161586165 0.12244659662246704
CurrentTrain: epoch  4, batch     0 | loss: 0.1228650Losses:  0.0008526508463546634 0.12263527512550354
CurrentTrain: epoch  4, batch     1 | loss: 0.1234879Losses:  0.0008408774738200009 0.08693723380565643
CurrentTrain: epoch  4, batch     2 | loss: 0.0877781Losses:  0.0014877835055813193 0.14352525770664215
CurrentTrain: epoch  4, batch     3 | loss: 0.1450130Losses:  0.0010713395895436406 0.12327632308006287
CurrentTrain: epoch  4, batch     4 | loss: 0.1243477Losses:  0.0010280993301421404 0.13056936860084534
CurrentTrain: epoch  4, batch     5 | loss: 0.1315975Losses:  0.0007403665804304183 0.10657794028520584
CurrentTrain: epoch  4, batch     6 | loss: 0.1073183Losses:  0.0006394411902874708 0.09566274285316467
CurrentTrain: epoch  4, batch     7 | loss: 0.0963022Losses:  0.0010489645646885037 0.13210543990135193
CurrentTrain: epoch  4, batch     8 | loss: 0.1331544Losses:  0.000809792778454721 0.12124812602996826
CurrentTrain: epoch  4, batch     9 | loss: 0.1220579Losses:  0.0005385340773500502 0.08348974585533142
CurrentTrain: epoch  4, batch    10 | loss: 0.0840283Losses:  0.002207850106060505 0.18899065256118774
CurrentTrain: epoch  4, batch    11 | loss: 0.1911985Losses:  0.0009225101093761623 0.09207278490066528
CurrentTrain: epoch  4, batch    12 | loss: 0.0929953Losses:  0.0035049994476139545 0.16614753007888794
CurrentTrain: epoch  4, batch    13 | loss: 0.1696525Losses:  0.0004336092679295689 0.07655898481607437
CurrentTrain: epoch  4, batch    14 | loss: 0.0769926Losses:  0.0008303133072331548 0.07660835981369019
CurrentTrain: epoch  4, batch    15 | loss: 0.0774387Losses:  0.0009051020024344325 0.13575144112110138
CurrentTrain: epoch  4, batch    16 | loss: 0.1366565Losses:  0.0008105778833851218 0.10034800320863724
CurrentTrain: epoch  4, batch    17 | loss: 0.1011586Losses:  0.0006568341632373631 0.11722241342067719
CurrentTrain: epoch  4, batch    18 | loss: 0.1178792Losses:  0.0007446928648278117 0.1459319293498993
CurrentTrain: epoch  4, batch    19 | loss: 0.1466766Losses:  0.0032132177148014307 0.18728986382484436
CurrentTrain: epoch  4, batch    20 | loss: 0.1905031Losses:  0.0005817113560624421 0.08314894139766693
CurrentTrain: epoch  4, batch    21 | loss: 0.0837307Losses:  0.0007887035608291626 0.13184651732444763
CurrentTrain: epoch  4, batch    22 | loss: 0.1326352Losses:  0.0003816586104221642 0.039202816784381866
CurrentTrain: epoch  4, batch    23 | loss: 0.0395845Losses:  0.0010651586344465613 0.09885049611330032
CurrentTrain: epoch  4, batch    24 | loss: 0.0999157Losses:  0.0008523660944774747 0.10385915637016296
CurrentTrain: epoch  4, batch    25 | loss: 0.1047115Losses:  0.0007229107432067394 0.10202290117740631
CurrentTrain: epoch  4, batch    26 | loss: 0.1027458Losses:  0.0018840968841686845 0.08392022550106049
CurrentTrain: epoch  4, batch    27 | loss: 0.0858043Losses:  0.0032233227975666523 0.2043202668428421
CurrentTrain: epoch  4, batch    28 | loss: 0.2075436Losses:  0.0008410483133047819 0.07279440760612488
CurrentTrain: epoch  4, batch    29 | loss: 0.0736355Losses:  0.00042994762770831585 0.06000753864645958
CurrentTrain: epoch  4, batch    30 | loss: 0.0604375Losses:  0.0069164857268333435 0.13175520300865173
CurrentTrain: epoch  4, batch    31 | loss: 0.1386717Losses:  0.0011769852135330439 0.14308121800422668
CurrentTrain: epoch  4, batch    32 | loss: 0.1442582Losses:  0.0008182398742064834 0.1451270580291748
CurrentTrain: epoch  4, batch    33 | loss: 0.1459453Losses:  0.0007246651803143322 0.059960056096315384
CurrentTrain: epoch  4, batch    34 | loss: 0.0606847Losses:  0.0015063381288200617 0.1258772313594818
CurrentTrain: epoch  4, batch    35 | loss: 0.1273836Losses:  0.0009747125441208482 0.12164894491434097
CurrentTrain: epoch  4, batch    36 | loss: 0.1226237Losses:  0.0008154637762345374 0.11763112246990204
CurrentTrain: epoch  4, batch    37 | loss: 0.1184466Losses:  0.0005655959248542786 0.07101081311702728
CurrentTrain: epoch  4, batch    38 | loss: 0.0715764Losses:  0.0004863422363996506 0.11314745247364044
CurrentTrain: epoch  4, batch    39 | loss: 0.1136338Losses:  0.0006415711250156164 0.06644541770219803
CurrentTrain: epoch  4, batch    40 | loss: 0.0670870Losses:  0.000550848082639277 0.10772746056318283
CurrentTrain: epoch  4, batch    41 | loss: 0.1082783Losses:  0.030845919623970985 0.24557512998580933
CurrentTrain: epoch  4, batch    42 | loss: 0.2764210Losses:  0.000365885382052511 0.05934390425682068
CurrentTrain: epoch  4, batch    43 | loss: 0.0597098Losses:  0.0008818265632726252 0.0790768563747406
CurrentTrain: epoch  4, batch    44 | loss: 0.0799587Losses:  0.00045895675430074334 0.09927088767290115
CurrentTrain: epoch  4, batch    45 | loss: 0.0997298Losses:  0.0012839866103604436 0.10023155063390732
CurrentTrain: epoch  4, batch    46 | loss: 0.1015155Losses:  0.0007474828162230551 0.11503428965806961
CurrentTrain: epoch  4, batch    47 | loss: 0.1157818Losses:  0.0004873840371146798 0.09537442028522491
CurrentTrain: epoch  4, batch    48 | loss: 0.0958618Losses:  0.0006016178522258997 0.08254480361938477
CurrentTrain: epoch  4, batch    49 | loss: 0.0831464Losses:  0.000763719086535275 0.1595190316438675
CurrentTrain: epoch  4, batch    50 | loss: 0.1602827Losses:  0.0006257343338802457 0.06300511956214905
CurrentTrain: epoch  4, batch    51 | loss: 0.0636309Losses:  0.0004370787355583161 0.11191875487565994
CurrentTrain: epoch  4, batch    52 | loss: 0.1123558Losses:  0.0007924394449219108 0.11668705940246582
CurrentTrain: epoch  4, batch    53 | loss: 0.1174795Losses:  0.0006977246957831085 0.09491462260484695
CurrentTrain: epoch  4, batch    54 | loss: 0.0956123Losses:  0.000636609154753387 0.11001752316951752
CurrentTrain: epoch  4, batch    55 | loss: 0.1106541Losses:  0.00043078509042970836 0.0764920562505722
CurrentTrain: epoch  4, batch    56 | loss: 0.0769228Losses:  0.0018550571985542774 0.13029100000858307
CurrentTrain: epoch  4, batch    57 | loss: 0.1321461Losses:  0.0006939839804545045 0.07242898643016815
CurrentTrain: epoch  4, batch    58 | loss: 0.0731230Losses:  0.0016003526980057359 0.07758286595344543
CurrentTrain: epoch  4, batch    59 | loss: 0.0791832Losses:  0.0008154973038472235 0.09102007001638412
CurrentTrain: epoch  4, batch    60 | loss: 0.0918356Losses:  0.0005324023077264428 0.08107690513134003
CurrentTrain: epoch  4, batch    61 | loss: 0.0816093Losses:  0.00030284904642030597 0.02930527552962303
CurrentTrain: epoch  4, batch    62 | loss: 0.0296081Losses:  0.0008277021697722375 0.12092386186122894
CurrentTrain: epoch  5, batch     0 | loss: 0.1217516Losses:  0.000806038617156446 0.11602141708135605
CurrentTrain: epoch  5, batch     1 | loss: 0.1168275Losses:  0.0005818908102810383 0.06405727565288544
CurrentTrain: epoch  5, batch     2 | loss: 0.0646392Losses:  0.0005112841026857495 0.07007284462451935
CurrentTrain: epoch  5, batch     3 | loss: 0.0705841Losses:  0.0006181590724736452 0.0637369304895401
CurrentTrain: epoch  5, batch     4 | loss: 0.0643551Losses:  0.0005004620179533958 0.08763992041349411
CurrentTrain: epoch  5, batch     5 | loss: 0.0881404Losses:  0.000575740123167634 0.08051171898841858
CurrentTrain: epoch  5, batch     6 | loss: 0.0810875Losses:  0.00234944187104702 0.17165802419185638
CurrentTrain: epoch  5, batch     7 | loss: 0.1740075Losses:  0.0008032164769247174 0.09181413799524307
CurrentTrain: epoch  5, batch     8 | loss: 0.0926174Losses:  0.00031924707582220435 0.06708577275276184
CurrentTrain: epoch  5, batch     9 | loss: 0.0674050Losses:  0.000578260631300509 0.13771846890449524
CurrentTrain: epoch  5, batch    10 | loss: 0.1382967Losses:  0.0006536104483529925 0.08427529036998749
CurrentTrain: epoch  5, batch    11 | loss: 0.0849289Losses:  0.0004075742035638541 0.0984562486410141
CurrentTrain: epoch  5, batch    12 | loss: 0.0988638Losses:  0.0006141556659713387 0.07979248464107513
CurrentTrain: epoch  5, batch    13 | loss: 0.0804066Losses:  0.0007787731592543423 0.10534337908029556
CurrentTrain: epoch  5, batch    14 | loss: 0.1061222Losses:  0.0005767153343185782 0.0756477415561676
CurrentTrain: epoch  5, batch    15 | loss: 0.0762245Losses:  0.000521533191204071 0.09799759835004807
CurrentTrain: epoch  5, batch    16 | loss: 0.0985191Losses:  0.0004709731147158891 0.09231865406036377
CurrentTrain: epoch  5, batch    17 | loss: 0.0927896Losses:  0.00168307323474437 0.1276547759771347
CurrentTrain: epoch  5, batch    18 | loss: 0.1293378Losses:  0.0007369618979282677 0.06352631747722626
CurrentTrain: epoch  5, batch    19 | loss: 0.0642633Losses:  0.0003942518960684538 0.07655936479568481
CurrentTrain: epoch  5, batch    20 | loss: 0.0769536Losses:  0.0005643052281811833 0.08633535355329514
CurrentTrain: epoch  5, batch    21 | loss: 0.0868997Losses:  0.00038600602420046926 0.09382164478302002
CurrentTrain: epoch  5, batch    22 | loss: 0.0942077Losses:  0.0007790850941091776 0.08202337473630905
CurrentTrain: epoch  5, batch    23 | loss: 0.0828025Losses:  0.0003295529168099165 0.051678113639354706
CurrentTrain: epoch  5, batch    24 | loss: 0.0520077Losses:  0.0005414016195572913 0.09033281356096268
CurrentTrain: epoch  5, batch    25 | loss: 0.0908742Losses:  0.0005799711216241121 0.10052695125341415
CurrentTrain: epoch  5, batch    26 | loss: 0.1011069Losses:  0.001228582696057856 0.1076955646276474
CurrentTrain: epoch  5, batch    27 | loss: 0.1089242Losses:  0.0005040793912485242 0.08872755616903305
CurrentTrain: epoch  5, batch    28 | loss: 0.0892316Losses:  0.0006418126868084073 0.0987296849489212
CurrentTrain: epoch  5, batch    29 | loss: 0.0993715Losses:  0.000396775605622679 0.05058053508400917
CurrentTrain: epoch  5, batch    30 | loss: 0.0509773Losses:  0.0004615793004631996 0.06910762935876846
CurrentTrain: epoch  5, batch    31 | loss: 0.0695692Losses:  0.0004024564696010202 0.05795619636774063
CurrentTrain: epoch  5, batch    32 | loss: 0.0583587Losses:  0.00035040234797634184 0.09343743324279785
CurrentTrain: epoch  5, batch    33 | loss: 0.0937878Losses:  0.0007064978126436472 0.09381379187107086
CurrentTrain: epoch  5, batch    34 | loss: 0.0945203Losses:  0.016971906647086143 0.18479716777801514
CurrentTrain: epoch  5, batch    35 | loss: 0.2017691Losses:  0.0005012177280150354 0.07708444446325302
CurrentTrain: epoch  5, batch    36 | loss: 0.0775857Losses:  0.0005238100420683622 0.0881761908531189
CurrentTrain: epoch  5, batch    37 | loss: 0.0887000Losses:  0.00072473258478567 0.07382288575172424
CurrentTrain: epoch  5, batch    38 | loss: 0.0745476Losses:  0.0005253924173302948 0.08999235183000565
CurrentTrain: epoch  5, batch    39 | loss: 0.0905177Losses:  0.000502678332850337 0.08891750127077103
CurrentTrain: epoch  5, batch    40 | loss: 0.0894202Losses:  0.000388801796361804 0.07679315656423569
CurrentTrain: epoch  5, batch    41 | loss: 0.0771820Losses:  0.0010436195880174637 0.15344738960266113
CurrentTrain: epoch  5, batch    42 | loss: 0.1544910Losses:  0.0011526343878358603 0.11751241981983185
CurrentTrain: epoch  5, batch    43 | loss: 0.1186651Losses:  0.0005090596387162805 0.09747815132141113
CurrentTrain: epoch  5, batch    44 | loss: 0.0979872Losses:  0.0005271500558592379 0.10576200485229492
CurrentTrain: epoch  5, batch    45 | loss: 0.1062892Losses:  0.0004725655307993293 0.09013889729976654
CurrentTrain: epoch  5, batch    46 | loss: 0.0906115Losses:  0.0004644995497073978 0.09798609465360641
CurrentTrain: epoch  5, batch    47 | loss: 0.0984506Losses:  0.0004903054796159267 0.08836887776851654
CurrentTrain: epoch  5, batch    48 | loss: 0.0888592Losses:  0.0005413981853052974 0.0887431800365448
CurrentTrain: epoch  5, batch    49 | loss: 0.0892846Losses:  0.0004353872500360012 0.07354293763637543
CurrentTrain: epoch  5, batch    50 | loss: 0.0739783Losses:  0.00040187500417232513 0.07631053775548935
CurrentTrain: epoch  5, batch    51 | loss: 0.0767124Losses:  0.0010806906502693892 0.12375962734222412
CurrentTrain: epoch  5, batch    52 | loss: 0.1248403Losses:  0.0003843446320388466 0.08259208500385284
CurrentTrain: epoch  5, batch    53 | loss: 0.0829764Losses:  0.0005302763311192393 0.10034266114234924
CurrentTrain: epoch  5, batch    54 | loss: 0.1008729Losses:  0.000511234684381634 0.1010395884513855
CurrentTrain: epoch  5, batch    55 | loss: 0.1015508Losses:  0.00043179828207939863 0.10024238377809525
CurrentTrain: epoch  5, batch    56 | loss: 0.1006742Losses:  0.00194255611859262 0.11802493035793304
CurrentTrain: epoch  5, batch    57 | loss: 0.1199675Losses:  0.00041870662244036794 0.07177937030792236
CurrentTrain: epoch  5, batch    58 | loss: 0.0721981Losses:  0.0004659294500015676 0.08113692700862885
CurrentTrain: epoch  5, batch    59 | loss: 0.0816029Losses:  0.0004623846907634288 0.06022720783948898
CurrentTrain: epoch  5, batch    60 | loss: 0.0606896Losses:  0.0003399643173906952 0.06681381165981293
CurrentTrain: epoch  5, batch    61 | loss: 0.0671538Losses:  0.0005829797592014074 0.043910473585128784
CurrentTrain: epoch  5, batch    62 | loss: 0.0444935Losses:  0.0003945895587094128 0.08677239716053009
CurrentTrain: epoch  6, batch     0 | loss: 0.0871670Losses:  0.0006406340398825705 0.07122772932052612
CurrentTrain: epoch  6, batch     1 | loss: 0.0718684Losses:  0.0003785307635553181 0.0620245486497879
CurrentTrain: epoch  6, batch     2 | loss: 0.0624031Losses:  0.001678363187238574 0.13008323311805725
CurrentTrain: epoch  6, batch     3 | loss: 0.1317616Losses:  0.0003139132750220597 0.0625196099281311
CurrentTrain: epoch  6, batch     4 | loss: 0.0628335Losses:  0.000614796532317996 0.0679253339767456
CurrentTrain: epoch  6, batch     5 | loss: 0.0685401Losses:  0.0008437858195975423 0.12137142568826675
CurrentTrain: epoch  6, batch     6 | loss: 0.1222152Losses:  0.0016860825708135962 0.08001165091991425
CurrentTrain: epoch  6, batch     7 | loss: 0.0816977Losses:  0.0004008531104773283 0.09760420769453049
CurrentTrain: epoch  6, batch     8 | loss: 0.0980051Losses:  0.0006812207866460085 0.07444339990615845
CurrentTrain: epoch  6, batch     9 | loss: 0.0751246Losses:  0.00039296361501328647 0.08207018673419952
CurrentTrain: epoch  6, batch    10 | loss: 0.0824632Losses:  0.00038018968189135194 0.05279418081045151
CurrentTrain: epoch  6, batch    11 | loss: 0.0531744Losses:  0.0004472149012144655 0.08398588001728058
CurrentTrain: epoch  6, batch    12 | loss: 0.0844331Losses:  0.0004699761338997632 0.05374014750123024
CurrentTrain: epoch  6, batch    13 | loss: 0.0542101Losses:  0.0004394165880512446 0.08819693326950073
CurrentTrain: epoch  6, batch    14 | loss: 0.0886363Losses:  0.0005777759943157434 0.08387617021799088
CurrentTrain: epoch  6, batch    15 | loss: 0.0844539Losses:  0.0004561319947242737 0.09175646305084229
CurrentTrain: epoch  6, batch    16 | loss: 0.0922126Losses:  0.0005317413015291095 0.08109408617019653
CurrentTrain: epoch  6, batch    17 | loss: 0.0816258Losses:  0.0006401964346878231 0.03606637567281723
CurrentTrain: epoch  6, batch    18 | loss: 0.0367066Losses:  0.0004202018026262522 0.08678249269723892
CurrentTrain: epoch  6, batch    19 | loss: 0.0872027Losses:  0.0010116305202245712 0.10583676397800446
CurrentTrain: epoch  6, batch    20 | loss: 0.1068484Losses:  0.00039302572258748114 0.07143662124872208
CurrentTrain: epoch  6, batch    21 | loss: 0.0718296Losses:  0.00032090095919556916 0.07029065489768982
CurrentTrain: epoch  6, batch    22 | loss: 0.0706116Losses:  0.0006313854246400297 0.06578579545021057
CurrentTrain: epoch  6, batch    23 | loss: 0.0664172Losses:  0.00037817953852936625 0.06947675347328186
CurrentTrain: epoch  6, batch    24 | loss: 0.0698549Losses:  0.0008713331772014499 0.09121105819940567
CurrentTrain: epoch  6, batch    25 | loss: 0.0920824Losses:  0.0004875541781075299 0.0799885094165802
CurrentTrain: epoch  6, batch    26 | loss: 0.0804761Losses:  0.00032399033079855144 0.0675429031252861
CurrentTrain: epoch  6, batch    27 | loss: 0.0678669Losses:  0.00036926602479070425 0.08532039821147919
CurrentTrain: epoch  6, batch    28 | loss: 0.0856897Losses:  0.000375987496227026 0.08040200173854828
CurrentTrain: epoch  6, batch    29 | loss: 0.0807780Losses:  0.0008634336409159005 0.0906929075717926
CurrentTrain: epoch  6, batch    30 | loss: 0.0915563Losses:  0.0002771444560494274 0.048420704901218414
CurrentTrain: epoch  6, batch    31 | loss: 0.0486978Losses:  0.00058400584384799 0.05631651729345322
CurrentTrain: epoch  6, batch    32 | loss: 0.0569005Losses:  0.0017876755446195602 0.10080128908157349
CurrentTrain: epoch  6, batch    33 | loss: 0.1025890Losses:  0.0005215463461354375 0.1031140387058258
CurrentTrain: epoch  6, batch    34 | loss: 0.1036356Losses:  0.000476792745757848 0.04846348240971565
CurrentTrain: epoch  6, batch    35 | loss: 0.0489403Losses:  0.0002762472431641072 0.04477560147643089
CurrentTrain: epoch  6, batch    36 | loss: 0.0450519Losses:  0.0005295812152326107 0.06901484727859497
CurrentTrain: epoch  6, batch    37 | loss: 0.0695444Losses:  0.0004596297221723944 0.08958970010280609
CurrentTrain: epoch  6, batch    38 | loss: 0.0900493Losses:  0.0003831664507742971 0.0460413433611393
CurrentTrain: epoch  6, batch    39 | loss: 0.0464245Losses:  0.0004769172228407115 0.07206694781780243
CurrentTrain: epoch  6, batch    40 | loss: 0.0725439Losses:  0.0009359283139929175 0.10611844062805176
CurrentTrain: epoch  6, batch    41 | loss: 0.1070544Losses:  0.00046897761058062315 0.08096583187580109
CurrentTrain: epoch  6, batch    42 | loss: 0.0814348Losses:  0.0003923323529306799 0.06928301602602005
CurrentTrain: epoch  6, batch    43 | loss: 0.0696753Losses:  0.0004673281800933182 0.06797686964273453
CurrentTrain: epoch  6, batch    44 | loss: 0.0684442Losses:  0.00038960494566708803 0.06288124620914459
CurrentTrain: epoch  6, batch    45 | loss: 0.0632709Losses:  0.0003284524427726865 0.08351047337055206
CurrentTrain: epoch  6, batch    46 | loss: 0.0838389Losses:  0.0012825066223740578 0.07080511748790741
CurrentTrain: epoch  6, batch    47 | loss: 0.0720876Losses:  0.0004644963482860476 0.07833036780357361
CurrentTrain: epoch  6, batch    48 | loss: 0.0787949Losses:  0.0002738053444772959 0.04515162110328674
CurrentTrain: epoch  6, batch    49 | loss: 0.0454254Losses:  0.0004395310825202614 0.05978153645992279
CurrentTrain: epoch  6, batch    50 | loss: 0.0602211Losses:  0.0004917717888019979 0.06369195878505707
CurrentTrain: epoch  6, batch    51 | loss: 0.0641837Losses:  0.000854051555506885 0.08617881685495377
CurrentTrain: epoch  6, batch    52 | loss: 0.0870329Losses:  0.0007042618235573173 0.03527463972568512
CurrentTrain: epoch  6, batch    53 | loss: 0.0359789Losses:  0.00037485131178982556 0.0711238905787468
CurrentTrain: epoch  6, batch    54 | loss: 0.0714987Losses:  0.0003499366284813732 0.07402833551168442
CurrentTrain: epoch  6, batch    55 | loss: 0.0743783Losses:  0.00038773694541305304 0.05156230181455612
CurrentTrain: epoch  6, batch    56 | loss: 0.0519500Losses:  0.0004652737989090383 0.06508637964725494
CurrentTrain: epoch  6, batch    57 | loss: 0.0655517Losses:  0.0009990541730076075 0.0714644193649292
CurrentTrain: epoch  6, batch    58 | loss: 0.0724635Losses:  0.0003679796354845166 0.08769658207893372
CurrentTrain: epoch  6, batch    59 | loss: 0.0880646Losses:  0.000907786306925118 0.05165566876530647
CurrentTrain: epoch  6, batch    60 | loss: 0.0525635Losses:  0.0004033642471767962 0.0660315752029419
CurrentTrain: epoch  6, batch    61 | loss: 0.0664349Losses:  0.0001872823340818286 0.08013308793306351
CurrentTrain: epoch  6, batch    62 | loss: 0.0803204Losses:  0.000411990680731833 0.051814015954732895
CurrentTrain: epoch  7, batch     0 | loss: 0.0522260Losses:  0.000337745324941352 0.05996820703148842
CurrentTrain: epoch  7, batch     1 | loss: 0.0603060Losses:  0.00033219336182810366 0.06494305282831192
CurrentTrain: epoch  7, batch     2 | loss: 0.0652752Losses:  0.0004861362394876778 0.06393919885158539
CurrentTrain: epoch  7, batch     3 | loss: 0.0644253Losses:  0.0008910989854484797 0.06845647096633911
CurrentTrain: epoch  7, batch     4 | loss: 0.0693476Losses:  0.0004315054975450039 0.05371449142694473
CurrentTrain: epoch  7, batch     5 | loss: 0.0541460Losses:  0.0003841267025563866 0.060627348721027374
CurrentTrain: epoch  7, batch     6 | loss: 0.0610115Losses:  0.0004798113077413291 0.08091520518064499
CurrentTrain: epoch  7, batch     7 | loss: 0.0813950Losses:  0.00031808018684387207 0.042720384895801544
CurrentTrain: epoch  7, batch     8 | loss: 0.0430385Losses:  0.00032425709650851786 0.0848614051938057
CurrentTrain: epoch  7, batch     9 | loss: 0.0851857Losses:  0.00039842340629547834 0.08896325528621674
CurrentTrain: epoch  7, batch    10 | loss: 0.0893617Losses:  0.0002699857868719846 0.05767294391989708
CurrentTrain: epoch  7, batch    11 | loss: 0.0579429Losses:  0.0002625221968628466 0.07187315821647644
CurrentTrain: epoch  7, batch    12 | loss: 0.0721357Losses:  0.000357370066922158 0.04720625653862953
CurrentTrain: epoch  7, batch    13 | loss: 0.0475636Losses:  0.0003260115045122802 0.06853973120450974
CurrentTrain: epoch  7, batch    14 | loss: 0.0688657Losses:  0.00039223680505529046 0.08765225112438202
CurrentTrain: epoch  7, batch    15 | loss: 0.0880445Losses:  0.0003782969724852592 0.0929059088230133
CurrentTrain: epoch  7, batch    16 | loss: 0.0932842Losses:  0.0010084924288094044 0.19142206013202667
CurrentTrain: epoch  7, batch    17 | loss: 0.1924306Losses:  0.0003650119761005044 0.05656427890062332
CurrentTrain: epoch  7, batch    18 | loss: 0.0569293Losses:  0.0004383271443657577 0.0601608082652092
CurrentTrain: epoch  7, batch    19 | loss: 0.0605991Losses:  0.0003857806441374123 0.04074765741825104
CurrentTrain: epoch  7, batch    20 | loss: 0.0411334Losses:  0.00038381494232453406 0.05819856375455856
CurrentTrain: epoch  7, batch    21 | loss: 0.0585824Losses:  0.0003557445597834885 0.05565747618675232
CurrentTrain: epoch  7, batch    22 | loss: 0.0560132Losses:  0.004861023742705584 0.12546154856681824
CurrentTrain: epoch  7, batch    23 | loss: 0.1303226Losses:  0.0004513002932071686 0.08176477253437042
CurrentTrain: epoch  7, batch    24 | loss: 0.0822161Losses:  0.0009477614657953382 0.07003624737262726
CurrentTrain: epoch  7, batch    25 | loss: 0.0709840Losses:  0.0003266554558649659 0.04622940719127655
CurrentTrain: epoch  7, batch    26 | loss: 0.0465561Losses:  0.00040444324258714914 0.05970427766442299
CurrentTrain: epoch  7, batch    27 | loss: 0.0601087Losses:  0.0003198765916749835 0.07097887992858887
CurrentTrain: epoch  7, batch    28 | loss: 0.0712988Losses:  0.00039358498179353774 0.04350310191512108
CurrentTrain: epoch  7, batch    29 | loss: 0.0438967Losses:  0.0004779159207828343 0.052886851131916046
CurrentTrain: epoch  7, batch    30 | loss: 0.0533648Losses:  0.000924495339859277 0.11391445249319077
CurrentTrain: epoch  7, batch    31 | loss: 0.1148390Losses:  0.0005525537999346852 0.0400514230132103
CurrentTrain: epoch  7, batch    32 | loss: 0.0406040Losses:  0.00037784542655572295 0.05851580947637558
CurrentTrain: epoch  7, batch    33 | loss: 0.0588937Losses:  0.0003962374175898731 0.049368273466825485
CurrentTrain: epoch  7, batch    34 | loss: 0.0497645Losses:  0.0016686846502125263 0.11258959770202637
CurrentTrain: epoch  7, batch    35 | loss: 0.1142583Losses:  0.0005667123477905989 0.05666844919323921
CurrentTrain: epoch  7, batch    36 | loss: 0.0572352Losses:  0.0002946877502836287 0.049775637686252594
CurrentTrain: epoch  7, batch    37 | loss: 0.0500703Losses:  0.00040511609404347837 0.06154472753405571
CurrentTrain: epoch  7, batch    38 | loss: 0.0619498Losses:  0.00031511159613728523 0.052907031029462814
CurrentTrain: epoch  7, batch    39 | loss: 0.0532221Losses:  0.000332583294948563 0.07324835658073425
CurrentTrain: epoch  7, batch    40 | loss: 0.0735809Losses:  0.0003017000562977046 0.05310424417257309
CurrentTrain: epoch  7, batch    41 | loss: 0.0534059Losses:  0.00036097990232519805 0.05551072955131531
CurrentTrain: epoch  7, batch    42 | loss: 0.0558717Losses:  0.0005745519883930683 0.06191212311387062
CurrentTrain: epoch  7, batch    43 | loss: 0.0624867Losses:  0.0003330002655275166 0.061336733400821686
CurrentTrain: epoch  7, batch    44 | loss: 0.0616697Losses:  0.0004071670409757644 0.052834298461675644
CurrentTrain: epoch  7, batch    45 | loss: 0.0532415Losses:  0.0003719143569469452 0.05360051244497299
CurrentTrain: epoch  7, batch    46 | loss: 0.0539724Losses:  0.00035871981526724994 0.057281266897916794
CurrentTrain: epoch  7, batch    47 | loss: 0.0576400Losses:  0.0003579924814403057 0.08686753362417221
CurrentTrain: epoch  7, batch    48 | loss: 0.0872255Losses:  0.0007585653802379966 0.10114184767007828
CurrentTrain: epoch  7, batch    49 | loss: 0.1019004Losses:  0.000411456567235291 0.048843368887901306
CurrentTrain: epoch  7, batch    50 | loss: 0.0492548Losses:  0.00034567771945148706 0.0540376640856266
CurrentTrain: epoch  7, batch    51 | loss: 0.0543833Losses:  0.0004632021300494671 0.09241025149822235
CurrentTrain: epoch  7, batch    52 | loss: 0.0928735Losses:  0.0041109598241746426 0.15623825788497925
CurrentTrain: epoch  7, batch    53 | loss: 0.1603492Losses:  0.0003354342479724437 0.055103980004787445
CurrentTrain: epoch  7, batch    54 | loss: 0.0554394Losses:  0.00033164143678732216 0.05728147178888321
CurrentTrain: epoch  7, batch    55 | loss: 0.0576131Losses:  0.00026881229132413864 0.043897517025470734
CurrentTrain: epoch  7, batch    56 | loss: 0.0441663Losses:  0.00034465937642380595 0.062335751950740814
CurrentTrain: epoch  7, batch    57 | loss: 0.0626804Losses:  0.0005982072907499969 0.09536637365818024
CurrentTrain: epoch  7, batch    58 | loss: 0.0959646Losses:  0.0003023759345524013 0.041488051414489746
CurrentTrain: epoch  7, batch    59 | loss: 0.0417904Losses:  0.0003713478799909353 0.07012858986854553
CurrentTrain: epoch  7, batch    60 | loss: 0.0704999Losses:  0.00047147274017333984 0.04950256645679474
CurrentTrain: epoch  7, batch    61 | loss: 0.0499740Losses:  0.00015120756870601326 0.035676613450050354
CurrentTrain: epoch  7, batch    62 | loss: 0.0358278Losses:  0.00040322443237528205 0.07154574990272522
CurrentTrain: epoch  8, batch     0 | loss: 0.0719490Losses:  0.0004916143952868879 0.07235316187143326
CurrentTrain: epoch  8, batch     1 | loss: 0.0728448Losses:  0.00031871735700406134 0.06934218108654022
CurrentTrain: epoch  8, batch     2 | loss: 0.0696609Losses:  0.0002828245924320072 0.03844625502824783
CurrentTrain: epoch  8, batch     3 | loss: 0.0387291Losses:  0.0003008457715623081 0.04893045872449875
CurrentTrain: epoch  8, batch     4 | loss: 0.0492313Losses:  0.0003469576477073133 0.06074213981628418
CurrentTrain: epoch  8, batch     5 | loss: 0.0610891Losses:  0.00045844350825063884 0.05632347613573074
CurrentTrain: epoch  8, batch     6 | loss: 0.0567819Losses:  0.0005353392916731536 0.04876311123371124
CurrentTrain: epoch  8, batch     7 | loss: 0.0492985Losses:  0.00042393984040245414 0.05552613362669945
CurrentTrain: epoch  8, batch     8 | loss: 0.0559501Losses:  0.00038290832890197635 0.06199817731976509
CurrentTrain: epoch  8, batch     9 | loss: 0.0623811Losses:  0.0003751390613615513 0.04611508175730705
CurrentTrain: epoch  8, batch    10 | loss: 0.0464902Losses:  0.0003580771153792739 0.05733383819460869
CurrentTrain: epoch  8, batch    11 | loss: 0.0576919Losses:  0.00039438268868252635 0.05130214989185333
CurrentTrain: epoch  8, batch    12 | loss: 0.0516965Losses:  0.00042755925096571445 0.052423931658267975
CurrentTrain: epoch  8, batch    13 | loss: 0.0528515Losses:  0.00039012424531392753 0.07767023146152496
CurrentTrain: epoch  8, batch    14 | loss: 0.0780604Losses:  0.00031130091520026326 0.06427481025457382
CurrentTrain: epoch  8, batch    15 | loss: 0.0645861Losses:  0.00044272272498346865 0.03874480724334717
CurrentTrain: epoch  8, batch    16 | loss: 0.0391875Losses:  0.0003398246772121638 0.06354986131191254
CurrentTrain: epoch  8, batch    17 | loss: 0.0638897Losses:  0.000375056843040511 0.05513938516378403
CurrentTrain: epoch  8, batch    18 | loss: 0.0555144Losses:  0.00029772735433652997 0.052282556891441345
CurrentTrain: epoch  8, batch    19 | loss: 0.0525803Losses:  0.0003481281455606222 0.06375163793563843
CurrentTrain: epoch  8, batch    20 | loss: 0.0640998Losses:  0.00036109727807343006 0.050435297191143036
CurrentTrain: epoch  8, batch    21 | loss: 0.0507964Losses:  0.0003359572438057512 0.06197313591837883
CurrentTrain: epoch  8, batch    22 | loss: 0.0623091Losses:  0.000388382381061092 0.06855334341526031
CurrentTrain: epoch  8, batch    23 | loss: 0.0689417Losses:  0.0004229544138070196 0.07928675413131714
CurrentTrain: epoch  8, batch    24 | loss: 0.0797097Losses:  0.0002665251959115267 0.03655300661921501
CurrentTrain: epoch  8, batch    25 | loss: 0.0368195Losses:  0.00042638776358217 0.07553503662347794
CurrentTrain: epoch  8, batch    26 | loss: 0.0759614Losses:  0.0003476825950201601 0.06356725841760635
CurrentTrain: epoch  8, batch    27 | loss: 0.0639149Losses:  0.000395521754398942 0.07721120119094849
CurrentTrain: epoch  8, batch    28 | loss: 0.0776067Losses:  0.0003309028979856521 0.05773940682411194
CurrentTrain: epoch  8, batch    29 | loss: 0.0580703Losses:  0.00044464870006777346 0.05521905794739723
CurrentTrain: epoch  8, batch    30 | loss: 0.0556637Losses:  0.00025324776652269065 0.05775370076298714
CurrentTrain: epoch  8, batch    31 | loss: 0.0580069Losses:  0.0002477302332408726 0.05818411707878113
CurrentTrain: epoch  8, batch    32 | loss: 0.0584318Losses:  0.0006180477794259787 0.06566259264945984
CurrentTrain: epoch  8, batch    33 | loss: 0.0662806Losses:  0.0003673234023153782 0.06866880506277084
CurrentTrain: epoch  8, batch    34 | loss: 0.0690361Losses:  0.00030568364309147 0.06458635628223419
CurrentTrain: epoch  8, batch    35 | loss: 0.0648920Losses:  0.00036101043224334717 0.04452057182788849
CurrentTrain: epoch  8, batch    36 | loss: 0.0448816Losses:  0.00040538006578572094 0.0675855353474617
CurrentTrain: epoch  8, batch    37 | loss: 0.0679909Losses:  0.00036202315823175013 0.06754195690155029
CurrentTrain: epoch  8, batch    38 | loss: 0.0679040Losses:  0.00032389044645242393 0.06569328159093857
CurrentTrain: epoch  8, batch    39 | loss: 0.0660172Losses:  0.001380064757540822 0.10343994200229645
CurrentTrain: epoch  8, batch    40 | loss: 0.1048200Losses:  0.0003422476293053478 0.054052822291851044
CurrentTrain: epoch  8, batch    41 | loss: 0.0543951Losses:  0.0003238647186663002 0.060645125806331635
CurrentTrain: epoch  8, batch    42 | loss: 0.0609690Losses:  0.000551024975720793 0.06828869879245758
CurrentTrain: epoch  8, batch    43 | loss: 0.0688397Losses:  0.0003488992224447429 0.06406253576278687
CurrentTrain: epoch  8, batch    44 | loss: 0.0644114Losses:  0.0003216392360627651 0.03948309272527695
CurrentTrain: epoch  8, batch    45 | loss: 0.0398047Losses:  0.0002849662268999964 0.04833154380321503
CurrentTrain: epoch  8, batch    46 | loss: 0.0486165Losses:  0.0003078102017752826 0.0683031752705574
CurrentTrain: epoch  8, batch    47 | loss: 0.0686110Losses:  0.000281485088635236 0.043380893766880035
CurrentTrain: epoch  8, batch    48 | loss: 0.0436624Losses:  0.0007855863077566028 0.0562245212495327
CurrentTrain: epoch  8, batch    49 | loss: 0.0570101Losses:  0.0003583438810892403 0.04506527632474899
CurrentTrain: epoch  8, batch    50 | loss: 0.0454236Losses:  0.00024186237715184689 0.04640417546033859
CurrentTrain: epoch  8, batch    51 | loss: 0.0466460Losses:  0.0005928043974563479 0.10285228490829468
CurrentTrain: epoch  8, batch    52 | loss: 0.1034451Losses:  0.00036936160176992416 0.061363477259874344
CurrentTrain: epoch  8, batch    53 | loss: 0.0617328Losses:  0.00040955343865789473 0.06424812972545624
CurrentTrain: epoch  8, batch    54 | loss: 0.0646577Losses:  0.00023441592929884791 0.053776003420352936
CurrentTrain: epoch  8, batch    55 | loss: 0.0540104Losses:  0.0004754403489641845 0.06131366640329361
CurrentTrain: epoch  8, batch    56 | loss: 0.0617891Losses:  0.00042765052057802677 0.06635011732578278
CurrentTrain: epoch  8, batch    57 | loss: 0.0667778Losses:  0.0002927766181528568 0.05971299111843109
CurrentTrain: epoch  8, batch    58 | loss: 0.0600058Losses:  0.00028127472614869475 0.07003796845674515
CurrentTrain: epoch  8, batch    59 | loss: 0.0703192Losses:  0.00040868244832381606 0.08210083097219467
CurrentTrain: epoch  8, batch    60 | loss: 0.0825095Losses:  0.0003231061273254454 0.06592054665088654
CurrentTrain: epoch  8, batch    61 | loss: 0.0662437Losses:  0.00017371171270497143 0.014399487525224686
CurrentTrain: epoch  8, batch    62 | loss: 0.0145732Losses:  0.0002773024025373161 0.044638484716415405
CurrentTrain: epoch  9, batch     0 | loss: 0.0449158Losses:  0.0003058356523979455 0.048505641520023346
CurrentTrain: epoch  9, batch     1 | loss: 0.0488115Losses:  0.00033346418058499694 0.050504256039857864
CurrentTrain: epoch  9, batch     2 | loss: 0.0508377Losses:  0.00032981386175379157 0.04927557706832886
CurrentTrain: epoch  9, batch     3 | loss: 0.0496054Losses:  0.00025353452656418085 0.035885654389858246
CurrentTrain: epoch  9, batch     4 | loss: 0.0361392Losses:  0.00032624960294924676 0.04323917627334595
CurrentTrain: epoch  9, batch     5 | loss: 0.0435654Losses:  0.0002960560377687216 0.05603496730327606
CurrentTrain: epoch  9, batch     6 | loss: 0.0563310Losses:  0.00035342585761100054 0.045936863869428635
CurrentTrain: epoch  9, batch     7 | loss: 0.0462903Losses:  0.0002525038144085556 0.04441247507929802
CurrentTrain: epoch  9, batch     8 | loss: 0.0446650Losses:  0.0003297155490145087 0.0667835995554924
CurrentTrain: epoch  9, batch     9 | loss: 0.0671133Losses:  0.0007923675002530217 0.06702437996864319
CurrentTrain: epoch  9, batch    10 | loss: 0.0678167Losses:  0.00032152706990018487 0.06782875955104828
CurrentTrain: epoch  9, batch    11 | loss: 0.0681503Losses:  0.00027386509464122355 0.06256794184446335
CurrentTrain: epoch  9, batch    12 | loss: 0.0628418Losses:  0.0007012092391960323 0.07828707247972488
CurrentTrain: epoch  9, batch    13 | loss: 0.0789883Losses:  0.00048090011114254594 0.08523793518543243
CurrentTrain: epoch  9, batch    14 | loss: 0.0857188Losses:  0.0003373352810740471 0.059069834649562836
CurrentTrain: epoch  9, batch    15 | loss: 0.0594072Losses:  0.00030606778454966843 0.06367051601409912
CurrentTrain: epoch  9, batch    16 | loss: 0.0639766Losses:  0.0003342074924148619 0.0643652081489563
CurrentTrain: epoch  9, batch    17 | loss: 0.0646994Losses:  0.00040373526280745864 0.04315068572759628
CurrentTrain: epoch  9, batch    18 | loss: 0.0435544Losses:  0.00029992725467309356 0.024260103702545166
CurrentTrain: epoch  9, batch    19 | loss: 0.0245600Losses:  0.0002696653245948255 0.0470736101269722
CurrentTrain: epoch  9, batch    20 | loss: 0.0473433Losses:  0.0003539902390912175 0.05426393821835518
CurrentTrain: epoch  9, batch    21 | loss: 0.0546179Losses:  0.00032812703284434974 0.0608736127614975
CurrentTrain: epoch  9, batch    22 | loss: 0.0612017Losses:  0.000331368442857638 0.0496922992169857
CurrentTrain: epoch  9, batch    23 | loss: 0.0500237Losses:  0.0002804163668770343 0.04258781671524048
CurrentTrain: epoch  9, batch    24 | loss: 0.0428682Losses:  0.0002865483402274549 0.05022319778800011
CurrentTrain: epoch  9, batch    25 | loss: 0.0505097Losses:  0.0004387106164358556 0.07258602976799011
CurrentTrain: epoch  9, batch    26 | loss: 0.0730247Losses:  0.0003434767131693661 0.04954905807971954
CurrentTrain: epoch  9, batch    27 | loss: 0.0498925Losses:  0.00029136979719623923 0.06605108827352524
CurrentTrain: epoch  9, batch    28 | loss: 0.0663425Losses:  0.0003036517009604722 0.05564769729971886
CurrentTrain: epoch  9, batch    29 | loss: 0.0559513Losses:  0.0003122027264907956 0.06598562002182007
CurrentTrain: epoch  9, batch    30 | loss: 0.0662978Losses:  0.0002994105452671647 0.07336500287055969
CurrentTrain: epoch  9, batch    31 | loss: 0.0736644Losses:  0.00024290592409670353 0.047120414674282074
CurrentTrain: epoch  9, batch    32 | loss: 0.0473633Losses:  0.00031614460749551654 0.022288627922534943
CurrentTrain: epoch  9, batch    33 | loss: 0.0226048Losses:  0.00033544059260748327 0.06439187377691269
CurrentTrain: epoch  9, batch    34 | loss: 0.0647273Losses:  0.000378045835532248 0.07051992416381836
CurrentTrain: epoch  9, batch    35 | loss: 0.0708980Losses:  0.0002925521694123745 0.030855629593133926
CurrentTrain: epoch  9, batch    36 | loss: 0.0311482Losses:  0.0003799276892095804 0.04984664171934128
CurrentTrain: epoch  9, batch    37 | loss: 0.0502266Losses:  0.00030802161199972034 0.03821057453751564
CurrentTrain: epoch  9, batch    38 | loss: 0.0385186Losses:  0.0017134473891928792 0.11206105351448059
CurrentTrain: epoch  9, batch    39 | loss: 0.1137745Losses:  0.00023071303439792246 0.04435783997178078
CurrentTrain: epoch  9, batch    40 | loss: 0.0445886Losses:  0.00023209673236124218 0.03186924383044243
CurrentTrain: epoch  9, batch    41 | loss: 0.0321013Losses:  0.0003187204129062593 0.03215543180704117
CurrentTrain: epoch  9, batch    42 | loss: 0.0324742Losses:  0.0002880244283005595 0.03166257217526436
CurrentTrain: epoch  9, batch    43 | loss: 0.0319506Losses:  0.0002913585922215134 0.04026462137699127
CurrentTrain: epoch  9, batch    44 | loss: 0.0405560Losses:  0.00041887941188178957 0.06810232996940613
CurrentTrain: epoch  9, batch    45 | loss: 0.0685212Losses:  0.0002302008360857144 0.035106077790260315
CurrentTrain: epoch  9, batch    46 | loss: 0.0353363Losses:  0.00032145832665264606 0.04354892298579216
CurrentTrain: epoch  9, batch    47 | loss: 0.0438704Losses:  0.0003712725010700524 0.047302428632974625
CurrentTrain: epoch  9, batch    48 | loss: 0.0476737Losses:  0.000420824479078874 0.07592988759279251
CurrentTrain: epoch  9, batch    49 | loss: 0.0763507Losses:  0.0003346851735841483 0.06527586281299591
CurrentTrain: epoch  9, batch    50 | loss: 0.0656106Losses:  0.00038665084866806865 0.059171274304389954
CurrentTrain: epoch  9, batch    51 | loss: 0.0595579Losses:  0.0003852889349218458 0.04108131304383278
CurrentTrain: epoch  9, batch    52 | loss: 0.0414666Losses:  0.00034378102282062173 0.06159226596355438
CurrentTrain: epoch  9, batch    53 | loss: 0.0619360Losses:  0.00043084737262688577 0.055813368409872055
CurrentTrain: epoch  9, batch    54 | loss: 0.0562442Losses:  0.00031653320183977485 0.03707661107182503
CurrentTrain: epoch  9, batch    55 | loss: 0.0373931Losses:  0.00028725783340632915 0.05663859099149704
CurrentTrain: epoch  9, batch    56 | loss: 0.0569258Losses:  0.0003192062140442431 0.07187935709953308
CurrentTrain: epoch  9, batch    57 | loss: 0.0721986Losses:  0.002005536574870348 0.10381140559911728
CurrentTrain: epoch  9, batch    58 | loss: 0.1058169Losses:  0.0002976892574224621 0.06873315572738647
CurrentTrain: epoch  9, batch    59 | loss: 0.0690308Losses:  0.006780775263905525 0.21828292310237885
CurrentTrain: epoch  9, batch    60 | loss: 0.2250637Losses:  0.0002910809707827866 0.05615406483411789
CurrentTrain: epoch  9, batch    61 | loss: 0.0564451Losses:  0.00015783836715854704 0.02365044876933098
CurrentTrain: epoch  9, batch    62 | loss: 0.0238083
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.65%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  0.024212485179305077 1.2520208358764648
CurrentTrain: epoch  0, batch     0 | loss: 1.2762333Losses:  0.023545486852526665 1.2866671085357666
CurrentTrain: epoch  0, batch     1 | loss: 1.3102126Losses:  0.01736191287636757 1.3802900314331055
CurrentTrain: epoch  0, batch     2 | loss: 1.3976519Losses:  0.0 1.4901162614933128e-07
CurrentTrain: epoch  0, batch     3 | loss: 0.0000001Losses:  0.02810695581138134 1.091225266456604
CurrentTrain: epoch  1, batch     0 | loss: 1.1193322Losses:  0.009621452540159225 1.1540457010269165
CurrentTrain: epoch  1, batch     1 | loss: 1.1636672Losses:  0.02848586067557335 1.043947696685791
CurrentTrain: epoch  1, batch     2 | loss: 1.0724336Losses:  1.2755474926962052e-05 0.09134314954280853
CurrentTrain: epoch  1, batch     3 | loss: 0.0913559Losses:  0.020967302843928337 0.8327885866165161
CurrentTrain: epoch  2, batch     0 | loss: 0.8537559Losses:  0.012000844813883305 0.7468568682670593
CurrentTrain: epoch  2, batch     1 | loss: 0.7588577Losses:  0.011418841779232025 0.667881429195404
CurrentTrain: epoch  2, batch     2 | loss: 0.6793002Losses:  8.708617679076269e-05 0.17691810429096222
CurrentTrain: epoch  2, batch     3 | loss: 0.1770052Losses:  0.0111855398863554 0.7911688089370728
CurrentTrain: epoch  3, batch     0 | loss: 0.8023543Losses:  0.003987371455878019 0.500167727470398
CurrentTrain: epoch  3, batch     1 | loss: 0.5041551Losses:  0.02891439013183117 0.9014126658439636
CurrentTrain: epoch  3, batch     2 | loss: 0.9303271Losses:  0.003767665009945631 0.31756719946861267
CurrentTrain: epoch  3, batch     3 | loss: 0.3213349Losses:  0.006715354043990374 0.5286003351211548
CurrentTrain: epoch  4, batch     0 | loss: 0.5353157Losses:  0.003828014712780714 0.5641788244247437
CurrentTrain: epoch  4, batch     1 | loss: 0.5680068Losses:  0.02435738779604435 0.5935533046722412
CurrentTrain: epoch  4, batch     2 | loss: 0.6179107Losses:  7.450608336512232e-06 0.0059225247241556644
CurrentTrain: epoch  4, batch     3 | loss: 0.0059300Losses:  0.006606734823435545 0.5031013488769531
CurrentTrain: epoch  5, batch     0 | loss: 0.5097081Losses:  0.0182526633143425 0.540210485458374
CurrentTrain: epoch  5, batch     1 | loss: 0.5584632Losses:  0.006377577781677246 0.6024276614189148
CurrentTrain: epoch  5, batch     2 | loss: 0.6088052Losses:  4.351148618297884e-06 0.02235564962029457
CurrentTrain: epoch  5, batch     3 | loss: 0.0223600Losses:  0.003460357431322336 0.5093722343444824
CurrentTrain: epoch  6, batch     0 | loss: 0.5128326Losses:  0.004542387556284666 0.47972026467323303
CurrentTrain: epoch  6, batch     1 | loss: 0.4842626Losses:  0.009702911600470543 0.5022916197776794
CurrentTrain: epoch  6, batch     2 | loss: 0.5119945Losses:  0.0 0.0
CurrentTrain: epoch  6, batch     3 | loss: 0.0000000Losses:  0.015309994108974934 0.4673202931880951
CurrentTrain: epoch  7, batch     0 | loss: 0.4826303Losses:  0.00878917332738638 0.5271608829498291
CurrentTrain: epoch  7, batch     1 | loss: 0.5359501Losses:  0.0030038137920200825 0.36700987815856934
CurrentTrain: epoch  7, batch     2 | loss: 0.3700137Losses:  0.0 0.0
CurrentTrain: epoch  7, batch     3 | loss: 0.0000000Losses:  0.004449763800948858 0.41189855337142944
CurrentTrain: epoch  8, batch     0 | loss: 0.4163483Losses:  0.004615002777427435 0.42827028036117554
CurrentTrain: epoch  8, batch     1 | loss: 0.4328853Losses:  0.004018579609692097 0.42075681686401367
CurrentTrain: epoch  8, batch     2 | loss: 0.4247754Losses:  2.9802766221109778e-05 0.022222721949219704
CurrentTrain: epoch  8, batch     3 | loss: 0.0222525Losses:  0.003652215236797929 0.3936675786972046
CurrentTrain: epoch  9, batch     0 | loss: 0.3973198Losses:  0.002854457125067711 0.347170352935791
CurrentTrain: epoch  9, batch     1 | loss: 0.3500248Losses:  0.006580419838428497 0.3906305432319641
CurrentTrain: epoch  9, batch     2 | loss: 0.3972110Losses:  1.3113108252582606e-05 0.010291596874594688
CurrentTrain: epoch  9, batch     3 | loss: 0.0103047
Losses:  0.0737784206867218 0.7247539758682251
MemoryTrain:  epoch  0, batch     0 | loss: 0.7985324Losses:  0.00010882955393753946 0.03507868945598602
MemoryTrain:  epoch  0, batch     1 | loss: 0.0351875Losses:  0.04439916834235191 0.5438413023948669
MemoryTrain:  epoch  1, batch     0 | loss: 0.5882404Losses:  0.02225300297141075 0.39448443055152893
MemoryTrain:  epoch  1, batch     1 | loss: 0.4167374Losses:  0.04347832500934601 0.5241175889968872
MemoryTrain:  epoch  2, batch     0 | loss: 0.5675959Losses:  0.0005135303945280612 0.11842798441648483
MemoryTrain:  epoch  2, batch     1 | loss: 0.1189415Losses:  0.004552566446363926 0.4365851879119873
MemoryTrain:  epoch  3, batch     0 | loss: 0.4411378Losses:  0.00151877012103796 0.15476298332214355
MemoryTrain:  epoch  3, batch     1 | loss: 0.1562818Losses:  0.02110043540596962 0.4920225739479065
MemoryTrain:  epoch  4, batch     0 | loss: 0.5131230Losses:  0.0003147800453007221 0.07729028910398483
MemoryTrain:  epoch  4, batch     1 | loss: 0.0776051Losses:  0.013113537803292274 0.5088642835617065
MemoryTrain:  epoch  5, batch     0 | loss: 0.5219778Losses:  0.00010620641842251644 0.03065500780940056
MemoryTrain:  epoch  5, batch     1 | loss: 0.0307612Losses:  0.011715616099536419 0.49325066804885864
MemoryTrain:  epoch  6, batch     0 | loss: 0.5049663Losses:  0.0005020927055738866 0.06742387264966965
MemoryTrain:  epoch  6, batch     1 | loss: 0.0679260Losses:  0.002760824281722307 0.4373771548271179
MemoryTrain:  epoch  7, batch     0 | loss: 0.4401380Losses:  0.002579322550445795 0.12099399417638779
MemoryTrain:  epoch  7, batch     1 | loss: 0.1235733Losses:  0.008415188640356064 0.43470120429992676
MemoryTrain:  epoch  8, batch     0 | loss: 0.4431164Losses:  0.0024265232495963573 0.14870893955230713
MemoryTrain:  epoch  8, batch     1 | loss: 0.1511355Losses:  0.006171312183141708 0.3707691431045532
MemoryTrain:  epoch  9, batch     0 | loss: 0.3769405Losses:  9.309263987233862e-05 0.03239677846431732
MemoryTrain:  epoch  9, batch     1 | loss: 0.0324899
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 87.85%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 77.92%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 73.90%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 72.86%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 71.62%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 68.15%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 65.76%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 64.89%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 64.32%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 63.52%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 63.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 64.39%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 68.25%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 67.86%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 90.26%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.70%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.09%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 91.45%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 91.16%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 91.04%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.78%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.73%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.82%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 90.96%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.14%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.43%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.49%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 91.47%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.37%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.15%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 90.79%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.59%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 90.16%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 90.12%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 89.71%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 89.23%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 88.99%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 88.68%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 88.44%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 88.22%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 87.93%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 87.71%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 87.29%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 87.16%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 86.89%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 86.36%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 85.84%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 85.46%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 84.96%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 84.60%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 84.18%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 83.96%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 83.44%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 82.98%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 82.60%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 82.28%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 81.97%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 81.55%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 81.31%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.90%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 80.38%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.93%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 79.49%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.00%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 78.74%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.48%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.56%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 78.64%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 78.77%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.92%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.89%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 78.96%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 79.25%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 79.49%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.60%   
cur_acc:  ['0.9464', '0.6786']
his_acc:  ['0.9464', '0.7960']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  0.024190042167901993 1.6988391876220703
CurrentTrain: epoch  0, batch     0 | loss: 1.7230293Losses:  0.0027184421196579933 1.6418648958206177
CurrentTrain: epoch  0, batch     1 | loss: 1.6445833Losses:  0.009800339117646217 1.8735878467559814
CurrentTrain: epoch  0, batch     2 | loss: 1.8833882Losses:  1.2576659173646476e-05 0.46960315108299255
CurrentTrain: epoch  0, batch     3 | loss: 0.4696157Losses:  0.014520049095153809 1.4831067323684692
CurrentTrain: epoch  1, batch     0 | loss: 1.4976268Losses:  0.008208846673369408 1.5257019996643066
CurrentTrain: epoch  1, batch     1 | loss: 1.5339109Losses:  0.004409788642078638 1.2455991506576538
CurrentTrain: epoch  1, batch     2 | loss: 1.2500089Losses:  3.3856013033073395e-05 0.12600281834602356
CurrentTrain: epoch  1, batch     3 | loss: 0.1260367Losses:  0.005154995247721672 0.9140429496765137
CurrentTrain: epoch  2, batch     0 | loss: 0.9191979Losses:  0.004256860818713903 1.1641095876693726
CurrentTrain: epoch  2, batch     1 | loss: 1.1683664Losses:  0.006130650639533997 1.372351050376892
CurrentTrain: epoch  2, batch     2 | loss: 1.3784817Losses:  0.00012708517897408456 0.05943404138088226
CurrentTrain: epoch  2, batch     3 | loss: 0.0595611Losses:  0.003352340077981353 1.0608443021774292
CurrentTrain: epoch  3, batch     0 | loss: 1.0641966Losses:  0.006035450380295515 0.8743373155593872
CurrentTrain: epoch  3, batch     1 | loss: 0.8803728Losses:  0.0031823806930333376 1.12210214138031
CurrentTrain: epoch  3, batch     2 | loss: 1.1252846Losses:  1.0371261851105373e-05 0.0433557853102684
CurrentTrain: epoch  3, batch     3 | loss: 0.0433662Losses:  0.009205683134496212 0.9379562139511108
CurrentTrain: epoch  4, batch     0 | loss: 0.9471619Losses:  0.00856566708534956 1.006975531578064
CurrentTrain: epoch  4, batch     1 | loss: 1.0155412Losses:  0.004382712300866842 0.784947395324707
CurrentTrain: epoch  4, batch     2 | loss: 0.7893301Losses:  8.494022767990828e-05 0.16555294394493103
CurrentTrain: epoch  4, batch     3 | loss: 0.1656379Losses:  0.005371954292058945 0.8397297859191895
CurrentTrain: epoch  5, batch     0 | loss: 0.8451017Losses:  0.002303723944351077 0.5813113451004028
CurrentTrain: epoch  5, batch     1 | loss: 0.5836151Losses:  0.0018089429941028357 0.5287654995918274
CurrentTrain: epoch  5, batch     2 | loss: 0.5305744Losses:  7.105126132955775e-05 0.533826470375061
CurrentTrain: epoch  5, batch     3 | loss: 0.5338975Losses:  0.003088087309151888 0.9207050800323486
CurrentTrain: epoch  6, batch     0 | loss: 0.9237932Losses:  0.007815595716238022 0.7539303302764893
CurrentTrain: epoch  6, batch     1 | loss: 0.7617459Losses:  0.0021714731119573116 0.5449862480163574
CurrentTrain: epoch  6, batch     2 | loss: 0.5471577Losses:  0.00018485108739696443 0.07759871333837509
CurrentTrain: epoch  6, batch     3 | loss: 0.0777836Losses:  0.007580236531794071 0.6484218239784241
CurrentTrain: epoch  7, batch     0 | loss: 0.6560020Losses:  0.00592802744358778 0.6372042894363403
CurrentTrain: epoch  7, batch     1 | loss: 0.6431323Losses:  0.003348947037011385 0.6299902200698853
CurrentTrain: epoch  7, batch     2 | loss: 0.6333392Losses:  0.0007591148023493588 0.3442480266094208
CurrentTrain: epoch  7, batch     3 | loss: 0.3450072Losses:  0.003804298583418131 0.6977968215942383
CurrentTrain: epoch  8, batch     0 | loss: 0.7016011Losses:  0.006238835398107767 0.6699950695037842
CurrentTrain: epoch  8, batch     1 | loss: 0.6762339Losses:  0.002611391246318817 0.395811527967453
CurrentTrain: epoch  8, batch     2 | loss: 0.3984229Losses:  1.8477456933396752e-06 0.016485709697008133
CurrentTrain: epoch  8, batch     3 | loss: 0.0164876Losses:  0.008483504876494408 0.4833812117576599
CurrentTrain: epoch  9, batch     0 | loss: 0.4918647Losses:  0.005051128100603819 0.3564486503601074
CurrentTrain: epoch  9, batch     1 | loss: 0.3614998Losses:  0.005117915570735931 0.5298600792884827
CurrentTrain: epoch  9, batch     2 | loss: 0.5349780Losses:  0.0018339273519814014 0.6525825262069702
CurrentTrain: epoch  9, batch     3 | loss: 0.6544164
Losses:  0.011096830479800701 0.6323018670082092
MemoryTrain:  epoch  0, batch     0 | loss: 0.6433987Losses:  0.024601442739367485 0.641546368598938
MemoryTrain:  epoch  0, batch     1 | loss: 0.6661478Losses:  0.018787425011396408 0.6673616170883179
MemoryTrain:  epoch  1, batch     0 | loss: 0.6861491Losses:  0.023425962775945663 0.49339422583580017
MemoryTrain:  epoch  1, batch     1 | loss: 0.5168202Losses:  0.02240787073969841 0.48638731241226196
MemoryTrain:  epoch  2, batch     0 | loss: 0.5087952Losses:  0.0076766000129282475 0.4768213629722595
MemoryTrain:  epoch  2, batch     1 | loss: 0.4844980Losses:  0.005350456573069096 0.540167510509491
MemoryTrain:  epoch  3, batch     0 | loss: 0.5455180Losses:  0.002661528531461954 0.39225131273269653
MemoryTrain:  epoch  3, batch     1 | loss: 0.3949128Losses:  0.006460247095674276 0.6385371685028076
MemoryTrain:  epoch  4, batch     0 | loss: 0.6449974Losses:  0.002539317589253187 0.3237009048461914
MemoryTrain:  epoch  4, batch     1 | loss: 0.3262402Losses:  0.0033851992338895798 0.4787450432777405
MemoryTrain:  epoch  5, batch     0 | loss: 0.4821302Losses:  0.0054839798249304295 0.44060006737709045
MemoryTrain:  epoch  5, batch     1 | loss: 0.4460841Losses:  0.0047953310422599316 0.4339936077594757
MemoryTrain:  epoch  6, batch     0 | loss: 0.4387890Losses:  0.002527861623093486 0.3797135055065155
MemoryTrain:  epoch  6, batch     1 | loss: 0.3822414Losses:  0.003263857215642929 0.3585171103477478
MemoryTrain:  epoch  7, batch     0 | loss: 0.3617810Losses:  0.00557007035240531 0.5080345869064331
MemoryTrain:  epoch  7, batch     1 | loss: 0.5136046Losses:  0.0062566837295889854 0.5435996055603027
MemoryTrain:  epoch  8, batch     0 | loss: 0.5498563Losses:  0.009481078945100307 0.28690940141677856
MemoryTrain:  epoch  8, batch     1 | loss: 0.2963905Losses:  0.004798850975930691 0.526280403137207
MemoryTrain:  epoch  9, batch     0 | loss: 0.5310792Losses:  0.017661619931459427 0.2990580201148987
MemoryTrain:  epoch  9, batch     1 | loss: 0.3167197
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 59.05%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 57.50%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 55.85%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 56.45%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 57.39%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 58.46%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 59.29%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 60.07%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 61.15%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 66.63%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 66.09%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 65.73%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 65.78%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.08%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 90.44%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.37%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 91.22%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.85%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.48%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.30%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 91.78%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 90.89%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 89.96%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.62%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 89.45%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 89.65%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 89.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 89.91%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.93%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 89.98%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 89.86%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.72%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.53%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 89.34%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 89.16%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 88.75%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.81%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 88.57%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 88.03%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 87.87%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 87.43%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 87.21%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 86.93%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 86.66%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 86.32%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 86.13%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 85.35%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 84.77%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 84.41%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 83.98%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.76%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 83.35%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 83.14%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 82.69%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 82.05%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 81.62%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 81.19%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 80.77%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 80.30%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 79.95%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.56%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 78.99%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 78.50%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 78.01%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 77.59%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 77.29%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.10%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.25%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 78.51%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 78.70%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 78.17%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 77.71%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 77.13%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 76.88%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 76.48%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 77.05%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 76.53%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 76.25%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 75.89%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 75.70%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 75.48%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 75.22%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.68%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 75.95%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 75.49%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 75.20%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 74.60%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 74.16%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.37%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 75.08%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 75.70%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.73%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 75.73%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 75.65%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 75.57%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 75.39%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 75.18%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 74.90%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 74.79%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 74.59%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 74.59%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 74.56%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 74.49%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 74.14%   
cur_acc:  ['0.9464', '0.6786', '0.6508']
his_acc:  ['0.9464', '0.7960', '0.7414']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  0.03643316030502319 1.7187411785125732
CurrentTrain: epoch  0, batch     0 | loss: 1.7551744Losses:  0.011226657778024673 1.5452544689178467
CurrentTrain: epoch  0, batch     1 | loss: 1.5564811Losses:  0.026768745854496956 1.5878305435180664
CurrentTrain: epoch  0, batch     2 | loss: 1.6145993Losses:  0.0001190971743199043 0.22996416687965393
CurrentTrain: epoch  0, batch     3 | loss: 0.2300833Losses:  0.00532479677349329 1.3353667259216309
CurrentTrain: epoch  1, batch     0 | loss: 1.3406916Losses:  0.0040015787817537785 1.0584772825241089
CurrentTrain: epoch  1, batch     1 | loss: 1.0624789Losses:  0.015406252816319466 1.5350372791290283
CurrentTrain: epoch  1, batch     2 | loss: 1.5504435Losses:  6.413665687432513e-05 0.03077591396868229
CurrentTrain: epoch  1, batch     3 | loss: 0.0308401Losses:  0.014814591966569424 1.2480541467666626
CurrentTrain: epoch  2, batch     0 | loss: 1.2628688Losses:  0.007878436706960201 1.033562183380127
CurrentTrain: epoch  2, batch     1 | loss: 1.0414406Losses:  0.011479245498776436 0.8286786675453186
CurrentTrain: epoch  2, batch     2 | loss: 0.8401579Losses:  0.00025907534291036427 0.0468316525220871
CurrentTrain: epoch  2, batch     3 | loss: 0.0470907Losses:  0.010990411974489689 0.7178564071655273
CurrentTrain: epoch  3, batch     0 | loss: 0.7288468Losses:  0.0037087835371494293 1.0006403923034668
CurrentTrain: epoch  3, batch     1 | loss: 1.0043492Losses:  0.008841454982757568 0.8934792280197144
CurrentTrain: epoch  3, batch     2 | loss: 0.9023207Losses:  0.0007329288055188954 0.0636734813451767
CurrentTrain: epoch  3, batch     3 | loss: 0.0644064Losses:  0.0014951328048482537 0.6615455150604248
CurrentTrain: epoch  4, batch     0 | loss: 0.6630406Losses:  0.0016730009810999036 0.6497869491577148
CurrentTrain: epoch  4, batch     1 | loss: 0.6514599Losses:  0.020774340257048607 0.4409406781196594
CurrentTrain: epoch  4, batch     2 | loss: 0.4617150Losses:  0.00011063233978347853 0.06360603868961334
CurrentTrain: epoch  4, batch     3 | loss: 0.0637167Losses:  0.009413324296474457 0.6735494136810303
CurrentTrain: epoch  5, batch     0 | loss: 0.6829627Losses:  0.010081716813147068 0.6603944897651672
CurrentTrain: epoch  5, batch     1 | loss: 0.6704762Losses:  0.009820067323744297 0.6419373154640198
CurrentTrain: epoch  5, batch     2 | loss: 0.6517574Losses:  0.0 0.0
CurrentTrain: epoch  5, batch     3 | loss: 0.0000000Losses:  0.00687168724834919 0.3360404670238495
CurrentTrain: epoch  6, batch     0 | loss: 0.3429122Losses:  0.0013103608507663012 0.4394257068634033
CurrentTrain: epoch  6, batch     1 | loss: 0.4407361Losses:  0.0024883411824703217 0.5737050771713257
CurrentTrain: epoch  6, batch     2 | loss: 0.5761934Losses:  0.0004683637525886297 0.11926229298114777
CurrentTrain: epoch  6, batch     3 | loss: 0.1197307Losses:  0.0022404075134545565 0.35754019021987915
CurrentTrain: epoch  7, batch     0 | loss: 0.3597806Losses:  0.00519784027710557 0.5258394479751587
CurrentTrain: epoch  7, batch     1 | loss: 0.5310373Losses:  0.0024622203782200813 0.5848312377929688
CurrentTrain: epoch  7, batch     2 | loss: 0.5872934Losses:  0.0017151035135611892 0.17983447015285492
CurrentTrain: epoch  7, batch     3 | loss: 0.1815496Losses:  0.006027489900588989 0.38108131289482117
CurrentTrain: epoch  8, batch     0 | loss: 0.3871088Losses:  0.002116650575771928 0.3362556993961334
CurrentTrain: epoch  8, batch     1 | loss: 0.3383723Losses:  0.001341961557045579 0.3632069528102875
CurrentTrain: epoch  8, batch     2 | loss: 0.3645489Losses:  3.60614612873178e-05 0.09067822992801666
CurrentTrain: epoch  8, batch     3 | loss: 0.0907143Losses:  0.0015044966712594032 0.3719016909599304
CurrentTrain: epoch  9, batch     0 | loss: 0.3734062Losses:  0.009636476635932922 0.3846960961818695
CurrentTrain: epoch  9, batch     1 | loss: 0.3943326Losses:  0.0012011651415377855 0.3239404559135437
CurrentTrain: epoch  9, batch     2 | loss: 0.3251416Losses:  0.0003993116260971874 0.10832538455724716
CurrentTrain: epoch  9, batch     3 | loss: 0.1087247
Losses:  0.003920658957213163 0.6098953485488892
MemoryTrain:  epoch  0, batch     0 | loss: 0.6138160Losses:  0.003390095429494977 0.3724159598350525
MemoryTrain:  epoch  0, batch     1 | loss: 0.3758061Losses:  0.0035619866102933884 0.3622938394546509
MemoryTrain:  epoch  0, batch     2 | loss: 0.3658558Losses:  0.0027003129944205284 0.43397945165634155
MemoryTrain:  epoch  1, batch     0 | loss: 0.4366798Losses:  0.0034942328929901123 0.3657348155975342
MemoryTrain:  epoch  1, batch     1 | loss: 0.3692290Losses:  0.001112739322707057 0.338541716337204
MemoryTrain:  epoch  1, batch     2 | loss: 0.3396544Losses:  0.009217949584126472 0.4867473840713501
MemoryTrain:  epoch  2, batch     0 | loss: 0.4959653Losses:  0.0027544747572392225 0.42345112562179565
MemoryTrain:  epoch  2, batch     1 | loss: 0.4262056Losses:  0.0021508142817765474 0.31142252683639526
MemoryTrain:  epoch  2, batch     2 | loss: 0.3135733Losses:  0.003315707203000784 0.5392722487449646
MemoryTrain:  epoch  3, batch     0 | loss: 0.5425879Losses:  0.0017449541483074427 0.27448827028274536
MemoryTrain:  epoch  3, batch     1 | loss: 0.2762332Losses:  0.0022501065395772457 0.27065756916999817
MemoryTrain:  epoch  3, batch     2 | loss: 0.2729077Losses:  0.00960836373269558 0.4833943843841553
MemoryTrain:  epoch  4, batch     0 | loss: 0.4930027Losses:  0.01031937450170517 0.4106638431549072
MemoryTrain:  epoch  4, batch     1 | loss: 0.4209832Losses:  0.0019396048737689853 0.22079689800739288
MemoryTrain:  epoch  4, batch     2 | loss: 0.2227365Losses:  0.009817618876695633 0.4036385416984558
MemoryTrain:  epoch  5, batch     0 | loss: 0.4134562Losses:  0.005333639215677977 0.5795009732246399
MemoryTrain:  epoch  5, batch     1 | loss: 0.5848346Losses:  0.0012861816212534904 0.1925019919872284
MemoryTrain:  epoch  5, batch     2 | loss: 0.1937882Losses:  0.00622746255248785 0.36825335025787354
MemoryTrain:  epoch  6, batch     0 | loss: 0.3744808Losses:  0.005418670363724232 0.3956575393676758
MemoryTrain:  epoch  6, batch     1 | loss: 0.4010762Losses:  0.007411422207951546 0.31374984979629517
MemoryTrain:  epoch  6, batch     2 | loss: 0.3211613Losses:  0.007263094186782837 0.3364163041114807
MemoryTrain:  epoch  7, batch     0 | loss: 0.3436794Losses:  0.007368250750005245 0.373105525970459
MemoryTrain:  epoch  7, batch     1 | loss: 0.3804738Losses:  0.0006717279320582747 0.24314644932746887
MemoryTrain:  epoch  7, batch     2 | loss: 0.2438182Losses:  0.006914976984262466 0.28143173456192017
MemoryTrain:  epoch  8, batch     0 | loss: 0.2883467Losses:  0.003165759611874819 0.4108542203903198
MemoryTrain:  epoch  8, batch     1 | loss: 0.4140200Losses:  0.002426473656669259 0.26710015535354614
MemoryTrain:  epoch  8, batch     2 | loss: 0.2695266Losses:  0.006728228647261858 0.4458789825439453
MemoryTrain:  epoch  9, batch     0 | loss: 0.4526072Losses:  0.002092415001243353 0.3034989833831787
MemoryTrain:  epoch  9, batch     1 | loss: 0.3055914Losses:  0.0008764964295551181 0.168779194355011
MemoryTrain:  epoch  9, batch     2 | loss: 0.1696557
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 59.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 68.58%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 76.10%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 76.02%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.32%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 76.51%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 77.52%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 76.79%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.50%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 88.27%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 87.93%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 86.68%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 86.49%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 86.52%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.74%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 86.95%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.23%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 87.41%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 87.24%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.16%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 86.92%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 86.77%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 86.54%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 86.39%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 86.17%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 86.03%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 85.75%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 85.17%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 84.82%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 84.41%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 84.30%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 84.12%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 83.81%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 83.50%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 83.12%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 82.90%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 82.61%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 82.12%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 81.65%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 81.32%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 80.67%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 80.29%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 80.05%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 79.62%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 79.15%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 78.68%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 78.34%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 77.64%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 77.20%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:  106 | acc: 12.50%,  total acc: 76.46%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.87%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 75.40%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 74.89%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 74.44%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 74.11%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 73.95%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 75.31%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 75.55%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 75.15%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.85%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 74.42%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 74.13%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 73.85%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.97%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.97%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 74.03%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 73.92%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.62%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.23%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 73.02%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.81%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.57%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 72.86%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 72.47%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 72.28%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 71.90%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.47%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 71.54%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.64%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 72.26%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 72.31%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 72.64%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 72.65%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 72.67%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 72.75%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 72.41%   [EVAL] batch:  176 | acc: 18.75%,  total acc: 72.10%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 71.91%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 71.75%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 71.56%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 71.37%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 71.41%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 71.50%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 71.45%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 71.51%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 71.49%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 71.51%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.81%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 71.86%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 71.89%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 71.64%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 71.41%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 71.37%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 71.23%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 71.04%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 70.88%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 70.63%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 70.51%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 70.32%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 70.08%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.03%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 70.99%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  224 | acc: 75.00%,  total acc: 71.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 72.51%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 72.46%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 72.52%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 72.75%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 72.95%   
cur_acc:  ['0.9464', '0.6786', '0.6508', '0.7679']
his_acc:  ['0.9464', '0.7960', '0.7414', '0.7295']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  0.009613292291760445 1.2536139488220215
CurrentTrain: epoch  0, batch     0 | loss: 1.2632272Losses:  0.014626098796725273 1.4183130264282227
CurrentTrain: epoch  0, batch     1 | loss: 1.4329392Losses:  0.0017738675232976675 1.0182442665100098
CurrentTrain: epoch  0, batch     2 | loss: 1.0200181Losses:  0.0 8.94069742685133e-08
CurrentTrain: epoch  0, batch     3 | loss: 0.0000001Losses:  0.01998128369450569 1.0196641683578491
CurrentTrain: epoch  1, batch     0 | loss: 1.0396454Losses:  0.0038048026617616415 1.2016183137893677
CurrentTrain: epoch  1, batch     1 | loss: 1.2054231Losses:  0.002679112832993269 0.9403553009033203
CurrentTrain: epoch  1, batch     2 | loss: 0.9430344Losses:  4.130687375436537e-05 0.2989863157272339
CurrentTrain: epoch  1, batch     3 | loss: 0.2990276Losses:  0.003959632944315672 1.148937702178955
CurrentTrain: epoch  2, batch     0 | loss: 1.1528974Losses:  0.0019261479610577226 0.9982464909553528
CurrentTrain: epoch  2, batch     1 | loss: 1.0001726Losses:  0.003547721542418003 0.7933865189552307
CurrentTrain: epoch  2, batch     2 | loss: 0.7969342Losses:  7.999263470992446e-05 0.03650352358818054
CurrentTrain: epoch  2, batch     3 | loss: 0.0365835Losses:  0.0054811155423521996 0.8005009889602661
CurrentTrain: epoch  3, batch     0 | loss: 0.8059821Losses:  0.003640972077846527 0.5951601266860962
CurrentTrain: epoch  3, batch     1 | loss: 0.5988011Losses:  0.007534483447670937 0.7072908878326416
CurrentTrain: epoch  3, batch     2 | loss: 0.7148254Losses:  0.00013149649021215737 0.24207527935504913
CurrentTrain: epoch  3, batch     3 | loss: 0.2422068Losses:  0.0021601184271275997 0.5563684701919556
CurrentTrain: epoch  4, batch     0 | loss: 0.5585286Losses:  0.002698383294045925 0.6499001979827881
CurrentTrain: epoch  4, batch     1 | loss: 0.6525986Losses:  0.004640400409698486 0.7047966122627258
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  0.9295225143432617 1.8446838855743408
CurrentTrain: epoch  0, batch     0 | loss: 2.7742064Losses:  0.982611894607544 2.1807472705841064
CurrentTrain: epoch  0, batch     1 | loss: 3.1633592Losses:  0.8956565856933594 1.6977624893188477
CurrentTrain: epoch  0, batch     2 | loss: 2.5934191Losses:  0.9430186152458191 1.7088372707366943
CurrentTrain: epoch  0, batch     3 | loss: 2.6518559Losses:  0.8545889854431152 1.591655969619751
CurrentTrain: epoch  0, batch     4 | loss: 2.4462450Losses:  0.9439846277236938 1.511876106262207
CurrentTrain: epoch  0, batch     5 | loss: 2.4558606Losses:  0.8149430751800537 1.7316639423370361
CurrentTrain: epoch  0, batch     6 | loss: 2.5466070Losses:  0.6658675074577332 1.4819717407226562
CurrentTrain: epoch  0, batch     7 | loss: 2.1478393Losses:  0.7430092692375183 1.4861581325531006
CurrentTrain: epoch  0, batch     8 | loss: 2.2291675Losses:  0.8974745273590088 1.7226237058639526
CurrentTrain: epoch  0, batch     9 | loss: 2.6200981Losses:  0.8087893724441528 1.4385414123535156
CurrentTrain: epoch  0, batch    10 | loss: 2.2473307Losses:  0.6516962051391602 1.2820483446121216
CurrentTrain: epoch  0, batch    11 | loss: 1.9337445Losses:  0.7746238112449646 1.4642542600631714
CurrentTrain: epoch  0, batch    12 | loss: 2.2388780Losses:  0.8336331844329834 1.396705985069275
CurrentTrain: epoch  0, batch    13 | loss: 2.2303391Losses:  0.5827069282531738 1.461441159248352
CurrentTrain: epoch  0, batch    14 | loss: 2.0441480Losses:  0.7344538569450378 1.5780330896377563
CurrentTrain: epoch  0, batch    15 | loss: 2.3124869Losses:  0.8932079672813416 1.1867642402648926
CurrentTrain: epoch  0, batch    16 | loss: 2.0799723Losses:  0.8420132398605347 1.6133004426956177
CurrentTrain: epoch  0, batch    17 | loss: 2.4553137Losses:  0.690909743309021 1.2281007766723633
CurrentTrain: epoch  0, batch    18 | loss: 1.9190105Losses:  0.6809215545654297 1.5758576393127441
CurrentTrain: epoch  0, batch    19 | loss: 2.2567792Losses:  0.4987477660179138 1.1731057167053223
CurrentTrain: epoch  0, batch    20 | loss: 1.6718535Losses:  0.6120032668113708 1.3486460447311401
CurrentTrain: epoch  0, batch    21 | loss: 1.9606493Losses:  0.5996679067611694 1.2747629880905151
CurrentTrain: epoch  0, batch    22 | loss: 1.8744309Losses:  0.5927489399909973 1.0266764163970947
CurrentTrain: epoch  0, batch    23 | loss: 1.6194253Losses:  0.5666517615318298 1.27103853225708
CurrentTrain: epoch  0, batch    24 | loss: 1.8376904Losses:  0.5638896822929382 1.0052422285079956
CurrentTrain: epoch  0, batch    25 | loss: 1.5691319Losses:  0.5448875427246094 1.03211510181427
CurrentTrain: epoch  0, batch    26 | loss: 1.5770026Losses:  0.4848446547985077 1.1635597944259644
CurrentTrain: epoch  0, batch    27 | loss: 1.6484045Losses:  0.5775143504142761 1.0616064071655273
CurrentTrain: epoch  0, batch    28 | loss: 1.6391208Losses:  0.576872706413269 1.1917093992233276
CurrentTrain: epoch  0, batch    29 | loss: 1.7685821Losses:  0.6660768389701843 1.2710294723510742
CurrentTrain: epoch  0, batch    30 | loss: 1.9371064Losses:  0.5429606437683105 0.9440088272094727
CurrentTrain: epoch  0, batch    31 | loss: 1.4869695Losses:  0.5644457340240479 0.899773120880127
CurrentTrain: epoch  0, batch    32 | loss: 1.4642189Losses:  0.3940557539463043 0.8047605752944946
CurrentTrain: epoch  0, batch    33 | loss: 1.1988163Losses:  0.5230745077133179 0.9010438919067383
CurrentTrain: epoch  0, batch    34 | loss: 1.4241184Losses:  0.4414309561252594 0.9905939698219299
CurrentTrain: epoch  0, batch    35 | loss: 1.4320250Losses:  0.4765568673610687 1.1945152282714844
CurrentTrain: epoch  0, batch    36 | loss: 1.6710721Losses:  0.5482378005981445 0.988632082939148
CurrentTrain: epoch  0, batch    37 | loss: 1.5368699Losses:  0.5065411329269409 1.1606377363204956
CurrentTrain: epoch  0, batch    38 | loss: 1.6671789Losses:  0.5246056914329529 1.067500352859497
CurrentTrain: epoch  0, batch    39 | loss: 1.5921061Losses:  0.5570380687713623 1.0678606033325195
CurrentTrain: epoch  0, batch    40 | loss: 1.6248987Losses:  0.3886769413948059 0.8833787441253662
CurrentTrain: epoch  0, batch    41 | loss: 1.2720556Losses:  0.35838764905929565 0.8862327337265015
CurrentTrain: epoch  0, batch    42 | loss: 1.2446203Losses:  0.29173505306243896 0.6655756235122681
CurrentTrain: epoch  0, batch    43 | loss: 0.9573107Losses:  0.3795679211616516 0.8659235835075378
CurrentTrain: epoch  0, batch    44 | loss: 1.2454915Losses:  0.36816656589508057 0.937461256980896
CurrentTrain: epoch  0, batch    45 | loss: 1.3056278Losses:  0.5460601449012756 0.7787249088287354
CurrentTrain: epoch  0, batch    46 | loss: 1.3247850Losses:  0.24601460993289948 0.8971659541130066
CurrentTrain: epoch  0, batch    47 | loss: 1.1431806Losses:  0.6087154150009155 1.0782959461212158
CurrentTrain: epoch  0, batch    48 | loss: 1.6870114Losses:  0.2791518568992615 0.9915822148323059
CurrentTrain: epoch  0, batch    49 | loss: 1.2707341Losses:  0.43866026401519775 0.9124711155891418
CurrentTrain: epoch  0, batch    50 | loss: 1.3511314Losses:  0.36001867055892944 0.8604127764701843
CurrentTrain: epoch  0, batch    51 | loss: 1.2204314Losses:  0.30445629358291626 0.7440464496612549
CurrentTrain: epoch  0, batch    52 | loss: 1.0485027Losses:  0.3199459910392761 0.8813978433609009
CurrentTrain: epoch  0, batch    53 | loss: 1.2013438Losses:  0.40954166650772095 1.0480618476867676
CurrentTrain: epoch  0, batch    54 | loss: 1.4576035Losses:  0.2862982749938965 0.764238715171814
CurrentTrain: epoch  0, batch    55 | loss: 1.0505370Losses:  0.4181794226169586 0.8518425822257996
CurrentTrain: epoch  0, batch    56 | loss: 1.2700220Losses:  0.37519800662994385 0.7917658090591431
CurrentTrain: epoch  0, batch    57 | loss: 1.1669638Losses:  0.3890881836414337 0.6601521372795105
CurrentTrain: epoch  0, batch    58 | loss: 1.0492404Losses:  0.2177087366580963 0.6003724932670593
CurrentTrain: epoch  0, batch    59 | loss: 0.8180813Losses:  0.18861998617649078 0.6508351564407349
CurrentTrain: epoch  0, batch    60 | loss: 0.8394551Losses:  0.3522769808769226 0.5393581986427307
CurrentTrain: epoch  0, batch    61 | loss: 0.8916352Losses:  0.5749781131744385 0.6377736330032349
CurrentTrain: epoch  0, batch    62 | loss: 1.2127517Losses:  0.25282052159309387 0.6986081600189209
CurrentTrain: epoch  1, batch     0 | loss: 0.9514287Losses:  0.2556470036506653 0.5849999785423279
CurrentTrain: epoch  1, batch     1 | loss: 0.8406470Losses:  0.25157061219215393 0.7246474623680115
CurrentTrain: epoch  1, batch     2 | loss: 0.9762181Losses:  0.1796225905418396 0.6541820764541626
CurrentTrain: epoch  1, batch     3 | loss: 0.8338047Losses:  0.3504107594490051 0.6873186230659485
CurrentTrain: epoch  1, batch     4 | loss: 1.0377294Losses:  0.20863553881645203 0.4673726260662079
CurrentTrain: epoch  1, batch     5 | loss: 0.6760082Losses:  0.22720246016979218 0.6528297662734985
CurrentTrain: epoch  1, batch     6 | loss: 0.8800322Losses:  0.13790345191955566 0.5869371294975281
CurrentTrain: epoch  1, batch     7 | loss: 0.7248406Losses:  0.1476905345916748 0.4749937951564789
CurrentTrain: epoch  1, batch     8 | loss: 0.6226844Losses:  0.29741397500038147 0.685890793800354
CurrentTrain: epoch  1, batch     9 | loss: 0.9833047Losses:  0.23016908764839172 0.5511612296104431
CurrentTrain: epoch  1, batch    10 | loss: 0.7813303Losses:  0.20756343007087708 0.7050517201423645
CurrentTrain: epoch  1, batch    11 | loss: 0.9126152Losses:  0.25946739315986633 0.6474772691726685
CurrentTrain: epoch  1, batch    12 | loss: 0.9069446Losses:  0.25898614525794983 0.48768553137779236
CurrentTrain: epoch  1, batch    13 | loss: 0.7466717Losses:  0.09445863962173462 0.5714225769042969
CurrentTrain: epoch  1, batch    14 | loss: 0.6658812Losses:  0.3518396019935608 0.598778486251831
CurrentTrain: epoch  1, batch    15 | loss: 0.9506181Losses:  0.14796777069568634 0.5449710488319397
CurrentTrain: epoch  1, batch    16 | loss: 0.6929388Losses:  0.14337393641471863 0.3618636131286621
CurrentTrain: epoch  1, batch    17 | loss: 0.5052376Losses:  0.10310357809066772 0.43387719988822937
CurrentTrain: epoch  1, batch    18 | loss: 0.5369807Losses:  0.21656470000743866 0.48010939359664917
CurrentTrain: epoch  1, batch    19 | loss: 0.6966741Losses:  0.31974461674690247 0.6196513772010803
CurrentTrain: epoch  1, batch    20 | loss: 0.9393960Losses:  0.2105329930782318 0.6422757506370544
CurrentTrain: epoch  1, batch    21 | loss: 0.8528087Losses:  0.184583842754364 0.6173282861709595
CurrentTrain: epoch  1, batch    22 | loss: 0.8019121Losses:  0.2506334185600281 0.37712180614471436
CurrentTrain: epoch  1, batch    23 | loss: 0.6277552Losses:  0.18445482850074768 0.5365240573883057
CurrentTrain: epoch  1, batch    24 | loss: 0.7209789Losses:  0.058277495205402374 0.39855748414993286
CurrentTrain: epoch  1, batch    25 | loss: 0.4568350Losses:  0.10575370490550995 0.5199784636497498
CurrentTrain: epoch  1, batch    26 | loss: 0.6257322Losses:  0.20423269271850586 0.5952969789505005
CurrentTrain: epoch  1, batch    27 | loss: 0.7995297Losses:  0.1300545036792755 0.41771650314331055
CurrentTrain: epoch  1, batch    28 | loss: 0.5477710Losses:  0.08778541535139084 0.5048055648803711
CurrentTrain: epoch  1, batch    29 | loss: 0.5925910Losses:  0.09244643151760101 0.35138607025146484
CurrentTrain: epoch  1, batch    30 | loss: 0.4438325Losses:  0.08466875553131104 0.48436790704727173
CurrentTrain: epoch  1, batch    31 | loss: 0.5690367Losses:  0.09211128205060959 0.5261886119842529
CurrentTrain: epoch  1, batch    32 | loss: 0.6182999Losses:  0.10536646097898483 0.33019179105758667
CurrentTrain: epoch  1, batch    33 | loss: 0.4355583Losses:  0.32765793800354004 0.6667546033859253
CurrentTrain: epoch  1, batch    34 | loss: 0.9944125Losses:  0.06294138729572296 0.4852355122566223
CurrentTrain: epoch  1, batch    35 | loss: 0.5481769Losses:  0.15061236917972565 0.28584957122802734
CurrentTrain: epoch  1, batch    36 | loss: 0.4364619Losses:  0.20575261116027832 0.5256115794181824
CurrentTrain: epoch  1, batch    37 | loss: 0.7313642Losses:  0.28714612126350403 0.620682954788208
CurrentTrain: epoch  1, batch    38 | loss: 0.9078290Losses:  0.05271616205573082 0.3252940773963928
CurrentTrain: epoch  1, batch    39 | loss: 0.3780102Losses:  0.17445780336856842 0.3910243511199951
CurrentTrain: epoch  1, batch    40 | loss: 0.5654821Losses:  0.23745524883270264 0.6715449690818787
CurrentTrain: epoch  1, batch    41 | loss: 0.9090002Losses:  0.2927764058113098 0.5525250434875488
CurrentTrain: epoch  1, batch    42 | loss: 0.8453014Losses:  0.263506680727005 0.4729596972465515
CurrentTrain: epoch  1, batch    43 | loss: 0.7364664Losses:  0.15719318389892578 0.4242357015609741
CurrentTrain: epoch  1, batch    44 | loss: 0.5814289Losses:  0.0827970802783966 0.3787047564983368
CurrentTrain: epoch  1, batch    45 | loss: 0.4615018Losses:  0.05057937279343605 0.40541622042655945
CurrentTrain: epoch  1, batch    46 | loss: 0.4559956Losses:  0.1655973643064499 0.5524630546569824
CurrentTrain: epoch  1, batch    47 | loss: 0.7180604Losses:  0.17678704857826233 0.4769507944583893
CurrentTrain: epoch  1, batch    48 | loss: 0.6537378Losses:  0.12572580575942993 0.4310147166252136
CurrentTrain: epoch  1, batch    49 | loss: 0.5567405Losses:  0.15349224209785461 0.3950119912624359
CurrentTrain: epoch  1, batch    50 | loss: 0.5485042Losses:  0.30070555210113525 0.6247789859771729
CurrentTrain: epoch  1, batch    51 | loss: 0.9254845Losses:  0.23473583161830902 0.459098756313324
CurrentTrain: epoch  1, batch    52 | loss: 0.6938346Losses:  0.10154092311859131 0.3538626730442047
CurrentTrain: epoch  1, batch    53 | loss: 0.4554036Losses:  0.09347967803478241 0.45027956366539
CurrentTrain: epoch  1, batch    54 | loss: 0.5437592Losses:  0.0266964603215456 0.1685047149658203
CurrentTrain: epoch  1, batch    55 | loss: 0.1952012Losses:  0.0518590472638607 0.25941202044487
CurrentTrain: epoch  1, batch    56 | loss: 0.3112711Losses:  0.14173954725265503 0.41969430446624756
CurrentTrain: epoch  1, batch    57 | loss: 0.5614339Losses:  0.07784982770681381 0.24869078397750854
CurrentTrain: epoch  1, batch    58 | loss: 0.3265406Losses:  0.25468185544013977 0.550891637802124
CurrentTrain: epoch  1, batch    59 | loss: 0.8055735Losses:  0.22382625937461853 0.5206327438354492
CurrentTrain: epoch  1, batch    60 | loss: 0.7444590Losses:  0.0759308934211731 0.3880663812160492
CurrentTrain: epoch  1, batch    61 | loss: 0.4639973Losses:  0.02864493429660797 0.12888818979263306
CurrentTrain: epoch  1, batch    62 | loss: 0.1575331Losses:  0.15035896003246307 0.30456769466400146
CurrentTrain: epoch  2, batch     0 | loss: 0.4549267Losses:  0.09998148679733276 0.31517231464385986
CurrentTrain: epoch  2, batch     1 | loss: 0.4151538Losses:  0.1303534358739853 0.4178392291069031
CurrentTrain: epoch  2, batch     2 | loss: 0.5481927Losses:  0.04027794674038887 0.35665053129196167
CurrentTrain: epoch  2, batch     3 | loss: 0.3969285Losses:  0.11037503182888031 0.32257014513015747
CurrentTrain: epoch  2, batch     4 | loss: 0.4329452Losses:  0.0640927255153656 0.37205949425697327
CurrentTrain: epoch  2, batch     5 | loss: 0.4361522Losses:  0.0399237796664238 0.23992782831192017
CurrentTrain: epoch  2, batch     6 | loss: 0.2798516Losses:  0.09530465304851532 0.2949446737766266
CurrentTrain: epoch  2, batch     7 | loss: 0.3902493Losses:  0.47238364815711975 0.40691572427749634
CurrentTrain: epoch  2, batch     8 | loss: 0.8792994Losses:  0.04893963038921356 0.20690971612930298
CurrentTrain: epoch  2, batch     9 | loss: 0.2558494Losses:  0.027489667758345604 0.31156468391418457
CurrentTrain: epoch  2, batch    10 | loss: 0.3390543Losses:  0.06087741255760193 0.3158901333808899
CurrentTrain: epoch  2, batch    11 | loss: 0.3767675Losses:  0.12951159477233887 0.3734263777732849
CurrentTrain: epoch  2, batch    12 | loss: 0.5029380Losses:  0.08651552349328995 0.2783817648887634
CurrentTrain: epoch  2, batch    13 | loss: 0.3648973Losses:  0.0757685974240303 0.2785971164703369
CurrentTrain: epoch  2, batch    14 | loss: 0.3543657Losses:  0.04396011680364609 0.25860562920570374
CurrentTrain: epoch  2, batch    15 | loss: 0.3025658Losses:  0.025118160992860794 0.2364383041858673
CurrentTrain: epoch  2, batch    16 | loss: 0.2615565Losses:  0.032893359661102295 0.23082728683948517
CurrentTrain: epoch  2, batch    17 | loss: 0.2637206Losses:  0.14211405813694 0.320989191532135
CurrentTrain: epoch  2, batch    18 | loss: 0.4631032Losses:  0.09705540537834167 0.22278454899787903
CurrentTrain: epoch  2, batch    19 | loss: 0.3198400Losses:  0.04071672260761261 0.28435179591178894
CurrentTrain: epoch  2, batch    20 | loss: 0.3250685Losses:  0.045483484864234924 0.22892135381698608
CurrentTrain: epoch  2, batch    21 | loss: 0.2744048Losses:  0.02360868826508522 0.2789429724216461
CurrentTrain: epoch  2, batch    22 | loss: 0.3025517Losses:  0.0635823905467987 0.24216723442077637
CurrentTrain: epoch  2, batch    23 | loss: 0.3057496Losses:  0.09342837333679199 0.286807119846344
CurrentTrain: epoch  2, batch    24 | loss: 0.3802355Losses:  0.04410329461097717 0.2221963107585907
CurrentTrain: epoch  2, batch    25 | loss: 0.2662996Losses:  0.02501840889453888 0.2053881585597992
CurrentTrain: epoch  2, batch    26 | loss: 0.2304066Losses:  0.03234906867146492 0.23070116341114044
CurrentTrain: epoch  2, batch    27 | loss: 0.2630502Losses:  0.02602173015475273 0.24959182739257812
CurrentTrain: epoch  2, batch    28 | loss: 0.2756135Losses:  0.029890261590480804 0.27563977241516113
CurrentTrain: epoch  2, batch    29 | loss: 0.3055300Losses:  0.10384288430213928 0.3512383699417114
CurrentTrain: epoch  2, batch    30 | loss: 0.4550813Losses:  0.044723883271217346 0.28045210242271423
CurrentTrain: epoch  2, batch    31 | loss: 0.3251760Losses:  0.39286649227142334 0.38147473335266113
CurrentTrain: epoch  2, batch    32 | loss: 0.7743412Losses:  0.050765104591846466 0.2787390947341919
CurrentTrain: epoch  2, batch    33 | loss: 0.3295042Losses:  0.1710938811302185 0.3659725785255432
CurrentTrain: epoch  2, batch    34 | loss: 0.5370665Losses:  0.05362296104431152 0.21248561143875122
CurrentTrain: epoch  2, batch    35 | loss: 0.2661086Losses:  0.014295828528702259 0.20227141678333282
CurrentTrain: epoch  2, batch    36 | loss: 0.2165672Losses:  0.03685172647237778 0.23503810167312622
CurrentTrain: epoch  2, batch    37 | loss: 0.2718898Losses:  0.03419541195034981 0.16404908895492554
CurrentTrain: epoch  2, batch    38 | loss: 0.1982445Losses:  0.015423853881657124 0.19017043709754944
CurrentTrain: epoch  2, batch    39 | loss: 0.2055943Losses:  0.03854335844516754 0.2683897316455841
CurrentTrain: epoch  2, batch    40 | loss: 0.3069331Losses:  0.09859474748373032 0.27736496925354004
CurrentTrain: epoch  2, batch    41 | loss: 0.3759597Losses:  0.027080871164798737 0.2422974556684494
CurrentTrain: epoch  2, batch    42 | loss: 0.2693783Losses:  0.05246125906705856 0.22693203389644623
CurrentTrain: epoch  2, batch    43 | loss: 0.2793933Losses:  0.05324337258934975 0.26445990800857544
CurrentTrain: epoch  2, batch    44 | loss: 0.3177033Losses:  0.07521583884954453 0.20846134424209595
CurrentTrain: epoch  2, batch    45 | loss: 0.2836772Losses:  0.019945451989769936 0.15438559651374817
CurrentTrain: epoch  2, batch    46 | loss: 0.1743311Losses:  0.12954312562942505 0.31845495104789734
CurrentTrain: epoch  2, batch    47 | loss: 0.4479981Losses:  0.021174082532525063 0.19279611110687256
CurrentTrain: epoch  2, batch    48 | loss: 0.2139702Losses:  0.0429656058549881 0.25677627325057983
CurrentTrain: epoch  2, batch    49 | loss: 0.2997419Losses:  0.051183465868234634 0.1736106276512146
CurrentTrain: epoch  2, batch    50 | loss: 0.2247941Losses:  0.01778777688741684 0.2009192407131195
CurrentTrain: epoch  2, batch    51 | loss: 0.2187070Losses:  0.13608112931251526 0.1771891713142395
CurrentTrain: epoch  2, batch    52 | loss: 0.3132703Losses:  0.04190525412559509 0.2328491508960724
CurrentTrain: epoch  2, batch    53 | loss: 0.2747544Losses:  0.023845160380005836 0.2148841917514801
CurrentTrain: epoch  2, batch    54 | loss: 0.2387294Losses:  0.03544573485851288 0.20637154579162598
CurrentTrain: epoch  2, batch    55 | loss: 0.2418173Losses:  0.10297239571809769 0.2945069670677185
CurrentTrain: epoch  2, batch    56 | loss: 0.3974794Losses:  0.0296026524156332 0.21305006742477417
CurrentTrain: epoch  2, batch    57 | loss: 0.2426527Losses:  0.05191822350025177 0.19813808798789978
CurrentTrain: epoch  2, batch    58 | loss: 0.2500563Losses:  0.013321676291525364 0.15439848601818085
CurrentTrain: epoch  2, batch    59 | loss: 0.1677202Losses:  0.04676227644085884 0.27942150831222534
CurrentTrain: epoch  2, batch    60 | loss: 0.3261838Losses:  0.03109154850244522 0.2090471386909485
CurrentTrain: epoch  2, batch    61 | loss: 0.2401387Losses:  0.01910211332142353 0.07008373737335205
CurrentTrain: epoch  2, batch    62 | loss: 0.0891858Losses:  0.025804217904806137 0.20199625194072723
CurrentTrain: epoch  3, batch     0 | loss: 0.2278005Losses:  0.3731158375740051 0.3099994957447052
CurrentTrain: epoch  3, batch     1 | loss: 0.6831154Losses:  0.04500390216708183 0.24034249782562256
CurrentTrain: epoch  3, batch     2 | loss: 0.2853464Losses:  0.044058602303266525 0.1515274941921234
CurrentTrain: epoch  3, batch     3 | loss: 0.1955861Losses:  0.014253211207687855 0.17888829112052917
CurrentTrain: epoch  3, batch     4 | loss: 0.1931415Losses:  0.03677746653556824 0.16071206331253052
CurrentTrain: epoch  3, batch     5 | loss: 0.1974895Losses:  0.02348323166370392 0.21076150238513947
CurrentTrain: epoch  3, batch     6 | loss: 0.2342447Losses:  0.01506017241626978 0.16617116332054138
CurrentTrain: epoch  3, batch     7 | loss: 0.1812313Losses:  0.09852530062198639 0.26696276664733887
CurrentTrain: epoch  3, batch     8 | loss: 0.3654881Losses:  0.01784522458910942 0.1921311616897583
CurrentTrain: epoch  3, batch     9 | loss: 0.2099764Losses:  0.10209621489048004 0.27788686752319336
CurrentTrain: epoch  3, batch    10 | loss: 0.3799831Losses:  0.026242036372423172 0.16169531643390656
CurrentTrain: epoch  3, batch    11 | loss: 0.1879373Losses:  0.00798337534070015 0.11832654476165771
CurrentTrain: epoch  3, batch    12 | loss: 0.1263099Losses:  0.05254056677222252 0.27954378724098206
CurrentTrain: epoch  3, batch    13 | loss: 0.3320844Losses:  0.01313757710158825 0.17259755730628967
CurrentTrain: epoch  3, batch    14 | loss: 0.1857351Losses:  0.01122249849140644 0.1853117048740387
CurrentTrain: epoch  3, batch    15 | loss: 0.1965342Losses:  0.025753213092684746 0.1979023814201355
CurrentTrain: epoch  3, batch    16 | loss: 0.2236556Losses:  0.04754741117358208 0.25823497772216797
CurrentTrain: epoch  3, batch    17 | loss: 0.3057824Losses:  0.017527995631098747 0.14303134381771088
CurrentTrain: epoch  3, batch    18 | loss: 0.1605593Losses:  0.0330333448946476 0.22824670374393463
CurrentTrain: epoch  3, batch    19 | loss: 0.2612801Losses:  0.022290537133812904 0.17240706086158752
CurrentTrain: epoch  3, batch    20 | loss: 0.1946976Losses:  0.01625053584575653 0.1730792224407196
CurrentTrain: epoch  3, batch    21 | loss: 0.1893298Losses:  0.07315985858440399 0.21819117665290833
CurrentTrain: epoch  3, batch    22 | loss: 0.2913510Losses:  0.017928466200828552 0.19212567806243896
CurrentTrain: epoch  3, batch    23 | loss: 0.2100541Losses:  0.009246358647942543 0.11113610863685608
CurrentTrain: epoch  3, batch    24 | loss: 0.1203825Losses:  0.008403785526752472 0.17857950925827026
CurrentTrain: epoch  3, batch    25 | loss: 0.1869833Losses:  0.01314006932079792 0.1656450629234314
CurrentTrain: epoch  3, batch    26 | loss: 0.1787851Losses:  0.01748408004641533 0.17198807001113892
CurrentTrain: epoch  3, batch    27 | loss: 0.1894722Losses:  0.02486303821206093 0.20822152495384216
CurrentTrain: epoch  3, batch    28 | loss: 0.2330846Losses:  0.046761903911828995 0.1990170031785965
CurrentTrain: epoch  3, batch    29 | loss: 0.2457789Losses:  0.06928873062133789 0.18184912204742432
CurrentTrain: epoch  3, batch    30 | loss: 0.2511379Losses:  0.015486342832446098 0.1382041871547699
CurrentTrain: epoch  3, batch    31 | loss: 0.1536905Losses:  0.07270708680152893 0.18255475163459778
CurrentTrain: epoch  3, batch    32 | loss: 0.2552618Losses:  0.022609110921621323 0.14116160571575165
CurrentTrain: epoch  3, batch    33 | loss: 0.1637707Losses:  0.011014467105269432 0.19131042063236237
CurrentTrain: epoch  3, batch    34 | loss: 0.2023249Losses:  0.01152121089398861 0.2045714557170868
CurrentTrain: epoch  3, batch    35 | loss: 0.2160927Losses:  0.26630616188049316 0.37601375579833984
CurrentTrain: epoch  3, batch    36 | loss: 0.6423199Losses:  0.01909652166068554 0.11573665589094162
CurrentTrain: epoch  3, batch    37 | loss: 0.1348332Losses:  0.008152348920702934 0.12310640513896942
CurrentTrain: epoch  3, batch    38 | loss: 0.1312588Losses:  0.08723683655261993 0.12125617265701294
CurrentTrain: epoch  3, batch    39 | loss: 0.2084930Losses:  0.009606659412384033 0.16362878680229187
CurrentTrain: epoch  3, batch    40 | loss: 0.1732354Losses:  0.00872604176402092 0.12290827929973602
CurrentTrain: epoch  3, batch    41 | loss: 0.1316343Losses:  0.0190109983086586 0.12152367830276489
CurrentTrain: epoch  3, batch    42 | loss: 0.1405347Losses:  0.017647024244070053 0.10316982865333557
CurrentTrain: epoch  3, batch    43 | loss: 0.1208169Losses:  0.036014653742313385 0.19853830337524414
CurrentTrain: epoch  3, batch    44 | loss: 0.2345529Losses:  0.024424470961093903 0.11015962809324265
CurrentTrain: epoch  3, batch    45 | loss: 0.1345841Losses:  0.007628284394741058 0.1230534240603447
CurrentTrain: epoch  3, batch    46 | loss: 0.1306817Losses:  0.012188953347504139 0.11108830571174622
CurrentTrain: epoch  3, batch    47 | loss: 0.1232773Losses:  0.008485663682222366 0.12508608400821686
CurrentTrain: epoch  3, batch    48 | loss: 0.1335717Losses:  0.03780055046081543 0.13963919878005981
CurrentTrain: epoch  3, batch    49 | loss: 0.1774397Losses:  0.016374729573726654 0.15737797319889069
CurrentTrain: epoch  3, batch    50 | loss: 0.1737527Losses:  0.017072435468435287 0.09310558438301086
CurrentTrain: epoch  3, batch    51 | loss: 0.1101780Losses:  0.009751270525157452 0.15889328718185425
CurrentTrain: epoch  3, batch    52 | loss: 0.1686446Losses:  0.01451091468334198 0.1445809304714203
CurrentTrain: epoch  3, batch    53 | loss: 0.1590918Losses:  0.007764006964862347 0.17277184128761292
CurrentTrain: epoch  3, batch    54 | loss: 0.1805359Losses:  0.011577842757105827 0.15549516677856445
CurrentTrain: epoch  3, batch    55 | loss: 0.1670730Losses:  0.012589970603585243 0.1622311770915985
CurrentTrain: epoch  3, batch    56 | loss: 0.1748212Losses:  0.05833014100790024 0.19538190960884094
CurrentTrain: epoch  3, batch    57 | loss: 0.2537121Losses:  0.02974090725183487 0.1025490090250969
CurrentTrain: epoch  3, batch    58 | loss: 0.1322899Losses:  0.028574610128998756 0.08440560102462769
CurrentTrain: epoch  3, batch    59 | loss: 0.1129802Losses:  0.03330164775252342 0.1832505762577057
CurrentTrain: epoch  3, batch    60 | loss: 0.2165522Losses:  0.006540948990732431 0.10644657909870148
CurrentTrain: epoch  3, batch    61 | loss: 0.1129875Losses:  0.004358360543847084 0.03852703422307968
CurrentTrain: epoch  3, batch    62 | loss: 0.0428854Losses:  0.011479873210191727 0.11704530566930771
CurrentTrain: epoch  4, batch     0 | loss: 0.1285252Losses:  0.007523700129240751 0.1191544383764267
CurrentTrain: epoch  4, batch     1 | loss: 0.1266781Losses:  0.00792473554611206 0.09580592811107635
CurrentTrain: epoch  4, batch     2 | loss: 0.1037307Losses:  0.02338707260787487 0.12582458555698395
CurrentTrain: epoch  4, batch     3 | loss: 0.1492117Losses:  0.008836116641759872 0.14176321029663086
CurrentTrain: epoch  4, batch     4 | loss: 0.1505993Losses:  0.0141623355448246 0.15470832586288452
CurrentTrain: epoch  4, batch     5 | loss: 0.1688707Losses:  0.007684588897973299 0.12474803626537323
CurrentTrain: epoch  4, batch     6 | loss: 0.1324326Losses:  0.006568497978150845 0.10930164158344269
CurrentTrain: epoch  4, batch     7 | loss: 0.1158701Losses:  0.013326454907655716 0.1454690396785736
CurrentTrain: epoch  4, batch     8 | loss: 0.1587955Losses:  0.01034410297870636 0.13847899436950684
CurrentTrain: epoch  4, batch     9 | loss: 0.1488231Losses:  0.008238418959081173 0.09798448532819748
CurrentTrain: epoch  4, batch    10 | loss: 0.1062229Losses:  0.026933785527944565 0.18451647460460663
CurrentTrain: epoch  4, batch    11 | loss: 0.2114503Losses:  0.009415026754140854 0.10492463409900665
CurrentTrain: epoch  4, batch    12 | loss: 0.1143397Losses:  0.03608207404613495 0.1790422797203064
CurrentTrain: epoch  4, batch    13 | loss: 0.2151244Losses:  0.003989686258137226 0.0909501314163208
CurrentTrain: epoch  4, batch    14 | loss: 0.0949398Losses:  0.008694193325936794 0.08874189853668213
CurrentTrain: epoch  4, batch    15 | loss: 0.0974361Losses:  0.006932498887181282 0.14920639991760254
CurrentTrain: epoch  4, batch    16 | loss: 0.1561389Losses:  0.017985904589295387 0.09493252635002136
CurrentTrain: epoch  4, batch    17 | loss: 0.1129184Losses:  0.008366224355995655 0.1378135085105896
CurrentTrain: epoch  4, batch    18 | loss: 0.1461797Losses:  0.005958543159067631 0.17171639204025269
CurrentTrain: epoch  4, batch    19 | loss: 0.1776749Losses:  0.08222439885139465 0.15983417630195618
CurrentTrain: epoch  4, batch    20 | loss: 0.2420586Losses:  0.011140147224068642 0.10142229497432709
CurrentTrain: epoch  4, batch    21 | loss: 0.1125624Losses:  0.004042522050440311 0.13798867166042328
CurrentTrain: epoch  4, batch    22 | loss: 0.1420312Losses:  0.0037216655910015106 0.04414699971675873
CurrentTrain: epoch  4, batch    23 | loss: 0.0478687Losses:  0.017747655510902405 0.10913302004337311
CurrentTrain: epoch  4, batch    24 | loss: 0.1268807Losses:  0.0056482236832380295 0.11840613931417465
CurrentTrain: epoch  4, batch    25 | loss: 0.1240544Losses:  0.0048576658591628075 0.1173945963382721
CurrentTrain: epoch  4, batch    26 | loss: 0.1222523Losses:  0.0140426279976964 0.07536125183105469
CurrentTrain: epoch  4, batch    27 | loss: 0.0894039Losses:  0.004855804145336151 0.12270822376012802
CurrentTrain: epoch  4, batch    28 | loss: 0.1275640Losses:  0.009152127429842949 0.08884399384260178
CurrentTrain: epoch  4, batch    29 | loss: 0.0979961Losses:  0.0035603181459009647 0.06660109013319016
CurrentTrain: epoch  4, batch    30 | loss: 0.0701614Losses:  0.019905097782611847 0.12812162935733795
CurrentTrain: epoch  4, batch    31 | loss: 0.1480267Losses:  0.005998279899358749 0.14831192791461945
CurrentTrain: epoch  4, batch    32 | loss: 0.1543102Losses:  0.011960281059145927 0.16231901943683624
CurrentTrain: epoch  4, batch    33 | loss: 0.1742793Losses:  0.00599299231544137 0.06530238687992096
CurrentTrain: epoch  4, batch    34 | loss: 0.0712954Losses:  0.034957293421030045 0.1286831945180893
CurrentTrain: epoch  4, batch    35 | loss: 0.1636405Losses:  0.010505211539566517 0.1344841718673706
CurrentTrain: epoch  4, batch    36 | loss: 0.1449894Losses:  0.014476912096142769 0.1405791938304901
CurrentTrain: epoch  4, batch    37 | loss: 0.1550561Losses:  0.008386265486478806 0.07793134450912476
CurrentTrain: epoch  4, batch    38 | loss: 0.0863176Losses:  0.003979821689426899 0.12372439354658127
CurrentTrain: epoch  4, batch    39 | loss: 0.1277042Losses:  0.004691123031079769 0.07519827783107758
CurrentTrain: epoch  4, batch    40 | loss: 0.0798894Losses:  0.005263728089630604 0.12280868738889694
CurrentTrain: epoch  4, batch    41 | loss: 0.1280724Losses:  0.19949419796466827 0.191810742020607
CurrentTrain: epoch  4, batch    42 | loss: 0.3913049Losses:  0.018066629767417908 0.06748774647712708
CurrentTrain: epoch  4, batch    43 | loss: 0.0855544Losses:  0.00481071975082159 0.08778930455446243
CurrentTrain: epoch  4, batch    44 | loss: 0.0926000Losses:  0.015823744237422943 0.11521285027265549
CurrentTrain: epoch  4, batch    45 | loss: 0.1310366Losses:  0.010453524999320507 0.11625616252422333
CurrentTrain: epoch  4, batch    46 | loss: 0.1267097Losses:  0.009065798483788967 0.12655530869960785
CurrentTrain: epoch  4, batch    47 | loss: 0.1356211Losses:  0.004865111783146858 0.1187540739774704
CurrentTrain: epoch  4, batch    48 | loss: 0.1236192Losses:  0.005582547280937433 0.09821279346942902
CurrentTrain: epoch  4, batch    49 | loss: 0.1037953Losses:  0.012046289630234241 0.16875505447387695
CurrentTrain: epoch  4, batch    50 | loss: 0.1808013Losses:  0.008325149305164814 0.07448611408472061
CurrentTrain: epoch  4, batch    51 | loss: 0.0828113Losses:  0.0060845487751066685 0.12343668937683105
CurrentTrain: epoch  4, batch    52 | loss: 0.1295212Losses:  0.10445540398359299 0.24034219980239868
CurrentTrain: epoch  4, batch    53 | loss: 0.3447976Losses:  0.009098365902900696 0.10448750853538513
CurrentTrain: epoch  4, batch    54 | loss: 0.1135859Losses:  0.007070896215736866 0.13085830211639404
CurrentTrain: epoch  4, batch    55 | loss: 0.1379292Losses:  0.003659401787444949 0.08627130091190338
CurrentTrain: epoch  4, batch    56 | loss: 0.0899307Losses:  0.006913742050528526 0.12561644613742828
CurrentTrain: epoch  4, batch    57 | loss: 0.1325302Losses:  0.006704322062432766 0.0784626454114914
CurrentTrain: epoch  4, batch    58 | loss: 0.0851670Losses:  0.006654542405158281 0.08347175270318985
CurrentTrain: epoch  4, batch    59 | loss: 0.0901263Losses:  0.005600570701062679 0.09991644322872162
CurrentTrain: epoch  4, batch    60 | loss: 0.1055170Losses:  0.010912394151091576 0.09525631368160248
CurrentTrain: epoch  4, batch    61 | loss: 0.1061687Losses:  0.29655012488365173 0.18903112411499023
CurrentTrain: epoch  4, batch    62 | loss: 0.4855812Losses:  0.005784143693745136 0.12072072923183441
CurrentTrain: epoch  5, batch     0 | loss: 0.1265049Losses:  0.006535300984978676 0.13008926808834076
CurrentTrain: epoch  5, batch     1 | loss: 0.1366246Losses:  0.006084431894123554 0.0735698938369751
CurrentTrain: epoch  5, batch     2 | loss: 0.0796543Losses:  0.029661117121577263 0.09248160570859909
CurrentTrain: epoch  5, batch     3 | loss: 0.1221427Losses:  0.00679941987618804 0.08263593912124634
CurrentTrain: epoch  5, batch     4 | loss: 0.0894354Losses:  0.0053537506610155106 0.10746943950653076
CurrentTrain: epoch  5, batch     5 | loss: 0.1128232Losses:  0.005331092514097691 0.09538282454013824
CurrentTrain: epoch  5, batch     6 | loss: 0.1007139Losses:  0.018343159928917885 0.13948188722133636
CurrentTrain: epoch  5, batch     7 | loss: 0.1578251Losses:  0.008434617891907692 0.1290861964225769
CurrentTrain: epoch  5, batch     8 | loss: 0.1375208Losses:  0.0028294683434069157 0.07909619063138962
CurrentTrain: epoch  5, batch     9 | loss: 0.0819257Losses:  0.025341089814901352 0.16082942485809326
CurrentTrain: epoch  5, batch    10 | loss: 0.1861705Losses:  0.005875339265912771 0.10563447326421738
CurrentTrain: epoch  5, batch    11 | loss: 0.1115098Losses:  0.00538089731708169 0.1307462751865387
CurrentTrain: epoch  5, batch    12 | loss: 0.1361272Losses:  0.008266654796898365 0.11176082491874695
CurrentTrain: epoch  5, batch    13 | loss: 0.1200275Losses:  0.004384330473840237 0.1311761885881424
CurrentTrain: epoch  5, batch    14 | loss: 0.1355605Losses:  0.0059645879082381725 0.087744802236557
CurrentTrain: epoch  5, batch    15 | loss: 0.0937094Losses:  0.005283907987177372 0.11910708248615265
CurrentTrain: epoch  5, batch    16 | loss: 0.1243910Losses:  0.0031328934710472822 0.10923916846513748
CurrentTrain: epoch  5, batch    17 | loss: 0.1123721Losses:  0.011544540524482727 0.1295931041240692
CurrentTrain: epoch  5, batch    18 | loss: 0.1411376Losses:  0.006096155382692814 0.07954239100217819
CurrentTrain: epoch  5, batch    19 | loss: 0.0856385Losses:  0.004527455661445856 0.08348473906517029
CurrentTrain: epoch  5, batch    20 | loss: 0.0880122Losses:  0.006972780916839838 0.09920340776443481
CurrentTrain: epoch  5, batch    21 | loss: 0.1061762Losses:  0.002957115415483713 0.11186269670724869
CurrentTrain: epoch  5, batch    22 | loss: 0.1148198Losses:  0.005843036808073521 0.08859319984912872
CurrentTrain: epoch  5, batch    23 | loss: 0.0944362Losses:  0.0021981133613735437 0.05994395166635513
CurrentTrain: epoch  5, batch    24 | loss: 0.0621421Losses:  0.009870746172964573 0.11016756296157837
CurrentTrain: epoch  5, batch    25 | loss: 0.1200383Losses:  0.004775987938046455 0.11713012307882309
CurrentTrain: epoch  5, batch    26 | loss: 0.1219061Losses:  0.006391050294041634 0.0926877036690712
CurrentTrain: epoch  5, batch    27 | loss: 0.0990788Losses:  0.004563787952065468 0.10203497111797333
CurrentTrain: epoch  5, batch    28 | loss: 0.1065988Losses:  0.005833561532199383 0.11618630588054657
CurrentTrain: epoch  5, batch    29 | loss: 0.1220199Losses:  0.0027810181491076946 0.057400986552238464
CurrentTrain: epoch  5, batch    30 | loss: 0.0601820Losses:  0.00674130953848362 0.08133899420499802
CurrentTrain: epoch  5, batch    31 | loss: 0.0880803Losses:  0.004561651963740587 0.06883661448955536
CurrentTrain: epoch  5, batch    32 | loss: 0.0733983Losses:  0.0026208097115159035 0.10155275464057922
CurrentTrain: epoch  5, batch    33 | loss: 0.1041736Losses:  0.005898489151149988 0.10563767701387405
CurrentTrain: epoch  5, batch    34 | loss: 0.1115362Losses:  0.0068182190880179405 0.08939288556575775
CurrentTrain: epoch  5, batch    35 | loss: 0.0962111Losses:  0.004857076331973076 0.09129424393177032
CurrentTrain: epoch  5, batch    36 | loss: 0.0961513Losses:  0.0036798908840864897 0.10144390910863876
CurrentTrain: epoch  5, batch    37 | loss: 0.1051238Losses:  0.0048117972910404205 0.08033895492553711
CurrentTrain: epoch  5, batch    38 | loss: 0.0851507Losses:  0.005211613141000271 0.10463031381368637
CurrentTrain: epoch  5, batch    39 | loss: 0.1098419Losses:  0.0028961619827896357 0.1001899242401123
CurrentTrain: epoch  5, batch    40 | loss: 0.1030861Losses:  0.0030142879113554955 0.08972691744565964
CurrentTrain: epoch  5, batch    41 | loss: 0.0927412Losses:  0.018688132986426353 0.14890077710151672
CurrentTrain: epoch  5, batch    42 | loss: 0.1675889Losses:  0.010794529691338539 0.10505691915750504
CurrentTrain: epoch  5, batch    43 | loss: 0.1158514Losses:  0.004496007692068815 0.11550001800060272
CurrentTrain: epoch  5, batch    44 | loss: 0.1199960Losses:  0.004091497045010328 0.12884238362312317
CurrentTrain: epoch  5, batch    45 | loss: 0.1329339Losses:  0.0036841570399701595 0.10775348544120789
CurrentTrain: epoch  5, batch    46 | loss: 0.1114376Losses:  0.003805156797170639 0.11927810311317444
CurrentTrain: epoch  5, batch    47 | loss: 0.1230833Losses:  0.0030965872574597597 0.09922577440738678
CurrentTrain: epoch  5, batch    48 | loss: 0.1023224Losses:  0.003340075258165598 0.10113493353128433
CurrentTrain: epoch  5, batch    49 | loss: 0.1044750Losses:  0.003791535273194313 0.08671221882104874
CurrentTrain: epoch  5, batch    50 | loss: 0.0905038Losses:  0.0034603732638061047 0.07827261090278625
CurrentTrain: epoch  5, batch    51 | loss: 0.0817330Losses:  0.0042167045176029205 0.12914755940437317
CurrentTrain: epoch  5, batch    52 | loss: 0.1333643Losses:  0.0029655611142516136 0.09498189389705658
CurrentTrain: epoch  5, batch    53 | loss: 0.0979475Losses:  0.0038154812064021826 0.1198999434709549
CurrentTrain: epoch  5, batch    54 | loss: 0.1237154Losses:  0.0034135112073272467 0.1186973974108696
CurrentTrain: epoch  5, batch    55 | loss: 0.1221109Losses:  0.0027425410225987434 0.10739827156066895
CurrentTrain: epoch  5, batch    56 | loss: 0.1101408Losses:  0.003663282375782728 0.07773396372795105
CurrentTrain: epoch  5, batch    57 | loss: 0.0813972Losses:  0.0033345031552016735 0.08327261358499527
CurrentTrain: epoch  5, batch    58 | loss: 0.0866071Losses:  0.004337676335126162 0.09496144950389862
CurrentTrain: epoch  5, batch    59 | loss: 0.0992991Losses:  0.0023272621911019087 0.06604135781526566
CurrentTrain: epoch  5, batch    60 | loss: 0.0683686Losses:  0.0026041397359222174 0.07709968090057373
CurrentTrain: epoch  5, batch    61 | loss: 0.0797038Losses:  0.009349482133984566 0.0442890003323555
CurrentTrain: epoch  5, batch    62 | loss: 0.0536385Losses:  0.0032323994673788548 0.09855875372886658
CurrentTrain: epoch  6, batch     0 | loss: 0.1017912Losses:  0.0031622429378330708 0.0751333013176918
CurrentTrain: epoch  6, batch     1 | loss: 0.0782955Losses:  0.002382098464295268 0.06967052817344666
CurrentTrain: epoch  6, batch     2 | loss: 0.0720526Losses:  0.00656041270121932 0.10868598520755768
CurrentTrain: epoch  6, batch     3 | loss: 0.1152464Losses:  0.002045733854174614 0.07320050150156021
CurrentTrain: epoch  6, batch     4 | loss: 0.0752462Losses:  0.0026560286059975624 0.0687195360660553
CurrentTrain: epoch  6, batch     5 | loss: 0.0713756Losses:  0.003357634413987398 0.09714608639478683
CurrentTrain: epoch  6, batch     6 | loss: 0.1005037Losses:  0.003848646068945527 0.07065047323703766
CurrentTrain: epoch  6, batch     7 | loss: 0.0744991Losses:  0.002589388284832239 0.10565897822380066
CurrentTrain: epoch  6, batch     8 | loss: 0.1082484Losses:  0.00476051215082407 0.07243616878986359
CurrentTrain: epoch  6, batch     9 | loss: 0.0771967Losses:  0.0024156603030860424 0.08961298316717148
CurrentTrain: epoch  6, batch    10 | loss: 0.0920286Losses:  0.002434949390590191 0.05923531949520111
CurrentTrain: epoch  6, batch    11 | loss: 0.0616703Losses:  0.00284141325391829 0.09540240466594696
CurrentTrain: epoch  6, batch    12 | loss: 0.0982438Losses:  0.0025442098267376423 0.05764910578727722
CurrentTrain: epoch  6, batch    13 | loss: 0.0601933Losses:  0.0025479490868747234 0.09973771125078201
CurrentTrain: epoch  6, batch    14 | loss: 0.1022857Losses:  0.0034788718912750483 0.09671570360660553
CurrentTrain: epoch  6, batch    15 | loss: 0.1001946Losses:  0.0027688266709446907 0.09909491240978241
CurrentTrain: epoch  6, batch    16 | loss: 0.1018637Losses:  0.002692797686904669 0.08878612518310547
CurrentTrain: epoch  6, batch    17 | loss: 0.0914789Losses:  0.0027263069059699774 0.04233161360025406
CurrentTrain: epoch  6, batch    18 | loss: 0.0450579Losses:  0.002787912730127573 0.09751871228218079
CurrentTrain: epoch  6, batch    19 | loss: 0.1003066Losses:  0.0034916475415229797 0.11512167006731033
CurrentTrain: epoch  6, batch    20 | loss: 0.1186133Losses:  0.0023286696523427963 0.07849650084972382
CurrentTrain: epoch  6, batch    21 | loss: 0.0808252Losses:  0.0031754765659570694 0.08218538016080856
CurrentTrain: epoch  6, batch    22 | loss: 0.0853609Losses:  0.0036062863655388355 0.066505067050457
CurrentTrain: epoch  6, batch    23 | loss: 0.0701114Losses:  0.003052136627957225 0.0770873874425888
CurrentTrain: epoch  6, batch    24 | loss: 0.0801395Losses:  0.0056668296456336975 0.08064384758472443
CurrentTrain: epoch  6, batch    25 | loss: 0.0863107Losses:  0.002894958946853876 0.09038209915161133
CurrentTrain: epoch  6, batch    26 | loss: 0.0932771Losses:  0.001976769883185625 0.07756492495536804
CurrentTrain: epoch  6, batch    27 | loss: 0.0795417Losses:  0.002118714153766632 0.09222769737243652
CurrentTrain: epoch  6, batch    28 | loss: 0.0943464Losses:  0.0020473534241318703 0.08885107934474945
CurrentTrain: epoch  6, batch    29 | loss: 0.0908984Losses:  0.00296553922817111 0.08945336192846298
CurrentTrain: epoch  6, batch    30 | loss: 0.0924189Losses:  0.002051842864602804 0.053194887936115265
CurrentTrain: epoch  6, batch    31 | loss: 0.0552467Losses:  0.0027262272778898478 0.06328420341014862
CurrentTrain: epoch  6, batch    32 | loss: 0.0660104Losses:  0.0036221768241375685 0.10555668920278549
CurrentTrain: epoch  6, batch    33 | loss: 0.1091789Losses:  0.0026435377076268196 0.11216586828231812
CurrentTrain: epoch  6, batch    34 | loss: 0.1148094Losses:  0.005613560788333416 0.05584784969687462
CurrentTrain: epoch  6, batch    35 | loss: 0.0614614Losses:  0.003859358374029398 0.05314277857542038
CurrentTrain: epoch  6, batch    36 | loss: 0.0570021Losses:  0.0027521064039319754 0.07846243679523468
CurrentTrain: epoch  6, batch    37 | loss: 0.0812145Losses:  0.0025188461877405643 0.09768584370613098
CurrentTrain: epoch  6, batch    38 | loss: 0.1002047Losses:  0.0022561498917639256 0.05384019762277603
CurrentTrain: epoch  6, batch    39 | loss: 0.0560963Losses:  0.002549057127907872 0.08155110478401184
CurrentTrain: epoch  6, batch    40 | loss: 0.0841002Losses:  0.0039392984472215176 0.10552375018596649
CurrentTrain: epoch  6, batch    41 | loss: 0.1094631Losses:  0.0027448623441159725 0.09165658056735992
CurrentTrain: epoch  6, batch    42 | loss: 0.0944014Losses:  0.0024411948397755623 0.07663564383983612
CurrentTrain: epoch  6, batch    43 | loss: 0.0790768Losses:  0.0028203679248690605 0.07595625519752502
CurrentTrain: epoch  6, batch    44 | loss: 0.0787766Losses:  0.0021513879764825106 0.06908470392227173
CurrentTrain: epoch  6, batch    45 | loss: 0.0712361Losses:  0.002334337681531906 0.09381429851055145
CurrentTrain: epoch  6, batch    46 | loss: 0.0961486Losses:  0.0030374745838344097 0.06236238777637482
CurrentTrain: epoch  6, batch    47 | loss: 0.0653999Losses:  0.002336439909413457 0.08931618928909302
CurrentTrain: epoch  6, batch    48 | loss: 0.0916526Losses:  0.0017326264642179012 0.04851801320910454
CurrentTrain: epoch  6, batch    49 | loss: 0.0502506Losses:  0.0022573615424335003 0.0689125806093216
CurrentTrain: epoch  6, batch    50 | loss: 0.0711699Losses:  0.0033675809390842915 0.07046488672494888
CurrentTrain: epoch  6, batch    51 | loss: 0.0738325Losses:  0.008519133552908897 0.0845574289560318
CurrentTrain: epoch  6, batch    52 | loss: 0.0930766Losses:  0.0033807512372732162 0.03585972264409065
CurrentTrain: epoch  6, batch    53 | loss: 0.0392405Losses:  0.0018983320333063602 0.07737015932798386
CurrentTrain: epoch  6, batch    54 | loss: 0.0792685Losses:  0.0021015414968132973 0.08192178606987
CurrentTrain: epoch  6, batch    55 | loss: 0.0840233Losses:  0.002427451778203249 0.05944120138883591
CurrentTrain: epoch  6, batch    56 | loss: 0.0618687Losses:  0.0023255953565239906 0.06892364472150803
CurrentTrain: epoch  6, batch    57 | loss: 0.0712492Losses:  0.0043075913563370705 0.07435505092144012
CurrentTrain: epoch  6, batch    58 | loss: 0.0786626Losses:  0.0016276038950309157 0.09619656205177307
CurrentTrain: epoch  6, batch    59 | loss: 0.0978242Losses:  0.0030279308557510376 0.05615949258208275
CurrentTrain: epoch  6, batch    60 | loss: 0.0591874Losses:  0.0036020660772919655 0.07266050577163696
CurrentTrain: epoch  6, batch    61 | loss: 0.0762626Losses:  0.0024263483937829733 0.09220287203788757
CurrentTrain: epoch  6, batch    62 | loss: 0.0946292Losses:  0.0033495589159429073 0.0577675923705101
CurrentTrain: epoch  7, batch     0 | loss: 0.0611171Losses:  0.0022324996534734964 0.06704303622245789
CurrentTrain: epoch  7, batch     1 | loss: 0.0692755Losses:  0.002602207940071821 0.07067694514989853
CurrentTrain: epoch  7, batch     2 | loss: 0.0732791Losses:  0.002452720422297716 0.06921681761741638
CurrentTrain: epoch  7, batch     3 | loss: 0.0716695Losses:  0.0062803165055811405 0.06510225683450699
CurrentTrain: epoch  7, batch     4 | loss: 0.0713826Losses:  0.001975397113710642 0.05767551437020302
CurrentTrain: epoch  7, batch     5 | loss: 0.0596509Losses:  0.001987007912248373 0.0678594559431076
CurrentTrain: epoch  7, batch     6 | loss: 0.0698465Losses:  0.00522264838218689 0.10396474599838257
CurrentTrain: epoch  7, batch     7 | loss: 0.1091874Losses:  0.001904819393530488 0.04607195034623146
CurrentTrain: epoch  7, batch     8 | loss: 0.0479768Losses:  0.003435320220887661 0.09364014863967896
CurrentTrain: epoch  7, batch     9 | loss: 0.0970755Losses:  0.003021274460479617 0.10230688005685806
CurrentTrain: epoch  7, batch    10 | loss: 0.1053282Losses:  0.0020365798845887184 0.0627889484167099
CurrentTrain: epoch  7, batch    11 | loss: 0.0648255Losses:  0.0013740326976403594 0.07826166599988937
CurrentTrain: epoch  7, batch    12 | loss: 0.0796357Losses:  0.0021033422090113163 0.052605271339416504
CurrentTrain: epoch  7, batch    13 | loss: 0.0547086Losses:  0.0017731665866449475 0.0779993012547493
CurrentTrain: epoch  7, batch    14 | loss: 0.0797725Losses:  0.002068064408376813 0.09822916239500046
CurrentTrain: epoch  7, batch    15 | loss: 0.1002972Losses:  0.0017010923475027084 0.10419173538684845
CurrentTrain: epoch  7, batch    16 | loss: 0.1058928Losses:  0.002424880862236023 0.11153057962656021
CurrentTrain: epoch  7, batch    17 | loss: 0.1139555Losses:  0.0020058187656104565 0.06489060819149017
CurrentTrain: epoch  7, batch    18 | loss: 0.0668964Losses:  0.0021575994323939085 0.06747797131538391
CurrentTrain: epoch  7, batch    19 | loss: 0.0696356Losses:  0.002097720978781581 0.0469680093228817
CurrentTrain: epoch  7, batch    20 | loss: 0.0490657Losses:  0.0027399456594139338 0.06508590281009674
CurrentTrain: epoch  7, batch    21 | loss: 0.0678258Losses:  0.00217177951708436 0.0624435693025589
CurrentTrain: epoch  7, batch    22 | loss: 0.0646153Losses:  0.004924585111439228 0.07200803607702255
CurrentTrain: epoch  7, batch    23 | loss: 0.0769326Losses:  0.0020059847738593817 0.08958996832370758
CurrentTrain: epoch  7, batch    24 | loss: 0.0915960Losses:  0.0034301786217838526 0.0735013410449028
CurrentTrain: epoch  7, batch    25 | loss: 0.0769315Losses:  0.0017647149506956339 0.04906344413757324
CurrentTrain: epoch  7, batch    26 | loss: 0.0508282Losses:  0.0022350021172314882 0.06379223614931107
CurrentTrain: epoch  7, batch    27 | loss: 0.0660272Losses:  0.0016603551339358091 0.07561580836772919
CurrentTrain: epoch  7, batch    28 | loss: 0.0772762Losses:  0.0017713585402816534 0.0474272146821022
CurrentTrain: epoch  7, batch    29 | loss: 0.0491986Losses:  0.0022457600571215153 0.05702462047338486
CurrentTrain: epoch  7, batch    30 | loss: 0.0592704Losses:  0.002014185767620802 0.06532895565032959
CurrentTrain: epoch  7, batch    31 | loss: 0.0673431Losses:  0.0073613659478724 0.04079730063676834
CurrentTrain: epoch  7, batch    32 | loss: 0.0481587Losses:  0.002195395529270172 0.06457420438528061
CurrentTrain: epoch  7, batch    33 | loss: 0.0667696Losses:  0.003372490406036377 0.052565522491931915
CurrentTrain: epoch  7, batch    34 | loss: 0.0559380Losses:  0.0029127432499080896 0.06142539530992508
CurrentTrain: epoch  7, batch    35 | loss: 0.0643381Losses:  0.0025780806317925453 0.0647759735584259
CurrentTrain: epoch  7, batch    36 | loss: 0.0673541Losses:  0.0016968719428405166 0.05568248778581619
CurrentTrain: epoch  7, batch    37 | loss: 0.0573794Losses:  0.002335089258849621 0.06803952157497406
CurrentTrain: epoch  7, batch    38 | loss: 0.0703746Losses:  0.0018095236737281084 0.05552012473344803
CurrentTrain: epoch  7, batch    39 | loss: 0.0573296Losses:  0.0019366759806871414 0.084755539894104
CurrentTrain: epoch  7, batch    40 | loss: 0.0866922Losses:  0.0014846990816295147 0.05826032534241676
CurrentTrain: epoch  7, batch    41 | loss: 0.0597450Losses:  0.002157619222998619 0.05831154063344002
CurrentTrain: epoch  7, batch    42 | loss: 0.0604692Losses:  0.0031020299065858126 0.06625805050134659
CurrentTrain: epoch  7, batch    43 | loss: 0.0693601Losses:  0.00171645637601614 0.06564419716596603
CurrentTrain: epoch  7, batch    44 | loss: 0.0673607Losses:  0.001856829272583127 0.057712800800800323
CurrentTrain: epoch  7, batch    45 | loss: 0.0595696Losses:  0.0021050632931292057 0.05978603661060333
CurrentTrain: epoch  7, batch    46 | loss: 0.0618911Losses:  0.002838084939867258 0.06313344091176987
CurrentTrain: epoch  7, batch    47 | loss: 0.0659715Losses:  0.0019131519366055727 0.09399116039276123
CurrentTrain: epoch  7, batch    48 | loss: 0.0959043Losses:  0.0020410860888659954 0.09493962675333023
CurrentTrain: epoch  7, batch    49 | loss: 0.0969807Losses:  0.0022941227070987225 0.054900381714105606
CurrentTrain: epoch  7, batch    50 | loss: 0.0571945Losses:  0.0020140635315328836 0.05951910465955734
CurrentTrain: epoch  7, batch    51 | loss: 0.0615332Losses:  0.0023733838461339474 0.09891930222511292
CurrentTrain: epoch  7, batch    52 | loss: 0.1012927Losses:  0.0028153443709015846 0.06534925103187561
CurrentTrain: epoch  7, batch    53 | loss: 0.0681646Losses:  0.0019781850278377533 0.057777803391218185
CurrentTrain: epoch  7, batch    54 | loss: 0.0597560Losses:  0.0016141347587108612 0.06398452818393707
CurrentTrain: epoch  7, batch    55 | loss: 0.0655987Losses:  0.0022795344702899456 0.04913828521966934
CurrentTrain: epoch  7, batch    56 | loss: 0.0514178Losses:  0.0018003981094807386 0.06850458681583405
CurrentTrain: epoch  7, batch    57 | loss: 0.0703050Losses:  0.0029159067198634148 0.09092729538679123
CurrentTrain: epoch  7, batch    58 | loss: 0.0938432Losses:  0.0017926872242242098 0.04543069005012512
CurrentTrain: epoch  7, batch    59 | loss: 0.0472234Losses:  0.0018113909754902124 0.07394274324178696
CurrentTrain: epoch  7, batch    60 | loss: 0.0757541Losses:  0.0020557744428515434 0.049696728587150574
CurrentTrain: epoch  7, batch    61 | loss: 0.0517525Losses:  0.0012953525874763727 0.03750143200159073
CurrentTrain: epoch  7, batch    62 | loss: 0.0387968Losses:  0.0016980415675789118 0.07866573333740234
CurrentTrain: epoch  8, batch     0 | loss: 0.0803638Losses:  0.0023674312978982925 0.07622276246547699
CurrentTrain: epoch  8, batch     1 | loss: 0.0785902Losses:  0.002427173312753439 0.09097443521022797
CurrentTrain: epoch  8, batch     2 | loss: 0.0934016Losses:  0.0014629187062382698 0.04009810462594032
CurrentTrain: epoch  8, batch     3 | loss: 0.0415610Losses:  0.0014321308117359877 0.05256502330303192
CurrentTrain: epoch  8, batch     4 | loss: 0.0539972Losses:  0.002244880422949791 0.06784837692975998
CurrentTrain: epoch  8, batch     5 | loss: 0.0700933Losses:  0.0016903411597013474 0.058566901832818985
CurrentTrain: epoch  8, batch     6 | loss: 0.0602572Losses:  0.002352135954424739 0.05157634615898132
CurrentTrain: epoch  8, batch     7 | loss: 0.0539285Losses:  0.0018428332405164838 0.0596618577837944
CurrentTrain: epoch  8, batch     8 | loss: 0.0615047Losses:  0.0017726864898577332 0.06520531326532364
CurrentTrain: epoch  8, batch     9 | loss: 0.0669780Losses:  0.0018704477697610855 0.04886355251073837
CurrentTrain: epoch  8, batch    10 | loss: 0.0507340Losses:  0.0025190794840455055 0.06349208950996399
CurrentTrain: epoch  8, batch    11 | loss: 0.0660112Losses:  0.002495291642844677 0.05683687701821327
CurrentTrain: epoch  8, batch    12 | loss: 0.0593322Losses:  0.0017495008651167154 0.05353347212076187
CurrentTrain: epoch  8, batch    13 | loss: 0.0552830Losses:  0.0034473666455596685 0.08604419976472855
CurrentTrain: epoch  8, batch    14 | loss: 0.0894916Losses:  0.0016546142287552357 0.06791503727436066
CurrentTrain: epoch  8, batch    15 | loss: 0.0695697Losses:  0.0018973757978528738 0.04146292060613632
CurrentTrain: epoch  8, batch    16 | loss: 0.0433603Losses:  0.0015166476368904114 0.06878063082695007
CurrentTrain: epoch  8, batch    17 | loss: 0.0702973Losses:  0.0018408674513921142 0.06289487332105637
CurrentTrain: epoch  8, batch    18 | loss: 0.0647357Losses:  0.0017122516874223948 0.05544058606028557
CurrentTrain: epoch  8, batch    19 | loss: 0.0571528Losses:  0.0015278114005923271 0.06790275871753693
CurrentTrain: epoch  8, batch    20 | loss: 0.0694306Losses:  0.0035807937383651733 0.056191399693489075
CurrentTrain: epoch  8, batch    21 | loss: 0.0597722Losses:  0.0019711419008672237 0.0665617510676384
CurrentTrain: epoch  8, batch    22 | loss: 0.0685329Losses:  0.0017954661743715405 0.07579807937145233
CurrentTrain: epoch  8, batch    23 | loss: 0.0775935Losses:  0.0021091133821755648 0.08243335038423538
CurrentTrain: epoch  8, batch    24 | loss: 0.0845425Losses:  0.0021317200735211372 0.04084855318069458
CurrentTrain: epoch  8, batch    25 | loss: 0.0429803Losses:  0.0023741365876048803 0.07353740930557251
CurrentTrain: epoch  8, batch    26 | loss: 0.0759115Losses:  0.0017511718906462193 0.06855984032154083
CurrentTrain: epoch  8, batch    27 | loss: 0.0703110Losses:  0.001536281080916524 0.08368644118309021
CurrentTrain: epoch  8, batch    28 | loss: 0.0852227Losses:  0.001404822338372469 0.06275346875190735
CurrentTrain: epoch  8, batch    29 | loss: 0.0641583Losses:  0.0021048900671303272 0.06189585477113724
CurrentTrain: epoch  8, batch    30 | loss: 0.0640007Losses:  0.0015246097464114428 0.06219983473420143
CurrentTrain: epoch  8, batch    31 | loss: 0.0637244Losses:  0.0013980563962832093 0.061876434832811356
CurrentTrain: epoch  8, batch    32 | loss: 0.0632745Losses:  0.002919453661888838 0.05599910765886307
CurrentTrain: epoch  8, batch    33 | loss: 0.0589186Losses:  0.0013840878382325172 0.07428045570850372
CurrentTrain: epoch  8, batch    34 | loss: 0.0756645Losses:  0.0015474501997232437 0.0688895434141159
CurrentTrain: epoch  8, batch    35 | loss: 0.0704370Losses:  0.0015881406143307686 0.04821173846721649
CurrentTrain: epoch  8, batch    36 | loss: 0.0497999Losses:  0.002067119348794222 0.06642195582389832
CurrentTrain: epoch  8, batch    37 | loss: 0.0684891Losses:  0.005143959075212479 0.0738058090209961
CurrentTrain: epoch  8, batch    38 | loss: 0.0789498Losses:  0.001604612683877349 0.07072671502828598
CurrentTrain: epoch  8, batch    39 | loss: 0.0723313Losses:  0.0017899712547659874 0.07266098260879517
CurrentTrain: epoch  8, batch    40 | loss: 0.0744510Losses:  0.0015374964568763971 0.06029308959841728
CurrentTrain: epoch  8, batch    41 | loss: 0.0618306Losses:  0.0015457209665328264 0.06704086065292358
CurrentTrain: epoch  8, batch    42 | loss: 0.0685866Losses:  0.004056636244058609 0.07546105980873108
CurrentTrain: epoch  8, batch    43 | loss: 0.0795177Losses:  0.001476423116400838 0.06916762888431549
CurrentTrain: epoch  8, batch    44 | loss: 0.0706441Losses:  0.002152529777958989 0.042165063321590424
CurrentTrain: epoch  8, batch    45 | loss: 0.0443176Losses:  0.0016331307124346495 0.05173453688621521
CurrentTrain: epoch  8, batch    46 | loss: 0.0533677Losses:  0.0012800898402929306 0.0737244114279747
CurrentTrain: epoch  8, batch    47 | loss: 0.0750045Losses:  0.0012880463618785143 0.04724075645208359
CurrentTrain: epoch  8, batch    48 | loss: 0.0485288Losses:  0.002678569871932268 0.054705411195755005
CurrentTrain: epoch  8, batch    49 | loss: 0.0573840Losses:  0.0015870080096647143 0.048800792545080185
CurrentTrain: epoch  8, batch    50 | loss: 0.0503878Losses:  0.0017422260716557503 0.051335640251636505
CurrentTrain: epoch  8, batch    51 | loss: 0.0530779Losses:  0.002283759880810976 0.10345073789358139
CurrentTrain: epoch  8, batch    52 | loss: 0.1057345Losses:  0.0014919848181307316 0.06716437637805939
CurrentTrain: epoch  8, batch    53 | loss: 0.0686564Losses:  0.0018408395117148757 0.0702696219086647
CurrentTrain: epoch  8, batch    54 | loss: 0.0721105Losses:  0.0012718215584754944 0.05830242484807968
CurrentTrain: epoch  8, batch    55 | loss: 0.0595742Losses:  0.0020724572241306305 0.06587833911180496
CurrentTrain: epoch  8, batch    56 | loss: 0.0679508Losses:  0.0017170329811051488 0.07228245586156845
CurrentTrain: epoch  8, batch    57 | loss: 0.0739995Losses:  0.0012207786785438657 0.06475359946489334
CurrentTrain: epoch  8, batch    58 | loss: 0.0659744Losses:  0.0012924696784466505 0.07767735421657562
CurrentTrain: epoch  8, batch    59 | loss: 0.0789698Losses:  0.0015167470555752516 0.08393216878175735
CurrentTrain: epoch  8, batch    60 | loss: 0.0854489Losses:  0.0012772716581821442 0.07177722454071045
CurrentTrain: epoch  8, batch    61 | loss: 0.0730545Losses:  0.0016665473813191056 0.01685417629778385
CurrentTrain: epoch  8, batch    62 | loss: 0.0185207Losses:  0.001323722768574953 0.04725668206810951
CurrentTrain: epoch  9, batch     0 | loss: 0.0485804Losses:  0.001504156505689025 0.0510372631251812
CurrentTrain: epoch  9, batch     1 | loss: 0.0525414Losses:  0.001640691072680056 0.056077904999256134
CurrentTrain: epoch  9, batch     2 | loss: 0.0577186Losses:  0.001508353278040886 0.05367714911699295
CurrentTrain: epoch  9, batch     3 | loss: 0.0551855Losses:  0.0013103035744279623 0.03958054631948471
CurrentTrain: epoch  9, batch     4 | loss: 0.0408909Losses:  0.001568224048241973 0.047058992087841034
CurrentTrain: epoch  9, batch     5 | loss: 0.0486272Losses:  0.0013410826213657856 0.05953103303909302
CurrentTrain: epoch  9, batch     6 | loss: 0.0608721Losses:  0.001647567143663764 0.04931389540433884
CurrentTrain: epoch  9, batch     7 | loss: 0.0509615Losses:  0.0013410380342975259 0.04797690734267235
CurrentTrain: epoch  9, batch     8 | loss: 0.0493179Losses:  0.0014198895078152418 0.07251820713281631
CurrentTrain: epoch  9, batch     9 | loss: 0.0739381Losses:  0.0050775338895618916 0.07237622141838074
CurrentTrain: epoch  9, batch    10 | loss: 0.0774538Losses:  0.0015028957277536392 0.0742049366235733
CurrentTrain: epoch  9, batch    11 | loss: 0.0757078Losses:  0.0012770649045705795 0.06813234090805054
CurrentTrain: epoch  9, batch    12 | loss: 0.0694094Losses:  0.001915068132802844 0.0680050253868103
CurrentTrain: epoch  9, batch    13 | loss: 0.0699201Losses:  0.0021122267935425043 0.08865299820899963
CurrentTrain: epoch  9, batch    14 | loss: 0.0907652Losses:  0.0013688033213838935 0.0649714469909668
CurrentTrain: epoch  9, batch    15 | loss: 0.0663403Losses:  0.0016008384991437197 0.06868493556976318
CurrentTrain: epoch  9, batch    16 | loss: 0.0702858Losses:  0.0014287226367741823 0.06990788877010345
CurrentTrain: epoch  9, batch    17 | loss: 0.0713366Losses:  0.0019655271898955107 0.04760017991065979
CurrentTrain: epoch  9, batch    18 | loss: 0.0495657Losses:  0.0023988063912838697 0.027625789865851402
CurrentTrain: epoch  9, batch    19 | loss: 0.0300246Losses:  0.0013384229969233274 0.0522565022110939
CurrentTrain: epoch  9, batch    20 | loss: 0.0535949Losses:  0.0013135646004229784 0.05813102424144745
CurrentTrain: epoch  9, batch    21 | loss: 0.0594446Losses:  0.002004853216931224 0.0669412687420845
CurrentTrain: epoch  9, batch    22 | loss: 0.0689461Losses:  0.0015536918072029948 0.055049508810043335
CurrentTrain: epoch  9, batch    23 | loss: 0.0566032Losses:  0.0016713377553969622 0.04752145707607269
CurrentTrain: epoch  9, batch    24 | loss: 0.0491928Losses:  0.0014441193779930472 0.05589430034160614
CurrentTrain: epoch  9, batch    25 | loss: 0.0573384Losses:  0.0018920289585366845 0.07503906637430191
CurrentTrain: epoch  9, batch    26 | loss: 0.0769311Losses:  0.0013743112795054913 0.0525512769818306
CurrentTrain: epoch  9, batch    27 | loss: 0.0539256Losses:  0.0012754274066537619 0.07205615937709808
CurrentTrain: epoch  9, batch    28 | loss: 0.0733316Losses:  0.0014213253743946552 0.061093758791685104
CurrentTrain: epoch  9, batch    29 | loss: 0.0625151Losses:  0.0017266897484660149 0.07256297767162323
CurrentTrain: epoch  9, batch    30 | loss: 0.0742897Losses:  0.0030834171921014786 0.08369111269712448
CurrentTrain: epoch  9, batch    31 | loss: 0.0867745Losses:  0.0012344017159193754 0.05140741541981697
CurrentTrain: epoch  9, batch    32 | loss: 0.0526418Losses:  0.0016131058800965548 0.02505618706345558
CurrentTrain: epoch  9, batch    33 | loss: 0.0266693Losses:  0.0017195582622662187 0.06979264318943024
CurrentTrain: epoch  9, batch    34 | loss: 0.0715122Losses:  0.002241304609924555 0.07927161455154419
CurrentTrain: epoch  9, batch    35 | loss: 0.0815129Losses:  0.0015511931851506233 0.034138794988393784
CurrentTrain: epoch  9, batch    36 | loss: 0.0356900Losses:  0.002039402723312378 0.05906070023775101
CurrentTrain: epoch  9, batch    37 | loss: 0.0611001Losses:  0.0016921572387218475 0.04247833788394928
CurrentTrain: epoch  9, batch    38 | loss: 0.0441705Losses:  0.0019124271348118782 0.07375633716583252
CurrentTrain: epoch  9, batch    39 | loss: 0.0756688Losses:  0.00132573745213449 0.047890447080135345
CurrentTrain: epoch  9, batch    40 | loss: 0.0492162Losses:  0.0014823948731645942 0.0351860448718071
CurrentTrain: epoch  9, batch    41 | loss: 0.0366684Losses:  0.0015494704712182283 0.03413032740354538
CurrentTrain: epoch  9, batch    42 | loss: 0.0356798Losses:  0.0012564813951030374 0.03330191969871521
CurrentTrain: epoch  9, batch    43 | loss: 0.0345584Losses:  0.0012551058316603303 0.043240275233983994
CurrentTrain: epoch  9, batch    44 | loss: 0.0444954Losses:  0.0030554579570889473 0.06201775372028351
CurrentTrain: epoch  9, batch    45 | loss: 0.0650732Losses:  0.001301376847550273 0.0384448878467083
CurrentTrain: epoch  9, batch    46 | loss: 0.0397463Losses:  0.001405075890943408 0.04571554809808731
CurrentTrain: epoch  9, batch    47 | loss: 0.0471206Losses:  0.0018412258941680193 0.05039193108677864
CurrentTrain: epoch  9, batch    48 | loss: 0.0522332Losses:  0.0011619448196142912 0.07848832756280899
CurrentTrain: epoch  9, batch    49 | loss: 0.0796503Losses:  0.0013313903473317623 0.069908007979393
CurrentTrain: epoch  9, batch    50 | loss: 0.0712394Losses:  0.009031237103044987 0.07696379721164703
CurrentTrain: epoch  9, batch    51 | loss: 0.0859950Losses:  0.0013781274901703 0.04363404959440231
CurrentTrain: epoch  9, batch    52 | loss: 0.0450122Losses:  0.001297591021284461 0.06653653085231781
CurrentTrain: epoch  9, batch    53 | loss: 0.0678341Losses:  0.0015470411162823439 0.05899546295404434
CurrentTrain: epoch  9, batch    54 | loss: 0.0605425Losses:  0.001958929467946291 0.04037833213806152
CurrentTrain: epoch  9, batch    55 | loss: 0.0423373Losses:  0.0015991020482033491 0.05858844891190529
CurrentTrain: epoch  9, batch    56 | loss: 0.0601876Losses:  0.0012161409249529243 0.07312416285276413
CurrentTrain: epoch  9, batch    57 | loss: 0.0743403Losses:  0.0016024955548346043 0.04365576058626175
CurrentTrain: epoch  9, batch    58 | loss: 0.0452583Losses:  0.0018340160604566336 0.06839297711849213
CurrentTrain: epoch  9, batch    59 | loss: 0.0702270Losses:  0.001186177833005786 0.06171676516532898
CurrentTrain: epoch  9, batch    60 | loss: 0.0629029Losses:  0.0014201715821400285 0.05892755463719368
CurrentTrain: epoch  9, batch    61 | loss: 0.0603477Losses:  0.0011407437268644571 0.02539851702749729
CurrentTrain: epoch  9, batch    62 | loss: 0.0265393
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.26%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.55%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.26%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.55%   
cur_acc:  ['0.9355']
his_acc:  ['0.9355']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  0.2153235822916031 1.2047390937805176
CurrentTrain: epoch  0, batch     0 | loss: 1.4200627Losses:  0.22636690735816956 1.2711279392242432
CurrentTrain: epoch  0, batch     1 | loss: 1.4974948Losses:  0.24882204830646515 1.514704704284668
CurrentTrain: epoch  0, batch     2 | loss: 1.7635268Losses:  1.001007318496704 1.788139627478813e-07
CurrentTrain: epoch  0, batch     3 | loss: 1.0010076Losses:  0.26303011178970337 1.0452957153320312
CurrentTrain: epoch  1, batch     0 | loss: 1.3083258Losses:  0.2954252362251282 1.1451380252838135
CurrentTrain: epoch  1, batch     1 | loss: 1.4405632Losses:  0.2707291543483734 1.335371732711792
CurrentTrain: epoch  1, batch     2 | loss: 1.6061009Losses:  0.0753084048628807 0.15938888490200043
CurrentTrain: epoch  1, batch     3 | loss: 0.2346973Losses:  0.19859668612480164 0.8348351120948792
CurrentTrain: epoch  2, batch     0 | loss: 1.0334318Losses:  0.15113139152526855 0.881249189376831
CurrentTrain: epoch  2, batch     1 | loss: 1.0323806Losses:  0.12411606311798096 0.8258108496665955
CurrentTrain: epoch  2, batch     2 | loss: 0.9499269Losses:  0.1963932067155838 0.027736792340874672
CurrentTrain: epoch  2, batch     3 | loss: 0.2241300Losses:  0.13166660070419312 0.8264222145080566
CurrentTrain: epoch  3, batch     0 | loss: 0.9580888Losses:  0.08288122713565826 0.5827447175979614
CurrentTrain: epoch  3, batch     1 | loss: 0.6656259Losses:  0.1324826031923294 0.9979503154754639
CurrentTrain: epoch  3, batch     2 | loss: 1.1304330Losses:  0.057423487305641174 0.31375545263290405
CurrentTrain: epoch  3, batch     3 | loss: 0.3711789Losses:  0.06866845488548279 0.5818095803260803
CurrentTrain: epoch  4, batch     0 | loss: 0.6504780Losses:  0.09004528820514679 0.5496814250946045
CurrentTrain: epoch  4, batch     1 | loss: 0.6397267Losses:  0.1734468936920166 0.7186351418495178
CurrentTrain: epoch  4, batch     2 | loss: 0.8920820Losses:  0.04438210278749466 0.01336115226149559
CurrentTrain: epoch  4, batch     3 | loss: 0.0577433Losses:  0.12537208199501038 0.5323427319526672
CurrentTrain: epoch  5, batch     0 | loss: 0.6577148Losses:  0.07234849035739899 0.5522162914276123
CurrentTrain: epoch  5, batch     1 | loss: 0.6245648Losses:  0.04740510135889053 0.6587433815002441
CurrentTrain: epoch  5, batch     2 | loss: 0.7061485Losses:  0.3776973783969879 0.034386102110147476
CurrentTrain: epoch  5, batch     3 | loss: 0.4120835Losses:  0.051211751997470856 0.587909460067749
CurrentTrain: epoch  6, batch     0 | loss: 0.6391212Losses:  0.05117397755384445 0.5221335887908936
CurrentTrain: epoch  6, batch     1 | loss: 0.5733076Losses:  0.10097411274909973 0.5377089977264404
CurrentTrain: epoch  6, batch     2 | loss: 0.6386831Losses:  0.028325501829385757 2.9802322387695312e-08
CurrentTrain: epoch  6, batch     3 | loss: 0.0283255Losses:  0.08185985684394836 0.5508652925491333
CurrentTrain: epoch  7, batch     0 | loss: 0.6327251Losses:  0.07139138132333755 0.5823104381561279
CurrentTrain: epoch  7, batch     1 | loss: 0.6537018Losses:  0.0350031778216362 0.41269516944885254
CurrentTrain: epoch  7, batch     2 | loss: 0.4476984Losses:  0.02534511871635914 0.0
CurrentTrain: epoch  7, batch     3 | loss: 0.0253451Losses:  0.044838570058345795 0.449243426322937
CurrentTrain: epoch  8, batch     0 | loss: 0.4940820Losses:  0.030160022899508476 0.45067858695983887
CurrentTrain: epoch  8, batch     1 | loss: 0.4808386Losses:  0.053777582943439484 0.47553396224975586
CurrentTrain: epoch  8, batch     2 | loss: 0.5293115Losses:  0.007497853599488735 0.026134541258215904
CurrentTrain: epoch  8, batch     3 | loss: 0.0336324Losses:  0.02168426290154457 0.45385855436325073
CurrentTrain: epoch  9, batch     0 | loss: 0.4755428Losses:  0.034130603075027466 0.385493665933609
CurrentTrain: epoch  9, batch     1 | loss: 0.4196243Losses:  0.02045881561934948 0.4238932728767395
CurrentTrain: epoch  9, batch     2 | loss: 0.4443521Losses:  0.006310156546533108 0.03594594448804855
CurrentTrain: epoch  9, batch     3 | loss: 0.0422561
Losses:  0.1823786497116089 0.7340551018714905
MemoryTrain:  epoch  0, batch     0 | loss: 0.9164338Losses:  0.038825031369924545 0.03654550760984421
MemoryTrain:  epoch  0, batch     1 | loss: 0.0753705Losses:  0.1542879045009613 0.518354058265686
MemoryTrain:  epoch  1, batch     0 | loss: 0.6726420Losses:  0.17042547464370728 0.5222461223602295
MemoryTrain:  epoch  1, batch     1 | loss: 0.6926716Losses:  0.1241530105471611 0.5436187982559204
MemoryTrain:  epoch  2, batch     0 | loss: 0.6677718Losses:  0.05114027112722397 0.1289936602115631
MemoryTrain:  epoch  2, batch     1 | loss: 0.1801339Losses:  0.06705404072999954 0.4805300831794739
MemoryTrain:  epoch  3, batch     0 | loss: 0.5475841Losses:  0.15450768172740936 0.182477205991745
MemoryTrain:  epoch  3, batch     1 | loss: 0.3369849Losses:  0.08643868565559387 0.5217924118041992
MemoryTrain:  epoch  4, batch     0 | loss: 0.6082311Losses:  0.009209206327795982 0.09888692945241928
MemoryTrain:  epoch  4, batch     1 | loss: 0.1080961Losses:  0.05540740489959717 0.5449987053871155
MemoryTrain:  epoch  5, batch     0 | loss: 0.6004061Losses:  0.022392068058252335 0.029289498925209045
MemoryTrain:  epoch  5, batch     1 | loss: 0.0516816Losses:  0.03965206444263458 0.5010504722595215
MemoryTrain:  epoch  6, batch     0 | loss: 0.5407025Losses:  0.020886415615677834 0.07723954319953918
MemoryTrain:  epoch  6, batch     1 | loss: 0.0981260Losses:  0.024437986314296722 0.4753199517726898
MemoryTrain:  epoch  7, batch     0 | loss: 0.4997579Losses:  0.05292964726686478 0.14401619136333466
MemoryTrain:  epoch  7, batch     1 | loss: 0.1969458Losses:  0.026144761592149734 0.4568563401699066
MemoryTrain:  epoch  8, batch     0 | loss: 0.4830011Losses:  0.016444679349660873 0.1451057493686676
MemoryTrain:  epoch  8, batch     1 | loss: 0.1615504Losses:  0.02242334559559822 0.42144346237182617
MemoryTrain:  epoch  9, batch     0 | loss: 0.4438668Losses:  0.02035888284444809 0.032747089862823486
MemoryTrain:  epoch  9, batch     1 | loss: 0.0531060
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 96.59%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 95.09%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 74.16%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 73.52%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 69.18%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 67.39%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 66.36%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 64.54%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.85%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.15%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.35%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.55%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.13%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 91.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.50%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.25%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.33%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 91.75%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 91.46%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.09%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 91.05%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 90.70%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 90.29%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 89.63%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 89.39%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 89.15%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 88.99%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.76%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 88.40%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 88.32%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 88.04%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 87.50%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.10%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 86.71%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 86.20%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.95%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 85.52%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 84.94%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 84.47%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 84.07%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 83.62%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 83.29%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 82.86%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 82.49%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.07%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 81.48%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 80.96%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 80.40%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 79.90%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 79.58%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 79.31%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.39%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.53%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 79.59%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.66%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 79.67%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.79%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.40%   
cur_acc:  ['0.9355', '0.6875']
his_acc:  ['0.9355', '0.8040']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  0.10578073561191559 1.652958869934082
CurrentTrain: epoch  0, batch     0 | loss: 1.7587396Losses:  0.0419195294380188 1.5809717178344727
CurrentTrain: epoch  0, batch     1 | loss: 1.6228912Losses:  0.06799584627151489 1.8014107942581177
CurrentTrain: epoch  0, batch     2 | loss: 1.8694067Losses:  0.10621031373739243 0.534736692905426
CurrentTrain: epoch  0, batch     3 | loss: 0.6409470Losses:  0.12942034006118774 1.4695580005645752
CurrentTrain: epoch  1, batch     0 | loss: 1.5989783Losses:  0.12249923497438431 1.5175788402557373
CurrentTrain: epoch  1, batch     1 | loss: 1.6400781Losses:  0.11106321215629578 1.2526813745498657
CurrentTrain: epoch  1, batch     2 | loss: 1.3637446Losses:  0.05976272374391556 0.1442909836769104
CurrentTrain: epoch  1, batch     3 | loss: 0.2040537Losses:  0.06044631451368332 0.9217230081558228
CurrentTrain: epoch  2, batch     0 | loss: 0.9821693Losses:  0.07496906071901321 1.1694673299789429
CurrentTrain: epoch  2, batch     1 | loss: 1.2444364Losses:  0.09216800332069397 1.38581383228302
CurrentTrain: epoch  2, batch     2 | loss: 1.4779818Losses:  0.020223673433065414 0.06128465384244919
CurrentTrain: epoch  2, batch     3 | loss: 0.0815083Losses:  0.05001157894730568 1.1105321645736694
CurrentTrain: epoch  3, batch     0 | loss: 1.1605438Losses:  0.04019151255488396 0.8778150081634521
CurrentTrain: epoch  3, batch     1 | loss: 0.9180065Losses:  0.05672745406627655 1.1459003686904907
CurrentTrain: epoch  3, batch     2 | loss: 1.2026278Losses:  0.09554272890090942 0.06822848320007324
CurrentTrain: epoch  3, batch     3 | loss: 0.1637712Losses:  0.06766659021377563 0.9900201559066772
CurrentTrain: epoch  4, batch     0 | loss: 1.0576868Losses:  0.0419931523501873 1.0139522552490234
CurrentTrain: epoch  4, batch     1 | loss: 1.0559454Losses:  0.058278001844882965 0.7920491099357605
CurrentTrain: epoch  4, batch     2 | loss: 0.8503271Losses:  0.0474172979593277 0.13229496777057648
CurrentTrain: epoch  4, batch     3 | loss: 0.1797123Losses:  0.052590981125831604 0.9237562417984009
CurrentTrain: epoch  5, batch     0 | loss: 0.9763472Losses:  0.0341620109975338 0.6010145545005798
CurrentTrain: epoch  5, batch     1 | loss: 0.6351765Losses:  0.10252153128385544 0.5348607301712036
CurrentTrain: epoch  5, batch     2 | loss: 0.6373823Losses:  0.07214570045471191 0.5281343460083008
CurrentTrain: epoch  5, batch     3 | loss: 0.6002800Losses:  0.04051312059164047 0.9192473888397217
CurrentTrain: epoch  6, batch     0 | loss: 0.9597605Losses:  0.044443175196647644 0.8208670616149902
CurrentTrain: epoch  6, batch     1 | loss: 0.8653103Losses:  0.032184209674596786 0.567378044128418
CurrentTrain: epoch  6, batch     2 | loss: 0.5995622Losses:  0.07927732914686203 0.0748673528432846
CurrentTrain: epoch  6, batch     3 | loss: 0.1541447Losses:  0.04265420883893967 0.6562682390213013
CurrentTrain: epoch  7, batch     0 | loss: 0.6989225Losses:  0.03789811581373215 0.6277559995651245
CurrentTrain: epoch  7, batch     1 | loss: 0.6656541Losses:  0.03700441122055054 0.6659202575683594
CurrentTrain: epoch  7, batch     2 | loss: 0.7029247Losses:  0.07967375963926315 0.3808317482471466
CurrentTrain: epoch  7, batch     3 | loss: 0.4605055Losses:  0.03129575401544571 0.7442218065261841
CurrentTrain: epoch  8, batch     0 | loss: 0.7755176Losses:  0.04419160261750221 0.6995483040809631
CurrentTrain: epoch  8, batch     1 | loss: 0.7437399Losses:  0.03417085483670235 0.42179960012435913
CurrentTrain: epoch  8, batch     2 | loss: 0.4559705Losses:  0.04235094040632248 0.01735759526491165
CurrentTrain: epoch  8, batch     3 | loss: 0.0597085Losses:  0.03535556048154831 0.5010806322097778
CurrentTrain: epoch  9, batch     0 | loss: 0.5364362Losses:  0.0270797461271286 0.38735729455947876
CurrentTrain: epoch  9, batch     1 | loss: 0.4144371Losses:  0.034592896699905396 0.5514236092567444
CurrentTrain: epoch  9, batch     2 | loss: 0.5860165Losses:  0.06631089746952057 0.5506250262260437
CurrentTrain: epoch  9, batch     3 | loss: 0.6169359
Losses:  0.09761719405651093 0.6916437149047852
MemoryTrain:  epoch  0, batch     0 | loss: 0.7892609Losses:  0.05912275239825249 0.6379716396331787
MemoryTrain:  epoch  0, batch     1 | loss: 0.6970944Losses:  0.08554399013519287 0.6376989483833313
MemoryTrain:  epoch  1, batch     0 | loss: 0.7232429Losses:  0.0809103399515152 0.47392845153808594
MemoryTrain:  epoch  1, batch     1 | loss: 0.5548388Losses:  0.05827314779162407 0.49618178606033325
MemoryTrain:  epoch  2, batch     0 | loss: 0.5544549Losses:  0.07115770876407623 0.48678550124168396
MemoryTrain:  epoch  2, batch     1 | loss: 0.5579432Losses:  0.047221023589372635 0.5678166747093201
MemoryTrain:  epoch  3, batch     0 | loss: 0.6150377Losses:  0.03319362550973892 0.3972399830818176
MemoryTrain:  epoch  3, batch     1 | loss: 0.4304336Losses:  0.044442083686590195 0.6596062779426575
MemoryTrain:  epoch  4, batch     0 | loss: 0.7040483Losses:  0.03382153436541557 0.32263103127479553
MemoryTrain:  epoch  4, batch     1 | loss: 0.3564526Losses:  0.03405524045228958 0.47777247428894043
MemoryTrain:  epoch  5, batch     0 | loss: 0.5118277Losses:  0.0370115302503109 0.4522465467453003
MemoryTrain:  epoch  5, batch     1 | loss: 0.4892581Losses:  0.030879370868206024 0.4523738622665405
MemoryTrain:  epoch  6, batch     0 | loss: 0.4832532Losses:  0.02726738527417183 0.39753419160842896
MemoryTrain:  epoch  6, batch     1 | loss: 0.4248016Losses:  0.05070798844099045 0.3575851321220398
MemoryTrain:  epoch  7, batch     0 | loss: 0.4082931Losses:  0.04419036954641342 0.5071427822113037
MemoryTrain:  epoch  7, batch     1 | loss: 0.5513331Losses:  0.03620105981826782 0.5473108291625977
MemoryTrain:  epoch  8, batch     0 | loss: 0.5835119Losses:  0.03923144191503525 0.29051461815834045
MemoryTrain:  epoch  8, batch     1 | loss: 0.3297461Losses:  0.02952352724969387 0.4962160587310791
MemoryTrain:  epoch  9, batch     0 | loss: 0.5257396Losses:  0.032452747225761414 0.28835928440093994
MemoryTrain:  epoch  9, batch     1 | loss: 0.3208120
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 36.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 57.35%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 56.60%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 63.19%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 61.42%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 59.79%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 58.67%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 59.18%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 67.43%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.06%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.14%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 91.73%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.09%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 92.27%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 91.88%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 90.92%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.06%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.10%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.14%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 91.21%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 91.39%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.37%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.15%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 90.95%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.74%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 90.47%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 90.17%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 89.61%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 89.21%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 88.75%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 88.66%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 88.36%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 88.14%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 88.06%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 87.71%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 87.36%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 86.90%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 86.37%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 85.99%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 85.48%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.24%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 84.82%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 84.60%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 84.12%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 83.66%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 83.09%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 82.58%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 82.15%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 81.73%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 81.49%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 80.90%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 80.27%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 79.76%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 79.09%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.60%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.24%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 77.93%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.71%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.20%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 79.12%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 78.74%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 78.42%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 78.25%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 77.93%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 77.58%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.65%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.89%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.22%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 77.83%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 77.54%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 77.13%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.89%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.66%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.39%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 77.07%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 76.64%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 76.31%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 76.14%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 75.73%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 75.40%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 75.70%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 76.93%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 76.81%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 76.72%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 76.49%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 76.27%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 76.19%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 76.01%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 75.87%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 75.79%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.88%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 75.84%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.90%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 75.70%   
cur_acc:  ['0.9355', '0.6875', '0.6806']
his_acc:  ['0.9355', '0.8040', '0.7570']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  0.20185667276382446 1.6354737281799316
CurrentTrain: epoch  0, batch     0 | loss: 1.8373303Losses:  0.07788920402526855 1.489011287689209
CurrentTrain: epoch  0, batch     1 | loss: 1.5669005Losses:  0.07235583662986755 1.5920716524124146
CurrentTrain: epoch  0, batch     2 | loss: 1.6644275Losses:  0.03527035936713219 0.23954689502716064
CurrentTrain: epoch  0, batch     3 | loss: 0.2748173Losses:  0.1060527116060257 1.3127801418304443
CurrentTrain: epoch  1, batch     0 | loss: 1.4188329Losses:  0.0633787140250206 1.0514551401138306
CurrentTrain: epoch  1, batch     1 | loss: 1.1148338Losses:  0.11288423836231232 1.5586469173431396
CurrentTrain: epoch  1, batch     2 | loss: 1.6715312Losses:  0.04192198067903519 0.04393969476222992
CurrentTrain: epoch  1, batch     3 | loss: 0.0858617Losses:  0.07821573317050934 1.2639600038528442
CurrentTrain: epoch  2, batch     0 | loss: 1.3421757Losses:  0.10170260816812515 1.077897548675537
CurrentTrain: epoch  2, batch     1 | loss: 1.1796001Losses:  0.04397742450237274 0.8448829054832458
CurrentTrain: epoch  2, batch     2 | loss: 0.8888603Losses:  0.017801456153392792 0.03875526785850525
CurrentTrain: epoch  2, batch     3 | loss: 0.0565567Losses:  0.049077607691287994 0.7401883006095886
CurrentTrain: epoch  3, batch     0 | loss: 0.7892659Losses:  0.0452873557806015 1.086089849472046
CurrentTrain: epoch  3, batch     1 | loss: 1.1313772Losses:  0.07503627240657806 0.9353744983673096
CurrentTrain: epoch  3, batch     2 | loss: 1.0104108Losses:  0.010158935561776161 0.06718778610229492
CurrentTrain: epoch  3, batch     3 | loss: 0.0773467Losses:  0.026235489174723625 0.6717244386672974
CurrentTrain: epoch  4, batch     0 | loss: 0.6979599Losses:  0.03354913741350174 0.6477092504501343
CurrentTrain: epoch  4, batch     1 | loss: 0.6812584Losses:  0.07608084380626678 0.552139937877655
CurrentTrain: epoch  4, batch     2 | loss: 0.6282208Losses:  0.06122549623250961 0.0629691407084465
CurrentTrain: epoch  4, batch     3 | loss: 0.1241946Losses:  0.054831378161907196 0.6537656784057617
CurrentTrain: epoch  5, batch     0 | loss: 0.7085971Losses:  0.029661990702152252 0.6516571640968323
CurrentTrain: epoch  5, batch     1 | loss: 0.6813192Losses:  0.025768958032131195 0.7277511358261108
CurrentTrain: epoch  5, batch     2 | loss: 0.7535201Losses:  0.051749926060438156 0.0
CurrentTrain: epoch  5, batch     3 | loss: 0.0517499Losses:  0.030933011323213577 0.33960795402526855
CurrentTrain: epoch  6, batch     0 | loss: 0.3705410Losses:  0.033200304955244064 0.49782729148864746
CurrentTrain: epoch  6, batch     1 | loss: 0.5310276Losses:  0.029304111376404762 0.6429106593132019
CurrentTrain: epoch  6, batch     2 | loss: 0.6722147Losses:  0.011784159578382969 0.11498444527387619
CurrentTrain: epoch  6, batch     3 | loss: 0.1267686Losses:  0.016691209748387337 0.370294988155365
CurrentTrain: epoch  7, batch     0 | loss: 0.3869862Losses:  0.023520318791270256 0.6228238344192505
CurrentTrain: epoch  7, batch     1 | loss: 0.6463441Losses:  0.028623659163713455 0.5779232978820801
CurrentTrain: epoch  7, batch     2 | loss: 0.6065469Losses:  0.03144393116235733 0.17666076123714447
CurrentTrain: epoch  7, batch     3 | loss: 0.2081047Losses:  0.032853446900844574 0.46327221393585205
CurrentTrain: epoch  8, batch     0 | loss: 0.4961257Losses:  0.01619763672351837 0.35001498460769653
CurrentTrain: epoch  8, batch     1 | loss: 0.3662126Losses:  0.01173302624374628 0.36905184388160706
CurrentTrain: epoch  8, batch     2 | loss: 0.3807849Losses:  0.004630847834050655 0.08899812400341034
CurrentTrain: epoch  8, batch     3 | loss: 0.0936290Losses:  0.013638542033731937 0.3737039566040039
CurrentTrain: epoch  9, batch     0 | loss: 0.3873425Losses:  0.02759455516934395 0.3759101331233978
CurrentTrain: epoch  9, batch     1 | loss: 0.4035047Losses:  0.01504083164036274 0.34528011083602905
CurrentTrain: epoch  9, batch     2 | loss: 0.3603210Losses:  0.03145815059542656 0.1914145052433014
CurrentTrain: epoch  9, batch     3 | loss: 0.2228727
Losses:  0.046668507158756256 0.5989742875099182
MemoryTrain:  epoch  0, batch     0 | loss: 0.6456428Losses:  0.031447988003492355 0.386275053024292
MemoryTrain:  epoch  0, batch     1 | loss: 0.4177230Losses:  0.05094161629676819 0.31730616092681885
MemoryTrain:  epoch  0, batch     2 | loss: 0.3682478Losses:  0.0737994909286499 0.46564704179763794
MemoryTrain:  epoch  1, batch     0 | loss: 0.5394465Losses:  0.0646250769495964 0.3822847902774811
MemoryTrain:  epoch  1, batch     1 | loss: 0.4469099Losses:  0.07007521390914917 0.3135432004928589
MemoryTrain:  epoch  1, batch     2 | loss: 0.3836184Losses:  0.07528823614120483 0.4554952085018158
MemoryTrain:  epoch  2, batch     0 | loss: 0.5307834Losses:  0.05626196786761284 0.4336938261985779
MemoryTrain:  epoch  2, batch     1 | loss: 0.4899558Losses:  0.06528808176517487 0.3017861247062683
MemoryTrain:  epoch  2, batch     2 | loss: 0.3670742Losses:  0.05034618452191353 0.5559650659561157
MemoryTrain:  epoch  3, batch     0 | loss: 0.6063113Losses:  0.05121108517050743 0.2817450165748596
MemoryTrain:  epoch  3, batch     1 | loss: 0.3329561Losses:  0.062109917402267456 0.26351243257522583
MemoryTrain:  epoch  3, batch     2 | loss: 0.3256223Losses:  0.03745794668793678 0.45979738235473633
MemoryTrain:  epoch  4, batch     0 | loss: 0.4972553Losses:  0.0502813458442688 0.4146988093852997
MemoryTrain:  epoch  4, batch     1 | loss: 0.4649802Losses:  0.04057837277650833 0.24270017445087433
MemoryTrain:  epoch  4, batch     2 | loss: 0.2832786Losses:  0.03747299313545227 0.4056535065174103
MemoryTrain:  epoch  5, batch     0 | loss: 0.4431265Losses:  0.03622662276029587 0.5791037082672119
MemoryTrain:  epoch  5, batch     1 | loss: 0.6153303Losses:  0.028159910812973976 0.20085452497005463
MemoryTrain:  epoch  5, batch     2 | loss: 0.2290144Losses:  0.03321254625916481 0.3588285744190216
MemoryTrain:  epoch  6, batch     0 | loss: 0.3920411Losses:  0.033441293984651566 0.4020368754863739
MemoryTrain:  epoch  6, batch     1 | loss: 0.4354782Losses:  0.029154986143112183 0.2910411059856415
MemoryTrain:  epoch  6, batch     2 | loss: 0.3201961Losses:  0.03544171527028084 0.3474452495574951
MemoryTrain:  epoch  7, batch     0 | loss: 0.3828870Losses:  0.04019569233059883 0.3660869300365448
MemoryTrain:  epoch  7, batch     1 | loss: 0.4062826Losses:  0.03390822187066078 0.25510093569755554
MemoryTrain:  epoch  7, batch     2 | loss: 0.2890092Losses:  0.028212012723088264 0.27791523933410645
MemoryTrain:  epoch  8, batch     0 | loss: 0.3061273Losses:  0.03184753656387329 0.4048938751220703
MemoryTrain:  epoch  8, batch     1 | loss: 0.4367414Losses:  0.030303895473480225 0.2742682695388794
MemoryTrain:  epoch  8, batch     2 | loss: 0.3045722Losses:  0.02720961719751358 0.44778457283973694
MemoryTrain:  epoch  9, batch     0 | loss: 0.4749942Losses:  0.030248936265707016 0.3139074444770813
MemoryTrain:  epoch  9, batch     1 | loss: 0.3441564Losses:  0.02698649652302265 0.14883029460906982
MemoryTrain:  epoch  9, batch     2 | loss: 0.1758168
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 60.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 77.16%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 77.31%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 77.16%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 77.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 89.19%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.66%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.69%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 89.22%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 88.11%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 87.90%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 87.89%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 88.34%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.42%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 88.66%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.72%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 88.70%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.60%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.57%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 88.30%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 88.13%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 87.58%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 86.97%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 86.68%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.25%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.19%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 85.99%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 85.65%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 85.32%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 85.00%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 84.75%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.44%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 83.94%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 83.51%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 83.09%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 82.62%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 82.41%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 82.02%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 81.76%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 81.31%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 80.82%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 80.33%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 79.92%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 79.39%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 78.99%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 78.71%   