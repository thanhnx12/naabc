#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  4.2357683181762695 1.8446838855743408
CurrentTrain: epoch  0, batch     0 | loss: 6.0804520Losses:  4.344927787780762 2.1789770126342773
CurrentTrain: epoch  0, batch     1 | loss: 6.5239048Losses:  4.558430194854736 1.698275089263916
CurrentTrain: epoch  0, batch     2 | loss: 6.2567053Losses:  4.651135444641113 1.7087725400924683
CurrentTrain: epoch  0, batch     3 | loss: 6.3599081Losses:  4.741326332092285 1.6166839599609375
CurrentTrain: epoch  0, batch     4 | loss: 6.3580103Losses:  4.935069561004639 1.5407447814941406
CurrentTrain: epoch  0, batch     5 | loss: 6.4758143Losses:  5.139118194580078 1.7609940767288208
CurrentTrain: epoch  0, batch     6 | loss: 6.9001122Losses:  5.110449314117432 1.4995753765106201
CurrentTrain: epoch  0, batch     7 | loss: 6.6100245Losses:  5.204567909240723 1.4989643096923828
CurrentTrain: epoch  0, batch     8 | loss: 6.7035322Losses:  5.5018463134765625 1.74544095993042
CurrentTrain: epoch  0, batch     9 | loss: 7.2472873Losses:  5.441015720367432 1.4652202129364014
CurrentTrain: epoch  0, batch    10 | loss: 6.9062357Losses:  5.7433247566223145 1.3265833854675293
CurrentTrain: epoch  0, batch    11 | loss: 7.0699081Losses:  5.890183448791504 1.4301035404205322
CurrentTrain: epoch  0, batch    12 | loss: 7.3202868Losses:  5.957231521606445 1.4181793928146362
CurrentTrain: epoch  0, batch    13 | loss: 7.3754110Losses:  5.920314311981201 1.4736331701278687
CurrentTrain: epoch  0, batch    14 | loss: 7.3939476Losses:  6.112974643707275 1.6613950729370117
CurrentTrain: epoch  0, batch    15 | loss: 7.7743697Losses:  6.280567169189453 1.1992448568344116
CurrentTrain: epoch  0, batch    16 | loss: 7.4798121Losses:  6.411548614501953 1.7580457925796509
CurrentTrain: epoch  0, batch    17 | loss: 8.1695948Losses:  6.638432502746582 1.571547269821167
CurrentTrain: epoch  0, batch    18 | loss: 8.2099800Losses:  6.934991359710693 1.7336125373840332
CurrentTrain: epoch  0, batch    19 | loss: 8.6686039Losses:  6.886970520019531 1.4434552192687988
CurrentTrain: epoch  0, batch    20 | loss: 8.3304253Losses:  7.20377254486084 1.747039794921875
CurrentTrain: epoch  0, batch    21 | loss: 8.9508123Losses:  7.311038017272949 1.9329293966293335
CurrentTrain: epoch  0, batch    22 | loss: 9.2439671Losses:  7.660480499267578 1.4686636924743652
CurrentTrain: epoch  0, batch    23 | loss: 9.1291447Losses:  7.397355556488037 1.389978051185608
CurrentTrain: epoch  0, batch    24 | loss: 8.7873335Losses:  7.879267692565918 1.4031050205230713
CurrentTrain: epoch  0, batch    25 | loss: 9.2823725Losses:  7.521902084350586 1.3070783615112305
CurrentTrain: epoch  0, batch    26 | loss: 8.8289804Losses:  8.172381401062012 1.5754187107086182
CurrentTrain: epoch  0, batch    27 | loss: 9.7477999Losses:  8.20591926574707 1.7770445346832275
CurrentTrain: epoch  0, batch    28 | loss: 9.9829636Losses:  8.754660606384277 1.6571266651153564
CurrentTrain: epoch  0, batch    29 | loss: 10.4117870Losses:  8.658610343933105 1.9940234422683716
CurrentTrain: epoch  0, batch    30 | loss: 10.6526337Losses:  8.711828231811523 1.1451081037521362
CurrentTrain: epoch  0, batch    31 | loss: 9.8569365Losses:  8.700201034545898 1.3474774360656738
CurrentTrain: epoch  0, batch    32 | loss: 10.0476780Losses:  8.831392288208008 0.9973562955856323
CurrentTrain: epoch  0, batch    33 | loss: 9.8287487Losses:  9.025894165039062 1.1951138973236084
CurrentTrain: epoch  0, batch    34 | loss: 10.2210083Losses:  9.4370756149292 1.3801789283752441
CurrentTrain: epoch  0, batch    35 | loss: 10.8172550Losses:  9.100481033325195 1.4630922079086304
CurrentTrain: epoch  0, batch    36 | loss: 10.5635729Losses:  9.746763229370117 1.3290247917175293
CurrentTrain: epoch  0, batch    37 | loss: 11.0757885Losses:  9.542229652404785 1.6248345375061035
CurrentTrain: epoch  0, batch    38 | loss: 11.1670647Losses:  9.681041717529297 1.7778267860412598
CurrentTrain: epoch  0, batch    39 | loss: 11.4588680Losses:  10.30959701538086 1.5865081548690796
CurrentTrain: epoch  0, batch    40 | loss: 11.8961048Losses:  10.112372398376465 1.410375952720642
CurrentTrain: epoch  0, batch    41 | loss: 11.5227480Losses:  10.081729888916016 1.2777177095413208
CurrentTrain: epoch  0, batch    42 | loss: 11.3594475Losses:  10.299518585205078 1.080132246017456
CurrentTrain: epoch  0, batch    43 | loss: 11.3796511Losses:  10.503106117248535 1.2436391115188599
CurrentTrain: epoch  0, batch    44 | loss: 11.7467451Losses:  11.007272720336914 1.318711757659912
CurrentTrain: epoch  0, batch    45 | loss: 12.3259850Losses:  10.496109008789062 1.0580923557281494
CurrentTrain: epoch  0, batch    46 | loss: 11.5542011Losses:  11.095059394836426 1.2918570041656494
CurrentTrain: epoch  0, batch    47 | loss: 12.3869162Losses:  10.767451286315918 1.449222207069397
CurrentTrain: epoch  0, batch    48 | loss: 12.2166739Losses:  11.348166465759277 1.211116075515747
CurrentTrain: epoch  0, batch    49 | loss: 12.5592823Losses:  11.825592041015625 1.488950252532959
CurrentTrain: epoch  0, batch    50 | loss: 13.3145428Losses:  11.453993797302246 1.3549749851226807
CurrentTrain: epoch  0, batch    51 | loss: 12.8089685Losses:  11.696250915527344 1.373278021812439
CurrentTrain: epoch  0, batch    52 | loss: 13.0695286Losses:  11.877124786376953 1.1375184059143066
CurrentTrain: epoch  0, batch    53 | loss: 13.0146427Losses:  11.992136001586914 1.5130265951156616
CurrentTrain: epoch  0, batch    54 | loss: 13.5051622Losses:  12.30865478515625 1.1155608892440796
CurrentTrain: epoch  0, batch    55 | loss: 13.4242153Losses:  12.71884822845459 0.9802048802375793
CurrentTrain: epoch  0, batch    56 | loss: 13.6990528Losses:  12.35313606262207 1.1898915767669678
CurrentTrain: epoch  0, batch    57 | loss: 13.5430279Losses:  12.535569190979004 0.9322880506515503
CurrentTrain: epoch  0, batch    58 | loss: 13.4678574Losses:  12.333735466003418 0.9157719612121582
CurrentTrain: epoch  0, batch    59 | loss: 13.2495079Losses:  12.72904109954834 1.0318608283996582
CurrentTrain: epoch  0, batch    60 | loss: 13.7609024Losses:  12.691726684570312 0.9332405924797058
CurrentTrain: epoch  0, batch    61 | loss: 13.6249676Losses:  13.07966423034668 0.9033098220825195
CurrentTrain: epoch  0, batch    62 | loss: 13.9829741Losses:  13.133955001831055 0.9533873796463013
CurrentTrain: epoch  1, batch     0 | loss: 14.0873423Losses:  12.774467468261719 0.8940281867980957
CurrentTrain: epoch  1, batch     1 | loss: 13.6684952Losses:  13.235136032104492 1.0581068992614746
CurrentTrain: epoch  1, batch     2 | loss: 14.2932434Losses:  13.05160140991211 0.9579587578773499
CurrentTrain: epoch  1, batch     3 | loss: 14.0095606Losses:  12.974040985107422 1.011573314666748
CurrentTrain: epoch  1, batch     4 | loss: 13.9856148Losses:  12.491549491882324 0.8837838768959045
CurrentTrain: epoch  1, batch     5 | loss: 13.3753338Losses:  13.067394256591797 0.8446100950241089
CurrentTrain: epoch  1, batch     6 | loss: 13.9120045Losses:  12.84878921508789 0.8584742546081543
CurrentTrain: epoch  1, batch     7 | loss: 13.7072639Losses:  13.011100769042969 0.6500735282897949
CurrentTrain: epoch  1, batch     8 | loss: 13.6611748Losses:  12.79325008392334 1.006910800933838
CurrentTrain: epoch  1, batch     9 | loss: 13.8001614Losses:  12.53002643585205 0.9594953656196594
CurrentTrain: epoch  1, batch    10 | loss: 13.4895220Losses:  12.836670875549316 1.1249349117279053
CurrentTrain: epoch  1, batch    11 | loss: 13.9616060Losses:  12.7348051071167 0.9769649505615234
CurrentTrain: epoch  1, batch    12 | loss: 13.7117701Losses:  13.198211669921875 0.659972071647644
CurrentTrain: epoch  1, batch    13 | loss: 13.8581839Losses:  12.604291915893555 0.942505955696106
CurrentTrain: epoch  1, batch    14 | loss: 13.5467978Losses:  12.082269668579102 0.9797676801681519
CurrentTrain: epoch  1, batch    15 | loss: 13.0620375Losses:  13.201334953308105 0.8659675717353821
CurrentTrain: epoch  1, batch    16 | loss: 14.0673027Losses:  12.91561508178711 0.5934232473373413
CurrentTrain: epoch  1, batch    17 | loss: 13.5090380Losses:  12.555364608764648 0.6923474669456482
CurrentTrain: epoch  1, batch    18 | loss: 13.2477121Losses:  12.505859375 0.7294951677322388
CurrentTrain: epoch  1, batch    19 | loss: 13.2353544Losses:  12.561532974243164 0.9189238548278809
CurrentTrain: epoch  1, batch    20 | loss: 13.4804573Losses:  12.720415115356445 0.9837004542350769
CurrentTrain: epoch  1, batch    21 | loss: 13.7041159Losses:  12.7603759765625 0.9421859383583069
CurrentTrain: epoch  1, batch    22 | loss: 13.7025623Losses:  12.809417724609375 0.6257171034812927
CurrentTrain: epoch  1, batch    23 | loss: 13.4351349Losses:  12.864042282104492 0.8561668395996094
CurrentTrain: epoch  1, batch    24 | loss: 13.7202091Losses:  12.909194946289062 0.7064286470413208
CurrentTrain: epoch  1, batch    25 | loss: 13.6156235Losses:  12.78197956085205 0.9742715954780579
CurrentTrain: epoch  1, batch    26 | loss: 13.7562513Losses:  12.7431001663208 0.9565355777740479
CurrentTrain: epoch  1, batch    27 | loss: 13.6996355Losses:  12.273483276367188 0.7411681413650513
CurrentTrain: epoch  1, batch    28 | loss: 13.0146513Losses:  12.742258071899414 0.8932453393936157
CurrentTrain: epoch  1, batch    29 | loss: 13.6355038Losses:  12.103328704833984 0.7655631303787231
CurrentTrain: epoch  1, batch    30 | loss: 12.8688917Losses:  12.582880020141602 0.789825439453125
CurrentTrain: epoch  1, batch    31 | loss: 13.3727055Losses:  12.99215316772461 0.8347796201705933
CurrentTrain: epoch  1, batch    32 | loss: 13.8269329Losses:  12.613404273986816 0.5252415537834167
CurrentTrain: epoch  1, batch    33 | loss: 13.1386461Losses:  12.604318618774414 0.8502444624900818
CurrentTrain: epoch  1, batch    34 | loss: 13.4545631Losses:  12.99316120147705 0.6880553960800171
CurrentTrain: epoch  1, batch    35 | loss: 13.6812162Losses:  12.239450454711914 0.452730268239975
CurrentTrain: epoch  1, batch    36 | loss: 12.6921806Losses:  12.413442611694336 0.78495192527771
CurrentTrain: epoch  1, batch    37 | loss: 13.1983948Losses:  12.915878295898438 0.8025542497634888
CurrentTrain: epoch  1, batch    38 | loss: 13.7184324Losses:  12.341081619262695 0.5725171566009521
CurrentTrain: epoch  1, batch    39 | loss: 12.9135990Losses:  12.36599349975586 0.6304543614387512
CurrentTrain: epoch  1, batch    40 | loss: 12.9964476Losses:  12.923173904418945 0.9291472434997559
CurrentTrain: epoch  1, batch    41 | loss: 13.8523216Losses:  12.579282760620117 0.7072767615318298
CurrentTrain: epoch  1, batch    42 | loss: 13.2865591Losses:  12.257762908935547 0.7105608582496643
CurrentTrain: epoch  1, batch    43 | loss: 12.9683237Losses:  12.305829048156738 0.6088041067123413
CurrentTrain: epoch  1, batch    44 | loss: 12.9146328Losses:  12.39010238647461 0.5642973780632019
CurrentTrain: epoch  1, batch    45 | loss: 12.9544001Losses:  12.584203720092773 0.6140210628509521
CurrentTrain: epoch  1, batch    46 | loss: 13.1982250Losses:  12.724884033203125 0.73207688331604
CurrentTrain: epoch  1, batch    47 | loss: 13.4569607Losses:  12.633530616760254 0.7048457264900208
CurrentTrain: epoch  1, batch    48 | loss: 13.3383760Losses:  12.540237426757812 0.6006675362586975
CurrentTrain: epoch  1, batch    49 | loss: 13.1409054Losses:  12.59556770324707 0.5081055164337158
CurrentTrain: epoch  1, batch    50 | loss: 13.1036730Losses:  12.102132797241211 0.6407546997070312
CurrentTrain: epoch  1, batch    51 | loss: 12.7428875Losses:  12.588460922241211 0.5767788290977478
CurrentTrain: epoch  1, batch    52 | loss: 13.1652393Losses:  12.114522933959961 0.5205983519554138
CurrentTrain: epoch  1, batch    53 | loss: 12.6351213Losses:  12.77917194366455 0.622048020362854
CurrentTrain: epoch  1, batch    54 | loss: 13.4012203Losses:  12.1029634475708 0.33358269929885864
CurrentTrain: epoch  1, batch    55 | loss: 12.4365463Losses:  12.393648147583008 0.4196684956550598
CurrentTrain: epoch  1, batch    56 | loss: 12.8133163Losses:  12.262338638305664 0.5945115685462952
CurrentTrain: epoch  1, batch    57 | loss: 12.8568506Losses:  12.443591117858887 0.48591890931129456
CurrentTrain: epoch  1, batch    58 | loss: 12.9295101Losses:  12.218029022216797 0.7674751877784729
CurrentTrain: epoch  1, batch    59 | loss: 12.9855042Losses:  12.501187324523926 0.7430109977722168
CurrentTrain: epoch  1, batch    60 | loss: 13.2441978Losses:  12.337907791137695 0.6294455528259277
CurrentTrain: epoch  1, batch    61 | loss: 12.9673538Losses:  12.454641342163086 0.1850692331790924
CurrentTrain: epoch  1, batch    62 | loss: 12.6397104Losses:  12.606330871582031 0.4943111538887024
CurrentTrain: epoch  2, batch     0 | loss: 13.1006422Losses:  11.821117401123047 0.5979971885681152
CurrentTrain: epoch  2, batch     1 | loss: 12.4191151Losses:  12.177911758422852 0.6184835433959961
CurrentTrain: epoch  2, batch     2 | loss: 12.7963953Losses:  12.68293571472168 0.5460928082466125
CurrentTrain: epoch  2, batch     3 | loss: 13.2290287Losses:  12.369918823242188 0.4528409242630005
CurrentTrain: epoch  2, batch     4 | loss: 12.8227596Losses:  12.443510055541992 0.6718419790267944
CurrentTrain: epoch  2, batch     5 | loss: 13.1153517Losses:  12.514363288879395 0.3703656494617462
CurrentTrain: epoch  2, batch     6 | loss: 12.8847294Losses:  11.995065689086914 0.5706958770751953
CurrentTrain: epoch  2, batch     7 | loss: 12.5657616Losses:  12.339744567871094 0.5150899887084961
CurrentTrain: epoch  2, batch     8 | loss: 12.8548346Losses:  12.006372451782227 0.36738914251327515
CurrentTrain: epoch  2, batch     9 | loss: 12.3737612Losses:  12.478937149047852 0.4341789484024048
CurrentTrain: epoch  2, batch    10 | loss: 12.9131165Losses:  12.545707702636719 0.4659249484539032
CurrentTrain: epoch  2, batch    11 | loss: 13.0116329Losses:  11.714984893798828 0.4867210388183594
CurrentTrain: epoch  2, batch    12 | loss: 12.2017059Losses:  12.133918762207031 0.4987429976463318
CurrentTrain: epoch  2, batch    13 | loss: 12.6326618Losses:  12.20600700378418 0.48635751008987427
CurrentTrain: epoch  2, batch    14 | loss: 12.6923647Losses:  11.89866828918457 0.5131455063819885
CurrentTrain: epoch  2, batch    15 | loss: 12.4118137Losses:  12.020931243896484 0.3556711673736572
CurrentTrain: epoch  2, batch    16 | loss: 12.3766022Losses:  12.536520004272461 0.3291078209877014
CurrentTrain: epoch  2, batch    17 | loss: 12.8656282Losses:  11.947225570678711 0.48181137442588806
CurrentTrain: epoch  2, batch    18 | loss: 12.4290371Losses:  11.889444351196289 0.42604300379753113
CurrentTrain: epoch  2, batch    19 | loss: 12.3154869Losses:  12.335434913635254 0.4379785656929016
CurrentTrain: epoch  2, batch    20 | loss: 12.7734137Losses:  12.076547622680664 0.3497242033481598
CurrentTrain: epoch  2, batch    21 | loss: 12.4262714Losses:  12.317011833190918 0.40042173862457275
CurrentTrain: epoch  2, batch    22 | loss: 12.7174339Losses:  11.956697463989258 0.4669804573059082
CurrentTrain: epoch  2, batch    23 | loss: 12.4236774Losses:  12.444404602050781 0.42072752118110657
CurrentTrain: epoch  2, batch    24 | loss: 12.8651323Losses:  12.079962730407715 0.3903544545173645
CurrentTrain: epoch  2, batch    25 | loss: 12.4703169Losses:  12.442283630371094 0.3898770213127136
CurrentTrain: epoch  2, batch    26 | loss: 12.8321609Losses:  12.258828163146973 0.34044894576072693
CurrentTrain: epoch  2, batch    27 | loss: 12.5992775Losses:  12.426092147827148 0.41398441791534424
CurrentTrain: epoch  2, batch    28 | loss: 12.8400764Losses:  12.201898574829102 0.3835713863372803
CurrentTrain: epoch  2, batch    29 | loss: 12.5854702Losses:  12.386805534362793 0.5401472449302673
CurrentTrain: epoch  2, batch    30 | loss: 12.9269524Losses:  12.005414962768555 0.5106495022773743
CurrentTrain: epoch  2, batch    31 | loss: 12.5160646Losses:  12.309194564819336 0.44364285469055176
CurrentTrain: epoch  2, batch    32 | loss: 12.7528372Losses:  12.543795585632324 0.3636449873447418
CurrentTrain: epoch  2, batch    33 | loss: 12.9074402Losses:  12.405073165893555 0.43858277797698975
CurrentTrain: epoch  2, batch    34 | loss: 12.8436556Losses:  11.999706268310547 0.3880769908428192
CurrentTrain: epoch  2, batch    35 | loss: 12.3877831Losses:  12.154367446899414 0.3216036558151245
CurrentTrain: epoch  2, batch    36 | loss: 12.4759712Losses:  12.622673034667969 0.35163527727127075
CurrentTrain: epoch  2, batch    37 | loss: 12.9743080Losses:  11.829036712646484 0.2763791084289551
CurrentTrain: epoch  2, batch    38 | loss: 12.1054153Losses:  12.420360565185547 0.3229096233844757
CurrentTrain: epoch  2, batch    39 | loss: 12.7432699Losses:  12.464622497558594 0.3533434271812439
CurrentTrain: epoch  2, batch    40 | loss: 12.8179655Losses:  12.629728317260742 0.3437947630882263
CurrentTrain: epoch  2, batch    41 | loss: 12.9735231Losses:  12.401981353759766 0.343169629573822
CurrentTrain: epoch  2, batch    42 | loss: 12.7451506Losses:  12.289012908935547 0.431938111782074
CurrentTrain: epoch  2, batch    43 | loss: 12.7209511Losses:  12.439336776733398 0.5556758642196655
CurrentTrain: epoch  2, batch    44 | loss: 12.9950123Losses:  12.150455474853516 0.3734460473060608
CurrentTrain: epoch  2, batch    45 | loss: 12.5239019Losses:  11.886069297790527 0.302219033241272
CurrentTrain: epoch  2, batch    46 | loss: 12.1882887Losses:  12.16856575012207 0.4985750913619995
CurrentTrain: epoch  2, batch    47 | loss: 12.6671410Losses:  12.300190925598145 0.28575509786605835
CurrentTrain: epoch  2, batch    48 | loss: 12.5859461Losses:  12.202543258666992 0.431247353553772
CurrentTrain: epoch  2, batch    49 | loss: 12.6337910Losses:  11.884525299072266 0.24866579473018646
CurrentTrain: epoch  2, batch    50 | loss: 12.1331911Losses:  12.198622703552246 0.3084600567817688
CurrentTrain: epoch  2, batch    51 | loss: 12.5070829Losses:  11.849530220031738 0.36132973432540894
CurrentTrain: epoch  2, batch    52 | loss: 12.2108603Losses:  12.032527923583984 0.32656851410865784
CurrentTrain: epoch  2, batch    53 | loss: 12.3590965Losses:  12.068594932556152 0.3459330201148987
CurrentTrain: epoch  2, batch    54 | loss: 12.4145279Losses:  11.898641586303711 0.3378237783908844
CurrentTrain: epoch  2, batch    55 | loss: 12.2364655Losses:  12.23292350769043 0.46168118715286255
CurrentTrain: epoch  2, batch    56 | loss: 12.6946049Losses:  11.887991905212402 0.37501418590545654
CurrentTrain: epoch  2, batch    57 | loss: 12.2630062Losses:  12.20677661895752 0.2905969023704529
CurrentTrain: epoch  2, batch    58 | loss: 12.4973736Losses:  11.988706588745117 0.2569096088409424
CurrentTrain: epoch  2, batch    59 | loss: 12.2456160Losses:  12.322617530822754 0.4012048542499542
CurrentTrain: epoch  2, batch    60 | loss: 12.7238226Losses:  12.233512878417969 0.31596672534942627
CurrentTrain: epoch  2, batch    61 | loss: 12.5494795Losses:  11.690996170043945 0.1541675329208374
CurrentTrain: epoch  2, batch    62 | loss: 11.8451633Losses:  11.65992546081543 0.3051215410232544
CurrentTrain: epoch  3, batch     0 | loss: 11.9650469Losses:  12.234457015991211 0.5595559477806091
CurrentTrain: epoch  3, batch     1 | loss: 12.7940130Losses:  11.999405860900879 0.3584616482257843
CurrentTrain: epoch  3, batch     2 | loss: 12.3578672Losses:  12.137080192565918 0.30602505803108215
CurrentTrain: epoch  3, batch     3 | loss: 12.4431057Losses:  12.045074462890625 0.2598768174648285
CurrentTrain: epoch  3, batch     4 | loss: 12.3049517Losses:  12.081299781799316 0.23185694217681885
CurrentTrain: epoch  3, batch     5 | loss: 12.3131571Losses:  11.968172073364258 0.3045283555984497
CurrentTrain: epoch  3, batch     6 | loss: 12.2727003Losses:  11.752054214477539 0.28366732597351074
CurrentTrain: epoch  3, batch     7 | loss: 12.0357218Losses:  12.3403959274292 0.4184492826461792
CurrentTrain: epoch  3, batch     8 | loss: 12.7588453Losses:  12.143289566040039 0.2854018807411194
CurrentTrain: epoch  3, batch     9 | loss: 12.4286919Losses:  12.06508731842041 0.3504304885864258
CurrentTrain: epoch  3, batch    10 | loss: 12.4155178Losses:  12.070355415344238 0.29049137234687805
CurrentTrain: epoch  3, batch    11 | loss: 12.3608465Losses:  11.853612899780273 0.20190581679344177
CurrentTrain: epoch  3, batch    12 | loss: 12.0555191Losses:  12.229179382324219 0.36013269424438477
CurrentTrain: epoch  3, batch    13 | loss: 12.5893116Losses:  12.045759201049805 0.31560564041137695
CurrentTrain: epoch  3, batch    14 | loss: 12.3613644Losses:  12.138996124267578 0.27846240997314453
CurrentTrain: epoch  3, batch    15 | loss: 12.4174585Losses:  12.108713150024414 0.32170814275741577
CurrentTrain: epoch  3, batch    16 | loss: 12.4304209Losses:  11.875882148742676 0.24706566333770752
CurrentTrain: epoch  3, batch    17 | loss: 12.1229477Losses:  12.042381286621094 0.22922782599925995
CurrentTrain: epoch  3, batch    18 | loss: 12.2716093Losses:  12.233304977416992 0.35996919870376587
CurrentTrain: epoch  3, batch    19 | loss: 12.5932741Losses:  11.80785083770752 0.27876123785972595
CurrentTrain: epoch  3, batch    20 | loss: 12.0866117Losses:  11.882279396057129 0.28014445304870605
CurrentTrain: epoch  3, batch    21 | loss: 12.1624241Losses:  11.853727340698242 0.3272102475166321
CurrentTrain: epoch  3, batch    22 | loss: 12.1809378Losses:  12.09815502166748 0.2998904883861542
CurrentTrain: epoch  3, batch    23 | loss: 12.3980455Losses:  12.025514602661133 0.1870456486940384
CurrentTrain: epoch  3, batch    24 | loss: 12.2125607Losses:  11.963933944702148 0.2862137258052826
CurrentTrain: epoch  3, batch    25 | loss: 12.2501478Losses:  11.817516326904297 0.35846537351608276
CurrentTrain: epoch  3, batch    26 | loss: 12.1759815Losses:  11.883977890014648 0.3206818103790283
CurrentTrain: epoch  3, batch    27 | loss: 12.2046595Losses:  11.957801818847656 0.34884729981422424
CurrentTrain: epoch  3, batch    28 | loss: 12.3066492Losses:  12.299697875976562 0.3263651728630066
CurrentTrain: epoch  3, batch    29 | loss: 12.6260633Losses:  11.715161323547363 0.3573361933231354
CurrentTrain: epoch  3, batch    30 | loss: 12.0724974Losses:  11.841449737548828 0.3408491015434265
CurrentTrain: epoch  3, batch    31 | loss: 12.1822987Losses:  11.962254524230957 0.3160548210144043
CurrentTrain: epoch  3, batch    32 | loss: 12.2783089Losses:  12.156120300292969 0.254401296377182
CurrentTrain: epoch  3, batch    33 | loss: 12.4105215Losses:  12.276844024658203 0.2447027713060379
CurrentTrain: epoch  3, batch    34 | loss: 12.5215464Losses:  12.124580383300781 0.29521501064300537
CurrentTrain: epoch  3, batch    35 | loss: 12.4197950Losses:  12.229430198669434 0.5194199681282043
CurrentTrain: epoch  3, batch    36 | loss: 12.7488499Losses:  11.851297378540039 0.24730050563812256
CurrentTrain: epoch  3, batch    37 | loss: 12.0985975Losses:  11.725462913513184 0.23583582043647766
CurrentTrain: epoch  3, batch    38 | loss: 11.9612989Losses:  11.48447036743164 0.270011842250824
CurrentTrain: epoch  3, batch    39 | loss: 11.7544823Losses:  11.911430358886719 0.2686510682106018
CurrentTrain: epoch  3, batch    40 | loss: 12.1800814Losses:  12.320551872253418 0.16780000925064087
CurrentTrain: epoch  3, batch    41 | loss: 12.4883518Losses:  11.805181503295898 0.25550252199172974
CurrentTrain: epoch  3, batch    42 | loss: 12.0606842Losses:  12.086526870727539 0.18945208191871643
CurrentTrain: epoch  3, batch    43 | loss: 12.2759790Losses:  12.179727554321289 0.3849143981933594
CurrentTrain: epoch  3, batch    44 | loss: 12.5646420Losses:  11.82371711730957 0.15882672369480133
CurrentTrain: epoch  3, batch    45 | loss: 11.9825439Losses:  11.674911499023438 0.21229034662246704
CurrentTrain: epoch  3, batch    46 | loss: 11.8872023Losses:  11.775093078613281 0.20575310289859772
CurrentTrain: epoch  3, batch    47 | loss: 11.9808464Losses:  12.027706146240234 0.16629649698734283
CurrentTrain: epoch  3, batch    48 | loss: 12.1940031Losses:  12.279474258422852 0.2577091455459595
CurrentTrain: epoch  3, batch    49 | loss: 12.5371838Losses:  11.940719604492188 0.2550961673259735
CurrentTrain: epoch  3, batch    50 | loss: 12.1958160Losses:  11.891019821166992 0.16144070029258728
CurrentTrain: epoch  3, batch    51 | loss: 12.0524607Losses:  12.120210647583008 0.2264314889907837
CurrentTrain: epoch  3, batch    52 | loss: 12.3466425Losses:  12.224635124206543 0.3667677640914917
CurrentTrain: epoch  3, batch    53 | loss: 12.5914030Losses:  12.144169807434082 0.2556537389755249
CurrentTrain: epoch  3, batch    54 | loss: 12.3998232Losses:  11.916261672973633 0.24691736698150635
CurrentTrain: epoch  3, batch    55 | loss: 12.1631794Losses:  12.10793399810791 0.2324027121067047
CurrentTrain: epoch  3, batch    56 | loss: 12.3403368Losses:  11.92536735534668 0.38783788681030273
CurrentTrain: epoch  3, batch    57 | loss: 12.3132057Losses:  11.683534622192383 0.20219913125038147
CurrentTrain: epoch  3, batch    58 | loss: 11.8857336Losses:  11.71885871887207 0.1400291472673416
CurrentTrain: epoch  3, batch    59 | loss: 11.8588877Losses:  12.076614379882812 0.29924339056015015
CurrentTrain: epoch  3, batch    60 | loss: 12.3758574Losses:  11.705385208129883 0.17291167378425598
CurrentTrain: epoch  3, batch    61 | loss: 11.8782969Losses:  11.795623779296875 0.08362480998039246
CurrentTrain: epoch  3, batch    62 | loss: 11.8792486Losses:  12.057311058044434 0.17185048758983612
CurrentTrain: epoch  4, batch     0 | loss: 12.2291613Losses:  12.040390014648438 0.23441633582115173
CurrentTrain: epoch  4, batch     1 | loss: 12.2748060Losses:  11.781965255737305 0.1814570128917694
CurrentTrain: epoch  4, batch     2 | loss: 11.9634218Losses:  11.644506454467773 0.2527911365032196
CurrentTrain: epoch  4, batch     3 | loss: 11.8972979Losses:  11.702032089233398 0.2506120800971985
CurrentTrain: epoch  4, batch     4 | loss: 11.9526443Losses:  11.906179428100586 0.25648033618927
CurrentTrain: epoch  4, batch     5 | loss: 12.1626596Losses:  11.958671569824219 0.17367909848690033
CurrentTrain: epoch  4, batch     6 | loss: 12.1323509Losses:  11.978384017944336 0.24067199230194092
CurrentTrain: epoch  4, batch     7 | loss: 12.2190561Losses:  11.736766815185547 0.24387487769126892
CurrentTrain: epoch  4, batch     8 | loss: 11.9806414Losses:  11.607738494873047 0.2442096322774887
CurrentTrain: epoch  4, batch     9 | loss: 11.8519478Losses:  11.665383338928223 0.17877048254013062
CurrentTrain: epoch  4, batch    10 | loss: 11.8441534Losses:  11.671270370483398 0.3482740521430969
CurrentTrain: epoch  4, batch    11 | loss: 12.0195446Losses:  11.582767486572266 0.19234788417816162
CurrentTrain: epoch  4, batch    12 | loss: 11.7751150Losses:  12.158731460571289 0.24480801820755005
CurrentTrain: epoch  4, batch    13 | loss: 12.4035397Losses:  11.988235473632812 0.1390918344259262
CurrentTrain: epoch  4, batch    14 | loss: 12.1273270Losses:  11.897256851196289 0.16008222103118896
CurrentTrain: epoch  4, batch    15 | loss: 12.0573387Losses:  12.298881530761719 0.23448289930820465
CurrentTrain: epoch  4, batch    16 | loss: 12.5333643Losses:  11.30177116394043 0.22260376811027527
CurrentTrain: epoch  4, batch    17 | loss: 11.5243750Losses:  11.935995101928711 0.20488670468330383
CurrentTrain: epoch  4, batch    18 | loss: 12.1408815Losses:  12.006391525268555 0.23865558207035065
CurrentTrain: epoch  4, batch    19 | loss: 12.2450476Losses:  11.97386360168457 0.3352084159851074
CurrentTrain: epoch  4, batch    20 | loss: 12.3090725Losses:  11.776588439941406 0.1478380709886551
CurrentTrain: epoch  4, batch    21 | loss: 11.9244261Losses:  12.105087280273438 0.18185584247112274
CurrentTrain: epoch  4, batch    22 | loss: 12.2869434Losses:  11.587334632873535 0.10211730003356934
CurrentTrain: epoch  4, batch    23 | loss: 11.6894522Losses:  12.194664001464844 0.15439513325691223
CurrentTrain: epoch  4, batch    24 | loss: 12.3490591Losses:  11.805265426635742 0.20027220249176025
CurrentTrain: epoch  4, batch    25 | loss: 12.0055380Losses:  11.937034606933594 0.18967758119106293
CurrentTrain: epoch  4, batch    26 | loss: 12.1267118Losses:  11.717470169067383 0.16574372351169586
CurrentTrain: epoch  4, batch    27 | loss: 11.8832140Losses:  12.191919326782227 0.28095871210098267
CurrentTrain: epoch  4, batch    28 | loss: 12.4728785Losses:  11.839808464050293 0.15720868110656738
CurrentTrain: epoch  4, batch    29 | loss: 11.9970169Losses:  11.617902755737305 0.1388169825077057
CurrentTrain: epoch  4, batch    30 | loss: 11.7567196Losses:  11.903950691223145 0.2384120374917984
CurrentTrain: epoch  4, batch    31 | loss: 12.1423626Losses:  11.992391586303711 0.23021727800369263
CurrentTrain: epoch  4, batch    32 | loss: 12.2226086Losses:  12.161632537841797 0.24046386778354645
CurrentTrain: epoch  4, batch    33 | loss: 12.4020967Losses:  11.430585861206055 0.14575673639774323
CurrentTrain: epoch  4, batch    34 | loss: 11.5763426Losses:  11.764700889587402 0.27394017577171326
CurrentTrain: epoch  4, batch    35 | loss: 12.0386410Losses:  12.016921997070312 0.24773578345775604
CurrentTrain: epoch  4, batch    36 | loss: 12.2646580Losses:  12.004886627197266 0.28211718797683716
CurrentTrain: epoch  4, batch    37 | loss: 12.2870035Losses:  11.668508529663086 0.15644550323486328
CurrentTrain: epoch  4, batch    38 | loss: 11.8249540Losses:  11.939979553222656 0.19548991322517395
CurrentTrain: epoch  4, batch    39 | loss: 12.1354694Losses:  11.822806358337402 0.14206618070602417
CurrentTrain: epoch  4, batch    40 | loss: 11.9648724Losses:  11.859657287597656 0.21677948534488678
CurrentTrain: epoch  4, batch    41 | loss: 12.0764370Losses:  12.077613830566406 0.32765352725982666
CurrentTrain: epoch  4, batch    42 | loss: 12.4052677Losses:  12.22024917602539 0.17479850351810455
CurrentTrain: epoch  4, batch    43 | loss: 12.3950481Losses:  11.620162963867188 0.17814162373542786
CurrentTrain: epoch  4, batch    44 | loss: 11.7983046Losses:  11.980958938598633 0.19876976311206818
CurrentTrain: epoch  4, batch    45 | loss: 12.1797285Losses:  12.070656776428223 0.16912740468978882
CurrentTrain: epoch  4, batch    46 | loss: 12.2397842Losses:  11.688590049743652 0.19678159058094025
CurrentTrain: epoch  4, batch    47 | loss: 11.8853712Losses:  11.849629402160645 0.1866309940814972
CurrentTrain: epoch  4, batch    48 | loss: 12.0362606Losses:  11.966110229492188 0.15055422484874725
CurrentTrain: epoch  4, batch    49 | loss: 12.1166649Losses:  12.068166732788086 0.2795058488845825
CurrentTrain: epoch  4, batch    50 | loss: 12.3476725Losses:  11.662334442138672 0.12112542986869812
CurrentTrain: epoch  4, batch    51 | loss: 11.7834597Losses:  12.093888282775879 0.1721573770046234
CurrentTrain: epoch  4, batch    52 | loss: 12.2660456Losses:  11.95928955078125 0.19841566681861877
CurrentTrain: epoch  4, batch    53 | loss: 12.1577053Losses:  11.888347625732422 0.15063858032226562
CurrentTrain: epoch  4, batch    54 | loss: 12.0389862Losses:  12.034722328186035 0.17159108817577362
CurrentTrain: epoch  4, batch    55 | loss: 12.2063131Losses:  11.651101112365723 0.145340234041214
CurrentTrain: epoch  4, batch    56 | loss: 11.7964411Losses:  11.654315948486328 0.2118537575006485
CurrentTrain: epoch  4, batch    57 | loss: 11.8661699Losses:  11.716571807861328 0.13374310731887817
CurrentTrain: epoch  4, batch    58 | loss: 11.8503151Losses:  11.597016334533691 0.12826326489448547
CurrentTrain: epoch  4, batch    59 | loss: 11.7252798Losses:  11.641875267028809 0.1431143879890442
CurrentTrain: epoch  4, batch    60 | loss: 11.7849894Losses:  11.545174598693848 0.15998421609401703
CurrentTrain: epoch  4, batch    61 | loss: 11.7051592Losses:  11.295085906982422 0.0733204036951065
CurrentTrain: epoch  4, batch    62 | loss: 11.3684063Losses:  11.712932586669922 0.20368489623069763
CurrentTrain: epoch  5, batch     0 | loss: 11.9166174Losses:  12.031648635864258 0.1891251802444458
CurrentTrain: epoch  5, batch     1 | loss: 12.2207737Losses:  11.757393836975098 0.1336309164762497
CurrentTrain: epoch  5, batch     2 | loss: 11.8910246Losses:  11.629104614257812 0.13741929829120636
CurrentTrain: epoch  5, batch     3 | loss: 11.7665243Losses:  11.520265579223633 0.1284143030643463
CurrentTrain: epoch  5, batch     4 | loss: 11.6486797Losses:  11.900666236877441 0.1519802063703537
CurrentTrain: epoch  5, batch     5 | loss: 12.0526466Losses:  11.648258209228516 0.14617785811424255
CurrentTrain: epoch  5, batch     6 | loss: 11.7944365Losses:  11.733270645141602 0.32574212551116943
CurrentTrain: epoch  5, batch     7 | loss: 12.0590124Losses:  11.283390998840332 0.17869019508361816
CurrentTrain: epoch  5, batch     8 | loss: 11.4620810Losses:  11.942450523376465 0.12543168663978577
CurrentTrain: epoch  5, batch     9 | loss: 12.0678825Losses:  12.029681205749512 0.2588781714439392
CurrentTrain: epoch  5, batch    10 | loss: 12.2885590Losses:  11.445996284484863 0.1551634669303894
CurrentTrain: epoch  5, batch    11 | loss: 11.6011600Losses:  11.954468727111816 0.15655119717121124
CurrentTrain: epoch  5, batch    12 | loss: 12.1110201Losses:  11.618324279785156 0.15876857936382294
CurrentTrain: epoch  5, batch    13 | loss: 11.7770929Losses:  11.715765953063965 0.17928358912467957
CurrentTrain: epoch  5, batch    14 | loss: 11.8950491Losses:  11.44975471496582 0.16066887974739075
CurrentTrain: epoch  5, batch    15 | loss: 11.6104240Losses:  11.529045104980469 0.1913265585899353
CurrentTrain: epoch  5, batch    16 | loss: 11.7203712Losses:  11.645265579223633 0.16354945302009583
CurrentTrain: epoch  5, batch    17 | loss: 11.8088150Losses:  11.437705039978027 0.2323438674211502
CurrentTrain: epoch  5, batch    18 | loss: 11.6700487Losses:  11.706087112426758 0.17498886585235596
CurrentTrain: epoch  5, batch    19 | loss: 11.8810759Losses:  11.667893409729004 0.15939286351203918
CurrentTrain: epoch  5, batch    20 | loss: 11.8272867Losses:  11.679373741149902 0.18950696289539337
CurrentTrain: epoch  5, batch    21 | loss: 11.8688803Losses:  11.823233604431152 0.14866766333580017
CurrentTrain: epoch  5, batch    22 | loss: 11.9719009Losses:  11.563224792480469 0.18547788262367249
CurrentTrain: epoch  5, batch    23 | loss: 11.7487030Losses:  11.730921745300293 0.10418127477169037
CurrentTrain: epoch  5, batch    24 | loss: 11.8351030Losses:  11.660372734069824 0.17011436820030212
CurrentTrain: epoch  5, batch    25 | loss: 11.8304873Losses:  11.800504684448242 0.16764189302921295
CurrentTrain: epoch  5, batch    26 | loss: 11.9681463Losses:  11.496853828430176 0.15453749895095825
CurrentTrain: epoch  5, batch    27 | loss: 11.6513910Losses:  11.50160026550293 0.1802016943693161
CurrentTrain: epoch  5, batch    28 | loss: 11.6818018Losses:  11.870885848999023 0.1662655770778656
CurrentTrain: epoch  5, batch    29 | loss: 12.0371513Losses:  11.61745834350586 0.10396550595760345
CurrentTrain: epoch  5, batch    30 | loss: 11.7214241Losses:  11.761009216308594 0.1311110109090805
CurrentTrain: epoch  5, batch    31 | loss: 11.8921204Losses:  11.723344802856445 0.10737060010433197
CurrentTrain: epoch  5, batch    32 | loss: 11.8307152Losses:  12.038095474243164 0.13426029682159424
CurrentTrain: epoch  5, batch    33 | loss: 12.1723557Losses:  12.009293556213379 0.14668723940849304
CurrentTrain: epoch  5, batch    34 | loss: 12.1559811Losses:  12.065958023071289 0.23623545467853546
CurrentTrain: epoch  5, batch    35 | loss: 12.3021936Losses:  11.910990715026855 0.1234247013926506
CurrentTrain: epoch  5, batch    36 | loss: 12.0344152Losses:  11.957757949829102 0.13871440291404724
CurrentTrain: epoch  5, batch    37 | loss: 12.0964727Losses:  11.607694625854492 0.15305744111537933
CurrentTrain: epoch  5, batch    38 | loss: 11.7607517Losses:  11.838479042053223 0.1639419049024582
CurrentTrain: epoch  5, batch    39 | loss: 12.0024214Losses:  11.741125106811523 0.16489550471305847
CurrentTrain: epoch  5, batch    40 | loss: 11.9060202Losses:  11.966257095336914 0.12908107042312622
CurrentTrain: epoch  5, batch    41 | loss: 12.0953379Losses:  11.551331520080566 0.22343680262565613
CurrentTrain: epoch  5, batch    42 | loss: 11.7747679Losses:  11.824207305908203 0.21111822128295898
CurrentTrain: epoch  5, batch    43 | loss: 12.0353260Losses:  11.839823722839355 0.15433435142040253
CurrentTrain: epoch  5, batch    44 | loss: 11.9941578Losses:  11.83733081817627 0.17680560052394867
CurrentTrain: epoch  5, batch    45 | loss: 12.0141363Losses:  12.163317680358887 0.1465047001838684
CurrentTrain: epoch  5, batch    46 | loss: 12.3098221Losses:  11.941551208496094 0.15507295727729797
CurrentTrain: epoch  5, batch    47 | loss: 12.0966244Losses:  11.695169448852539 0.1570502668619156
CurrentTrain: epoch  5, batch    48 | loss: 11.8522196Losses:  11.70300579071045 0.154748797416687
CurrentTrain: epoch  5, batch    49 | loss: 11.8577547Losses:  11.816255569458008 0.14017769694328308
CurrentTrain: epoch  5, batch    50 | loss: 11.9564333Losses:  11.684818267822266 0.127614364027977
CurrentTrain: epoch  5, batch    51 | loss: 11.8124323Losses:  11.723485946655273 0.21833057701587677
CurrentTrain: epoch  5, batch    52 | loss: 11.9418163Losses:  11.750633239746094 0.14759385585784912
CurrentTrain: epoch  5, batch    53 | loss: 11.8982267Losses:  11.79665756225586 0.35933369398117065
CurrentTrain: epoch  5, batch    54 | loss: 12.1559916Losses:  12.02489185333252 0.1619194895029068
CurrentTrain: epoch  5, batch    55 | loss: 12.1868114Losses:  11.809965133666992 0.18677672743797302
CurrentTrain: epoch  5, batch    56 | loss: 11.9967422Losses:  11.776556015014648 0.14967672526836395
CurrentTrain: epoch  5, batch    57 | loss: 11.9262323Losses:  11.780747413635254 0.147105872631073
CurrentTrain: epoch  5, batch    58 | loss: 11.9278536Losses:  12.118098258972168 0.13367311656475067
CurrentTrain: epoch  5, batch    59 | loss: 12.2517710Losses:  11.694952964782715 0.12029503285884857
CurrentTrain: epoch  5, batch    60 | loss: 11.8152475Losses:  11.699631690979004 0.13163027167320251
CurrentTrain: epoch  5, batch    61 | loss: 11.8312616Losses:  11.193916320800781 0.11016011238098145
CurrentTrain: epoch  5, batch    62 | loss: 11.3040762Losses:  11.800792694091797 0.1400812864303589
CurrentTrain: epoch  6, batch     0 | loss: 11.9408741Losses:  11.295778274536133 0.15568840503692627
CurrentTrain: epoch  6, batch     1 | loss: 11.4514666Losses:  11.781068801879883 0.12062286585569382
CurrentTrain: epoch  6, batch     2 | loss: 11.9016914Losses:  12.030671119689941 0.3788633346557617
CurrentTrain: epoch  6, batch     3 | loss: 12.4095345Losses:  11.928836822509766 0.11378970742225647
CurrentTrain: epoch  6, batch     4 | loss: 12.0426264Losses:  11.408662796020508 0.14852958917617798
CurrentTrain: epoch  6, batch     5 | loss: 11.5571928Losses:  11.703251838684082 0.15952105820178986
CurrentTrain: epoch  6, batch     6 | loss: 11.8627729Losses:  11.426666259765625 0.18370142579078674
CurrentTrain: epoch  6, batch     7 | loss: 11.6103678Losses:  11.927528381347656 0.17211338877677917
CurrentTrain: epoch  6, batch     8 | loss: 12.0996418Losses:  11.986271858215332 0.10850277543067932
CurrentTrain: epoch  6, batch     9 | loss: 12.0947742Losses:  11.689964294433594 0.15349197387695312
CurrentTrain: epoch  6, batch    10 | loss: 11.8434563Losses:  11.402695655822754 0.13321515917778015
CurrentTrain: epoch  6, batch    11 | loss: 11.5359106Losses:  11.685823440551758 0.16523557901382446
CurrentTrain: epoch  6, batch    12 | loss: 11.8510590Losses:  11.698043823242188 0.1078825294971466
CurrentTrain: epoch  6, batch    13 | loss: 11.8059263Losses:  11.69475269317627 0.1661619246006012
CurrentTrain: epoch  6, batch    14 | loss: 11.8609142Losses:  11.50256633758545 0.17989298701286316
CurrentTrain: epoch  6, batch    15 | loss: 11.6824589Losses:  11.789653778076172 0.14738711714744568
CurrentTrain: epoch  6, batch    16 | loss: 11.9370413Losses:  11.62214183807373 0.14817947149276733
CurrentTrain: epoch  6, batch    17 | loss: 11.7703209Losses:  11.266568183898926 0.07879638671875
CurrentTrain: epoch  6, batch    18 | loss: 11.3453646Losses:  11.687426567077637 0.15437915921211243
CurrentTrain: epoch  6, batch    19 | loss: 11.8418055Losses:  11.832115173339844 0.17382587492465973
CurrentTrain: epoch  6, batch    20 | loss: 12.0059414Losses:  11.313138961791992 0.16634568572044373
CurrentTrain: epoch  6, batch    21 | loss: 11.4794846Losses:  11.684371948242188 0.15409238636493683
CurrentTrain: epoch  6, batch    22 | loss: 11.8384647Losses:  11.776135444641113 0.12248624116182327
CurrentTrain: epoch  6, batch    23 | loss: 11.8986216Losses:  11.855243682861328 0.12181195616722107
CurrentTrain: epoch  6, batch    24 | loss: 11.9770555Losses:  11.537422180175781 0.17779958248138428
CurrentTrain: epoch  6, batch    25 | loss: 11.7152214Losses:  11.609838485717773 0.14727666974067688
CurrentTrain: epoch  6, batch    26 | loss: 11.7571154Losses:  11.743764877319336 0.12831741571426392
CurrentTrain: epoch  6, batch    27 | loss: 11.8720827Losses:  11.811422348022461 0.1403443068265915
CurrentTrain: epoch  6, batch    28 | loss: 11.9517670Losses:  11.815256118774414 0.13507987558841705
CurrentTrain: epoch  6, batch    29 | loss: 11.9503365Losses:  11.683061599731445 0.18580850958824158
CurrentTrain: epoch  6, batch    30 | loss: 11.8688698Losses:  11.620201110839844 0.09815506637096405
CurrentTrain: epoch  6, batch    31 | loss: 11.7183561Losses:  11.908895492553711 0.094320148229599
CurrentTrain: epoch  6, batch    32 | loss: 12.0032158Losses:  11.82172966003418 0.14619331061840057
CurrentTrain: epoch  6, batch    33 | loss: 11.9679232Losses:  12.088220596313477 0.12940436601638794
CurrentTrain: epoch  6, batch    34 | loss: 12.2176247Losses:  11.961997985839844 0.07472285628318787
CurrentTrain: epoch  6, batch    35 | loss: 12.0367212Losses:  11.914682388305664 0.07646428793668747
CurrentTrain: epoch  6, batch    36 | loss: 11.9911470Losses:  11.616917610168457 0.12628333270549774
CurrentTrain: epoch  6, batch    37 | loss: 11.7432013Losses:  11.759517669677734 0.15999720990657806
CurrentTrain: epoch  6, batch    38 | loss: 11.9195147Losses:  11.554304122924805 0.09330658614635468
CurrentTrain: epoch  6, batch    39 | loss: 11.6476107Losses:  11.632594108581543 0.12194639444351196
CurrentTrain: epoch  6, batch    40 | loss: 11.7545404Losses:  11.73587703704834 0.15304967761039734
CurrentTrain: epoch  6, batch    41 | loss: 11.8889265Losses:  11.86449146270752 0.1359335482120514
CurrentTrain: epoch  6, batch    42 | loss: 12.0004253Losses:  11.693485260009766 0.12058528512716293
CurrentTrain: epoch  6, batch    43 | loss: 11.8140707Losses:  11.466846466064453 0.1278752088546753
CurrentTrain: epoch  6, batch    44 | loss: 11.5947218Losses:  11.566068649291992 0.12273543328046799
CurrentTrain: epoch  6, batch    45 | loss: 11.6888037Losses:  11.920077323913574 0.1337193250656128
CurrentTrain: epoch  6, batch    46 | loss: 12.0537968Losses:  11.196767807006836 0.13784898817539215
CurrentTrain: epoch  6, batch    47 | loss: 11.3346167Losses:  11.501604080200195 0.14495140314102173
CurrentTrain: epoch  6, batch    48 | loss: 11.6465559Losses:  11.546566009521484 0.0898403525352478
CurrentTrain: epoch  6, batch    49 | loss: 11.6364059Losses:  11.495689392089844 0.12066160142421722
CurrentTrain: epoch  6, batch    50 | loss: 11.6163511Losses:  11.488435745239258 0.10797439515590668
CurrentTrain: epoch  6, batch    51 | loss: 11.5964098Losses:  11.537595748901367 0.12264467775821686
CurrentTrain: epoch  6, batch    52 | loss: 11.6602402Losses:  11.512205123901367 0.05740269273519516
CurrentTrain: epoch  6, batch    53 | loss: 11.5696077Losses:  11.764884948730469 0.12116604298353195
CurrentTrain: epoch  6, batch    54 | loss: 11.8860512Losses:  11.894428253173828 0.11642661690711975
CurrentTrain: epoch  6, batch    55 | loss: 12.0108547Losses:  11.834807395935059 0.08020137995481491
CurrentTrain: epoch  6, batch    56 | loss: 11.9150085Losses:  11.737499237060547 0.10795660316944122
CurrentTrain: epoch  6, batch    57 | loss: 11.8454561Losses:  11.587267875671387 0.12960657477378845
CurrentTrain: epoch  6, batch    58 | loss: 11.7168741Losses:  11.937093734741211 0.1336977183818817
CurrentTrain: epoch  6, batch    59 | loss: 12.0707912Losses:  11.614295959472656 0.09811680763959885
CurrentTrain: epoch  6, batch    60 | loss: 11.7124128Losses:  11.85190486907959 0.10750116407871246
CurrentTrain: epoch  6, batch    61 | loss: 11.9594059Losses:  12.114819526672363 0.11234354972839355
CurrentTrain: epoch  6, batch    62 | loss: 12.2271633Losses:  11.637619018554688 0.09632619470357895
CurrentTrain: epoch  7, batch     0 | loss: 11.7339449Losses:  11.980600357055664 0.09741917997598648
CurrentTrain: epoch  7, batch     1 | loss: 12.0780191Losses:  11.846172332763672 0.10235988348722458
CurrentTrain: epoch  7, batch     2 | loss: 11.9485321Losses:  11.55510139465332 0.11549020558595657
CurrentTrain: epoch  7, batch     3 | loss: 11.6705914Losses:  11.531638145446777 0.11081668734550476
CurrentTrain: epoch  7, batch     4 | loss: 11.6424551Losses:  11.301673889160156 0.11133769154548645
CurrentTrain: epoch  7, batch     5 | loss: 11.4130116Losses:  11.485822677612305 0.12048757076263428
CurrentTrain: epoch  7, batch     6 | loss: 11.6063099Losses:  11.466516494750977 0.1381336748600006
CurrentTrain: epoch  7, batch     7 | loss: 11.6046505Losses:  11.574406623840332 0.08466358482837677
CurrentTrain: epoch  7, batch     8 | loss: 11.6590700Losses:  11.969170570373535 0.12382231652736664
CurrentTrain: epoch  7, batch     9 | loss: 12.0929928Losses:  11.865327835083008 0.12592250108718872
CurrentTrain: epoch  7, batch    10 | loss: 11.9912500Losses:  11.69379997253418 0.10044679045677185
CurrentTrain: epoch  7, batch    11 | loss: 11.7942467Losses:  11.816460609436035 0.10932222753763199
CurrentTrain: epoch  7, batch    12 | loss: 11.9257832Losses:  11.771373748779297 0.08507747948169708
CurrentTrain: epoch  7, batch    13 | loss: 11.8564510Losses:  11.717840194702148 0.11566842347383499
CurrentTrain: epoch  7, batch    14 | loss: 11.8335085Losses:  11.870111465454102 0.13130483031272888
CurrentTrain: epoch  7, batch    15 | loss: 12.0014162Losses:  12.089488983154297 0.1325380504131317
CurrentTrain: epoch  7, batch    16 | loss: 12.2220268Losses:  12.088424682617188 0.17953461408615112
CurrentTrain: epoch  7, batch    17 | loss: 12.2679596Losses:  11.505786895751953 0.10696826875209808
CurrentTrain: epoch  7, batch    18 | loss: 11.6127548Losses:  11.454421997070312 0.12854811549186707
CurrentTrain: epoch  7, batch    19 | loss: 11.5829697Losses:  11.55762767791748 0.09273773431777954
CurrentTrain: epoch  7, batch    20 | loss: 11.6503658Losses:  11.981380462646484 0.22489096224308014
CurrentTrain: epoch  7, batch    21 | loss: 12.2062712Losses:  11.430240631103516 0.10902726650238037
CurrentTrain: epoch  7, batch    22 | loss: 11.5392675Losses:  11.736957550048828 0.10270214825868607
CurrentTrain: epoch  7, batch    23 | loss: 11.8396597Losses:  12.00658893585205 0.12606897950172424
CurrentTrain: epoch  7, batch    24 | loss: 12.1326580Losses:  11.783248901367188 0.10587890446186066
CurrentTrain: epoch  7, batch    25 | loss: 11.8891277Losses:  11.588006973266602 0.09640759229660034
CurrentTrain: epoch  7, batch    26 | loss: 11.6844149Losses:  11.543066024780273 0.12613362073898315
CurrentTrain: epoch  7, batch    27 | loss: 11.6691999Losses:  12.006580352783203 0.11857501417398453
CurrentTrain: epoch  7, batch    28 | loss: 12.1251554Losses:  11.311037063598633 0.09662888944149017
CurrentTrain: epoch  7, batch    29 | loss: 11.4076662Losses:  11.452516555786133 0.10923302173614502
CurrentTrain: epoch  7, batch    30 | loss: 11.5617495Losses:  11.618138313293457 0.12076683342456818
CurrentTrain: epoch  7, batch    31 | loss: 11.7389050Losses:  11.187479019165039 0.10203129798173904
CurrentTrain: epoch  7, batch    32 | loss: 11.2895107Losses:  11.666084289550781 0.10665622353553772
CurrentTrain: epoch  7, batch    33 | loss: 11.7727404Losses:  11.095525741577148 0.12023256719112396
CurrentTrain: epoch  7, batch    34 | loss: 11.2157583Losses:  11.189729690551758 0.1308811604976654
CurrentTrain: epoch  7, batch    35 | loss: 11.3206110Losses:  11.224632263183594 0.13022372126579285
CurrentTrain: epoch  7, batch    36 | loss: 11.3548555Losses:  11.842686653137207 0.09617121517658234
CurrentTrain: epoch  7, batch    37 | loss: 11.9388580Losses:  11.888805389404297 0.11384116113185883
CurrentTrain: epoch  7, batch    38 | loss: 12.0026464Losses:  11.652666091918945 0.09419642388820648
CurrentTrain: epoch  7, batch    39 | loss: 11.7468624Losses:  11.763508796691895 0.13146193325519562
CurrentTrain: epoch  7, batch    40 | loss: 11.8949709Losses:  11.894838333129883 0.09557522088289261
CurrentTrain: epoch  7, batch    41 | loss: 11.9904137Losses:  11.661687850952148 0.10716182738542557
CurrentTrain: epoch  7, batch    42 | loss: 11.7688494Losses:  11.441495895385742 0.12100933492183685
CurrentTrain: epoch  7, batch    43 | loss: 11.5625048Losses:  11.477709770202637 0.12174592912197113
CurrentTrain: epoch  7, batch    44 | loss: 11.5994558Losses:  11.646089553833008 0.10763786733150482
CurrentTrain: epoch  7, batch    45 | loss: 11.7537270Losses:  11.450481414794922 0.09885057806968689
CurrentTrain: epoch  7, batch    46 | loss: 11.5493317Losses:  11.498631477355957 0.11684785038232803
CurrentTrain: epoch  7, batch    47 | loss: 11.6154795Losses:  12.05540657043457 0.13339009881019592
CurrentTrain: epoch  7, batch    48 | loss: 12.1887970Losses:  11.988422393798828 0.1317768394947052
CurrentTrain: epoch  7, batch    49 | loss: 12.1201992Losses:  11.549867630004883 0.09271829575300217
CurrentTrain: epoch  7, batch    50 | loss: 11.6425858Losses:  11.730165481567383 0.10465441644191742
CurrentTrain: epoch  7, batch    51 | loss: 11.8348198Losses:  12.27932357788086 0.11096583306789398
CurrentTrain: epoch  7, batch    52 | loss: 12.3902893Losses:  11.707170486450195 0.1179351732134819
CurrentTrain: epoch  7, batch    53 | loss: 11.8251057Losses:  11.576909065246582 0.11884406208992004
CurrentTrain: epoch  7, batch    54 | loss: 11.6957531Losses:  11.892844200134277 0.09831364452838898
CurrentTrain: epoch  7, batch    55 | loss: 11.9911575Losses:  11.215539932250977 0.10218019783496857
CurrentTrain: epoch  7, batch    56 | loss: 11.3177204Losses:  11.412662506103516 0.1351231336593628
CurrentTrain: epoch  7, batch    57 | loss: 11.5477858Losses:  11.68832015991211 0.13903120160102844
CurrentTrain: epoch  7, batch    58 | loss: 11.8273516Losses:  11.60029125213623 0.08234262466430664
CurrentTrain: epoch  7, batch    59 | loss: 11.6826344Losses:  11.562704086303711 0.1463160514831543
CurrentTrain: epoch  7, batch    60 | loss: 11.7090206Losses:  11.697216033935547 0.08237873017787933
CurrentTrain: epoch  7, batch    61 | loss: 11.7795944Losses:  11.666482925415039 0.06788107007741928
CurrentTrain: epoch  7, batch    62 | loss: 11.7343636Losses:  11.754180908203125 0.1279107630252838
CurrentTrain: epoch  8, batch     0 | loss: 11.8820915Losses:  11.685625076293945 0.14320924878120422
CurrentTrain: epoch  8, batch     1 | loss: 11.8288345Losses:  11.579258918762207 0.13852405548095703
CurrentTrain: epoch  8, batch     2 | loss: 11.7177830Losses:  11.631143569946289 0.07434116303920746
CurrentTrain: epoch  8, batch     3 | loss: 11.7054844Losses:  11.844253540039062 0.09723318368196487
CurrentTrain: epoch  8, batch     4 | loss: 11.9414864Losses:  11.498056411743164 0.12159623205661774
CurrentTrain: epoch  8, batch     5 | loss: 11.6196527Losses:  11.628700256347656 0.10968296229839325
CurrentTrain: epoch  8, batch     6 | loss: 11.7383833Losses:  11.531732559204102 0.10385681688785553
CurrentTrain: epoch  8, batch     7 | loss: 11.6355896Losses:  11.404985427856445 0.1190057024359703
CurrentTrain: epoch  8, batch     8 | loss: 11.5239916Losses:  11.649922370910645 0.12479172646999359
CurrentTrain: epoch  8, batch     9 | loss: 11.7747145Losses:  11.31800651550293 0.10312116146087646
CurrentTrain: epoch  8, batch    10 | loss: 11.4211273Losses:  11.377016067504883 0.11966317147016525
CurrentTrain: epoch  8, batch    11 | loss: 11.4966793Losses:  11.29779052734375 0.1095958799123764
CurrentTrain: epoch  8, batch    12 | loss: 11.4073868Losses:  11.37312126159668 0.10491088032722473
CurrentTrain: epoch  8, batch    13 | loss: 11.4780321Losses:  11.836380004882812 0.11175239086151123
CurrentTrain: epoch  8, batch    14 | loss: 11.9481325Losses:  11.75200080871582 0.1145283579826355
CurrentTrain: epoch  8, batch    15 | loss: 11.8665295Losses:  11.456681251525879 0.07801588624715805
CurrentTrain: epoch  8, batch    16 | loss: 11.5346975Losses:  11.581964492797852 0.12033569812774658
CurrentTrain: epoch  8, batch    17 | loss: 11.7023001Losses:  11.73262882232666 0.106271892786026
CurrentTrain: epoch  8, batch    18 | loss: 11.8389006Losses:  11.432077407836914 0.10734868794679642
CurrentTrain: epoch  8, batch    19 | loss: 11.5394258Losses:  11.301878929138184 0.12012887001037598
CurrentTrain: epoch  8, batch    20 | loss: 11.4220076Losses:  11.463598251342773 0.10580912232398987
CurrentTrain: epoch  8, batch    21 | loss: 11.5694075Losses:  11.896806716918945 0.09610823541879654
CurrentTrain: epoch  8, batch    22 | loss: 11.9929152Losses:  11.535703659057617 0.1316167116165161
CurrentTrain: epoch  8, batch    23 | loss: 11.6673203Losses:  11.56964111328125 0.13871489465236664
CurrentTrain: epoch  8, batch    24 | loss: 11.7083559Losses:  11.603790283203125 0.08281822502613068
CurrentTrain: epoch  8, batch    25 | loss: 11.6866083Losses:  11.845226287841797 0.09812348335981369
CurrentTrain: epoch  8, batch    26 | loss: 11.9433498Losses:  11.575892448425293 0.12048176676034927
CurrentTrain: epoch  8, batch    27 | loss: 11.6963739Losses:  11.952850341796875 0.3331656754016876
CurrentTrain: epoch  8, batch    28 | loss: 12.2860165Losses:  11.62195873260498 0.11259420216083527
CurrentTrain: epoch  8, batch    29 | loss: 11.7345533Losses:  11.50987434387207 0.11809832602739334
CurrentTrain: epoch  8, batch    30 | loss: 11.6279726Losses:  11.454925537109375 0.1083037480711937
CurrentTrain: epoch  8, batch    31 | loss: 11.5632296Losses:  11.825016975402832 0.09800723940134048
CurrentTrain: epoch  8, batch    32 | loss: 11.9230242Losses:  11.34201431274414 0.09741359204053879
CurrentTrain: epoch  8, batch    33 | loss: 11.4394283Losses:  11.61147689819336 0.12689150869846344
CurrentTrain: epoch  8, batch    34 | loss: 11.7383680Losses:  11.787864685058594 0.10749252885580063
CurrentTrain: epoch  8, batch    35 | loss: 11.8953571Losses:  11.140207290649414 0.09629422426223755
CurrentTrain: epoch  8, batch    36 | loss: 11.2365017Losses:  11.827279090881348 0.09664275497198105
CurrentTrain: epoch  8, batch    37 | loss: 11.9239216Losses:  11.91226577758789 0.12477585673332214
CurrentTrain: epoch  8, batch    38 | loss: 12.0370417Losses:  11.717724800109863 0.11449170112609863
CurrentTrain: epoch  8, batch    39 | loss: 11.8322163Losses:  11.818842887878418 0.21948695182800293
CurrentTrain: epoch  8, batch    40 | loss: 12.0383301Losses:  11.539833068847656 0.11598581075668335
CurrentTrain: epoch  8, batch    41 | loss: 11.6558189Losses:  11.407727241516113 0.12242813408374786
CurrentTrain: epoch  8, batch    42 | loss: 11.5301552Losses:  11.578238487243652 0.12500819563865662
CurrentTrain: epoch  8, batch    43 | loss: 11.7032471Losses:  11.680452346801758 0.11475937813520432
CurrentTrain: epoch  8, batch    44 | loss: 11.7952118Losses:  11.687606811523438 0.07406085729598999
CurrentTrain: epoch  8, batch    45 | loss: 11.7616673Losses:  11.818013191223145 0.09139560908079147
CurrentTrain: epoch  8, batch    46 | loss: 11.9094086Losses:  12.029642105102539 0.08949489146471024
CurrentTrain: epoch  8, batch    47 | loss: 12.1191368Losses:  11.33541488647461 0.1003478616476059
CurrentTrain: epoch  8, batch    48 | loss: 11.4357624Losses:  11.789081573486328 0.07682565599679947
CurrentTrain: epoch  8, batch    49 | loss: 11.8659077Losses:  11.613203048706055 0.08991815894842148
CurrentTrain: epoch  8, batch    50 | loss: 11.7031212Losses:  11.6782808303833 0.08164370059967041
CurrentTrain: epoch  8, batch    51 | loss: 11.7599249Losses:  12.048196792602539 0.12234765291213989
CurrentTrain: epoch  8, batch    52 | loss: 12.1705446Losses:  11.8712797164917 0.10546501725912094
CurrentTrain: epoch  8, batch    53 | loss: 11.9767447Losses:  11.676359176635742 0.11099694669246674
CurrentTrain: epoch  8, batch    54 | loss: 11.7873564Losses:  11.757511138916016 0.09213560819625854
CurrentTrain: epoch  8, batch    55 | loss: 11.8496466Losses:  11.564151763916016 0.10926967859268188
CurrentTrain: epoch  8, batch    56 | loss: 11.6734219Losses:  11.46885871887207 0.13610821962356567
CurrentTrain: epoch  8, batch    57 | loss: 11.6049671Losses:  11.539395332336426 0.12054683268070221
CurrentTrain: epoch  8, batch    58 | loss: 11.6599426Losses:  11.921257019042969 0.1139189675450325
CurrentTrain: epoch  8, batch    59 | loss: 12.0351763Losses:  11.913162231445312 0.10853821039199829
CurrentTrain: epoch  8, batch    60 | loss: 12.0217009Losses:  11.42805290222168 0.1317935585975647
CurrentTrain: epoch  8, batch    61 | loss: 11.5598469Losses:  10.94450569152832 0.027683619409799576
CurrentTrain: epoch  8, batch    62 | loss: 10.9721889Losses:  11.629744529724121 0.08338628709316254
CurrentTrain: epoch  9, batch     0 | loss: 11.7131310Losses:  11.683391571044922 0.0925222858786583
CurrentTrain: epoch  9, batch     1 | loss: 11.7759142Losses:  11.521390914916992 0.10476099699735641
CurrentTrain: epoch  9, batch     2 | loss: 11.6261520Losses:  11.611212730407715 0.09372184425592422
CurrentTrain: epoch  9, batch     3 | loss: 11.7049341Losses:  11.713438034057617 0.07401265949010849
CurrentTrain: epoch  9, batch     4 | loss: 11.7874508Losses:  11.555360794067383 0.0856235921382904
CurrentTrain: epoch  9, batch     5 | loss: 11.6409845Losses:  11.58814811706543 0.10779757797718048
CurrentTrain: epoch  9, batch     6 | loss: 11.6959457Losses:  11.2528076171875 0.10012885928153992
CurrentTrain: epoch  9, batch     7 | loss: 11.3529367Losses:  11.784910202026367 0.08304168283939362
CurrentTrain: epoch  9, batch     8 | loss: 11.8679523Losses:  11.71590805053711 0.10607092082500458
CurrentTrain: epoch  9, batch     9 | loss: 11.8219786Losses:  11.552082061767578 0.10884764790534973
CurrentTrain: epoch  9, batch    10 | loss: 11.6609297Losses:  11.869417190551758 0.11398623883724213
CurrentTrain: epoch  9, batch    11 | loss: 11.9834032Losses:  11.75467586517334 0.11435884237289429
CurrentTrain: epoch  9, batch    12 | loss: 11.8690348Losses:  11.305164337158203 0.12084894627332687
CurrentTrain: epoch  9, batch    13 | loss: 11.4260130Losses:  11.849617004394531 0.13496391475200653
CurrentTrain: epoch  9, batch    14 | loss: 11.9845810Losses:  11.490011215209961 0.11834250390529633
CurrentTrain: epoch  9, batch    15 | loss: 11.6083536Losses:  11.756361961364746 0.11088882386684418
CurrentTrain: epoch  9, batch    16 | loss: 11.8672504Losses:  11.562992095947266 0.11442552506923676
CurrentTrain: epoch  9, batch    17 | loss: 11.6774178Losses:  11.326629638671875 0.09381383657455444
CurrentTrain: epoch  9, batch    18 | loss: 11.4204435Losses:  11.563709259033203 0.04878396540880203
CurrentTrain: epoch  9, batch    19 | loss: 11.6124935Losses:  11.417521476745605 0.09749838709831238
CurrentTrain: epoch  9, batch    20 | loss: 11.5150194Losses:  11.753620147705078 0.09715326130390167
CurrentTrain: epoch  9, batch    21 | loss: 11.8507738Losses:  11.630134582519531 0.11112423986196518
CurrentTrain: epoch  9, batch    22 | loss: 11.7412586Losses:  11.540420532226562 0.09290684759616852
CurrentTrain: epoch  9, batch    23 | loss: 11.6333275Losses:  11.614412307739258 0.08264099061489105
CurrentTrain: epoch  9, batch    24 | loss: 11.6970530Losses:  11.840142250061035 0.08961152285337448
CurrentTrain: epoch  9, batch    25 | loss: 11.9297533Losses:  11.846355438232422 0.1077442318201065
CurrentTrain: epoch  9, batch    26 | loss: 11.9540997Losses:  11.342482566833496 0.11115753650665283
CurrentTrain: epoch  9, batch    27 | loss: 11.4536400Losses:  11.631879806518555 0.12587346136569977
CurrentTrain: epoch  9, batch    28 | loss: 11.7577534Losses:  11.828954696655273 0.09659282863140106
CurrentTrain: epoch  9, batch    29 | loss: 11.9255476Losses:  11.596622467041016 0.11351773142814636
CurrentTrain: epoch  9, batch    30 | loss: 11.7101402Losses:  11.876640319824219 0.11809059232473373
CurrentTrain: epoch  9, batch    31 | loss: 11.9947309Losses:  11.51573371887207 0.09362927824258804
CurrentTrain: epoch  9, batch    32 | loss: 11.6093626Losses:  11.062013626098633 0.06357994675636292
CurrentTrain: epoch  9, batch    33 | loss: 11.1255932Losses:  11.589670181274414 0.11823587119579315
CurrentTrain: epoch  9, batch    34 | loss: 11.7079058Losses:  11.76630687713623 0.1236787810921669
CurrentTrain: epoch  9, batch    35 | loss: 11.8899860Losses:  11.347893714904785 0.0692654550075531
CurrentTrain: epoch  9, batch    36 | loss: 11.4171591Losses:  11.403824806213379 0.11128000915050507
CurrentTrain: epoch  9, batch    37 | loss: 11.5151052Losses:  11.392094612121582 0.07779249548912048
CurrentTrain: epoch  9, batch    38 | loss: 11.4698868Losses:  11.841960906982422 0.14055201411247253
CurrentTrain: epoch  9, batch    39 | loss: 11.9825125Losses:  11.698378562927246 0.09163650870323181
CurrentTrain: epoch  9, batch    40 | loss: 11.7900152Losses:  11.479193687438965 0.06748171150684357
CurrentTrain: epoch  9, batch    41 | loss: 11.5466757Losses:  11.515265464782715 0.06609552353620529
CurrentTrain: epoch  9, batch    42 | loss: 11.5813608Losses:  11.435702323913574 0.07324591279029846
CurrentTrain: epoch  9, batch    43 | loss: 11.5089483Losses:  11.300931930541992 0.10028032213449478
CurrentTrain: epoch  9, batch    44 | loss: 11.4012127Losses:  11.691950798034668 0.10358935594558716
CurrentTrain: epoch  9, batch    45 | loss: 11.7955399Losses:  11.640039443969727 0.07154252380132675
CurrentTrain: epoch  9, batch    46 | loss: 11.7115822Losses:  11.829265594482422 0.07464222609996796
CurrentTrain: epoch  9, batch    47 | loss: 11.9039078Losses:  11.642173767089844 0.08943937718868256
CurrentTrain: epoch  9, batch    48 | loss: 11.7316132Losses:  12.335530281066895 0.3009358048439026
CurrentTrain: epoch  9, batch    49 | loss: 12.6364660Losses:  11.724961280822754 0.11755669116973877
CurrentTrain: epoch  9, batch    50 | loss: 11.8425179Losses:  11.60619068145752 0.11318352073431015
CurrentTrain: epoch  9, batch    51 | loss: 11.7193747Losses:  11.597062110900879 0.08594848960638046
CurrentTrain: epoch  9, batch    52 | loss: 11.6830111Losses:  11.771907806396484 0.11477313190698624
CurrentTrain: epoch  9, batch    53 | loss: 11.8866806Losses:  11.569849014282227 0.10769779235124588
CurrentTrain: epoch  9, batch    54 | loss: 11.6775465Losses:  11.498603820800781 0.08435376733541489
CurrentTrain: epoch  9, batch    55 | loss: 11.5829573Losses:  11.571706771850586 0.11985636502504349
CurrentTrain: epoch  9, batch    56 | loss: 11.6915636Losses:  11.893301010131836 0.11286943405866623
CurrentTrain: epoch  9, batch    57 | loss: 12.0061703Losses:  11.618480682373047 0.08323162794113159
CurrentTrain: epoch  9, batch    58 | loss: 11.7017126Losses:  11.660628318786621 0.11925852298736572
CurrentTrain: epoch  9, batch    59 | loss: 11.7798872Losses:  11.621068954467773 0.11041386425495148
CurrentTrain: epoch  9, batch    60 | loss: 11.7314825Losses:  11.535101890563965 0.11600039899349213
CurrentTrain: epoch  9, batch    61 | loss: 11.6511021Losses:  11.583696365356445 0.0543849840760231
CurrentTrain: epoch  9, batch    62 | loss: 11.6380816
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 94.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 94.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.74%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 93.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.52%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.14%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.26%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.34%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.64%   
cur_acc:  ['0.9464']
his_acc:  ['0.9464']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  2.755307912826538 1.343280553817749
CurrentTrain: epoch  0, batch     0 | loss: 4.0985885Losses:  4.071736812591553 1.1905784606933594
CurrentTrain: epoch  0, batch     1 | loss: 5.2623153Losses:  7.316776275634766 1.2719862461090088
CurrentTrain: epoch  0, batch     2 | loss: 8.5887623Losses:  7.960597991943359 1.4901162614933128e-07
CurrentTrain: epoch  0, batch     3 | loss: 7.9605980Losses:  11.897979736328125 1.2241864204406738
CurrentTrain: epoch  1, batch     0 | loss: 13.1221657Losses:  12.493989944458008 1.2875114679336548
CurrentTrain: epoch  1, batch     1 | loss: 13.7815018Losses:  12.796602249145508 1.2001742124557495
CurrentTrain: epoch  1, batch     2 | loss: 13.9967766Losses:  10.80953311920166 0.17456285655498505
CurrentTrain: epoch  1, batch     3 | loss: 10.9840956Losses:  12.097444534301758 0.946880578994751
CurrentTrain: epoch  2, batch     0 | loss: 13.0443249Losses:  12.498321533203125 0.9787200689315796
CurrentTrain: epoch  2, batch     1 | loss: 13.4770412Losses:  11.065835952758789 0.9098564386367798
CurrentTrain: epoch  2, batch     2 | loss: 11.9756927Losses:  12.088776588439941 0.5539705753326416
CurrentTrain: epoch  2, batch     3 | loss: 12.6427469Losses:  11.80405044555664 1.099926233291626
CurrentTrain: epoch  3, batch     0 | loss: 12.9039764Losses:  10.939335823059082 0.7874579429626465
CurrentTrain: epoch  3, batch     1 | loss: 11.7267933Losses:  11.873461723327637 1.1698896884918213
CurrentTrain: epoch  3, batch     2 | loss: 13.0433512Losses:  10.130059242248535 0.23040321469306946
CurrentTrain: epoch  3, batch     3 | loss: 10.3604622Losses:  11.323270797729492 0.8745418787002563
CurrentTrain: epoch  4, batch     0 | loss: 12.1978130Losses:  10.563030242919922 0.8792494535446167
CurrentTrain: epoch  4, batch     1 | loss: 11.4422798Losses:  11.339422225952148 0.9960489273071289
CurrentTrain: epoch  4, batch     2 | loss: 12.3354712Losses:  11.107771873474121 0.054528091102838516
CurrentTrain: epoch  4, batch     3 | loss: 11.1623001Losses:  11.062593460083008 0.9671077132225037
CurrentTrain: epoch  5, batch     0 | loss: 12.0297012Losses:  10.841927528381348 0.9942485690116882
CurrentTrain: epoch  5, batch     1 | loss: 11.8361759Losses:  10.749557495117188 1.0667641162872314
CurrentTrain: epoch  5, batch     2 | loss: 11.8163214Losses:  11.550329208374023 0.13700664043426514
CurrentTrain: epoch  5, batch     3 | loss: 11.6873360Losses:  10.967081069946289 1.0605125427246094
CurrentTrain: epoch  6, batch     0 | loss: 12.0275936Losses:  10.884445190429688 0.9088907241821289
CurrentTrain: epoch  6, batch     1 | loss: 11.7933359Losses:  10.460294723510742 0.9422138929367065
CurrentTrain: epoch  6, batch     2 | loss: 11.4025087Losses:  9.554340362548828 1.1920930376163597e-07
CurrentTrain: epoch  6, batch     3 | loss: 9.5543404Losses:  10.350378036499023 0.9514719247817993
CurrentTrain: epoch  7, batch     0 | loss: 11.3018503Losses:  10.366678237915039 0.9853262901306152
CurrentTrain: epoch  7, batch     1 | loss: 11.3520050Losses:  10.718860626220703 0.8023571968078613
CurrentTrain: epoch  7, batch     2 | loss: 11.5212173Losses:  11.104837417602539 1.4901162614933128e-07
CurrentTrain: epoch  7, batch     3 | loss: 11.1048374Losses:  10.092757225036621 0.8376249074935913
CurrentTrain: epoch  8, batch     0 | loss: 10.9303818Losses:  10.902663230895996 0.9524542093276978
CurrentTrain: epoch  8, batch     1 | loss: 11.8551178Losses:  9.919872283935547 0.7851563096046448
CurrentTrain: epoch  8, batch     2 | loss: 10.7050285Losses:  8.946708679199219 0.08256066590547562
CurrentTrain: epoch  8, batch     3 | loss: 9.0292692Losses:  9.851410865783691 0.8330742120742798
CurrentTrain: epoch  9, batch     0 | loss: 10.6844854Losses:  10.345697402954102 0.8146043419837952
CurrentTrain: epoch  9, batch     1 | loss: 11.1603022Losses:  10.167739868164062 0.7085659503936768
CurrentTrain: epoch  9, batch     2 | loss: 10.8763056Losses:  9.861248016357422 0.12624239921569824
CurrentTrain: epoch  9, batch     3 | loss: 9.9874907
Losses:  0.00019006003276444972 0.916767954826355
MemoryTrain:  epoch  0, batch     0 | loss: 0.9169580Losses:  0.0008900131215341389 0.0884474366903305
MemoryTrain:  epoch  0, batch     1 | loss: 0.0893375Losses:  10.346396446228027 0.7649562358856201
MemoryTrain:  epoch  1, batch     0 | loss: 11.1113529Losses:  13.064465522766113 0.4941331446170807
MemoryTrain:  epoch  1, batch     1 | loss: 13.5585985Losses:  10.947940826416016 0.7222713232040405
MemoryTrain:  epoch  2, batch     0 | loss: 11.6702118Losses:  10.170961380004883 0.23499253392219543
MemoryTrain:  epoch  2, batch     1 | loss: 10.4059544Losses:  10.74429988861084 0.6466078758239746
MemoryTrain:  epoch  3, batch     0 | loss: 11.3909073Losses:  9.918877601623535 0.28816136717796326
MemoryTrain:  epoch  3, batch     1 | loss: 10.2070389Losses:  10.337353706359863 0.8072081804275513
MemoryTrain:  epoch  4, batch     0 | loss: 11.1445618Losses:  9.944549560546875 0.14771342277526855
MemoryTrain:  epoch  4, batch     1 | loss: 10.0922632Losses:  10.216144561767578 0.7970777153968811
MemoryTrain:  epoch  5, batch     0 | loss: 11.0132227Losses:  9.282806396484375 0.20589254796504974
MemoryTrain:  epoch  5, batch     1 | loss: 9.4886990Losses:  10.142234802246094 0.780634880065918
MemoryTrain:  epoch  6, batch     0 | loss: 10.9228697Losses:  9.023965835571289 0.15387271344661713
MemoryTrain:  epoch  6, batch     1 | loss: 9.1778383Losses:  9.823739051818848 0.7551119327545166
MemoryTrain:  epoch  7, batch     0 | loss: 10.5788507Losses:  9.156312942504883 0.1292765736579895
MemoryTrain:  epoch  7, batch     1 | loss: 9.2855892Losses:  9.656465530395508 0.740148663520813
MemoryTrain:  epoch  8, batch     0 | loss: 10.3966141Losses:  9.487197875976562 0.29330307245254517
MemoryTrain:  epoch  8, batch     1 | loss: 9.7805014Losses:  9.593170166015625 0.8067952394485474
MemoryTrain:  epoch  9, batch     0 | loss: 10.3999653Losses:  9.061250686645508 0.1041044145822525
MemoryTrain:  epoch  9, batch     1 | loss: 9.1653547
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 63.34%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 62.03%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 61.89%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 61.34%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 61.08%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 59.51%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 58.64%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 57.94%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 57.27%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 57.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 57.60%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 57.93%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 58.25%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 58.64%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 59.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 60.49%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 60.83%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 61.48%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 61.51%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.31%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.89%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.90%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 92.60%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 92.66%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 92.14%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 91.81%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 91.23%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 90.67%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 90.20%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 89.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 89.64%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.53%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.42%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 89.16%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 88.83%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 88.81%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 88.48%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 88.32%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 88.01%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 87.94%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 87.57%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 87.29%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 87.01%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 86.46%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 85.85%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 85.53%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 84.88%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 84.18%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 83.62%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 83.20%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 82.80%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 82.27%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 82.07%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 81.44%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 80.82%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 80.46%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 80.29%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 80.00%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 79.44%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.82%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 78.27%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 77.78%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 77.31%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 77.01%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 76.83%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 76.81%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 76.90%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 76.76%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 76.75%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.73%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.05%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 77.13%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 77.30%   
cur_acc:  ['0.9464', '0.6151']
his_acc:  ['0.9464', '0.7730']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  4.531426906585693 1.758963942527771
CurrentTrain: epoch  0, batch     0 | loss: 6.2903910Losses:  5.590321063995361 1.6657536029815674
CurrentTrain: epoch  0, batch     1 | loss: 7.2560749Losses:  7.7490410804748535 1.8448805809020996
CurrentTrain: epoch  0, batch     2 | loss: 9.5939217Losses:  9.211045265197754 0.5036066770553589
CurrentTrain: epoch  0, batch     3 | loss: 9.7146521Losses:  11.071723937988281 1.5961787700653076
CurrentTrain: epoch  1, batch     0 | loss: 12.6679029Losses:  11.153097152709961 1.6106849908828735
CurrentTrain: epoch  1, batch     1 | loss: 12.7637825Losses:  11.077431678771973 1.470157265663147
CurrentTrain: epoch  1, batch     2 | loss: 12.5475893Losses:  11.226951599121094 0.24086304008960724
CurrentTrain: epoch  1, batch     3 | loss: 11.4678144Losses:  10.767362594604492 1.1150944232940674
CurrentTrain: epoch  2, batch     0 | loss: 11.8824568Losses:  11.163158416748047 1.4268572330474854
CurrentTrain: epoch  2, batch     1 | loss: 12.5900154Losses:  11.191814422607422 1.6221411228179932
CurrentTrain: epoch  2, batch     2 | loss: 12.8139553Losses:  10.231819152832031 0.14167754352092743
CurrentTrain: epoch  2, batch     3 | loss: 10.3734970Losses:  10.878585815429688 1.39143705368042
CurrentTrain: epoch  3, batch     0 | loss: 12.2700233Losses:  10.699577331542969 1.1856917142868042
CurrentTrain: epoch  3, batch     1 | loss: 11.8852692Losses:  10.78006649017334 1.2918202877044678
CurrentTrain: epoch  3, batch     2 | loss: 12.0718870Losses:  11.167856216430664 0.14744044840335846
CurrentTrain: epoch  3, batch     3 | loss: 11.3152971Losses:  10.837743759155273 1.314281702041626
CurrentTrain: epoch  4, batch     0 | loss: 12.1520252Losses:  10.700632095336914 1.3474400043487549
CurrentTrain: epoch  4, batch     1 | loss: 12.0480719Losses:  10.513504981994629 1.009093999862671
CurrentTrain: epoch  4, batch     2 | loss: 11.5225992Losses:  10.480256080627441 0.2295130491256714
CurrentTrain: epoch  4, batch     3 | loss: 10.7097692Losses:  10.852025985717773 1.152202844619751
CurrentTrain: epoch  5, batch     0 | loss: 12.0042286Losses:  10.486515045166016 0.9215071201324463
CurrentTrain: epoch  5, batch     1 | loss: 11.4080219Losses:  10.504949569702148 0.8424099683761597
CurrentTrain: epoch  5, batch     2 | loss: 11.3473597Losses:  10.981860160827637 0.633101224899292
CurrentTrain: epoch  5, batch     3 | loss: 11.6149616Losses:  10.561629295349121 1.2629005908966064
CurrentTrain: epoch  6, batch     0 | loss: 11.8245296Losses:  10.665979385375977 1.1198627948760986
CurrentTrain: epoch  6, batch     1 | loss: 11.7858419Losses:  10.460457801818848 0.901993989944458
CurrentTrain: epoch  6, batch     2 | loss: 11.3624516Losses:  10.793855667114258 0.15181571245193481
CurrentTrain: epoch  6, batch     3 | loss: 10.9456711Losses:  10.505701065063477 0.9482632279396057
CurrentTrain: epoch  7, batch     0 | loss: 11.4539642Losses:  10.50851821899414 0.926907479763031
CurrentTrain: epoch  7, batch     1 | loss: 11.4354258Losses:  10.497846603393555 0.9234675168991089
CurrentTrain: epoch  7, batch     2 | loss: 11.4213142Losses:  11.08275032043457 0.4115593433380127
CurrentTrain: epoch  7, batch     3 | loss: 11.4943094Losses:  10.516633987426758 1.068610668182373
CurrentTrain: epoch  8, batch     0 | loss: 11.5852451Losses:  10.419330596923828 1.013462781906128
CurrentTrain: epoch  8, batch     1 | loss: 11.4327936Losses:  10.36447525024414 0.6771005988121033
CurrentTrain: epoch  8, batch     2 | loss: 11.0415754Losses:  10.238762855529785 0.12034797668457031
CurrentTrain: epoch  8, batch     3 | loss: 10.3591108Losses:  10.465682983398438 0.7386058568954468
CurrentTrain: epoch  9, batch     0 | loss: 11.2042885Losses:  10.255016326904297 0.6070610284805298
CurrentTrain: epoch  9, batch     1 | loss: 10.8620777Losses:  10.372846603393555 0.8672400116920471
CurrentTrain: epoch  9, batch     2 | loss: 11.2400866Losses:  10.898237228393555 0.5189613103866577
CurrentTrain: epoch  9, batch     3 | loss: 11.4171982
Losses:  0.0002902272972278297 0.8648674488067627
MemoryTrain:  epoch  0, batch     0 | loss: 0.8651577Losses:  0.0004879048210568726 0.9769821763038635
MemoryTrain:  epoch  0, batch     1 | loss: 0.9774701Losses:  10.670169830322266 0.884830117225647
MemoryTrain:  epoch  1, batch     0 | loss: 11.5550003Losses:  10.422070503234863 0.7424225211143494
MemoryTrain:  epoch  1, batch     1 | loss: 11.1644926Losses:  10.309083938598633 0.7122364640235901
MemoryTrain:  epoch  2, batch     0 | loss: 11.0213203Losses:  10.687726974487305 0.7931677103042603
MemoryTrain:  epoch  2, batch     1 | loss: 11.4808950Losses:  10.549699783325195 0.7794250845909119
MemoryTrain:  epoch  3, batch     0 | loss: 11.3291245Losses:  10.250541687011719 0.6790527105331421
MemoryTrain:  epoch  3, batch     1 | loss: 10.9295940Losses:  10.791397094726562 0.9095028638839722
MemoryTrain:  epoch  4, batch     0 | loss: 11.7009001Losses:  9.67709732055664 0.5661099553108215
MemoryTrain:  epoch  4, batch     1 | loss: 10.2432070Losses:  10.03909969329834 0.7981557846069336
MemoryTrain:  epoch  5, batch     0 | loss: 10.8372555Losses:  10.355929374694824 0.6984269618988037
MemoryTrain:  epoch  5, batch     1 | loss: 11.0543566Losses:  10.068901062011719 0.7770420908927917
MemoryTrain:  epoch  6, batch     0 | loss: 10.8459435Losses:  10.243010520935059 0.6837015748023987
MemoryTrain:  epoch  6, batch     1 | loss: 10.9267120Losses:  10.007715225219727 0.6834113597869873
MemoryTrain:  epoch  7, batch     0 | loss: 10.6911268Losses:  10.171618461608887 0.8144766688346863
MemoryTrain:  epoch  7, batch     1 | loss: 10.9860954Losses:  10.191640853881836 0.8557050228118896
MemoryTrain:  epoch  8, batch     0 | loss: 11.0473461Losses:  9.831693649291992 0.5408567786216736
MemoryTrain:  epoch  8, batch     1 | loss: 10.3725500Losses:  10.17021369934082 0.8643745183944702
MemoryTrain:  epoch  9, batch     0 | loss: 11.0345879Losses:  9.801615715026855 0.5614573955535889
MemoryTrain:  epoch  9, batch     1 | loss: 10.3630733
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 26.56%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.26%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 57.37%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 56.03%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 54.17%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 52.62%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 55.33%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 57.12%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 58.11%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 59.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.92%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 64.40%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 64.45%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 64.41%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 64.46%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 63.66%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 62.95%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 63.47%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 63.88%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 64.42%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 63.99%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.49%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.42%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 92.98%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.27%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 92.08%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 91.60%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.37%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.31%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.29%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.36%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 90.71%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 90.32%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 89.76%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 89.04%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 88.51%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 87.92%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 87.99%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 87.82%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.74%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 87.66%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.58%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 87.20%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 86.75%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 86.18%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.12%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 85.78%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 85.51%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 85.11%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 84.51%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 84.07%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 83.76%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 83.06%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 82.38%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 81.84%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 81.45%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 81.12%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 80.68%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 80.43%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 79.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 79.58%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 79.35%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 79.07%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 78.85%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 78.63%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 78.42%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.15%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.55%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 76.95%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 76.53%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 76.07%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 75.78%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 75.66%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 75.82%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 76.58%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 76.76%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 77.22%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 76.88%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 76.07%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 75.73%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 75.43%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 75.24%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 75.38%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.61%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 75.58%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 75.22%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 74.82%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 74.60%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 74.39%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 74.09%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 74.59%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 74.10%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.65%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 73.30%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 72.82%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 72.40%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.60%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 73.74%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 73.76%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 73.67%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 73.71%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 73.58%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 73.41%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 73.24%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 73.18%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:  180 | acc: 25.00%,  total acc: 72.86%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 72.91%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 72.97%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 73.02%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 73.03%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 72.84%   
cur_acc:  ['0.9464', '0.6151', '0.6399']
his_acc:  ['0.9464', '0.7730', '0.7284']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  2.9125821590423584 1.6546649932861328
CurrentTrain: epoch  0, batch     0 | loss: 4.5672474Losses:  4.85359525680542 1.573678970336914
CurrentTrain: epoch  0, batch     1 | loss: 6.4272742Losses:  6.38400936126709 1.4585294723510742
CurrentTrain: epoch  0, batch     2 | loss: 7.8425388Losses:  9.210315704345703 0.21393878757953644
CurrentTrain: epoch  0, batch     3 | loss: 9.4242544Losses:  10.832837104797363 1.4518176317214966
CurrentTrain: epoch  1, batch     0 | loss: 12.2846546Losses:  11.125699043273926 1.168990135192871
CurrentTrain: epoch  1, batch     1 | loss: 12.2946892Losses:  10.913697242736816 1.5811443328857422
CurrentTrain: epoch  1, batch     2 | loss: 12.4948416Losses:  11.094186782836914 0.12884056568145752
CurrentTrain: epoch  1, batch     3 | loss: 11.2230272Losses:  11.10724925994873 1.3560101985931396
CurrentTrain: epoch  2, batch     0 | loss: 12.4632597Losses:  10.747685432434082 1.2788249254226685
CurrentTrain: epoch  2, batch     1 | loss: 12.0265102Losses:  10.40892219543457 0.9945782423019409
CurrentTrain: epoch  2, batch     2 | loss: 11.4035006Losses:  10.38393783569336 0.1301165521144867
CurrentTrain: epoch  2, batch     3 | loss: 10.5140543Losses:  10.530366897583008 0.951757550239563
CurrentTrain: epoch  3, batch     0 | loss: 11.4821243Losses:  10.568927764892578 1.3989982604980469
CurrentTrain: epoch  3, batch     1 | loss: 11.9679260Losses:  10.615781784057617 1.0963795185089111
CurrentTrain: epoch  3, batch     2 | loss: 11.7121611Losses:  9.026957511901855 0.17502237856388092
CurrentTrain: epoch  3, batch     3 | loss: 9.2019796Losses:  10.218210220336914 0.9863110780715942
CurrentTrain: epoch  4, batch     0 | loss: 11.2045212Losses:  10.441150665283203 0.9655921459197998
CurrentTrain: epoch  4, batch     1 | loss: 11.4067430Losses:  10.35200309753418 0.7658325433731079
CurrentTrain: epoch  4, batch     2 | loss: 11.1178360Losses:  10.098913192749023 0.11621727794408798
CurrentTrain: epoch  4, batch     3 | loss: 10.2151308Losses:  10.319978713989258 1.0123034715652466
CurrentTrain: epoch  5, batch     0 | loss: 11.3322821Losses:  10.165237426757812 1.0335643291473389
CurrentTrain: epoch  5, batch     1 | loss: 11.1988020Losses:  10.089455604553223 1.0881143808364868
CurrentTrain: epoch  5, batch     2 | loss: 11.1775703Losses:  9.869577407836914 5.960464477539063e-08
CurrentTrain: epoch  5, batch     3 | loss: 9.8695774Losses:  9.761176109313965 0.7750844359397888
CurrentTrain: epoch  6, batch     0 | loss: 10.5362606Losses:  10.194477081298828 0.8120246529579163
CurrentTrain: epoch  6, batch     1 | loss: 11.0065022Losses:  10.358288764953613 0.9939307570457458
CurrentTrain: epoch  6, batch     2 | loss: 11.3522196Losses:  9.850808143615723 0.28504040837287903
CurrentTrain: epoch  6, batch     3 | loss: 10.1358490Losses:  9.74641227722168 0.6945323348045349
CurrentTrain: epoch  7, batch     0 | loss: 10.4409447Losses:  10.00574016571045 1.0124340057373047
CurrentTrain: epoch  7, batch     1 | loss: 11.0181742Losses:  10.187199592590332 0.8866555094718933
CurrentTrain: epoch  7, batch     2 | loss: 11.0738554Losses:  10.226987838745117 0.41071251034736633
CurrentTrain: epoch  7, batch     3 | loss: 10.6377001Losses:  10.14310073852539 0.7458944916725159
CurrentTrain: epoch  8, batch     0 | loss: 10.8889952Losses:  9.705917358398438 0.7412408590316772
CurrentTrain: epoch  8, batch     1 | loss: 10.4471579Losses:  9.733141899108887 0.7442085146903992
CurrentTrain: epoch  8, batch     2 | loss: 10.4773502Losses:  10.371338844299316 0.16649630665779114
CurrentTrain: epoch  8, batch     3 | loss: 10.5378351Losses:  9.962705612182617 0.7021812200546265
CurrentTrain: epoch  9, batch     0 | loss: 10.6648865Losses:  9.895951271057129 0.7326916456222534
CurrentTrain: epoch  9, batch     1 | loss: 10.6286430Losses:  9.516910552978516 0.6792287826538086
CurrentTrain: epoch  9, batch     2 | loss: 10.1961393Losses:  10.14973258972168 0.31133800745010376
CurrentTrain: epoch  9, batch     3 | loss: 10.4610710
Losses:  0.0003902630414813757 0.97804856300354
MemoryTrain:  epoch  0, batch     0 | loss: 0.9784389Losses:  0.0005343304947018623 0.6742388010025024
MemoryTrain:  epoch  0, batch     1 | loss: 0.6747732Losses:  0.0009595982264727354 0.4594815969467163
MemoryTrain:  epoch  0, batch     2 | loss: 0.4604412Losses:  10.42214584350586 0.6967434883117676
MemoryTrain:  epoch  1, batch     0 | loss: 11.1188889Losses:  10.791664123535156 0.6300752758979797
MemoryTrain:  epoch  1, batch     1 | loss: 11.4217396Losses:  11.047874450683594 0.6320475339889526
MemoryTrain:  epoch  1, batch     2 | loss: 11.6799221Losses:  10.717193603515625 0.8150802254676819
MemoryTrain:  epoch  2, batch     0 | loss: 11.5322742Losses:  10.203407287597656 0.6219536066055298
MemoryTrain:  epoch  2, batch     1 | loss: 10.8253613Losses:  11.41965389251709 0.41093289852142334
MemoryTrain:  epoch  2, batch     2 | loss: 11.8305864Losses:  10.683124542236328 0.8242246508598328
MemoryTrain:  epoch  3, batch     0 | loss: 11.5073490Losses:  10.361627578735352 0.5043344497680664
MemoryTrain:  epoch  3, batch     1 | loss: 10.8659620Losses:  10.65188217163086 0.43164917826652527
MemoryTrain:  epoch  3, batch     2 | loss: 11.0835314Losses:  10.468454360961914 0.7541995048522949
MemoryTrain:  epoch  4, batch     0 | loss: 11.2226543Losses:  10.453853607177734 0.5626640915870667
MemoryTrain:  epoch  4, batch     1 | loss: 11.0165176Losses:  10.391892433166504 0.4238411784172058
MemoryTrain:  epoch  4, batch     2 | loss: 10.8157339Losses:  10.165987014770508 0.7080004215240479
MemoryTrain:  epoch  5, batch     0 | loss: 10.8739872Losses:  10.772032737731934 0.9059388637542725
MemoryTrain:  epoch  5, batch     1 | loss: 11.6779718Losses:  9.923622131347656 0.31204932928085327
MemoryTrain:  epoch  5, batch     2 | loss: 10.2356710Losses:  10.281554222106934 0.5952425003051758
MemoryTrain:  epoch  6, batch     0 | loss: 10.8767967Losses:  10.213061332702637 0.6639953851699829
MemoryTrain:  epoch  6, batch     1 | loss: 10.8770571Losses:  10.777148246765137 0.4912421703338623
MemoryTrain:  epoch  6, batch     2 | loss: 11.2683907Losses:  10.275562286376953 0.5500761270523071
MemoryTrain:  epoch  7, batch     0 | loss: 10.8256388Losses:  10.212203979492188 0.5914607048034668
MemoryTrain:  epoch  7, batch     1 | loss: 10.8036652Losses:  10.56008243560791 0.5134252309799194
MemoryTrain:  epoch  7, batch     2 | loss: 11.0735073Losses:  9.973677635192871 0.5259578227996826
MemoryTrain:  epoch  8, batch     0 | loss: 10.4996357Losses:  10.358985900878906 0.7023297548294067
MemoryTrain:  epoch  8, batch     1 | loss: 11.0613155Losses:  10.770284652709961 0.4030182361602783
MemoryTrain:  epoch  8, batch     2 | loss: 11.1733027Losses:  10.419424057006836 0.7437021136283875
MemoryTrain:  epoch  9, batch     0 | loss: 11.1631260Losses:  10.331789016723633 0.5612173080444336
MemoryTrain:  epoch  9, batch     1 | loss: 10.8930063Losses:  9.830526351928711 0.26826512813568115
MemoryTrain:  epoch  9, batch     2 | loss: 10.0987911
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  6.214608192443848 1.8446838855743408
CurrentTrain: epoch  0, batch     0 | loss: 8.0592918Losses:  11.215117454528809 2.1758346557617188
CurrentTrain: epoch  0, batch     1 | loss: 13.3909521Losses:  11.676534652709961 1.6950421333312988
CurrentTrain: epoch  0, batch     2 | loss: 13.3715763Losses:  11.042564392089844 1.710303783416748
CurrentTrain: epoch  0, batch     3 | loss: 12.7528687Losses:  11.495859146118164 1.609061598777771
CurrentTrain: epoch  0, batch     4 | loss: 13.1049204Losses:  11.48792839050293 1.4972673654556274
CurrentTrain: epoch  0, batch     5 | loss: 12.9851961Losses:  11.45262336730957 1.7367912530899048
CurrentTrain: epoch  0, batch     6 | loss: 13.1894150Losses:  11.230013847351074 1.5510915517807007
CurrentTrain: epoch  0, batch     7 | loss: 12.7811050Losses:  11.27529525756836 1.5048142671585083
CurrentTrain: epoch  0, batch     8 | loss: 12.7801094Losses:  11.12397575378418 1.8327276706695557
CurrentTrain: epoch  0, batch     9 | loss: 12.9567032Losses:  11.263895034790039 1.5483571290969849
CurrentTrain: epoch  0, batch    10 | loss: 12.8122520Losses:  10.973739624023438 1.4172019958496094
CurrentTrain: epoch  0, batch    11 | loss: 12.3909416Losses:  10.79800033569336 1.67659330368042
CurrentTrain: epoch  0, batch    12 | loss: 12.4745941Losses:  10.664375305175781 1.751054048538208
CurrentTrain: epoch  0, batch    13 | loss: 12.4154291Losses:  10.668012619018555 1.997913122177124
CurrentTrain: epoch  0, batch    14 | loss: 12.6659260Losses:  10.587950706481934 2.8032944202423096
CurrentTrain: epoch  0, batch    15 | loss: 13.3912449Losses:  10.322717666625977 1.7506687641143799
CurrentTrain: epoch  0, batch    16 | loss: 12.0733862Losses:  10.362192153930664 2.5906717777252197
CurrentTrain: epoch  0, batch    17 | loss: 12.9528637Losses:  10.151189804077148 2.4692635536193848
CurrentTrain: epoch  0, batch    18 | loss: 12.6204529Losses:  9.805832862854004 2.515207290649414
CurrentTrain: epoch  0, batch    19 | loss: 12.3210402Losses:  9.922148704528809 2.1511216163635254
CurrentTrain: epoch  0, batch    20 | loss: 12.0732708Losses:  9.650825500488281 2.290968894958496
CurrentTrain: epoch  0, batch    21 | loss: 11.9417944Losses:  9.565942764282227 2.1269736289978027
CurrentTrain: epoch  0, batch    22 | loss: 11.6929169Losses:  9.265782356262207 1.6055119037628174
CurrentTrain: epoch  0, batch    23 | loss: 10.8712940Losses:  9.450733184814453 1.6861494779586792
CurrentTrain: epoch  0, batch    24 | loss: 11.1368828Losses:  9.064945220947266 1.4798532724380493
CurrentTrain: epoch  0, batch    25 | loss: 10.5447989Losses:  9.345914840698242 1.3927321434020996
CurrentTrain: epoch  0, batch    26 | loss: 10.7386475Losses:  8.82319450378418 1.6005181074142456
CurrentTrain: epoch  0, batch    27 | loss: 10.4237127Losses:  8.869789123535156 1.424161434173584
CurrentTrain: epoch  0, batch    28 | loss: 10.2939510Losses:  8.552963256835938 1.5506538152694702
CurrentTrain: epoch  0, batch    29 | loss: 10.1036167Losses:  8.37702751159668 1.678067922592163
CurrentTrain: epoch  0, batch    30 | loss: 10.0550957Losses:  8.352398872375488 1.301490306854248
CurrentTrain: epoch  0, batch    31 | loss: 9.6538887Losses:  8.336694717407227 1.1130588054656982
CurrentTrain: epoch  0, batch    32 | loss: 9.4497538Losses:  8.500597953796387 1.1179661750793457
CurrentTrain: epoch  0, batch    33 | loss: 9.6185646Losses:  8.17403793334961 1.2854321002960205
CurrentTrain: epoch  0, batch    34 | loss: 9.4594698Losses:  7.7198333740234375 1.38919997215271
CurrentTrain: epoch  0, batch    35 | loss: 9.1090336Losses:  8.184038162231445 1.4607263803482056
CurrentTrain: epoch  0, batch    36 | loss: 9.6447649Losses:  7.631904602050781 1.495858907699585
CurrentTrain: epoch  0, batch    37 | loss: 9.1277637Losses:  7.791146278381348 1.686633586883545
CurrentTrain: epoch  0, batch    38 | loss: 9.4777794Losses:  7.651721954345703 1.510516881942749
CurrentTrain: epoch  0, batch    39 | loss: 9.1622391Losses:  7.328249931335449 1.534253478050232
CurrentTrain: epoch  0, batch    40 | loss: 8.8625031Losses:  7.334607124328613 1.4544833898544312
CurrentTrain: epoch  0, batch    41 | loss: 8.7890902Losses:  7.246387481689453 1.405519723892212
CurrentTrain: epoch  0, batch    42 | loss: 8.6519070Losses:  7.218996524810791 1.1514954566955566
CurrentTrain: epoch  0, batch    43 | loss: 8.3704920Losses:  6.751082897186279 1.3863940238952637
CurrentTrain: epoch  0, batch    44 | loss: 8.1374769Losses:  6.542166233062744 1.3893460035324097
CurrentTrain: epoch  0, batch    45 | loss: 7.9315124Losses:  6.818254470825195 1.0840678215026855
CurrentTrain: epoch  0, batch    46 | loss: 7.9023223Losses:  6.456875801086426 1.481053113937378
CurrentTrain: epoch  0, batch    47 | loss: 7.9379292Losses:  6.6519622802734375 1.562086820602417
CurrentTrain: epoch  0, batch    48 | loss: 8.2140493Losses:  6.127743721008301 1.3660035133361816
CurrentTrain: epoch  0, batch    49 | loss: 7.4937472Losses:  6.017632961273193 1.4916205406188965
CurrentTrain: epoch  0, batch    50 | loss: 7.5092535Losses:  5.98512077331543 1.4738538265228271
CurrentTrain: epoch  0, batch    51 | loss: 7.4589748Losses:  5.898965835571289 1.2436259984970093
CurrentTrain: epoch  0, batch    52 | loss: 7.1425920Losses:  5.899904727935791 1.4309123754501343
CurrentTrain: epoch  0, batch    53 | loss: 7.3308172Losses:  5.589651107788086 1.6772470474243164
CurrentTrain: epoch  0, batch    54 | loss: 7.2668982Losses:  5.486428260803223 1.2994911670684814
CurrentTrain: epoch  0, batch    55 | loss: 6.7859192Losses:  5.3576979637146 1.0777583122253418
CurrentTrain: epoch  0, batch    56 | loss: 6.4354563Losses:  5.190187454223633 1.484283447265625
CurrentTrain: epoch  0, batch    57 | loss: 6.6744709Losses:  5.077603340148926 1.1217491626739502
CurrentTrain: epoch  0, batch    58 | loss: 6.1993523Losses:  5.316695213317871 1.1465144157409668
CurrentTrain: epoch  0, batch    59 | loss: 6.4632096Losses:  4.804081916809082 1.2620328664779663
CurrentTrain: epoch  0, batch    60 | loss: 6.0661149Losses:  4.697379112243652 1.1184661388397217
CurrentTrain: epoch  0, batch    61 | loss: 5.8158455Losses:  4.643983840942383 0.9943474531173706
CurrentTrain: epoch  0, batch    62 | loss: 5.6383314Losses:  4.694061279296875 1.1605095863342285
CurrentTrain: epoch  1, batch     0 | loss: 5.8545709Losses:  4.85380220413208 1.1086111068725586
CurrentTrain: epoch  1, batch     1 | loss: 5.9624133Losses:  4.763899803161621 1.1996351480484009
CurrentTrain: epoch  1, batch     2 | loss: 5.9635348Losses:  4.803532600402832 1.1822164058685303
CurrentTrain: epoch  1, batch     3 | loss: 5.9857492Losses:  4.668468475341797 1.2863644361495972
CurrentTrain: epoch  1, batch     4 | loss: 5.9548330Losses:  4.537850856781006 1.0324280261993408
CurrentTrain: epoch  1, batch     5 | loss: 5.5702791Losses:  4.719465732574463 1.079285740852356
CurrentTrain: epoch  1, batch     6 | loss: 5.7987514Losses:  4.544788360595703 1.121030330657959
CurrentTrain: epoch  1, batch     7 | loss: 5.6658187Losses:  4.613433361053467 0.9449834227561951
CurrentTrain: epoch  1, batch     8 | loss: 5.5584168Losses:  4.621891975402832 1.3428484201431274
CurrentTrain: epoch  1, batch     9 | loss: 5.9647403Losses:  4.634271144866943 1.1364270448684692
CurrentTrain: epoch  1, batch    10 | loss: 5.7706981Losses:  4.52733850479126 1.3077348470687866
CurrentTrain: epoch  1, batch    11 | loss: 5.8350735Losses:  4.721596717834473 1.2117750644683838
CurrentTrain: epoch  1, batch    12 | loss: 5.9333715Losses:  4.529679775238037 0.876391589641571
CurrentTrain: epoch  1, batch    13 | loss: 5.4060712Losses:  4.569828033447266 1.1645381450653076
CurrentTrain: epoch  1, batch    14 | loss: 5.7343664Losses:  4.581235885620117 1.1831820011138916
CurrentTrain: epoch  1, batch    15 | loss: 5.7644176Losses:  4.605044364929199 1.0431013107299805
CurrentTrain: epoch  1, batch    16 | loss: 5.6481457Losses:  4.364677429199219 0.7978307008743286
CurrentTrain: epoch  1, batch    17 | loss: 5.1625080Losses:  4.617099761962891 0.8677813410758972
CurrentTrain: epoch  1, batch    18 | loss: 5.4848809Losses:  4.5906782150268555 0.9514762163162231
CurrentTrain: epoch  1, batch    19 | loss: 5.5421543Losses:  4.573552131652832 1.0227644443511963
CurrentTrain: epoch  1, batch    20 | loss: 5.5963163Losses:  4.508702278137207 1.1847867965698242
CurrentTrain: epoch  1, batch    21 | loss: 5.6934891Losses:  4.516043663024902 1.1044710874557495
CurrentTrain: epoch  1, batch    22 | loss: 5.6205149Losses:  4.496121406555176 0.8049365282058716
CurrentTrain: epoch  1, batch    23 | loss: 5.3010578Losses:  4.340226173400879 1.0300006866455078
CurrentTrain: epoch  1, batch    24 | loss: 5.3702269Losses:  4.555785655975342 0.9094364643096924
CurrentTrain: epoch  1, batch    25 | loss: 5.4652224Losses:  4.521846294403076 1.1089997291564941
CurrentTrain: epoch  1, batch    26 | loss: 5.6308460Losses:  4.536928176879883 1.0164161920547485
CurrentTrain: epoch  1, batch    27 | loss: 5.5533442Losses:  4.520142555236816 0.8822497129440308
CurrentTrain: epoch  1, batch    28 | loss: 5.4023924Losses:  4.548892974853516 1.0261218547821045
CurrentTrain: epoch  1, batch    29 | loss: 5.5750151Losses:  4.574267387390137 0.8390871286392212
CurrentTrain: epoch  1, batch    30 | loss: 5.4133544Losses:  4.423707008361816 0.9968972206115723
CurrentTrain: epoch  1, batch    31 | loss: 5.4206042Losses:  4.514103412628174 1.0161774158477783
CurrentTrain: epoch  1, batch    32 | loss: 5.5302811Losses:  4.436993598937988 0.6612517833709717
CurrentTrain: epoch  1, batch    33 | loss: 5.0982456Losses:  4.429459571838379 0.9986311197280884
CurrentTrain: epoch  1, batch    34 | loss: 5.4280906Losses:  4.3925323486328125 0.9540356397628784
CurrentTrain: epoch  1, batch    35 | loss: 5.3465681Losses:  4.464969158172607 0.7473950386047363
CurrentTrain: epoch  1, batch    36 | loss: 5.2123642Losses:  4.519428730010986 1.0033996105194092
CurrentTrain: epoch  1, batch    37 | loss: 5.5228281Losses:  4.550663948059082 1.0427069664001465
CurrentTrain: epoch  1, batch    38 | loss: 5.5933709Losses:  4.391795635223389 0.8876684904098511
CurrentTrain: epoch  1, batch    39 | loss: 5.2794642Losses:  4.566521644592285 0.7317448258399963
CurrentTrain: epoch  1, batch    40 | loss: 5.2982664Losses:  4.383355140686035 1.1603293418884277
CurrentTrain: epoch  1, batch    41 | loss: 5.5436845Losses:  4.365000247955322 0.9287776350975037
CurrentTrain: epoch  1, batch    42 | loss: 5.2937779Losses:  4.500399589538574 1.0090641975402832
CurrentTrain: epoch  1, batch    43 | loss: 5.5094638Losses:  4.513136863708496 0.7660052180290222
CurrentTrain: epoch  1, batch    44 | loss: 5.2791419Losses:  4.493668556213379 0.7345266342163086
CurrentTrain: epoch  1, batch    45 | loss: 5.2281952Losses:  4.415566921234131 0.8330200910568237
CurrentTrain: epoch  1, batch    46 | loss: 5.2485871Losses:  4.601923942565918 0.9038881063461304
CurrentTrain: epoch  1, batch    47 | loss: 5.5058122Losses:  4.439725399017334 0.8740290403366089
CurrentTrain: epoch  1, batch    48 | loss: 5.3137546Losses:  4.501260757446289 0.7689225673675537
CurrentTrain: epoch  1, batch    49 | loss: 5.2701836Losses:  4.512564659118652 0.6993860602378845
CurrentTrain: epoch  1, batch    50 | loss: 5.2119508Losses:  4.538054466247559 0.9394869208335876
CurrentTrain: epoch  1, batch    51 | loss: 5.4775414Losses:  4.415575981140137 0.7018742561340332
CurrentTrain: epoch  1, batch    52 | loss: 5.1174502Losses:  4.434134006500244 0.641622006893158
CurrentTrain: epoch  1, batch    53 | loss: 5.0757561Losses:  4.4964599609375 0.8444238305091858
CurrentTrain: epoch  1, batch    54 | loss: 5.3408837Losses:  4.420144081115723 0.43146443367004395
CurrentTrain: epoch  1, batch    55 | loss: 4.8516083Losses:  4.486155986785889 0.5423341393470764
CurrentTrain: epoch  1, batch    56 | loss: 5.0284901Losses:  4.386307239532471 0.7810680866241455
CurrentTrain: epoch  1, batch    57 | loss: 5.1673756Losses:  4.450344085693359 0.6328476667404175
CurrentTrain: epoch  1, batch    58 | loss: 5.0831919Losses:  4.391543388366699 0.9553222060203552
CurrentTrain: epoch  1, batch    59 | loss: 5.3468657Losses:  4.3378448486328125 0.8217480182647705
CurrentTrain: epoch  1, batch    60 | loss: 5.1595926Losses:  4.3668012619018555 0.8981415629386902
CurrentTrain: epoch  1, batch    61 | loss: 5.2649426Losses:  4.435340404510498 0.25680533051490784
CurrentTrain: epoch  1, batch    62 | loss: 4.6921458Losses:  4.334591865539551 0.6498053073883057
CurrentTrain: epoch  2, batch     0 | loss: 4.9843969Losses:  4.413956642150879 0.6418801546096802
CurrentTrain: epoch  2, batch     1 | loss: 5.0558367Losses:  4.3695526123046875 0.77115797996521
CurrentTrain: epoch  2, batch     2 | loss: 5.1407108Losses:  4.363973617553711 0.7137739062309265
CurrentTrain: epoch  2, batch     3 | loss: 5.0777473Losses:  4.36490535736084 0.6165615320205688
CurrentTrain: epoch  2, batch     4 | loss: 4.9814668Losses:  4.331867694854736 0.8180104494094849
CurrentTrain: epoch  2, batch     5 | loss: 5.1498780Losses:  4.361011505126953 0.502644956111908
CurrentTrain: epoch  2, batch     6 | loss: 4.8636565Losses:  4.5390214920043945 0.6891862750053406
CurrentTrain: epoch  2, batch     7 | loss: 5.2282076Losses:  4.465296268463135 0.5866773724555969
CurrentTrain: epoch  2, batch     8 | loss: 5.0519738Losses:  4.3541412353515625 0.4530600905418396
CurrentTrain: epoch  2, batch     9 | loss: 4.8072014Losses:  4.371213912963867 0.6853461265563965
CurrentTrain: epoch  2, batch    10 | loss: 5.0565600Losses:  4.329119682312012 0.7144200801849365
CurrentTrain: epoch  2, batch    11 | loss: 5.0435400Losses:  4.311850070953369 0.6752280592918396
CurrentTrain: epoch  2, batch    12 | loss: 4.9870782Losses:  4.459799766540527 0.5858292579650879
CurrentTrain: epoch  2, batch    13 | loss: 5.0456290Losses:  4.44092321395874 0.6661834716796875
CurrentTrain: epoch  2, batch    14 | loss: 5.1071067Losses:  4.476197242736816 0.6257725954055786
CurrentTrain: epoch  2, batch    15 | loss: 5.1019697Losses:  4.355657577514648 0.4813401699066162
CurrentTrain: epoch  2, batch    16 | loss: 4.8369980Losses:  4.276015758514404 0.4560273289680481
CurrentTrain: epoch  2, batch    17 | loss: 4.7320433Losses:  4.567866325378418 0.6062798500061035
CurrentTrain: epoch  2, batch    18 | loss: 5.1741462Losses:  4.365642070770264 0.5157703161239624
CurrentTrain: epoch  2, batch    19 | loss: 4.8814125Losses:  4.368875026702881 0.5457274913787842
CurrentTrain: epoch  2, batch    20 | loss: 4.9146023Losses:  4.352613925933838 0.4929264187812805
CurrentTrain: epoch  2, batch    21 | loss: 4.8455405Losses:  4.2385759353637695 0.5848561525344849
CurrentTrain: epoch  2, batch    22 | loss: 4.8234320Losses:  4.490413188934326 0.6656492948532104
CurrentTrain: epoch  2, batch    23 | loss: 5.1560626Losses:  4.297635078430176 0.6790345907211304
CurrentTrain: epoch  2, batch    24 | loss: 4.9766698Losses:  4.34584903717041 0.5531337261199951
CurrentTrain: epoch  2, batch    25 | loss: 4.8989830Losses:  4.381058692932129 0.4967864751815796
CurrentTrain: epoch  2, batch    26 | loss: 4.8778453Losses:  4.358652591705322 0.4987383484840393
CurrentTrain: epoch  2, batch    27 | loss: 4.8573909Losses:  4.3060407638549805 0.49130305647850037
CurrentTrain: epoch  2, batch    28 | loss: 4.7973437Losses:  4.391416549682617 0.5172616243362427
CurrentTrain: epoch  2, batch    29 | loss: 4.9086781Losses:  4.330557823181152 0.7382713556289673
CurrentTrain: epoch  2, batch    30 | loss: 5.0688291Losses:  4.334414482116699 0.6198321580886841
CurrentTrain: epoch  2, batch    31 | loss: 4.9542465Losses:  4.38109827041626 0.5479816198348999
CurrentTrain: epoch  2, batch    32 | loss: 4.9290800Losses:  4.248025894165039 0.5648275017738342
CurrentTrain: epoch  2, batch    33 | loss: 4.8128533Losses:  4.373239040374756 0.5657392144203186
CurrentTrain: epoch  2, batch    34 | loss: 4.9389782Losses:  4.367419719696045 0.48914292454719543
CurrentTrain: epoch  2, batch    35 | loss: 4.8565626Losses:  4.355510234832764 0.4212552011013031
CurrentTrain: epoch  2, batch    36 | loss: 4.7767653Losses:  4.321847915649414 0.5121359825134277
CurrentTrain: epoch  2, batch    37 | loss: 4.8339839Losses:  4.357619762420654 0.4551660120487213
CurrentTrain: epoch  2, batch    38 | loss: 4.8127856Losses:  4.283810615539551 0.4200423061847687
CurrentTrain: epoch  2, batch    39 | loss: 4.7038531Losses:  4.20984411239624 0.5090774297714233
CurrentTrain: epoch  2, batch    40 | loss: 4.7189217Losses:  4.3382568359375 0.465410053730011
CurrentTrain: epoch  2, batch    41 | loss: 4.8036671Losses:  4.395263195037842 0.44697150588035583
CurrentTrain: epoch  2, batch    42 | loss: 4.8422346Losses:  4.351322650909424 0.5601018071174622
CurrentTrain: epoch  2, batch    43 | loss: 4.9114246Losses:  4.283281326293945 0.6294673681259155
CurrentTrain: epoch  2, batch    44 | loss: 4.9127488Losses:  4.30894136428833 0.4451153874397278
CurrentTrain: epoch  2, batch    45 | loss: 4.7540569Losses:  4.319182395935059 0.3830966055393219
CurrentTrain: epoch  2, batch    46 | loss: 4.7022791Losses:  4.293135643005371 0.5517377853393555
CurrentTrain: epoch  2, batch    47 | loss: 4.8448734Losses:  4.2544450759887695 0.5029456615447998
CurrentTrain: epoch  2, batch    48 | loss: 4.7573910Losses:  4.270761489868164 0.5282484292984009
CurrentTrain: epoch  2, batch    49 | loss: 4.7990098Losses:  4.29325008392334 0.3221585154533386
CurrentTrain: epoch  2, batch    50 | loss: 4.6154084Losses:  4.251996994018555 0.45012950897216797
CurrentTrain: epoch  2, batch    51 | loss: 4.7021265Losses:  4.2274675369262695 0.4282453656196594
CurrentTrain: epoch  2, batch    52 | loss: 4.6557131Losses:  4.23265266418457 0.43789181113243103
CurrentTrain: epoch  2, batch    53 | loss: 4.6705446Losses:  4.19724178314209 0.4941426217556
CurrentTrain: epoch  2, batch    54 | loss: 4.6913843Losses:  4.222978115081787 0.46205759048461914
CurrentTrain: epoch  2, batch    55 | loss: 4.6850357Losses:  4.193793296813965 0.6235839128494263
CurrentTrain: epoch  2, batch    56 | loss: 4.8173771Losses:  4.2834858894348145 0.5079659819602966
CurrentTrain: epoch  2, batch    57 | loss: 4.7914519Losses:  4.18660306930542 0.41478726267814636
CurrentTrain: epoch  2, batch    58 | loss: 4.6013904Losses:  4.220070838928223 0.4017335772514343
CurrentTrain: epoch  2, batch    59 | loss: 4.6218042Losses:  4.154184341430664 0.4955158233642578
CurrentTrain: epoch  2, batch    60 | loss: 4.6497002Losses:  4.125024318695068 0.5251791477203369
CurrentTrain: epoch  2, batch    61 | loss: 4.6502037Losses:  4.187336444854736 0.16659530997276306
CurrentTrain: epoch  2, batch    62 | loss: 4.3539319Losses:  4.181432723999023 0.4436871111392975
CurrentTrain: epoch  3, batch     0 | loss: 4.6251197Losses:  4.2504425048828125 0.5930346846580505
CurrentTrain: epoch  3, batch     1 | loss: 4.8434772Losses:  4.126890182495117 0.47493621706962585
CurrentTrain: epoch  3, batch     2 | loss: 4.6018262Losses:  4.160386562347412 0.36067211627960205
CurrentTrain: epoch  3, batch     3 | loss: 4.5210586Losses:  4.1733927726745605 0.3579900860786438
CurrentTrain: epoch  3, batch     4 | loss: 4.5313830Losses:  4.175287246704102 0.3576580286026001
CurrentTrain: epoch  3, batch     5 | loss: 4.5329452Losses:  4.166906356811523 0.43280190229415894
CurrentTrain: epoch  3, batch     6 | loss: 4.5997081Losses:  4.205366134643555 0.3542149066925049
CurrentTrain: epoch  3, batch     7 | loss: 4.5595808Losses:  4.1539506912231445 0.5004768371582031
CurrentTrain: epoch  3, batch     8 | loss: 4.6544275Losses:  4.184629440307617 0.46734631061553955
CurrentTrain: epoch  3, batch     9 | loss: 4.6519756Losses:  4.16216516494751 0.4442346394062042
CurrentTrain: epoch  3, batch    10 | loss: 4.6064000Losses:  4.159361839294434 0.43975144624710083
CurrentTrain: epoch  3, batch    11 | loss: 4.5991135Losses:  4.142540454864502 0.2721933126449585
CurrentTrain: epoch  3, batch    12 | loss: 4.4147339Losses:  4.252443313598633 0.5033496618270874
CurrentTrain: epoch  3, batch    13 | loss: 4.7557931Losses:  4.1622467041015625 0.4651094973087311
CurrentTrain: epoch  3, batch    14 | loss: 4.6273561Losses:  4.1374077796936035 0.40282660722732544
CurrentTrain: epoch  3, batch    15 | loss: 4.5402346Losses:  4.1425347328186035 0.35892337560653687
CurrentTrain: epoch  3, batch    16 | loss: 4.5014582Losses:  4.223814010620117 0.39386749267578125
CurrentTrain: epoch  3, batch    17 | loss: 4.6176815Losses:  4.121596813201904 0.3318924903869629
CurrentTrain: epoch  3, batch    18 | loss: 4.4534893Losses:  4.151618957519531 0.4765039086341858
CurrentTrain: epoch  3, batch    19 | loss: 4.6281228Losses:  4.19059944152832 0.329379141330719
CurrentTrain: epoch  3, batch    20 | loss: 4.5199785Losses:  4.183320045471191 0.36909836530685425
CurrentTrain: epoch  3, batch    21 | loss: 4.5524182Losses:  4.130041122436523 0.3466702699661255
CurrentTrain: epoch  3, batch    22 | loss: 4.4767113Losses:  4.147910118103027 0.40545010566711426
CurrentTrain: epoch  3, batch    23 | loss: 4.5533600Losses:  4.163311958312988 0.21408195793628693
CurrentTrain: epoch  3, batch    24 | loss: 4.3773937Losses:  4.1702046394348145 0.3198888301849365
CurrentTrain: epoch  3, batch    25 | loss: 4.4900932Losses:  4.17000150680542 0.3707292675971985
CurrentTrain: epoch  3, batch    26 | loss: 4.5407310Losses:  4.194201469421387 0.3265969753265381
CurrentTrain: epoch  3, batch    27 | loss: 4.5207987Losses:  4.209418296813965 0.46076083183288574
CurrentTrain: epoch  3, batch    28 | loss: 4.6701794Losses:  4.198779106140137 0.41314029693603516
CurrentTrain: epoch  3, batch    29 | loss: 4.6119194Losses:  4.192568302154541 0.37684720754623413
CurrentTrain: epoch  3, batch    30 | loss: 4.5694156Losses:  4.118427276611328 0.37606289982795715
CurrentTrain: epoch  3, batch    31 | loss: 4.4944901Losses:  4.15771484375 0.30317607522010803
CurrentTrain: epoch  3, batch    32 | loss: 4.4608908Losses:  4.164721965789795 0.31832411885261536
CurrentTrain: epoch  3, batch    33 | loss: 4.4830461Losses:  4.143902778625488 0.3418273329734802
CurrentTrain: epoch  3, batch    34 | loss: 4.4857302Losses:  4.09088659286499 0.3620779514312744
CurrentTrain: epoch  3, batch    35 | loss: 4.4529648Losses:  4.139126300811768 0.5541774034500122
CurrentTrain: epoch  3, batch    36 | loss: 4.6933036Losses:  4.2042012214660645 0.2720952033996582
CurrentTrain: epoch  3, batch    37 | loss: 4.4762964Losses:  4.189791679382324 0.29287028312683105
CurrentTrain: epoch  3, batch    38 | loss: 4.4826622Losses:  4.164216041564941 0.2824675142765045
CurrentTrain: epoch  3, batch    39 | loss: 4.4466834Losses:  4.159295082092285 0.3334466218948364
CurrentTrain: epoch  3, batch    40 | loss: 4.4927416Losses:  4.075699806213379 0.23647496104240417
CurrentTrain: epoch  3, batch    41 | loss: 4.3121748Losses:  4.2150654792785645 0.28225862979888916
CurrentTrain: epoch  3, batch    42 | loss: 4.4973240Losses:  4.1860127449035645 0.2528195083141327
CurrentTrain: epoch  3, batch    43 | loss: 4.4388323Losses:  4.1612043380737305 0.4852224290370941
CurrentTrain: epoch  3, batch    44 | loss: 4.6464267Losses:  4.154027938842773 0.22055763006210327
CurrentTrain: epoch  3, batch    45 | loss: 4.3745856Losses:  4.122297286987305 0.24681344628334045
CurrentTrain: epoch  3, batch    46 | loss: 4.3691106Losses:  4.101308345794678 0.3009890615940094
CurrentTrain: epoch  3, batch    47 | loss: 4.4022975Losses:  4.110272407531738 0.21145889163017273
CurrentTrain: epoch  3, batch    48 | loss: 4.3217311Losses:  4.105792045593262 0.28503504395484924
CurrentTrain: epoch  3, batch    49 | loss: 4.3908272Losses:  4.104383945465088 0.31761589646339417
CurrentTrain: epoch  3, batch    50 | loss: 4.4219999Losses:  4.1417999267578125 0.21968093514442444
CurrentTrain: epoch  3, batch    51 | loss: 4.3614807Losses:  4.093354225158691 0.3048027455806732
CurrentTrain: epoch  3, batch    52 | loss: 4.3981571Losses:  4.142852306365967 0.38610315322875977
CurrentTrain: epoch  3, batch    53 | loss: 4.5289555Losses:  4.136560440063477 0.31935915350914
CurrentTrain: epoch  3, batch    54 | loss: 4.4559197Losses:  4.119172096252441 0.36630016565322876
CurrentTrain: epoch  3, batch    55 | loss: 4.4854722Losses:  4.056807041168213 0.3329924941062927
CurrentTrain: epoch  3, batch    56 | loss: 4.3897996Losses:  4.088481903076172 0.40664732456207275
CurrentTrain: epoch  3, batch    57 | loss: 4.4951291Losses:  4.111994743347168 0.20800670981407166
CurrentTrain: epoch  3, batch    58 | loss: 4.3200016Losses:  4.041072845458984 0.1634698361158371
CurrentTrain: epoch  3, batch    59 | loss: 4.2045426Losses:  4.070578575134277 0.380117267370224
CurrentTrain: epoch  3, batch    60 | loss: 4.4506960Losses:  4.059020519256592 0.1957227885723114
CurrentTrain: epoch  3, batch    61 | loss: 4.2547431Losses:  4.113961219787598 0.09862177073955536
CurrentTrain: epoch  3, batch    62 | loss: 4.2125831Losses:  4.0481157302856445 0.270017147064209
CurrentTrain: epoch  4, batch     0 | loss: 4.3181329Losses:  4.115649223327637 0.22908331453800201
CurrentTrain: epoch  4, batch     1 | loss: 4.3447328Losses:  4.060583114624023 0.23611173033714294
CurrentTrain: epoch  4, batch     2 | loss: 4.2966948Losses:  4.01809549331665 0.2333223521709442
CurrentTrain: epoch  4, batch     3 | loss: 4.2514176Losses:  4.139265537261963 0.311158150434494
CurrentTrain: epoch  4, batch     4 | loss: 4.4504237Losses:  4.087373733520508 0.2842659652233124
CurrentTrain: epoch  4, batch     5 | loss: 4.3716397Losses:  4.106203079223633 0.29017454385757446
CurrentTrain: epoch  4, batch     6 | loss: 4.3963776Losses:  4.084692001342773 0.2231167107820511
CurrentTrain: epoch  4, batch     7 | loss: 4.3078089Losses:  4.07433557510376 0.2875255048274994
CurrentTrain: epoch  4, batch     8 | loss: 4.3618612Losses:  4.101999282836914 0.3117613196372986
CurrentTrain: epoch  4, batch     9 | loss: 4.4137607Losses:  4.069382667541504 0.1969669759273529
CurrentTrain: epoch  4, batch    10 | loss: 4.2663498Losses:  4.162529945373535 0.4244885742664337
CurrentTrain: epoch  4, batch    11 | loss: 4.5870185Losses:  4.0619730949401855 0.2075825035572052
CurrentTrain: epoch  4, batch    12 | loss: 4.2695556Losses:  4.102470397949219 0.32967305183410645
CurrentTrain: epoch  4, batch    13 | loss: 4.4321432Losses:  4.010571002960205 0.165084570646286
CurrentTrain: epoch  4, batch    14 | loss: 4.1756554Losses:  4.099061489105225 0.20641925930976868
CurrentTrain: epoch  4, batch    15 | loss: 4.3054810Losses:  4.054049491882324 0.29104241728782654
CurrentTrain: epoch  4, batch    16 | loss: 4.3450918Losses:  4.0835113525390625 0.20506015419960022
CurrentTrain: epoch  4, batch    17 | loss: 4.2885714Losses:  4.062609672546387 0.24372754991054535
CurrentTrain: epoch  4, batch    18 | loss: 4.3063374Losses:  4.089282035827637 0.29278814792633057
CurrentTrain: epoch  4, batch    19 | loss: 4.3820701Losses:  4.061848163604736 0.3544335663318634
CurrentTrain: epoch  4, batch    20 | loss: 4.4162817Losses:  4.109471797943115 0.1930416226387024
CurrentTrain: epoch  4, batch    21 | loss: 4.3025136Losses:  4.057277679443359 0.2783677577972412
CurrentTrain: epoch  4, batch    22 | loss: 4.3356457Losses:  4.046197891235352 0.0808396115899086
CurrentTrain: epoch  4, batch    23 | loss: 4.1270375Losses:  4.123584747314453 0.29151421785354614
CurrentTrain: epoch  4, batch    24 | loss: 4.4150991Losses:  4.086949348449707 0.24783840775489807
CurrentTrain: epoch  4, batch    25 | loss: 4.3347878Losses:  4.046273231506348 0.2347138524055481
CurrentTrain: epoch  4, batch    26 | loss: 4.2809873Losses:  4.002236366271973 0.17525681853294373
CurrentTrain: epoch  4, batch    27 | loss: 4.1774931Losses:  4.0930376052856445 0.29632601141929626
CurrentTrain: epoch  4, batch    28 | loss: 4.3893638Losses:  4.125942230224609 0.17005760967731476
CurrentTrain: epoch  4, batch    29 | loss: 4.2960000Losses:  4.01525354385376 0.14793317019939423
CurrentTrain: epoch  4, batch    30 | loss: 4.1631866Losses:  4.098458290100098 0.26919978857040405
CurrentTrain: epoch  4, batch    31 | loss: 4.3676581Losses:  4.0772857666015625 0.30222266912460327
CurrentTrain: epoch  4, batch    32 | loss: 4.3795085Losses:  4.067868232727051 0.31349635124206543
CurrentTrain: epoch  4, batch    33 | loss: 4.3813648Losses:  4.073700904846191 0.14587590098381042
CurrentTrain: epoch  4, batch    34 | loss: 4.2195768Losses:  4.113175392150879 0.27318185567855835
CurrentTrain: epoch  4, batch    35 | loss: 4.3863573Losses:  4.03306770324707 0.31373292207717896
CurrentTrain: epoch  4, batch    36 | loss: 4.3468008Losses:  4.082895755767822 0.284270316362381
CurrentTrain: epoch  4, batch    37 | loss: 4.3671660Losses:  4.047701358795166 0.19116556644439697
CurrentTrain: epoch  4, batch    38 | loss: 4.2388668Losses:  4.068071365356445 0.23455435037612915
CurrentTrain: epoch  4, batch    39 | loss: 4.3026257Losses:  4.0211615562438965 0.18623360991477966
CurrentTrain: epoch  4, batch    40 | loss: 4.2073951Losses:  3.998828411102295 0.218942791223526
CurrentTrain: epoch  4, batch    41 | loss: 4.2177711Losses:  4.09965181350708 0.2936001420021057
CurrentTrain: epoch  4, batch    42 | loss: 4.3932519Losses:  4.0398945808410645 0.1619138866662979
CurrentTrain: epoch  4, batch    43 | loss: 4.2018085Losses:  4.015427589416504 0.18386438488960266
CurrentTrain: epoch  4, batch    44 | loss: 4.1992922Losses:  4.063821792602539 0.22146639227867126
CurrentTrain: epoch  4, batch    45 | loss: 4.2852883Losses:  4.059415817260742 0.3054521381855011
CurrentTrain: epoch  4, batch    46 | loss: 4.3648682Losses:  3.9899742603302 0.21889035403728485
CurrentTrain: epoch  4, batch    47 | loss: 4.2088647Losses:  4.042760848999023 0.22198832035064697
CurrentTrain: epoch  4, batch    48 | loss: 4.2647491Losses:  4.0201520919799805 0.19073593616485596
CurrentTrain: epoch  4, batch    49 | loss: 4.2108879Losses:  4.0474653244018555 0.3519110381603241
CurrentTrain: epoch  4, batch    50 | loss: 4.3993764Losses:  4.0171895027160645 0.16524073481559753
CurrentTrain: epoch  4, batch    51 | loss: 4.1824303Losses:  4.005314826965332 0.20821039378643036
CurrentTrain: epoch  4, batch    52 | loss: 4.2135253Losses:  4.0582380294799805 0.2555161118507385
CurrentTrain: epoch  4, batch    53 | loss: 4.3137541Losses:  4.069170951843262 0.2064506709575653
CurrentTrain: epoch  4, batch    54 | loss: 4.2756214Losses:  4.005393981933594 0.2253798246383667
CurrentTrain: epoch  4, batch    55 | loss: 4.2307739Losses:  4.049320220947266 0.15955211222171783
CurrentTrain: epoch  4, batch    56 | loss: 4.2088723Losses:  4.037877559661865 0.2880956530570984
CurrentTrain: epoch  4, batch    57 | loss: 4.3259730Losses:  4.015955924987793 0.16558891534805298
CurrentTrain: epoch  4, batch    58 | loss: 4.1815448Losses:  4.078627109527588 0.18647050857543945
CurrentTrain: epoch  4, batch    59 | loss: 4.2650976Losses:  4.000401020050049 0.2017931342124939
CurrentTrain: epoch  4, batch    60 | loss: 4.2021942Losses:  4.034550666809082 0.1882651150226593
CurrentTrain: epoch  4, batch    61 | loss: 4.2228160Losses:  4.092973709106445 0.12635986506938934
CurrentTrain: epoch  4, batch    62 | loss: 4.2193336Losses:  4.070162296295166 0.24298249185085297
CurrentTrain: epoch  5, batch     0 | loss: 4.3131447Losses:  4.043507099151611 0.2730554938316345
CurrentTrain: epoch  5, batch     1 | loss: 4.3165627Losses:  4.007742404937744 0.17331050336360931
CurrentTrain: epoch  5, batch     2 | loss: 4.1810527Losses:  4.038966178894043 0.14381754398345947
CurrentTrain: epoch  5, batch     3 | loss: 4.1827836Losses:  4.095930099487305 0.19671675562858582
CurrentTrain: epoch  5, batch     4 | loss: 4.2926469Losses:  4.042236804962158 0.16648855805397034
CurrentTrain: epoch  5, batch     5 | loss: 4.2087255Losses:  4.038304805755615 0.1711767017841339
CurrentTrain: epoch  5, batch     6 | loss: 4.2094817Losses:  4.009706974029541 0.3990337550640106
CurrentTrain: epoch  5, batch     7 | loss: 4.4087405Losses:  4.088428497314453 0.24030330777168274
CurrentTrain: epoch  5, batch     8 | loss: 4.3287320Losses:  4.031603813171387 0.1353205293416977
CurrentTrain: epoch  5, batch     9 | loss: 4.1669245Losses:  4.012853622436523 0.27242761850357056
CurrentTrain: epoch  5, batch    10 | loss: 4.2852812Losses:  4.039802551269531 0.207770437002182
CurrentTrain: epoch  5, batch    11 | loss: 4.2475729Losses:  4.019105911254883 0.1870897114276886
CurrentTrain: epoch  5, batch    12 | loss: 4.2061958Losses:  4.0113091468811035 0.1663873791694641
CurrentTrain: epoch  5, batch    13 | loss: 4.1776967Losses:  3.9952664375305176 0.22026537358760834
CurrentTrain: epoch  5, batch    14 | loss: 4.2155318Losses:  4.018121719360352 0.19645598530769348
CurrentTrain: epoch  5, batch    15 | loss: 4.2145777Losses:  4.042921543121338 0.22876910865306854
CurrentTrain: epoch  5, batch    16 | loss: 4.2716908Losses:  4.00740909576416 0.18515877425670624
CurrentTrain: epoch  5, batch    17 | loss: 4.1925678Losses:  3.999843120574951 0.27012959122657776
CurrentTrain: epoch  5, batch    18 | loss: 4.2699728Losses:  4.034774303436279 0.2164144515991211
CurrentTrain: epoch  5, batch    19 | loss: 4.2511888Losses:  4.00737190246582 0.168256014585495
CurrentTrain: epoch  5, batch    20 | loss: 4.1756277Losses:  3.99415922164917 0.17342978715896606
CurrentTrain: epoch  5, batch    21 | loss: 4.1675892Losses:  4.06231689453125 0.1610351949930191
CurrentTrain: epoch  5, batch    22 | loss: 4.2233520Losses:  4.035860538482666 0.225419282913208
CurrentTrain: epoch  5, batch    23 | loss: 4.2612801Losses:  4.046051979064941 0.11135293543338776
CurrentTrain: epoch  5, batch    24 | loss: 4.1574049Losses:  3.9934558868408203 0.19972683489322662
CurrentTrain: epoch  5, batch    25 | loss: 4.1931829Losses:  4.053596496582031 0.20769168436527252
CurrentTrain: epoch  5, batch    26 | loss: 4.2612882Losses:  4.031198501586914 0.18243050575256348
CurrentTrain: epoch  5, batch    27 | loss: 4.2136288Losses:  4.001866340637207 0.17873944342136383
CurrentTrain: epoch  5, batch    28 | loss: 4.1806059Losses:  4.017320156097412 0.22657160460948944
CurrentTrain: epoch  5, batch    29 | loss: 4.2438917Losses:  4.012443542480469 0.127241313457489
CurrentTrain: epoch  5, batch    30 | loss: 4.1396847Losses:  4.007652282714844 0.14616046845912933
CurrentTrain: epoch  5, batch    31 | loss: 4.1538129Losses:  4.044017791748047 0.11635829508304596
CurrentTrain: epoch  5, batch    32 | loss: 4.1603761Losses:  3.9767980575561523 0.18160949647426605
CurrentTrain: epoch  5, batch    33 | loss: 4.1584077Losses:  4.033677101135254 0.17510131001472473
CurrentTrain: epoch  5, batch    34 | loss: 4.2087784Losses:  4.020556449890137 0.20933076739311218
CurrentTrain: epoch  5, batch    35 | loss: 4.2298870Losses:  4.031168460845947 0.15814930200576782
CurrentTrain: epoch  5, batch    36 | loss: 4.1893177Losses:  4.02573299407959 0.17673051357269287
CurrentTrain: epoch  5, batch    37 | loss: 4.2024636Losses:  4.023634910583496 0.17732249200344086
CurrentTrain: epoch  5, batch    38 | loss: 4.2009573Losses:  4.036309719085693 0.25244539976119995
CurrentTrain: epoch  5, batch    39 | loss: 4.2887549Losses:  3.981314182281494 0.18167425692081451
CurrentTrain: epoch  5, batch    40 | loss: 4.1629887Losses:  4.007546424865723 0.15535326302051544
CurrentTrain: epoch  5, batch    41 | loss: 4.1628995Losses:  4.036322593688965 0.30293941497802734
CurrentTrain: epoch  5, batch    42 | loss: 4.3392620Losses:  4.005229949951172 0.2531748414039612
CurrentTrain: epoch  5, batch    43 | loss: 4.2584047Losses:  4.010069847106934 0.18225200474262238
CurrentTrain: epoch  5, batch    44 | loss: 4.1923218Losses:  3.991783380508423 0.2307683229446411
CurrentTrain: epoch  5, batch    45 | loss: 4.2225518Losses:  3.9947047233581543 0.19186805188655853
CurrentTrain: epoch  5, batch    46 | loss: 4.1865726Losses:  4.006010055541992 0.1942010521888733
CurrentTrain: epoch  5, batch    47 | loss: 4.2002110Losses:  3.972858428955078 0.17816372215747833
CurrentTrain: epoch  5, batch    48 | loss: 4.1510220Losses:  4.014911651611328 0.1618126630783081
CurrentTrain: epoch  5, batch    49 | loss: 4.1767244Losses:  3.972330093383789 0.14772957563400269
CurrentTrain: epoch  5, batch    50 | loss: 4.1200595Losses:  3.9998528957366943 0.13431742787361145
CurrentTrain: epoch  5, batch    51 | loss: 4.1341705Losses:  4.011799335479736 0.20012129843235016
CurrentTrain: epoch  5, batch    52 | loss: 4.2119207Losses:  4.017331600189209 0.15804821252822876
CurrentTrain: epoch  5, batch    53 | loss: 4.1753798Losses:  3.957921028137207 0.25081929564476013
CurrentTrain: epoch  5, batch    54 | loss: 4.2087402Losses:  3.9834165573120117 0.2066505253314972
CurrentTrain: epoch  5, batch    55 | loss: 4.1900673Losses:  4.0198211669921875 0.1994413286447525
CurrentTrain: epoch  5, batch    56 | loss: 4.2192626Losses:  4.032655715942383 0.16381415724754333
CurrentTrain: epoch  5, batch    57 | loss: 4.1964698Losses:  3.9596588611602783 0.14319536089897156
CurrentTrain: epoch  5, batch    58 | loss: 4.1028543Losses:  4.005915641784668 0.17746050655841827
CurrentTrain: epoch  5, batch    59 | loss: 4.1833763Losses:  3.9897305965423584 0.11921355128288269
CurrentTrain: epoch  5, batch    60 | loss: 4.1089439Losses:  4.006129264831543 0.11415109783411026
CurrentTrain: epoch  5, batch    61 | loss: 4.1202803Losses:  4.012212753295898 0.1127031072974205
CurrentTrain: epoch  5, batch    62 | loss: 4.1249161Losses:  4.0059404373168945 0.18010461330413818
CurrentTrain: epoch  6, batch     0 | loss: 4.1860452Losses:  3.974900722503662 0.13736316561698914
CurrentTrain: epoch  6, batch     1 | loss: 4.1122637Losses:  3.9635469913482666 0.11742229759693146
CurrentTrain: epoch  6, batch     2 | loss: 4.0809693Losses:  4.02056884765625 0.23013219237327576
CurrentTrain: epoch  6, batch     3 | loss: 4.2507010Losses:  4.003427505493164 0.11022411286830902
CurrentTrain: epoch  6, batch     4 | loss: 4.1136518Losses:  3.994962453842163 0.12346261739730835
CurrentTrain: epoch  6, batch     5 | loss: 4.1184249Losses:  4.001800060272217 0.18254733085632324
CurrentTrain: epoch  6, batch     6 | loss: 4.1843472Losses:  3.9959604740142822 0.2049030065536499
CurrentTrain: epoch  6, batch     7 | loss: 4.2008634Losses:  3.9782307147979736 0.1936315894126892
CurrentTrain: epoch  6, batch     8 | loss: 4.1718621Losses:  4.009886741638184 0.12860319018363953
CurrentTrain: epoch  6, batch     9 | loss: 4.1384897Losses:  3.99250864982605 0.1518242508172989
CurrentTrain: epoch  6, batch    10 | loss: 4.1443329Losses:  4.068232536315918 0.09592998772859573
CurrentTrain: epoch  6, batch    11 | loss: 4.1641626Losses:  4.030304908752441 0.17156565189361572
CurrentTrain: epoch  6, batch    12 | loss: 4.2018704Losses:  4.0073041915893555 0.1240772157907486
CurrentTrain: epoch  6, batch    13 | loss: 4.1313815Losses:  4.008552551269531 0.1573439985513687
CurrentTrain: epoch  6, batch    14 | loss: 4.1658964Losses:  3.98256778717041 0.1654512882232666
CurrentTrain: epoch  6, batch    15 | loss: 4.1480188Losses:  3.988595962524414 0.16328443586826324
CurrentTrain: epoch  6, batch    16 | loss: 4.1518803Losses:  3.9894535541534424 0.1523035168647766
CurrentTrain: epoch  6, batch    17 | loss: 4.1417570Losses:  4.03321647644043 0.06395673751831055
CurrentTrain: epoch  6, batch    18 | loss: 4.0971732Losses:  3.989529609680176 0.16286370158195496
CurrentTrain: epoch  6, batch    19 | loss: 4.1523933Losses:  3.981077194213867 0.21630194783210754
CurrentTrain: epoch  6, batch    20 | loss: 4.1973791Losses:  4.041426658630371 0.15816940367221832
CurrentTrain: epoch  6, batch    21 | loss: 4.1995959Losses:  4.036890029907227 0.15161608159542084
CurrentTrain: epoch  6, batch    22 | loss: 4.1885061Losses:  3.9801039695739746 0.1446758508682251
CurrentTrain: epoch  6, batch    23 | loss: 4.1247797Losses:  4.0053911209106445 0.13009384274482727
CurrentTrain: epoch  6, batch    24 | loss: 4.1354852Losses:  4.03276252746582 0.16686537861824036
CurrentTrain: epoch  6, batch    25 | loss: 4.1996279Losses:  4.030080795288086 0.1529218852519989
CurrentTrain: epoch  6, batch    26 | loss: 4.1830025Losses:  3.983978748321533 0.12603622674942017
CurrentTrain: epoch  6, batch    27 | loss: 4.1100149Losses:  3.9751968383789062 0.17305536568164825
CurrentTrain: epoch  6, batch    28 | loss: 4.1482520Losses:  3.9903202056884766 0.15304070711135864
CurrentTrain: epoch  6, batch    29 | loss: 4.1433611Losses:  3.980792760848999 0.2426229566335678
CurrentTrain: epoch  6, batch    30 | loss: 4.2234159Losses:  3.9254188537597656 0.0849798321723938
CurrentTrain: epoch  6, batch    31 | loss: 4.0103989Losses:  4.006736755371094 0.11755911260843277
CurrentTrain: epoch  6, batch    32 | loss: 4.1242957Losses:  3.9862558841705322 0.19565677642822266
CurrentTrain: epoch  6, batch    33 | loss: 4.1819124Losses:  4.02313232421875 0.18242013454437256
CurrentTrain: epoch  6, batch    34 | loss: 4.2055526Losses:  3.977217197418213 0.10210766643285751
CurrentTrain: epoch  6, batch    35 | loss: 4.0793247Losses:  4.045135021209717 0.08251181989908218
CurrentTrain: epoch  6, batch    36 | loss: 4.1276469Losses:  3.9920454025268555 0.14802414178848267
CurrentTrain: epoch  6, batch    37 | loss: 4.1400695Losses:  3.9866340160369873 0.20486705005168915
CurrentTrain: epoch  6, batch    38 | loss: 4.1915011Losses:  3.9543862342834473 0.0881638452410698
CurrentTrain: epoch  6, batch    39 | loss: 4.0425501Losses:  3.9975314140319824 0.1496056616306305
CurrentTrain: epoch  6, batch    40 | loss: 4.1471372Losses:  3.9778428077697754 0.19779109954833984
CurrentTrain: epoch  6, batch    41 | loss: 4.1756339Losses:  3.961277961730957 0.15878891944885254
CurrentTrain: epoch  6, batch    42 | loss: 4.1200666Losses:  3.952523708343506 0.13174614310264587
CurrentTrain: epoch  6, batch    43 | loss: 4.0842700Losses:  3.9764180183410645 0.13847047090530396
CurrentTrain: epoch  6, batch    44 | loss: 4.1148887Losses:  3.995148181915283 0.12879136204719543
CurrentTrain: epoch  6, batch    45 | loss: 4.1239395Losses:  3.978433132171631 0.16636328399181366
CurrentTrain: epoch  6, batch    46 | loss: 4.1447964Losses:  3.9920339584350586 0.12565019726753235
CurrentTrain: epoch  6, batch    47 | loss: 4.1176844Losses:  3.971666097640991 0.1799793392419815
CurrentTrain: epoch  6, batch    48 | loss: 4.1516457Losses:  3.9919593334198 0.08943110704421997
CurrentTrain: epoch  6, batch    49 | loss: 4.0813904Losses:  3.995879650115967 0.11204921454191208
CurrentTrain: epoch  6, batch    50 | loss: 4.1079288Losses:  3.9612131118774414 0.12024231255054474
CurrentTrain: epoch  6, batch    51 | loss: 4.0814552Losses:  3.961338520050049 0.16427482664585114
CurrentTrain: epoch  6, batch    52 | loss: 4.1256132Losses:  3.971865653991699 0.11885273456573486
CurrentTrain: epoch  6, batch    53 | loss: 4.0907183Losses:  3.977825164794922 0.12330827116966248
CurrentTrain: epoch  6, batch    54 | loss: 4.1011333Losses:  3.9499049186706543 0.14140936732292175
CurrentTrain: epoch  6, batch    55 | loss: 4.0913143Losses:  4.021068572998047 0.10118971019983292
CurrentTrain: epoch  6, batch    56 | loss: 4.1222582Losses:  3.982086181640625 0.12633809447288513
CurrentTrain: epoch  6, batch    57 | loss: 4.1084242Losses:  3.976626396179199 0.15517234802246094
CurrentTrain: epoch  6, batch    58 | loss: 4.1317987Losses:  3.9927680492401123 0.15862347185611725
CurrentTrain: epoch  6, batch    59 | loss: 4.1513915Losses:  4.013530731201172 0.10868450254201889
CurrentTrain: epoch  6, batch    60 | loss: 4.1222153Losses:  3.9806647300720215 0.12682592868804932
CurrentTrain: epoch  6, batch    61 | loss: 4.1074905Losses:  3.957998514175415 0.15173469483852386
CurrentTrain: epoch  6, batch    62 | loss: 4.1097331Losses:  4.03272819519043 0.09396658837795258
CurrentTrain: epoch  7, batch     0 | loss: 4.1266947Losses:  4.002933979034424 0.10458710044622421
CurrentTrain: epoch  7, batch     1 | loss: 4.1075211Losses:  3.97556209564209 0.12712453305721283
CurrentTrain: epoch  7, batch     2 | loss: 4.1026864Losses:  3.9692487716674805 0.12734001874923706
CurrentTrain: epoch  7, batch     3 | loss: 4.0965886Losses:  3.973280429840088 0.14805254340171814
CurrentTrain: epoch  7, batch     4 | loss: 4.1213331Losses:  3.978808879852295 0.09334160387516022
CurrentTrain: epoch  7, batch     5 | loss: 4.0721507Losses:  3.9691002368927 0.11503498256206512
CurrentTrain: epoch  7, batch     6 | loss: 4.0841351Losses:  4.007990837097168 0.1876620203256607
CurrentTrain: epoch  7, batch     7 | loss: 4.1956530Losses:  3.927779197692871 0.07481461018323898
CurrentTrain: epoch  7, batch     8 | loss: 4.0025940Losses:  3.935539722442627 0.1580062210559845
CurrentTrain: epoch  7, batch     9 | loss: 4.0935459Losses:  3.942472219467163 0.1608642190694809
CurrentTrain: epoch  7, batch    10 | loss: 4.1033363Losses:  3.923809051513672 0.09783142805099487
CurrentTrain: epoch  7, batch    11 | loss: 4.0216403Losses:  3.9404120445251465 0.11297862231731415
CurrentTrain: epoch  7, batch    12 | loss: 4.0533905Losses:  3.9752426147460938 0.09497661143541336
CurrentTrain: epoch  7, batch    13 | loss: 4.0702190Losses:  3.9428863525390625 0.11746831238269806
CurrentTrain: epoch  7, batch    14 | loss: 4.0603547Losses:  3.900419235229492 0.15091226994991302
CurrentTrain: epoch  7, batch    15 | loss: 4.0513315Losses:  4.0138139724731445 0.1557779312133789
CurrentTrain: epoch  7, batch    16 | loss: 4.1695919Losses:  3.9618911743164062 0.23583681881427765
CurrentTrain: epoch  7, batch    17 | loss: 4.1977282Losses:  3.9719245433807373 0.10086933523416519
CurrentTrain: epoch  7, batch    18 | loss: 4.0727940Losses:  4.009981632232666 0.11630202829837799
CurrentTrain: epoch  7, batch    19 | loss: 4.1262836Losses:  4.006953716278076 0.07730785012245178
CurrentTrain: epoch  7, batch    20 | loss: 4.0842614Losses:  3.9325942993164062 0.1854763925075531
CurrentTrain: epoch  7, batch    21 | loss: 4.1180706Losses:  3.9679346084594727 0.10080677270889282
CurrentTrain: epoch  7, batch    22 | loss: 4.0687413Losses:  3.984631299972534 0.15810349583625793
CurrentTrain: epoch  7, batch    23 | loss: 4.1427350Losses:  3.963625431060791 0.13541872799396515
CurrentTrain: epoch  7, batch    24 | loss: 4.0990443Losses:  4.044442653656006 0.1521952897310257
CurrentTrain: epoch  7, batch    25 | loss: 4.1966381Losses:  3.957014560699463 0.08105060458183289
CurrentTrain: epoch  7, batch    26 | loss: 4.0380650Losses:  3.9709525108337402 0.10181914269924164
CurrentTrain: epoch  7, batch    27 | loss: 4.0727715Losses:  3.9802632331848145 0.1157095730304718
CurrentTrain: epoch  7, batch    28 | loss: 4.0959730Losses:  3.9954776763916016 0.07477930933237076
CurrentTrain: epoch  7, batch    29 | loss: 4.0702572Losses:  4.0107502937316895 0.1014874204993248
CurrentTrain: epoch  7, batch    30 | loss: 4.1122379Losses:  3.980196237564087 0.10767220705747604
CurrentTrain: epoch  7, batch    31 | loss: 4.0878682Losses:  3.993664264678955 0.06661203503608704
CurrentTrain: epoch  7, batch    32 | loss: 4.0602765Losses:  4.059315204620361 0.11608316749334335
CurrentTrain: epoch  7, batch    33 | loss: 4.1753983Losses:  4.016821384429932 0.10614943504333496
CurrentTrain: epoch  7, batch    34 | loss: 4.1229706Losses:  3.9606428146362305 0.1129380539059639
CurrentTrain: epoch  7, batch    35 | loss: 4.0735807Losses:  4.052806854248047 0.10850844532251358
CurrentTrain: epoch  7, batch    36 | loss: 4.1613154Losses:  3.94604229927063 0.09125371277332306
CurrentTrain: epoch  7, batch    37 | loss: 4.0372958Losses:  3.964884042739868 0.12616874277591705
CurrentTrain: epoch  7, batch    38 | loss: 4.0910530Losses:  3.940955638885498 0.09094580262899399
CurrentTrain: epoch  7, batch    39 | loss: 4.0319014Losses:  3.9501609802246094 0.12944750487804413
CurrentTrain: epoch  7, batch    40 | loss: 4.0796084Losses:  3.952320098876953 0.08547437936067581
CurrentTrain: epoch  7, batch    41 | loss: 4.0377946Losses:  3.942535638809204 0.09627257287502289
CurrentTrain: epoch  7, batch    42 | loss: 4.0388083Losses:  3.9556996822357178 0.10834725201129913
CurrentTrain: epoch  7, batch    43 | loss: 4.0640469Losses:  3.9639906883239746 0.0926397517323494
CurrentTrain: epoch  7, batch    44 | loss: 4.0566306Losses:  3.922741174697876 0.10898572951555252
CurrentTrain: epoch  7, batch    45 | loss: 4.0317268Losses:  3.971994638442993 0.10133762657642365
CurrentTrain: epoch  7, batch    46 | loss: 4.0733323Losses:  3.969425916671753 0.10436037927865982
CurrentTrain: epoch  7, batch    47 | loss: 4.0737863Losses:  3.9527716636657715 0.14619982242584229
CurrentTrain: epoch  7, batch    48 | loss: 4.0989714Losses:  3.927328109741211 0.1586272418498993
CurrentTrain: epoch  7, batch    49 | loss: 4.0859551Losses:  3.911773443222046 0.09796559065580368
CurrentTrain: epoch  7, batch    50 | loss: 4.0097389Losses:  3.950718879699707 0.09531158208847046
CurrentTrain: epoch  7, batch    51 | loss: 4.0460305Losses:  4.007880210876465 0.1511225700378418
CurrentTrain: epoch  7, batch    52 | loss: 4.1590028Losses:  3.9957218170166016 0.11479153484106064
CurrentTrain: epoch  7, batch    53 | loss: 4.1105132Losses:  3.994188070297241 0.1048159971833229
CurrentTrain: epoch  7, batch    54 | loss: 4.0990043Losses:  3.9662232398986816 0.10807663947343826
CurrentTrain: epoch  7, batch    55 | loss: 4.0742998Losses:  3.975571393966675 0.07846415042877197
CurrentTrain: epoch  7, batch    56 | loss: 4.0540357Losses:  3.9464776515960693 0.10265213251113892
CurrentTrain: epoch  7, batch    57 | loss: 4.0491300Losses:  3.9349892139434814 0.14957809448242188
CurrentTrain: epoch  7, batch    58 | loss: 4.0845671Losses:  3.996183156967163 0.07222231477499008
CurrentTrain: epoch  7, batch    59 | loss: 4.0684056Losses:  3.9834532737731934 0.1214488223195076
CurrentTrain: epoch  7, batch    60 | loss: 4.1049023Losses:  3.964890480041504 0.1010495126247406
CurrentTrain: epoch  7, batch    61 | loss: 4.0659399Losses:  4.022117614746094 0.05644657835364342
CurrentTrain: epoch  7, batch    62 | loss: 4.0785642Losses:  3.9711084365844727 0.13156776130199432
CurrentTrain: epoch  8, batch     0 | loss: 4.1026764Losses:  3.9996180534362793 0.11178627610206604
CurrentTrain: epoch  8, batch     1 | loss: 4.1114044Losses:  3.961520195007324 0.13792207837104797
CurrentTrain: epoch  8, batch     2 | loss: 4.0994425Losses:  3.9848506450653076 0.07002567499876022
CurrentTrain: epoch  8, batch     3 | loss: 4.0548763Losses:  3.920179843902588 0.07615219056606293
CurrentTrain: epoch  8, batch     4 | loss: 3.9963319Losses:  3.95442533493042 0.10661754012107849
CurrentTrain: epoch  8, batch     5 | loss: 4.0610428Losses:  3.9744668006896973 0.10493575781583786
CurrentTrain: epoch  8, batch     6 | loss: 4.0794024Losses:  3.9824674129486084 0.08983258903026581
CurrentTrain: epoch  8, batch     7 | loss: 4.0723000Losses:  3.9621636867523193 0.09684084355831146
CurrentTrain: epoch  8, batch     8 | loss: 4.0590043Losses:  3.968334674835205 0.10421861708164215
CurrentTrain: epoch  8, batch     9 | loss: 4.0725532Losses:  3.9368786811828613 0.07861914485692978
CurrentTrain: epoch  8, batch    10 | loss: 4.0154977Losses:  3.9501829147338867 0.11555881798267365
CurrentTrain: epoch  8, batch    11 | loss: 4.0657415Losses:  3.9683384895324707 0.08520980179309845
CurrentTrain: epoch  8, batch    12 | loss: 4.0535483Losses:  3.9622721672058105 0.09250110387802124
CurrentTrain: epoch  8, batch    13 | loss: 4.0547733Losses:  3.915069818496704 0.14052720367908478
CurrentTrain: epoch  8, batch    14 | loss: 4.0555968Losses:  3.945162773132324 0.10072533786296844
CurrentTrain: epoch  8, batch    15 | loss: 4.0458879Losses:  3.9308547973632812 0.08172494173049927
CurrentTrain: epoch  8, batch    16 | loss: 4.0125799Losses:  3.971595287322998 0.12036773562431335
CurrentTrain: epoch  8, batch    17 | loss: 4.0919628Losses:  3.9633889198303223 0.09120798110961914
CurrentTrain: epoch  8, batch    18 | loss: 4.0545969Losses:  3.983543634414673 0.08289886265993118
CurrentTrain: epoch  8, batch    19 | loss: 4.0664425Losses:  3.8933701515197754 0.09508723020553589
CurrentTrain: epoch  8, batch    20 | loss: 3.9884574Losses:  3.944395065307617 0.09542796015739441
CurrentTrain: epoch  8, batch    21 | loss: 4.0398231Losses:  3.9434800148010254 0.0989350751042366
CurrentTrain: epoch  8, batch    22 | loss: 4.0424151Losses:  3.9685001373291016 0.11417818069458008
CurrentTrain: epoch  8, batch    23 | loss: 4.0826783Losses:  3.9578824043273926 0.1206619068980217
CurrentTrain: epoch  8, batch    24 | loss: 4.0785441Losses:  3.943976879119873 0.06314966827630997
CurrentTrain: epoch  8, batch    25 | loss: 4.0071263Losses:  3.9530739784240723 0.10421623289585114
CurrentTrain: epoch  8, batch    26 | loss: 4.0572901Losses:  3.9992828369140625 0.10973440855741501
CurrentTrain: epoch  8, batch    27 | loss: 4.1090174Losses:  3.9467146396636963 0.13481372594833374
CurrentTrain: epoch  8, batch    28 | loss: 4.0815282Losses:  3.9612064361572266 0.1034092977643013
CurrentTrain: epoch  8, batch    29 | loss: 4.0646157Losses:  3.932515859603882 0.10971088707447052
CurrentTrain: epoch  8, batch    30 | loss: 4.0422268Losses:  3.952577590942383 0.1076752245426178
CurrentTrain: epoch  8, batch    31 | loss: 4.0602527Losses:  3.975846767425537 0.08729514479637146
CurrentTrain: epoch  8, batch    32 | loss: 4.0631418Losses:  3.9466772079467773 0.13264912366867065
CurrentTrain: epoch  8, batch    33 | loss: 4.0793262Losses:  3.9585886001586914 0.1121615469455719
CurrentTrain: epoch  8, batch    34 | loss: 4.0707502Losses:  3.947190523147583 0.11180765181779861
CurrentTrain: epoch  8, batch    35 | loss: 4.0589981Losses:  3.9620394706726074 0.07656344771385193
CurrentTrain: epoch  8, batch    36 | loss: 4.0386028Losses:  3.982588291168213 0.12990710139274597
CurrentTrain: epoch  8, batch    37 | loss: 4.1124954Losses:  3.910400390625 0.12439405918121338
CurrentTrain: epoch  8, batch    38 | loss: 4.0347943Losses:  3.980867862701416 0.11228038370609283
CurrentTrain: epoch  8, batch    39 | loss: 4.0931482Losses:  3.9844584465026855 0.12691116333007812
CurrentTrain: epoch  8, batch    40 | loss: 4.1113696Losses:  3.970902442932129 0.09090810269117355
CurrentTrain: epoch  8, batch    41 | loss: 4.0618105Losses:  3.9815261363983154 0.09920552372932434
CurrentTrain: epoch  8, batch    42 | loss: 4.0807319Losses:  3.9594216346740723 0.12316802889108658
CurrentTrain: epoch  8, batch    43 | loss: 4.0825896Losses:  3.9764747619628906 0.10380075871944427
CurrentTrain: epoch  8, batch    44 | loss: 4.0802755Losses:  4.00272798538208 0.0740676075220108
CurrentTrain: epoch  8, batch    45 | loss: 4.0767956Losses:  3.9199090003967285 0.07989726960659027
CurrentTrain: epoch  8, batch    46 | loss: 3.9998062Losses:  3.971208095550537 0.10912598669528961
CurrentTrain: epoch  8, batch    47 | loss: 4.0803342Losses:  3.98295259475708 0.06760462373495102
CurrentTrain: epoch  8, batch    48 | loss: 4.0505571Losses:  3.9521689414978027 0.09561373293399811
CurrentTrain: epoch  8, batch    49 | loss: 4.0477829Losses:  3.9523792266845703 0.07307078689336777
CurrentTrain: epoch  8, batch    50 | loss: 4.0254502Losses:  3.965803861618042 0.07561548799276352
CurrentTrain: epoch  8, batch    51 | loss: 4.0414195Losses:  3.963064193725586 0.16527332365512848
CurrentTrain: epoch  8, batch    52 | loss: 4.1283374Losses:  3.9809587001800537 0.10165082663297653
CurrentTrain: epoch  8, batch    53 | loss: 4.0826097Losses:  3.9359731674194336 0.10414228588342667
CurrentTrain: epoch  8, batch    54 | loss: 4.0401154Losses:  3.9384121894836426 0.08490490168333054
CurrentTrain: epoch  8, batch    55 | loss: 4.0233169Losses:  3.965217113494873 0.10265280306339264
CurrentTrain: epoch  8, batch    56 | loss: 4.0678701Losses:  3.952669143676758 0.10339732468128204
CurrentTrain: epoch  8, batch    57 | loss: 4.0560665Losses:  3.9864399433135986 0.08841577172279358
CurrentTrain: epoch  8, batch    58 | loss: 4.0748558Losses:  3.9714772701263428 0.11370933800935745
CurrentTrain: epoch  8, batch    59 | loss: 4.0851865Losses:  3.911807060241699 0.1365652233362198
CurrentTrain: epoch  8, batch    60 | loss: 4.0483723Losses:  3.9724631309509277 0.11363157629966736
CurrentTrain: epoch  8, batch    61 | loss: 4.0860949Losses:  3.9416232109069824 0.030411282554268837
CurrentTrain: epoch  8, batch    62 | loss: 3.9720345Losses:  3.9484686851501465 0.07909392565488815
CurrentTrain: epoch  9, batch     0 | loss: 4.0275626Losses:  3.9882004261016846 0.07820296287536621
CurrentTrain: epoch  9, batch     1 | loss: 4.0664034Losses:  3.9732918739318848 0.08770079910755157
CurrentTrain: epoch  9, batch     2 | loss: 4.0609927Losses:  3.99668025970459 0.08020369708538055
CurrentTrain: epoch  9, batch     3 | loss: 4.0768838Losses:  3.948298454284668 0.06817954033613205
CurrentTrain: epoch  9, batch     4 | loss: 4.0164781Losses:  3.9523844718933105 0.08051300048828125
CurrentTrain: epoch  9, batch     5 | loss: 4.0328975Losses:  3.9636154174804688 0.0945359319448471
CurrentTrain: epoch  9, batch     6 | loss: 4.0581512Losses:  3.9458706378936768 0.07607429474592209
CurrentTrain: epoch  9, batch     7 | loss: 4.0219450Losses:  3.9149672985076904 0.06975141912698746
CurrentTrain: epoch  9, batch     8 | loss: 3.9847188Losses:  3.9511008262634277 0.1089727133512497
CurrentTrain: epoch  9, batch     9 | loss: 4.0600734Losses:  3.945640802383423 0.11101695895195007
CurrentTrain: epoch  9, batch    10 | loss: 4.0566578Losses:  3.972564458847046 0.11986532062292099
CurrentTrain: epoch  9, batch    11 | loss: 4.0924296Losses:  3.955660104751587 0.09510596096515656
CurrentTrain: epoch  9, batch    12 | loss: 4.0507660Losses:  3.959238052368164 0.10433453321456909
CurrentTrain: epoch  9, batch    13 | loss: 4.0635724Losses:  3.9509198665618896 0.1462903916835785
CurrentTrain: epoch  9, batch    14 | loss: 4.0972104Losses:  3.9799551963806152 0.09800294041633606
CurrentTrain: epoch  9, batch    15 | loss: 4.0779581Losses:  3.944425582885742 0.10221204161643982
CurrentTrain: epoch  9, batch    16 | loss: 4.0466375Losses:  3.9864652156829834 0.10407242178916931
CurrentTrain: epoch  9, batch    17 | loss: 4.0905375Losses:  3.947411060333252 0.07560991495847702
CurrentTrain: epoch  9, batch    18 | loss: 4.0230207Losses:  3.894246816635132 0.05569332838058472
CurrentTrain: epoch  9, batch    19 | loss: 3.9499402Losses:  3.9869279861450195 0.08192965388298035
CurrentTrain: epoch  9, batch    20 | loss: 4.0688577Losses:  3.9359679222106934 0.082819864153862
CurrentTrain: epoch  9, batch    21 | loss: 4.0187879Losses:  3.9800591468811035 0.10411150753498077
CurrentTrain: epoch  9, batch    22 | loss: 4.0841708Losses:  3.939692497253418 0.08011206984519958
CurrentTrain: epoch  9, batch    23 | loss: 4.0198045Losses:  3.951249122619629 0.06827566027641296
CurrentTrain: epoch  9, batch    24 | loss: 4.0195246Losses:  3.9690654277801514 0.08190688490867615
CurrentTrain: epoch  9, batch    25 | loss: 4.0509725Losses:  3.918822765350342 0.11802826076745987
CurrentTrain: epoch  9, batch    26 | loss: 4.0368509Losses:  3.942310333251953 0.07839928567409515
CurrentTrain: epoch  9, batch    27 | loss: 4.0207095Losses:  3.9422101974487305 0.10777372121810913
CurrentTrain: epoch  9, batch    28 | loss: 4.0499840Losses:  3.9232711791992188 0.08647279441356659
CurrentTrain: epoch  9, batch    29 | loss: 4.0097442Losses:  3.955718517303467 0.09921115636825562
CurrentTrain: epoch  9, batch    30 | loss: 4.0549297Losses:  3.928408145904541 0.11254489421844482
CurrentTrain: epoch  9, batch    31 | loss: 4.0409532Losses:  3.9060750007629395 0.07303272187709808
CurrentTrain: epoch  9, batch    32 | loss: 3.9791076Losses:  3.9095005989074707 0.046968284994363785
CurrentTrain: epoch  9, batch    33 | loss: 3.9564688Losses:  3.9361085891723633 0.10132741928100586
CurrentTrain: epoch  9, batch    34 | loss: 4.0374360Losses:  3.932729721069336 0.14215248823165894
CurrentTrain: epoch  9, batch    35 | loss: 4.0748820Losses:  3.9325056076049805 0.053675781935453415
CurrentTrain: epoch  9, batch    36 | loss: 3.9861815Losses:  3.9180479049682617 0.09832853823900223
CurrentTrain: epoch  9, batch    37 | loss: 4.0163765Losses:  3.9901437759399414 0.07254008948802948
CurrentTrain: epoch  9, batch    38 | loss: 4.0626841Losses:  3.9378137588500977 0.10827764123678207
CurrentTrain: epoch  9, batch    39 | loss: 4.0460916Losses:  4.02590799331665 0.06974594295024872
CurrentTrain: epoch  9, batch    40 | loss: 4.0956540Losses:  3.9323792457580566 0.04896584153175354
CurrentTrain: epoch  9, batch    41 | loss: 3.9813452Losses:  3.9585399627685547 0.05946527421474457
CurrentTrain: epoch  9, batch    42 | loss: 4.0180054Losses:  3.934389114379883 0.05000051110982895
CurrentTrain: epoch  9, batch    43 | loss: 3.9843895Losses:  3.9465670585632324 0.06779661774635315
CurrentTrain: epoch  9, batch    44 | loss: 4.0143638Losses:  3.9305782318115234 0.09805803000926971
CurrentTrain: epoch  9, batch    45 | loss: 4.0286365Losses:  3.9214372634887695 0.06081186607480049
CurrentTrain: epoch  9, batch    46 | loss: 3.9822490Losses:  3.9630327224731445 0.0672408938407898
CurrentTrain: epoch  9, batch    47 | loss: 4.0302734Losses:  3.9459667205810547 0.07968339323997498
CurrentTrain: epoch  9, batch    48 | loss: 4.0256500Losses:  4.012633323669434 0.11390029639005661
CurrentTrain: epoch  9, batch    49 | loss: 4.1265335Losses:  3.959496259689331 0.09673254191875458
CurrentTrain: epoch  9, batch    50 | loss: 4.0562286Losses:  3.9643263816833496 0.09772709012031555
CurrentTrain: epoch  9, batch    51 | loss: 4.0620537Losses:  3.954333782196045 0.06415434181690216
CurrentTrain: epoch  9, batch    52 | loss: 4.0184879Losses:  3.919867992401123 0.0940224677324295
CurrentTrain: epoch  9, batch    53 | loss: 4.0138903Losses:  3.9432477951049805 0.08606867492198944
CurrentTrain: epoch  9, batch    54 | loss: 4.0293164Losses:  3.9374544620513916 0.06714838743209839
CurrentTrain: epoch  9, batch    55 | loss: 4.0046029Losses:  3.9066402912139893 0.09494754672050476
CurrentTrain: epoch  9, batch    56 | loss: 4.0015879Losses:  3.9431138038635254 0.11230171471834183
CurrentTrain: epoch  9, batch    57 | loss: 4.0554156Losses:  3.911954402923584 0.0783524289727211
CurrentTrain: epoch  9, batch    58 | loss: 3.9903069Losses:  3.914698600769043 0.0905543863773346
CurrentTrain: epoch  9, batch    59 | loss: 4.0052528Losses:  3.9560039043426514 0.09098021686077118
CurrentTrain: epoch  9, batch    60 | loss: 4.0469842Losses:  3.941004753112793 0.08375830948352814
CurrentTrain: epoch  9, batch    61 | loss: 4.0247631Losses:  3.948535680770874 0.035959966480731964
CurrentTrain: epoch  9, batch    62 | loss: 3.9844956
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  3.9120230674743652 1.3515336513519287
CurrentTrain: epoch  0, batch     0 | loss: 5.2635565Losses:  9.17744255065918 1.2479760646820068
CurrentTrain: epoch  0, batch     1 | loss: 10.4254189Losses:  6.1096343994140625 1.4617291688919067
CurrentTrain: epoch  0, batch     2 | loss: 7.5713634Losses:  5.628641605377197 5.960465188081798e-08
CurrentTrain: epoch  0, batch     3 | loss: 5.6286416Losses:  2.137495756149292 1.223100185394287
CurrentTrain: epoch  1, batch     0 | loss: 3.3605959Losses:  2.184502363204956 1.2873295545578003
CurrentTrain: epoch  1, batch     1 | loss: 3.4718318Losses:  2.318178653717041 1.2303067445755005
CurrentTrain: epoch  1, batch     2 | loss: 3.5484853Losses:  1.7165822982788086 0.0843721553683281
CurrentTrain: epoch  1, batch     3 | loss: 1.8009545Losses:  2.0301132202148438 0.9113553166389465
CurrentTrain: epoch  2, batch     0 | loss: 2.9414685Losses:  1.988852858543396 1.0517737865447998
CurrentTrain: epoch  2, batch     1 | loss: 3.0406265Losses:  2.0978410243988037 0.8388234972953796
CurrentTrain: epoch  2, batch     2 | loss: 2.9366646Losses:  2.838623523712158 0.6555812358856201
CurrentTrain: epoch  2, batch     3 | loss: 3.4942048Losses:  2.0676918029785156 1.1044905185699463
CurrentTrain: epoch  3, batch     0 | loss: 3.1721823Losses:  1.9557856321334839 0.7065086364746094
CurrentTrain: epoch  3, batch     1 | loss: 2.6622944Losses:  1.878269076347351 1.1719982624053955
CurrentTrain: epoch  3, batch     2 | loss: 3.0502672Losses:  2.168846368789673 0.2553154230117798
CurrentTrain: epoch  3, batch     3 | loss: 2.4241619Losses:  1.9494743347167969 0.7409669160842896
CurrentTrain: epoch  4, batch     0 | loss: 2.6904411Losses:  1.8948254585266113 0.793022871017456
CurrentTrain: epoch  4, batch     1 | loss: 2.6878483Losses:  1.8917946815490723 0.849448025226593
CurrentTrain: epoch  4, batch     2 | loss: 2.7412426Losses:  1.7946205139160156 0.0361776202917099
CurrentTrain: epoch  4, batch     3 | loss: 1.8307981Losses:  1.8862826824188232 0.6715470552444458
CurrentTrain: epoch  5, batch     0 | loss: 2.5578299Losses:  1.764976143836975 0.7834358215332031
CurrentTrain: epoch  5, batch     1 | loss: 2.5484118Losses:  1.8159315586090088 0.849723219871521
CurrentTrain: epoch  5, batch     2 | loss: 2.6656547Losses:  1.9286367893218994 0.026052478700876236
CurrentTrain: epoch  5, batch     3 | loss: 1.9546893Losses:  1.7980766296386719 0.822210967540741
CurrentTrain: epoch  6, batch     0 | loss: 2.6202877Losses:  1.7934584617614746 0.6838671565055847
CurrentTrain: epoch  6, batch     1 | loss: 2.4773257Losses:  1.7518799304962158 0.6813770532608032
CurrentTrain: epoch  6, batch     2 | loss: 2.4332571Losses:  1.7495126724243164 0.0
CurrentTrain: epoch  6, batch     3 | loss: 1.7495127Losses:  1.7703804969787598 0.7080506682395935
CurrentTrain: epoch  7, batch     0 | loss: 2.4784312Losses:  1.760280728340149 0.7839033603668213
CurrentTrain: epoch  7, batch     1 | loss: 2.5441842Losses:  1.736133098602295 0.6542825698852539
CurrentTrain: epoch  7, batch     2 | loss: 2.3904157Losses:  1.709428310394287 0.0
CurrentTrain: epoch  7, batch     3 | loss: 1.7094283Losses:  1.6982170343399048 0.6260278224945068
CurrentTrain: epoch  8, batch     0 | loss: 2.3242450Losses:  1.7853307723999023 0.6913996934890747
CurrentTrain: epoch  8, batch     1 | loss: 2.4767303Losses:  1.716892957687378 0.6378264427185059
CurrentTrain: epoch  8, batch     2 | loss: 2.3547194Losses:  1.6994829177856445 0.03378267586231232
CurrentTrain: epoch  8, batch     3 | loss: 1.7332656Losses:  1.6804603338241577 0.6042873859405518
CurrentTrain: epoch  9, batch     0 | loss: 2.2847476Losses:  1.7301753759384155 0.5920345187187195
CurrentTrain: epoch  9, batch     1 | loss: 2.3222098Losses:  1.7203996181488037 0.5784741640090942
CurrentTrain: epoch  9, batch     2 | loss: 2.2988739Losses:  1.6908049583435059 0.017986714839935303
CurrentTrain: epoch  9, batch     3 | loss: 1.7087917
Losses:  2.995732307434082 0.9180451035499573
MemoryTrain:  epoch  0, batch     0 | loss: 3.9137774Losses:  10.393885612487793 0.054753921926021576
MemoryTrain:  epoch  0, batch     1 | loss: 10.4486399Losses:  0.010392551310360432 0.7006391286849976
MemoryTrain:  epoch  1, batch     0 | loss: 0.7110317Losses:  0.04707249626517296 0.529637336730957
MemoryTrain:  epoch  1, batch     1 | loss: 0.5767098Losses:  0.009629979729652405 0.6053757667541504
MemoryTrain:  epoch  2, batch     0 | loss: 0.6150057Losses:  0.008016133680939674 0.23087090253829956
MemoryTrain:  epoch  2, batch     1 | loss: 0.2388870Losses:  0.0062963226810097694 0.6046316623687744
MemoryTrain:  epoch  3, batch     0 | loss: 0.6109280Losses:  0.0017332741990685463 0.2540799379348755
MemoryTrain:  epoch  3, batch     1 | loss: 0.2558132Losses:  0.005162213463336229 0.6748505234718323
MemoryTrain:  epoch  4, batch     0 | loss: 0.6800128Losses:  0.003034012857824564 0.11737287044525146
MemoryTrain:  epoch  4, batch     1 | loss: 0.1204069Losses:  0.004957921337336302 0.6375039219856262
MemoryTrain:  epoch  5, batch     0 | loss: 0.6424618Losses:  0.0028188053984194994 0.045361824333667755
MemoryTrain:  epoch  5, batch     1 | loss: 0.0481806Losses:  0.006156889721751213 0.6105836629867554
MemoryTrain:  epoch  6, batch     0 | loss: 0.6167405Losses:  0.0007194848731160164 0.13160552084445953
MemoryTrain:  epoch  6, batch     1 | loss: 0.1323250Losses:  0.005019410979002714 0.6027025580406189
MemoryTrain:  epoch  7, batch     0 | loss: 0.6077220Losses:  0.001073489198461175 0.10806722193956375
MemoryTrain:  epoch  7, batch     1 | loss: 0.1091407Losses:  0.00321359746158123 0.5716325044631958
MemoryTrain:  epoch  8, batch     0 | loss: 0.5748461Losses:  0.0035002408549189568 0.21881353855133057
MemoryTrain:  epoch  8, batch     1 | loss: 0.2223138Losses:  0.00268650334328413 0.4901297092437744
MemoryTrain:  epoch  9, batch     0 | loss: 0.4928162Losses:  0.003407485317438841 0.04531721770763397
MemoryTrain:  epoch  9, batch     1 | loss: 0.0487247
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 84.54%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 75.20%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 68.29%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 64.49%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 64.32%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 63.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.82%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 90.99%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.54%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 91.47%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 91.27%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.42%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.63%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.82%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 91.85%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.99%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.78%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 91.64%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 91.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 91.04%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 90.43%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 89.92%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 89.38%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 89.43%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 89.19%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 89.10%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 88.72%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 88.64%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 88.27%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 87.85%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 87.23%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 86.63%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 86.10%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 85.72%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 85.29%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.05%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 84.63%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 84.41%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 83.94%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 83.42%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 82.97%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 82.40%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 82.03%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 81.67%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 81.31%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 80.90%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 80.50%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 80.10%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 79.77%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 79.39%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 79.24%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.74%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.03%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.20%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 80.37%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.48%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.70%   
cur_acc:  ['0.9435', '0.6935']
his_acc:  ['0.9435', '0.8070']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  3.9120230674743652 1.6151022911071777
CurrentTrain: epoch  0, batch     0 | loss: 5.5271254Losses:  8.882802963256836 1.609508752822876
CurrentTrain: epoch  0, batch     1 | loss: 10.4923115Losses:  5.843964099884033 1.800708532333374
CurrentTrain: epoch  0, batch     2 | loss: 7.6446724Losses:  3.9565987586975098 0.3927033543586731
CurrentTrain: epoch  0, batch     3 | loss: 4.3493023Losses:  2.5554051399230957 1.4848105907440186
CurrentTrain: epoch  1, batch     0 | loss: 4.0402155Losses:  3.599008560180664 1.5112601518630981
CurrentTrain: epoch  1, batch     1 | loss: 5.1102686Losses:  2.665445327758789 1.4730629920959473
CurrentTrain: epoch  1, batch     2 | loss: 4.1385083Losses:  3.1399240493774414 0.212510883808136
CurrentTrain: epoch  1, batch     3 | loss: 3.3524349Losses:  2.4485456943511963 1.0212290287017822
CurrentTrain: epoch  2, batch     0 | loss: 3.4697747Losses:  3.008613348007202 1.4011802673339844
CurrentTrain: epoch  2, batch     1 | loss: 4.4097939Losses:  3.3640341758728027 1.4487709999084473
CurrentTrain: epoch  2, batch     2 | loss: 4.8128052Losses:  1.9475293159484863 0.1081569492816925
CurrentTrain: epoch  2, batch     3 | loss: 2.0556862Losses:  2.8083767890930176 1.2513787746429443
CurrentTrain: epoch  3, batch     0 | loss: 4.0597553Losses:  2.516604423522949 1.078627347946167
CurrentTrain: epoch  3, batch     1 | loss: 3.5952318Losses:  3.037137985229492 1.2476615905761719
CurrentTrain: epoch  3, batch     2 | loss: 4.2847996Losses:  2.5634450912475586 0.14467257261276245
CurrentTrain: epoch  3, batch     3 | loss: 2.7081177Losses:  2.764366626739502 1.0888957977294922
CurrentTrain: epoch  4, batch     0 | loss: 3.8532624Losses:  2.6843419075012207 1.1771790981292725
CurrentTrain: epoch  4, batch     1 | loss: 3.8615210Losses:  2.207274913787842 0.989980936050415
CurrentTrain: epoch  4, batch     2 | loss: 3.1972558Losses:  1.956552267074585 0.12524215877056122
CurrentTrain: epoch  4, batch     3 | loss: 2.0817945Losses:  2.6693410873413086 1.003159523010254
CurrentTrain: epoch  5, batch     0 | loss: 3.6725006Losses:  2.145580291748047 0.8480967283248901
CurrentTrain: epoch  5, batch     1 | loss: 2.9936771Losses:  2.1964502334594727 0.8044922947883606
CurrentTrain: epoch  5, batch     2 | loss: 3.0009425Losses:  3.7581968307495117 0.43913114070892334
CurrentTrain: epoch  5, batch     3 | loss: 4.1973281Losses:  2.425903797149658 1.1644248962402344
CurrentTrain: epoch  6, batch     0 | loss: 3.5903287Losses:  2.420248508453369 1.0291879177093506
CurrentTrain: epoch  6, batch     1 | loss: 3.4494364Losses:  2.093217372894287 0.8108208179473877
CurrentTrain: epoch  6, batch     2 | loss: 2.9040382Losses:  1.9047857522964478 0.14068686962127686
CurrentTrain: epoch  6, batch     3 | loss: 2.0454726Losses:  1.9676003456115723 0.8575299978256226
CurrentTrain: epoch  7, batch     0 | loss: 2.8251305Losses:  2.1819210052490234 0.8998802900314331
CurrentTrain: epoch  7, batch     1 | loss: 3.0818014Losses:  2.2247726917266846 0.8881207704544067
CurrentTrain: epoch  7, batch     2 | loss: 3.1128936Losses:  2.057352066040039 0.551817774772644
CurrentTrain: epoch  7, batch     3 | loss: 2.6091700Losses:  1.9759557247161865 1.0232279300689697
CurrentTrain: epoch  8, batch     0 | loss: 2.9991837Losses:  2.0813393592834473 0.9425095319747925
CurrentTrain: epoch  8, batch     1 | loss: 3.0238490Losses:  1.9979082345962524 0.701600193977356
CurrentTrain: epoch  8, batch     2 | loss: 2.6995084Losses:  1.7386043071746826 0.02344321832060814
CurrentTrain: epoch  8, batch     3 | loss: 1.7620475Losses:  1.9254710674285889 0.7899932265281677
CurrentTrain: epoch  9, batch     0 | loss: 2.7154644Losses:  1.8037946224212646 0.5982984900474548
CurrentTrain: epoch  9, batch     1 | loss: 2.4020932Losses:  2.008521556854248 0.8606046438217163
CurrentTrain: epoch  9, batch     2 | loss: 2.8691263Losses:  1.8369381427764893 0.4203331470489502
CurrentTrain: epoch  9, batch     3 | loss: 2.2572713
Losses:  3.4011974334716797 0.8955879211425781
MemoryTrain:  epoch  0, batch     0 | loss: 4.2967854Losses:  10.92042350769043 0.8977562189102173
MemoryTrain:  epoch  0, batch     1 | loss: 11.8181801Losses:  0.02390095591545105 0.8887914419174194
MemoryTrain:  epoch  1, batch     0 | loss: 0.9126924Losses:  0.005000259727239609 0.660210907459259
MemoryTrain:  epoch  1, batch     1 | loss: 0.6652111Losses:  0.008706054650247097 0.6186169385910034
MemoryTrain:  epoch  2, batch     0 | loss: 0.6273230Losses:  0.022019198164343834 0.816054105758667
MemoryTrain:  epoch  2, batch     1 | loss: 0.8380733Losses:  0.013494826853275299 0.6774412989616394
MemoryTrain:  epoch  3, batch     0 | loss: 0.6909361Losses:  0.007184252608567476 0.6609247922897339
MemoryTrain:  epoch  3, batch     1 | loss: 0.6681091Losses:  0.008908173069357872 0.840190052986145
MemoryTrain:  epoch  4, batch     0 | loss: 0.8490982Losses:  0.006118508987128735 0.45504826307296753
MemoryTrain:  epoch  4, batch     1 | loss: 0.4611668Losses:  0.01087932102382183 0.7154586315155029
MemoryTrain:  epoch  5, batch     0 | loss: 0.7263380Losses:  0.008669547736644745 0.5759117603302002
MemoryTrain:  epoch  5, batch     1 | loss: 0.5845813Losses:  0.0038842917419970036 0.5599935054779053
MemoryTrain:  epoch  6, batch     0 | loss: 0.5638778Losses:  0.007398419547826052 0.6214908957481384
MemoryTrain:  epoch  6, batch     1 | loss: 0.6288893Losses:  0.004232201725244522 0.5262256860733032
MemoryTrain:  epoch  7, batch     0 | loss: 0.5304579Losses:  0.009248882532119751 0.6644553542137146
MemoryTrain:  epoch  7, batch     1 | loss: 0.6737043Losses:  0.007398003712296486 0.7136315703392029
MemoryTrain:  epoch  8, batch     0 | loss: 0.7210296Losses:  0.006674745120108128 0.40435999631881714
MemoryTrain:  epoch  8, batch     1 | loss: 0.4110347Losses:  0.007709152065217495 0.6839094161987305
MemoryTrain:  epoch  9, batch     0 | loss: 0.6916186Losses:  0.005769415758550167 0.42061856389045715
MemoryTrain:  epoch  9, batch     1 | loss: 0.4263880
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 51.64%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 59.26%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 57.37%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 53.02%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 53.32%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 54.36%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 55.33%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.43%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 57.29%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 58.28%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 64.13%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 63.43%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 63.02%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 62.75%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 62.62%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 62.73%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 62.84%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 62.95%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 63.05%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 63.15%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 63.42%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 63.51%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 63.00%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 88.35%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.12%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 90.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.53%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 89.89%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.09%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.59%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 91.21%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.47%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 91.70%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.73%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 91.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.84%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 91.87%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 91.81%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 91.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.45%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 90.87%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 90.59%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 90.23%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 89.97%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 89.56%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 89.01%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 88.62%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 88.24%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 87.86%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 87.43%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 87.14%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 87.01%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 86.81%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 86.33%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 86.01%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 85.42%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 84.84%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 84.47%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 84.05%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.83%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 83.35%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 83.21%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 82.75%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 82.55%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 82.41%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 82.28%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 82.15%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 81.90%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 81.66%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 81.19%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 80.73%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 80.34%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 79.95%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 79.69%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 79.54%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.71%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 80.01%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 80.52%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 80.53%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 80.58%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 80.64%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 80.54%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 80.55%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 79.96%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 79.43%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 79.05%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 78.73%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 78.41%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 78.15%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 78.45%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.56%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 78.28%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.90%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 77.53%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 77.24%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 77.01%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.74%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 77.24%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 76.77%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 76.31%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 75.97%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 75.52%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 75.08%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 75.08%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 76.05%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.41%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 76.18%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 76.13%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 75.87%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 75.61%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 75.46%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 75.39%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 75.21%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.17%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 75.14%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 75.10%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 75.07%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 74.93%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 74.93%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.90%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 74.67%   
cur_acc:  ['0.9435', '0.6935', '0.6300']
his_acc:  ['0.9435', '0.8070', '0.7467']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  3.9120230674743652 1.7509863376617432
CurrentTrain: epoch  0, batch     0 | loss: 5.6630096Losses:  8.468265533447266 1.518315315246582
CurrentTrain: epoch  0, batch     1 | loss: 9.9865808Losses:  7.246460437774658 1.6651573181152344
CurrentTrain: epoch  0, batch     2 | loss: 8.9116173Losses:  3.672924041748047 0.22057735919952393
CurrentTrain: epoch  0, batch     3 | loss: 3.8935013Losses:  2.3326568603515625 1.4019112586975098
CurrentTrain: epoch  1, batch     0 | loss: 3.7345681Losses:  2.1245412826538086 1.1231613159179688
CurrentTrain: epoch  1, batch     1 | loss: 3.2477026Losses:  2.339585781097412 1.6155182123184204
CurrentTrain: epoch  1, batch     2 | loss: 3.9551039Losses:  1.8304955959320068 0.048789024353027344
CurrentTrain: epoch  1, batch     3 | loss: 1.8792846Losses:  2.6500298976898193 1.385223388671875
CurrentTrain: epoch  2, batch     0 | loss: 4.0352535Losses:  2.2657580375671387 1.2804086208343506
CurrentTrain: epoch  2, batch     1 | loss: 3.5461667Losses:  1.915703296661377 0.9134436845779419
CurrentTrain: epoch  2, batch     2 | loss: 2.8291469Losses:  1.89723801612854 0.06708436459302902
CurrentTrain: epoch  2, batch     3 | loss: 1.9643223Losses:  2.1942763328552246 0.8904181718826294
CurrentTrain: epoch  3, batch     0 | loss: 3.0846944Losses:  2.0734567642211914 1.3297538757324219
CurrentTrain: epoch  3, batch     1 | loss: 3.4032106Losses:  1.9867522716522217 1.0917105674743652
CurrentTrain: epoch  3, batch     2 | loss: 3.0784628Losses:  1.7548779249191284 0.09269824624061584
CurrentTrain: epoch  3, batch     3 | loss: 1.8475761Losses:  1.9745757579803467 0.8860737085342407
CurrentTrain: epoch  4, batch     0 | loss: 2.8606496Losses:  1.8500385284423828 0.7779724597930908
CurrentTrain: epoch  4, batch     1 | loss: 2.6280110Losses:  2.103956699371338 0.5974708199501038
CurrentTrain: epoch  4, batch     2 | loss: 2.7014275Losses:  1.9023191928863525 0.11701850593090057
CurrentTrain: epoch  4, batch     3 | loss: 2.0193377Losses:  2.033527135848999 0.9226678609848022
CurrentTrain: epoch  5, batch     0 | loss: 2.9561949Losses:  1.789605736732483 0.7314608097076416
CurrentTrain: epoch  5, batch     1 | loss: 2.5210667Losses:  1.7930229902267456 0.8246972560882568
CurrentTrain: epoch  5, batch     2 | loss: 2.6177201Losses:  1.7802534103393555 0.0
CurrentTrain: epoch  5, batch     3 | loss: 1.7802534Losses:  1.7851181030273438 0.5072199106216431
CurrentTrain: epoch  6, batch     0 | loss: 2.2923379Losses:  1.788719654083252 0.6588889360427856
CurrentTrain: epoch  6, batch     1 | loss: 2.4476085Losses:  1.8204784393310547 0.7392905354499817
CurrentTrain: epoch  6, batch     2 | loss: 2.5597689Losses:  1.6969540119171143 0.1679711788892746
CurrentTrain: epoch  6, batch     3 | loss: 1.8649251Losses:  1.7311511039733887 0.49590662121772766
CurrentTrain: epoch  7, batch     0 | loss: 2.2270577Losses:  1.7268788814544678 0.6756407022476196
CurrentTrain: epoch  7, batch     1 | loss: 2.4025197Losses:  1.747964859008789 0.7304989695549011
CurrentTrain: epoch  7, batch     2 | loss: 2.4784639Losses:  1.7044627666473389 0.2590025067329407
CurrentTrain: epoch  7, batch     3 | loss: 1.9634652Losses:  1.7386164665222168 0.5727949142456055
CurrentTrain: epoch  8, batch     0 | loss: 2.3114114Losses:  1.7209274768829346 0.4856405258178711
CurrentTrain: epoch  8, batch     1 | loss: 2.2065680Losses:  1.706571102142334 0.45843279361724854
CurrentTrain: epoch  8, batch     2 | loss: 2.1650038Losses:  1.7509753704071045 0.12588045001029968
CurrentTrain: epoch  8, batch     3 | loss: 1.8768559Losses:  1.6962045431137085 0.5456677675247192
CurrentTrain: epoch  9, batch     0 | loss: 2.2418723Losses:  1.722734808921814 0.4479140043258667
CurrentTrain: epoch  9, batch     1 | loss: 2.1706488Losses:  1.6775026321411133 0.4272412359714508
CurrentTrain: epoch  9, batch     2 | loss: 2.1047440Losses:  1.6814745664596558 0.1358814239501953
CurrentTrain: epoch  9, batch     3 | loss: 1.8173560
Losses:  3.6888794898986816 0.756432294845581
MemoryTrain:  epoch  0, batch     0 | loss: 4.4453115Losses:  10.583404541015625 0.5161530375480652
MemoryTrain:  epoch  0, batch     1 | loss: 11.0995579Losses:  11.414937973022461 0.4082537293434143
MemoryTrain:  epoch  0, batch     2 | loss: 11.8231916Losses:  0.010333199054002762 0.5975186824798584
MemoryTrain:  epoch  1, batch     0 | loss: 0.6078519Losses:  0.004034167155623436 0.49149084091186523
MemoryTrain:  epoch  1, batch     1 | loss: 0.4955250Losses:  0.008016105741262436 0.4969848096370697
MemoryTrain:  epoch  1, batch     2 | loss: 0.5050009Losses:  0.005497271195054054 0.6408394575119019
MemoryTrain:  epoch  2, batch     0 | loss: 0.6463367Losses:  0.0033419744577258825 0.6153382062911987
MemoryTrain:  epoch  2, batch     1 | loss: 0.6186802Losses:  0.009442431852221489 0.43712085485458374
MemoryTrain:  epoch  2, batch     2 | loss: 0.4465633Losses:  0.005381076596677303 0.7333176136016846
MemoryTrain:  epoch  3, batch     0 | loss: 0.7386987Losses:  0.009405027143657207 0.4025236964225769
MemoryTrain:  epoch  3, batch     1 | loss: 0.4119287Losses:  0.006287558004260063 0.3386290371417999
MemoryTrain:  epoch  3, batch     2 | loss: 0.3449166Losses:  0.005893633235245943 0.6482685804367065
MemoryTrain:  epoch  4, batch     0 | loss: 0.6541622Losses:  0.0040293787606060505 0.5762410163879395
MemoryTrain:  epoch  4, batch     1 | loss: 0.5802704Losses:  0.0065758503042161465 0.34893879294395447
MemoryTrain:  epoch  4, batch     2 | loss: 0.3555146Losses:  0.004755045287311077 0.5884682536125183
MemoryTrain:  epoch  5, batch     0 | loss: 0.5932233Losses:  0.007088160142302513 0.713750958442688
MemoryTrain:  epoch  5, batch     1 | loss: 0.7208391Losses:  0.0020169210620224476 0.27202683687210083
MemoryTrain:  epoch  5, batch     2 | loss: 0.2740438Losses:  0.005258683115243912 0.4821697771549225
MemoryTrain:  epoch  6, batch     0 | loss: 0.4874285Losses:  0.004440308082848787 0.5457285046577454
MemoryTrain:  epoch  6, batch     1 | loss: 0.5501688Losses:  0.006949145346879959 0.38358861207962036
MemoryTrain:  epoch  6, batch     2 | loss: 0.3905378Losses:  0.0024557081051170826 0.45448362827301025
MemoryTrain:  epoch  7, batch     0 | loss: 0.4569393Losses:  0.004680149722844362 0.5159077644348145
MemoryTrain:  epoch  7, batch     1 | loss: 0.5205879Losses:  0.01052081398665905 0.3249470889568329
MemoryTrain:  epoch  7, batch     2 | loss: 0.3354679Losses:  0.002909738104790449 0.40541476011276245
MemoryTrain:  epoch  8, batch     0 | loss: 0.4083245Losses:  0.004205223172903061 0.5501506924629211
MemoryTrain:  epoch  8, batch     1 | loss: 0.5543559Losses:  0.006446163170039654 0.3558620810508728
MemoryTrain:  epoch  8, batch     2 | loss: 0.3623082Losses:  0.005371759179979563 0.5857941508293152
MemoryTrain:  epoch  9, batch     0 | loss: 0.5911659Losses:  0.0045259855687618256 0.4095870852470398
MemoryTrain:  epoch  9, batch     1 | loss: 0.4141131Losses:  0.002499133814126253 0.21451781690120697
MemoryTrain:  epoch  9, batch     2 | loss: 0.2170170
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 79.41%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.45%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.48%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 78.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 79.42%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 79.56%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 79.86%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 86.35%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.70%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.79%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 88.56%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.18%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 89.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.93%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.42%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 88.69%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 88.67%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 88.92%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 89.09%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 89.26%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.15%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.04%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.94%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 88.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 88.65%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 88.31%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 87.90%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 87.74%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 87.42%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 87.19%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 86.81%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 86.30%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 85.59%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 85.17%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 84.63%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 84.20%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 83.89%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 83.65%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 83.36%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 82.80%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 82.31%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 81.91%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 81.51%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 81.31%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 80.93%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 80.25%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 79.89%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 79.66%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 79.37%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 79.09%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 78.99%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 78.89%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 78.50%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.89%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 77.41%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 76.88%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 76.41%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 76.06%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.88%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 77.23%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 76.62%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 76.27%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 75.87%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.38%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.28%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 75.68%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 75.27%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 74.87%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 74.25%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 74.04%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.78%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.34%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 73.89%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.45%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 73.13%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 72.70%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 72.25%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 72.73%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 72.74%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 72.59%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 72.35%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.18%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 72.13%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 71.72%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 71.63%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 71.55%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 71.50%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 71.48%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 71.57%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 71.42%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 71.40%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 71.32%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 71.81%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 71.73%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 71.64%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 71.52%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 71.37%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 71.26%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 71.22%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 70.97%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 70.84%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 70.74%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 70.37%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 71.28%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 71.58%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  224 | acc: 75.00%,  total acc: 71.64%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 73.01%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 73.04%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 73.02%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 73.58%   
cur_acc:  ['0.9435', '0.6935', '0.6300', '0.7986']
his_acc:  ['0.9435', '0.8070', '0.7467', '0.7358']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  3.9120230674743652 1.2873022556304932
CurrentTrain: epoch  0, batch     0 | loss: 5.1993256Losses:  8.346909523010254 1.4346977472305298
CurrentTrain: epoch  0, batch     1 | loss: 9.7816076Losses:  6.727032661437988 1.143799066543579
CurrentTrain: epoch  0, batch     2 | loss: 7.8708315Losses:  5.608397483825684 8.94069742685133e-08
CurrentTrain: epoch  0, batch     3 | loss: 5.6083975Losses:  2.516704797744751 1.1348192691802979
CurrentTrain: epoch  1, batch     0 | loss: 3.6515241Losses:  2.7435812950134277 1.3251436948776245
CurrentTrain: epoch  1, batch     1 | loss: 4.0687251Losses:  2.9326748847961426 1.2243616580963135
CurrentTrain: epoch  1, batch     2 | loss: 4.1570368Losses:  2.6801533699035645 0.3127080500125885
CurrentTrain: epoch  1, batch     3 | loss: 2.9928615Losses:  2.662193775177002 1.3395628929138184
CurrentTrain: epoch  2, batch     0 | loss: 4.0017567Losses:  2.9873158931732178 1.2444736957550049
CurrentTrain: epoch  2, batch     1 | loss: 4.2317896Losses:  2.4999492168426514 1.0727715492248535
CurrentTrain: epoch  2, batch     2 | loss: 3.5727208Losses:  2.892313241958618 0.05141134187579155
CurrentTrain: epoch  2, batch     3 | loss: 2.9437246Losses:  2.7753491401672363 1.0110610723495483
CurrentTrain: epoch  3, batch     0 | loss: 3.7864103Losses:  2.357388734817505 0.8254766464233398
CurrentTrain: epoch  3, batch     1 | loss: 3.1828654Losses:  2.613074541091919 1.0520033836364746
CurrentTrain: epoch  3, batch     2 | loss: 3.6650779Losses:  2.110330104827881 0.40558162331581116
CurrentTrain: epoch  3, batch     3 | loss: 2.5159118Losses:  2.6849794387817383 0.7917351722717285
CurrentTrain: epoch  4, batch     0 | loss: 3.4767146Losses:  2.3423986434936523 0.9793573617935181
CurrentTrain: epoch  4, batch     1 | loss: 3.3217559Losses:  2.3459420204162598 0.9089524745941162
CurrentTrain: epoch  4, batch     2 | loss: 3.2548945Losses:  1.6334046125411987 0.08057934045791626
CurrentTrain: epoch  4, batch     3 | loss: 1.7139840Losses:  2.1181604862213135 0.9701846837997437
CurrentTrain: epoch  5, batch     0 | loss: 3.0883451Losses:  2.1104376316070557 0.8026788830757141
CurrentTrain: epoch  5, batch     1 | loss: 2.9131165Losses:  2.2242815494537354 0.8835349082946777
CurrentTrain: epoch  5, batch     2 | loss: 3.1078165Losses:  3.6257429122924805 0.3402058184146881
CurrentTrain: epoch  5, batch     3 | loss: 3.9659488Losses:  2.2827987670898438 0.7443350553512573
CurrentTrain: epoch  6, batch     0 | loss: 3.0271339Losses:  2.048492670059204 0.6356083154678345
CurrentTrain: epoch  6, batch     1 | loss: 2.6841011Losses:  2.0494673252105713 0.9061060547828674
CurrentTrain: epoch  6, batch     2 | loss: 2.9555733Losses:  1.7037677764892578 0.18897107243537903
CurrentTrain: epoch  6, batch     3 | loss: 1.8927388Losses:  2.070230722427368 0.8208026885986328
CurrentTrain: epoch  7, batch     0 | loss: 2.8910334Losses:  1.9927942752838135 0.7129334211349487
CurrentTrain: epoch  7, batch     1 | loss: 2.7057276Losses:  1.894834041595459 0.7225854992866516
CurrentTrain: epoch  7, batch     2 | loss: 2.6174195Losses:  1.8162841796875 0.3838838040828705
CurrentTrain: epoch  7, batch     3 | loss: 2.2001679Losses:  1.8281176090240479 0.7826797962188721
CurrentTrain: epoch  8, batch     0 | loss: 2.6107974Losses:  1.952379822731018 0.6975544691085815
CurrentTrain: epoch  8, batch     1 | loss: 2.6499343Losses:  2.066941261291504 0.5864776372909546
CurrentTrain: epoch  8, batch     2 | loss: 2.6534190Losses:  1.8079729080200195 0.10282302647829056
CurrentTrain: epoch  8, batch     3 | loss: 1.9107959Losses:  1.8925946950912476 0.6861165165901184
CurrentTrain: epoch  9, batch     0 | loss: 2.5787113Losses:  1.7716970443725586 0.5873103141784668
CurrentTrain: epoch  9, batch     1 | loss: 2.3590074Losses:  2.0552427768707275 0.6245883703231812
CurrentTrain: epoch  9, batch     2 | loss: 2.6798310Losses:  1.667288064956665 0.038016706705093384
CurrentTrain: epoch  9, batch     3 | loss: 1.7053047
Losses:  3.9120230674743652 0.569566011428833
MemoryTrain:  epoch  0, batch     0 | loss: 4.4815893Losses:  10.198221206665039 0.7269981503486633
MemoryTrain:  epoch  0, batch     1 | loss: 10.9252195Losses:  11.24501895904541 0.6166390180587769
MemoryTrain:  epoch  0, batch     2 | loss: 11.8616581Losses:  11.82679557800293 0.044638801366090775
MemoryTrain:  epoch  0, batch     3 | loss: 11.8714342Losses:  0.014924135059118271 0.5817120671272278
MemoryTrain:  epoch  1, batch     0 | loss: 0.5966362Losses:  0.016455430537462234 0.734567403793335
MemoryTrain:  epoch  1, batch     1 | loss: 0.7510228Losses:  0.009963486343622208 0.6523894667625427
MemoryTrain:  epoch  1, batch     2 | loss: 0.6623530Losses:  0.0006977380835451186 0.020758580416440964
MemoryTrain:  epoch  1, batch     3 | loss: 0.0214563Losses:  0.00964854471385479 0.6684703826904297
MemoryTrain:  epoch  2, batch     0 | loss: 0.6781189Losses:  0.008650279603898525 0.6503335237503052
MemoryTrain:  epoch  2, batch     1 | loss: 0.6589838Losses:  0.010214608162641525 0.5562700629234314
MemoryTrain:  epoch  2, batch     2 | loss: 0.5664847Losses:  0.020555954426527023 0.3920131325721741
MemoryTrain:  epoch  2, batch     3 | loss: 0.4125691Losses:  0.005328268278390169 0.43359750509262085
MemoryTrain:  epoch  3, batch     0 | loss: 0.4389258Losses:  0.007274821400642395 0.9264835715293884
MemoryTrain:  epoch  3, batch     1 | loss: 0.9337584Losses:  0.013505169190466404 0.641958475112915
MemoryTrain:  epoch  3, batch     2 | loss: 0.6554636Losses:  0.002039405284449458 0.21831896901130676
MemoryTrain:  epoch  3, batch     3 | loss: 0.2203584Losses:  0.011967075988650322 0.610468327999115
MemoryTrain:  epoch  4, batch     0 | loss: 0.6224354Losses:  0.004703742917627096 0.7322713136672974
MemoryTrain:  epoch  4, batch     1 | loss: 0.7369751Losses:  0.015972714871168137 0.5988000631332397
MemoryTrain:  epoch  4, batch     2 | loss: 0.6147728Losses:  0.001693354919552803 0.021751943975687027
MemoryTrain:  epoch  4, batch     3 | loss: 0.0234453Losses:  0.013578135520219803 0.4888715147972107
MemoryTrain:  epoch  5, batch     0 | loss: 0.5024496Losses:  0.008473331108689308 0.6126129031181335
MemoryTrain:  epoch  5, batch     1 | loss: 0.6210862Losses:  0.0069137196987867355 0.7078373432159424
MemoryTrain:  epoch  5, batch     2 | loss: 0.7147511Losses:  0.00512884883210063 0.0033431686460971832
MemoryTrain:  epoch  5, batch     3 | loss: 0.0084720Losses:  0.008973874151706696 0.6130308508872986
MemoryTrain:  epoch  6, batch     0 | loss: 0.6220047Losses:  0.006044290028512478 0.6197150945663452
MemoryTrain:  epoch  6, batch     1 | loss: 0.6257594Losses:  0.009898215532302856 0.5475759506225586
MemoryTrain:  epoch  6, batch     2 | loss: 0.5574741Losses:  0.00711579667404294 0.013241758570075035
MemoryTrain:  epoch  6, batch     3 | loss: 0.0203576Losses:  0.01564250886440277 0.5906496644020081
MemoryTrain:  epoch  7, batch     0 | loss: 0.6062922Losses:  0.00345904310233891 0.4771605432033539
MemoryTrain:  epoch  7, batch     1 | loss: 0.4806196Losses:  0.010305196978151798 0.6073588132858276
MemoryTrain:  epoch  7, batch     2 | loss: 0.6176640Losses:  0.003631712170317769 0.0027769075240939856
MemoryTrain:  epoch  7, batch     3 | loss: 0.0064086Losses:  0.00797104649245739 0.6522791385650635
MemoryTrain:  epoch  8, batch     0 | loss: 0.6602502Losses:  0.007486503571271896 0.4120830297470093
MemoryTrain:  epoch  8, batch     1 | loss: 0.4195695Losses:  0.009340436197817326 0.4663499593734741
MemoryTrain:  epoch  8, batch     2 | loss: 0.4756904Losses:  0.005489087197929621 0.07034613192081451
MemoryTrain:  epoch  8, batch     3 | loss: 0.0758352Losses:  0.008078519254922867 0.5305960774421692
MemoryTrain:  epoch  9, batch     0 | loss: 0.5386746Losses:  0.004862840287387371 0.5615144968032837
MemoryTrain:  epoch  9, batch     1 | loss: 0.5663773Losses:  0.015208842232823372 0.44391345977783203
MemoryTrain:  epoch  9, batch     2 | loss: 0.4591223Losses:  0.006182403303682804 0.0021845200099051
MemoryTrain:  epoch  9, batch     3 | loss: 0.0083669
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 65.21%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 63.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 61.81%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 60.64%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 60.86%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 61.22%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 62.65%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.83%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 86.99%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.13%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 86.81%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 86.27%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 86.07%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 85.78%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 85.59%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 85.66%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 85.91%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 85.96%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 86.21%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.62%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 86.37%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 86.30%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 86.23%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 85.83%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 85.03%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 84.42%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 83.57%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 82.99%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 82.27%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 81.56%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 80.87%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 80.73%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 80.51%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 80.23%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 79.81%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 79.62%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 79.49%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 79.31%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 79.08%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 78.63%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 78.19%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 77.83%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 77.54%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 77.04%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 76.96%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 76.04%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 75.60%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 75.42%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 75.35%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 74.54%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 74.14%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 73.86%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 73.48%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 73.27%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 74.85%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 75.10%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 75.25%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 74.65%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 74.06%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 73.78%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 73.40%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 73.03%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 72.71%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 72.68%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 73.18%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 72.75%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 72.41%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 72.07%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 71.68%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 71.40%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 71.73%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.73%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 71.34%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 70.96%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 70.74%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.36%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 69.95%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.20%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 70.39%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 70.29%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 70.02%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 69.46%   [EVAL] batch:  176 | acc: 18.75%,  total acc: 69.17%   [EVAL] batch:  177 | acc: 18.75%,  total acc: 68.89%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 68.68%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 68.54%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 68.41%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 68.72%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 69.01%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 69.07%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 68.97%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 68.84%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 68.72%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 68.66%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 68.57%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 68.41%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 68.33%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 68.27%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 67.94%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 67.68%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 67.45%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 67.49%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 68.47%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 68.44%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 70.01%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 69.86%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 69.93%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 70.37%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 70.34%   [EVAL] batch:  252 | acc: 50.00%,  total acc: 70.26%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 70.15%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 70.14%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 69.99%   [EVAL] batch:  258 | acc: 43.75%,  total acc: 69.88%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 69.78%   [EVAL] batch:  260 | acc: 25.00%,  total acc: 69.61%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 69.56%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 69.50%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 70.05%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 70.02%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 69.99%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 69.96%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 69.88%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 69.70%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 69.56%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 69.21%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 69.21%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 69.29%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  297 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 69.67%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.55%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.75%   
cur_acc:  ['0.9435', '0.6935', '0.6300', '0.7986', '0.7183']
his_acc:  ['0.9435', '0.8070', '0.7467', '0.7358', '0.7075']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  3.9120230674743652 1.537118673324585
CurrentTrain: epoch  0, batch     0 | loss: 5.4491415Losses:  9.237981796264648 1.196338415145874
CurrentTrain: epoch  0, batch     1 | loss: 10.4343204Losses:  6.936397552490234 1.3788750171661377
CurrentTrain: epoch  0, batch     2 | loss: 8.3152723Losses:  5.622403621673584 2.9802322387695312e-08
CurrentTrain: epoch  0, batch     3 | loss: 5.6224036Losses:  2.961836099624634 1.5295805931091309
CurrentTrain: epoch  1, batch     0 | loss: 4.4914169Losses:  2.5805766582489014 1.1231509447097778
CurrentTrain: epoch  1, batch     1 | loss: 3.7037277Losses:  2.4280033111572266 1.1579983234405518
CurrentTrain: epoch  1, batch     2 | loss: 3.5860016Losses:  2.3707950115203857 0.35345831513404846
CurrentTrain: epoch  1, batch     3 | loss: 2.7242534Losses:  2.5127792358398438 1.0878643989562988
CurrentTrain: epoch  2, batch     0 | loss: 3.6006436Losses:  2.693385362625122 1.150165319442749
CurrentTrain: epoch  2, batch     1 | loss: 3.8435507Losses:  2.403225898742676 1.037815809249878
CurrentTrain: epoch  2, batch     2 | loss: 3.4410417Losses:  2.9872660636901855 0.3090527653694153
CurrentTrain: epoch  2, batch     3 | loss: 3.2963188Losses:  2.22000789642334 0.862909197807312
CurrentTrain: epoch  3, batch     0 | loss: 3.0829172Losses:  2.658815860748291 1.2164685726165771
CurrentTrain: epoch  3, batch     1 | loss: 3.8752844Losses:  2.329042911529541 1.2003223896026611
CurrentTrain: epoch  3, batch     2 | loss: 3.5293653Losses:  2.0933995246887207 0.15305384993553162
CurrentTrain: epoch  3, batch     3 | loss: 2.2464533Losses:  2.0205507278442383 0.8214653134346008
CurrentTrain: epoch  4, batch     0 | loss: 2.8420160Losses:  2.298920154571533 0.9175351858139038
CurrentTrain: epoch  4, batch     1 | loss: 3.2164555Losses:  2.037294387817383 1.0204691886901855
CurrentTrain: epoch  4, batch     2 | loss: 3.0577636Losses:  1.8192903995513916 0.07184244692325592
CurrentTrain: epoch  4, batch     3 | loss: 1.8911328Losses:  1.9734022617340088 1.0050146579742432
CurrentTrain: epoch  5, batch     0 | loss: 2.9784169Losses:  1.9883818626403809 0.7125111818313599
CurrentTrain: epoch  5, batch     1 | loss: 2.7008929Losses:  1.9241888523101807 0.9171342849731445
CurrentTrain: epoch  5, batch     2 | loss: 2.8413231Losses:  1.891360878944397 0.1573544144630432
CurrentTrain: epoch  5, batch     3 | loss: 2.0487154Losses:  1.8063552379608154 0.7622864246368408
CurrentTrain: epoch  6, batch     0 | loss: 2.5686417Losses:  2.038661479949951 1.0054024457931519
CurrentTrain: epoch  6, batch     1 | loss: 3.0440640Losses:  1.896087408065796 0.9951696991920471
CurrentTrain: epoch  6, batch     2 | loss: 2.8912570Losses:  1.8527967929840088 0.007636042311787605
CurrentTrain: epoch  6, batch     3 | loss: 1.8604329Losses:  1.8405041694641113 0.8222053647041321
CurrentTrain: epoch  7, batch     0 | loss: 2.6627095Losses:  1.8383660316467285 0.8758312463760376
CurrentTrain: epoch  7, batch     1 | loss: 2.7141972Losses:  1.765160083770752 0.621840238571167
CurrentTrain: epoch  7, batch     2 | loss: 2.3870003Losses:  1.9046008586883545 0.06214636191725731
CurrentTrain: epoch  7, batch     3 | loss: 1.9667472Losses:  1.8278818130493164 0.5396900773048401
CurrentTrain: epoch  8, batch     0 | loss: 2.3675718Losses:  1.7419419288635254 0.6305620670318604
CurrentTrain: epoch  8, batch     1 | loss: 2.3725040Losses:  1.8742069005966187 0.8942633867263794
CurrentTrain: epoch  8, batch     2 | loss: 2.7684703Losses:  1.745643138885498 0.010694831609725952
CurrentTrain: epoch  8, batch     3 | loss: 1.7563380Losses:  1.7312456369400024 0.6189998984336853
CurrentTrain: epoch  9, batch     0 | loss: 2.3502455Losses:  1.7774312496185303 0.661894679069519
CurrentTrain: epoch  9, batch     1 | loss: 2.4393258Losses:  1.842742681503296 0.7301574349403381
CurrentTrain: epoch  9, batch     2 | loss: 2.5729001Losses:  1.889636516571045 0.5001144409179688
CurrentTrain: epoch  9, batch     3 | loss: 2.3897510
Losses:  4.094344615936279 0.6796700358390808
MemoryTrain:  epoch  0, batch     0 | loss: 4.7740145Losses:  10.782869338989258 0.9432287216186523
MemoryTrain:  epoch  0, batch     1 | loss: 11.7260981Losses:  10.962218284606934 0.6405621767044067
MemoryTrain:  epoch  0, batch     2 | loss: 11.6027803Losses:  11.745264053344727 0.6662521362304688
MemoryTrain:  epoch  0, batch     3 | loss: 12.4115162Losses:  0.01491700392216444 0.6208310127258301
MemoryTrain:  epoch  1, batch     0 | loss: 0.6357480Losses:  0.018046505749225616 0.564559280872345
MemoryTrain:  epoch  1, batch     1 | loss: 0.5826058Losses:  0.03788489103317261 0.6951338052749634
MemoryTrain:  epoch  1, batch     2 | loss: 0.7330187Losses:  0.022235199809074402 0.6533796191215515
MemoryTrain:  epoch  1, batch     3 | loss: 0.6756148Losses:  0.017516087740659714 0.7695184946060181
MemoryTrain:  epoch  2, batch     0 | loss: 0.7870346Losses:  0.011296669021248817 0.6328873038291931
MemoryTrain:  epoch  2, batch     1 | loss: 0.6441840Losses:  0.005860797129571438 0.4597473442554474
MemoryTrain:  epoch  2, batch     2 | loss: 0.4656081Losses:  0.007748473435640335 0.40556222200393677
MemoryTrain:  epoch  2, batch     3 | loss: 0.4133107Losses:  0.0074541871435940266 0.5141168832778931
MemoryTrain:  epoch  3, batch     0 | loss: 0.5215711Losses:  0.016113871708512306 0.5198032259941101
MemoryTrain:  epoch  3, batch     1 | loss: 0.5359171Losses:  0.008578711189329624 0.5084700584411621
MemoryTrain:  epoch  3, batch     2 | loss: 0.5170488Losses:  0.018728304654359818 0.6258672475814819
MemoryTrain:  epoch  3, batch     3 | loss: 0.6445956Losses:  0.026241950690746307 0.7920694351196289
MemoryTrain:  epoch  4, batch     0 | loss: 0.8183114Losses:  0.011848999187350273 0.4037439227104187
MemoryTrain:  epoch  4, batch     1 | loss: 0.4155929Losses:  0.009228458628058434 0.4709019064903259
MemoryTrain:  epoch  4, batch     2 | loss: 0.4801304Losses:  0.006757780909538269 0.4218995273113251
MemoryTrain:  epoch  4, batch     3 | loss: 0.4286573Losses:  0.011451062746345997 0.4116951823234558
MemoryTrain:  epoch  5, batch     0 | loss: 0.4231462Losses:  0.007716469932347536 0.6407281160354614
MemoryTrain:  epoch  5, batch     1 | loss: 0.6484446Losses:  0.012041537091135979 0.5726193785667419
MemoryTrain:  epoch  5, batch     2 | loss: 0.5846609Losses:  0.012579071335494518 0.4408283829689026
MemoryTrain:  epoch  5, batch     3 | loss: 0.4534075Losses:  0.008297257125377655 0.477317214012146
MemoryTrain:  epoch  6, batch     0 | loss: 0.4856145Losses:  0.004930103663355112 0.4748096168041229
MemoryTrain:  epoch  6, batch     1 | loss: 0.4797397Losses:  0.021337023004889488 0.5242586135864258
MemoryTrain:  epoch  6, batch     2 | loss: 0.5455956Losses:  0.009814686141908169 0.4411690831184387
MemoryTrain:  epoch  6, batch     3 | loss: 0.4509838Losses:  0.015130201354622841 0.541231632232666
MemoryTrain:  epoch  7, batch     0 | loss: 0.5563619Losses:  0.01674550771713257 0.5407781600952148
MemoryTrain:  epoch  7, batch     1 | loss: 0.5575237Losses:  0.004904931876808405 0.4529842734336853
MemoryTrain:  epoch  7, batch     2 | loss: 0.4578892Losses:  0.005207216367125511 0.4115642309188843
MemoryTrain:  epoch  7, batch     3 | loss: 0.4167714Losses:  0.01026907004415989 0.4952561855316162
MemoryTrain:  epoch  8, batch     0 | loss: 0.5055252Losses:  0.009734353050589561 0.3995848298072815
MemoryTrain:  epoch  8, batch     1 | loss: 0.4093192Losses:  0.009640628471970558 0.6254030466079712
MemoryTrain:  epoch  8, batch     2 | loss: 0.6350437Losses:  0.016099490225315094 0.40286505222320557
MemoryTrain:  epoch  8, batch     3 | loss: 0.4189645Losses:  0.008529054000973701 0.4085157811641693
MemoryTrain:  epoch  9, batch     0 | loss: 0.4170448Losses:  0.019576124846935272 0.6368371844291687
MemoryTrain:  epoch  9, batch     1 | loss: 0.6564133Losses:  0.0068858894519507885 0.47324877977371216
MemoryTrain:  epoch  9, batch     2 | loss: 0.4801347Losses:  0.010555155575275421 0.3215061128139496
MemoryTrain:  epoch  9, batch     3 | loss: 0.3320613
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 47.43%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 45.14%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 44.41%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 49.11%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 51.14%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.95%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 56.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 70.78%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 73.47%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 72.79%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 70.75%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 69.91%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 68.08%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 67.96%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.06%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.95%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.26%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.04%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 81.98%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.56%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 80.46%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 79.59%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 79.13%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 78.50%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 77.71%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 77.30%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 77.41%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 77.69%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 77.74%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 77.79%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 77.83%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 77.14%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 76.46%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 75.80%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 75.32%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 74.84%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 74.38%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 74.16%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 73.80%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 73.74%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 73.53%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 73.40%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 73.06%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 72.94%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 72.78%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 72.49%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 72.18%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 71.81%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 71.38%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 71.09%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 70.75%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 70.34%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 70.14%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 69.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 69.49%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 68.99%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 68.81%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 68.52%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 68.11%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 67.72%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 66.87%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 68.04%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 68.00%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 67.56%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 67.03%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 66.75%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 66.38%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 65.96%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 66.02%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 65.67%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 64.99%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 64.76%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 65.40%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 65.05%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 64.71%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 64.53%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 64.23%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 63.86%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 63.93%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 64.08%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 64.60%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 64.85%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 64.87%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 64.93%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 64.99%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 64.63%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 64.44%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 64.14%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 63.84%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 63.47%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 63.18%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 62.96%   [EVAL] batch:  176 | acc: 18.75%,  total acc: 62.71%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 62.54%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 62.43%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 62.33%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 62.22%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 62.26%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 62.47%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 62.63%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 62.73%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 62.90%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 63.18%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 63.43%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 63.33%   [EVAL] batch:  195 | acc: 43.75%,  total acc: 63.23%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 63.20%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 63.10%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 62.91%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 62.75%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 62.66%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 62.47%   [EVAL] batch:  202 | acc: 31.25%,  total acc: 62.32%   [EVAL] batch:  203 | acc: 6.25%,  total acc: 62.04%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 61.89%   [EVAL] batch:  205 | acc: 12.50%,  total acc: 61.65%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 61.53%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 61.42%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 61.24%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 61.19%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 61.08%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 60.85%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 60.92%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.10%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.28%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 61.81%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 61.96%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 61.99%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 62.22%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 62.36%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 62.53%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 62.69%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 64.04%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 64.04%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 64.00%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  246 | acc: 87.50%,  total acc: 64.37%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 64.67%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 64.62%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 64.57%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 64.40%   [EVAL] batch:  260 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 64.23%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 64.20%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 64.17%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 64.12%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 64.14%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 64.23%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  276 | acc: 50.00%,  total acc: 64.94%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 64.91%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 64.92%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:  282 | acc: 0.00%,  total acc: 64.60%   [EVAL] batch:  283 | acc: 0.00%,  total acc: 64.37%   [EVAL] batch:  284 | acc: 6.25%,  total acc: 64.17%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 63.96%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 63.76%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 63.69%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 63.93%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 64.05%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 64.11%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 64.11%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 64.11%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 64.15%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.26%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 65.11%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 65.29%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 65.39%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 65.34%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 65.17%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 65.06%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 64.98%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 64.87%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 65.18%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 64.98%   [EVAL] batch:  327 | acc: 0.00%,  total acc: 64.79%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 64.63%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 64.43%   [EVAL] batch:  330 | acc: 6.25%,  total acc: 64.26%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 64.31%   [EVAL] batch:  332 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.63%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  351 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 66.04%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:  355 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 66.47%   [EVAL] batch:  364 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:  365 | acc: 18.75%,  total acc: 66.19%   [EVAL] batch:  366 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  367 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 65.97%   
cur_acc:  ['0.9435', '0.6935', '0.6300', '0.7986', '0.7183', '0.6796']
his_acc:  ['0.9435', '0.8070', '0.7467', '0.7358', '0.7075', '0.6597']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  3.9120230674743652 1.5389879941940308
CurrentTrain: epoch  0, batch     0 | loss: 5.4510112Losses:  9.439665794372559 1.4667233228683472
CurrentTrain: epoch  0, batch     1 | loss: 10.9063892Losses:  6.415478706359863 1.6892976760864258
CurrentTrain: epoch  0, batch     2 | loss: 8.1047764Losses:  4.206944465637207 0.44007837772369385
CurrentTrain: epoch  0, batch     3 | loss: 4.6470227Losses:  3.3423092365264893 1.615736722946167
CurrentTrain: epoch  1, batch     0 | loss: 4.9580460Losses:  3.0447707176208496 1.3742482662200928
CurrentTrain: epoch  1, batch     1 | loss: 4.4190187Losses:  2.892077922821045 1.5329571962356567
CurrentTrain: epoch  1, batch     2 | loss: 4.4250350Losses:  3.498648166656494 0.377707839012146
CurrentTrain: epoch  1, batch     3 | loss: 3.8763561Losses:  2.8167319297790527 1.1993683576583862
CurrentTrain: epoch  2, batch     0 | loss: 4.0161004Losses:  3.079026937484741 1.2646490335464478
CurrentTrain: epoch  2, batch     1 | loss: 4.3436761Losses:  3.1434693336486816 1.6042653322219849
CurrentTrain: epoch  2, batch     2 | loss: 4.7477345Losses:  3.488129138946533 0.4659513831138611
CurrentTrain: epoch  2, batch     3 | loss: 3.9540806Losses:  2.918639898300171 1.1898307800292969
CurrentTrain: epoch  3, batch     0 | loss: 4.1084709Losses:  2.7469730377197266 1.3068649768829346
CurrentTrain: epoch  3, batch     1 | loss: 4.0538378Losses:  2.5563595294952393 1.207417368888855
CurrentTrain: epoch  3, batch     2 | loss: 3.7637768Losses:  1.8061872720718384 0.15232154726982117
CurrentTrain: epoch  3, batch     3 | loss: 1.9585088Losses:  2.662832260131836 1.2792418003082275
CurrentTrain: epoch  4, batch     0 | loss: 3.9420741Losses:  2.0916085243225098 1.070759654045105
CurrentTrain: epoch  4, batch     1 | loss: 3.1623683Losses:  2.3655238151550293 1.2672523260116577
CurrentTrain: epoch  4, batch     2 | loss: 3.6327763Losses:  4.030699729919434 0.36616528034210205
CurrentTrain: epoch  4, batch     3 | loss: 4.3968649Losses:  2.563011407852173 1.000463843345642
CurrentTrain: epoch  5, batch     0 | loss: 3.5634751Losses:  2.110579490661621 1.0933016538619995
CurrentTrain: epoch  5, batch     1 | loss: 3.2038813Losses:  2.148829698562622 1.2343933582305908
CurrentTrain: epoch  5, batch     2 | loss: 3.3832231Losses:  1.8150875568389893 0.1780443787574768
CurrentTrain: epoch  5, batch     3 | loss: 1.9931319Losses:  2.3409080505371094 1.0712306499481201
CurrentTrain: epoch  6, batch     0 | loss: 3.4121387Losses:  2.0772109031677246 0.8778063654899597
CurrentTrain: epoch  6, batch     1 | loss: 2.9550173Losses:  2.098574638366699 0.8825808763504028
CurrentTrain: epoch  6, batch     2 | loss: 2.9811554Losses:  2.625375986099243 0.13587433099746704
CurrentTrain: epoch  6, batch     3 | loss: 2.7612503Losses:  1.970881700515747 0.9212996363639832
CurrentTrain: epoch  7, batch     0 | loss: 2.8921814Losses:  2.1993370056152344 1.0547358989715576
CurrentTrain: epoch  7, batch     1 | loss: 3.2540729Losses:  1.9718804359436035 1.1706238985061646
CurrentTrain: epoch  7, batch     2 | loss: 3.1425042Losses:  1.6903367042541504 0.5741221904754639
CurrentTrain: epoch  7, batch     3 | loss: 2.2644589Losses:  2.0411157608032227 1.1200976371765137
CurrentTrain: epoch  8, batch     0 | loss: 3.1612134Losses:  1.9422199726104736 0.8040927052497864
CurrentTrain: epoch  8, batch     1 | loss: 2.7463126Losses:  1.9094221591949463 1.1129642724990845
CurrentTrain: epoch  8, batch     2 | loss: 3.0223866Losses:  1.7884690761566162 0.09404309839010239
CurrentTrain: epoch  8, batch     3 | loss: 1.8825122Losses:  1.8179577589035034 1.1261825561523438
CurrentTrain: epoch  9, batch     0 | loss: 2.9441404Losses:  1.972041130065918 1.0537205934524536
CurrentTrain: epoch  9, batch     1 | loss: 3.0257616Losses:  1.8185911178588867 0.9027122855186462
CurrentTrain: epoch  9, batch     2 | loss: 2.7213035Losses:  1.7972843647003174 0.4120103120803833
CurrentTrain: epoch  9, batch     3 | loss: 2.2092948
Losses:  4.248495101928711 0.7298556566238403
MemoryTrain:  epoch  0, batch     0 | loss: 4.9783506Losses:  10.900880813598633 0.8535880446434021
MemoryTrain:  epoch  0, batch     1 | loss: 11.7544689Losses:  11.020319938659668 0.8061803579330444
MemoryTrain:  epoch  0, batch     2 | loss: 11.8264999Losses:  11.353923797607422 0.6592997312545776
MemoryTrain:  epoch  0, batch     3 | loss: 12.0132236Losses:  11.6004638671875 0.19664528965950012
MemoryTrain:  epoch  0, batch     4 | loss: 11.7971096Losses:  0.02493288554251194 0.4941047430038452
MemoryTrain:  epoch  1, batch     0 | loss: 0.5190376Losses:  0.02331327646970749 0.5847867727279663
MemoryTrain:  epoch  1, batch     1 | loss: 0.6081001Losses:  0.022305600345134735 0.6819673180580139
MemoryTrain:  epoch  1, batch     2 | loss: 0.7042729Losses:  0.020600896328687668 0.7444013357162476
MemoryTrain:  epoch  1, batch     3 | loss: 0.7650023Losses:  0.051965147256851196 0.4137488603591919
MemoryTrain:  epoch  1, batch     4 | loss: 0.4657140Losses:  0.016134031116962433 0.6029808521270752
MemoryTrain:  epoch  2, batch     0 | loss: 0.6191149Losses:  0.011168036609888077 0.7134575843811035
MemoryTrain:  epoch  2, batch     1 | loss: 0.7246256Losses:  0.01176594290882349 0.7456347942352295
MemoryTrain:  epoch  2, batch     2 | loss: 0.7574008Losses:  0.00769534707069397 0.7647401690483093
MemoryTrain:  epoch  2, batch     3 | loss: 0.7724355Losses:  0.01325023453682661 0.2444370836019516
MemoryTrain:  epoch  2, batch     4 | loss: 0.2576873Losses:  0.010120788589119911 0.6792178750038147
MemoryTrain:  epoch  3, batch     0 | loss: 0.6893387Losses:  0.011401386000216007 0.5611376762390137
MemoryTrain:  epoch  3, batch     1 | loss: 0.5725391Losses:  0.02167295478284359 0.4690183997154236
MemoryTrain:  epoch  3, batch     2 | loss: 0.4906914Losses:  0.015862304717302322 0.5981517434120178
MemoryTrain:  epoch  3, batch     3 | loss: 0.6140140Losses:  0.005089932586997747 0.31706470251083374
MemoryTrain:  epoch  3, batch     4 | loss: 0.3221546Losses:  0.006277722772210836 0.4663892984390259
MemoryTrain:  epoch  4, batch     0 | loss: 0.4726670Losses:  0.011638535186648369 0.559977650642395
MemoryTrain:  epoch  4, batch     1 | loss: 0.5716162Losses:  0.004427192732691765 0.5122634172439575
MemoryTrain:  epoch  4, batch     2 | loss: 0.5166906Losses:  0.018439102917909622 0.8244715929031372
MemoryTrain:  epoch  4, batch     3 | loss: 0.8429107Losses:  0.017987947911024094 0.3879624307155609
MemoryTrain:  epoch  4, batch     4 | loss: 0.4059504Losses:  0.006235914770513773 0.6017279624938965
MemoryTrain:  epoch  5, batch     0 | loss: 0.6079639Losses:  0.013231957331299782 0.6244683265686035
MemoryTrain:  epoch  5, batch     1 | loss: 0.6377003Losses:  0.007702921982854605 0.6058578491210938
MemoryTrain:  epoch  5, batch     2 | loss: 0.6135608Losses:  0.012140459381043911 0.5925868153572083
MemoryTrain:  epoch  5, batch     3 | loss: 0.6047273Losses:  0.006096227094531059 0.1133427545428276
MemoryTrain:  epoch  5, batch     4 | loss: 0.1194390Losses:  0.011631099507212639 0.6056060791015625
MemoryTrain:  epoch  6, batch     0 | loss: 0.6172372Losses:  0.01113364938646555 0.4518747627735138
MemoryTrain:  epoch  6, batch     1 | loss: 0.4630084Losses:  0.016367845237255096 0.6036791205406189
MemoryTrain:  epoch  6, batch     2 | loss: 0.6200470Losses:  0.010295264422893524 0.546255350112915
MemoryTrain:  epoch  6, batch     3 | loss: 0.5565506Losses:  0.007373514585196972 0.1953054666519165
MemoryTrain:  epoch  6, batch     4 | loss: 0.2026790Losses:  0.014114166609942913 0.4666624069213867
MemoryTrain:  epoch  7, batch     0 | loss: 0.4807766Losses:  0.02501840330660343 0.6677019000053406
MemoryTrain:  epoch  7, batch     1 | loss: 0.6927203Losses:  0.00683886231854558 0.4822070598602295
MemoryTrain:  epoch  7, batch     2 | loss: 0.4890459Losses:  0.0102701336145401 0.5318914651870728
MemoryTrain:  epoch  7, batch     3 | loss: 0.5421616Losses:  0.006628627888858318 0.11238569021224976
MemoryTrain:  epoch  7, batch     4 | loss: 0.1190143Losses:  0.009763073176145554 0.43342524766921997
MemoryTrain:  epoch  8, batch     0 | loss: 0.4431883Losses:  0.01643056422472 0.6902021765708923
MemoryTrain:  epoch  8, batch     1 | loss: 0.7066327Losses:  0.012804563157260418 0.6541522145271301
MemoryTrain:  epoch  8, batch     2 | loss: 0.6669568Losses:  0.013299103826284409 0.46501636505126953
MemoryTrain:  epoch  8, batch     3 | loss: 0.4783155Losses:  0.006411446258425713 0.1661129593849182
MemoryTrain:  epoch  8, batch     4 | loss: 0.1725244Losses:  0.018939930945634842 0.6365190744400024
MemoryTrain:  epoch  9, batch     0 | loss: 0.6554590Losses:  0.011388824321329594 0.4155709147453308
MemoryTrain:  epoch  9, batch     1 | loss: 0.4269598Losses:  0.01375522930175066 0.5804477334022522
MemoryTrain:  epoch  9, batch     2 | loss: 0.5942029Losses:  0.008773380890488625 0.4909411370754242
MemoryTrain:  epoch  9, batch     3 | loss: 0.4997145Losses:  0.009101251140236855 0.12167350947856903
MemoryTrain:  epoch  9, batch     4 | loss: 0.1307748
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 22.92%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 59.56%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 58.55%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 57.19%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 53.41%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 52.17%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 51.30%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 50.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 48.56%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.76%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 45.09%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 43.53%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 42.71%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 41.53%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 42.19%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 44.49%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 45.54%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 46.53%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 47.30%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 48.36%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 49.20%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 50.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 51.22%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 51.93%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 52.76%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 53.41%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 52.36%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 51.22%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 50.27%   [EVAL] batch:   47 | acc: 0.00%,  total acc: 49.22%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 48.21%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 47.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 48.04%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 48.92%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 49.65%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 50.35%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 51.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 51.90%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 52.05%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 52.12%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 52.19%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 52.25%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 52.72%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 52.18%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.56%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 82.61%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.31%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 82.23%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 81.85%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 81.60%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 81.02%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 80.04%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 78.60%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 78.02%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 77.66%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 77.22%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 75.88%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 75.67%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 75.28%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 75.09%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 74.91%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 74.91%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 75.61%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 75.68%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 75.08%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 74.43%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 73.80%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 72.81%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 72.38%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 72.03%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 71.84%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 71.40%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 70.98%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 70.88%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 70.79%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 70.33%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 69.56%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 69.15%   [EVAL] batch:   94 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 68.42%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 68.17%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 67.92%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 67.68%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 66.96%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 66.69%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 66.49%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 66.24%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 65.91%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 65.34%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 65.09%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 66.48%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 66.43%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 65.92%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 65.40%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 65.14%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 64.73%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 64.33%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 64.10%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 64.18%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  135 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 64.36%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 63.94%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 63.62%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 63.30%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 63.12%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 62.89%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 62.67%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 63.41%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 63.08%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 62.70%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 62.58%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 62.26%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 61.90%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 61.86%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 61.79%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 61.75%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 61.68%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 61.65%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:  162 | acc: 31.25%,  total acc: 61.43%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 61.24%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 61.14%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 60.88%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 60.67%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 60.53%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 60.47%   [EVAL] batch:  169 | acc: 12.50%,  total acc: 60.18%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 60.01%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 59.74%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 59.54%   [EVAL] batch:  173 | acc: 0.00%,  total acc: 59.20%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 58.93%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 58.77%   [EVAL] batch:  176 | acc: 12.50%,  total acc: 58.51%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 58.32%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 58.24%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 58.13%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 58.01%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 58.04%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 58.16%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 58.29%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 58.31%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 58.43%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 58.49%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 58.61%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 58.80%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 58.91%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 59.06%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 59.15%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 59.36%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 59.44%   [EVAL] batch:  194 | acc: 25.00%,  total acc: 59.26%   [EVAL] batch:  195 | acc: 12.50%,  total acc: 59.02%   [EVAL] batch:  196 | acc: 25.00%,  total acc: 58.85%   [EVAL] batch:  197 | acc: 25.00%,  total acc: 58.68%   [EVAL] batch:  198 | acc: 12.50%,  total acc: 58.45%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 58.31%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 58.24%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 58.11%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 58.04%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 57.87%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 57.77%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 57.68%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 57.58%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 57.36%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 57.12%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 56.90%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 56.72%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 56.49%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 56.54%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 56.75%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 56.95%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 57.15%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 57.54%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 57.71%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 57.76%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 57.92%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 58.05%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 58.07%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 58.15%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 58.11%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 58.30%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 58.48%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 58.66%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 58.99%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 59.15%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 59.29%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 59.47%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 59.64%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 59.81%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 59.98%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 60.15%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 60.16%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 60.09%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 60.08%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 60.11%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 60.07%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 60.08%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 60.14%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 60.28%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 60.39%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 60.84%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 60.95%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 60.88%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 60.81%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 60.72%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 60.68%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 60.64%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 60.64%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 60.58%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 60.59%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 60.55%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 60.48%   [EVAL] batch:  260 | acc: 25.00%,  total acc: 60.34%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 60.31%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 60.38%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 60.37%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 60.38%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 60.31%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 60.37%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 60.45%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 60.57%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 60.65%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 61.08%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 61.18%   [EVAL] batch:  274 | acc: 87.50%,  total acc: 61.27%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 61.30%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 61.30%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 61.31%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 61.29%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 61.36%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 61.39%   [EVAL] batch:  281 | acc: 31.25%,  total acc: 61.28%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 61.17%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 60.98%   [EVAL] batch:  284 | acc: 6.25%,  total acc: 60.79%   [EVAL] batch:  285 | acc: 6.25%,  total acc: 60.60%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 60.45%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 60.39%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 60.47%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 60.56%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 60.61%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 60.70%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 60.75%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 60.84%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 60.89%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 60.90%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 60.88%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 60.91%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 60.91%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 60.96%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 61.22%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 61.47%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 61.60%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 61.93%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 62.01%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 62.22%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 62.34%   [EVAL] batch:  312 | acc: 68.75%,  total acc: 62.36%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 62.30%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 62.10%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 61.95%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 61.83%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 61.76%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 61.86%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 61.95%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 62.15%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 62.23%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 62.31%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 62.12%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 61.93%   [EVAL] batch:  327 | acc: 0.00%,  total acc: 61.74%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 61.55%   [EVAL] batch:  329 | acc: 0.00%,  total acc: 61.36%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 61.18%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 61.20%   [EVAL] batch:  332 | acc: 87.50%,  total acc: 61.28%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 61.40%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 61.47%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 61.84%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 62.15%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 62.26%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 62.37%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 62.48%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 62.55%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 62.72%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 62.80%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 63.00%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 63.10%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 63.10%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 63.17%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 63.25%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 63.34%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 63.42%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 63.61%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 63.65%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 63.62%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 63.49%   [EVAL] batch:  365 | acc: 25.00%,  total acc: 63.39%   [EVAL] batch:  366 | acc: 25.00%,  total acc: 63.28%   [EVAL] batch:  367 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 63.08%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 63.02%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 62.97%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 63.00%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 62.87%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 62.77%   [EVAL] batch:  377 | acc: 31.25%,  total acc: 62.68%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 62.58%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 62.57%   [EVAL] batch:  380 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 62.55%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 62.63%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 62.78%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 62.82%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 62.89%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 62.85%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 62.84%   [EVAL] batch:  390 | acc: 68.75%,  total acc: 62.85%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 62.88%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 62.79%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 62.71%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 62.58%   [EVAL] batch:  396 | acc: 18.75%,  total acc: 62.47%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 62.37%   [EVAL] batch:  398 | acc: 31.25%,  total acc: 62.30%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 62.20%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 62.06%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 61.91%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 61.76%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 61.60%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 61.50%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 61.36%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 61.44%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:  409 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 61.56%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 61.59%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 61.65%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 61.70%   [EVAL] batch:  414 | acc: 93.75%,  total acc: 61.78%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 61.84%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 61.89%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 61.99%   [EVAL] batch:  419 | acc: 6.25%,  total acc: 61.86%   [EVAL] batch:  420 | acc: 0.00%,  total acc: 61.71%   [EVAL] batch:  421 | acc: 6.25%,  total acc: 61.58%   [EVAL] batch:  422 | acc: 0.00%,  total acc: 61.44%   [EVAL] batch:  423 | acc: 0.00%,  total acc: 61.29%   [EVAL] batch:  424 | acc: 6.25%,  total acc: 61.16%   [EVAL] batch:  425 | acc: 81.25%,  total acc: 61.21%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 61.29%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 61.35%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 61.41%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 61.48%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 61.56%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 61.53%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 61.52%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 61.51%   [EVAL] batch:  435 | acc: 56.25%,  total acc: 61.50%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 61.54%   [EVAL] batch:  437 | acc: 18.75%,  total acc: 61.44%   
cur_acc:  ['0.9435', '0.6935', '0.6300', '0.7986', '0.7183', '0.6796', '0.5218']
his_acc:  ['0.9435', '0.8070', '0.7467', '0.7358', '0.7075', '0.6597', '0.6144']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  3.9120230674743652 1.6185485124588013
CurrentTrain: epoch  0, batch     0 | loss: 5.5305715Losses:  8.489227294921875 1.4747565984725952
CurrentTrain: epoch  0, batch     1 | loss: 9.9639835Losses:  6.26163911819458 1.2563104629516602
CurrentTrain: epoch  0, batch     2 | loss: 7.5179496Losses:  3.9493491649627686 0.2281826287508011
CurrentTrain: epoch  0, batch     3 | loss: 4.1775317Losses:  2.9198436737060547 1.0084714889526367
CurrentTrain: epoch  1, batch     0 | loss: 3.9283152Losses:  2.570868968963623 1.2799041271209717
CurrentTrain: epoch  1, batch     1 | loss: 3.8507731Losses:  2.7697432041168213 1.1323175430297852
CurrentTrain: epoch  1, batch     2 | loss: 3.9020607Losses:  1.980310082435608 2.9802322387695312e-08
CurrentTrain: epoch  1, batch     3 | loss: 1.9803101Losses:  3.0690488815307617 1.1802613735198975
CurrentTrain: epoch  2, batch     0 | loss: 4.2493105Losses:  2.904822587966919 0.9860795736312866
CurrentTrain: epoch  2, batch     1 | loss: 3.8909020Losses:  2.200000762939453 0.9125550389289856
CurrentTrain: epoch  2, batch     2 | loss: 3.1125557Losses:  2.3501362800598145 0.2935771346092224
CurrentTrain: epoch  2, batch     3 | loss: 2.6437135Losses:  2.91678524017334 1.162219762802124
CurrentTrain: epoch  3, batch     0 | loss: 4.0790052Losses:  2.12805438041687 0.900433361530304
CurrentTrain: epoch  3, batch     1 | loss: 3.0284877Losses:  2.7838175296783447 0.9035984873771667
CurrentTrain: epoch  3, batch     2 | loss: 3.6874161Losses:  3.4435315132141113 1.0362298488616943
CurrentTrain: epoch  3, batch     3 | loss: 4.4797611Losses:  2.422563076019287 0.8203699588775635
CurrentTrain: epoch  4, batch     0 | loss: 3.2429330Losses:  2.3267860412597656 0.8863654136657715
CurrentTrain: epoch  4, batch     1 | loss: 3.2131515Losses:  2.1677794456481934 1.1259957551956177
CurrentTrain: epoch  4, batch     2 | loss: 3.2937751Losses:  2.9848155975341797 0.16529497504234314
CurrentTrain: epoch  4, batch     3 | loss: 3.1501105Losses:  2.263113260269165 1.0095176696777344
CurrentTrain: epoch  5, batch     0 | loss: 3.2726309Losses:  2.2442476749420166 0.7093865871429443
CurrentTrain: epoch  5, batch     1 | loss: 2.9536343Losses:  2.0673487186431885 0.6909512281417847
CurrentTrain: epoch  5, batch     2 | loss: 2.7582998Losses:  1.7752180099487305 0.06829951703548431
CurrentTrain: epoch  5, batch     3 | loss: 1.8435175Losses:  2.124223232269287 0.9917221069335938
CurrentTrain: epoch  6, batch     0 | loss: 3.1159453Losses:  1.998350739479065 0.8010026216506958
CurrentTrain: epoch  6, batch     1 | loss: 2.7993534Losses:  1.9703431129455566 0.5304529070854187
CurrentTrain: epoch  6, batch     2 | loss: 2.5007961Losses:  2.0440566539764404 0.3174675703048706
CurrentTrain: epoch  6, batch     3 | loss: 2.3615241Losses:  2.017803192138672 0.8801765441894531
CurrentTrain: epoch  7, batch     0 | loss: 2.8979797Losses:  1.9297924041748047 0.5903466939926147
CurrentTrain: epoch  7, batch     1 | loss: 2.5201392Losses:  1.9584882259368896 0.8424142599105835
CurrentTrain: epoch  7, batch     2 | loss: 2.8009024Losses:  1.7914621829986572 0.08532541990280151
CurrentTrain: epoch  7, batch     3 | loss: 1.8767877Losses:  1.9006829261779785 0.8482630252838135
CurrentTrain: epoch  8, batch     0 | loss: 2.7489460Losses:  1.9971601963043213 0.5780654549598694
CurrentTrain: epoch  8, batch     1 | loss: 2.5752256Losses:  1.767228603363037 0.6197018623352051
CurrentTrain: epoch  8, batch     2 | loss: 2.3869305Losses:  1.7445520162582397 0.10192467272281647
CurrentTrain: epoch  8, batch     3 | loss: 1.8464767Losses:  1.7476054430007935 0.47552502155303955
CurrentTrain: epoch  9, batch     0 | loss: 2.2231305Losses:  1.808833360671997 0.8119211196899414
CurrentTrain: epoch  9, batch     1 | loss: 2.6207545Losses:  1.9045891761779785 0.7036476135253906
CurrentTrain: epoch  9, batch     2 | loss: 2.6082368Losses:  1.777030348777771 0.07044460624456406
CurrentTrain: epoch  9, batch     3 | loss: 1.8474749
Losses:  4.382026672363281 0.5291706323623657
MemoryTrain:  epoch  0, batch     0 | loss: 4.9111972Losses:  10.18087100982666 0.608310878276825
MemoryTrain:  epoch  0, batch     1 | loss: 10.7891817Losses:  11.182918548583984 0.9139819145202637
MemoryTrain:  epoch  0, batch     2 | loss: 12.0969009Losses:  11.64773178100586 0.6443377733230591
MemoryTrain:  epoch  0, batch     3 | loss: 12.2920694Losses:  11.250978469848633 0.5540958642959595
MemoryTrain:  epoch  0, batch     4 | loss: 11.8050747Losses:  0.02858481928706169 0.5653860569000244
MemoryTrain:  epoch  1, batch     0 | loss: 0.5939709Losses:  0.022335465997457504 0.4576786160469055
MemoryTrain:  epoch  1, batch     1 | loss: 0.4800141Losses:  0.029826415702700615 0.5554441213607788
MemoryTrain:  epoch  1, batch     2 | loss: 0.5852705Losses:  0.02091342955827713 0.7710972428321838
MemoryTrain:  epoch  1, batch     3 | loss: 0.7920107Losses:  0.025138376280665398 0.554674506187439
MemoryTrain:  epoch  1, batch     4 | loss: 0.5798129Losses:  0.01681247167289257 0.4998289942741394
MemoryTrain:  epoch  2, batch     0 | loss: 0.5166414Losses:  0.021807514131069183 0.8059967756271362
MemoryTrain:  epoch  2, batch     1 | loss: 0.8278043Losses:  0.025087110698223114 0.5770382285118103
MemoryTrain:  epoch  2, batch     2 | loss: 0.6021253Losses:  0.010221807286143303 0.590907633304596
MemoryTrain:  epoch  2, batch     3 | loss: 0.6011294Losses:  0.0134130185469985 0.40235641598701477
MemoryTrain:  epoch  2, batch     4 | loss: 0.4157694Losses:  0.016559477895498276 0.6235690116882324
MemoryTrain:  epoch  3, batch     0 | loss: 0.6401285Losses:  0.017204221338033676 0.6625686883926392
MemoryTrain:  epoch  3, batch     1 | loss: 0.6797729Losses:  0.016578609123826027 0.41180381178855896
MemoryTrain:  epoch  3, batch     2 | loss: 0.4283824Losses:  0.014051854610443115 0.5147786140441895
MemoryTrain:  epoch  3, batch     3 | loss: 0.5288305Losses:  0.006852502003312111 0.5219032764434814
MemoryTrain:  epoch  3, batch     4 | loss: 0.5287558Losses:  0.01723381131887436 0.5278713703155518
MemoryTrain:  epoch  4, batch     0 | loss: 0.5451052Losses:  0.01483878493309021 0.4700740575790405
MemoryTrain:  epoch  4, batch     1 | loss: 0.4849128Losses:  0.008934598416090012 0.4278700649738312
MemoryTrain:  epoch  4, batch     2 | loss: 0.4368047Losses:  0.020158231258392334 0.5459412336349487
MemoryTrain:  epoch  4, batch     3 | loss: 0.5660995Losses:  0.012775416485965252 0.6420429944992065
MemoryTrain:  epoch  4, batch     4 | loss: 0.6548184Losses:  0.013149388134479523 0.3035011291503906
MemoryTrain:  epoch  5, batch     0 | loss: 0.3166505Losses:  0.015308606438338757 0.5870139598846436
MemoryTrain:  epoch  5, batch     1 | loss: 0.6023226Losses:  0.014758105389773846 0.5000141859054565
MemoryTrain:  epoch  5, batch     2 | loss: 0.5147723Losses:  0.012808475643396378 0.5941554307937622
MemoryTrain:  epoch  5, batch     3 | loss: 0.6069639Losses:  0.014482245780527592 0.49411827325820923
MemoryTrain:  epoch  5, batch     4 | loss: 0.5086005Losses:  0.012958364561200142 0.47993266582489014
MemoryTrain:  epoch  6, batch     0 | loss: 0.4928910Losses:  0.011707570403814316 0.34071916341781616
MemoryTrain:  epoch  6, batch     1 | loss: 0.3524267Losses:  0.011530469171702862 0.4829593896865845
MemoryTrain:  epoch  6, batch     2 | loss: 0.4944898Losses:  0.03323969617486 0.714751124382019
MemoryTrain:  epoch  6, batch     3 | loss: 0.7479908Losses:  0.009471352212131023 0.48977917432785034
MemoryTrain:  epoch  6, batch     4 | loss: 0.4992505Losses:  0.013046875596046448 0.6360217332839966
MemoryTrain:  epoch  7, batch     0 | loss: 0.6490686Losses:  0.020591512322425842 0.5185481309890747
MemoryTrain:  epoch  7, batch     1 | loss: 0.5391396Losses:  0.012033352628350258 0.41208916902542114
MemoryTrain:  epoch  7, batch     2 | loss: 0.4241225Losses:  0.02036585658788681 0.5119637846946716
MemoryTrain:  epoch  7, batch     3 | loss: 0.5323296Losses:  0.010660110041499138 0.3405781388282776
MemoryTrain:  epoch  7, batch     4 | loss: 0.3512383Losses:  0.014150803908705711 0.47662028670310974
MemoryTrain:  epoch  8, batch     0 | loss: 0.4907711Losses:  0.011499380692839622 0.5200581550598145
MemoryTrain:  epoch  8, batch     1 | loss: 0.5315576Losses:  0.017188433557748795 0.461780309677124
MemoryTrain:  epoch  8, batch     2 | loss: 0.4789687Losses:  0.012540596537292004 0.47729384899139404
MemoryTrain:  epoch  8, batch     3 | loss: 0.4898345Losses:  0.009897271171212196 0.4685114622116089
MemoryTrain:  epoch  8, batch     4 | loss: 0.4784087Losses:  0.014524945989251137 0.43698251247406006
MemoryTrain:  epoch  9, batch     0 | loss: 0.4515074Losses:  0.007408885285258293 0.42463845014572144
MemoryTrain:  epoch  9, batch     1 | loss: 0.4320473Losses:  0.03201021999120712 0.616155207157135
MemoryTrain:  epoch  9, batch     2 | loss: 0.6481654Losses:  0.006227174773812294 0.34804767370224
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.790603637695312 1.8446838855743408
CurrentTrain: epoch  0, batch     0 | loss: 13.6352873Losses:  12.882108688354492 2.180497646331787
CurrentTrain: epoch  0, batch     1 | loss: 15.0626068Losses:  13.198368072509766 1.6947343349456787
CurrentTrain: epoch  0, batch     2 | loss: 14.8931026Losses:  12.836082458496094 1.7115907669067383
CurrentTrain: epoch  0, batch     3 | loss: 14.5476732Losses:  13.009395599365234 1.594553828239441
CurrentTrain: epoch  0, batch     4 | loss: 14.6039495Losses:  12.865983963012695 1.4847921133041382
CurrentTrain: epoch  0, batch     5 | loss: 14.3507757Losses:  12.336124420166016 1.7189971208572388
CurrentTrain: epoch  0, batch     6 | loss: 14.0551214Losses:  12.481547355651855 1.5266774892807007
CurrentTrain: epoch  0, batch     7 | loss: 14.0082245Losses:  12.570840835571289 1.4967564344406128
CurrentTrain: epoch  0, batch     8 | loss: 14.0675974Losses:  12.201203346252441 1.7841187715530396
CurrentTrain: epoch  0, batch     9 | loss: 13.9853220Losses:  12.393925666809082 1.5016331672668457
CurrentTrain: epoch  0, batch    10 | loss: 13.8955593Losses:  12.023885726928711 1.3506367206573486
CurrentTrain: epoch  0, batch    11 | loss: 13.3745222Losses:  11.771618843078613 1.5821871757507324
CurrentTrain: epoch  0, batch    12 | loss: 13.3538055Losses:  11.798096656799316 1.535919189453125
CurrentTrain: epoch  0, batch    13 | loss: 13.3340158Losses:  11.626349449157715 1.7983756065368652
CurrentTrain: epoch  0, batch    14 | loss: 13.4247246Losses:  11.65964126586914 2.1435747146606445
CurrentTrain: epoch  0, batch    15 | loss: 13.8032160Losses:  11.378331184387207 1.4695961475372314
CurrentTrain: epoch  0, batch    16 | loss: 12.8479271Losses:  11.479841232299805 2.0052952766418457
CurrentTrain: epoch  0, batch    17 | loss: 13.4851360Losses:  11.498146057128906 1.7177271842956543
CurrentTrain: epoch  0, batch    18 | loss: 13.2158737Losses:  11.128531455993652 1.7516908645629883
CurrentTrain: epoch  0, batch    19 | loss: 12.8802223Losses:  11.075489044189453 1.409255027770996
CurrentTrain: epoch  0, batch    20 | loss: 12.4847441Losses:  10.813135147094727 1.5555481910705566
CurrentTrain: epoch  0, batch    21 | loss: 12.3686829Losses:  10.630768775939941 1.438846468925476
CurrentTrain: epoch  0, batch    22 | loss: 12.0696154Losses:  10.313468933105469 1.1719428300857544
CurrentTrain: epoch  0, batch    23 | loss: 11.4854116Losses:  10.532732009887695 1.410292387008667
CurrentTrain: epoch  0, batch    24 | loss: 11.9430246Losses:  10.03175163269043 1.1876879930496216
CurrentTrain: epoch  0, batch    25 | loss: 11.2194395Losses:  10.436735153198242 1.2369922399520874
CurrentTrain: epoch  0, batch    26 | loss: 11.6737270Losses:  9.87910270690918 1.3534646034240723
CurrentTrain: epoch  0, batch    27 | loss: 11.2325668Losses:  10.010368347167969 1.2555941343307495
CurrentTrain: epoch  0, batch    28 | loss: 11.2659626Losses:  9.82020092010498 1.2499425411224365
CurrentTrain: epoch  0, batch    29 | loss: 11.0701437Losses:  9.710309982299805 1.4522032737731934
CurrentTrain: epoch  0, batch    30 | loss: 11.1625137Losses:  9.817508697509766 1.052901268005371
CurrentTrain: epoch  0, batch    31 | loss: 10.8704100Losses:  9.759468078613281 1.025534987449646
CurrentTrain: epoch  0, batch    32 | loss: 10.7850027Losses:  9.710594177246094 0.9816863536834717
CurrentTrain: epoch  0, batch    33 | loss: 10.6922808Losses:  9.652456283569336 1.0831677913665771
CurrentTrain: epoch  0, batch    34 | loss: 10.7356243Losses:  9.131695747375488 1.1211082935333252
CurrentTrain: epoch  0, batch    35 | loss: 10.2528038Losses:  9.616426467895508 1.2941339015960693
CurrentTrain: epoch  0, batch    36 | loss: 10.9105606Losses:  9.105113983154297 1.1075825691223145
CurrentTrain: epoch  0, batch    37 | loss: 10.2126961Losses:  9.519662857055664 1.3355926275253296
CurrentTrain: epoch  0, batch    38 | loss: 10.8552551Losses:  9.107057571411133 1.247278094291687
CurrentTrain: epoch  0, batch    39 | loss: 10.3543358Losses:  9.02031135559082 1.2146894931793213
CurrentTrain: epoch  0, batch    40 | loss: 10.2350006Losses:  8.644828796386719 1.077113389968872
CurrentTrain: epoch  0, batch    41 | loss: 9.7219419Losses:  8.518653869628906 1.128772258758545
CurrentTrain: epoch  0, batch    42 | loss: 9.6474266Losses:  8.375802993774414 0.8065122365951538
CurrentTrain: epoch  0, batch    43 | loss: 9.1823149Losses:  8.089629173278809 0.9916726350784302
CurrentTrain: epoch  0, batch    44 | loss: 9.0813017Losses:  7.920493125915527 1.101912498474121
CurrentTrain: epoch  0, batch    45 | loss: 9.0224056Losses:  8.380575180053711 0.9052286148071289
CurrentTrain: epoch  0, batch    46 | loss: 9.2858038Losses:  7.597620964050293 1.0866082906723022
CurrentTrain: epoch  0, batch    47 | loss: 8.6842289Losses:  8.172414779663086 1.2759554386138916
CurrentTrain: epoch  0, batch    48 | loss: 9.4483700Losses:  7.660238742828369 1.153031349182129
CurrentTrain: epoch  0, batch    49 | loss: 8.8132706Losses:  7.746065139770508 1.1386276483535767
CurrentTrain: epoch  0, batch    50 | loss: 8.8846931Losses:  7.38640022277832 1.0367956161499023
CurrentTrain: epoch  0, batch    51 | loss: 8.4231958Losses:  7.331951141357422 0.8596635460853577
CurrentTrain: epoch  0, batch    52 | loss: 8.1916151Losses:  7.532163619995117 1.0735458135604858
CurrentTrain: epoch  0, batch    53 | loss: 8.6057091Losses:  7.416317939758301 1.254809856414795
CurrentTrain: epoch  0, batch    54 | loss: 8.6711273Losses:  6.860708713531494 0.9189192056655884
CurrentTrain: epoch  0, batch    55 | loss: 7.7796278Losses:  6.642520904541016 0.8493960499763489
CurrentTrain: epoch  0, batch    56 | loss: 7.4919171Losses:  6.964746475219727 0.9760639667510986
CurrentTrain: epoch  0, batch    57 | loss: 7.9408102Losses:  6.6142258644104 0.8214986324310303
CurrentTrain: epoch  0, batch    58 | loss: 7.4357243Losses:  6.695570945739746 0.7601029872894287
CurrentTrain: epoch  0, batch    59 | loss: 7.4556742Losses:  6.1100921630859375 0.8140542507171631
CurrentTrain: epoch  0, batch    60 | loss: 6.9241467Losses:  6.606369495391846 0.6659046411514282
CurrentTrain: epoch  0, batch    61 | loss: 7.2722740Losses:  7.191379070281982 0.7296182513237
CurrentTrain: epoch  0, batch    62 | loss: 7.9209971Losses:  6.383916854858398 0.8396751880645752
CurrentTrain: epoch  1, batch     0 | loss: 7.2235918Losses:  5.929101467132568 0.7069734334945679
CurrentTrain: epoch  1, batch     1 | loss: 6.6360750Losses:  6.153318405151367 0.8946205377578735
CurrentTrain: epoch  1, batch     2 | loss: 7.0479388Losses:  5.928778648376465 0.8319469690322876
CurrentTrain: epoch  1, batch     3 | loss: 6.7607255Losses:  6.411143779754639 0.8803616762161255
CurrentTrain: epoch  1, batch     4 | loss: 7.2915053Losses:  5.848355293273926 0.6334733366966248
CurrentTrain: epoch  1, batch     5 | loss: 6.4818287Losses:  5.94049596786499 0.7584623098373413
CurrentTrain: epoch  1, batch     6 | loss: 6.6989584Losses:  5.602858066558838 0.701314389705658
CurrentTrain: epoch  1, batch     7 | loss: 6.3041725Losses:  5.713626384735107 0.5928243398666382
CurrentTrain: epoch  1, batch     8 | loss: 6.3064508Losses:  6.042672157287598 0.8942802548408508
CurrentTrain: epoch  1, batch     9 | loss: 6.9369526Losses:  5.9875898361206055 0.7322484850883484
CurrentTrain: epoch  1, batch    10 | loss: 6.7198381Losses:  6.0113067626953125 0.9188165068626404
CurrentTrain: epoch  1, batch    11 | loss: 6.9301233Losses:  5.964424133300781 0.8358300924301147
CurrentTrain: epoch  1, batch    12 | loss: 6.8002543Losses:  6.2201080322265625 0.5976628065109253
CurrentTrain: epoch  1, batch    13 | loss: 6.8177710Losses:  5.580741882324219 0.7183420658111572
CurrentTrain: epoch  1, batch    14 | loss: 6.2990837Losses:  6.278866767883301 0.7824966907501221
CurrentTrain: epoch  1, batch    15 | loss: 7.0613632Losses:  6.014688968658447 0.6776264905929565
CurrentTrain: epoch  1, batch    16 | loss: 6.6923156Losses:  5.51302433013916 0.45240432024002075
CurrentTrain: epoch  1, batch    17 | loss: 5.9654288Losses:  5.5268235206604 0.5653159618377686
CurrentTrain: epoch  1, batch    18 | loss: 6.0921392Losses:  5.746478080749512 0.6622617840766907
CurrentTrain: epoch  1, batch    19 | loss: 6.4087400Losses:  5.889609336853027 0.7756423950195312
CurrentTrain: epoch  1, batch    20 | loss: 6.6652517Losses:  5.98267936706543 0.809965968132019
CurrentTrain: epoch  1, batch    21 | loss: 6.7926455Losses:  5.826706886291504 0.7484753131866455
CurrentTrain: epoch  1, batch    22 | loss: 6.5751820Losses:  5.982362747192383 0.5066203474998474
CurrentTrain: epoch  1, batch    23 | loss: 6.4889832Losses:  5.863463878631592 0.6472691893577576
CurrentTrain: epoch  1, batch    24 | loss: 6.5107331Losses:  5.31488037109375 0.5300331115722656
CurrentTrain: epoch  1, batch    25 | loss: 5.8449135Losses:  5.575470924377441 0.7054077982902527
CurrentTrain: epoch  1, batch    26 | loss: 6.2808785Losses:  5.665275573730469 0.749494731426239
CurrentTrain: epoch  1, batch    27 | loss: 6.4147701Losses:  5.469257831573486 0.5817455649375916
CurrentTrain: epoch  1, batch    28 | loss: 6.0510035Losses:  5.3457441329956055 0.6768053770065308
CurrentTrain: epoch  1, batch    29 | loss: 6.0225496Losses:  5.362095355987549 0.48425227403640747
CurrentTrain: epoch  1, batch    30 | loss: 5.8463478Losses:  5.250823020935059 0.6345598101615906
CurrentTrain: epoch  1, batch    31 | loss: 5.8853827Losses:  5.242365837097168 0.6601659059524536
CurrentTrain: epoch  1, batch    32 | loss: 5.9025316Losses:  5.266963958740234 0.43190422654151917
CurrentTrain: epoch  1, batch    33 | loss: 5.6988683Losses:  5.621603012084961 0.7798724174499512
CurrentTrain: epoch  1, batch    34 | loss: 6.4014754Losses:  5.228883743286133 0.6022958755493164
CurrentTrain: epoch  1, batch    35 | loss: 5.8311796Losses:  5.722214221954346 0.43021273612976074
CurrentTrain: epoch  1, batch    36 | loss: 6.1524267Losses:  5.648190498352051 0.6688570976257324
CurrentTrain: epoch  1, batch    37 | loss: 6.3170476Losses:  5.824211120605469 0.7140395045280457
CurrentTrain: epoch  1, batch    38 | loss: 6.5382504Losses:  5.050361156463623 0.4718755781650543
CurrentTrain: epoch  1, batch    39 | loss: 5.5222368Losses:  5.590741157531738 0.4883117079734802
CurrentTrain: epoch  1, batch    40 | loss: 6.0790529Losses:  5.954197406768799 0.8292580842971802
CurrentTrain: epoch  1, batch    41 | loss: 6.7834554Losses:  5.490651607513428 0.6528634428977966
CurrentTrain: epoch  1, batch    42 | loss: 6.1435151Losses:  5.623462677001953 0.6991957426071167
CurrentTrain: epoch  1, batch    43 | loss: 6.3226585Losses:  5.802094459533691 0.5917803049087524
CurrentTrain: epoch  1, batch    44 | loss: 6.3938746Losses:  5.115427017211914 0.5095726251602173
CurrentTrain: epoch  1, batch    45 | loss: 5.6249995Losses:  4.986939430236816 0.548987627029419
CurrentTrain: epoch  1, batch    46 | loss: 5.5359268Losses:  5.520327091217041 0.6791566610336304
CurrentTrain: epoch  1, batch    47 | loss: 6.1994839Losses:  5.482499122619629 0.6102449297904968
CurrentTrain: epoch  1, batch    48 | loss: 6.0927439Losses:  5.132065773010254 0.5514819025993347
CurrentTrain: epoch  1, batch    49 | loss: 5.6835475Losses:  5.242216110229492 0.5085581541061401
CurrentTrain: epoch  1, batch    50 | loss: 5.7507744Losses:  5.649921417236328 0.6864122152328491
CurrentTrain: epoch  1, batch    51 | loss: 6.3363338Losses:  5.3115386962890625 0.5760645866394043
CurrentTrain: epoch  1, batch    52 | loss: 5.8876033Losses:  5.079061031341553 0.4552425444126129
CurrentTrain: epoch  1, batch    53 | loss: 5.5343037Losses:  5.309532165527344 0.5673702359199524
CurrentTrain: epoch  1, batch    54 | loss: 5.8769026Losses:  4.594964027404785 0.2510523796081543
CurrentTrain: epoch  1, batch    55 | loss: 4.8460164Losses:  4.7682037353515625 0.38147836923599243
CurrentTrain: epoch  1, batch    56 | loss: 5.1496820Losses:  5.107093811035156 0.5314439535140991
CurrentTrain: epoch  1, batch    57 | loss: 5.6385379Losses:  4.979730129241943 0.36544349789619446
CurrentTrain: epoch  1, batch    58 | loss: 5.3451738Losses:  5.377592086791992 0.6632575392723083
CurrentTrain: epoch  1, batch    59 | loss: 6.0408497Losses:  5.20839262008667 0.668004035949707
CurrentTrain: epoch  1, batch    60 | loss: 5.8763967Losses:  4.888345718383789 0.5109719038009644
CurrentTrain: epoch  1, batch    61 | loss: 5.3993177Losses:  4.3622918128967285 0.17280367016792297
CurrentTrain: epoch  1, batch    62 | loss: 4.5350957Losses:  5.149557590484619 0.43017613887786865
CurrentTrain: epoch  2, batch     0 | loss: 5.5797338Losses:  4.874622821807861 0.4128309488296509
CurrentTrain: epoch  2, batch     1 | loss: 5.2874537Losses:  4.964108467102051 0.5313791036605835
CurrentTrain: epoch  2, batch     2 | loss: 5.4954877Losses:  4.712582588195801 0.45129072666168213
CurrentTrain: epoch  2, batch     3 | loss: 5.1638732Losses:  4.950246810913086 0.4032938480377197
CurrentTrain: epoch  2, batch     4 | loss: 5.3535404Losses:  4.796876907348633 0.5292847752571106
CurrentTrain: epoch  2, batch     5 | loss: 5.3261619Losses:  4.822301864624023 0.31816405057907104
CurrentTrain: epoch  2, batch     6 | loss: 5.1404657Losses:  5.061762809753418 0.4179205894470215
CurrentTrain: epoch  2, batch     7 | loss: 5.4796834Losses:  5.488317489624023 0.4604599177837372
CurrentTrain: epoch  2, batch     8 | loss: 5.9487772Losses:  4.598387718200684 0.27554404735565186
CurrentTrain: epoch  2, batch     9 | loss: 4.8739319Losses:  4.629363059997559 0.3994449973106384
CurrentTrain: epoch  2, batch    10 | loss: 5.0288081Losses:  5.052547454833984 0.4026128649711609
CurrentTrain: epoch  2, batch    11 | loss: 5.4551601Losses:  4.966341018676758 0.43928641080856323
CurrentTrain: epoch  2, batch    12 | loss: 5.4056273Losses:  5.1043291091918945 0.40775376558303833
CurrentTrain: epoch  2, batch    13 | loss: 5.5120831Losses:  5.063664436340332 0.3829250633716583
CurrentTrain: epoch  2, batch    14 | loss: 5.4465895Losses:  4.704200744628906 0.36364275217056274
CurrentTrain: epoch  2, batch    15 | loss: 5.0678434Losses:  4.4330549240112305 0.3109824061393738
CurrentTrain: epoch  2, batch    16 | loss: 4.7440372Losses:  4.401026248931885 0.2913166284561157
CurrentTrain: epoch  2, batch    17 | loss: 4.6923428Losses:  5.087994575500488 0.420090913772583
CurrentTrain: epoch  2, batch    18 | loss: 5.5080853Losses:  4.830641746520996 0.32453274726867676
CurrentTrain: epoch  2, batch    19 | loss: 5.1551743Losses:  4.679007530212402 0.38052716851234436
CurrentTrain: epoch  2, batch    20 | loss: 5.0595345Losses:  4.574190616607666 0.28949570655822754
CurrentTrain: epoch  2, batch    21 | loss: 4.8636866Losses:  4.493036270141602 0.3629639744758606
CurrentTrain: epoch  2, batch    22 | loss: 4.8560004Losses:  4.856585502624512 0.31661713123321533
CurrentTrain: epoch  2, batch    23 | loss: 5.1732025Losses:  4.800188064575195 0.4498818516731262
CurrentTrain: epoch  2, batch    24 | loss: 5.2500701Losses:  4.616623401641846 0.2942509055137634
CurrentTrain: epoch  2, batch    25 | loss: 4.9108744Losses:  4.620278358459473 0.27323126792907715
CurrentTrain: epoch  2, batch    26 | loss: 4.8935099Losses:  4.672522068023682 0.29447871446609497
CurrentTrain: epoch  2, batch    27 | loss: 4.9670010Losses:  4.576617240905762 0.31609469652175903
CurrentTrain: epoch  2, batch    28 | loss: 4.8927121Losses:  4.5971784591674805 0.36521849036216736
CurrentTrain: epoch  2, batch    29 | loss: 4.9623971Losses:  4.859362602233887 0.4810096025466919
CurrentTrain: epoch  2, batch    30 | loss: 5.3403721Losses:  4.602764129638672 0.358942449092865
CurrentTrain: epoch  2, batch    31 | loss: 4.9617066Losses:  5.357476234436035 0.46493691205978394
CurrentTrain: epoch  2, batch    32 | loss: 5.8224130Losses:  4.503452301025391 0.3521386384963989
CurrentTrain: epoch  2, batch    33 | loss: 4.8555908Losses:  4.844025135040283 0.42035409808158875
CurrentTrain: epoch  2, batch    34 | loss: 5.2643790Losses:  4.859918117523193 0.29211702942848206
CurrentTrain: epoch  2, batch    35 | loss: 5.1520352Losses:  4.479063034057617 0.28870028257369995
CurrentTrain: epoch  2, batch    36 | loss: 4.7677631Losses:  4.760776996612549 0.2875846326351166
CurrentTrain: epoch  2, batch    37 | loss: 5.0483618Losses:  4.6094794273376465 0.23377248644828796
CurrentTrain: epoch  2, batch    38 | loss: 4.8432517Losses:  4.360932350158691 0.27028554677963257
CurrentTrain: epoch  2, batch    39 | loss: 4.6312180Losses:  4.560232162475586 0.33591142296791077
CurrentTrain: epoch  2, batch    40 | loss: 4.8961434Losses:  5.060711860656738 0.3470000624656677
CurrentTrain: epoch  2, batch    41 | loss: 5.4077120Losses:  4.624018669128418 0.32060521841049194
CurrentTrain: epoch  2, batch    42 | loss: 4.9446239Losses:  4.79599666595459 0.3299853503704071
CurrentTrain: epoch  2, batch    43 | loss: 5.1259818Losses:  4.590804100036621 0.3461798131465912
CurrentTrain: epoch  2, batch    44 | loss: 4.9369841Losses:  4.725272178649902 0.281783789396286
CurrentTrain: epoch  2, batch    45 | loss: 5.0070558Losses:  4.403665542602539 0.2267913967370987
CurrentTrain: epoch  2, batch    46 | loss: 4.6304569Losses:  4.862847328186035 0.410115122795105
CurrentTrain: epoch  2, batch    47 | loss: 5.2729626Losses:  4.409831523895264 0.24939347803592682
CurrentTrain: epoch  2, batch    48 | loss: 4.6592250Losses:  4.648676872253418 0.3536792993545532
CurrentTrain: epoch  2, batch    49 | loss: 5.0023561Losses:  4.39808464050293 0.2084471732378006
CurrentTrain: epoch  2, batch    50 | loss: 4.6065316Losses:  4.378640174865723 0.266366571187973
CurrentTrain: epoch  2, batch    51 | loss: 4.6450067Losses:  4.771224021911621 0.24684149026870728
CurrentTrain: epoch  2, batch    52 | loss: 5.0180655Losses:  4.3849029541015625 0.2913442850112915
CurrentTrain: epoch  2, batch    53 | loss: 4.6762471Losses:  4.3863959312438965 0.27721548080444336
CurrentTrain: epoch  2, batch    54 | loss: 4.6636114Losses:  4.456516265869141 0.28089457750320435
CurrentTrain: epoch  2, batch    55 | loss: 4.7374110Losses:  4.647160530090332 0.4384612739086151
CurrentTrain: epoch  2, batch    56 | loss: 5.0856218Losses:  4.566767692565918 0.2746597230434418
CurrentTrain: epoch  2, batch    57 | loss: 4.8414273Losses:  4.522421836853027 0.2618674635887146
CurrentTrain: epoch  2, batch    58 | loss: 4.7842894Losses:  4.417580604553223 0.2231067717075348
CurrentTrain: epoch  2, batch    59 | loss: 4.6406875Losses:  4.401697158813477 0.349703311920166
CurrentTrain: epoch  2, batch    60 | loss: 4.7514005Losses:  4.503505706787109 0.2779741883277893
CurrentTrain: epoch  2, batch    61 | loss: 4.7814798Losses:  4.294343948364258 0.10341030359268188
CurrentTrain: epoch  2, batch    62 | loss: 4.3977542Losses:  4.439620018005371 0.26707351207733154
CurrentTrain: epoch  3, batch     0 | loss: 4.7066936Losses:  4.99075984954834 0.3793928921222687
CurrentTrain: epoch  3, batch     1 | loss: 5.3701530Losses:  4.369693279266357 0.278655469417572
CurrentTrain: epoch  3, batch     2 | loss: 4.6483488Losses:  4.567419528961182 0.2123584896326065
CurrentTrain: epoch  3, batch     3 | loss: 4.7797780Losses:  4.478737831115723 0.2344736009836197
CurrentTrain: epoch  3, batch     4 | loss: 4.7132115Losses:  4.411922454833984 0.19946861267089844
CurrentTrain: epoch  3, batch     5 | loss: 4.6113911Losses:  4.359187126159668 0.2572624683380127
CurrentTrain: epoch  3, batch     6 | loss: 4.6164494Losses:  4.330695152282715 0.23180516064167023
CurrentTrain: epoch  3, batch     7 | loss: 4.5625005Losses:  4.766862869262695 0.38612425327301025
CurrentTrain: epoch  3, batch     8 | loss: 5.1529870Losses:  4.544301986694336 0.2607920169830322
CurrentTrain: epoch  3, batch     9 | loss: 4.8050938Losses:  4.564356803894043 0.34015601873397827
CurrentTrain: epoch  3, batch    10 | loss: 4.9045129Losses:  4.430144786834717 0.2182351052761078
CurrentTrain: epoch  3, batch    11 | loss: 4.6483798Losses:  4.19450569152832 0.16837948560714722
CurrentTrain: epoch  3, batch    12 | loss: 4.3628850Losses:  4.514065742492676 0.3528791069984436
CurrentTrain: epoch  3, batch    13 | loss: 4.8669448Losses:  4.274966716766357 0.22673186659812927
CurrentTrain: epoch  3, batch    14 | loss: 4.5016985Losses:  4.279616832733154 0.24818353354930878
CurrentTrain: epoch  3, batch    15 | loss: 4.5278006Losses:  4.255199432373047 0.25091552734375
CurrentTrain: epoch  3, batch    16 | loss: 4.5061150Losses:  4.538837909698486 0.24475815892219543
CurrentTrain: epoch  3, batch    17 | loss: 4.7835960Losses:  4.450519561767578 0.19743795692920685
CurrentTrain: epoch  3, batch    18 | loss: 4.6479573Losses:  4.468474388122559 0.269779235124588
CurrentTrain: epoch  3, batch    19 | loss: 4.7382536Losses:  4.300493240356445 0.21749897301197052
CurrentTrain: epoch  3, batch    20 | loss: 4.5179920Losses:  4.336590766906738 0.22901597619056702
CurrentTrain: epoch  3, batch    21 | loss: 4.5656066Losses:  4.408334255218506 0.28808295726776123
CurrentTrain: epoch  3, batch    22 | loss: 4.6964173Losses:  4.307384014129639 0.2524765133857727
CurrentTrain: epoch  3, batch    23 | loss: 4.5598607Losses:  4.289555549621582 0.15511715412139893
CurrentTrain: epoch  3, batch    24 | loss: 4.4446726Losses:  4.212289333343506 0.23924481868743896
CurrentTrain: epoch  3, batch    25 | loss: 4.4515343Losses:  4.277739524841309 0.23627230525016785
CurrentTrain: epoch  3, batch    26 | loss: 4.5140119Losses:  4.316277980804443 0.2632887363433838
CurrentTrain: epoch  3, batch    27 | loss: 4.5795670Losses:  4.401756763458252 0.2772602438926697
CurrentTrain: epoch  3, batch    28 | loss: 4.6790171Losses:  4.78156042098999 0.2662429213523865
CurrentTrain: epoch  3, batch    29 | loss: 5.0478034Losses:  4.637119770050049 0.26754361391067505
CurrentTrain: epoch  3, batch    30 | loss: 4.9046636Losses:  4.190217971801758 0.18787166476249695
CurrentTrain: epoch  3, batch    31 | loss: 4.3780894Losses:  4.530514717102051 0.2304258495569229
CurrentTrain: epoch  3, batch    32 | loss: 4.7609406Losses:  4.356814861297607 0.19397148489952087
CurrentTrain: epoch  3, batch    33 | loss: 4.5507865Losses:  4.224981784820557 0.23854279518127441
CurrentTrain: epoch  3, batch    34 | loss: 4.4635248Losses:  4.260547637939453 0.2784896790981293
CurrentTrain: epoch  3, batch    35 | loss: 4.5390372Losses:  4.899685382843018 0.460845947265625
CurrentTrain: epoch  3, batch    36 | loss: 5.3605313Losses:  4.291214942932129 0.16874656081199646
CurrentTrain: epoch  3, batch    37 | loss: 4.4599614Losses:  4.17633056640625 0.17746607959270477
CurrentTrain: epoch  3, batch    38 | loss: 4.3537965Losses:  4.744218826293945 0.2064492404460907
CurrentTrain: epoch  3, batch    39 | loss: 4.9506679Losses:  4.2511491775512695 0.22752343118190765
CurrentTrain: epoch  3, batch    40 | loss: 4.4786725Losses:  4.224184513092041 0.1697150021791458
CurrentTrain: epoch  3, batch    41 | loss: 4.3938994Losses:  4.8168840408325195 0.27890312671661377
CurrentTrain: epoch  3, batch    42 | loss: 5.0957870Losses:  4.247494697570801 0.14402443170547485
CurrentTrain: epoch  3, batch    43 | loss: 4.3915191Losses:  4.932596206665039 0.3242107331752777
CurrentTrain: epoch  3, batch    44 | loss: 5.2568069Losses:  4.979753494262695 0.26547685265541077
CurrentTrain: epoch  3, batch    45 | loss: 5.2452302Losses:  4.210495948791504 0.17875120043754578
CurrentTrain: epoch  3, batch    46 | loss: 4.3892469Losses:  4.2553935050964355 0.15994900465011597
CurrentTrain: epoch  3, batch    47 | loss: 4.4153423Losses:  4.276738166809082 0.16705229878425598
CurrentTrain: epoch  3, batch    48 | loss: 4.4437904Losses:  4.327020168304443 0.19566090404987335
CurrentTrain: epoch  3, batch    49 | loss: 4.5226812Losses:  4.299317836761475 0.2130473554134369
CurrentTrain: epoch  3, batch    50 | loss: 4.5123653Losses:  4.4739670753479 0.13870497047901154
CurrentTrain: epoch  3, batch    51 | loss: 4.6126719Losses:  4.39653205871582 0.22132425010204315
CurrentTrain: epoch  3, batch    52 | loss: 4.6178565Losses:  4.796757698059082 0.2904828190803528
CurrentTrain: epoch  3, batch    53 | loss: 5.0872407Losses:  4.25816011428833 0.2271529883146286
CurrentTrain: epoch  3, batch    54 | loss: 4.4853129Losses:  4.359443187713623 0.21588407456874847
CurrentTrain: epoch  3, batch    55 | loss: 4.5753274Losses:  4.275096893310547 0.22242864966392517
CurrentTrain: epoch  3, batch    56 | loss: 4.4975257Losses:  4.405701637268066 0.2322937250137329
CurrentTrain: epoch  3, batch    57 | loss: 4.6379952Losses:  4.313531875610352 0.14543108642101288
CurrentTrain: epoch  3, batch    58 | loss: 4.4589629Losses:  4.278524398803711 0.12857499718666077
CurrentTrain: epoch  3, batch    59 | loss: 4.4070992Losses:  4.392056465148926 0.24753062427043915
CurrentTrain: epoch  3, batch    60 | loss: 4.6395869Losses:  4.118415832519531 0.15232694149017334
CurrentTrain: epoch  3, batch    61 | loss: 4.2707429Losses:  4.14869499206543 0.0568799152970314
CurrentTrain: epoch  3, batch    62 | loss: 4.2055750Losses:  4.1775007247924805 0.16432245075702667
CurrentTrain: epoch  4, batch     0 | loss: 4.3418231Losses:  4.234356880187988 0.18050137162208557
CurrentTrain: epoch  4, batch     1 | loss: 4.4148583Losses:  4.2288665771484375 0.1507091075181961
CurrentTrain: epoch  4, batch     2 | loss: 4.3795757Losses:  4.077527046203613 0.16270563006401062
CurrentTrain: epoch  4, batch     3 | loss: 4.2402325Losses:  4.266013145446777 0.20505812764167786
CurrentTrain: epoch  4, batch     4 | loss: 4.4710712Losses:  4.260002136230469 0.21562165021896362
CurrentTrain: epoch  4, batch     5 | loss: 4.4756236Losses:  4.341884613037109 0.1913171112537384
CurrentTrain: epoch  4, batch     6 | loss: 4.5332017Losses:  4.306501388549805 0.1558886170387268
CurrentTrain: epoch  4, batch     7 | loss: 4.4623899Losses:  4.148006439208984 0.2012544572353363
CurrentTrain: epoch  4, batch     8 | loss: 4.3492608Losses:  4.207830429077148 0.20018675923347473
CurrentTrain: epoch  4, batch     9 | loss: 4.4080172Losses:  4.206111431121826 0.14365002512931824
CurrentTrain: epoch  4, batch    10 | loss: 4.3497615Losses:  4.400758743286133 0.26490581035614014
CurrentTrain: epoch  4, batch    11 | loss: 4.6656647Losses:  4.297985076904297 0.17315132915973663
CurrentTrain: epoch  4, batch    12 | loss: 4.4711366Losses:  4.410484313964844 0.2302379012107849
CurrentTrain: epoch  4, batch    13 | loss: 4.6407223Losses:  4.0232343673706055 0.12613314390182495
CurrentTrain: epoch  4, batch    14 | loss: 4.1493673Losses:  4.2697343826293945 0.12629282474517822
CurrentTrain: epoch  4, batch    15 | loss: 4.3960271Losses:  4.329977035522461 0.20726600289344788
CurrentTrain: epoch  4, batch    16 | loss: 4.5372429Losses:  4.122604846954346 0.12499888241291046
CurrentTrain: epoch  4, batch    17 | loss: 4.2476039Losses:  4.257585525512695 0.19273149967193604
CurrentTrain: epoch  4, batch    18 | loss: 4.4503169Losses:  4.178524017333984 0.22676414251327515
CurrentTrain: epoch  4, batch    19 | loss: 4.4052882Losses:  4.618990898132324 0.25019100308418274
CurrentTrain: epoch  4, batch    20 | loss: 4.8691821Losses:  4.408638954162598 0.14043980836868286
CurrentTrain: epoch  4, batch    21 | loss: 4.5490789Losses:  4.06605339050293 0.185379296541214
CurrentTrain: epoch  4, batch    22 | loss: 4.2514329Losses:  4.053459644317627 0.06827067583799362
CurrentTrain: epoch  4, batch    23 | loss: 4.1217303Losses:  4.467526435852051 0.14985057711601257
CurrentTrain: epoch  4, batch    24 | loss: 4.6173768Losses:  4.144355297088623 0.1713452786207199
CurrentTrain: epoch  4, batch    25 | loss: 4.3157005Losses:  4.080833435058594 0.16190654039382935
CurrentTrain: epoch  4, batch    26 | loss: 4.2427402Losses:  4.0612568855285645 0.11084017157554626
CurrentTrain: epoch  4, batch    27 | loss: 4.1720972Losses:  4.2766008377075195 0.19012096524238586
CurrentTrain: epoch  4, batch    28 | loss: 4.4667220Losses:  4.2436370849609375 0.12752753496170044
CurrentTrain: epoch  4, batch    29 | loss: 4.3711648Losses:  4.032914161682129 0.09840945154428482
CurrentTrain: epoch  4, batch    30 | loss: 4.1313238Losses:  4.310332775115967 0.1631927639245987
CurrentTrain: epoch  4, batch    31 | loss: 4.4735255Losses:  4.113992214202881 0.19697566330432892
CurrentTrain: epoch  4, batch    32 | loss: 4.3109679Losses:  4.238901138305664 0.21525734663009644
CurrentTrain: epoch  4, batch    33 | loss: 4.4541583Losses:  4.094895839691162 0.09650232642889023
CurrentTrain: epoch  4, batch    34 | loss: 4.1913981Losses:  4.180999279022217 0.17279726266860962
CurrentTrain: epoch  4, batch    35 | loss: 4.3537965Losses:  4.189350605010986 0.1933184266090393
CurrentTrain: epoch  4, batch    36 | loss: 4.3826690Losses:  4.361088752746582 0.2085103690624237
CurrentTrain: epoch  4, batch    37 | loss: 4.5695992Losses:  4.061481475830078 0.11870159208774567
CurrentTrain: epoch  4, batch    38 | loss: 4.1801829Losses:  4.096571922302246 0.17625615000724792
CurrentTrain: epoch  4, batch    39 | loss: 4.2728281Losses:  4.043590545654297 0.11677511036396027
CurrentTrain: epoch  4, batch    40 | loss: 4.1603656Losses:  4.025365829467773 0.17633819580078125
CurrentTrain: epoch  4, batch    41 | loss: 4.2017040Losses:  4.892009258270264 0.3054024875164032
CurrentTrain: epoch  4, batch    42 | loss: 5.1974115Losses:  4.2099385261535645 0.09303392469882965
CurrentTrain: epoch  4, batch    43 | loss: 4.3029723Losses:  4.029179573059082 0.11800608783960342
CurrentTrain: epoch  4, batch    44 | loss: 4.1471858Losses:  4.2608642578125 0.1723819375038147
CurrentTrain: epoch  4, batch    45 | loss: 4.4332461Losses:  4.378661632537842 0.14598603546619415
CurrentTrain: epoch  4, batch    46 | loss: 4.5246477Losses:  4.232311248779297 0.15895743668079376
CurrentTrain: epoch  4, batch    47 | loss: 4.3912687Losses:  4.183581352233887 0.16515091061592102
CurrentTrain: epoch  4, batch    48 | loss: 4.3487325Losses:  4.0986833572387695 0.13441979885101318
CurrentTrain: epoch  4, batch    49 | loss: 4.2331033Losses:  4.274444580078125 0.21045827865600586
CurrentTrain: epoch  4, batch    50 | loss: 4.4849029Losses:  4.076364994049072 0.09848086535930634
CurrentTrain: epoch  4, batch    51 | loss: 4.1748457Losses:  4.0669450759887695 0.16565412282943726
CurrentTrain: epoch  4, batch    52 | loss: 4.2325993Losses:  4.164827346801758 0.15476243197917938
CurrentTrain: epoch  4, batch    53 | loss: 4.3195896Losses:  4.369891166687012 0.137261301279068
CurrentTrain: epoch  4, batch    54 | loss: 4.5071526Losses:  4.074801445007324 0.167789489030838
CurrentTrain: epoch  4, batch    55 | loss: 4.2425909Losses:  4.163746356964111 0.1202658861875534
CurrentTrain: epoch  4, batch    56 | loss: 4.2840123Losses:  4.1349358558654785 0.16762752830982208
CurrentTrain: epoch  4, batch    57 | loss: 4.3025632Losses:  4.053550720214844 0.10933557152748108
CurrentTrain: epoch  4, batch    58 | loss: 4.1628861Losses:  4.09950590133667 0.10682977735996246
CurrentTrain: epoch  4, batch    59 | loss: 4.2063355Losses:  4.039182186126709 0.13491728901863098
CurrentTrain: epoch  4, batch    60 | loss: 4.1740994Losses:  4.119417667388916 0.13062037527561188
CurrentTrain: epoch  4, batch    61 | loss: 4.2500381Losses:  4.081862926483154 0.05457812547683716
CurrentTrain: epoch  4, batch    62 | loss: 4.1364412Losses:  4.1064229011535645 0.17158889770507812
CurrentTrain: epoch  5, batch     0 | loss: 4.2780118Losses:  4.097553253173828 0.17474600672721863
CurrentTrain: epoch  5, batch     1 | loss: 4.2722993Losses:  4.048474311828613 0.09786134958267212
CurrentTrain: epoch  5, batch     2 | loss: 4.1463356Losses:  4.054856300354004 0.11697112768888474
CurrentTrain: epoch  5, batch     3 | loss: 4.1718273Losses:  4.116692066192627 0.1088164895772934
CurrentTrain: epoch  5, batch     4 | loss: 4.2255087Losses:  4.262979984283447 0.14152517914772034
CurrentTrain: epoch  5, batch     5 | loss: 4.4045053Losses:  4.115453720092773 0.13283208012580872
CurrentTrain: epoch  5, batch     6 | loss: 4.2482858Losses:  4.013603210449219 0.162051260471344
CurrentTrain: epoch  5, batch     7 | loss: 4.1756544Losses:  4.094948768615723 0.14448067545890808
CurrentTrain: epoch  5, batch     8 | loss: 4.2394295Losses:  4.041322708129883 0.10986210405826569
CurrentTrain: epoch  5, batch     9 | loss: 4.1511850Losses:  4.159465312957764 0.21136678755283356
CurrentTrain: epoch  5, batch    10 | loss: 4.3708320Losses:  4.055953025817871 0.13319920003414154
CurrentTrain: epoch  5, batch    11 | loss: 4.1891522Losses:  4.149872779846191 0.15418142080307007
CurrentTrain: epoch  5, batch    12 | loss: 4.3040543Losses:  4.038252830505371 0.12202739715576172
CurrentTrain: epoch  5, batch    13 | loss: 4.1602802Losses:  4.0067596435546875 0.16392023861408234
CurrentTrain: epoch  5, batch    14 | loss: 4.1706800Losses:  4.09398078918457 0.11729761213064194
CurrentTrain: epoch  5, batch    15 | loss: 4.2112784Losses:  4.049755096435547 0.14975492656230927
CurrentTrain: epoch  5, batch    16 | loss: 4.1995101Losses:  4.01708459854126 0.14357641339302063
CurrentTrain: epoch  5, batch    17 | loss: 4.1606612Losses:  4.101748466491699 0.16618387401103973
CurrentTrain: epoch  5, batch    18 | loss: 4.2679324Losses:  4.065879821777344 0.0975554957985878
CurrentTrain: epoch  5, batch    19 | loss: 4.1634355Losses:  4.0213422775268555 0.10844248533248901
CurrentTrain: epoch  5, batch    20 | loss: 4.1297846Losses:  4.187459945678711 0.12777766585350037
CurrentTrain: epoch  5, batch    21 | loss: 4.3152375Losses:  4.0669264793396 0.1384267359972
CurrentTrain: epoch  5, batch    22 | loss: 4.2053533Losses:  4.045421600341797 0.11967507004737854
CurrentTrain: epoch  5, batch    23 | loss: 4.1650968Losses:  4.038619041442871 0.08116240054368973
CurrentTrain: epoch  5, batch    24 | loss: 4.1197815Losses:  4.172235488891602 0.14179718494415283
CurrentTrain: epoch  5, batch    25 | loss: 4.3140326Losses:  4.062159538269043 0.14243236184120178
CurrentTrain: epoch  5, batch    26 | loss: 4.2045918Losses:  4.032452583312988 0.12733140587806702
CurrentTrain: epoch  5, batch    27 | loss: 4.1597838Losses:  4.0653076171875 0.14184148609638214
CurrentTrain: epoch  5, batch    28 | loss: 4.2071490Losses:  4.027507781982422 0.1473540961742401
CurrentTrain: epoch  5, batch    29 | loss: 4.1748619Losses:  4.001587390899658 0.07919777184724808
CurrentTrain: epoch  5, batch    30 | loss: 4.0807853Losses:  4.073577880859375 0.10274725407361984
CurrentTrain: epoch  5, batch    31 | loss: 4.1763253Losses:  4.079068183898926 0.09323880076408386
CurrentTrain: epoch  5, batch    32 | loss: 4.1723070Losses:  4.037967681884766 0.12968966364860535
CurrentTrain: epoch  5, batch    33 | loss: 4.1676574Losses:  4.147059440612793 0.14185985922813416
CurrentTrain: epoch  5, batch    34 | loss: 4.2889194Losses:  4.726627349853516 0.24055682122707367
CurrentTrain: epoch  5, batch    35 | loss: 4.9671841Losses:  4.04057502746582 0.11674875020980835
CurrentTrain: epoch  5, batch    36 | loss: 4.1573238Losses:  4.022222518920898 0.13665547966957092
CurrentTrain: epoch  5, batch    37 | loss: 4.1588778Losses:  4.02348518371582 0.10771887004375458
CurrentTrain: epoch  5, batch    38 | loss: 4.1312041Losses:  4.044349193572998 0.13032302260398865
CurrentTrain: epoch  5, batch    39 | loss: 4.1746721Losses:  4.080381393432617 0.13598403334617615
CurrentTrain: epoch  5, batch    40 | loss: 4.2163653Losses:  4.168758392333984 0.11848092079162598
CurrentTrain: epoch  5, batch    41 | loss: 4.2872391Losses:  4.089871883392334 0.18046101927757263
CurrentTrain: epoch  5, batch    42 | loss: 4.2703328Losses:  4.051603317260742 0.1296374499797821
CurrentTrain: epoch  5, batch    43 | loss: 4.1812406Losses:  4.0232319831848145 0.14470791816711426
CurrentTrain: epoch  5, batch    44 | loss: 4.1679401Losses:  4.007040023803711 0.16221898794174194
CurrentTrain: epoch  5, batch    45 | loss: 4.1692591Losses:  4.2910308837890625 0.13945922255516052
CurrentTrain: epoch  5, batch    46 | loss: 4.4304900Losses:  4.148625373840332 0.14849713444709778
CurrentTrain: epoch  5, batch    47 | loss: 4.2971225Losses:  3.964226245880127 0.1382988691329956
CurrentTrain: epoch  5, batch    48 | loss: 4.1025252Losses:  4.013644218444824 0.1335926353931427
CurrentTrain: epoch  5, batch    49 | loss: 4.1472368Losses:  4.121227264404297 0.1167326420545578
CurrentTrain: epoch  5, batch    50 | loss: 4.2379599Losses:  4.2080559730529785 0.10496290773153305
CurrentTrain: epoch  5, batch    51 | loss: 4.3130188Losses:  4.0142621994018555 0.16866394877433777
CurrentTrain: epoch  5, batch    52 | loss: 4.1829262Losses:  4.021069049835205 0.1267450451850891
CurrentTrain: epoch  5, batch    53 | loss: 4.1478143Losses:  3.972311496734619 0.1574958860874176
CurrentTrain: epoch  5, batch    54 | loss: 4.1298075Losses:  4.185029029846191 0.15513876080513
CurrentTrain: epoch  5, batch    55 | loss: 4.3401680Losses:  4.000462532043457 0.14186127483844757
CurrentTrain: epoch  5, batch    56 | loss: 4.1423240Losses:  4.051630020141602 0.1119372770190239
CurrentTrain: epoch  5, batch    57 | loss: 4.1635671Losses:  4.025120735168457 0.11003868281841278
CurrentTrain: epoch  5, batch    58 | loss: 4.1351595Losses:  4.2656965255737305 0.1299302577972412
CurrentTrain: epoch  5, batch    59 | loss: 4.3956270Losses:  3.9774413108825684 0.09862163662910461
CurrentTrain: epoch  5, batch    60 | loss: 4.0760632Losses:  3.999232769012451 0.10975410044193268
CurrentTrain: epoch  5, batch    61 | loss: 4.1089869Losses:  4.035689830780029 0.06650145351886749
CurrentTrain: epoch  5, batch    62 | loss: 4.1021914Losses:  4.014886379241943 0.1292693167924881
CurrentTrain: epoch  6, batch     0 | loss: 4.1441555Losses:  4.020718574523926 0.10234029591083527
CurrentTrain: epoch  6, batch     1 | loss: 4.1230588Losses:  4.019814491271973 0.09882962703704834
CurrentTrain: epoch  6, batch     2 | loss: 4.1186442Losses:  4.81871223449707 0.3204006254673004
CurrentTrain: epoch  6, batch     3 | loss: 5.1391129Losses:  4.139039516448975 0.10061433166265488
CurrentTrain: epoch  6, batch     4 | loss: 4.2396541Losses:  3.99360728263855 0.09645060449838638
CurrentTrain: epoch  6, batch     5 | loss: 4.0900578Losses:  4.004227638244629 0.13111864030361176
CurrentTrain: epoch  6, batch     6 | loss: 4.1353464Losses:  4.05963659286499 0.10146495699882507
CurrentTrain: epoch  6, batch     7 | loss: 4.1611013Losses:  4.117966651916504 0.14075195789337158
CurrentTrain: epoch  6, batch     8 | loss: 4.2587185Losses:  4.406333923339844 0.09498120844364166
CurrentTrain: epoch  6, batch     9 | loss: 4.5013151Losses:  3.991642951965332 0.12495564669370651
CurrentTrain: epoch  6, batch    10 | loss: 4.1165986Losses:  4.226424694061279 0.08924450725317001
CurrentTrain: epoch  6, batch    11 | loss: 4.3156691Losses:  4.022992134094238 0.13214285671710968
CurrentTrain: epoch  6, batch    12 | loss: 4.1551352Losses:  4.012826919555664 0.0852566659450531
CurrentTrain: epoch  6, batch    13 | loss: 4.0980835Losses:  4.07277774810791 0.13085179030895233
CurrentTrain: epoch  6, batch    14 | loss: 4.2036295Losses:  3.9821860790252686 0.12825974822044373
CurrentTrain: epoch  6, batch    15 | loss: 4.1104460Losses:  4.059178829193115 0.13225916028022766
CurrentTrain: epoch  6, batch    16 | loss: 4.1914382Losses:  3.9925355911254883 0.12205719947814941
CurrentTrain: epoch  6, batch    17 | loss: 4.1145926Losses:  4.020545959472656 0.05152890086174011
CurrentTrain: epoch  6, batch    18 | loss: 4.0720749Losses:  3.9811079502105713 0.13526681065559387
CurrentTrain: epoch  6, batch    19 | loss: 4.1163750Losses:  3.991257667541504 0.15034794807434082
CurrentTrain: epoch  6, batch    20 | loss: 4.1416054Losses:  4.0324907302856445 0.11991746723651886
CurrentTrain: epoch  6, batch    21 | loss: 4.1524081Losses:  4.123952865600586 0.11854749917984009
CurrentTrain: epoch  6, batch    22 | loss: 4.2425003Losses:  4.022006034851074 0.09630934149026871
CurrentTrain: epoch  6, batch    23 | loss: 4.1183152Losses:  4.00156307220459 0.10472676903009415
CurrentTrain: epoch  6, batch    24 | loss: 4.1062899Losses:  4.043944358825684 0.09925530850887299
CurrentTrain: epoch  6, batch    25 | loss: 4.1431994Losses:  4.017761707305908 0.12611758708953857
CurrentTrain: epoch  6, batch    26 | loss: 4.1438794Losses:  4.087233543395996 0.11046496033668518
CurrentTrain: epoch  6, batch    27 | loss: 4.1976986Losses:  3.978687286376953 0.1322702169418335
CurrentTrain: epoch  6, batch    28 | loss: 4.1109576Losses:  3.9892430305480957 0.12250471115112305
CurrentTrain: epoch  6, batch    29 | loss: 4.1117477Losses:  4.044102668762207 0.12125536799430847
CurrentTrain: epoch  6, batch    30 | loss: 4.1653581Losses:  4.039332389831543 0.08084158599376678
CurrentTrain: epoch  6, batch    31 | loss: 4.1201739Losses:  4.012036323547363 0.0874301940202713
CurrentTrain: epoch  6, batch    32 | loss: 4.0994663Losses:  4.108352184295654 0.1347678303718567
CurrentTrain: epoch  6, batch    33 | loss: 4.2431202Losses:  4.035121917724609 0.13547545671463013
CurrentTrain: epoch  6, batch    34 | loss: 4.1705976Losses:  4.021632194519043 0.0702122151851654
CurrentTrain: epoch  6, batch    35 | loss: 4.0918446Losses:  4.047074317932129 0.06717900931835175
CurrentTrain: epoch  6, batch    36 | loss: 4.1142535Losses:  3.9837732315063477 0.11274045705795288
CurrentTrain: epoch  6, batch    37 | loss: 4.0965137Losses:  4.159326076507568 0.13647887110710144
CurrentTrain: epoch  6, batch    38 | loss: 4.2958050Losses:  4.028287887573242 0.08448853343725204
CurrentTrain: epoch  6, batch    39 | loss: 4.1127763Losses:  3.991696834564209 0.11100845038890839
CurrentTrain: epoch  6, batch    40 | loss: 4.1027055Losses:  4.05856990814209 0.13046656548976898
CurrentTrain: epoch  6, batch    41 | loss: 4.1890364Losses:  4.076237678527832 0.12511946260929108
CurrentTrain: epoch  6, batch    42 | loss: 4.2013574Losses:  3.941995143890381 0.10827162116765976
CurrentTrain: epoch  6, batch    43 | loss: 4.0502667Losses:  4.003759384155273 0.10072306543588638
CurrentTrain: epoch  6, batch    44 | loss: 4.1044827Losses:  3.984398365020752 0.09992675483226776
CurrentTrain: epoch  6, batch    45 | loss: 4.0843253Losses:  4.055522441864014 0.118012934923172
CurrentTrain: epoch  6, batch    46 | loss: 4.1735353Losses:  3.9828665256500244 0.09211339056491852
CurrentTrain: epoch  6, batch    47 | loss: 4.0749798Losses:  3.9700424671173096 0.12197118252515793
CurrentTrain: epoch  6, batch    48 | loss: 4.0920138Losses:  3.9822895526885986 0.06615032255649567
CurrentTrain: epoch  6, batch    49 | loss: 4.0484400Losses:  3.990135908126831 0.0983537882566452
CurrentTrain: epoch  6, batch    50 | loss: 4.0884895Losses:  3.966360569000244 0.09841977059841156
CurrentTrain: epoch  6, batch    51 | loss: 4.0647802Losses:  3.992623805999756 0.09212063997983932
CurrentTrain: epoch  6, batch    52 | loss: 4.0847445Losses:  3.9762654304504395 0.04057571664452553
CurrentTrain: epoch  6, batch    53 | loss: 4.0168409Losses:  3.9687607288360596 0.10739752650260925
CurrentTrain: epoch  6, batch    54 | loss: 4.0761580Losses:  3.9627914428710938 0.10959244519472122
CurrentTrain: epoch  6, batch    55 | loss: 4.0723839Losses:  4.029150009155273 0.07150547951459885
CurrentTrain: epoch  6, batch    56 | loss: 4.1006556Losses:  3.995314836502075 0.08673128485679626
CurrentTrain: epoch  6, batch    57 | loss: 4.0820460Losses:  4.022943496704102 0.10252795368432999
CurrentTrain: epoch  6, batch    58 | loss: 4.1254716Losses:  3.9988958835601807 0.12879060208797455
CurrentTrain: epoch  6, batch    59 | loss: 4.1276865Losses:  4.1221208572387695 0.07963179796934128
CurrentTrain: epoch  6, batch    60 | loss: 4.2017527Losses:  3.9947433471679688 0.10124853253364563
CurrentTrain: epoch  6, batch    61 | loss: 4.0959921Losses:  3.978874444961548 0.1161252036690712
CurrentTrain: epoch  6, batch    62 | loss: 4.0949998Losses:  4.048529624938965 0.0748998373746872
CurrentTrain: epoch  7, batch     0 | loss: 4.1234293Losses:  4.145272731781006 0.0919782817363739
CurrentTrain: epoch  7, batch     1 | loss: 4.2372508Losses:  3.974057197570801 0.09390699863433838
CurrentTrain: epoch  7, batch     2 | loss: 4.0679641Losses:  4.003342628479004 0.10048883408308029
CurrentTrain: epoch  7, batch     3 | loss: 4.1038313Losses:  3.970081329345703 0.08701881766319275
CurrentTrain: epoch  7, batch     4 | loss: 4.0571003Losses:  3.960035800933838 0.07683603465557098
CurrentTrain: epoch  7, batch     5 | loss: 4.0368719Losses:  4.007761478424072 0.0949631780385971
CurrentTrain: epoch  7, batch     6 | loss: 4.1027246Losses:  4.01038932800293 0.12027856707572937
CurrentTrain: epoch  7, batch     7 | loss: 4.1306677Losses:  3.939422607421875 0.06699281930923462
CurrentTrain: epoch  7, batch     8 | loss: 4.0064154Losses:  3.9563097953796387 0.12138242274522781
CurrentTrain: epoch  7, batch     9 | loss: 4.0776920Losses:  3.9571547508239746 0.12669499218463898
CurrentTrain: epoch  7, batch    10 | loss: 4.0838499Losses:  3.933744430541992 0.0872042179107666
CurrentTrain: epoch  7, batch    11 | loss: 4.0209484Losses:  3.9439785480499268 0.10425041615962982
CurrentTrain: epoch  7, batch    12 | loss: 4.0482287Losses:  4.035737991333008 0.07669590413570404
CurrentTrain: epoch  7, batch    13 | loss: 4.1124339Losses:  3.9468462467193604 0.10443002730607986
CurrentTrain: epoch  7, batch    14 | loss: 4.0512762Losses:  3.9182705879211426 0.1290753334760666
CurrentTrain: epoch  7, batch    15 | loss: 4.0473461Losses:  4.1189799308776855 0.13760367035865784
CurrentTrain: epoch  7, batch    16 | loss: 4.2565837Losses:  4.106547832489014 0.1761758178472519
CurrentTrain: epoch  7, batch    17 | loss: 4.2827234Losses:  3.96380615234375 0.09022307395935059
CurrentTrain: epoch  7, batch    18 | loss: 4.0540295Losses:  4.018772602081299 0.09735140204429626
CurrentTrain: epoch  7, batch    19 | loss: 4.1161242Losses:  4.024390697479248 0.06259560585021973
CurrentTrain: epoch  7, batch    20 | loss: 4.0869865Losses:  4.480831623077393 0.15823587775230408
CurrentTrain: epoch  7, batch    21 | loss: 4.6390676Losses:  4.027527332305908 0.08886031806468964
CurrentTrain: epoch  7, batch    22 | loss: 4.1163878Losses:  3.974278211593628 0.08309054374694824
CurrentTrain: epoch  7, batch    23 | loss: 4.0573688Losses:  4.023998737335205 0.1214623674750328
CurrentTrain: epoch  7, batch    24 | loss: 4.1454611Losses:  4.055682182312012 0.09217114746570587
CurrentTrain: epoch  7, batch    25 | loss: 4.1478534Losses:  3.957423686981201 0.07261255383491516
CurrentTrain: epoch  7, batch    26 | loss: 4.0300364Losses:  3.9753499031066895 0.09460097551345825
CurrentTrain: epoch  7, batch    27 | loss: 4.0699511Losses:  4.103304862976074 0.10512545704841614
CurrentTrain: epoch  7, batch    28 | loss: 4.2084303Losses:  3.9825639724731445 0.06420134007930756
CurrentTrain: epoch  7, batch    29 | loss: 4.0467653Losses:  4.046298027038574 0.08133257180452347
CurrentTrain: epoch  7, batch    30 | loss: 4.1276307Losses:  4.010084629058838 0.10634402185678482
CurrentTrain: epoch  7, batch    31 | loss: 4.1164289Losses:  3.984215259552002 0.05759431794285774
CurrentTrain: epoch  7, batch    32 | loss: 4.0418096Losses:  4.060420989990234 0.08783893287181854
CurrentTrain: epoch  7, batch    33 | loss: 4.1482601Losses:  4.046839237213135 0.07194080203771591
CurrentTrain: epoch  7, batch    34 | loss: 4.1187801Losses:  3.951814889907837 0.08893441408872604
CurrentTrain: epoch  7, batch    35 | loss: 4.0407491Losses:  4.05094575881958 0.0964023768901825
CurrentTrain: epoch  7, batch    36 | loss: 4.1473479Losses:  4.014892578125 0.07851824909448624
CurrentTrain: epoch  7, batch    37 | loss: 4.0934110Losses:  4.075639724731445 0.0926874428987503
CurrentTrain: epoch  7, batch    38 | loss: 4.1683273Losses:  3.9489824771881104 0.07151340693235397
CurrentTrain: epoch  7, batch    39 | loss: 4.0204959Losses:  4.099592208862305 0.11443664133548737
CurrentTrain: epoch  7, batch    40 | loss: 4.2140288Losses:  3.9894540309906006 0.08745922148227692
CurrentTrain: epoch  7, batch    41 | loss: 4.0769134Losses:  3.9852912425994873 0.08172035217285156
CurrentTrain: epoch  7, batch    42 | loss: 4.0670118Losses:  3.994044780731201 0.08882483839988708
CurrentTrain: epoch  7, batch    43 | loss: 4.0828695Losses:  3.9700303077697754 0.09316898882389069
CurrentTrain: epoch  7, batch    44 | loss: 4.0631995Losses:  3.9322257041931152 0.07657516002655029
CurrentTrain: epoch  7, batch    45 | loss: 4.0088010Losses:  3.9717931747436523 0.07861267775297165
CurrentTrain: epoch  7, batch    46 | loss: 4.0504060Losses:  3.9682350158691406 0.08904905617237091
CurrentTrain: epoch  7, batch    47 | loss: 4.0572839Losses:  4.036674976348877 0.12582868337631226
CurrentTrain: epoch  7, batch    48 | loss: 4.1625037Losses:  4.058725357055664 0.11842374503612518
CurrentTrain: epoch  7, batch    49 | loss: 4.1771493Losses:  3.915656566619873 0.07075561583042145
CurrentTrain: epoch  7, batch    50 | loss: 3.9864123Losses:  3.955864429473877 0.07497622817754745
CurrentTrain: epoch  7, batch    51 | loss: 4.0308409Losses:  4.092647552490234 0.12333567440509796
CurrentTrain: epoch  7, batch    52 | loss: 4.2159834Losses:  4.0231781005859375 0.09084054082632065
CurrentTrain: epoch  7, batch    53 | loss: 4.1140184Losses:  4.026167869567871 0.08722706139087677
CurrentTrain: epoch  7, batch    54 | loss: 4.1133947Losses:  4.033665657043457 0.08178380131721497
CurrentTrain: epoch  7, batch    55 | loss: 4.1154494Losses:  3.9682888984680176 0.07461780309677124
CurrentTrain: epoch  7, batch    56 | loss: 4.0429068Losses:  3.9627180099487305 0.09940799325704575
CurrentTrain: epoch  7, batch    57 | loss: 4.0621262Losses:  3.946615219116211 0.12114954739809036
CurrentTrain: epoch  7, batch    58 | loss: 4.0677648Losses:  3.994528293609619 0.0581212192773819
CurrentTrain: epoch  7, batch    59 | loss: 4.0526495Losses:  4.007405757904053 0.10306444019079208
CurrentTrain: epoch  7, batch    60 | loss: 4.1104703Losses:  3.9659423828125 0.0646352544426918
CurrentTrain: epoch  7, batch    61 | loss: 4.0305777Losses:  4.017012119293213 0.049283239990472794
CurrentTrain: epoch  7, batch    62 | loss: 4.0662951Losses:  4.017247200012207 0.10783131420612335
CurrentTrain: epoch  8, batch     0 | loss: 4.1250787Losses:  4.007037162780762 0.10588900744915009
CurrentTrain: epoch  8, batch     1 | loss: 4.1129260Losses:  3.9777283668518066 0.10712280124425888
CurrentTrain: epoch  8, batch     2 | loss: 4.0848513Losses:  3.9870548248291016 0.05263737589120865
CurrentTrain: epoch  8, batch     3 | loss: 4.0396924Losses:  3.9549367427825928 0.08035320043563843
CurrentTrain: epoch  8, batch     4 | loss: 4.0352898Losses:  3.966782331466675 0.0950082391500473
CurrentTrain: epoch  8, batch     5 | loss: 4.0617905Losses:  4.000091552734375 0.08242030441761017
CurrentTrain: epoch  8, batch     6 | loss: 4.0825119Losses:  3.995126724243164 0.07560005784034729
CurrentTrain: epoch  8, batch     7 | loss: 4.0707269Losses:  3.9656643867492676 0.08843547105789185
CurrentTrain: epoch  8, batch     8 | loss: 4.0541000Losses:  3.9712729454040527 0.09442630410194397
CurrentTrain: epoch  8, batch     9 | loss: 4.0656991Losses:  3.9533095359802246 0.07316567003726959
CurrentTrain: epoch  8, batch    10 | loss: 4.0264754Losses:  3.9452767372131348 0.08777068555355072
CurrentTrain: epoch  8, batch    11 | loss: 4.0330472Losses:  3.9707601070404053 0.08156522363424301
CurrentTrain: epoch  8, batch    12 | loss: 4.0523252Losses:  3.9770145416259766 0.07129264622926712
CurrentTrain: epoch  8, batch    13 | loss: 4.0483074Losses:  3.93971586227417 0.10980181396007538
CurrentTrain: epoch  8, batch    14 | loss: 4.0495176Losses:  3.9725496768951416 0.09564173221588135
CurrentTrain: epoch  8, batch    15 | loss: 4.0681915Losses:  3.9349613189697266 0.05451246351003647
CurrentTrain: epoch  8, batch    16 | loss: 3.9894738Losses:  3.9934585094451904 0.090445876121521
CurrentTrain: epoch  8, batch    17 | loss: 4.0839043Losses:  4.0104146003723145 0.08484172821044922
CurrentTrain: epoch  8, batch    18 | loss: 4.0952563Losses:  3.9850847721099854 0.07648329436779022
CurrentTrain: epoch  8, batch    19 | loss: 4.0615683Losses:  3.892634391784668 0.08654801547527313
CurrentTrain: epoch  8, batch    20 | loss: 3.9791825Losses:  3.964062213897705 0.0831226035952568
CurrentTrain: epoch  8, batch    21 | loss: 4.0471849Losses:  3.9699604511260986 0.08902157843112946
CurrentTrain: epoch  8, batch    22 | loss: 4.0589819Losses:  3.974177122116089 0.10293871909379959
CurrentTrain: epoch  8, batch    23 | loss: 4.0771160Losses:  3.9765701293945312 0.10848647356033325
CurrentTrain: epoch  8, batch    24 | loss: 4.0850568Losses:  3.999495267868042 0.060510482639074326
CurrentTrain: epoch  8, batch    25 | loss: 4.0600057Losses:  3.9650192260742188 0.08498766273260117
CurrentTrain: epoch  8, batch    26 | loss: 4.0500069Losses:  4.009045600891113 0.0986146479845047
CurrentTrain: epoch  8, batch    27 | loss: 4.1076603Losses:  4.000704765319824 0.1070946678519249
CurrentTrain: epoch  8, batch    28 | loss: 4.1077995Losses:  3.965437412261963 0.08792756497859955
CurrentTrain: epoch  8, batch    29 | loss: 4.0533648Losses:  3.9387826919555664 0.0859307199716568
CurrentTrain: epoch  8, batch    30 | loss: 4.0247135Losses:  3.963909149169922 0.08849547803401947
CurrentTrain: epoch  8, batch    31 | loss: 4.0524044Losses:  3.9936838150024414 0.08562281727790833
CurrentTrain: epoch  8, batch    32 | loss: 4.0793066Losses:  3.9493939876556396 0.07326383143663406
CurrentTrain: epoch  8, batch    33 | loss: 4.0226579Losses:  3.9685802459716797 0.10248684883117676
CurrentTrain: epoch  8, batch    34 | loss: 4.0710669Losses:  3.9554312229156494 0.09084419906139374
CurrentTrain: epoch  8, batch    35 | loss: 4.0462756Losses:  3.9588961601257324 0.06798887252807617
CurrentTrain: epoch  8, batch    36 | loss: 4.0268850Losses:  3.9945077896118164 0.08505447953939438
CurrentTrain: epoch  8, batch    37 | loss: 4.0795622Losses:  3.9410927295684814 0.09569992125034332
CurrentTrain: epoch  8, batch    38 | loss: 4.0367928Losses:  3.99064040184021 0.09315548092126846
CurrentTrain: epoch  8, batch    39 | loss: 4.0837960Losses:  4.027283191680908 0.10674639791250229
CurrentTrain: epoch  8, batch    40 | loss: 4.1340294Losses:  3.973796844482422 0.08137845993041992
CurrentTrain: epoch  8, batch    41 | loss: 4.0551753Losses:  3.9787087440490723 0.09058935940265656
CurrentTrain: epoch  8, batch    42 | loss: 4.0692983Losses:  3.994086742401123 0.09833874553442001
CurrentTrain: epoch  8, batch    43 | loss: 4.0924253Losses:  3.9800829887390137 0.090022973716259
CurrentTrain: epoch  8, batch    44 | loss: 4.0701060Losses:  4.020211219787598 0.055864155292510986
CurrentTrain: epoch  8, batch    45 | loss: 4.0760756Losses:  3.932526111602783 0.07010376453399658
CurrentTrain: epoch  8, batch    46 | loss: 4.0026298Losses:  3.9860081672668457 0.09205074608325958
CurrentTrain: epoch  8, batch    47 | loss: 4.0780587Losses:  3.992067575454712 0.06687387824058533
CurrentTrain: epoch  8, batch    48 | loss: 4.0589414Losses:  3.9590840339660645 0.06624150276184082
CurrentTrain: epoch  8, batch    49 | loss: 4.0253258Losses:  3.9578652381896973 0.062420040369033813
CurrentTrain: epoch  8, batch    50 | loss: 4.0202851Losses:  3.9764389991760254 0.06737075001001358
CurrentTrain: epoch  8, batch    51 | loss: 4.0438099Losses:  3.98317289352417 0.11598662286996841
CurrentTrain: epoch  8, batch    52 | loss: 4.0991597Losses:  3.9900500774383545 0.08679692447185516
CurrentTrain: epoch  8, batch    53 | loss: 4.0768471Losses:  3.9487674236297607 0.09210561960935593
CurrentTrain: epoch  8, batch    54 | loss: 4.0408731Losses:  3.96152400970459 0.0778927430510521
CurrentTrain: epoch  8, batch    55 | loss: 4.0394168Losses:  3.972064971923828 0.08305282890796661
CurrentTrain: epoch  8, batch    56 | loss: 4.0551176Losses:  3.9592490196228027 0.09637410193681717
CurrentTrain: epoch  8, batch    57 | loss: 4.0556231Losses:  3.9915947914123535 0.08381685614585876
CurrentTrain: epoch  8, batch    58 | loss: 4.0754118Losses:  3.9841437339782715 0.09772974252700806
CurrentTrain: epoch  8, batch    59 | loss: 4.0818734Losses:  3.92395281791687 0.10196831822395325
CurrentTrain: epoch  8, batch    60 | loss: 4.0259213Losses:  3.972017288208008 0.09464965760707855
CurrentTrain: epoch  8, batch    61 | loss: 4.0666671Losses:  3.933995246887207 0.02288016304373741
CurrentTrain: epoch  8, batch    62 | loss: 3.9568753Losses:  3.9487152099609375 0.058826714754104614
CurrentTrain: epoch  9, batch     0 | loss: 4.0075421Losses:  4.002080917358398 0.06830098479986191
CurrentTrain: epoch  9, batch     1 | loss: 4.0703821Losses:  3.9817042350769043 0.07623639702796936
CurrentTrain: epoch  9, batch     2 | loss: 4.0579405Losses:  4.008419513702393 0.06853805482387543
CurrentTrain: epoch  9, batch     3 | loss: 4.0769577Losses:  3.957608699798584 0.05370495095849037
CurrentTrain: epoch  9, batch     4 | loss: 4.0113134Losses:  3.961040496826172 0.0642026737332344
CurrentTrain: epoch  9, batch     5 | loss: 4.0252433Losses:  3.966160297393799 0.07932420074939728
CurrentTrain: epoch  9, batch     6 | loss: 4.0454845Losses:  3.9499473571777344 0.0654773935675621
CurrentTrain: epoch  9, batch     7 | loss: 4.0154247Losses:  3.930572509765625 0.06474770605564117
CurrentTrain: epoch  9, batch     8 | loss: 3.9953203Losses:  3.9577317237854004 0.08769170939922333
CurrentTrain: epoch  9, batch     9 | loss: 4.0454235Losses:  3.96148943901062 0.08795008063316345
CurrentTrain: epoch  9, batch    10 | loss: 4.0494394Losses:  3.988149642944336 0.09547075629234314
CurrentTrain: epoch  9, batch    11 | loss: 4.0836205Losses:  3.9643735885620117 0.08864009380340576
CurrentTrain: epoch  9, batch    12 | loss: 4.0530138Losses:  3.9656624794006348 0.07773640751838684
CurrentTrain: epoch  9, batch    13 | loss: 4.0433989Losses:  3.974651336669922 0.10558394342660904
CurrentTrain: epoch  9, batch    14 | loss: 4.0802355Losses:  3.979588508605957 0.0880434513092041
CurrentTrain: epoch  9, batch    15 | loss: 4.0676317Losses:  3.9544239044189453 0.08808410912752151
CurrentTrain: epoch  9, batch    16 | loss: 4.0425081Losses:  3.9886245727539062 0.08772511780261993
CurrentTrain: epoch  9, batch    17 | loss: 4.0763497Losses:  3.955207586288452 0.06070661544799805
CurrentTrain: epoch  9, batch    18 | loss: 4.0159140Losses:  3.902061700820923 0.03642524778842926
CurrentTrain: epoch  9, batch    19 | loss: 3.9384871Losses:  3.989532232284546 0.0687422975897789
CurrentTrain: epoch  9, batch    20 | loss: 4.0582747Losses:  3.945164680480957 0.07504212111234665
CurrentTrain: epoch  9, batch    21 | loss: 4.0202069Losses:  3.9984984397888184 0.08120419085025787
CurrentTrain: epoch  9, batch    22 | loss: 4.0797029Losses:  3.9456095695495605 0.07122999429702759
CurrentTrain: epoch  9, batch    23 | loss: 4.0168395Losses:  3.9666457176208496 0.05833593010902405
CurrentTrain: epoch  9, batch    24 | loss: 4.0249815Losses:  3.9829554557800293 0.06672091037034988
CurrentTrain: epoch  9, batch    25 | loss: 4.0496764Losses:  3.938217878341675 0.09084972739219666
CurrentTrain: epoch  9, batch    26 | loss: 4.0290675Losses:  3.948406934738159 0.07126312702894211
CurrentTrain: epoch  9, batch    27 | loss: 4.0196700Losses:  3.9516489505767822 0.09537611901760101
CurrentTrain: epoch  9, batch    28 | loss: 4.0470252Losses:  3.93458890914917 0.0752621740102768
CurrentTrain: epoch  9, batch    29 | loss: 4.0098510Losses:  3.9721713066101074 0.08951090276241302
CurrentTrain: epoch  9, batch    30 | loss: 4.0616822Losses:  3.9467828273773193 0.09971573948860168
CurrentTrain: epoch  9, batch    31 | loss: 4.0464988Losses:  3.91654634475708 0.06614719331264496
CurrentTrain: epoch  9, batch    32 | loss: 3.9826934Losses:  3.9190478324890137 0.03510250523686409
CurrentTrain: epoch  9, batch    33 | loss: 3.9541504Losses:  3.9486804008483887 0.08572830259799957
CurrentTrain: epoch  9, batch    34 | loss: 4.0344086Losses:  3.9490854740142822 0.0989515408873558
CurrentTrain: epoch  9, batch    35 | loss: 4.0480371Losses:  3.939584255218506 0.04313140735030174
CurrentTrain: epoch  9, batch    36 | loss: 3.9827156Losses:  3.9328794479370117 0.07705453783273697
CurrentTrain: epoch  9, batch    37 | loss: 4.0099339Losses:  3.991067409515381 0.05332191661000252
CurrentTrain: epoch  9, batch    38 | loss: 4.0443892Losses:  3.9549713134765625 0.0912046954035759
CurrentTrain: epoch  9, batch    39 | loss: 4.0461760Losses:  4.028439521789551 0.0632849857211113
CurrentTrain: epoch  9, batch    40 | loss: 4.0917244Losses:  3.9392459392547607 0.04554462432861328
CurrentTrain: epoch  9, batch    41 | loss: 3.9847906Losses:  3.9662699699401855 0.04292622581124306
CurrentTrain: epoch  9, batch    42 | loss: 4.0091963Losses:  3.9480724334716797 0.04292883723974228
CurrentTrain: epoch  9, batch    43 | loss: 3.9910014Losses:  3.9561767578125 0.06248115748167038
CurrentTrain: epoch  9, batch    44 | loss: 4.0186577Losses:  3.933276414871216 0.07463020086288452
CurrentTrain: epoch  9, batch    45 | loss: 4.0079064Losses:  3.9351582527160645 0.049443550407886505
CurrentTrain: epoch  9, batch    46 | loss: 3.9846017Losses:  3.9781532287597656 0.060248635709285736
CurrentTrain: epoch  9, batch    47 | loss: 4.0384021Losses:  3.949141502380371 0.06310387700796127
CurrentTrain: epoch  9, batch    48 | loss: 4.0122452Losses:  4.017725467681885 0.09419400244951248
CurrentTrain: epoch  9, batch    49 | loss: 4.1119194Losses:  3.962352752685547 0.08536022156476974
CurrentTrain: epoch  9, batch    50 | loss: 4.0477128Losses:  3.9953174591064453 0.08717714250087738
CurrentTrain: epoch  9, batch    51 | loss: 4.0824947Losses:  3.9565207958221436 0.05457611009478569
CurrentTrain: epoch  9, batch    52 | loss: 4.0110970Losses:  3.937223434448242 0.08317916095256805
CurrentTrain: epoch  9, batch    53 | loss: 4.0204024Losses:  3.95407772064209 0.07146633416414261
CurrentTrain: epoch  9, batch    54 | loss: 4.0255442Losses:  3.9386425018310547 0.05361858382821083
CurrentTrain: epoch  9, batch    55 | loss: 3.9922612Losses:  3.916494131088257 0.07639212906360626
CurrentTrain: epoch  9, batch    56 | loss: 3.9928863Losses:  3.9577367305755615 0.08786025643348694
CurrentTrain: epoch  9, batch    57 | loss: 4.0455971Losses:  3.9214019775390625 0.05535278469324112
CurrentTrain: epoch  9, batch    58 | loss: 3.9767547Losses:  3.927236795425415 0.08033417165279388
CurrentTrain: epoch  9, batch    59 | loss: 4.0075707Losses:  3.9681813716888428 0.0811619907617569
CurrentTrain: epoch  9, batch    60 | loss: 4.0493436Losses:  3.949354887008667 0.07528989017009735
CurrentTrain: epoch  9, batch    61 | loss: 4.0246449Losses:  3.9536945819854736 0.03355848789215088
CurrentTrain: epoch  9, batch    62 | loss: 3.9872532
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.19%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.08%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.06%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.35%   
cur_acc:  ['0.9435']
his_acc:  ['0.9435']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  8.213312149047852 1.2150464057922363
CurrentTrain: epoch  0, batch     0 | loss: 9.4283581Losses:  10.0943603515625 1.0604689121246338
CurrentTrain: epoch  0, batch     1 | loss: 11.1548290Losses:  7.165970802307129 1.2934253215789795
CurrentTrain: epoch  0, batch     2 | loss: 8.4593964Losses:  8.2793607711792 1.1920928955078125e-07
CurrentTrain: epoch  0, batch     3 | loss: 8.2793608Losses:  4.4615068435668945 1.1533913612365723
CurrentTrain: epoch  1, batch     0 | loss: 5.6148982Losses:  4.521435737609863 1.1448843479156494
CurrentTrain: epoch  1, batch     1 | loss: 5.6663198Losses:  4.467973709106445 1.1529757976531982
CurrentTrain: epoch  1, batch     2 | loss: 5.6209497Losses:  2.5232155323028564 0.29320967197418213
CurrentTrain: epoch  1, batch     3 | loss: 2.8164253Losses:  4.353664398193359 0.9773678779602051
CurrentTrain: epoch  2, batch     0 | loss: 5.3310323Losses:  4.169283866882324 0.9939963817596436
CurrentTrain: epoch  2, batch     1 | loss: 5.1632805Losses:  3.583934783935547 0.9058253765106201
CurrentTrain: epoch  2, batch     2 | loss: 4.4897604Losses:  4.47068977355957 0.14846116304397583
CurrentTrain: epoch  2, batch     3 | loss: 4.6191511Losses:  4.145238399505615 0.9555293321609497
CurrentTrain: epoch  3, batch     0 | loss: 5.1007676Losses:  3.243748188018799 0.8413376808166504
CurrentTrain: epoch  3, batch     1 | loss: 4.0850859Losses:  3.526359796524048 1.072614312171936
CurrentTrain: epoch  3, batch     2 | loss: 4.5989742Losses:  2.5118837356567383 0.2950698733329773
CurrentTrain: epoch  3, batch     3 | loss: 2.8069537Losses:  3.158702850341797 0.7675248384475708
CurrentTrain: epoch  4, batch     0 | loss: 3.9262276Losses:  2.9264752864837646 0.7523612380027771
CurrentTrain: epoch  4, batch     1 | loss: 3.6788366Losses:  3.606992244720459 0.8575538396835327
CurrentTrain: epoch  4, batch     2 | loss: 4.4645462Losses:  2.4409332275390625 0.03485792130231857
CurrentTrain: epoch  4, batch     3 | loss: 2.4757912Losses:  3.43666410446167 0.7492768168449402
CurrentTrain: epoch  5, batch     0 | loss: 4.1859407Losses:  2.632491111755371 0.7246254086494446
CurrentTrain: epoch  5, batch     1 | loss: 3.3571165Losses:  2.7107291221618652 0.8864240646362305
CurrentTrain: epoch  5, batch     2 | loss: 3.5971532Losses:  4.326835632324219 0.04393491894006729
CurrentTrain: epoch  5, batch     3 | loss: 4.3707705Losses:  2.756441593170166 0.7144433259963989
CurrentTrain: epoch  6, batch     0 | loss: 3.4708848Losses:  2.7045695781707764 0.6819783449172974
CurrentTrain: epoch  6, batch     1 | loss: 3.3865480Losses:  2.971970319747925 0.7982250452041626
CurrentTrain: epoch  6, batch     2 | loss: 3.7701955Losses:  2.5014431476593018 8.94069742685133e-08
CurrentTrain: epoch  6, batch     3 | loss: 2.5014431Losses:  2.8165059089660645 0.7076680660247803
CurrentTrain: epoch  7, batch     0 | loss: 3.5241740Losses:  2.491978406906128 0.7962421178817749
CurrentTrain: epoch  7, batch     1 | loss: 3.2882204Losses:  2.432983875274658 0.6044115424156189
CurrentTrain: epoch  7, batch     2 | loss: 3.0373955Losses:  1.979142665863037 5.960464477539063e-08
CurrentTrain: epoch  7, batch     3 | loss: 1.9791427Losses:  2.3285717964172363 0.6758246421813965
CurrentTrain: epoch  8, batch     0 | loss: 3.0043964Losses:  2.4080138206481934 0.5830145478248596
CurrentTrain: epoch  8, batch     1 | loss: 2.9910283Losses:  2.527967691421509 0.6943151950836182
CurrentTrain: epoch  8, batch     2 | loss: 3.2222829Losses:  1.779616355895996 0.04467124491930008
CurrentTrain: epoch  8, batch     3 | loss: 1.8242877Losses:  2.180633068084717 0.7183741331100464
CurrentTrain: epoch  9, batch     0 | loss: 2.8990073Losses:  2.4313158988952637 0.5181066989898682
CurrentTrain: epoch  9, batch     1 | loss: 2.9494226Losses:  2.0002613067626953 0.5925874710083008
CurrentTrain: epoch  9, batch     2 | loss: 2.5928488Losses:  1.7969715595245361 0.048323117196559906
CurrentTrain: epoch  9, batch     3 | loss: 1.8452947
Losses:  6.189332962036133 0.8731368780136108
MemoryTrain:  epoch  0, batch     0 | loss: 7.0624700Losses:  9.041006088256836 0.060796108096838
MemoryTrain:  epoch  0, batch     1 | loss: 9.1018019Losses:  1.8275233507156372 0.7174755334854126
MemoryTrain:  epoch  1, batch     0 | loss: 2.5449989Losses:  3.032029151916504 0.4833868443965912
MemoryTrain:  epoch  1, batch     1 | loss: 3.5154159Losses:  1.7823847532272339 0.7342532873153687
MemoryTrain:  epoch  2, batch     0 | loss: 2.5166380Losses:  0.5962365865707397 0.25698691606521606
MemoryTrain:  epoch  2, batch     1 | loss: 0.8532235Losses:  1.3208754062652588 0.6565218567848206
MemoryTrain:  epoch  3, batch     0 | loss: 1.9773972Losses:  1.1245033740997314 0.3096216320991516
MemoryTrain:  epoch  3, batch     1 | loss: 1.4341249Losses:  1.254934549331665 0.8059791326522827
MemoryTrain:  epoch  4, batch     0 | loss: 2.0609136Losses:  0.26051872968673706 0.18216918408870697
MemoryTrain:  epoch  4, batch     1 | loss: 0.4426879Losses:  1.0733740329742432 0.7958884239196777
MemoryTrain:  epoch  5, batch     0 | loss: 1.8692625Losses:  0.4204055070877075 0.04108058661222458
MemoryTrain:  epoch  5, batch     1 | loss: 0.4614861Losses:  0.9582921266555786 0.755144476890564
MemoryTrain:  epoch  6, batch     0 | loss: 1.7134366Losses:  0.30364492535591125 0.169922336935997
MemoryTrain:  epoch  6, batch     1 | loss: 0.4735672Losses:  0.7275713682174683 0.7384166121482849
MemoryTrain:  epoch  7, batch     0 | loss: 1.4659879Losses:  1.0947386026382446 0.11339827626943588
MemoryTrain:  epoch  7, batch     1 | loss: 1.2081369Losses:  0.822932779788971 0.6842584013938904
MemoryTrain:  epoch  8, batch     0 | loss: 1.5071912Losses:  0.3193742036819458 0.31087440252304077
MemoryTrain:  epoch  8, batch     1 | loss: 0.6302486Losses:  0.6758714914321899 0.7316505908966064
MemoryTrain:  epoch  9, batch     0 | loss: 1.4075221Losses:  0.4547916054725647 0.08768020570278168
MemoryTrain:  epoch  9, batch     1 | loss: 0.5424718
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 88.12%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 83.93%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 73.68%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 73.24%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 71.59%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 70.42%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 69.57%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.83%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.16%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 93.44%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.24%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.15%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 93.07%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 93.28%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.39%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.14%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 93.07%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.91%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.85%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.69%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.55%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 92.41%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 92.11%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 92.13%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.87%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.82%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 91.69%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 91.57%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 91.31%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 90.94%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 90.49%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 90.18%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 89.81%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 89.25%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 88.63%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 88.29%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 87.83%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 86.99%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 86.68%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 86.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 85.64%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 85.36%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 85.07%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.86%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.64%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 84.43%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.00%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 83.39%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.86%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 82.33%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.81%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.53%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 81.36%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 82.04%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 82.28%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.80%   
cur_acc:  ['0.9435', '0.7183']
his_acc:  ['0.9435', '0.8280']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  7.264342308044434 1.6264690160751343
CurrentTrain: epoch  0, batch     0 | loss: 8.8908110Losses:  8.638877868652344 1.6171793937683105
CurrentTrain: epoch  0, batch     1 | loss: 10.2560577Losses:  6.771636009216309 1.7460509538650513
CurrentTrain: epoch  0, batch     2 | loss: 8.5176868Losses:  6.46412467956543 0.44790467619895935
CurrentTrain: epoch  0, batch     3 | loss: 6.9120293Losses:  3.881145715713501 1.5251586437225342
CurrentTrain: epoch  1, batch     0 | loss: 5.4063044Losses:  4.638448715209961 1.5843892097473145
CurrentTrain: epoch  1, batch     1 | loss: 6.2228379Losses:  3.6139607429504395 1.4441888332366943
CurrentTrain: epoch  1, batch     2 | loss: 5.0581493Losses:  4.332423686981201 0.23877748847007751
CurrentTrain: epoch  1, batch     3 | loss: 4.5712013Losses:  3.187654972076416 1.0596271753311157
CurrentTrain: epoch  2, batch     0 | loss: 4.2472820Losses:  3.822497844696045 1.3735640048980713
CurrentTrain: epoch  2, batch     1 | loss: 5.1960621Losses:  4.330384731292725 1.6221733093261719
CurrentTrain: epoch  2, batch     2 | loss: 5.9525580Losses:  2.134335517883301 0.10055316984653473
CurrentTrain: epoch  2, batch     3 | loss: 2.2348888Losses:  3.497305393218994 1.3467730283737183
CurrentTrain: epoch  3, batch     0 | loss: 4.8440785Losses:  3.18977952003479 1.1265913248062134
CurrentTrain: epoch  3, batch     1 | loss: 4.3163710Losses:  3.7834632396698 1.3262004852294922
CurrentTrain: epoch  3, batch     2 | loss: 5.1096640Losses:  4.00731897354126 0.15001671016216278
CurrentTrain: epoch  3, batch     3 | loss: 4.1573358Losses:  3.768711566925049 1.2240979671478271
CurrentTrain: epoch  4, batch     0 | loss: 4.9928093Losses:  3.3549551963806152 1.259380578994751
CurrentTrain: epoch  4, batch     1 | loss: 4.6143360Losses:  2.748974561691284 0.9262301325798035
CurrentTrain: epoch  4, batch     2 | loss: 3.6752048Losses:  3.019650936126709 0.18055713176727295
CurrentTrain: epoch  4, batch     3 | loss: 3.2002082Losses:  3.612466812133789 1.141772985458374
CurrentTrain: epoch  5, batch     0 | loss: 4.7542400Losses:  2.7871620655059814 0.8748297095298767
CurrentTrain: epoch  5, batch     1 | loss: 3.6619918Losses:  2.8827271461486816 0.7908865809440613
CurrentTrain: epoch  5, batch     2 | loss: 3.6736138Losses:  4.707403659820557 0.592555046081543
CurrentTrain: epoch  5, batch     3 | loss: 5.2999587Losses:  3.2200393676757812 1.212354302406311
CurrentTrain: epoch  6, batch     0 | loss: 4.4323936Losses:  3.189484119415283 1.0811245441436768
CurrentTrain: epoch  6, batch     1 | loss: 4.2706089Losses:  2.594665288925171 0.7875035405158997
CurrentTrain: epoch  6, batch     2 | loss: 3.3821688Losses:  2.4014406204223633 0.15407797694206238
CurrentTrain: epoch  6, batch     3 | loss: 2.5555186Losses:  2.6503958702087402 0.8861485123634338
CurrentTrain: epoch  7, batch     0 | loss: 3.5365443Losses:  2.891411542892456 0.8528940677642822
CurrentTrain: epoch  7, batch     1 | loss: 3.7443056Losses:  2.909557819366455 0.9343773126602173
CurrentTrain: epoch  7, batch     2 | loss: 3.8439350Losses:  3.8942320346832275 0.463577538728714
CurrentTrain: epoch  7, batch     3 | loss: 4.3578095Losses:  2.818148136138916 0.9966756105422974
CurrentTrain: epoch  8, batch     0 | loss: 3.8148236Losses:  2.798943042755127 0.9768699407577515
CurrentTrain: epoch  8, batch     1 | loss: 3.7758131Losses:  2.565277338027954 0.6236860156059265
CurrentTrain: epoch  8, batch     2 | loss: 3.1889634Losses:  2.1485378742218018 0.06486505270004272
CurrentTrain: epoch  8, batch     3 | loss: 2.2134030Losses:  2.453166961669922 0.723973274230957
CurrentTrain: epoch  9, batch     0 | loss: 3.1771402Losses:  2.252511978149414 0.6102466583251953
CurrentTrain: epoch  9, batch     1 | loss: 2.8627586Losses:  2.882106304168701 0.8314160108566284
CurrentTrain: epoch  9, batch     2 | loss: 3.7135224Losses:  3.8757433891296387 0.5162386894226074
CurrentTrain: epoch  9, batch     3 | loss: 4.3919821
Losses:  5.711088180541992 0.9813317060470581
MemoryTrain:  epoch  0, batch     0 | loss: 6.6924200Losses:  8.382582664489746 0.9032329320907593
MemoryTrain:  epoch  0, batch     1 | loss: 9.2858152Losses:  1.1161437034606934 0.9312196969985962
MemoryTrain:  epoch  1, batch     0 | loss: 2.0473633Losses:  1.1877514123916626 0.7673175930976868
MemoryTrain:  epoch  1, batch     1 | loss: 1.9550691Losses:  0.8299811482429504 0.7764290571212769
MemoryTrain:  epoch  2, batch     0 | loss: 1.6064103Losses:  1.1247700452804565 0.795993447303772
MemoryTrain:  epoch  2, batch     1 | loss: 1.9207635Losses:  0.8659551739692688 0.7808276414871216
MemoryTrain:  epoch  3, batch     0 | loss: 1.6467829Losses:  0.7129486203193665 0.7085688710212708
MemoryTrain:  epoch  3, batch     1 | loss: 1.4215175Losses:  0.8843475580215454 0.9341608285903931
MemoryTrain:  epoch  4, batch     0 | loss: 1.8185084Losses:  0.5911639928817749 0.586181640625
MemoryTrain:  epoch  4, batch     1 | loss: 1.1773456Losses:  0.6495954990386963 0.7879064679145813
MemoryTrain:  epoch  5, batch     0 | loss: 1.4375019Losses:  0.6562149524688721 0.7029531002044678
MemoryTrain:  epoch  5, batch     1 | loss: 1.3591681Losses:  0.6497887372970581 0.7815231084823608
MemoryTrain:  epoch  6, batch     0 | loss: 1.4313118Losses:  0.501732349395752 0.7322131991386414
MemoryTrain:  epoch  6, batch     1 | loss: 1.2339456Losses:  0.7040578126907349 0.6277142763137817
MemoryTrain:  epoch  7, batch     0 | loss: 1.3317721Losses:  0.5870354771614075 0.800235390663147
MemoryTrain:  epoch  7, batch     1 | loss: 1.3872709Losses:  0.5240645408630371 0.8551250100135803
MemoryTrain:  epoch  8, batch     0 | loss: 1.3791895Losses:  0.5217029452323914 0.4923269748687744
MemoryTrain:  epoch  8, batch     1 | loss: 1.0140300Losses:  0.48440974950790405 0.8259608745574951
MemoryTrain:  epoch  9, batch     0 | loss: 1.3103707Losses:  0.482318639755249 0.5375593900680542
MemoryTrain:  epoch  9, batch     1 | loss: 1.0198780
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.29%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 61.52%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 63.42%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 71.01%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 70.71%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 70.79%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 70.64%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 71.44%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.53%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.95%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.21%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 93.01%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 92.71%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 92.09%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.21%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.14%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.30%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 92.36%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 92.29%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 92.15%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 92.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.02%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.83%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 91.69%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 91.48%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.46%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 91.19%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 90.81%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 90.70%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 90.30%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 90.06%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 89.75%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 89.31%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 89.15%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 88.79%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 88.17%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 87.57%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 87.11%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 86.65%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 86.40%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 85.91%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 85.67%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 85.12%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.78%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.56%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.01%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 83.87%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.67%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.24%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 82.58%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 82.00%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 81.31%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 80.80%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 80.41%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 80.14%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 81.09%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 81.20%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 81.50%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 81.30%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 81.15%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 81.06%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.34%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 81.75%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 81.29%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 80.98%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 80.54%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 80.28%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 80.03%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 79.69%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 80.06%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 80.46%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 79.93%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 79.40%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 78.96%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 78.61%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 78.10%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 77.64%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.79%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.95%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 78.09%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.48%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.74%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 78.93%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 78.78%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 78.58%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 78.52%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 78.54%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 78.52%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 78.50%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 78.41%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 78.35%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 78.40%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 78.42%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 78.48%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.49%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 78.51%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 78.29%   
cur_acc:  ['0.9435', '0.7183', '0.7153']
his_acc:  ['0.9435', '0.8280', '0.7829']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  7.6111955642700195 1.601461410522461
CurrentTrain: epoch  0, batch     0 | loss: 9.2126570Losses:  7.731202125549316 1.4788970947265625
CurrentTrain: epoch  0, batch     1 | loss: 9.2100992Losses:  7.24416971206665 1.6359935998916626
CurrentTrain: epoch  0, batch     2 | loss: 8.8801632Losses:  3.7284951210021973 0.3624011278152466
CurrentTrain: epoch  0, batch     3 | loss: 4.0908961Losses:  3.325209140777588 1.4240802526474
CurrentTrain: epoch  1, batch     0 | loss: 4.7492895Losses:  2.9774582386016846 1.1656774282455444
CurrentTrain: epoch  1, batch     1 | loss: 4.1431355Losses:  3.219243049621582 1.7240650653839111
CurrentTrain: epoch  1, batch     2 | loss: 4.9433079Losses:  2.8083064556121826 0.19022108614444733
CurrentTrain: epoch  1, batch     3 | loss: 2.9985275Losses:  3.479257583618164 1.4219406843185425
CurrentTrain: epoch  2, batch     0 | loss: 4.9011984Losses:  2.7720999717712402 1.3155673742294312
CurrentTrain: epoch  2, batch     1 | loss: 4.0876675Losses:  2.3322131633758545 1.0778673887252808
CurrentTrain: epoch  2, batch     2 | loss: 3.4100804Losses:  2.198676824569702 0.10972355306148529
CurrentTrain: epoch  2, batch     3 | loss: 2.3084004Losses:  2.614271879196167 1.0440219640731812
CurrentTrain: epoch  3, batch     0 | loss: 3.6582937Losses:  2.550139904022217 1.3301401138305664
CurrentTrain: epoch  3, batch     1 | loss: 3.8802800Losses:  2.510806083679199 1.225626826286316
CurrentTrain: epoch  3, batch     2 | loss: 3.7364330Losses:  1.8449959754943848 0.13766692578792572
CurrentTrain: epoch  3, batch     3 | loss: 1.9826629Losses:  2.319916009902954 0.8745934963226318
CurrentTrain: epoch  4, batch     0 | loss: 3.1945095Losses:  2.311882972717285 0.902961254119873
CurrentTrain: epoch  4, batch     1 | loss: 3.2148442Losses:  2.59547758102417 0.8197448253631592
CurrentTrain: epoch  4, batch     2 | loss: 3.4152224Losses:  2.1377077102661133 0.12387891113758087
CurrentTrain: epoch  4, batch     3 | loss: 2.2615867Losses:  2.4928760528564453 0.9358283877372742
CurrentTrain: epoch  5, batch     0 | loss: 3.4287045Losses:  2.4010634422302246 0.8796751499176025
CurrentTrain: epoch  5, batch     1 | loss: 3.2807386Losses:  2.1219382286071777 1.0375698804855347
CurrentTrain: epoch  5, batch     2 | loss: 3.1595082Losses:  1.9538538455963135 8.94069742685133e-08
CurrentTrain: epoch  5, batch     3 | loss: 1.9538540Losses:  2.0051536560058594 0.7291083931922913
CurrentTrain: epoch  6, batch     0 | loss: 2.7342620Losses:  2.283059597015381 0.7172982692718506
CurrentTrain: epoch  6, batch     1 | loss: 3.0003579Losses:  2.3652892112731934 0.9233483076095581
CurrentTrain: epoch  6, batch     2 | loss: 3.2886376Losses:  2.161001205444336 0.1722523421049118
CurrentTrain: epoch  6, batch     3 | loss: 2.3332536Losses:  2.0266692638397217 0.6727205514907837
CurrentTrain: epoch  7, batch     0 | loss: 2.6993899Losses:  2.0339674949645996 0.9584754109382629
CurrentTrain: epoch  7, batch     1 | loss: 2.9924428Losses:  2.2050163745880127 0.8731441497802734
CurrentTrain: epoch  7, batch     2 | loss: 3.0781605Losses:  2.3282179832458496 0.22994005680084229
CurrentTrain: epoch  7, batch     3 | loss: 2.5581579Losses:  2.2001662254333496 0.7768522500991821
CurrentTrain: epoch  8, batch     0 | loss: 2.9770184Losses:  2.02225923538208 0.6664415597915649
CurrentTrain: epoch  8, batch     1 | loss: 2.6887007Losses:  1.894856333732605 0.6220917701721191
CurrentTrain: epoch  8, batch     2 | loss: 2.5169482Losses:  2.0471043586730957 0.12753431499004364
CurrentTrain: epoch  8, batch     3 | loss: 2.1746387Losses:  1.9876124858856201 0.6749616265296936
CurrentTrain: epoch  9, batch     0 | loss: 2.6625741Losses:  2.1377716064453125 0.553895890712738
CurrentTrain: epoch  9, batch     1 | loss: 2.6916676Losses:  1.8562051057815552 0.6185815334320068
CurrentTrain: epoch  9, batch     2 | loss: 2.4747868Losses:  1.9398034811019897 0.391141802072525
CurrentTrain: epoch  9, batch     3 | loss: 2.3309453
Losses:  5.515669822692871 0.9071786999702454
MemoryTrain:  epoch  0, batch     0 | loss: 6.4228487Losses:  8.4889497756958 0.5969557762145996
MemoryTrain:  epoch  0, batch     1 | loss: 9.0859051Losses:  10.068660736083984 0.4089510440826416
MemoryTrain:  epoch  0, batch     2 | loss: 10.4776115Losses:  1.1510872840881348 0.7234333753585815
MemoryTrain:  epoch  1, batch     0 | loss: 1.8745207Losses:  1.27341628074646 0.6384321451187134
MemoryTrain:  epoch  1, batch     1 | loss: 1.9118484Losses:  1.1963531970977783 0.5894930362701416
MemoryTrain:  epoch  1, batch     2 | loss: 1.7858462Losses:  1.0668373107910156 0.8056427836418152
MemoryTrain:  epoch  2, batch     0 | loss: 1.8724802Losses:  1.0929932594299316 0.6969857215881348
MemoryTrain:  epoch  2, batch     1 | loss: 1.7899790Losses:  1.265389323234558 0.4378023147583008
MemoryTrain:  epoch  2, batch     2 | loss: 1.7031916Losses:  0.9259486794471741 0.8808937668800354
MemoryTrain:  epoch  3, batch     0 | loss: 1.8068424Losses:  0.9249963760375977 0.5040655136108398
MemoryTrain:  epoch  3, batch     1 | loss: 1.4290619Losses:  0.9669061899185181 0.47043490409851074
MemoryTrain:  epoch  3, batch     2 | loss: 1.4373411Losses:  0.6600969433784485 0.777445912361145
MemoryTrain:  epoch  4, batch     0 | loss: 1.4375429Losses:  1.01253080368042 0.6316853761672974
MemoryTrain:  epoch  4, batch     1 | loss: 1.6442162Losses:  0.8669978976249695 0.4539077579975128
MemoryTrain:  epoch  4, batch     2 | loss: 1.3209057Losses:  0.7379045486450195 0.6729578971862793
MemoryTrain:  epoch  5, batch     0 | loss: 1.4108624Losses:  0.6778597831726074 0.928920567035675
MemoryTrain:  epoch  5, batch     1 | loss: 1.6067803Losses:  0.6348423957824707 0.3596581220626831
MemoryTrain:  epoch  5, batch     2 | loss: 0.9945005Losses:  0.632995069026947 0.6177303791046143
MemoryTrain:  epoch  6, batch     0 | loss: 1.2507255Losses:  0.7116473913192749 0.7015092372894287
MemoryTrain:  epoch  6, batch     1 | loss: 1.4131566Losses:  0.7221387028694153 0.5264143943786621
MemoryTrain:  epoch  6, batch     2 | loss: 1.2485530Losses:  0.6382051110267639 0.6025712490081787
MemoryTrain:  epoch  7, batch     0 | loss: 1.2407763Losses:  0.6880621910095215 0.6244953870773315
MemoryTrain:  epoch  7, batch     1 | loss: 1.3125576Losses:  0.6141283512115479 0.49672722816467285
MemoryTrain:  epoch  7, batch     2 | loss: 1.1108556Losses:  0.5484491586685181 0.520369291305542
MemoryTrain:  epoch  8, batch     0 | loss: 1.0688184Losses:  0.6559062004089355 0.7661170959472656
MemoryTrain:  epoch  8, batch     1 | loss: 1.4220233Losses:  0.6598093509674072 0.4130197763442993
MemoryTrain:  epoch  8, batch     2 | loss: 1.0728291Losses:  0.5782451033592224 0.803446888923645
MemoryTrain:  epoch  9, batch     0 | loss: 1.3816919Losses:  0.588397741317749 0.5682144165039062
MemoryTrain:  epoch  9, batch     1 | loss: 1.1566122Losses:  0.48296642303466797 0.3170168399810791
MemoryTrain:  epoch  9, batch     2 | loss: 0.7999833
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 12.50%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 57.29%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 55.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 65.04%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.87%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 90.80%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 90.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.85%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 90.57%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 90.30%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 90.15%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.90%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 89.65%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 89.52%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 89.65%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 89.83%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.89%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 90.04%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 90.14%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 90.15%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 90.12%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 90.17%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.05%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 89.85%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.74%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.64%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 89.61%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 89.51%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 89.18%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 88.86%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 88.69%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 88.31%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 88.15%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 87.57%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 87.36%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 86.94%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 86.53%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 86.26%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 85.87%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 85.35%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 84.77%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 84.34%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 83.92%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.70%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 83.23%   [EVAL] batch:   98 | acc: 56.25%,  total acc: 82.95%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 82.38%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 81.99%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 81.68%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.43%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 81.19%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 81.01%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 80.90%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 80.37%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.75%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.19%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.52%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 78.04%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.62%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.38%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.58%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.77%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 79.17%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 78.83%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 78.70%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.63%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.37%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.47%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 79.44%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 79.00%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 78.66%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 78.24%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 77.99%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 77.75%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 77.43%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 77.81%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 77.34%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 76.84%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 76.50%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 76.01%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 75.56%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.56%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 76.13%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 76.12%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 76.06%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.98%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.94%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 76.00%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 76.06%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.15%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.34%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 76.46%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.69%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 76.80%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 76.63%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 76.55%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 76.31%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 76.24%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 76.21%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 75.89%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 75.73%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 75.61%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 75.39%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 75.09%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 74.82%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 74.55%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.32%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 74.06%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 74.74%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 74.83%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.89%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 74.94%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 74.97%   [EVAL] batch:  224 | acc: 75.00%,  total acc: 74.97%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 76.26%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 76.33%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 76.37%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 76.34%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 76.38%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.85%   
cur_acc:  ['0.9435', '0.7183', '0.7153', '0.7768']
his_acc:  ['0.9435', '0.8280', '0.7829', '0.7685']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  6.843903541564941 1.3041768074035645
CurrentTrain: epoch  0, batch     0 | loss: 8.1480808Losses:  8.003800392150879 1.534839153289795
CurrentTrain: epoch  0, batch     1 | loss: 9.5386391Losses:  6.866499900817871 1.2186329364776611
CurrentTrain: epoch  0, batch     2 | loss: 8.0851326Losses:  5.207333087921143 1.1920930376163597e-07
CurrentTrain: epoch  0, batch     3 | loss: 5.2073331Losses:  4.090404510498047 1.2879605293273926
CurrentTrain: epoch  1, batch     0 | loss: 5.3783650Losses:  3.5161426067352295 1.2894086837768555
CurrentTrain: epoch  1, batch     1 | loss: 4.8055515Losses:  3.1876883506774902 1.1878397464752197
CurrentTrain: epoch  1, batch     2 | loss: 4.3755283Losses:  3.3044657707214355 0.4267023205757141
CurrentTrain: epoch  1, batch     3 | loss: 3.7311680Losses:  3.3764352798461914 1.3567137718200684
CurrentTrain: epoch  2, batch     0 | loss: 4.7331491Losses:  3.6149234771728516 1.2122386693954468
CurrentTrain: epoch  2, batch     1 | loss: 4.8271623Losses:  3.0506229400634766 1.0606727600097656
CurrentTrain: epoch  2, batch     2 | loss: 4.1112957Losses:  2.1483218669891357 0.060093119740486145
CurrentTrain: epoch  2, batch     3 | loss: 2.2084150Losses:  3.415536403656006 1.082607626914978
CurrentTrain: epoch  3, batch     0 | loss: 4.4981441Losses:  2.8583028316497803 0.8637768030166626
CurrentTrain: epoch  3, batch     1 | loss: 3.7220798Losses:  2.862537384033203 1.0053563117980957
CurrentTrain: epoch  3, batch     2 | loss: 3.8678937Losses:  3.5552728176116943 0.37492161989212036
CurrentTrain: epoch  3, batch     3 | loss: 3.9301944Losses:  2.7123374938964844 0.8780828714370728
CurrentTrain: epoch  4, batch     0 | loss: 3.5904202Losses:  3.1169233322143555 0.8591352701187134
CurrentTrain: epoch  4, batch     1 | loss: 3.9760585Losses:  2.846035957336426 0.9830304384231567
CurrentTrain: epoch  4, batch     2 | loss: 3.8290663Losses:  1.7855746746063232 0.10280273854732513
CurrentTrain: epoch  4, batch     3 | loss: 1.8883774Losses:  2.7148070335388184 1.0563428401947021
CurrentTrain: epoch  5, batch     0 | loss: 3.7711499Losses:  2.5259087085723877 0.7209365367889404
CurrentTrain: epoch  5, batch     1 | loss: 3.2468452Losses:  2.753526210784912 0.8238621950149536
CurrentTrain: epoch  5, batch     2 | loss: 3.5773883Losses:  3.519618272781372 0.24113059043884277
CurrentTrain: epoch  5, batch     3 | loss: 3.7607489Losses:  2.5816164016723633 0.7230769395828247
CurrentTrain: epoch  6, batch     0 | loss: 3.3046932Losses:  2.5503058433532715 0.7120755910873413
CurrentTrain: epoch  6, batch     1 | loss: 3.2623816Losses:  2.672366142272949 0.8901458978652954
CurrentTrain: epoch  6, batch     2 | loss: 3.5625119Losses:  2.1730215549468994 0.13368479907512665
CurrentTrain: epoch  6, batch     3 | loss: 2.3067064Losses:  2.389247417449951 0.7798906564712524
CurrentTrain: epoch  7, batch     0 | loss: 3.1691380Losses:  2.4327802658081055 0.7311618328094482
CurrentTrain: epoch  7, batch     1 | loss: 3.1639421Losses:  2.541963577270508 0.7339867949485779
CurrentTrain: epoch  7, batch     2 | loss: 3.2759504Losses:  2.6600122451782227 0.32135283946990967
CurrentTrain: epoch  7, batch     3 | loss: 2.9813652Losses:  2.2190287113189697 0.7152265310287476
CurrentTrain: epoch  8, batch     0 | loss: 2.9342551Losses:  2.5919885635375977 0.7457034587860107
CurrentTrain: epoch  8, batch     1 | loss: 3.3376920Losses:  2.321051597595215 0.633865237236023
CurrentTrain: epoch  8, batch     2 | loss: 2.9549170Losses:  1.7479875087738037 0.055870264768600464
CurrentTrain: epoch  8, batch     3 | loss: 1.8038578Losses:  2.34016752243042 0.7123535871505737
CurrentTrain: epoch  9, batch     0 | loss: 3.0525212Losses:  2.0204050540924072 0.5076343417167664
CurrentTrain: epoch  9, batch     1 | loss: 2.5280395Losses:  2.376506805419922 0.5777456760406494
CurrentTrain: epoch  9, batch     2 | loss: 2.9542525Losses:  1.9874404668807983 0.09886730462312698
CurrentTrain: epoch  9, batch     3 | loss: 2.0863078
Losses:  5.699704647064209 0.5869360566139221
MemoryTrain:  epoch  0, batch     0 | loss: 6.2866406Losses:  8.308578491210938 0.8849882483482361
MemoryTrain:  epoch  0, batch     1 | loss: 9.1935663Losses:  9.567256927490234 0.7070624828338623
MemoryTrain:  epoch  0, batch     2 | loss: 10.2743196Losses:  11.484573364257812 0.050753120332956314
MemoryTrain:  epoch  0, batch     3 | loss: 11.5353270Losses:  1.5223850011825562 0.7701927423477173
MemoryTrain:  epoch  1, batch     0 | loss: 2.2925777Losses:  1.2703229188919067 0.8051833510398865
MemoryTrain:  epoch  1, batch     1 | loss: 2.0755062Losses:  1.0749777555465698 0.7025033831596375
MemoryTrain:  epoch  1, batch     2 | loss: 1.7774811Losses:  0.44881659746170044 0.023141492158174515
MemoryTrain:  epoch  1, batch     3 | loss: 0.4719581Losses:  1.4777549505233765 0.6936873197555542
MemoryTrain:  epoch  2, batch     0 | loss: 2.1714423Losses:  0.7985966205596924 0.6970369219779968
MemoryTrain:  epoch  2, batch     1 | loss: 1.4956336Losses:  1.0907140970230103 0.6000016927719116
MemoryTrain:  epoch  2, batch     2 | loss: 1.6907158Losses:  0.778355598449707 0.383303701877594
MemoryTrain:  epoch  2, batch     3 | loss: 1.1616592Losses:  0.9961975812911987 0.5461227297782898
MemoryTrain:  epoch  3, batch     0 | loss: 1.5423203Losses:  0.8102321624755859 0.9257452487945557
MemoryTrain:  epoch  3, batch     1 | loss: 1.7359774Losses:  1.3282780647277832 0.7342158555984497
MemoryTrain:  epoch  3, batch     2 | loss: 2.0624938Losses:  0.8772167563438416 0.1580665558576584
MemoryTrain:  epoch  3, batch     3 | loss: 1.0352833Losses:  1.082455039024353 0.6767724752426147
MemoryTrain:  epoch  4, batch     0 | loss: 1.7592275Losses:  0.9530543684959412 0.8051785826683044
MemoryTrain:  epoch  4, batch     1 | loss: 1.7582330Losses:  0.8898971676826477 0.7261306643486023
MemoryTrain:  epoch  4, batch     2 | loss: 1.6160278Losses:  0.47680148482322693 0.028165845200419426
MemoryTrain:  epoch  4, batch     3 | loss: 0.5049673Losses:  1.0942332744598389 0.5784077048301697
MemoryTrain:  epoch  5, batch     0 | loss: 1.6726410Losses:  0.7290903925895691 0.7446268796920776
MemoryTrain:  epoch  5, batch     1 | loss: 1.4737172Losses:  0.8071742057800293 0.8345828056335449
MemoryTrain:  epoch  5, batch     2 | loss: 1.6417570Losses:  0.5012666583061218 0.017217572778463364
MemoryTrain:  epoch  5, batch     3 | loss: 0.5184842Losses:  0.8166183233261108 0.621549129486084
MemoryTrain:  epoch  6, batch     0 | loss: 1.4381675Losses:  0.689346194267273 0.7155314683914185
MemoryTrain:  epoch  6, batch     1 | loss: 1.4048777Losses:  0.8233999013900757 0.7710960507392883
MemoryTrain:  epoch  6, batch     2 | loss: 1.5944960Losses:  0.5345184803009033 0.06233980134129524
MemoryTrain:  epoch  6, batch     3 | loss: 0.5968583Losses:  0.775241494178772 0.7823265790939331
MemoryTrain:  epoch  7, batch     0 | loss: 1.5575681Losses:  0.6745290756225586 0.5821635723114014
MemoryTrain:  epoch  7, batch     1 | loss: 1.2566926Losses:  0.6860073804855347 0.7446398735046387
MemoryTrain:  epoch  7, batch     2 | loss: 1.4306473Losses:  0.47026610374450684 0.009665640071034431
MemoryTrain:  epoch  7, batch     3 | loss: 0.4799317Losses:  0.7080424427986145 0.8132656812667847
MemoryTrain:  epoch  8, batch     0 | loss: 1.5213082Losses:  0.624928891658783 0.5533676147460938
MemoryTrain:  epoch  8, batch     1 | loss: 1.1782966Losses:  0.631506085395813 0.6219645738601685
MemoryTrain:  epoch  8, batch     2 | loss: 1.2534707Losses:  0.5718938112258911 0.12392711639404297
MemoryTrain:  epoch  8, batch     3 | loss: 0.6958209Losses:  0.6329416036605835 0.721376895904541
MemoryTrain:  epoch  9, batch     0 | loss: 1.3543185Losses:  0.6144766807556152 0.7178893089294434
MemoryTrain:  epoch  9, batch     1 | loss: 1.3323660Losses:  0.6296294927597046 0.6106847524642944
MemoryTrain:  epoch  9, batch     2 | loss: 1.2403142Losses:  0.43208202719688416 0.007258724421262741
MemoryTrain:  epoch  9, batch     3 | loss: 0.4393407
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 64.51%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 61.43%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 60.24%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 58.39%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 58.49%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 58.91%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 59.60%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 60.97%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 61.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.15%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.69%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.03%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 88.70%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 88.47%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.02%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 87.81%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 87.70%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.70%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 87.88%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.22%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 88.56%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.45%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 88.44%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.34%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 88.32%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 88.07%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 87.74%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.34%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 87.27%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.12%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 86.82%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.68%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 86.19%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 85.70%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 85.51%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 85.18%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 84.79%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 84.41%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 84.04%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 83.40%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 82.85%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 82.43%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 82.03%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 81.83%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 81.38%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 81.19%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 80.69%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 80.32%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 80.02%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 79.73%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 79.51%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 79.29%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 79.13%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 78.62%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.01%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.47%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.88%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 76.41%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 76.00%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.77%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.27%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.83%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 77.71%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 77.47%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 77.40%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.06%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.35%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.92%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 77.59%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 77.22%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.98%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.43%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 76.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.82%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 76.32%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.82%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 75.45%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 74.96%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.48%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 75.19%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 75.15%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.15%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 75.19%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.37%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 75.44%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 75.48%   [EVAL] batch:  171 | acc: 81.25%,  total acc: 75.51%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 75.47%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 75.50%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.61%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 75.46%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.39%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.35%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 75.64%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.80%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.09%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 76.32%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 76.31%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 76.33%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 76.26%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 76.16%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 76.06%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 75.96%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 75.86%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 75.61%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 75.49%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 75.15%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 74.88%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 74.58%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 74.32%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.08%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 73.79%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 73.74%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 74.67%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 74.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.47%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 76.62%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 76.46%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 76.31%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 76.13%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 75.93%   [EVAL] batch:  255 | acc: 50.00%,  total acc: 75.83%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 75.73%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 75.70%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 75.70%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 75.65%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 75.53%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 75.45%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 75.75%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 75.63%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 75.56%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 75.52%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 75.42%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 75.36%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 75.07%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 74.91%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 74.72%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 74.48%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 74.37%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 74.33%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 74.33%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 74.42%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 74.40%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 74.30%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 74.28%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 74.21%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.25%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.42%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 75.06%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.06%   
cur_acc:  ['0.9435', '0.7183', '0.7153', '0.7768', '0.6815']
his_acc:  ['0.9435', '0.8280', '0.7829', '0.7685', '0.7506']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  7.066123962402344 1.4440151453018188
CurrentTrain: epoch  0, batch     0 | loss: 8.5101395Losses:  8.822000503540039 1.1350294351577759
CurrentTrain: epoch  0, batch     1 | loss: 9.9570303Losses:  7.849801063537598 1.310399055480957
CurrentTrain: epoch  0, batch     2 | loss: 9.1602001Losses:  5.542386054992676 5.960464477539063e-08
CurrentTrain: epoch  0, batch     3 | loss: 5.5423861Losses:  4.73755407333374 1.4787111282348633
CurrentTrain: epoch  1, batch     0 | loss: 6.2162652Losses:  3.875383138656616 1.1854023933410645
CurrentTrain: epoch  1, batch     1 | loss: 5.0607853Losses:  3.285383939743042 1.1660617589950562
CurrentTrain: epoch  1, batch     2 | loss: 4.4514456Losses:  4.1737165451049805 0.30980026721954346
CurrentTrain: epoch  1, batch     3 | loss: 4.4835167Losses:  3.8306124210357666 1.1268621683120728
CurrentTrain: epoch  2, batch     0 | loss: 4.9574747Losses:  4.125523567199707 1.1526942253112793
CurrentTrain: epoch  2, batch     1 | loss: 5.2782178Losses:  3.359182834625244 1.1615493297576904
CurrentTrain: epoch  2, batch     2 | loss: 4.5207319Losses:  4.46112060546875 0.45353543758392334
CurrentTrain: epoch  2, batch     3 | loss: 4.9146562Losses:  3.0706002712249756 1.0241436958312988
CurrentTrain: epoch  3, batch     0 | loss: 4.0947437Losses:  3.7631003856658936 1.2152190208435059
CurrentTrain: epoch  3, batch     1 | loss: 4.9783192Losses:  3.7844619750976562 1.2507617473602295
CurrentTrain: epoch  3, batch     2 | loss: 5.0352240Losses:  4.510276794433594 0.1694711148738861
CurrentTrain: epoch  3, batch     3 | loss: 4.6797481Losses:  2.946207284927368 0.8487484455108643
CurrentTrain: epoch  4, batch     0 | loss: 3.7949557Losses:  3.864236354827881 0.9769066572189331
CurrentTrain: epoch  4, batch     1 | loss: 4.8411431Losses:  3.496192455291748 1.0525691509246826
CurrentTrain: epoch  4, batch     2 | loss: 4.5487614Losses:  2.176421642303467 0.2633588910102844
CurrentTrain: epoch  4, batch     3 | loss: 2.4397805Losses:  3.089291572570801 0.989133358001709
CurrentTrain: epoch  5, batch     0 | loss: 4.0784249Losses:  3.429969072341919 0.7970191240310669
CurrentTrain: epoch  5, batch     1 | loss: 4.2269883Losses:  2.916111946105957 0.9997528791427612
CurrentTrain: epoch  5, batch     2 | loss: 3.9158649Losses:  2.7801096439361572 0.13476064801216125
CurrentTrain: epoch  5, batch     3 | loss: 2.9148703Losses:  2.485971212387085 0.840133786201477
CurrentTrain: epoch  6, batch     0 | loss: 3.3261051Losses:  3.1494369506835938 1.0101354122161865
CurrentTrain: epoch  6, batch     1 | loss: 4.1595726Losses:  3.069169759750366 1.021500587463379
CurrentTrain: epoch  6, batch     2 | loss: 4.0906706Losses:  4.135260581970215 0.023855024948716164
CurrentTrain: epoch  6, batch     3 | loss: 4.1591158Losses:  2.8657302856445312 0.893891453742981
CurrentTrain: epoch  7, batch     0 | loss: 3.7596216Losses:  2.96633243560791 0.8739256858825684
CurrentTrain: epoch  7, batch     1 | loss: 3.8402581Losses:  2.454878330230713 0.7447870969772339
CurrentTrain: epoch  7, batch     2 | loss: 3.1996655Losses:  3.4947636127471924 0.1574593186378479
CurrentTrain: epoch  7, batch     3 | loss: 3.6522229Losses:  2.4611220359802246 0.5990457534790039
CurrentTrain: epoch  8, batch     0 | loss: 3.0601678Losses:  2.247786521911621 0.6501389741897583
CurrentTrain: epoch  8, batch     1 | loss: 2.8979254Losses:  3.1973154544830322 0.9277061223983765
CurrentTrain: epoch  8, batch     2 | loss: 4.1250215Losses:  1.7642552852630615 0.031070010736584663
CurrentTrain: epoch  8, batch     3 | loss: 1.7953253Losses:  2.247149705886841 0.6593635082244873
CurrentTrain: epoch  9, batch     0 | loss: 2.9065132Losses:  2.212967872619629 0.6458213329315186
CurrentTrain: epoch  9, batch     1 | loss: 2.8587892Losses:  2.7456932067871094 0.7800270318984985
CurrentTrain: epoch  9, batch     2 | loss: 3.5257201Losses:  4.532366752624512 0.3710215091705322
CurrentTrain: epoch  9, batch     3 | loss: 4.9033880
Losses:  5.849289894104004 0.7946987152099609
MemoryTrain:  epoch  0, batch     0 | loss: 6.6439886Losses:  8.267522811889648 0.9219175577163696
MemoryTrain:  epoch  0, batch     1 | loss: 9.1894407Losses:  8.44894027709961 0.7651578187942505
MemoryTrain:  epoch  0, batch     2 | loss: 9.2140980Losses:  9.852374076843262 0.6156190633773804
MemoryTrain:  epoch  0, batch     3 | loss: 10.4679928Losses:  1.481533408164978 0.7095342874526978
MemoryTrain:  epoch  1, batch     0 | loss: 2.1910677Losses:  1.2088638544082642 0.7857062220573425
MemoryTrain:  epoch  1, batch     1 | loss: 1.9945700Losses:  1.4745917320251465 0.795360267162323
MemoryTrain:  epoch  1, batch     2 | loss: 2.2699521Losses:  1.128800630569458 0.6442960500717163
MemoryTrain:  epoch  1, batch     3 | loss: 1.7730967Losses:  1.2701761722564697 0.8182373642921448
MemoryTrain:  epoch  2, batch     0 | loss: 2.0884135Losses:  1.1213573217391968 0.7328787446022034
MemoryTrain:  epoch  2, batch     1 | loss: 1.8542361Losses:  0.9466217756271362 0.6257662773132324
MemoryTrain:  epoch  2, batch     2 | loss: 1.5723881Losses:  1.2849899530410767 0.5325042009353638
MemoryTrain:  epoch  2, batch     3 | loss: 1.8174942Losses:  0.9091404676437378 0.6280216574668884
MemoryTrain:  epoch  3, batch     0 | loss: 1.5371621Losses:  0.8460795283317566 0.6664876341819763
MemoryTrain:  epoch  3, batch     1 | loss: 1.5125672Losses:  1.0455753803253174 0.5959277749061584
MemoryTrain:  epoch  3, batch     2 | loss: 1.6415031Losses:  1.334343433380127 0.7463278770446777
MemoryTrain:  epoch  3, batch     3 | loss: 2.0806713Losses:  1.2479053735733032 0.9425524473190308
MemoryTrain:  epoch  4, batch     0 | loss: 2.1904578Losses:  0.7874188423156738 0.5488948822021484
MemoryTrain:  epoch  4, batch     1 | loss: 1.3363137Losses:  0.765925943851471 0.603352963924408
MemoryTrain:  epoch  4, batch     2 | loss: 1.3692789Losses:  0.9071235656738281 0.5220547318458557
MemoryTrain:  epoch  4, batch     3 | loss: 1.4291782Losses:  0.8275972604751587 0.5546735525131226
MemoryTrain:  epoch  5, batch     0 | loss: 1.3822708Losses:  0.8057397603988647 0.8531701564788818
MemoryTrain:  epoch  5, batch     1 | loss: 1.6589099Losses:  0.9134042859077454 0.6926311254501343
MemoryTrain:  epoch  5, batch     2 | loss: 1.6060355Losses:  0.7924370765686035 0.6029565930366516
MemoryTrain:  epoch  5, batch     3 | loss: 1.3953936Losses:  0.7554738521575928 0.6468465328216553
MemoryTrain:  epoch  6, batch     0 | loss: 1.4023204Losses:  0.7190961837768555 0.6697711944580078
MemoryTrain:  epoch  6, batch     1 | loss: 1.3888674Losses:  0.842904806137085 0.7669132351875305
MemoryTrain:  epoch  6, batch     2 | loss: 1.6098180Losses:  0.7936210036277771 0.5366588234901428
MemoryTrain:  epoch  6, batch     3 | loss: 1.3302798Losses:  0.8283103108406067 0.6848360896110535
MemoryTrain:  epoch  7, batch     0 | loss: 1.5131464Losses:  0.7781760692596436 0.7632368803024292
MemoryTrain:  epoch  7, batch     1 | loss: 1.5414129Losses:  0.6739463806152344 0.6256304979324341
MemoryTrain:  epoch  7, batch     2 | loss: 1.2995769Losses:  0.7788099050521851 0.513683021068573
MemoryTrain:  epoch  7, batch     3 | loss: 1.2924929Losses:  0.7457528710365295 0.6623304486274719
MemoryTrain:  epoch  8, batch     0 | loss: 1.4080833Losses:  0.6753140091896057 0.6069028973579407
MemoryTrain:  epoch  8, batch     1 | loss: 1.2822169Losses:  0.7264803647994995 0.7267826795578003
MemoryTrain:  epoch  8, batch     2 | loss: 1.4532630Losses:  0.7869361639022827 0.5748751163482666
MemoryTrain:  epoch  8, batch     3 | loss: 1.3618113Losses:  0.672290027141571 0.6012805104255676
MemoryTrain:  epoch  9, batch     0 | loss: 1.2735705Losses:  0.7757168412208557 0.8298273086547852
MemoryTrain:  epoch  9, batch     1 | loss: 1.6055441Losses:  0.6430638432502747 0.6544162034988403
MemoryTrain:  epoch  9, batch     2 | loss: 1.2974801Losses:  0.6780790686607361 0.4745416045188904
MemoryTrain:  epoch  9, batch     3 | loss: 1.1526207
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 51.14%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 56.73%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 72.81%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 69.33%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 69.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 71.41%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 70.76%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 70.18%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 70.37%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 70.34%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 70.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 70.24%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.58%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.52%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 88.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 88.93%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 87.82%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 86.97%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 86.56%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 85.86%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 85.02%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 84.86%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 84.75%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 84.61%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 84.47%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 84.24%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 84.64%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.67%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.71%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 84.54%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 84.09%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 84.05%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 83.94%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.91%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 83.54%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 83.28%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.57%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 82.49%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 81.82%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 81.68%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 81.32%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 80.97%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 80.56%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 80.23%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 79.64%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 79.12%   [EVAL] batch:   94 | acc: 43.75%,  total acc: 78.75%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 78.39%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.22%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 77.81%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 77.65%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 77.25%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 76.98%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.84%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 76.58%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 76.37%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 76.30%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 75.99%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.41%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 74.83%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.26%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 73.82%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 74.33%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 74.09%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 73.91%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 73.77%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 73.68%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 73.55%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.63%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.79%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 73.74%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 73.01%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 72.84%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 72.60%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.35%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.93%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 72.45%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.98%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 71.17%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.71%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 71.65%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 71.65%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 71.86%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 71.73%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 71.55%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 71.39%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 71.34%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 71.21%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.22%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.63%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 72.64%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 72.56%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 72.09%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 72.01%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 71.94%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 71.71%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 71.45%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 71.17%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 70.89%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 70.62%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 70.34%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 71.36%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 71.36%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.82%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 73.01%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 73.33%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 73.16%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 72.69%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:  255 | acc: 25.00%,  total acc: 72.31%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 72.20%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 72.18%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 72.12%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 72.01%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 72.00%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 71.95%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 71.92%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 72.31%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 72.26%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 71.92%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 71.84%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 71.61%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 71.43%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 71.22%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 70.97%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 70.88%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 70.85%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 70.84%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 70.95%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 70.92%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 70.81%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 70.73%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 70.60%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 70.46%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 70.40%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 71.41%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 71.22%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 71.03%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 70.85%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 70.70%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 70.42%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 70.92%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.94%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 70.77%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 70.73%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 71.29%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 71.31%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 71.51%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 71.72%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 71.79%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 71.87%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 71.83%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 71.58%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 71.47%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 71.30%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 71.19%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 71.60%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 71.50%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.43%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 71.33%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 71.33%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 71.31%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 71.38%   
cur_acc:  ['0.9435', '0.7183', '0.7153', '0.7768', '0.6815', '0.7024']
his_acc:  ['0.9435', '0.8280', '0.7829', '0.7685', '0.7506', '0.7138']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  6.8542938232421875 1.3493568897247314
CurrentTrain: epoch  0, batch     0 | loss: 8.2036505Losses:  8.845016479492188 1.493740200996399
CurrentTrain: epoch  0, batch     1 | loss: 10.3387566Losses:  7.186313629150391 1.5791288614273071
CurrentTrain: epoch  0, batch     2 | loss: 8.7654428Losses:  7.683837413787842 0.41586750745773315
CurrentTrain: epoch  0, batch     3 | loss: 8.0997047Losses:  4.26352596282959 1.643662929534912
CurrentTrain: epoch  1, batch     0 | loss: 5.9071889Losses:  4.111698627471924 1.3618907928466797
CurrentTrain: epoch  1, batch     1 | loss: 5.4735894Losses:  3.6972904205322266 1.5058388710021973
CurrentTrain: epoch  1, batch     2 | loss: 5.2031293Losses:  4.384434223175049 0.41619616746902466
CurrentTrain: epoch  1, batch     3 | loss: 4.8006306Losses:  3.5965607166290283 1.3133630752563477
CurrentTrain: epoch  2, batch     0 | loss: 4.9099236Losses:  3.5007591247558594 1.3434685468673706
CurrentTrain: epoch  2, batch     1 | loss: 4.8442278Losses:  3.6224944591522217 1.560065507888794
CurrentTrain: epoch  2, batch     2 | loss: 5.1825600Losses:  4.894616603851318 0.4901469349861145
CurrentTrain: epoch  2, batch     3 | loss: 5.3847637Losses:  3.5510075092315674 1.261557936668396
CurrentTrain: epoch  3, batch     0 | loss: 4.8125653Losses:  3.3952102661132812 1.3579580783843994
CurrentTrain: epoch  3, batch     1 | loss: 4.7531681Losses:  2.992011070251465 1.232579231262207
CurrentTrain: epoch  3, batch     2 | loss: 4.2245903Losses:  2.8055546283721924 0.13819652795791626
CurrentTrain: epoch  3, batch     3 | loss: 2.9437511Losses:  3.3435983657836914 1.3392164707183838
CurrentTrain: epoch  4, batch     0 | loss: 4.6828146Losses:  2.699188709259033 0.9853099584579468
CurrentTrain: epoch  4, batch     1 | loss: 3.6844988Losses:  3.0554285049438477 1.2893319129943848
CurrentTrain: epoch  4, batch     2 | loss: 4.3447604Losses:  4.31707763671875 0.3263019621372223
CurrentTrain: epoch  4, batch     3 | loss: 4.6433797Losses:  3.0693163871765137 1.05441153049469
CurrentTrain: epoch  5, batch     0 | loss: 4.1237278Losses:  2.873044967651367 1.1302094459533691
CurrentTrain: epoch  5, batch     1 | loss: 4.0032544Losses:  2.8302345275878906 1.2060490846633911
CurrentTrain: epoch  5, batch     2 | loss: 4.0362835Losses:  2.389909267425537 0.17007926106452942
CurrentTrain: epoch  5, batch     3 | loss: 2.5599885Losses:  2.9664664268493652 1.115408182144165
CurrentTrain: epoch  6, batch     0 | loss: 4.0818748Losses:  2.9551873207092285 0.9481080770492554
CurrentTrain: epoch  6, batch     1 | loss: 3.9032955Losses:  2.3200907707214355 0.80967116355896
CurrentTrain: epoch  6, batch     2 | loss: 3.1297619Losses:  3.7471814155578613 0.330162912607193
CurrentTrain: epoch  6, batch     3 | loss: 4.0773444Losses:  2.3201770782470703 0.9169267416000366
CurrentTrain: epoch  7, batch     0 | loss: 3.2371039Losses:  2.8444061279296875 0.9446762204170227
CurrentTrain: epoch  7, batch     1 | loss: 3.7890823Losses:  2.631070613861084 1.1793243885040283
CurrentTrain: epoch  7, batch     2 | loss: 3.8103950Losses:  3.465970039367676 0.8204239010810852
CurrentTrain: epoch  7, batch     3 | loss: 4.2863941Losses:  2.5434179306030273 1.044497013092041
CurrentTrain: epoch  8, batch     0 | loss: 3.5879149Losses:  2.515061855316162 0.7611413598060608
CurrentTrain: epoch  8, batch     1 | loss: 3.2762032Losses:  2.5746548175811768 1.0966179370880127
CurrentTrain: epoch  8, batch     2 | loss: 3.6712728Losses:  2.083962917327881 0.13030312955379486
CurrentTrain: epoch  8, batch     3 | loss: 2.2142661Losses:  2.26543927192688 1.0325847864151
CurrentTrain: epoch  9, batch     0 | loss: 3.2980242Losses:  2.592609405517578 0.9387483596801758
CurrentTrain: epoch  9, batch     1 | loss: 3.5313578Losses:  2.4026365280151367 0.88053297996521
CurrentTrain: epoch  9, batch     2 | loss: 3.2831695Losses:  2.01102876663208 0.4256457984447479
CurrentTrain: epoch  9, batch     3 | loss: 2.4366746
Losses:  6.003934383392334 0.7376101016998291
MemoryTrain:  epoch  0, batch     0 | loss: 6.7415447Losses:  8.415149688720703 0.9129159450531006
MemoryTrain:  epoch  0, batch     1 | loss: 9.3280659Losses:  8.989517211914062 0.8992112874984741
MemoryTrain:  epoch  0, batch     2 | loss: 9.8887281Losses:  9.508402824401855 0.7728990316390991
MemoryTrain:  epoch  0, batch     3 | loss: 10.2813015Losses:  9.289741516113281 0.2024666666984558
MemoryTrain:  epoch  0, batch     4 | loss: 9.4922085Losses:  1.0345001220703125 0.5357000827789307
MemoryTrain:  epoch  1, batch     0 | loss: 1.5702002Losses:  1.5583710670471191 0.6713460683822632
MemoryTrain:  epoch  1, batch     1 | loss: 2.2297173Losses:  1.113262414932251 0.7478436231613159
MemoryTrain:  epoch  1, batch     2 | loss: 1.8611060Losses:  1.6420800685882568 0.8278457522392273
MemoryTrain:  epoch  1, batch     3 | loss: 2.4699259Losses:  1.2181587219238281 0.5745108127593994
MemoryTrain:  epoch  1, batch     4 | loss: 1.7926695Losses:  1.3053152561187744 0.687671959400177
MemoryTrain:  epoch  2, batch     0 | loss: 1.9929872Losses:  0.9131385087966919 0.692203164100647
MemoryTrain:  epoch  2, batch     1 | loss: 1.6053417Losses:  1.1639251708984375 0.8126851916313171
MemoryTrain:  epoch  2, batch     2 | loss: 1.9766104Losses:  1.301203727722168 0.8633536100387573
MemoryTrain:  epoch  2, batch     3 | loss: 2.1645575Losses:  1.099961280822754 0.4295045733451843
MemoryTrain:  epoch  2, batch     4 | loss: 1.5294659Losses:  0.8973302841186523 0.8045214414596558
MemoryTrain:  epoch  3, batch     0 | loss: 1.7018517Losses:  0.9906689524650574 0.7346493601799011
MemoryTrain:  epoch  3, batch     1 | loss: 1.7253183Losses:  1.0706791877746582 0.6417972445487976
MemoryTrain:  epoch  3, batch     2 | loss: 1.7124765Losses:  1.1856507062911987 0.8100476264953613
MemoryTrain:  epoch  3, batch     3 | loss: 1.9956983Losses:  1.071042776107788 0.32787659764289856
MemoryTrain:  epoch  3, batch     4 | loss: 1.3989193Losses:  0.8595905900001526 0.6007664203643799
MemoryTrain:  epoch  4, batch     0 | loss: 1.4603570Losses:  1.1349506378173828 0.6646796464920044
MemoryTrain:  epoch  4, batch     1 | loss: 1.7996303Losses:  0.6982635259628296 0.6516109704971313
MemoryTrain:  epoch  4, batch     2 | loss: 1.3498745Losses:  1.1992721557617188 0.9223768711090088
MemoryTrain:  epoch  4, batch     3 | loss: 2.1216490Losses:  0.9502028822898865 0.47698864340782166
MemoryTrain:  epoch  4, batch     4 | loss: 1.4271915Losses:  0.8430191278457642 0.6692057847976685
MemoryTrain:  epoch  5, batch     0 | loss: 1.5122249Losses:  0.9546555280685425 0.7942004203796387
MemoryTrain:  epoch  5, batch     1 | loss: 1.7488559Losses:  0.8485566973686218 0.8166524171829224
MemoryTrain:  epoch  5, batch     2 | loss: 1.6652091Losses:  1.0445680618286133 0.749290943145752
MemoryTrain:  epoch  5, batch     3 | loss: 1.7938590Losses:  0.6595022082328796 0.20419394969940186
MemoryTrain:  epoch  5, batch     4 | loss: 0.8636962Losses:  0.9682902097702026 0.7034845352172852
MemoryTrain:  epoch  6, batch     0 | loss: 1.6717747Losses:  0.7688077688217163 0.6049250364303589
MemoryTrain:  epoch  6, batch     1 | loss: 1.3737328Losses:  0.9742693901062012 0.813126266002655
MemoryTrain:  epoch  6, batch     2 | loss: 1.7873957Losses:  0.7952306270599365 0.6929195523262024
MemoryTrain:  epoch  6, batch     3 | loss: 1.4881501Losses:  0.8880099058151245 0.31535500288009644
MemoryTrain:  epoch  6, batch     4 | loss: 1.2033648Losses:  0.8484222888946533 0.65427565574646
MemoryTrain:  epoch  7, batch     0 | loss: 1.5026979Losses:  1.0103272199630737 0.8137156367301941
MemoryTrain:  epoch  7, batch     1 | loss: 1.8240428Losses:  0.7997556924819946 0.6982606649398804
MemoryTrain:  epoch  7, batch     2 | loss: 1.4980164Losses:  0.809808611869812 0.6126872897148132
MemoryTrain:  epoch  7, batch     3 | loss: 1.4224958Losses:  0.7030078768730164 0.1796126365661621
MemoryTrain:  epoch  7, batch     4 | loss: 0.8826205Losses:  0.6811944842338562 0.5756852626800537
MemoryTrain:  epoch  8, batch     0 | loss: 1.2568798Losses:  0.9259965419769287 0.8642368316650391
MemoryTrain:  epoch  8, batch     1 | loss: 1.7902334Losses:  0.8519070148468018 0.7884390950202942
MemoryTrain:  epoch  8, batch     2 | loss: 1.6403461Losses:  0.7906348705291748 0.5674294233322144
MemoryTrain:  epoch  8, batch     3 | loss: 1.3580643Losses:  0.8171566724777222 0.25945883989334106
MemoryTrain:  epoch  8, batch     4 | loss: 1.0766156Losses:  0.8641775846481323 0.7799854278564453
MemoryTrain:  epoch  9, batch     0 | loss: 1.6441630Losses:  0.7058870792388916 0.6401872634887695
MemoryTrain:  epoch  9, batch     1 | loss: 1.3460743Losses:  0.8114767074584961 0.7163553237915039
MemoryTrain:  epoch  9, batch     2 | loss: 1.5278320Losses:  0.7726677060127258 0.6491066813468933
MemoryTrain:  epoch  9, batch     3 | loss: 1.4217744Losses:  0.7574481964111328 0.24567389488220215
MemoryTrain:  epoch  9, batch     4 | loss: 1.0031221
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 44.89%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 60.76%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 54.55%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 53.26%   [EVAL] batch:   23 | acc: 12.50%,  total acc: 51.56%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 48.32%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 46.53%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 44.87%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 43.53%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 42.29%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 40.93%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.60%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 42.99%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 43.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 45.18%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 46.18%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 47.30%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 48.52%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 49.36%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.62%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 51.68%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 52.53%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 53.49%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 54.40%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 55.14%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 55.57%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 55.98%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 58.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 57.97%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 58.49%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 59.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.60%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 59.32%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 59.16%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 59.00%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 58.81%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 58.87%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.33%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.30%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 87.63%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.63%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.99%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 86.94%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 86.07%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 85.02%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 84.22%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 83.75%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 82.52%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 82.79%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 82.48%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 82.37%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 82.26%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 82.07%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 82.47%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 82.53%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 82.60%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.58%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 82.32%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 81.90%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.81%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.72%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 81.72%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 81.64%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 81.40%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 81.17%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 80.59%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 80.45%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 79.81%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 79.35%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 79.03%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 78.78%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 78.46%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 77.89%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 77.39%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 77.11%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 76.69%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 76.48%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 76.08%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 75.95%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 75.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 74.88%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 74.42%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 73.78%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 73.28%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 72.67%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 72.24%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 71.68%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 73.12%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 73.00%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 73.07%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 73.00%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.74%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 72.61%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.82%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.93%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 73.05%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.62%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 72.28%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.85%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 71.48%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 71.24%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.01%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.65%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 71.18%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 70.71%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 70.37%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 69.92%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.47%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.07%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.25%   [EVAL] batch:  163 | acc: 6.25%,  total acc: 69.86%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 69.66%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 68.79%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 68.53%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 68.31%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 68.17%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 68.10%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 68.00%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 67.90%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 67.84%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 67.92%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.96%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.62%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 69.52%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 69.42%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 69.47%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 69.22%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 68.78%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  206 | acc: 18.75%,  total acc: 68.27%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 67.97%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 67.64%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 67.35%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  211 | acc: 0.00%,  total acc: 66.75%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  219 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  224 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 69.70%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 69.86%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.48%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 70.34%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 70.16%   [EVAL] batch:  252 | acc: 12.50%,  total acc: 69.94%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 69.71%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 69.34%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.23%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 69.13%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 69.04%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 69.06%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 69.01%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.28%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 69.63%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 69.49%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 69.45%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 69.22%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 69.08%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 68.86%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 68.68%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.49%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 68.25%   [EVAL] batch:  287 | acc: 6.25%,  total acc: 68.03%   [EVAL] batch:  288 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 67.56%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 67.33%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:  292 | acc: 12.50%,  total acc: 66.92%   [EVAL] batch:  293 | acc: 12.50%,  total acc: 66.73%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 66.63%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 66.46%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 66.40%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 66.30%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 67.34%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 67.20%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 67.05%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 66.90%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 66.76%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 67.16%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 67.10%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 67.06%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.84%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 68.25%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 68.03%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 67.87%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 67.75%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.95%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  361 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 68.22%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 68.17%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 68.08%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 68.00%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.90%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 67.88%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 67.88%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 67.97%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 67.82%   [EVAL] batch:  376 | acc: 6.25%,  total acc: 67.66%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 67.49%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 67.38%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 67.05%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 67.33%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 67.38%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 67.22%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 67.12%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 66.98%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 66.84%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 66.69%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 66.53%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.90%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 66.11%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 66.79%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 66.77%   [EVAL] batch:  426 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 66.79%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 66.83%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 66.75%   [EVAL] batch:  434 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 66.58%   
cur_acc:  ['0.9435', '0.7183', '0.7153', '0.7768', '0.6815', '0.7024', '0.5833']
his_acc:  ['0.9435', '0.8280', '0.7829', '0.7685', '0.7506', '0.7138', '0.6658']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  7.107330322265625 1.5242812633514404
CurrentTrain: epoch  0, batch     0 | loss: 8.6316118Losses:  8.723167419433594 1.4137688875198364
CurrentTrain: epoch  0, batch     1 | loss: 10.1369362Losses:  7.045276641845703 1.2562116384506226
CurrentTrain: epoch  0, batch     2 | loss: 8.3014879Losses:  4.374240875244141 0.2432679533958435
CurrentTrain: epoch  0, batch     3 | loss: 4.6175089Losses:  3.999575614929199 1.0241875648498535
CurrentTrain: epoch  1, batch     0 | loss: 5.0237632Losses:  4.307931423187256 1.330415964126587
CurrentTrain: epoch  1, batch     1 | loss: 5.6383476Losses:  4.181463718414307 1.0945792198181152
CurrentTrain: epoch  1, batch     2 | loss: 5.2760429Losses:  2.1690046787261963 5.960464477539063e-08
CurrentTrain: epoch  1, batch     3 | loss: 2.1690047Losses:  3.671943426132202 1.1373505592346191
CurrentTrain: epoch  2, batch     0 | loss: 4.8092937Losses:  3.8499813079833984 1.0250333547592163
CurrentTrain: epoch  2, batch     1 | loss: 4.8750148Losses:  3.794705390930176 0.9243553876876831
CurrentTrain: epoch  2, batch     2 | loss: 4.7190609Losses:  2.718238353729248 0.21026180684566498
CurrentTrain: epoch  2, batch     3 | loss: 2.9285002Losses:  3.551896572113037 1.125277042388916
CurrentTrain: epoch  3, batch     0 | loss: 4.6771736Losses:  3.7053909301757812 0.9334338903427124
CurrentTrain: epoch  3, batch     1 | loss: 4.6388249Losses:  3.156149387359619 0.9951018691062927
CurrentTrain: epoch  3, batch     2 | loss: 4.1512513Losses:  4.383963108062744 0.9777628183364868
CurrentTrain: epoch  3, batch     3 | loss: 5.3617258Losses:  3.155851125717163 0.8353056907653809
CurrentTrain: epoch  4, batch     0 | loss: 3.9911568Losses:  2.8684751987457275 0.9349921941757202
CurrentTrain: epoch  4, batch     1 | loss: 3.8034673Losses:  3.8268749713897705 1.1445738077163696
CurrentTrain: epoch  4, batch     2 | loss: 4.9714489Losses:  5.079139709472656 0.17887581884860992
CurrentTrain: epoch  4, batch     3 | loss: 5.2580156Losses:  3.200920581817627 1.0401134490966797
CurrentTrain: epoch  5, batch     0 | loss: 4.2410340Losses:  3.667025089263916 0.7613714933395386
CurrentTrain: epoch  5, batch     1 | loss: 4.4283967Losses:  2.7087230682373047 0.7637835741043091
CurrentTrain: epoch  5, batch     2 | loss: 3.4725065Losses:  1.9558522701263428 0.12183672189712524
CurrentTrain: epoch  5, batch     3 | loss: 2.0776889Losses:  3.2544121742248535 1.0250167846679688
CurrentTrain: epoch  6, batch     0 | loss: 4.2794290Losses:  3.2549304962158203 0.8779149055480957
CurrentTrain: epoch  6, batch     1 | loss: 4.1328454Losses:  2.44757342338562 0.537340521812439
CurrentTrain: epoch  6, batch     2 | loss: 2.9849138Losses:  2.208357095718384 0.27991414070129395
CurrentTrain: epoch  6, batch     3 | loss: 2.4882712Losses:  3.001086711883545 0.9325371980667114
CurrentTrain: epoch  7, batch     0 | loss: 3.9336238Losses:  2.919796943664551 0.6232627630233765
CurrentTrain: epoch  7, batch     1 | loss: 3.5430598Losses:  2.5731825828552246 0.8711194396018982
CurrentTrain: epoch  7, batch     2 | loss: 3.4443021Losses:  2.1362857818603516 0.09270942211151123
CurrentTrain: epoch  7, batch     3 | loss: 2.2289953Losses:  3.1932902336120605 0.8640260696411133
CurrentTrain: epoch  8, batch     0 | loss: 4.0573163Losses:  2.5508360862731934 0.6320370435714722
CurrentTrain: epoch  8, batch     1 | loss: 3.1828732Losses:  2.3337643146514893 0.6695667505264282
CurrentTrain: epoch  8, batch     2 | loss: 3.0033312Losses:  2.833488941192627 0.07724866271018982
CurrentTrain: epoch  8, batch     3 | loss: 2.9107375Losses:  2.1322009563446045 0.5261001586914062
CurrentTrain: epoch  9, batch     0 | loss: 2.6583011Losses:  2.7581474781036377 0.8578939437866211
CurrentTrain: epoch  9, batch     1 | loss: 3.6160414Losses:  2.7021872997283936 0.7322927117347717
CurrentTrain: epoch  9, batch     2 | loss: 3.4344800Losses:  2.7912039756774902 0.13795608282089233
CurrentTrain: epoch  9, batch     3 | loss: 2.9291601
Losses:  6.01439094543457 0.6775931119918823
MemoryTrain:  epoch  0, batch     0 | loss: 6.6919842Losses:  8.158207893371582 0.767497181892395
MemoryTrain:  epoch  0, batch     1 | loss: 8.9257050Losses:  8.72398567199707 1.050293207168579
MemoryTrain:  epoch  0, batch     2 | loss: 9.7742786Losses:  9.437175750732422 0.7145246267318726
MemoryTrain:  epoch  0, batch     3 | loss: 10.1517000Losses:  9.848152160644531 0.6316354274749756
MemoryTrain:  epoch  0, batch     4 | loss: 10.4797878Losses:  1.6627283096313477 0.6834776401519775
MemoryTrain:  epoch  1, batch     0 | loss: 2.3462059Losses:  1.0457110404968262 0.6185953617095947
MemoryTrain:  epoch  1, batch     1 | loss: 1.6643064Losses:  1.146241545677185 0.7223542332649231
MemoryTrain:  epoch  1, batch     2 | loss: 1.8685958Losses:  1.7231385707855225 0.8571593165397644
MemoryTrain:  epoch  1, batch     3 | loss: 2.5802979Losses:  1.2469534873962402 0.6495611071586609
MemoryTrain:  epoch  1, batch     4 | loss: 1.8965147Losses:  1.159392237663269 0.6770437359809875
MemoryTrain:  epoch  2, batch     0 | loss: 1.8364360Losses:  1.4830337762832642 0.8612759113311768
MemoryTrain:  epoch  2, batch     1 | loss: 2.3443098Losses:  1.1208091974258423 0.718636691570282
MemoryTrain:  epoch  2, batch     2 | loss: 1.8394458Losses:  1.1730146408081055 0.7437880635261536
MemoryTrain:  epoch  2, batch     3 | loss: 1.9168026Losses:  0.9928395748138428 0.5239963531494141
MemoryTrain:  epoch  2, batch     4 | loss: 1.5168359Losses:  1.1509807109832764 0.8127669095993042
MemoryTrain:  epoch  3, batch     0 | loss: 1.9637476Losses:  1.1266365051269531 0.7723362445831299
MemoryTrain:  epoch  3, batch     1 | loss: 1.8989727Losses:  0.9688447713851929 0.5358387231826782
MemoryTrain:  epoch  3, batch     2 | loss: 1.5046835Losses:  1.1215193271636963 0.6569594740867615
MemoryTrain:  epoch  3, batch     3 | loss: 1.7784789Losses:  1.024817705154419 0.5829386711120605
MemoryTrain:  epoch  3, batch     4 | loss: 1.6077564Losses:  1.057998538017273 0.6700594425201416
MemoryTrain:  epoch  4, batch     0 | loss: 1.7280580Losses:  0.9648584127426147 0.6236016750335693
MemoryTrain:  epoch  4, batch     1 | loss: 1.5884601Losses:  0.9412745833396912 0.6034986972808838
MemoryTrain:  epoch  4, batch     2 | loss: 1.5447733Losses:  1.0144360065460205 0.7376600503921509
MemoryTrain:  epoch  4, batch     3 | loss: 1.7520961Losses:  0.9832839369773865 0.7610009908676147
MemoryTrain:  epoch  4, batch     4 | loss: 1.7442849Losses:  0.9086449146270752 0.45493003726005554
MemoryTrain:  epoch  5, batch     0 | loss: 1.3635750Losses:  0.8398122787475586 0.7692515850067139
MemoryTrain:  epoch  5, batch     1 | loss: 1.6090639Losses:  0.9483667612075806 0.700640082359314
MemoryTrain:  epoch  5, batch     2 | loss: 1.6490068Losses:  0.9939276576042175 0.6923916339874268
MemoryTrain:  epoch  5, batch     3 | loss: 1.6863194Losses:  0.8890247344970703 0.677016019821167
MemoryTrain:  epoch  5, batch     4 | loss: 1.5660408Losses:  0.855078935623169 0.675905704498291
MemoryTrain:  epoch  6, batch     0 | loss: 1.5309846Losses:  0.8580650091171265 0.5322069525718689
MemoryTrain:  epoch  6, batch     1 | loss: 1.3902719Losses:  0.8926812410354614 0.6552562713623047
MemoryTrain:  epoch  6, batch     2 | loss: 1.5479375Losses:  1.0950090885162354 0.8220170736312866
MemoryTrain:  epoch  6, batch     3 | loss: 1.9170262Losses:  0.847894012928009 0.639312744140625
MemoryTrain:  epoch  6, batch     4 | loss: 1.4872067Losses:  0.9460344910621643 0.7561124563217163
MemoryTrain:  epoch  7, batch     0 | loss: 1.7021470Losses:  0.9448621273040771 0.6613843441009521
MemoryTrain:  epoch  7, batch     1 | loss: 1.6062465Losses:  0.7871441841125488 0.5745568871498108
MemoryTrain:  epoch  7, batch     2 | loss: 1.3617010Losses:  1.0013898611068726 0.7245997786521912
MemoryTrain:  epoch  7, batch     3 | loss: 1.7259896Losses:  0.8089091181755066 0.5012544989585876
MemoryTrain:  epoch  7, batch     4 | loss: 1.3101636Losses:  0.7664493918418884 0.6740643382072449
MemoryTrain:  epoch  8, batch     0 | loss: 1.4405137Losses:  0.9000799655914307 0.6594217419624329
MemoryTrain:  epoch  8, batch     1 | loss: 1.5595016Losses:  0.8851345181465149 0.5836491584777832
MemoryTrain:  epoch  8, batch     2 | loss: 1.4687836Losses:  0.8659895658493042 0.6310257315635681
MemoryTrain:  epoch  8, batch     3 | loss: 1.4970152Losses:  0.860808253288269 0.639369785785675
MemoryTrain:  epoch  8, batch     4 | loss: 1.5001781Losses:  0.8612945079803467 0.5852265357971191
MemoryTrain:  epoch  9, batch     0 | loss: 1.4465210Losses:  0.8668188452720642 0.5706477165222168
MemoryTrain:  epoch  9, batch     1 | loss: 1.4374666Losses:  0.9639328122138977 0.8327921628952026
MemoryTrain:  epoch  9, batch     2 | loss: 1.7967250Losses:  0.7432076930999756 0.4993351101875305
MemoryTrain:  epoch  9, batch     3 | loss: 1.2425427Losses:  0.8904407620429993 0.6623696088790894
MemoryTrain:  epoch  9, batch     4 | loss: 1.5528104
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 59.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 73.72%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 73.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 73.47%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 72.28%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 70.79%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 68.96%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 67.16%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.64%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.84%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.78%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 85.88%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.57%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.49%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 84.76%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 82.84%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 82.19%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 81.56%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 80.56%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 80.18%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 79.62%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 78.88%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 78.54%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 77.81%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 78.51%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 78.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 78.70%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 78.33%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.29%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 78.24%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 78.28%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 77.94%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 77.83%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 77.65%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 77.62%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 76.99%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 76.25%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 75.61%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 75.07%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 74.53%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 74.28%   [EVAL] batch:   95 | acc: 37.50%,  total acc: 73.89%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 73.71%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 73.28%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 73.04%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 72.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 72.02%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 71.24%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 70.76%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 70.23%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 69.82%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 69.48%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 70.94%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 70.87%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 70.95%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 71.04%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 71.07%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 71.08%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.87%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 70.70%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 70.58%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 70.47%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 70.64%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 70.85%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 71.17%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 70.59%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 70.18%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 69.77%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 69.37%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 69.01%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 69.45%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 69.00%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.55%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 68.18%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 67.74%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.31%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.52%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 67.72%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 66.77%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 66.52%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 66.27%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 66.24%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 66.16%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 65.87%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 65.69%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 67.32%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 67.32%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 67.20%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 67.12%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 67.03%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 66.69%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 66.48%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 66.34%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 65.75%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 65.43%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 65.15%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 64.87%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 64.59%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 67.67%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 68.10%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 67.96%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 67.71%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 67.33%   [EVAL] batch:  255 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 67.04%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 66.93%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 66.74%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 67.46%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 67.29%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 67.23%   [EVAL] batch:  279 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  280 | acc: 37.50%,  total acc: 67.04%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 66.98%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 66.70%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 66.35%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 66.11%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:  288 | acc: 12.50%,  total acc: 65.74%   [EVAL] batch:  289 | acc: 12.50%,  total acc: 65.56%   [EVAL] batch:  290 | acc: 12.50%,  total acc: 65.38%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 65.15%   [EVAL] batch:  292 | acc: 37.50%,  total acc: 65.06%   [EVAL] batch:  293 | acc: 12.50%,  total acc: 64.88%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 64.79%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 64.63%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 64.60%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 64.51%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 65.69%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 65.34%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 65.15%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 64.96%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 64.82%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 64.69%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 64.77%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 65.04%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 64.92%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 64.89%   [EVAL] batch:  329 | acc: 43.75%,  total acc: 64.83%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 64.75%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.01%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.27%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 66.15%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 66.00%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 65.90%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 65.77%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 65.60%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 65.50%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 65.55%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 65.89%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 65.80%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 65.80%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 65.77%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 65.63%   [EVAL] batch:  377 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 65.32%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 65.16%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 65.01%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 65.06%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 65.54%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 65.27%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 65.17%   [EVAL] batch:  398 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 64.91%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 64.76%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 64.60%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 64.44%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 64.29%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 64.15%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 63.99%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 64.04%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.59%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 64.71%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 64.75%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  425 | acc: 37.50%,  total acc: 64.76%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 64.78%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 64.83%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 64.83%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 64.81%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 64.72%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 64.65%   [EVAL] batch:  438 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:  439 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  440 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  441 | acc: 43.75%,  total acc: 64.52%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 64.48%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 64.41%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 64.45%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 64.46%   [EVAL] batch:  446 | acc: 37.50%,  total acc: 64.40%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 64.40%   [EVAL] batch:  448 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.42%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 64.57%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 64.63%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 64.70%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  456 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 64.67%   [EVAL] batch:  458 | acc: 18.75%,  total acc: 64.57%   [EVAL] batch:  459 | acc: 50.00%,  total acc: 64.54%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 64.45%   [EVAL] batch:  461 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  475 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 65.55%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 65.46%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 65.41%   [EVAL] batch:  484 | acc: 31.25%,  total acc: 65.34%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 65.30%   [EVAL] batch:  486 | acc: 37.50%,  total acc: 65.25%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 65.52%   [EVAL] batch:  494 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:  495 | acc: 25.00%,  total acc: 65.36%   [EVAL] batch:  496 | acc: 18.75%,  total acc: 65.27%   [EVAL] batch:  497 | acc: 37.50%,  total acc: 65.21%   [EVAL] batch:  498 | acc: 31.25%,  total acc: 65.14%   [EVAL] batch:  499 | acc: 31.25%,  total acc: 65.08%   
cur_acc:  ['0.9435', '0.7183', '0.7153', '0.7768', '0.6815', '0.7024', '0.5833', '0.6716']
his_acc:  ['0.9435', '0.8280', '0.7829', '0.7685', '0.7506', '0.7138', '0.6658', '0.6508']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.830145835876465 2.1285247802734375
CurrentTrain: epoch  0, batch     0 | loss: 13.9586706Losses:  12.54277229309082 2.184195041656494
CurrentTrain: epoch  0, batch     1 | loss: 14.7269669Losses:  12.800271987915039 1.7025200128555298
CurrentTrain: epoch  0, batch     2 | loss: 14.5027924Losses:  12.935129165649414 2.0111308097839355
CurrentTrain: epoch  0, batch     3 | loss: 14.9462605Losses:  12.805746078491211 1.6991324424743652
CurrentTrain: epoch  0, batch     4 | loss: 14.5048790Losses:  12.630903244018555 1.8394945859909058
CurrentTrain: epoch  0, batch     5 | loss: 14.4703979Losses:  12.722433090209961 1.635284185409546
CurrentTrain: epoch  0, batch     6 | loss: 14.3577175Losses:  12.068395614624023 1.4758710861206055
CurrentTrain: epoch  0, batch     7 | loss: 13.5442667Losses:  12.330266952514648 1.62028968334198
CurrentTrain: epoch  0, batch     8 | loss: 13.9505568Losses:  11.953206062316895 1.7184280157089233
CurrentTrain: epoch  0, batch     9 | loss: 13.6716337Losses:  12.149959564208984 1.5947600603103638
CurrentTrain: epoch  0, batch    10 | loss: 13.7447195Losses:  11.6824951171875 1.568260669708252
CurrentTrain: epoch  0, batch    11 | loss: 13.2507553Losses:  11.56456470489502 1.727698802947998
CurrentTrain: epoch  0, batch    12 | loss: 13.2922630Losses:  11.595611572265625 1.71300208568573
CurrentTrain: epoch  0, batch    13 | loss: 13.3086138Losses:  11.17648696899414 1.3105740547180176
CurrentTrain: epoch  0, batch    14 | loss: 12.4870605Losses:  11.564775466918945 1.5340981483459473
CurrentTrain: epoch  0, batch    15 | loss: 13.0988731Losses:  11.273319244384766 1.8012187480926514
CurrentTrain: epoch  0, batch    16 | loss: 13.0745382Losses:  11.326669692993164 1.567399263381958
CurrentTrain: epoch  0, batch    17 | loss: 12.8940687Losses:  10.96883773803711 1.5822269916534424
CurrentTrain: epoch  0, batch    18 | loss: 12.5510645Losses:  11.186328887939453 1.649343490600586
CurrentTrain: epoch  0, batch    19 | loss: 12.8356724Losses:  10.752758979797363 1.5759360790252686
CurrentTrain: epoch  0, batch    20 | loss: 12.3286953Losses:  10.68482780456543 1.51002836227417
CurrentTrain: epoch  0, batch    21 | loss: 12.1948566Losses:  10.652546882629395 1.1307481527328491
CurrentTrain: epoch  0, batch    22 | loss: 11.7832947Losses:  10.063606262207031 1.3168299198150635
CurrentTrain: epoch  0, batch    23 | loss: 11.3804359Losses:  10.516343116760254 1.4437824487686157
CurrentTrain: epoch  0, batch    24 | loss: 11.9601259Losses:  10.665224075317383 1.1638959646224976
CurrentTrain: epoch  0, batch    25 | loss: 11.8291197Losses:  10.306304931640625 1.393515706062317
CurrentTrain: epoch  0, batch    26 | loss: 11.6998205Losses:  10.338913917541504 1.6055822372436523
CurrentTrain: epoch  0, batch    27 | loss: 11.9444962Losses:  10.238269805908203 1.3140324354171753
CurrentTrain: epoch  0, batch    28 | loss: 11.5523024Losses:  10.114799499511719 1.1669467687606812
CurrentTrain: epoch  0, batch    29 | loss: 11.2817459Losses:  9.714818000793457 1.2913556098937988
CurrentTrain: epoch  0, batch    30 | loss: 11.0061741Losses:  9.755556106567383 1.6343117952346802
CurrentTrain: epoch  0, batch    31 | loss: 11.3898678Losses:  9.715191841125488 1.4224916696548462
CurrentTrain: epoch  0, batch    32 | loss: 11.1376839Losses:  9.667000770568848 1.3231735229492188
CurrentTrain: epoch  0, batch    33 | loss: 10.9901743Losses:  9.19626235961914 1.1847360134124756
CurrentTrain: epoch  0, batch    34 | loss: 10.3809986Losses:  9.728915214538574 1.0944342613220215
CurrentTrain: epoch  0, batch    35 | loss: 10.8233490Losses:  9.979471206665039 1.1199710369110107
CurrentTrain: epoch  0, batch    36 | loss: 11.0994425Losses:  9.066770553588867 1.2693917751312256
CurrentTrain: epoch  0, batch    37 | loss: 10.3361626Losses:  9.145439147949219 1.2131505012512207
CurrentTrain: epoch  0, batch    38 | loss: 10.3585892Losses:  8.618346214294434 1.2772296667099
CurrentTrain: epoch  0, batch    39 | loss: 9.8955755Losses:  8.695494651794434 1.2310349941253662
CurrentTrain: epoch  0, batch    40 | loss: 9.9265299Losses:  8.893049240112305 1.192941427230835
CurrentTrain: epoch  0, batch    41 | loss: 10.0859909Losses:  8.462763786315918 1.0214107036590576
CurrentTrain: epoch  0, batch    42 | loss: 9.4841747Losses:  8.180107116699219 0.9446145296096802
CurrentTrain: epoch  0, batch    43 | loss: 9.1247215Losses:  8.31903076171875 1.042400598526001
CurrentTrain: epoch  0, batch    44 | loss: 9.3614311Losses:  8.202378273010254 1.0589569807052612
CurrentTrain: epoch  0, batch    45 | loss: 9.2613354Losses:  8.555485725402832 1.1085529327392578
CurrentTrain: epoch  0, batch    46 | loss: 9.6640387Losses:  7.932429790496826 1.0867493152618408
CurrentTrain: epoch  0, batch    47 | loss: 9.0191793Losses:  8.027381896972656 1.0905804634094238
CurrentTrain: epoch  0, batch    48 | loss: 9.1179619Losses:  7.571783065795898 1.1293926239013672
CurrentTrain: epoch  0, batch    49 | loss: 8.7011757Losses:  8.304922103881836 0.8056648969650269
CurrentTrain: epoch  0, batch    50 | loss: 9.1105871Losses:  7.435538291931152 0.8819695115089417
CurrentTrain: epoch  0, batch    51 | loss: 8.3175077Losses:  7.413507461547852 1.0475291013717651
CurrentTrain: epoch  0, batch    52 | loss: 8.4610367Losses:  7.400017738342285 0.836972177028656
CurrentTrain: epoch  0, batch    53 | loss: 8.2369900Losses:  7.234306335449219 0.9833223223686218
CurrentTrain: epoch  0, batch    54 | loss: 8.2176285Losses:  7.699446678161621 1.1927177906036377
CurrentTrain: epoch  0, batch    55 | loss: 8.8921642Losses:  6.735378265380859 0.9546251893043518
CurrentTrain: epoch  0, batch    56 | loss: 7.6900034Losses:  6.519976615905762 0.7113009095191956
CurrentTrain: epoch  0, batch    57 | loss: 7.2312775Losses:  6.796926498413086 0.8964135646820068
CurrentTrain: epoch  0, batch    58 | loss: 7.6933403Losses:  7.226574420928955 0.8671096563339233
CurrentTrain: epoch  0, batch    59 | loss: 8.0936842Losses:  6.562737941741943 0.8774218559265137
CurrentTrain: epoch  0, batch    60 | loss: 7.4401598Losses:  6.438577651977539 0.870974063873291
CurrentTrain: epoch  0, batch    61 | loss: 7.3095517Losses:  6.388520240783691 0.831376850605011
CurrentTrain: epoch  0, batch    62 | loss: 7.2198973Losses:  6.641143798828125 0.830017626285553
CurrentTrain: epoch  1, batch     0 | loss: 7.4711614Losses:  5.722787380218506 0.7182493805885315
CurrentTrain: epoch  1, batch     1 | loss: 6.4410367Losses:  5.996859550476074 0.9623335003852844
CurrentTrain: epoch  1, batch     2 | loss: 6.9591932Losses:  6.0252790451049805 0.7927695512771606
CurrentTrain: epoch  1, batch     3 | loss: 6.8180485Losses:  5.605924606323242 0.7579901218414307
CurrentTrain: epoch  1, batch     4 | loss: 6.3639145Losses:  5.597002029418945 0.5502729415893555
CurrentTrain: epoch  1, batch     5 | loss: 6.1472750Losses:  6.027935028076172 0.6205594539642334
CurrentTrain: epoch  1, batch     6 | loss: 6.6484947Losses:  6.103346824645996 0.8990170955657959
CurrentTrain: epoch  1, batch     7 | loss: 7.0023642Losses:  5.9289116859436035 0.8359397053718567
CurrentTrain: epoch  1, batch     8 | loss: 6.7648516Losses:  5.741847991943359 0.6186957359313965
CurrentTrain: epoch  1, batch     9 | loss: 6.3605437Losses:  5.898587703704834 0.7947369813919067
CurrentTrain: epoch  1, batch    10 | loss: 6.6933246Losses:  6.3352742195129395 0.8431740403175354
CurrentTrain: epoch  1, batch    11 | loss: 7.1784482Losses:  5.748668193817139 0.7903239727020264
CurrentTrain: epoch  1, batch    12 | loss: 6.5389919Losses:  6.108807563781738 0.875184178352356
CurrentTrain: epoch  1, batch    13 | loss: 6.9839916Losses:  6.165244102478027 0.9042970538139343
CurrentTrain: epoch  1, batch    14 | loss: 7.0695410Losses:  5.973872184753418 0.7930317521095276
CurrentTrain: epoch  1, batch    15 | loss: 6.7669039Losses:  5.568602085113525 0.6666331887245178
CurrentTrain: epoch  1, batch    16 | loss: 6.2352352Losses:  6.342303276062012 0.6913882493972778
CurrentTrain: epoch  1, batch    17 | loss: 7.0336914Losses:  5.775835037231445 0.7648863792419434
CurrentTrain: epoch  1, batch    18 | loss: 6.5407214Losses:  5.707390785217285 0.7755357027053833
CurrentTrain: epoch  1, batch    19 | loss: 6.4829264Losses:  5.935736656188965 0.7701602578163147
CurrentTrain: epoch  1, batch    20 | loss: 6.7058969Losses:  5.772609710693359 0.6470722556114197
CurrentTrain: epoch  1, batch    21 | loss: 6.4196820Losses:  5.800166130065918 0.7236942648887634
CurrentTrain: epoch  1, batch    22 | loss: 6.5238605Losses:  5.893143653869629 0.7168501615524292
CurrentTrain: epoch  1, batch    23 | loss: 6.6099939Losses:  5.946164131164551 0.7216200828552246
CurrentTrain: epoch  1, batch    24 | loss: 6.6677842Losses:  5.466264724731445 0.7291327714920044
CurrentTrain: epoch  1, batch    25 | loss: 6.1953974Losses:  5.974751949310303 0.7689841985702515
CurrentTrain: epoch  1, batch    26 | loss: 6.7437363Losses:  5.221556186676025 0.4398873448371887
CurrentTrain: epoch  1, batch    27 | loss: 5.6614437Losses:  5.306720733642578 0.44622498750686646
CurrentTrain: epoch  1, batch    28 | loss: 5.7529459Losses:  5.578372001647949 0.6788501739501953
CurrentTrain: epoch  1, batch    29 | loss: 6.2572222Losses:  5.469725608825684 0.654198408126831
CurrentTrain: epoch  1, batch    30 | loss: 6.1239243Losses:  5.890740871429443 0.6524418592453003
CurrentTrain: epoch  1, batch    31 | loss: 6.5431828Losses:  5.291379928588867 0.36411604285240173
CurrentTrain: epoch  1, batch    32 | loss: 5.6554961Losses:  5.579631328582764 0.682976484298706
CurrentTrain: epoch  1, batch    33 | loss: 6.2626076Losses:  5.5677900314331055 0.5148909091949463
CurrentTrain: epoch  1, batch    34 | loss: 6.0826807Losses:  5.442249298095703 0.6094942092895508
CurrentTrain: epoch  1, batch    35 | loss: 6.0517435Losses:  5.455977439880371 0.6649601459503174
CurrentTrain: epoch  1, batch    36 | loss: 6.1209373Losses:  5.496513843536377 0.6874574422836304
CurrentTrain: epoch  1, batch    37 | loss: 6.1839714Losses:  5.623874187469482 0.6715097427368164
CurrentTrain: epoch  1, batch    38 | loss: 6.2953839Losses:  4.930963516235352 0.39850446581840515
CurrentTrain: epoch  1, batch    39 | loss: 5.3294678Losses:  5.644909858703613 0.5999862551689148
CurrentTrain: epoch  1, batch    40 | loss: 6.2448959Losses:  5.7535529136657715 0.6863146424293518
CurrentTrain: epoch  1, batch    41 | loss: 6.4398675Losses:  5.156608581542969 0.6612679958343506
CurrentTrain: epoch  1, batch    42 | loss: 5.8178768Losses:  5.536302089691162 0.6092718243598938
CurrentTrain: epoch  1, batch    43 | loss: 6.1455741Losses:  5.944355010986328 0.7007808685302734
CurrentTrain: epoch  1, batch    44 | loss: 6.6451359Losses:  5.233351707458496 0.5714900493621826
CurrentTrain: epoch  1, batch    45 | loss: 5.8048420Losses:  5.298707962036133 0.5004324913024902
CurrentTrain: epoch  1, batch    46 | loss: 5.7991405Losses:  5.268139839172363 0.5214730501174927
CurrentTrain: epoch  1, batch    47 | loss: 5.7896128Losses:  5.7289228439331055 0.5122573375701904
CurrentTrain: epoch  1, batch    48 | loss: 6.2411804Losses:  4.982108116149902 0.45282769203186035
CurrentTrain: epoch  1, batch    49 | loss: 5.4349356Losses:  5.097066879272461 0.508369505405426
CurrentTrain: epoch  1, batch    50 | loss: 5.6054363Losses:  4.71597957611084 0.24755321443080902
CurrentTrain: epoch  1, batch    51 | loss: 4.9635329Losses:  5.139465808868408 0.39015552401542664
CurrentTrain: epoch  1, batch    52 | loss: 5.5296211Losses:  4.850348472595215 0.5009157061576843
CurrentTrain: epoch  1, batch    53 | loss: 5.3512640Losses:  5.729850769042969 0.49690738320350647
CurrentTrain: epoch  1, batch    54 | loss: 6.2267580Losses:  4.804975986480713 0.24718579649925232
CurrentTrain: epoch  1, batch    55 | loss: 5.0521617Losses:  5.321495056152344 0.6023367643356323
CurrentTrain: epoch  1, batch    56 | loss: 5.9238319Losses:  5.336341857910156 0.4697694480419159
CurrentTrain: epoch  1, batch    57 | loss: 5.8061113Losses:  5.178004264831543 0.45950794219970703
CurrentTrain: epoch  1, batch    58 | loss: 5.6375122Losses:  5.036189079284668 0.5657728910446167
CurrentTrain: epoch  1, batch    59 | loss: 5.6019621Losses:  4.786610126495361 0.3929944336414337
CurrentTrain: epoch  1, batch    60 | loss: 5.1796045Losses:  5.304677963256836 0.4534454345703125
CurrentTrain: epoch  1, batch    61 | loss: 5.7581234Losses:  5.189889907836914 0.27506887912750244
CurrentTrain: epoch  1, batch    62 | loss: 5.4649587Losses:  4.9165778160095215 0.41911426186561584
CurrentTrain: epoch  2, batch     0 | loss: 5.3356919Losses:  4.634671211242676 0.3505362868309021
CurrentTrain: epoch  2, batch     1 | loss: 4.9852076Losses:  4.587586402893066 0.3847707509994507
CurrentTrain: epoch  2, batch     2 | loss: 4.9723573Losses:  4.753979682922363 0.4254820644855499
CurrentTrain: epoch  2, batch     3 | loss: 5.1794620Losses:  4.707945346832275 0.48571574687957764
CurrentTrain: epoch  2, batch     4 | loss: 5.1936612Losses:  5.195807933807373 0.3878902792930603
CurrentTrain: epoch  2, batch     5 | loss: 5.5836983Losses:  4.625538349151611 0.4014957547187805
CurrentTrain: epoch  2, batch     6 | loss: 5.0270343Losses:  5.2139129638671875 0.5958683490753174
CurrentTrain: epoch  2, batch     7 | loss: 5.8097811Losses:  5.302651405334473 0.4850412607192993
CurrentTrain: epoch  2, batch     8 | loss: 5.7876925Losses:  4.570591926574707 0.3296085596084595
CurrentTrain: epoch  2, batch     9 | loss: 4.9002004Losses:  4.836963176727295 0.37602680921554565
CurrentTrain: epoch  2, batch    10 | loss: 5.2129898Losses:  4.625600814819336 0.403804749250412
CurrentTrain: epoch  2, batch    11 | loss: 5.0294056Losses:  4.757006645202637 0.2731325030326843
CurrentTrain: epoch  2, batch    12 | loss: 5.0301390Losses:  4.807469844818115 0.45738017559051514
CurrentTrain: epoch  2, batch    13 | loss: 5.2648501Losses:  4.943965435028076 0.3312515914440155
CurrentTrain: epoch  2, batch    14 | loss: 5.2752171Losses:  4.652968406677246 0.3164421319961548
CurrentTrain: epoch  2, batch    15 | loss: 4.9694104Losses:  4.774806022644043 0.3516950011253357
CurrentTrain: epoch  2, batch    16 | loss: 5.1265011Losses:  5.140505313873291 0.474458783864975
CurrentTrain: epoch  2, batch    17 | loss: 5.6149640Losses:  4.760334014892578 0.42008352279663086
CurrentTrain: epoch  2, batch    18 | loss: 5.1804175Losses:  4.8788676261901855 0.34495809674263
CurrentTrain: epoch  2, batch    19 | loss: 5.2238259Losses:  5.418566703796387 0.2280140221118927
CurrentTrain: epoch  2, batch    20 | loss: 5.6465807Losses:  4.837855339050293 0.42229384183883667
CurrentTrain: epoch  2, batch    21 | loss: 5.2601490Losses:  4.430419445037842 0.3172895908355713
CurrentTrain: epoch  2, batch    22 | loss: 4.7477093Losses:  4.734406471252441 0.3261803388595581
CurrentTrain: epoch  2, batch    23 | loss: 5.0605869Losses:  4.498776435852051 0.3631938397884369
CurrentTrain: epoch  2, batch    24 | loss: 4.8619704Losses:  4.59673547744751 0.21616438031196594
CurrentTrain: epoch  2, batch    25 | loss: 4.8129001Losses:  4.413779258728027 0.2654007077217102
CurrentTrain: epoch  2, batch    26 | loss: 4.6791801Losses:  4.608074188232422 0.29769590497016907
CurrentTrain: epoch  2, batch    27 | loss: 4.9057703Losses:  4.513965129852295 0.26557058095932007
CurrentTrain: epoch  2, batch    28 | loss: 4.7795358Losses:  4.796285629272461 0.41999292373657227
CurrentTrain: epoch  2, batch    29 | loss: 5.2162786Losses:  4.758181571960449 0.3115089535713196
CurrentTrain: epoch  2, batch    30 | loss: 5.0696907Losses:  4.885175704956055 0.4250403046607971
CurrentTrain: epoch  2, batch    31 | loss: 5.3102160Losses:  4.650847434997559 0.2602204382419586
#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.790603637695312 1.8446838855743408
CurrentTrain: epoch  0, batch     0 | loss: 13.6352873Losses:  13.610809326171875 2.180497646331787
CurrentTrain: epoch  0, batch     1 | loss: 15.7913074Losses:  13.991414070129395 1.695072054862976
CurrentTrain: epoch  0, batch     2 | loss: 15.6864862Losses:  13.438653945922852 1.7126469612121582
CurrentTrain: epoch  0, batch     3 | loss: 15.1513004Losses:  13.652498245239258 1.5969964265823364
CurrentTrain: epoch  0, batch     4 | loss: 15.2494946Losses:  13.423798561096191 1.4856184720993042
CurrentTrain: epoch  0, batch     5 | loss: 14.9094172Losses:  12.813584327697754 1.7226226329803467
CurrentTrain: epoch  0, batch     6 | loss: 14.5362072Losses:  13.02552604675293 1.5342388153076172
CurrentTrain: epoch  0, batch     7 | loss: 14.5597649Losses:  13.11590576171875 1.5010467767715454
CurrentTrain: epoch  0, batch     8 | loss: 14.6169529Losses:  12.658163070678711 1.7966235876083374
CurrentTrain: epoch  0, batch     9 | loss: 14.4547863Losses:  12.978662490844727 1.5144627094268799
CurrentTrain: epoch  0, batch    10 | loss: 14.4931250Losses:  12.51284122467041 1.3649914264678955
CurrentTrain: epoch  0, batch    11 | loss: 13.8778324Losses:  12.210262298583984 1.612074613571167
CurrentTrain: epoch  0, batch    12 | loss: 13.8223372Losses:  12.25843334197998 1.5757477283477783
CurrentTrain: epoch  0, batch    13 | loss: 13.8341808Losses:  12.057132720947266 1.8530619144439697
CurrentTrain: epoch  0, batch    14 | loss: 13.9101944Losses:  12.1458740234375 2.2540135383605957
CurrentTrain: epoch  0, batch    15 | loss: 14.3998871Losses:  11.780024528503418 1.5158346891403198
CurrentTrain: epoch  0, batch    16 | loss: 13.2958593Losses:  11.915242195129395 2.091268539428711
CurrentTrain: epoch  0, batch    17 | loss: 14.0065107Losses:  11.85228157043457 1.8090705871582031
CurrentTrain: epoch  0, batch    18 | loss: 13.6613522Losses:  11.41946792602539 1.797403335571289
CurrentTrain: epoch  0, batch    19 | loss: 13.2168713Losses:  11.361886978149414 1.4744532108306885
CurrentTrain: epoch  0, batch    20 | loss: 12.8363400Losses:  11.064075469970703 1.6147180795669556
CurrentTrain: epoch  0, batch    21 | loss: 12.6787939Losses:  10.81937026977539 1.4763696193695068
CurrentTrain: epoch  0, batch    22 | loss: 12.2957401Losses:  10.479249954223633 1.207776665687561
CurrentTrain: epoch  0, batch    23 | loss: 11.6870270Losses:  10.761131286621094 1.4450651407241821
CurrentTrain: epoch  0, batch    24 | loss: 12.2061968Losses:  10.130608558654785 1.2265324592590332
CurrentTrain: epoch  0, batch    25 | loss: 11.3571415Losses:  10.559687614440918 1.265633225440979
CurrentTrain: epoch  0, batch    26 | loss: 11.8253212Losses:  9.962583541870117 1.3888139724731445
CurrentTrain: epoch  0, batch    27 | loss: 11.3513975Losses:  10.10601806640625 1.2865447998046875
CurrentTrain: epoch  0, batch    28 | loss: 11.3925629Losses:  9.884166717529297 1.2771565914154053
CurrentTrain: epoch  0, batch    29 | loss: 11.1613235Losses:  9.762788772583008 1.4861793518066406
CurrentTrain: epoch  0, batch    30 | loss: 11.2489681Losses:  9.886054039001465 1.0766361951828003
CurrentTrain: epoch  0, batch    31 | loss: 10.9626904Losses:  9.826093673706055 1.048862338066101
CurrentTrain: epoch  0, batch    32 | loss: 10.8749561Losses:  9.783143997192383 1.0092198848724365
CurrentTrain: epoch  0, batch    33 | loss: 10.7923641Losses:  9.704360961914062 1.1133252382278442
CurrentTrain: epoch  0, batch    34 | loss: 10.8176861Losses:  9.116517066955566 1.1539146900177002
CurrentTrain: epoch  0, batch    35 | loss: 10.2704315Losses:  9.645774841308594 1.317635416984558
CurrentTrain: epoch  0, batch    36 | loss: 10.9634104Losses:  9.071857452392578 1.1428054571151733
CurrentTrain: epoch  0, batch    37 | loss: 10.2146626Losses:  9.520954132080078 1.379478931427002
CurrentTrain: epoch  0, batch    38 | loss: 10.9004326Losses:  9.079986572265625 1.2885596752166748
CurrentTrain: epoch  0, batch    39 | loss: 10.3685465Losses:  8.944368362426758 1.2551746368408203
CurrentTrain: epoch  0, batch    40 | loss: 10.1995430Losses:  8.555975914001465 1.1286914348602295
CurrentTrain: epoch  0, batch    41 | loss: 9.6846676Losses:  8.45850944519043 1.1717692613601685
CurrentTrain: epoch  0, batch    42 | loss: 9.6302786Losses:  8.30605697631836 0.8366614580154419
CurrentTrain: epoch  0, batch    43 | loss: 9.1427183Losses:  7.943556308746338 1.0229105949401855
CurrentTrain: epoch  0, batch    44 | loss: 8.9664669Losses:  7.775141716003418 1.1341099739074707
CurrentTrain: epoch  0, batch    45 | loss: 8.9092522Losses:  8.245519638061523 0.9320346117019653
CurrentTrain: epoch  0, batch    46 | loss: 9.1775541Losses:  7.429649353027344 1.1258865594863892
CurrentTrain: epoch  0, batch    47 | loss: 8.5555363Losses:  8.00916862487793 1.3096939325332642
CurrentTrain: epoch  0, batch    48 | loss: 9.3188629Losses:  7.449488639831543 1.1855559349060059
CurrentTrain: epoch  0, batch    49 | loss: 8.6350441Losses:  7.560523986816406 1.1966800689697266
CurrentTrain: epoch  0, batch    50 | loss: 8.7572041Losses:  7.162678241729736 1.086488127708435
CurrentTrain: epoch  0, batch    51 | loss: 8.2491665Losses:  7.162210941314697 0.8942251205444336
CurrentTrain: epoch  0, batch    52 | loss: 8.0564365Losses:  7.355406761169434 1.1052305698394775
CurrentTrain: epoch  0, batch    53 | loss: 8.4606371Losses:  7.17186164855957 1.2898476123809814
CurrentTrain: epoch  0, batch    54 | loss: 8.4617090Losses:  6.616057872772217 0.9629234671592712
CurrentTrain: epoch  0, batch    55 | loss: 7.5789814Losses:  6.4447174072265625 0.8609917759895325
CurrentTrain: epoch  0, batch    56 | loss: 7.3057094Losses:  6.702581882476807 1.0212838649749756
CurrentTrain: epoch  0, batch    57 | loss: 7.7238655Losses:  6.3423919677734375 0.8480865955352783
CurrentTrain: epoch  0, batch    58 | loss: 7.1904783Losses:  6.467799186706543 0.7966605424880981
CurrentTrain: epoch  0, batch    59 | loss: 7.2644596Losses:  5.844562530517578 0.8510791063308716
CurrentTrain: epoch  0, batch    60 | loss: 6.6956415Losses:  6.305201530456543 0.7011549472808838
CurrentTrain: epoch  0, batch    61 | loss: 7.0063562Losses:  6.821629047393799 0.7583943009376526
CurrentTrain: epoch  0, batch    62 | loss: 7.5800233Losses:  6.027168273925781 0.8711812496185303
CurrentTrain: epoch  1, batch     0 | loss: 6.8983498Losses:  5.60353422164917 0.7449657917022705
CurrentTrain: epoch  1, batch     1 | loss: 6.3485003Losses:  5.846981525421143 0.9234679937362671
CurrentTrain: epoch  1, batch     2 | loss: 6.7704496Losses:  5.649607181549072 0.8685712814331055
CurrentTrain: epoch  1, batch     3 | loss: 6.5181785Losses:  6.0789594650268555 0.9293515682220459
CurrentTrain: epoch  1, batch     4 | loss: 7.0083113Losses:  5.513218879699707 0.6674548387527466
CurrentTrain: epoch  1, batch     5 | loss: 6.1806736Losses:  5.634268760681152 0.7848609685897827
CurrentTrain: epoch  1, batch     6 | loss: 6.4191298Losses:  5.330748558044434 0.7358248233795166
CurrentTrain: epoch  1, batch     7 | loss: 6.0665731Losses:  5.48991584777832 0.6132868528366089
CurrentTrain: epoch  1, batch     8 | loss: 6.1032028Losses:  5.723049163818359 0.941973090171814
CurrentTrain: epoch  1, batch     9 | loss: 6.6650224Losses:  5.666093826293945 0.7603909969329834
CurrentTrain: epoch  1, batch    10 | loss: 6.4264851Losses:  5.705083847045898 0.963267982006073
CurrentTrain: epoch  1, batch    11 | loss: 6.6683517Losses:  5.672669410705566 0.8740648031234741
CurrentTrain: epoch  1, batch    12 | loss: 6.5467343Losses:  5.939504623413086 0.6178281307220459
CurrentTrain: epoch  1, batch    13 | loss: 6.5573330Losses:  5.311458587646484 0.7504069805145264
CurrentTrain: epoch  1, batch    14 | loss: 6.0618658Losses:  5.906708717346191 0.8121396899223328
CurrentTrain: epoch  1, batch    15 | loss: 6.7188482Losses:  5.77268648147583 0.7070598006248474
CurrentTrain: epoch  1, batch    16 | loss: 6.4797463Losses:  5.273624420166016 0.4747859239578247
CurrentTrain: epoch  1, batch    17 | loss: 5.7484102Losses:  5.301025390625 0.5904912948608398
CurrentTrain: epoch  1, batch    18 | loss: 5.8915167Losses:  5.50767707824707 0.6845621466636658
CurrentTrain: epoch  1, batch    19 | loss: 6.1922393Losses:  5.639304161071777 0.797674298286438
CurrentTrain: epoch  1, batch    20 | loss: 6.4369783Losses:  5.738035202026367 0.8403483629226685
CurrentTrain: epoch  1, batch    21 | loss: 6.5783834Losses:  5.563773155212402 0.7834768891334534
CurrentTrain: epoch  1, batch    22 | loss: 6.3472500Losses:  5.734302520751953 0.518707811832428
CurrentTrain: epoch  1, batch    23 | loss: 6.2530103Losses:  5.611921787261963 0.670191764831543
CurrentTrain: epoch  1, batch    24 | loss: 6.2821136Losses:  5.152322769165039 0.5567996501922607
CurrentTrain: epoch  1, batch    25 | loss: 5.7091227Losses:  5.360489845275879 0.7378301620483398
CurrentTrain: epoch  1, batch    26 | loss: 6.0983200Losses:  5.422801494598389 0.7746801376342773
CurrentTrain: epoch  1, batch    27 | loss: 6.1974816Losses:  5.238494873046875 0.6181405186653137
CurrentTrain: epoch  1, batch    28 | loss: 5.8566356Losses:  5.153869152069092 0.7017392516136169
CurrentTrain: epoch  1, batch    29 | loss: 5.8556085Losses:  5.158806800842285 0.5081180930137634
CurrentTrain: epoch  1, batch    30 | loss: 5.6669250Losses:  5.05393123626709 0.6649508476257324
CurrentTrain: epoch  1, batch    31 | loss: 5.7188821Losses:  5.078173637390137 0.6803924441337585
CurrentTrain: epoch  1, batch    32 | loss: 5.7585659Losses:  5.0749969482421875 0.4494193196296692
CurrentTrain: epoch  1, batch    33 | loss: 5.5244164Losses:  5.364266395568848 0.7833132743835449
CurrentTrain: epoch  1, batch    34 | loss: 6.1475797Losses:  5.061020374298096 0.6208940744400024
CurrentTrain: epoch  1, batch    35 | loss: 5.6819143Losses:  5.467763900756836 0.45672011375427246
CurrentTrain: epoch  1, batch    36 | loss: 5.9244843Losses:  5.39420223236084 0.70741868019104
CurrentTrain: epoch  1, batch    37 | loss: 6.1016207Losses:  5.642384052276611 0.7371474504470825
CurrentTrain: epoch  1, batch    38 | loss: 6.3795314Losses:  4.904568195343018 0.5019603371620178
CurrentTrain: epoch  1, batch    39 | loss: 5.4065285Losses:  5.407005310058594 0.5099006295204163
CurrentTrain: epoch  1, batch    40 | loss: 5.9169059Losses:  5.746399402618408 0.8660233020782471
CurrentTrain: epoch  1, batch    41 | loss: 6.6124229Losses:  5.278267860412598 0.6838245391845703
CurrentTrain: epoch  1, batch    42 | loss: 5.9620924Losses:  5.406029224395752 0.7362957000732422
CurrentTrain: epoch  1, batch    43 | loss: 6.1423249Losses:  5.531073570251465 0.6244455575942993
CurrentTrain: epoch  1, batch    44 | loss: 6.1555190Losses:  4.945202350616455 0.5362402200698853
CurrentTrain: epoch  1, batch    45 | loss: 5.4814425Losses:  4.8448381423950195 0.5745549201965332
CurrentTrain: epoch  1, batch    46 | loss: 5.4193931Losses:  5.363458633422852 0.7079862952232361
CurrentTrain: epoch  1, batch    47 | loss: 6.0714450Losses:  5.266209602355957 0.6379961967468262
CurrentTrain: epoch  1, batch    48 | loss: 5.9042058Losses:  4.97224235534668 0.5780304670333862
CurrentTrain: epoch  1, batch    49 | loss: 5.5502729Losses:  5.084850311279297 0.5202540159225464
CurrentTrain: epoch  1, batch    50 | loss: 5.6051044Losses:  5.378876209259033 0.7080264091491699
CurrentTrain: epoch  1, batch    51 | loss: 6.0869026Losses:  5.174043655395508 0.5874632000923157
CurrentTrain: epoch  1, batch    52 | loss: 5.7615070Losses:  4.935389518737793 0.47334587574005127
CurrentTrain: epoch  1, batch    53 | loss: 5.4087353Losses:  5.148553848266602 0.5952115654945374
CurrentTrain: epoch  1, batch    54 | loss: 5.7437654Losses:  4.512162208557129 0.2589077353477478
CurrentTrain: epoch  1, batch    55 | loss: 4.7710700Losses:  4.657153129577637 0.3941906988620758
CurrentTrain: epoch  1, batch    56 | loss: 5.0513439Losses:  4.911391258239746 0.5588808059692383
CurrentTrain: epoch  1, batch    57 | loss: 5.4702721Losses:  4.837418556213379 0.38938960433006287
CurrentTrain: epoch  1, batch    58 | loss: 5.2268081Losses:  5.161887168884277 0.6816452741622925
CurrentTrain: epoch  1, batch    59 | loss: 5.8435326Losses:  5.0178022384643555 0.686551570892334
CurrentTrain: epoch  1, batch    60 | loss: 5.7043538Losses:  4.728912830352783 0.5430657863616943
CurrentTrain: epoch  1, batch    61 | loss: 5.2719784Losses:  4.305446147918701 0.1789245456457138
CurrentTrain: epoch  1, batch    62 | loss: 4.4843707Losses:  4.977503776550293 0.44800621271133423
CurrentTrain: epoch  2, batch     0 | loss: 5.4255099Losses:  4.754021644592285 0.4418257176876068
CurrentTrain: epoch  2, batch     1 | loss: 5.1958475Losses:  4.794372081756592 0.5335782170295715
CurrentTrain: epoch  2, batch     2 | loss: 5.3279505Losses:  4.6121368408203125 0.474137544631958
CurrentTrain: epoch  2, batch     3 | loss: 5.0862741Losses:  4.804820537567139 0.4170875549316406
CurrentTrain: epoch  2, batch     4 | loss: 5.2219081Losses:  4.652968406677246 0.5573116540908813
CurrentTrain: epoch  2, batch     5 | loss: 5.2102799Losses:  4.694777965545654 0.33758896589279175
CurrentTrain: epoch  2, batch     6 | loss: 5.0323668Losses:  4.917445182800293 0.4603888988494873
CurrentTrain: epoch  2, batch     7 | loss: 5.3778343Losses:  5.3477630615234375 0.4680045247077942
CurrentTrain: epoch  2, batch     8 | loss: 5.8157678Losses:  4.494554042816162 0.2869986891746521
CurrentTrain: epoch  2, batch     9 | loss: 4.7815528Losses:  4.536073684692383 0.4140676259994507
CurrentTrain: epoch  2, batch    10 | loss: 4.9501414Losses:  4.900745391845703 0.43041449785232544
CurrentTrain: epoch  2, batch    11 | loss: 5.3311601Losses:  4.799639701843262 0.46375274658203125
CurrentTrain: epoch  2, batch    12 | loss: 5.2633924Losses:  4.987063884735107 0.42106032371520996
CurrentTrain: epoch  2, batch    13 | loss: 5.4081240Losses:  4.926807403564453 0.39704012870788574
CurrentTrain: epoch  2, batch    14 | loss: 5.3238478Losses:  4.590033531188965 0.3778407573699951
CurrentTrain: epoch  2, batch    15 | loss: 4.9678745Losses:  4.3692626953125 0.3269754648208618
CurrentTrain: epoch  2, batch    16 | loss: 4.6962380Losses:  4.332378387451172 0.3036494851112366
CurrentTrain: epoch  2, batch    17 | loss: 4.6360278Losses:  4.945520401000977 0.42382121086120605
CurrentTrain: epoch  2, batch    18 | loss: 5.3693419Losses:  4.757336139678955 0.3389657735824585
CurrentTrain: epoch  2, batch    19 | loss: 5.0963020Losses:  4.574174880981445 0.39269930124282837
CurrentTrain: epoch  2, batch    20 | loss: 4.9668741Losses:  4.485313415527344 0.3041306436061859
CurrentTrain: epoch  2, batch    21 | loss: 4.7894440Losses:  4.420096397399902 0.3741194009780884
CurrentTrain: epoch  2, batch    22 | loss: 4.7942157Losses:  4.734355926513672 0.3417787253856659
CurrentTrain: epoch  2, batch    23 | loss: 5.0761347Losses:  4.65199089050293 0.46918004751205444
CurrentTrain: epoch  2, batch    24 | loss: 5.1211710Losses:  4.529112339019775 0.3129431903362274
CurrentTrain: epoch  2, batch    25 | loss: 4.8420553Losses:  4.531536102294922 0.28498056530952454
CurrentTrain: epoch  2, batch    26 | loss: 4.8165169Losses:  4.576866149902344 0.30667930841445923
CurrentTrain: epoch  2, batch    27 | loss: 4.8835454Losses:  4.506827354431152 0.3270173668861389
CurrentTrain: epoch  2, batch    28 | loss: 4.8338447Losses:  4.516596794128418 0.3796018362045288
CurrentTrain: epoch  2, batch    29 | loss: 4.8961987Losses:  4.7074503898620605 0.5113386511802673
CurrentTrain: epoch  2, batch    30 | loss: 5.2187891Losses:  4.529785633087158 0.3801528215408325
CurrentTrain: epoch  2, batch    31 | loss: 4.9099383Losses:  5.167084693908691 0.4503779411315918
CurrentTrain: epoch  2, batch    32 | loss: 5.6174626Losses:  4.439295768737793 0.36972665786743164
CurrentTrain: epoch  2, batch    33 | loss: 4.8090224Losses:  4.718667030334473 0.43665480613708496
CurrentTrain: epoch  2, batch    34 | loss: 5.1553221Losses:  4.751835346221924 0.2988187074661255
CurrentTrain: epoch  2, batch    35 | loss: 5.0506539Losses:  4.435481071472168 0.29748862981796265
CurrentTrain: epoch  2, batch    36 | loss: 4.7329698Losses:  4.640796661376953 0.3040802478790283
CurrentTrain: epoch  2, batch    37 | loss: 4.9448767Losses:  4.525238990783691 0.24766170978546143
CurrentTrain: epoch  2, batch    38 | loss: 4.7729006Losses:  4.315921783447266 0.27892419695854187
CurrentTrain: epoch  2, batch    39 | loss: 4.5948458Losses:  4.448301792144775 0.34900426864624023
CurrentTrain: epoch  2, batch    40 | loss: 4.7973061Losses:  4.891170501708984 0.34920603036880493
CurrentTrain: epoch  2, batch    41 | loss: 5.2403765Losses:  4.5299530029296875 0.33222877979278564
CurrentTrain: epoch  2, batch    42 | loss: 4.8621817Losses:  4.647915840148926 0.3482934832572937
CurrentTrain: epoch  2, batch    43 | loss: 4.9962091Losses:  4.488204002380371 0.3700360655784607
CurrentTrain: epoch  2, batch    44 | loss: 4.8582401Losses:  4.608880043029785 0.29074448347091675
CurrentTrain: epoch  2, batch    45 | loss: 4.8996243Losses:  4.352845191955566 0.23683549463748932
CurrentTrain: epoch  2, batch    46 | loss: 4.5896807Losses:  4.750247955322266 0.42355090379714966
CurrentTrain: epoch  2, batch    47 | loss: 5.1737990Losses:  4.358164310455322 0.26763057708740234
CurrentTrain: epoch  2, batch    48 | loss: 4.6257949Losses:  4.539166450500488 0.3823399245738983
CurrentTrain: epoch  2, batch    49 | loss: 4.9215064Losses:  4.342013359069824 0.21406525373458862
CurrentTrain: epoch  2, batch    50 | loss: 4.5560784Losses:  4.330449104309082 0.2842743396759033
CurrentTrain: epoch  2, batch    51 | loss: 4.6147232Losses:  4.692824363708496 0.266921728849411
CurrentTrain: epoch  2, batch    52 | loss: 4.9597459Losses:  4.31856107711792 0.3041253685951233
CurrentTrain: epoch  2, batch    53 | loss: 4.6226864Losses:  4.322297096252441 0.2928526997566223
CurrentTrain: epoch  2, batch    54 | loss: 4.6151500Losses:  4.404581069946289 0.31302231550216675
CurrentTrain: epoch  2, batch    55 | loss: 4.7176032Losses:  4.512759685516357 0.42725569009780884
CurrentTrain: epoch  2, batch    56 | loss: 4.9400153Losses:  4.492532730102539 0.3239581286907196
CurrentTrain: epoch  2, batch    57 | loss: 4.8164907Losses:  4.417751312255859 0.27961570024490356
CurrentTrain: epoch  2, batch    58 | loss: 4.6973672Losses:  4.351532936096191 0.23696644604206085
CurrentTrain: epoch  2, batch    59 | loss: 4.5884995Losses:  4.2970781326293945 0.35554343461990356
CurrentTrain: epoch  2, batch    60 | loss: 4.6526217Losses:  4.408525466918945 0.2930067181587219
CurrentTrain: epoch  2, batch    61 | loss: 4.7015324Losses:  4.228141784667969 0.11130927503108978
CurrentTrain: epoch  2, batch    62 | loss: 4.3394508Losses:  4.350788116455078 0.28560489416122437
CurrentTrain: epoch  3, batch     0 | loss: 4.6363931Losses:  4.925315856933594 0.39146625995635986
CurrentTrain: epoch  3, batch     1 | loss: 5.3167820Losses:  4.310688018798828 0.2927953600883484
CurrentTrain: epoch  3, batch     2 | loss: 4.6034832Losses:  4.468204975128174 0.22341278195381165
CurrentTrain: epoch  3, batch     3 | loss: 4.6916180Losses:  4.393176078796387 0.23979954421520233
CurrentTrain: epoch  3, batch     4 | loss: 4.6329756Losses:  4.328244686126709 0.2120770663022995
CurrentTrain: epoch  3, batch     5 | loss: 4.5403218Losses:  4.291838645935059 0.268358051776886
CurrentTrain: epoch  3, batch     6 | loss: 4.5601969Losses:  4.278928756713867 0.24190554022789001
CurrentTrain: epoch  3, batch     7 | loss: 4.5208344Losses:  4.567917823791504 0.3549480736255646
CurrentTrain: epoch  3, batch     8 | loss: 4.9228659Losses:  4.441509246826172 0.2696809768676758
CurrentTrain: epoch  3, batch     9 | loss: 4.7111902Losses:  4.503438949584961 0.3484998941421509
CurrentTrain: epoch  3, batch    10 | loss: 4.8519387Losses:  4.354317665100098 0.23402400314807892
CurrentTrain: epoch  3, batch    11 | loss: 4.5883417Losses:  4.1705193519592285 0.17197668552398682
CurrentTrain: epoch  3, batch    12 | loss: 4.3424959Losses:  4.439517498016357 0.3646623492240906
CurrentTrain: epoch  3, batch    13 | loss: 4.8041797Losses:  4.223418235778809 0.24258053302764893
CurrentTrain: epoch  3, batch    14 | loss: 4.4659986Losses:  4.22026252746582 0.25985047221183777
CurrentTrain: epoch  3, batch    15 | loss: 4.4801130Losses:  4.220175743103027 0.2526547312736511
CurrentTrain: epoch  3, batch    16 | loss: 4.4728303Losses:  4.450894355773926 0.24930182099342346
CurrentTrain: epoch  3, batch    17 | loss: 4.7001963Losses:  4.343593597412109 0.20315036177635193
CurrentTrain: epoch  3, batch    18 | loss: 4.5467439Losses:  4.358365058898926 0.2842909097671509
CurrentTrain: epoch  3, batch    19 | loss: 4.6426558Losses:  4.243982791900635 0.22727176547050476
CurrentTrain: epoch  3, batch    20 | loss: 4.4712543Losses:  4.281637191772461 0.23602041602134705
CurrentTrain: epoch  3, batch    21 | loss: 4.5176578Losses:  4.353241920471191 0.29532158374786377
CurrentTrain: epoch  3, batch    22 | loss: 4.6485634Losses:  4.240092754364014 0.26441431045532227
CurrentTrain: epoch  3, batch    23 | loss: 4.5045071Losses:  4.235624313354492 0.15558619797229767
CurrentTrain: epoch  3, batch    24 | loss: 4.3912106Losses:  4.175457954406738 0.24695605039596558
CurrentTrain: epoch  3, batch    25 | loss: 4.4224138Losses:  4.221511363983154 0.2474021017551422
CurrentTrain: epoch  3, batch    26 | loss: 4.4689136Losses:  4.266910076141357 0.28006601333618164
CurrentTrain: epoch  3, batch    27 | loss: 4.5469761Losses:  4.311392784118652 0.2919705808162689
CurrentTrain: epoch  3, batch    28 | loss: 4.6033635Losses:  4.679440498352051 0.2843433618545532
CurrentTrain: epoch  3, batch    29 | loss: 4.9637837Losses:  4.533662796020508 0.2729074954986572
CurrentTrain: epoch  3, batch    30 | loss: 4.8065701Losses:  4.153206825256348 0.19864237308502197
CurrentTrain: epoch  3, batch    31 | loss: 4.3518491Losses:  4.3927388191223145 0.2294456958770752
CurrentTrain: epoch  3, batch    32 | loss: 4.6221848Losses:  4.3032684326171875 0.20936530828475952
CurrentTrain: epoch  3, batch    33 | loss: 4.5126338Losses:  4.17747688293457 0.24867910146713257
CurrentTrain: epoch  3, batch    34 | loss: 4.4261560Losses:  4.208870887756348 0.28509852290153503
CurrentTrain: epoch  3, batch    35 | loss: 4.4939694Losses:  4.811891555786133 0.47040829062461853
CurrentTrain: epoch  3, batch    36 | loss: 5.2823000Losses:  4.260322570800781 0.1781923472881317
CurrentTrain: epoch  3, batch    37 | loss: 4.4385147Losses:  4.150038719177246 0.18363399803638458
CurrentTrain: epoch  3, batch    38 | loss: 4.3336725Losses:  4.597494125366211 0.20179182291030884
CurrentTrain: epoch  3, batch    39 | loss: 4.7992859Losses:  4.193852424621582 0.2339620590209961
CurrentTrain: epoch  3, batch    40 | loss: 4.4278145Losses:  4.175861835479736 0.17574691772460938
CurrentTrain: epoch  3, batch    41 | loss: 4.3516088Losses:  4.70011043548584 0.27165302634239197
CurrentTrain: epoch  3, batch    42 | loss: 4.9717636Losses:  4.2074151039123535 0.15095993876457214
CurrentTrain: epoch  3, batch    43 | loss: 4.3583751Losses:  4.699891090393066 0.31491270661354065
CurrentTrain: epoch  3, batch    44 | loss: 5.0148039Losses:  4.69024658203125 0.23378777503967285
CurrentTrain: epoch  3, batch    45 | loss: 4.9240341Losses:  4.155625343322754 0.18126049637794495
CurrentTrain: epoch  3, batch    46 | loss: 4.3368859Losses:  4.200777053833008 0.17274340987205505
CurrentTrain: epoch  3, batch    47 | loss: 4.3735204Losses:  4.206374168395996 0.16909392178058624
CurrentTrain: epoch  3, batch    48 | loss: 4.3754683Losses:  4.235138416290283 0.1962205469608307
CurrentTrain: epoch  3, batch    49 | loss: 4.4313588Losses:  4.225204944610596 0.22254908084869385
CurrentTrain: epoch  3, batch    50 | loss: 4.4477539Losses:  4.3643927574157715 0.14457327127456665
CurrentTrain: epoch  3, batch    51 | loss: 4.5089660Losses:  4.2827887535095215 0.22547872364521027
CurrentTrain: epoch  3, batch    52 | loss: 4.5082674Losses:  4.681225776672363 0.2911067605018616
CurrentTrain: epoch  3, batch    53 | loss: 4.9723325Losses:  4.20113468170166 0.22986242175102234
CurrentTrain: epoch  3, batch    54 | loss: 4.4309969Losses:  4.276388168334961 0.22017423808574677
CurrentTrain: epoch  3, batch    55 | loss: 4.4965625Losses:  4.192868232727051 0.2266823649406433
CurrentTrain: epoch  3, batch    56 | loss: 4.4195504Losses:  4.294196128845215 0.2419370412826538
CurrentTrain: epoch  3, batch    57 | loss: 4.5361333Losses:  4.248797416687012 0.1520615667104721
CurrentTrain: epoch  3, batch    58 | loss: 4.4008589Losses:  4.206852912902832 0.13087961077690125
CurrentTrain: epoch  3, batch    59 | loss: 4.3377323Losses:  4.303872585296631 0.25487375259399414
CurrentTrain: epoch  3, batch    60 | loss: 4.5587463Losses:  4.094706058502197 0.153689444065094
CurrentTrain: epoch  3, batch    61 | loss: 4.2483954Losses:  4.1075568199157715 0.058502115309238434
CurrentTrain: epoch  3, batch    62 | loss: 4.1660590Losses:  4.134364128112793 0.17295823991298676
CurrentTrain: epoch  4, batch     0 | loss: 4.3073225Losses:  4.164234638214111 0.17896735668182373
CurrentTrain: epoch  4, batch     1 | loss: 4.3432021Losses:  4.155652046203613 0.15666607022285461
CurrentTrain: epoch  4, batch     2 | loss: 4.3123183Losses:  4.039392471313477 0.16798757016658783
CurrentTrain: epoch  4, batch     3 | loss: 4.2073798Losses:  4.205209732055664 0.20992018282413483
CurrentTrain: epoch  4, batch     4 | loss: 4.4151301Losses:  4.180294036865234 0.22358255088329315
CurrentTrain: epoch  4, batch     5 | loss: 4.4038768Losses:  4.244918346405029 0.19359393417835236
CurrentTrain: epoch  4, batch     6 | loss: 4.4385123Losses:  4.195952892303467 0.15615573525428772
CurrentTrain: epoch  4, batch     7 | loss: 4.3521085Losses:  4.1184983253479 0.20393434166908264
CurrentTrain: epoch  4, batch     8 | loss: 4.3224325Losses:  4.163775444030762 0.20344674587249756
CurrentTrain: epoch  4, batch     9 | loss: 4.3672223Losses:  4.144150733947754 0.14927539229393005
CurrentTrain: epoch  4, batch    10 | loss: 4.2934260Losses:  4.335892677307129 0.2967647314071655
CurrentTrain: epoch  4, batch    11 | loss: 4.6326575Losses:  4.196362495422363 0.17204713821411133
CurrentTrain: epoch  4, batch    12 | loss: 4.3684096Losses:  4.308399200439453 0.23259422183036804
CurrentTrain: epoch  4, batch    13 | loss: 4.5409932Losses:  3.998910427093506 0.12461504340171814
CurrentTrain: epoch  4, batch    14 | loss: 4.1235256Losses:  4.1764140129089355 0.12983720004558563
CurrentTrain: epoch  4, batch    15 | loss: 4.3062510Losses:  4.219474792480469 0.211573526263237
CurrentTrain: epoch  4, batch    16 | loss: 4.4310484Losses:  4.107337474822998 0.13582532107830048
CurrentTrain: epoch  4, batch    17 | loss: 4.2431626Losses:  4.187191009521484 0.19577127695083618
CurrentTrain: epoch  4, batch    18 | loss: 4.3829622Losses:  4.129335403442383 0.2295360416173935
CurrentTrain: epoch  4, batch    19 | loss: 4.3588715Losses:  4.503096580505371 0.23799750208854675
CurrentTrain: epoch  4, batch    20 | loss: 4.7410941Losses:  4.304523468017578 0.13814830780029297
CurrentTrain: epoch  4, batch    21 | loss: 4.4426718Losses:  4.043569564819336 0.188726544380188
CurrentTrain: epoch  4, batch    22 | loss: 4.2322960Losses:  4.038178443908691 0.06716024875640869
CurrentTrain: epoch  4, batch    23 | loss: 4.1053386Losses:  4.352801322937012 0.1489356905221939
CurrentTrain: epoch  4, batch    24 | loss: 4.5017371Losses:  4.10969877243042 0.17393919825553894
CurrentTrain: epoch  4, batch    25 | loss: 4.2836380Losses:  4.052850723266602 0.16357654333114624
CurrentTrain: epoch  4, batch    26 | loss: 4.2164273Losses:  4.040331840515137 0.11405745148658752
CurrentTrain: epoch  4, batch    27 | loss: 4.1543894Losses:  4.215616226196289 0.18625855445861816
CurrentTrain: epoch  4, batch    28 | loss: 4.4018745Losses:  4.183255672454834 0.1324934959411621
CurrentTrain: epoch  4, batch    29 | loss: 4.3157492Losses:  4.021382808685303 0.09898031502962112
CurrentTrain: epoch  4, batch    30 | loss: 4.1203632Losses:  4.242502212524414 0.169626846909523
CurrentTrain: epoch  4, batch    31 | loss: 4.4121289Losses:  4.07822322845459 0.20123352110385895
CurrentTrain: epoch  4, batch    32 | loss: 4.2794566Losses:  4.176448822021484 0.22103124856948853
CurrentTrain: epoch  4, batch    33 | loss: 4.3974800Losses:  4.078439712524414 0.0987299308180809
CurrentTrain: epoch  4, batch    34 | loss: 4.1771698Losses:  4.157528877258301 0.17868000268936157
CurrentTrain: epoch  4, batch    35 | loss: 4.3362088Losses:  4.105304718017578 0.19773338735103607
CurrentTrain: epoch  4, batch    36 | loss: 4.3030381Losses:  4.270109176635742 0.20624518394470215
CurrentTrain: epoch  4, batch    37 | loss: 4.4763546Losses:  4.041676998138428 0.12140113115310669
CurrentTrain: epoch  4, batch    38 | loss: 4.1630783Losses:  4.074438095092773 0.1763143539428711
CurrentTrain: epoch  4, batch    39 | loss: 4.2507524Losses:  4.017108917236328 0.11965036392211914
CurrentTrain: epoch  4, batch    40 | loss: 4.1367593Losses:  3.9990344047546387 0.17471930384635925
CurrentTrain: epoch  4, batch    41 | loss: 4.1737537Losses:  4.818212509155273 0.30135852098464966
CurrentTrain: epoch  4, batch    42 | loss: 5.1195712Losses:  4.182182788848877 0.09882020950317383
CurrentTrain: epoch  4, batch    43 | loss: 4.2810030Losses:  4.009467601776123 0.12290921807289124
CurrentTrain: epoch  4, batch    44 | loss: 4.1323767Losses:  4.1946587562561035 0.1748383790254593
CurrentTrain: epoch  4, batch    45 | loss: 4.3694973Losses:  4.274135589599609 0.14706671237945557
CurrentTrain: epoch  4, batch    46 | loss: 4.4212022Losses:  4.164087295532227 0.1600293219089508
CurrentTrain: epoch  4, batch    47 | loss: 4.3241167Losses:  4.132760047912598 0.16453692317008972
CurrentTrain: epoch  4, batch    48 | loss: 4.2972970Losses:  4.050721168518066 0.1348901391029358
CurrentTrain: epoch  4, batch    49 | loss: 4.1856112Losses:  4.186540603637695 0.2183595448732376
CurrentTrain: epoch  4, batch    50 | loss: 4.4049001Losses:  4.051608085632324 0.10185611248016357
CurrentTrain: epoch  4, batch    51 | loss: 4.1534643Losses:  4.029499530792236 0.16628900170326233
CurrentTrain: epoch  4, batch    52 | loss: 4.1957884Losses:  4.1066508293151855 0.1605980098247528
CurrentTrain: epoch  4, batch    53 | loss: 4.2672486Losses:  4.264293193817139 0.1412612348794937
CurrentTrain: epoch  4, batch    54 | loss: 4.4055543Losses:  4.03419303894043 0.17041908204555511
CurrentTrain: epoch  4, batch    55 | loss: 4.2046123Losses:  4.112569332122803 0.11812381446361542
CurrentTrain: epoch  4, batch    56 | loss: 4.2306933Losses:  4.103291034698486 0.1767970621585846
CurrentTrain: epoch  4, batch    57 | loss: 4.2800879Losses:  4.037623405456543 0.11190143972635269
CurrentTrain: epoch  4, batch    58 | loss: 4.1495247Losses:  4.084421634674072 0.10853175818920135
CurrentTrain: epoch  4, batch    59 | loss: 4.1929536Losses:  4.01626443862915 0.1360463798046112
CurrentTrain: epoch  4, batch    60 | loss: 4.1523108Losses:  4.096557140350342 0.13322629034519196
CurrentTrain: epoch  4, batch    61 | loss: 4.2297835Losses:  4.072037696838379 0.059259749948978424
CurrentTrain: epoch  4, batch    62 | loss: 4.1312976Losses:  4.090078353881836 0.17199194431304932
CurrentTrain: epoch  5, batch     0 | loss: 4.2620702Losses:  4.065460205078125 0.17903517186641693
CurrentTrain: epoch  5, batch     1 | loss: 4.2444954Losses:  4.031402587890625 0.10228721797466278
CurrentTrain: epoch  5, batch     2 | loss: 4.1336899Losses:  4.032960891723633 0.11631033569574356
CurrentTrain: epoch  5, batch     3 | loss: 4.1492710Losses:  4.097382068634033 0.11405190825462341
CurrentTrain: epoch  5, batch     4 | loss: 4.2114339Losses:  4.1843180656433105 0.14145660400390625
CurrentTrain: epoch  5, batch     5 | loss: 4.3257747Losses:  4.079758644104004 0.13462913036346436
CurrentTrain: epoch  5, batch     6 | loss: 4.2143879Losses:  3.9909827709198 0.16946406662464142
CurrentTrain: epoch  5, batch     7 | loss: 4.1604466Losses:  4.078203201293945 0.14648643136024475
CurrentTrain: epoch  5, batch     8 | loss: 4.2246895Losses:  4.023472309112549 0.11016426235437393
CurrentTrain: epoch  5, batch     9 | loss: 4.1336365Losses:  4.126411437988281 0.2141587734222412
CurrentTrain: epoch  5, batch    10 | loss: 4.3405704Losses:  4.041116714477539 0.1351049542427063
CurrentTrain: epoch  5, batch    11 | loss: 4.1762218Losses:  4.094026565551758 0.15389877557754517
CurrentTrain: epoch  5, batch    12 | loss: 4.2479253Losses:  4.019533157348633 0.12378983944654465
CurrentTrain: epoch  5, batch    13 | loss: 4.1433229Losses:  3.986989974975586 0.16752594709396362
CurrentTrain: epoch  5, batch    14 | loss: 4.1545157Losses:  4.053755760192871 0.12012014538049698
CurrentTrain: epoch  5, batch    15 | loss: 4.1738758Losses:  4.034440040588379 0.1518276333808899
CurrentTrain: epoch  5, batch    16 | loss: 4.1862679Losses:  4.001931667327881 0.14527657628059387
CurrentTrain: epoch  5, batch    17 | loss: 4.1472082Losses:  4.0584516525268555 0.17496246099472046
CurrentTrain: epoch  5, batch    18 | loss: 4.2334142Losses:  4.04624080657959 0.09912273287773132
CurrentTrain: epoch  5, batch    19 | loss: 4.1453633Losses:  4.0019989013671875 0.11013653129339218
CurrentTrain: epoch  5, batch    20 | loss: 4.1121354Losses:  4.118288040161133 0.13023534417152405
CurrentTrain: epoch  5, batch    21 | loss: 4.2485232Losses:  4.053501605987549 0.1366702765226364
CurrentTrain: epoch  5, batch    22 | loss: 4.1901717Losses:  4.036462783813477 0.1302260160446167
CurrentTrain: epoch  5, batch    23 | loss: 4.1666889Losses:  4.031778335571289 0.0825815424323082
CurrentTrain: epoch  5, batch    24 | loss: 4.1143599Losses:  4.1056623458862305 0.14361277222633362
CurrentTrain: epoch  5, batch    25 | loss: 4.2492752Losses:  4.048773765563965 0.1454024463891983
CurrentTrain: epoch  5, batch    26 | loss: 4.1941762Losses:  4.019258499145508 0.1330479085445404
CurrentTrain: epoch  5, batch    27 | loss: 4.1523066Losses:  4.032564163208008 0.14231616258621216
CurrentTrain: epoch  5, batch    28 | loss: 4.1748805Losses:  4.000175476074219 0.1503782868385315
CurrentTrain: epoch  5, batch    29 | loss: 4.1505537Losses:  3.9886889457702637 0.0813622921705246
CurrentTrain: epoch  5, batch    30 | loss: 4.0700512Losses:  4.03806734085083 0.10544037818908691
CurrentTrain: epoch  5, batch    31 | loss: 4.1435080Losses:  4.058882713317871 0.09452424198389053
CurrentTrain: epoch  5, batch    32 | loss: 4.1534071Losses:  4.000032424926758 0.13316449522972107
CurrentTrain: epoch  5, batch    33 | loss: 4.1331968Losses:  4.098337173461914 0.14028748869895935
CurrentTrain: epoch  5, batch    34 | loss: 4.2386246Losses:  4.665045738220215 0.22816473245620728
CurrentTrain: epoch  5, batch    35 | loss: 4.8932104Losses:  4.020437240600586 0.11859892308712006
CurrentTrain: epoch  5, batch    36 | loss: 4.1390362Losses:  4.001432418823242 0.14004135131835938
CurrentTrain: epoch  5, batch    37 | loss: 4.1414738Losses:  4.011627197265625 0.11730057001113892
CurrentTrain: epoch  5, batch    38 | loss: 4.1289277Losses:  4.028772830963135 0.13375243544578552
CurrentTrain: epoch  5, batch    39 | loss: 4.1625252Losses:  4.0454301834106445 0.13855034112930298
CurrentTrain: epoch  5, batch    40 | loss: 4.1839805Losses:  4.105226516723633 0.11923342943191528
CurrentTrain: epoch  5, batch    41 | loss: 4.2244601Losses:  4.0743207931518555 0.19830414652824402
CurrentTrain: epoch  5, batch    42 | loss: 4.2726250Losses:  4.017843246459961 0.13716652989387512
CurrentTrain: epoch  5, batch    43 | loss: 4.1550097Losses:  4.005113124847412 0.14848794043064117
CurrentTrain: epoch  5, batch    44 | loss: 4.1536012Losses:  3.9841957092285156 0.16570007801055908
CurrentTrain: epoch  5, batch    45 | loss: 4.1498957Losses:  4.17075777053833 0.14481033384799957
CurrentTrain: epoch  5, batch    46 | loss: 4.3155680Losses:  4.0856547355651855 0.15032577514648438
CurrentTrain: epoch  5, batch    47 | loss: 4.2359805Losses:  3.9519565105438232 0.14107048511505127
CurrentTrain: epoch  5, batch    48 | loss: 4.0930271Losses:  4.00135612487793 0.13329066336154938
CurrentTrain: epoch  5, batch    49 | loss: 4.1346469Losses:  4.054671764373779 0.11662653833627701
CurrentTrain: epoch  5, batch    50 | loss: 4.1712985Losses:  4.127401351928711 0.11169259995222092
CurrentTrain: epoch  5, batch    51 | loss: 4.2390938Losses:  4.001417636871338 0.17298159003257751
CurrentTrain: epoch  5, batch    52 | loss: 4.1743994Losses:  4.007752418518066 0.12678144872188568
CurrentTrain: epoch  5, batch    53 | loss: 4.1345339Losses:  3.953578472137451 0.16156914830207825
CurrentTrain: epoch  5, batch    54 | loss: 4.1151476Losses:  4.101743698120117 0.1586436778306961
CurrentTrain: epoch  5, batch    55 | loss: 4.2603874Losses:  3.9909958839416504 0.1430676281452179
CurrentTrain: epoch  5, batch    56 | loss: 4.1340637Losses:  4.030865669250488 0.11606323719024658
CurrentTrain: epoch  5, batch    57 | loss: 4.1469288Losses:  3.983322858810425 0.10995785892009735
CurrentTrain: epoch  5, batch    58 | loss: 4.0932808Losses:  4.15639591217041 0.13317009806632996
CurrentTrain: epoch  5, batch    59 | loss: 4.2895660Losses:  3.9683282375335693 0.09833081066608429
CurrentTrain: epoch  5, batch    60 | loss: 4.0666590Losses:  3.983468770980835 0.10800439119338989
CurrentTrain: epoch  5, batch    61 | loss: 4.0914731Losses:  4.022542953491211 0.07262661308050156
CurrentTrain: epoch  5, batch    62 | loss: 4.0951695Losses:  3.999028444290161 0.13193613290786743
CurrentTrain: epoch  6, batch     0 | loss: 4.1309648Losses:  3.991055965423584 0.10315410792827606
CurrentTrain: epoch  6, batch     1 | loss: 4.0942101Losses:  3.9836504459381104 0.0993950143456459
CurrentTrain: epoch  6, batch     2 | loss: 4.0830455Losses:  4.580589771270752 0.2571694254875183
CurrentTrain: epoch  6, batch     3 | loss: 4.8377590Losses:  4.027299880981445 0.10053859651088715
CurrentTrain: epoch  6, batch     4 | loss: 4.1278386Losses:  3.9810025691986084 0.09755109250545502
CurrentTrain: epoch  6, batch     5 | loss: 4.0785537Losses:  3.9864635467529297 0.13406239449977875
CurrentTrain: epoch  6, batch     6 | loss: 4.1205258Losses:  4.009164333343506 0.10332503169775009
CurrentTrain: epoch  6, batch     7 | loss: 4.1124892Losses:  4.0023417472839355 0.14136353135108948
CurrentTrain: epoch  6, batch     8 | loss: 4.1437054Losses:  4.165192604064941 0.09701570868492126
CurrentTrain: epoch  6, batch     9 | loss: 4.2622085Losses:  3.981801748275757 0.12628713250160217
CurrentTrain: epoch  6, batch    10 | loss: 4.1080890Losses:  4.085451126098633 0.08833838254213333
CurrentTrain: epoch  6, batch    11 | loss: 4.1737895Losses:  4.00970983505249 0.1334860473871231
CurrentTrain: epoch  6, batch    12 | loss: 4.1431961Losses:  3.998593330383301 0.08655854314565659
CurrentTrain: epoch  6, batch    13 | loss: 4.0851517Losses:  4.017426490783691 0.1317509412765503
CurrentTrain: epoch  6, batch    14 | loss: 4.1491776Losses:  3.9721384048461914 0.13450279831886292
CurrentTrain: epoch  6, batch    15 | loss: 4.1066413Losses:  3.9983084201812744 0.1345173567533493
CurrentTrain: epoch  6, batch    16 | loss: 4.1328259Losses:  3.9786744117736816 0.12610214948654175
CurrentTrain: epoch  6, batch    17 | loss: 4.1047764Losses:  4.01115608215332 0.05084868520498276
CurrentTrain: epoch  6, batch    18 | loss: 4.0620046Losses:  3.9657604694366455 0.1390635073184967
CurrentTrain: epoch  6, batch    19 | loss: 4.1048241Losses:  3.973299026489258 0.15332764387130737
CurrentTrain: epoch  6, batch    20 | loss: 4.1266265Losses:  4.028000831604004 0.1172991394996643
CurrentTrain: epoch  6, batch    21 | loss: 4.1452999Losses:  4.04817008972168 0.12127241492271423
CurrentTrain: epoch  6, batch    22 | loss: 4.1694427Losses:  4.002741813659668 0.09901487827301025
CurrentTrain: epoch  6, batch    23 | loss: 4.1017566Losses:  3.984790802001953 0.10753604769706726
CurrentTrain: epoch  6, batch    24 | loss: 4.0923266Losses:  4.032376766204834 0.11060694605112076
CurrentTrain: epoch  6, batch    25 | loss: 4.1429839Losses:  4.003900051116943 0.1265697181224823
CurrentTrain: epoch  6, batch    26 | loss: 4.1304698Losses:  4.006072521209717 0.10901688784360886
CurrentTrain: epoch  6, batch    27 | loss: 4.1150894Losses:  3.9599575996398926 0.129806786775589
CurrentTrain: epoch  6, batch    28 | loss: 4.0897646Losses:  3.9758596420288086 0.12213315069675446
CurrentTrain: epoch  6, batch    29 | loss: 4.0979929Losses:  3.9926085472106934 0.12355490773916245
CurrentTrain: epoch  6, batch    30 | loss: 4.1161633Losses:  3.9842443466186523 0.08062800019979477
CurrentTrain: epoch  6, batch    31 | loss: 4.0648723Losses:  3.9981884956359863 0.08786314725875854
CurrentTrain: epoch  6, batch    32 | loss: 4.0860515Losses:  4.063220024108887 0.14123013615608215
CurrentTrain: epoch  6, batch    33 | loss: 4.2044501Losses:  4.02031135559082 0.14064258337020874
CurrentTrain: epoch  6, batch    34 | loss: 4.1609540Losses:  3.989929437637329 0.0730847418308258
CurrentTrain: epoch  6, batch    35 | loss: 4.0630140Losses:  4.0301947593688965 0.06759551167488098
CurrentTrain: epoch  6, batch    36 | loss: 4.0977902Losses:  3.97003436088562 0.11283192783594131
CurrentTrain: epoch  6, batch    37 | loss: 4.0828662Losses:  4.074338912963867 0.1399134248495102
CurrentTrain: epoch  6, batch    38 | loss: 4.2142525Losses:  3.96636962890625 0.08164335042238235
CurrentTrain: epoch  6, batch    39 | loss: 4.0480132Losses:  3.9823336601257324 0.11249639093875885
CurrentTrain: epoch  6, batch    40 | loss: 4.0948300Losses:  4.005555629730225 0.13525156676769257
CurrentTrain: epoch  6, batch    41 | loss: 4.1408072Losses:  3.993061065673828 0.12482914328575134
CurrentTrain: epoch  6, batch    42 | loss: 4.1178904Losses:  3.928483009338379 0.10766762495040894
CurrentTrain: epoch  6, batch    43 | loss: 4.0361505Losses:  3.968759059906006 0.09984584152698517
CurrentTrain: epoch  6, batch    44 | loss: 4.0686049Losses:  3.9727747440338135 0.0976252406835556
CurrentTrain: epoch  6, batch    45 | loss: 4.0703998Losses:  4.003193378448486 0.11988028138875961
CurrentTrain: epoch  6, batch    46 | loss: 4.1230736Losses:  3.975796699523926 0.09084911644458771
CurrentTrain: epoch  6, batch    47 | loss: 4.0666456Losses:  3.9584624767303467 0.12299443036317825
CurrentTrain: epoch  6, batch    48 | loss: 4.0814571Losses:  3.973231792449951 0.06678430736064911
CurrentTrain: epoch  6, batch    49 | loss: 4.0400162Losses:  3.9819231033325195 0.09686513990163803
CurrentTrain: epoch  6, batch    50 | loss: 4.0787883Losses:  3.9539003372192383 0.09777391701936722
CurrentTrain: epoch  6, batch    51 | loss: 4.0516744Losses:  3.975973606109619 0.09806719422340393
CurrentTrain: epoch  6, batch    52 | loss: 4.0740409Losses:  3.9667088985443115 0.04603906348347664
CurrentTrain: epoch  6, batch    53 | loss: 4.0127478Losses:  3.9581222534179688 0.10650409013032913
CurrentTrain: epoch  6, batch    54 | loss: 4.0646262Losses:  3.9477386474609375 0.10922081768512726
CurrentTrain: epoch  6, batch    55 | loss: 4.0569596Losses:  4.012162685394287 0.07349028438329697
CurrentTrain: epoch  6, batch    56 | loss: 4.0856528Losses:  3.977226734161377 0.08889316022396088
CurrentTrain: epoch  6, batch    57 | loss: 4.0661197Losses:  3.971529960632324 0.10279793292284012
CurrentTrain: epoch  6, batch    58 | loss: 4.0743279Losses:  3.9862687587738037 0.12601196765899658
CurrentTrain: epoch  6, batch    59 | loss: 4.1122808Losses:  4.042815208435059 0.0791582316160202
CurrentTrain: epoch  6, batch    60 | loss: 4.1219735Losses:  3.9789226055145264 0.10117073357105255
CurrentTrain: epoch  6, batch    61 | loss: 4.0800934Losses:  3.9545841217041016 0.11748834699392319
CurrentTrain: epoch  6, batch    62 | loss: 4.0720725Losses:  4.028253555297852 0.07716480642557144
CurrentTrain: epoch  7, batch     0 | loss: 4.1054182Losses:  4.039241790771484 0.08958272635936737
CurrentTrain: epoch  7, batch     1 | loss: 4.1288247Losses:  3.96103572845459 0.09382382780313492
CurrentTrain: epoch  7, batch     2 | loss: 4.0548596Losses:  3.9690918922424316 0.09919387102127075
CurrentTrain: epoch  7, batch     3 | loss: 4.0682859Losses:  3.958112955093384 0.08954939246177673
CurrentTrain: epoch  7, batch     4 | loss: 4.0476623Losses:  3.9521729946136475 0.07701686024665833
CurrentTrain: epoch  7, batch     5 | loss: 4.0291901Losses:  3.970945358276367 0.09197375178337097
CurrentTrain: epoch  7, batch     6 | loss: 4.0629191Losses:  3.998551368713379 0.11974331736564636
CurrentTrain: epoch  7, batch     7 | loss: 4.1182947Losses:  3.9250893592834473 0.06535114347934723
CurrentTrain: epoch  7, batch     8 | loss: 3.9904406Losses:  3.937119483947754 0.120280921459198
CurrentTrain: epoch  7, batch     9 | loss: 4.0574002Losses:  3.9354798793792725 0.12470707297325134
CurrentTrain: epoch  7, batch    10 | loss: 4.0601869Losses:  3.918367862701416 0.08485352247953415
CurrentTrain: epoch  7, batch    11 | loss: 4.0032215Losses:  3.9296553134918213 0.1002546027302742
CurrentTrain: epoch  7, batch    12 | loss: 4.0299101Losses:  3.9861793518066406 0.07454520463943481
CurrentTrain: epoch  7, batch    13 | loss: 4.0607247Losses:  3.932767868041992 0.10172416269779205
CurrentTrain: epoch  7, batch    14 | loss: 4.0344920Losses:  3.897843837738037 0.12703271210193634
CurrentTrain: epoch  7, batch    15 | loss: 4.0248766Losses:  4.039129257202148 0.13369128108024597
CurrentTrain: epoch  7, batch    16 | loss: 4.1728206Losses:  4.097264289855957 0.20676663517951965
CurrentTrain: epoch  7, batch    17 | loss: 4.3040309Losses:  3.953615665435791 0.08763706684112549
CurrentTrain: epoch  7, batch    18 | loss: 4.0412526Losses:  4.011212348937988 0.09670431911945343
CurrentTrain: epoch  7, batch    19 | loss: 4.1079168Losses:  4.008914470672607 0.06555426865816116
CurrentTrain: epoch  7, batch    20 | loss: 4.0744686Losses:  3.935729503631592 0.08740634471178055
CurrentTrain: epoch  7, batch    21 | loss: 4.0231357Losses:  3.9475293159484863 0.08481641113758087
CurrentTrain: epoch  7, batch    22 | loss: 4.0323458Losses:  3.9618237018585205 0.08328350633382797
CurrentTrain: epoch  7, batch    23 | loss: 4.0451074Losses:  3.9590883255004883 0.11808603256940842
CurrentTrain: epoch  7, batch    24 | loss: 4.0771742Losses:  4.041887283325195 0.0928235873579979
CurrentTrain: epoch  7, batch    25 | loss: 4.1347108Losses:  3.9425947666168213 0.06934528052806854
CurrentTrain: epoch  7, batch    26 | loss: 4.0119400Losses:  3.9594833850860596 0.0892261266708374
CurrentTrain: epoch  7, batch    27 | loss: 4.0487094Losses:  3.9639792442321777 0.1024603620171547
CurrentTrain: epoch  7, batch    28 | loss: 4.0664396Losses:  3.9740099906921387 0.06359143555164337
CurrentTrain: epoch  7, batch    29 | loss: 4.0376015Losses:  3.9896512031555176 0.0782276839017868
CurrentTrain: epoch  7, batch    30 | loss: 4.0678787Losses:  3.9664621353149414 0.08784395456314087
CurrentTrain: epoch  7, batch    31 | loss: 4.0543060Losses:  3.9784202575683594 0.057081304490566254
CurrentTrain: epoch  7, batch    32 | loss: 4.0355015Losses:  4.0468034744262695 0.08588332682847977
CurrentTrain: epoch  7, batch    33 | loss: 4.1326866Losses:  3.9993996620178223 0.07017265260219574
CurrentTrain: epoch  7, batch    34 | loss: 4.0695724Losses:  3.9419379234313965 0.08828926086425781
CurrentTrain: epoch  7, batch    35 | loss: 4.0302272Losses:  4.032753944396973 0.09097201377153397
CurrentTrain: epoch  7, batch    36 | loss: 4.1237259Losses:  3.9314589500427246 0.07418845593929291
CurrentTrain: epoch  7, batch    37 | loss: 4.0056472Losses:  3.9514174461364746 0.09187942743301392
CurrentTrain: epoch  7, batch    38 | loss: 4.0432968Losses:  3.935667037963867 0.07193250209093094
CurrentTrain: epoch  7, batch    39 | loss: 4.0075994Losses:  3.954148769378662 0.11006136238574982
CurrentTrain: epoch  7, batch    40 | loss: 4.0642099Losses:  3.941279411315918 0.08034583926200867
CurrentTrain: epoch  7, batch    41 | loss: 4.0216250Losses:  3.938103675842285 0.08002956956624985
CurrentTrain: epoch  7, batch    42 | loss: 4.0181332Losses:  3.9506561756134033 0.08781313896179199
CurrentTrain: epoch  7, batch    43 | loss: 4.0384693Losses:  3.954401731491089 0.08717701584100723
CurrentTrain: epoch  7, batch    44 | loss: 4.0415788Losses:  3.912904739379883 0.07730670273303986
CurrentTrain: epoch  7, batch    45 | loss: 3.9902115Losses:  3.9585466384887695 0.0765320435166359
CurrentTrain: epoch  7, batch    46 | loss: 4.0350785Losses:  3.9549098014831543 0.08812041580677032
CurrentTrain: epoch  7, batch    47 | loss: 4.0430303Losses:  3.946563243865967 0.11995363235473633
CurrentTrain: epoch  7, batch    48 | loss: 4.0665169Losses:  3.9241280555725098 0.117898128926754
CurrentTrain: epoch  7, batch    49 | loss: 4.0420260Losses:  3.902482032775879 0.0723131075501442
CurrentTrain: epoch  7, batch    50 | loss: 3.9747951Losses:  3.9400010108947754 0.0762823149561882
CurrentTrain: epoch  7, batch    51 | loss: 4.0162835Losses:  4.000263690948486 0.11851683259010315
CurrentTrain: epoch  7, batch    52 | loss: 4.1187806Losses:  3.985063076019287 0.08932811766862869
CurrentTrain: epoch  7, batch    53 | loss: 4.0743914Losses:  3.9741172790527344 0.08039537072181702
CurrentTrain: epoch  7, batch    54 | loss: 4.0545125Losses:  3.9582583904266357 0.08115878701210022
CurrentTrain: epoch  7, batch    55 | loss: 4.0394173Losses:  3.952629804611206 0.06743122637271881
CurrentTrain: epoch  7, batch    56 | loss: 4.0200610Losses:  3.9337034225463867 0.09232257306575775
CurrentTrain: epoch  7, batch    57 | loss: 4.0260258Losses:  3.9267477989196777 0.11974488198757172
CurrentTrain: epoch  7, batch    58 | loss: 4.0464926Losses:  3.9836220741271973 0.05905849486589432
CurrentTrain: epoch  7, batch    59 | loss: 4.0426807Losses:  3.9735257625579834 0.09784751385450363
CurrentTrain: epoch  7, batch    60 | loss: 4.0713735Losses:  3.9554061889648438 0.06706628948450089
CurrentTrain: epoch  7, batch    61 | loss: 4.0224724Losses:  4.003162384033203 0.04869668930768967
CurrentTrain: epoch  7, batch    62 | loss: 4.0518589Losses:  3.9666149616241455 0.10367098450660706
CurrentTrain: epoch  8, batch     0 | loss: 4.0702858Losses:  3.986738681793213 0.09773115813732147
CurrentTrain: epoch  8, batch     1 | loss: 4.0844698Losses:  3.944037675857544 0.1025252640247345
CurrentTrain: epoch  8, batch     2 | loss: 4.0465631Losses:  3.977097511291504 0.05374637618660927
CurrentTrain: epoch  8, batch     3 | loss: 4.0308437Losses:  3.913616418838501 0.07056476175785065
CurrentTrain: epoch  8, batch     4 | loss: 3.9841812Losses:  3.9463047981262207 0.08724537491798401
CurrentTrain: epoch  8, batch     5 | loss: 4.0335503Losses:  3.9600634574890137 0.0775601863861084
CurrentTrain: epoch  8, batch     6 | loss: 4.0376234Losses:  3.9754061698913574 0.07222090661525726
CurrentTrain: epoch  8, batch     7 | loss: 4.0476270Losses:  3.95345139503479 0.08303657174110413
CurrentTrain: epoch  8, batch     8 | loss: 4.0364881Losses:  3.9559879302978516 0.08874963223934174
CurrentTrain: epoch  8, batch     9 | loss: 4.0447373Losses:  3.9172987937927246 0.06738539040088654
CurrentTrain: epoch  8, batch    10 | loss: 3.9846842Losses:  3.931035280227661 0.08411756157875061
CurrentTrain: epoch  8, batch    11 | loss: 4.0151529Losses:  3.9627127647399902 0.0765727162361145
CurrentTrain: epoch  8, batch    12 | loss: 4.0392857Losses:  3.951533555984497 0.06872254610061646
CurrentTrain: epoch  8, batch    13 | loss: 4.0202560Losses:  3.9124555587768555 0.10342177748680115
CurrentTrain: epoch  8, batch    14 | loss: 4.0158772Losses:  3.934828281402588 0.08614105731248856
CurrentTrain: epoch  8, batch    15 | loss: 4.0209694Losses:  3.9241833686828613 0.05552676320075989
CurrentTrain: epoch  8, batch    16 | loss: 3.9797101Losses:  3.9599523544311523 0.085051991045475
CurrentTrain: epoch  8, batch    17 | loss: 4.0450044Losses:  3.948591709136963 0.07930653542280197
CurrentTrain: epoch  8, batch    18 | loss: 4.0278983Losses:  3.971329689025879 0.07360168546438217
CurrentTrain: epoch  8, batch    19 | loss: 4.0449314Losses:  3.878913164138794 0.08327733725309372
CurrentTrain: epoch  8, batch    20 | loss: 3.9621904Losses:  3.9330270290374756 0.077147476375103
CurrentTrain: epoch  8, batch    21 | loss: 4.0101743Losses:  3.9417080879211426 0.08360269665718079
CurrentTrain: epoch  8, batch    22 | loss: 4.0253110Losses:  3.957864999771118 0.10004173219203949
CurrentTrain: epoch  8, batch    23 | loss: 4.0579066Losses:  3.9609341621398926 0.10498949885368347
CurrentTrain: epoch  8, batch    24 | loss: 4.0659237Losses:  3.9327545166015625 0.05657561868429184
CurrentTrain: epoch  8, batch    25 | loss: 3.9893301Losses:  3.952007293701172 0.0827626883983612
CurrentTrain: epoch  8, batch    26 | loss: 4.0347700Losses:  3.9931411743164062 0.09134802222251892
CurrentTrain: epoch  8, batch    27 | loss: 4.0844893Losses:  3.9441750049591064 0.10539896041154861
CurrentTrain: epoch  8, batch    28 | loss: 4.0495739Losses:  3.9535651206970215 0.08291950076818466
CurrentTrain: epoch  8, batch    29 | loss: 4.0364847Losses:  3.925391674041748 0.08432331681251526
CurrentTrain: epoch  8, batch    30 | loss: 4.0097151Losses:  3.947849750518799 0.08146685361862183
CurrentTrain: epoch  8, batch    31 | loss: 4.0293164Losses:  3.976320266723633 0.0783039778470993
CurrentTrain: epoch  8, batch    32 | loss: 4.0546241Losses:  3.9353036880493164 0.07142657786607742
CurrentTrain: epoch  8, batch    33 | loss: 4.0067301Losses:  3.9510302543640137 0.09487949311733246
CurrentTrain: epoch  8, batch    34 | loss: 4.0459099Losses:  3.9414587020874023 0.08813268691301346
CurrentTrain: epoch  8, batch    35 | loss: 4.0295916Losses:  3.9477527141571045 0.06302961707115173
CurrentTrain: epoch  8, batch    36 | loss: 4.0107822Losses:  3.9763104915618896 0.08295181393623352
CurrentTrain: epoch  8, batch    37 | loss: 4.0592623Losses:  3.9126229286193848 0.08856917917728424
CurrentTrain: epoch  8, batch    38 | loss: 4.0011921Losses:  3.982046127319336 0.0915098786354065
CurrentTrain: epoch  8, batch    39 | loss: 4.0735559Losses:  4.007719993591309 0.10523439943790436
CurrentTrain: epoch  8, batch    40 | loss: 4.1129546Losses:  3.9612369537353516 0.07850901782512665
CurrentTrain: epoch  8, batch    41 | loss: 4.0397458Losses:  3.9670488834381104 0.08587007224559784
CurrentTrain: epoch  8, batch    42 | loss: 4.0529189Losses:  3.9701459407806396 0.09656185656785965
CurrentTrain: epoch  8, batch    43 | loss: 4.0667076Losses:  3.9680075645446777 0.08687418699264526
CurrentTrain: epoch  8, batch    44 | loss: 4.0548816Losses:  4.005409240722656 0.05695801228284836
CurrentTrain: epoch  8, batch    45 | loss: 4.0623674Losses:  3.9143669605255127 0.0685231164097786
CurrentTrain: epoch  8, batch    46 | loss: 3.9828901Losses:  3.966244697570801 0.08857578039169312
CurrentTrain: epoch  8, batch    47 | loss: 4.0548205Losses:  3.9807724952697754 0.06234099715948105
CurrentTrain: epoch  8, batch    48 | loss: 4.0431137Losses:  3.9413557052612305 0.06652442365884781
CurrentTrain: epoch  8, batch    49 | loss: 4.0078802Losses:  3.9431748390197754 0.06320977210998535
CurrentTrain: epoch  8, batch    50 | loss: 4.0063848Losses:  3.9620699882507324 0.068123459815979
CurrentTrain: epoch  8, batch    51 | loss: 4.0301933Losses:  3.9597830772399902 0.11728917062282562
CurrentTrain: epoch  8, batch    52 | loss: 4.0770721Losses:  3.971676826477051 0.08502550423145294
CurrentTrain: epoch  8, batch    53 | loss: 4.0567021Losses:  3.9328794479370117 0.08934664726257324
CurrentTrain: epoch  8, batch    54 | loss: 4.0222263Losses:  3.9420316219329834 0.07444584369659424
CurrentTrain: epoch  8, batch    55 | loss: 4.0164776Losses:  3.953568935394287 0.08183996379375458
CurrentTrain: epoch  8, batch    56 | loss: 4.0354090Losses:  3.9438977241516113 0.09294262528419495
CurrentTrain: epoch  8, batch    57 | loss: 4.0368404Losses:  3.978865623474121 0.08024273812770844
CurrentTrain: epoch  8, batch    58 | loss: 4.0591083Losses:  3.9690866470336914 0.09454069286584854
CurrentTrain: epoch  8, batch    59 | loss: 4.0636272Losses:  3.9067344665527344 0.101915642619133
CurrentTrain: epoch  8, batch    60 | loss: 4.0086503Losses:  3.9598617553710938 0.0907767042517662
CurrentTrain: epoch  8, batch    61 | loss: 4.0506387Losses:  3.9272522926330566 0.023521549999713898
CurrentTrain: epoch  8, batch    62 | loss: 3.9507740Losses:  3.9368221759796143 0.059288930147886276
CurrentTrain: epoch  9, batch     0 | loss: 3.9961112Losses:  3.9861936569213867 0.06727610528469086
CurrentTrain: epoch  9, batch     1 | loss: 4.0534697Losses:  3.969501256942749 0.07364995777606964
CurrentTrain: epoch  9, batch     2 | loss: 4.0431514Losses:  3.9941790103912354 0.06769701838493347
CurrentTrain: epoch  9, batch     3 | loss: 4.0618758Losses:  3.942037582397461 0.0531308576464653
CurrentTrain: epoch  9, batch     4 | loss: 3.9951684Losses:  3.9465887546539307 0.06229330599308014
CurrentTrain: epoch  9, batch     5 | loss: 4.0088820Losses:  3.952568531036377 0.07636412978172302
CurrentTrain: epoch  9, batch     6 | loss: 4.0289326Losses:  3.9387030601501465 0.06433694064617157
CurrentTrain: epoch  9, batch     7 | loss: 4.0030398Losses:  3.914552688598633 0.06162241846323013
CurrentTrain: epoch  9, batch     8 | loss: 3.9761751Losses:  3.942791700363159 0.08790320158004761
CurrentTrain: epoch  9, batch     9 | loss: 4.0306950Losses:  3.946256637573242 0.08667431026697159
CurrentTrain: epoch  9, batch    10 | loss: 4.0329309Losses:  3.9717857837677 0.09356085956096649
CurrentTrain: epoch  9, batch    11 | loss: 4.0653467Losses:  3.9494123458862305 0.08560235798358917
CurrentTrain: epoch  9, batch    12 | loss: 4.0350146Losses:  3.9538722038269043 0.07900501787662506
CurrentTrain: epoch  9, batch    13 | loss: 4.0328774Losses:  3.9551644325256348 0.10628809779882431
CurrentTrain: epoch  9, batch    14 | loss: 4.0614524Losses:  3.9697208404541016 0.08575886487960815
CurrentTrain: epoch  9, batch    15 | loss: 4.0554795Losses:  3.939419746398926 0.0871029794216156
CurrentTrain: epoch  9, batch    16 | loss: 4.0265226Losses:  3.9747211933135986 0.08701634407043457
CurrentTrain: epoch  9, batch    17 | loss: 4.0617375Losses:  3.943171501159668 0.06291449069976807
CurrentTrain: epoch  9, batch    18 | loss: 4.0060859Losses:  3.8861262798309326 0.03852303698658943
CurrentTrain: epoch  9, batch    19 | loss: 3.9246492Losses:  3.980175018310547 0.0672687441110611
CurrentTrain: epoch  9, batch    20 | loss: 4.0474439Losses:  3.93094801902771 0.07286322116851807
CurrentTrain: epoch  9, batch    21 | loss: 4.0038114Losses:  3.9834277629852295 0.08173766732215881
CurrentTrain: epoch  9, batch    22 | loss: 4.0651655Losses:  3.93264102935791 0.06968880444765091
CurrentTrain: epoch  9, batch    23 | loss: 4.0023298Losses:  3.951967477798462 0.05878511816263199
CurrentTrain: epoch  9, batch    24 | loss: 4.0107527Losses:  3.967630386352539 0.06924150884151459
CurrentTrain: epoch  9, batch    25 | loss: 4.0368719Losses:  3.9198837280273438 0.09009000658988953
CurrentTrain: epoch  9, batch    26 | loss: 4.0099735Losses:  3.9371228218078613 0.06893918663263321
CurrentTrain: epoch  9, batch    27 | loss: 4.0060620Losses:  3.9372048377990723 0.09418594837188721
CurrentTrain: epoch  9, batch    28 | loss: 4.0313907Losses:  3.9198546409606934 0.074643075466156
CurrentTrain: epoch  9, batch    29 | loss: 3.9944978Losses:  3.9565324783325195 0.08842655271291733
CurrentTrain: epoch  9, batch    30 | loss: 4.0449591Losses:  3.9288949966430664 0.09963305294513702
CurrentTrain: epoch  9, batch    31 | loss: 4.0285282Losses:  3.903580665588379 0.06498263031244278
CurrentTrain: epoch  9, batch    32 | loss: 3.9685633Losses:  3.908461570739746 0.03595030680298805
CurrentTrain: epoch  9, batch    33 | loss: 3.9444120Losses:  3.935201644897461 0.08438730239868164
CurrentTrain: epoch  9, batch    34 | loss: 4.0195889Losses:  3.932827949523926 0.10112465918064117
CurrentTrain: epoch  9, batch    35 | loss: 4.0339527Losses:  3.9286446571350098 0.043660812079906464
CurrentTrain: epoch  9, batch    36 | loss: 3.9723055Losses:  3.9210073947906494 0.0780472680926323
CurrentTrain: epoch  9, batch    37 | loss: 3.9990547Losses:  3.9815003871917725 0.05565497651696205
CurrentTrain: epoch  9, batch    38 | loss: 4.0371552Losses:  3.938249111175537 0.09163491427898407
CurrentTrain: epoch  9, batch    39 | loss: 4.0298839Losses:  4.018310546875 0.06173859164118767
CurrentTrain: epoch  9, batch    40 | loss: 4.0800490Losses:  3.9278202056884766 0.04545338451862335
CurrentTrain: epoch  9, batch    41 | loss: 3.9732735Losses:  3.9547085762023926 0.04371277242898941
CurrentTrain: epoch  9, batch    42 | loss: 3.9984214Losses:  3.935654878616333 0.04295770823955536
CurrentTrain: epoch  9, batch    43 | loss: 3.9786127Losses:  3.9461967945098877 0.06244019418954849
CurrentTrain: epoch  9, batch    44 | loss: 4.0086370Losses:  3.9185264110565186 0.07303936034440994
CurrentTrain: epoch  9, batch    45 | loss: 3.9915657Losses:  3.9206109046936035 0.05036034435033798
CurrentTrain: epoch  9, batch    46 | loss: 3.9709713Losses:  3.963383674621582 0.05774093419313431
CurrentTrain: epoch  9, batch    47 | loss: 4.0211248Losses:  3.9366538524627686 0.0646737739443779
CurrentTrain: epoch  9, batch    48 | loss: 4.0013275Losses:  4.0036821365356445 0.09333771467208862
CurrentTrain: epoch  9, batch    49 | loss: 4.0970197Losses:  3.949634552001953 0.08450546860694885
CurrentTrain: epoch  9, batch    50 | loss: 4.0341401Losses:  3.9613122940063477 0.07992298901081085
CurrentTrain: epoch  9, batch    51 | loss: 4.0412354Losses:  3.944835662841797 0.05507547780871391
CurrentTrain: epoch  9, batch    52 | loss: 3.9999111Losses:  3.9201159477233887 0.08108861744403839
CurrentTrain: epoch  9, batch    53 | loss: 4.0012045Losses:  3.942092180252075 0.0714249238371849
CurrentTrain: epoch  9, batch    54 | loss: 4.0135169Losses:  3.929266929626465 0.05349760502576828
CurrentTrain: epoch  9, batch    55 | loss: 3.9827645Losses:  3.902273654937744 0.07593309879302979
CurrentTrain: epoch  9, batch    56 | loss: 3.9782066Losses:  3.9434444904327393 0.08790651708841324
CurrentTrain: epoch  9, batch    57 | loss: 4.0313511Losses:  3.903947353363037 0.05568624287843704
CurrentTrain: epoch  9, batch    58 | loss: 3.9596336Losses:  3.9118540287017822 0.07931164652109146
CurrentTrain: epoch  9, batch    59 | loss: 3.9911656Losses:  3.9534780979156494 0.08039052039384842
CurrentTrain: epoch  9, batch    60 | loss: 4.0338688Losses:  3.936772346496582 0.07296144962310791
CurrentTrain: epoch  9, batch    61 | loss: 4.0097337Losses:  3.9416496753692627 0.03346449136734009
CurrentTrain: epoch  9, batch    62 | loss: 3.9751141
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [0 4 6 0 0 0 0 0 8 1 0 0 0 7 0 1 0 3 5 2]
Losses:  8.193069458007812 1.2317427396774292
CurrentTrain: epoch  0, batch     0 | loss: 9.4248123Losses:  10.721698760986328 1.0570486783981323
CurrentTrain: epoch  0, batch     1 | loss: 11.7787476Losses:  7.419323921203613 1.2952700853347778
CurrentTrain: epoch  0, batch     2 | loss: 8.7145939Losses:  8.041659355163574 1.4901162614933128e-07
CurrentTrain: epoch  0, batch     3 | loss: 8.0416594Losses:  3.9666852951049805 1.1636145114898682
CurrentTrain: epoch  1, batch     0 | loss: 5.1302996Losses:  4.052902698516846 1.1785463094711304
CurrentTrain: epoch  1, batch     1 | loss: 5.2314491Losses:  4.0682549476623535 1.172304391860962
CurrentTrain: epoch  1, batch     2 | loss: 5.2405596Losses:  2.371265172958374 0.2771257162094116
CurrentTrain: epoch  1, batch     3 | loss: 2.6483908Losses:  3.865511417388916 0.9760562777519226
CurrentTrain: epoch  2, batch     0 | loss: 4.8415675Losses:  3.7566018104553223 1.0161447525024414
CurrentTrain: epoch  2, batch     1 | loss: 4.7727466Losses:  3.0975987911224365 0.8920754194259644
CurrentTrain: epoch  2, batch     2 | loss: 3.9896741Losses:  3.697307586669922 0.11878921836614609
CurrentTrain: epoch  2, batch     3 | loss: 3.8160968Losses:  3.60282564163208 0.9481508731842041
CurrentTrain: epoch  3, batch     0 | loss: 4.5509768Losses:  2.9444491863250732 0.8518415093421936
CurrentTrain: epoch  3, batch     1 | loss: 3.7962906Losses:  3.2233409881591797 1.0833444595336914
CurrentTrain: epoch  3, batch     2 | loss: 4.3066854Losses:  2.2674551010131836 0.29675379395484924
CurrentTrain: epoch  3, batch     3 | loss: 2.5642090Losses:  2.9897053241729736 0.75461345911026
CurrentTrain: epoch  4, batch     0 | loss: 3.7443187Losses:  2.6874847412109375 0.7511050701141357
CurrentTrain: epoch  4, batch     1 | loss: 3.4385898Losses:  3.2006683349609375 0.8293899297714233
CurrentTrain: epoch  4, batch     2 | loss: 4.0300584Losses:  2.0843002796173096 0.03203562647104263
CurrentTrain: epoch  4, batch     3 | loss: 2.1163359Losses:  3.0307037830352783 0.7280527353286743
CurrentTrain: epoch  5, batch     0 | loss: 3.7587566Losses:  2.330811023712158 0.712693989276886
CurrentTrain: epoch  5, batch     1 | loss: 3.0435050Losses:  2.436159610748291 0.8646554350852966
CurrentTrain: epoch  5, batch     2 | loss: 3.3008151Losses:  3.6198668479919434 0.04558737576007843
CurrentTrain: epoch  5, batch     3 | loss: 3.6654541Losses:  2.486372947692871 0.7017043232917786
CurrentTrain: epoch  6, batch     0 | loss: 3.1880772Losses:  2.3794355392456055 0.6402586698532104
CurrentTrain: epoch  6, batch     1 | loss: 3.0196943Losses:  2.544901132583618 0.7972698211669922
CurrentTrain: epoch  6, batch     2 | loss: 3.3421710Losses:  2.318908452987671 5.960464477539063e-08
CurrentTrain: epoch  6, batch     3 | loss: 2.3189085Losses:  2.508556604385376 0.6916358470916748
CurrentTrain: epoch  7, batch     0 | loss: 3.2001925Losses:  2.1334280967712402 0.767731785774231
CurrentTrain: epoch  7, batch     1 | loss: 2.9011598Losses:  2.175652027130127 0.577521800994873
CurrentTrain: epoch  7, batch     2 | loss: 2.7531738Losses:  1.9452955722808838 5.960464477539063e-08
CurrentTrain: epoch  7, batch     3 | loss: 1.9452956Losses:  2.1113297939300537 0.6610674262046814
CurrentTrain: epoch  8, batch     0 | loss: 2.7723973Losses:  2.1830644607543945 0.5668314695358276
CurrentTrain: epoch  8, batch     1 | loss: 2.7498960Losses:  2.2731409072875977 0.6447134613990784
CurrentTrain: epoch  8, batch     2 | loss: 2.9178543Losses:  1.7313578128814697 0.043987102806568146
CurrentTrain: epoch  8, batch     3 | loss: 1.7753450Losses:  1.9911041259765625 0.6691057085990906
CurrentTrain: epoch  9, batch     0 | loss: 2.6602099Losses:  2.1830873489379883 0.5030026435852051
CurrentTrain: epoch  9, batch     1 | loss: 2.6860900Losses:  1.8748587369918823 0.5812182426452637
CurrentTrain: epoch  9, batch     2 | loss: 2.4560771Losses:  1.7079800367355347 0.040251411497592926
CurrentTrain: epoch  9, batch     3 | loss: 1.7482314
Losses:  6.21087121963501 0.869616687297821
MemoryTrain:  epoch  0, batch     0 | loss: 7.0804877Losses:  9.779559135437012 0.06110478937625885
MemoryTrain:  epoch  0, batch     1 | loss: 9.8406639Losses:  1.271084189414978 0.7348755598068237
MemoryTrain:  epoch  1, batch     0 | loss: 2.0059597Losses:  2.7155847549438477 0.497123658657074
MemoryTrain:  epoch  1, batch     1 | loss: 3.2127085Losses:  1.408904790878296 0.6918771862983704
MemoryTrain:  epoch  2, batch     0 | loss: 2.1007819Losses:  0.3905360698699951 0.24618718028068542
MemoryTrain:  epoch  2, batch     1 | loss: 0.6367233Losses:  1.0371085405349731 0.6537685990333557
MemoryTrain:  epoch  3, batch     0 | loss: 1.6908772Losses:  0.8205093145370483 0.2746760845184326
MemoryTrain:  epoch  3, batch     1 | loss: 1.0951854Losses:  0.9534547924995422 0.8188135027885437
MemoryTrain:  epoch  4, batch     0 | loss: 1.7722683Losses:  0.12922517955303192 0.17436549067497253
MemoryTrain:  epoch  4, batch     1 | loss: 0.3035907Losses:  0.7092676162719727 0.7909432053565979
MemoryTrain:  epoch  5, batch     0 | loss: 1.5002108Losses:  0.1926223784685135 0.06153732165694237
MemoryTrain:  epoch  5, batch     1 | loss: 0.2541597Losses:  0.6080780029296875 0.758324384689331
MemoryTrain:  epoch  6, batch     0 | loss: 1.3664024Losses:  0.13047051429748535 0.1724327802658081
MemoryTrain:  epoch  6, batch     1 | loss: 0.3029033Losses:  0.39805686473846436 0.7252659201622009
MemoryTrain:  epoch  7, batch     0 | loss: 1.1233227Losses:  0.7049574255943298 0.11403749883174896
MemoryTrain:  epoch  7, batch     1 | loss: 0.8189949Losses:  0.44602328538894653 0.6930488348007202
MemoryTrain:  epoch  8, batch     0 | loss: 1.1390722Losses:  0.14659762382507324 0.29858630895614624
MemoryTrain:  epoch  8, batch     1 | loss: 0.4451839Losses:  0.3642244040966034 0.7138323783874512
MemoryTrain:  epoch  9, batch     0 | loss: 1.0780568Losses:  0.2255563735961914 0.08725343644618988
MemoryTrain:  epoch  9, batch     1 | loss: 0.3128098
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 85.76%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.51%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.14%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.73%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.71%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.52%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 94.18%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 93.85%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.65%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.65%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 93.55%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 93.65%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.84%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 93.66%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 93.66%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 93.40%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 93.15%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 92.82%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.76%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 92.47%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 92.25%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 91.88%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 91.84%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 91.49%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 91.29%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 91.10%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 90.84%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 90.45%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 90.20%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 89.96%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 89.51%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 89.42%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 89.06%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 88.51%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 87.90%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 87.57%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 87.17%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 86.92%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 86.48%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 86.24%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 85.62%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 85.21%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 84.93%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 84.47%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 84.25%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.05%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 83.79%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 83.35%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.75%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 82.22%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 81.65%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.14%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.86%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.64%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 81.30%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 81.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 81.46%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 82.15%   
cur_acc:  ['0.9484', '0.7014']
his_acc:  ['0.9484', '0.8215']
Clustering into  14  clusters
Clusters:  [ 0  7  8  0  0  0  0  0 10  1  0  0  0 11  0  1  0 13 12  6  0  5  2  0
  0  9  0  4  0  3]
Losses:  7.361885070800781 1.6400278806686401
CurrentTrain: epoch  0, batch     0 | loss: 9.0019131Losses:  9.346559524536133 1.6050140857696533
CurrentTrain: epoch  0, batch     1 | loss: 10.9515734Losses:  6.944430351257324 1.7627547979354858
CurrentTrain: epoch  0, batch     2 | loss: 8.7071848Losses:  6.286916732788086 0.4415435194969177
CurrentTrain: epoch  0, batch     3 | loss: 6.7284603Losses:  3.6271557807922363 1.5517265796661377
CurrentTrain: epoch  1, batch     0 | loss: 5.1788826Losses:  4.488429069519043 1.5952588319778442
CurrentTrain: epoch  1, batch     1 | loss: 6.0836878Losses:  3.4140357971191406 1.4472380876541138
CurrentTrain: epoch  1, batch     2 | loss: 4.8612738Losses:  4.127897262573242 0.24213817715644836
CurrentTrain: epoch  1, batch     3 | loss: 4.3700356Losses:  3.012543201446533 1.07047438621521
CurrentTrain: epoch  2, batch     0 | loss: 4.0830173Losses:  3.706329822540283 1.376373529434204
CurrentTrain: epoch  2, batch     1 | loss: 5.0827036Losses:  4.204148292541504 1.6249483823776245
CurrentTrain: epoch  2, batch     2 | loss: 5.8290968Losses:  2.0293188095092773 0.09969566762447357
CurrentTrain: epoch  2, batch     3 | loss: 2.1290145Losses:  3.3735060691833496 1.3669724464416504
CurrentTrain: epoch  3, batch     0 | loss: 4.7404785Losses:  3.0569822788238525 1.13357675075531
CurrentTrain: epoch  3, batch     1 | loss: 4.1905589Losses:  3.727654218673706 1.317784070968628
CurrentTrain: epoch  3, batch     2 | loss: 5.0454383Losses:  3.8290162086486816 0.1633543074131012
CurrentTrain: epoch  3, batch     3 | loss: 3.9923706Losses:  3.635592222213745 1.2245571613311768
CurrentTrain: epoch  4, batch     0 | loss: 4.8601494Losses:  3.2324728965759277 1.250824213027954
CurrentTrain: epoch  4, batch     1 | loss: 4.4832973Losses:  2.6396279335021973 0.9394094944000244
CurrentTrain: epoch  4, batch     2 | loss: 3.5790374Losses:  3.068765163421631 0.1794155389070511
CurrentTrain: epoch  4, batch     3 | loss: 3.2481806Losses:  3.4817304611206055 1.156611442565918
CurrentTrain: epoch  5, batch     0 | loss: 4.6383419Losses:  2.69386625289917 0.886060357093811
CurrentTrain: epoch  5, batch     1 | loss: 3.5799265Losses:  2.7961206436157227 0.797736406326294
CurrentTrain: epoch  5, batch     2 | loss: 3.5938570Losses:  4.595254898071289 0.5920400023460388
CurrentTrain: epoch  5, batch     3 | loss: 5.1872950Losses:  3.148813486099243 1.2198940515518188
CurrentTrain: epoch  6, batch     0 | loss: 4.3687077Losses:  3.056652069091797 1.1115188598632812
CurrentTrain: epoch  6, batch     1 | loss: 4.1681709Losses:  2.4873361587524414 0.7963064908981323
CurrentTrain: epoch  6, batch     2 | loss: 3.2836428Losses:  2.3091747760772705 0.1590314358472824
CurrentTrain: epoch  6, batch     3 | loss: 2.4682062Losses:  2.567777633666992 0.8995892405509949
CurrentTrain: epoch  7, batch     0 | loss: 3.4673669Losses:  2.793710708618164 0.8655091524124146
CurrentTrain: epoch  7, batch     1 | loss: 3.6592197Losses:  2.7825889587402344 0.9243972301483154
CurrentTrain: epoch  7, batch     2 | loss: 3.7069862Losses:  3.6861276626586914 0.47583672404289246
CurrentTrain: epoch  7, batch     3 | loss: 4.1619644Losses:  2.735696792602539 0.9970009326934814
CurrentTrain: epoch  8, batch     0 | loss: 3.7326977Losses:  2.653149366378784 0.9821963310241699
CurrentTrain: epoch  8, batch     1 | loss: 3.6353457Losses:  2.492197036743164 0.6280179023742676
CurrentTrain: epoch  8, batch     2 | loss: 3.1202149Losses:  2.0588645935058594 0.06093334034085274
CurrentTrain: epoch  8, batch     3 | loss: 2.1197979Losses:  2.3401968479156494 0.7337595224380493
CurrentTrain: epoch  9, batch     0 | loss: 3.0739565Losses:  2.190650224685669 0.6255311965942383
CurrentTrain: epoch  9, batch     1 | loss: 2.8161814Losses:  2.766845464706421 0.8279826641082764
CurrentTrain: epoch  9, batch     2 | loss: 3.5948281Losses:  3.5801548957824707 0.529911994934082
CurrentTrain: epoch  9, batch     3 | loss: 4.1100669
Losses:  5.701570510864258 0.9195504784584045
MemoryTrain:  epoch  0, batch     0 | loss: 6.6211209Losses:  9.237146377563477 0.8895966410636902
MemoryTrain:  epoch  0, batch     1 | loss: 10.1267433Losses:  0.7778753638267517 0.925223708152771
MemoryTrain:  epoch  1, batch     0 | loss: 1.7030990Losses:  0.6397846937179565 0.753006100654602
MemoryTrain:  epoch  1, batch     1 | loss: 1.3927908Losses:  0.44981688261032104 0.7788159251213074
MemoryTrain:  epoch  2, batch     0 | loss: 1.2286328Losses:  0.6766270995140076 0.8074026703834534
MemoryTrain:  epoch  2, batch     1 | loss: 1.4840298Losses:  0.6443148851394653 0.7824109792709351
MemoryTrain:  epoch  3, batch     0 | loss: 1.4267259Losses:  0.3545819818973541 0.7269091010093689
MemoryTrain:  epoch  3, batch     1 | loss: 1.0814911Losses:  0.5016204714775085 0.911212682723999
MemoryTrain:  epoch  4, batch     0 | loss: 1.4128332Losses:  0.3481787443161011 0.5725079774856567
MemoryTrain:  epoch  4, batch     1 | loss: 0.9206867Losses:  0.36049607396125793 0.8053478002548218
MemoryTrain:  epoch  5, batch     0 | loss: 1.1658438Losses:  0.38840726017951965 0.6963512301445007
MemoryTrain:  epoch  5, batch     1 | loss: 1.0847585Losses:  0.29600995779037476 0.737661600112915
MemoryTrain:  epoch  6, batch     0 | loss: 1.0336716Losses:  0.25133901834487915 0.7235210537910461
MemoryTrain:  epoch  6, batch     1 | loss: 0.9748601Losses:  0.3128891587257385 0.617374062538147
MemoryTrain:  epoch  7, batch     0 | loss: 0.9302632Losses:  0.33295106887817383 0.7816757559776306
MemoryTrain:  epoch  7, batch     1 | loss: 1.1146269Losses:  0.2864276170730591 0.8576804399490356
MemoryTrain:  epoch  8, batch     0 | loss: 1.1441081Losses:  0.26179465651512146 0.4646146297454834
MemoryTrain:  epoch  8, batch     1 | loss: 0.7264093Losses:  0.2494153082370758 0.8279327154159546
MemoryTrain:  epoch  9, batch     0 | loss: 1.0773480Losses:  0.24308492243289948 0.5083531141281128
MemoryTrain:  epoch  9, batch     1 | loss: 0.7514380
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 61.91%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 71.39%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 71.60%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 72.06%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 72.00%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 71.93%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 72.15%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 72.52%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.21%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 92.90%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 92.04%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.06%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.89%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.95%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 92.00%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 91.75%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 91.61%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 91.39%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 91.33%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 91.28%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.11%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 90.98%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 90.78%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 90.82%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 90.78%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 90.51%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 90.18%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 89.85%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 89.68%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 89.15%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 88.99%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 88.76%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 88.40%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 88.11%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 87.57%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 87.03%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 86.64%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 86.20%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 85.95%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 85.59%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 85.00%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.65%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 84.31%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 83.77%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 83.51%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.31%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 82.89%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 82.23%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 81.65%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 80.97%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 80.35%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.91%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 79.65%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.82%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.17%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.46%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 80.89%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 81.30%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 81.20%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 81.05%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 81.06%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 80.96%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 80.96%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 81.20%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 81.30%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 81.61%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 81.57%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 81.12%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 80.80%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 80.36%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 80.11%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 79.85%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 79.51%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.89%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 80.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 79.76%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 79.28%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 78.88%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 78.49%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 77.98%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 77.52%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 77.47%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 77.53%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 77.76%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 78.79%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 78.80%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 78.78%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 78.68%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 78.70%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 78.67%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 78.55%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 78.49%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 78.54%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 78.56%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 78.54%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 78.59%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 78.67%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 78.46%   
cur_acc:  ['0.9484', '0.7014', '0.7252']
his_acc:  ['0.9484', '0.8215', '0.7846']
Clustering into  19  clusters
Clusters:  [ 0 11 17  0  0  0  0  0 15  3  0  0  0 13  0  3  0 12  9 18  1 14 16  0
  0  6  0 10  0  5  2  4  0  0  0  1  7  0  0  8]
Losses:  7.564324378967285 1.6162121295928955
CurrentTrain: epoch  0, batch     0 | loss: 9.1805363Losses:  8.235971450805664 1.48623526096344
CurrentTrain: epoch  0, batch     1 | loss: 9.7222071Losses:  7.650390148162842 1.630587100982666
CurrentTrain: epoch  0, batch     2 | loss: 9.2809772Losses:  3.7323391437530518 0.3652249574661255
CurrentTrain: epoch  0, batch     3 | loss: 4.0975642Losses:  3.024641275405884 1.4128613471984863
CurrentTrain: epoch  1, batch     0 | loss: 4.4375029Losses:  2.7149088382720947 1.1652992963790894
CurrentTrain: epoch  1, batch     1 | loss: 3.8802080Losses:  2.8918371200561523 1.7064310312271118
CurrentTrain: epoch  1, batch     2 | loss: 4.5982680Losses:  2.4681687355041504 0.18726477026939392
CurrentTrain: epoch  1, batch     3 | loss: 2.6554334Losses:  3.1766111850738525 1.4209306240081787
CurrentTrain: epoch  2, batch     0 | loss: 4.5975418Losses:  2.3891496658325195 1.2242851257324219
CurrentTrain: epoch  2, batch     1 | loss: 3.6134348Losses:  2.1146163940429688 1.0615861415863037
CurrentTrain: epoch  2, batch     2 | loss: 3.1762025Losses:  1.9018769264221191 0.0978848934173584
CurrentTrain: epoch  2, batch     3 | loss: 1.9997618Losses:  2.387763023376465 0.9929144382476807
CurrentTrain: epoch  3, batch     0 | loss: 3.3806775Losses:  2.356645345687866 1.2543944120407104
CurrentTrain: epoch  3, batch     1 | loss: 3.6110396Losses:  2.427931785583496 1.2131727933883667
CurrentTrain: epoch  3, batch     2 | loss: 3.6411047Losses:  1.7818896770477295 0.12267936766147614
CurrentTrain: epoch  3, batch     3 | loss: 1.9045690Losses:  2.2213728427886963 0.8376579284667969
CurrentTrain: epoch  4, batch     0 | loss: 3.0590308Losses:  2.2167630195617676 0.8578372001647949
CurrentTrain: epoch  4, batch     1 | loss: 3.0746002Losses:  2.4859941005706787 0.7664176225662231
CurrentTrain: epoch  4, batch     2 | loss: 3.2524118Losses:  2.0865206718444824 0.10596127063035965
CurrentTrain: epoch  4, batch     3 | loss: 2.1924820Losses:  2.3481321334838867 0.887540876865387
CurrentTrain: epoch  5, batch     0 | loss: 3.2356730Losses:  2.1793198585510254 0.7810763120651245
CurrentTrain: epoch  5, batch     1 | loss: 2.9603963Losses:  2.01993989944458 0.9872599840164185
CurrentTrain: epoch  5, batch     2 | loss: 3.0071998Losses:  1.8724608421325684 8.94069742685133e-08
CurrentTrain: epoch  5, batch     3 | loss: 1.8724610Losses:  1.9295713901519775 0.6525205373764038
CurrentTrain: epoch  6, batch     0 | loss: 2.5820918Losses:  2.0819966793060303 0.6571562886238098
CurrentTrain: epoch  6, batch     1 | loss: 2.7391529Losses:  2.182952880859375 0.8913184404373169
CurrentTrain: epoch  6, batch     2 | loss: 3.0742712Losses:  1.973891019821167 0.1677449494600296
CurrentTrain: epoch  6, batch     3 | loss: 2.1416359Losses:  1.9160693883895874 0.6299610137939453
CurrentTrain: epoch  7, batch     0 | loss: 2.5460305Losses:  1.899829626083374 0.8979855179786682
CurrentTrain: epoch  7, batch     1 | loss: 2.7978151Losses:  2.0140388011932373 0.8000816106796265
CurrentTrain: epoch  7, batch     2 | loss: 2.8141203Losses:  2.072908878326416 0.209937185049057
CurrentTrain: epoch  7, batch     3 | loss: 2.2828460Losses:  2.0248265266418457 0.7122829556465149
CurrentTrain: epoch  8, batch     0 | loss: 2.7371094Losses:  1.8909974098205566 0.6187810897827148
CurrentTrain: epoch  8, batch     1 | loss: 2.5097785Losses:  1.7877180576324463 0.5783886313438416
CurrentTrain: epoch  8, batch     2 | loss: 2.3661067Losses:  1.8865482807159424 0.12558996677398682
CurrentTrain: epoch  8, batch     3 | loss: 2.0121384Losses:  1.8634710311889648 0.6255137324333191
CurrentTrain: epoch  9, batch     0 | loss: 2.4889848Losses:  1.8959263563156128 0.5163053274154663
CurrentTrain: epoch  9, batch     1 | loss: 2.4122317Losses:  1.7812130451202393 0.5802662372589111
CurrentTrain: epoch  9, batch     2 | loss: 2.3614793Losses:  1.9532160758972168 0.35017403960227966
CurrentTrain: epoch  9, batch     3 | loss: 2.3033900
Losses:  5.625343322753906 0.89773029088974
MemoryTrain:  epoch  0, batch     0 | loss: 6.5230737Losses:  9.355058670043945 0.5756891369819641
MemoryTrain:  epoch  0, batch     1 | loss: 9.9307480Losses:  11.283525466918945 0.38761329650878906
MemoryTrain:  epoch  0, batch     2 | loss: 11.6711388Losses:  0.7173665761947632 0.6922445297241211
MemoryTrain:  epoch  1, batch     0 | loss: 1.4096111Losses:  0.8681657910346985 0.6073811054229736
MemoryTrain:  epoch  1, batch     1 | loss: 1.4755468Losses:  0.8465067148208618 0.549460768699646
MemoryTrain:  epoch  1, batch     2 | loss: 1.3959675Losses:  0.6374331712722778 0.8049265146255493
MemoryTrain:  epoch  2, batch     0 | loss: 1.4423597Losses:  0.6808344721794128 0.7019321918487549
MemoryTrain:  epoch  2, batch     1 | loss: 1.3827667Losses:  0.8629391193389893 0.42947664856910706
MemoryTrain:  epoch  2, batch     2 | loss: 1.2924157Losses:  0.5349422693252563 0.8501423001289368
MemoryTrain:  epoch  3, batch     0 | loss: 1.3850846Losses:  0.5989406108856201 0.5164401531219482
MemoryTrain:  epoch  3, batch     1 | loss: 1.1153808Losses:  0.6290838718414307 0.5383265614509583
MemoryTrain:  epoch  3, batch     2 | loss: 1.1674104Losses:  0.35023343563079834 0.7892359495162964
MemoryTrain:  epoch  4, batch     0 | loss: 1.1394694Losses:  0.6553062200546265 0.6382633447647095
MemoryTrain:  epoch  4, batch     1 | loss: 1.2935696Losses:  0.43176931142807007 0.4367455244064331
MemoryTrain:  epoch  4, batch     2 | loss: 0.8685148Losses:  0.3758348822593689 0.6858060359954834
MemoryTrain:  epoch  5, batch     0 | loss: 1.0616410Losses:  0.40338531136512756 0.919921338558197
MemoryTrain:  epoch  5, batch     1 | loss: 1.3233067Losses:  0.29922521114349365 0.33702871203422546
MemoryTrain:  epoch  5, batch     2 | loss: 0.6362540Losses:  0.3833218514919281 0.6122217774391174
MemoryTrain:  epoch  6, batch     0 | loss: 0.9955436Losses:  0.3740180432796478 0.6774958372116089
MemoryTrain:  epoch  6, batch     1 | loss: 1.0515139Losses:  0.45858854055404663 0.5370709896087646
MemoryTrain:  epoch  6, batch     2 | loss: 0.9956595Losses:  0.3225679397583008 0.5865103602409363
MemoryTrain:  epoch  7, batch     0 | loss: 0.9090783Losses:  0.4581213891506195 0.6277754306793213
MemoryTrain:  epoch  7, batch     1 | loss: 1.0858968Losses:  0.3338715434074402 0.4876704216003418
MemoryTrain:  epoch  7, batch     2 | loss: 0.8215420Losses:  0.27330079674720764 0.4896473288536072
MemoryTrain:  epoch  8, batch     0 | loss: 0.7629482Losses:  0.3753710985183716 0.7524502277374268
MemoryTrain:  epoch  8, batch     1 | loss: 1.1278213Losses:  0.37868207693099976 0.40337276458740234
MemoryTrain:  epoch  8, batch     2 | loss: 0.7820548Losses:  0.30190274119377136 0.7938162684440613
MemoryTrain:  epoch  9, batch     0 | loss: 1.0957190Losses:  0.29662057757377625 0.5392276048660278
MemoryTrain:  epoch  9, batch     1 | loss: 0.8358482Losses:  0.24612517654895782 0.2713642716407776
MemoryTrain:  epoch  9, batch     2 | loss: 0.5174894
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 57.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 68.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 76.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 77.16%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.55%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 78.56%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 78.71%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 79.17%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 91.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 91.25%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.12%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 90.52%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 90.15%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.90%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 89.55%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.21%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 89.36%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 89.65%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 89.82%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.76%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 89.64%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 89.36%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 89.39%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 89.26%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 89.16%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 89.14%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 89.02%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 88.70%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 88.16%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 87.94%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 87.28%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 87.07%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 86.73%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 86.32%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 86.13%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 85.35%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 84.77%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 84.41%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 83.98%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 83.76%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 83.42%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 83.27%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 82.75%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 82.36%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 81.99%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 81.67%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 81.43%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.31%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 81.07%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 80.55%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 79.92%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 79.36%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 78.69%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 78.10%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.68%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 77.43%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.87%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 79.05%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 79.37%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 79.18%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 79.10%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 79.02%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 78.99%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 79.14%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 78.79%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 78.37%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 77.88%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 77.56%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.87%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 77.94%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 77.47%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 77.00%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 76.70%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 76.21%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 75.76%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 75.76%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 75.83%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 76.09%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.20%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 76.21%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 76.20%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 76.24%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.37%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 76.69%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 76.62%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 76.61%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 76.66%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 76.65%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 76.71%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 76.92%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 77.05%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.26%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 77.28%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 77.17%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 77.16%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 77.11%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 77.01%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 76.91%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 76.83%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 76.72%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 76.47%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 76.34%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 76.21%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 76.03%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 75.78%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 75.54%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 75.30%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 75.06%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 74.76%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  222 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  224 | acc: 81.25%,  total acc: 75.67%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.87%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 77.10%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 77.12%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 77.19%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 77.23%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.36%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.68%   
cur_acc:  ['0.9484', '0.7014', '0.7252', '0.7917']
his_acc:  ['0.9484', '0.8215', '0.7846', '0.7768']
Clustering into  24  clusters
Clusters:  [ 1  6 17  2  1  1 22  1 18  3  1  1  1 13  1  3  1 23 19 21  0 16 20  1
  1 15  1  9  1 14  2 12  1  1 11  0  7  1  1  8  6 10  1  1  1  4  1  1
  5  1]
Losses:  6.896842956542969 1.2978978157043457
CurrentTrain: epoch  0, batch     0 | loss: 8.1947403Losses:  8.530977249145508 1.5338001251220703
CurrentTrain: epoch  0, batch     1 | loss: 10.0647774Losses:  7.16818380355835 1.199558973312378
CurrentTrain: epoch  0, batch     2 | loss: 8.3677425Losses:  5.241585731506348 1.1920930376163597e-07
CurrentTrain: epoch  0, batch     3 | loss: 5.2415857Losses:  3.8874549865722656 1.2804101705551147
CurrentTrain: epoch  1, batch     0 | loss: 5.1678653Losses:  3.346665620803833 1.3048138618469238
CurrentTrain: epoch  1, batch     1 | loss: 4.6514797Losses:  3.1483161449432373 1.1863927841186523
CurrentTrain: epoch  1, batch     2 | loss: 4.3347092Losses:  3.181638717651367 0.42387858033180237
CurrentTrain: epoch  1, batch     3 | loss: 3.6055174Losses:  3.3085508346557617 1.3568137884140015
CurrentTrain: epoch  2, batch     0 | loss: 4.6653647Losses:  3.5114965438842773 1.2056080102920532
CurrentTrain: epoch  2, batch     1 | loss: 4.7171044Losses:  2.9841363430023193 1.0444436073303223
CurrentTrain: epoch  2, batch     2 | loss: 4.0285797Losses:  2.0074777603149414 0.05879068002104759
CurrentTrain: epoch  2, batch     3 | loss: 2.0662684Losses:  3.3404016494750977 1.0774660110473633
CurrentTrain: epoch  3, batch     0 | loss: 4.4178677Losses:  2.7271924018859863 0.8363475799560547
CurrentTrain: epoch  3, batch     1 | loss: 3.5635400Losses:  2.745063066482544 0.9738763570785522
CurrentTrain: epoch  3, batch     2 | loss: 3.7189393Losses:  3.347099781036377 0.386750191450119
CurrentTrain: epoch  3, batch     3 | loss: 3.7338500Losses:  2.637211322784424 0.8462778925895691
CurrentTrain: epoch  4, batch     0 | loss: 3.4834893Losses:  2.9212217330932617 0.8297760486602783
CurrentTrain: epoch  4, batch     1 | loss: 3.7509978Losses:  2.7843589782714844 0.969443142414093
CurrentTrain: epoch  4, batch     2 | loss: 3.7538021Losses:  1.6947569847106934 0.09832946211099625
CurrentTrain: epoch  4, batch     3 | loss: 1.7930864Losses:  2.588613986968994 1.0192456245422363
CurrentTrain: epoch  5, batch     0 | loss: 3.6078596Losses:  2.3632373809814453 0.704912006855011
CurrentTrain: epoch  5, batch     1 | loss: 3.0681493Losses:  2.638190507888794 0.7942137718200684
CurrentTrain: epoch  5, batch     2 | loss: 3.4324043Losses:  3.4854934215545654 0.23118825256824493
CurrentTrain: epoch  5, batch     3 | loss: 3.7166817Losses:  2.4781599044799805 0.7059584856033325
CurrentTrain: epoch  6, batch     0 | loss: 3.1841183Losses:  2.4211878776550293 0.6834086179733276
CurrentTrain: epoch  6, batch     1 | loss: 3.1045966Losses:  2.516690731048584 0.8546712398529053
CurrentTrain: epoch  6, batch     2 | loss: 3.3713620Losses:  1.9807202816009521 0.12604190409183502
CurrentTrain: epoch  6, batch     3 | loss: 2.1067622Losses:  2.2717905044555664 0.7576024532318115
CurrentTrain: epoch  7, batch     0 | loss: 3.0293930Losses:  2.305929660797119 0.7185782194137573
CurrentTrain: epoch  7, batch     1 | loss: 3.0245080Losses:  2.3744935989379883 0.7009863257408142
CurrentTrain: epoch  7, batch     2 | loss: 3.0754800Losses:  2.4391398429870605 0.29407402873039246
CurrentTrain: epoch  7, batch     3 | loss: 2.7332139Losses:  2.0961294174194336 0.7064785957336426
CurrentTrain: epoch  8, batch     0 | loss: 2.8026080Losses:  2.3995361328125 0.7184134721755981
CurrentTrain: epoch  8, batch     1 | loss: 3.1179495Losses:  2.1692256927490234 0.599022626876831
CurrentTrain: epoch  8, batch     2 | loss: 2.7682483Losses:  1.6961750984191895 0.059578873217105865
CurrentTrain: epoch  8, batch     3 | loss: 1.7557540Losses:  2.152271270751953 0.6849737167358398
CurrentTrain: epoch  9, batch     0 | loss: 2.8372450Losses:  1.8941975831985474 0.4942381978034973
CurrentTrain: epoch  9, batch     1 | loss: 2.3884358Losses:  2.2198362350463867 0.5576052069664001
CurrentTrain: epoch  9, batch     2 | loss: 2.7774415Losses:  1.8352888822555542 0.09200739860534668
CurrentTrain: epoch  9, batch     3 | loss: 1.9272963
Losses:  5.833064079284668 0.6001352667808533
MemoryTrain:  epoch  0, batch     0 | loss: 6.4331994Losses:  9.161937713623047 0.8628444671630859
MemoryTrain:  epoch  0, batch     1 | loss: 10.0247822Losses:  10.578170776367188 0.7090224623680115
MemoryTrain:  epoch  0, batch     2 | loss: 11.2871933Losses:  12.69346809387207 0.05356280505657196
MemoryTrain:  epoch  0, batch     3 | loss: 12.7470312Losses:  1.1982159614562988 0.7687307596206665
MemoryTrain:  epoch  1, batch     0 | loss: 1.9669467Losses:  0.8471083641052246 0.8061854839324951
MemoryTrain:  epoch  1, batch     1 | loss: 1.6532938Losses:  0.6495754718780518 0.6984310746192932
MemoryTrain:  epoch  1, batch     2 | loss: 1.3480065Losses:  0.19630640745162964 0.02894304320216179
MemoryTrain:  epoch  1, batch     3 | loss: 0.2252495Losses:  0.9958163499832153 0.7156156301498413
MemoryTrain:  epoch  2, batch     0 | loss: 1.7114320Losses:  0.46148553490638733 0.6995185613632202
MemoryTrain:  epoch  2, batch     1 | loss: 1.1610041Losses:  0.7024616003036499 0.628088116645813
MemoryTrain:  epoch  2, batch     2 | loss: 1.3305497Losses:  0.4243813157081604 0.3793763220310211
MemoryTrain:  epoch  2, batch     3 | loss: 0.8037577Losses:  0.5473333597183228 0.547913134098053
MemoryTrain:  epoch  3, batch     0 | loss: 1.0952466Losses:  0.4688996970653534 0.9320952892303467
MemoryTrain:  epoch  3, batch     1 | loss: 1.4009950Losses:  0.8205759525299072 0.7215012311935425
MemoryTrain:  epoch  3, batch     2 | loss: 1.5420772Losses:  0.4588548541069031 0.14965707063674927
MemoryTrain:  epoch  3, batch     3 | loss: 0.6085119Losses:  0.5988504886627197 0.6948383450508118
MemoryTrain:  epoch  4, batch     0 | loss: 1.2936888Losses:  0.39612656831741333 0.7889883518218994
MemoryTrain:  epoch  4, batch     1 | loss: 1.1851149Losses:  0.46466729044914246 0.7116177678108215
MemoryTrain:  epoch  4, batch     2 | loss: 1.1762850Losses:  0.2180195301771164 0.02675168588757515
MemoryTrain:  epoch  4, batch     3 | loss: 0.2447712Losses:  0.4201890826225281 0.5960653424263
MemoryTrain:  epoch  5, batch     0 | loss: 1.0162544Losses:  0.4032641351222992 0.723044753074646
MemoryTrain:  epoch  5, batch     1 | loss: 1.1263089Losses:  0.4692680835723877 0.8379946947097778
MemoryTrain:  epoch  5, batch     2 | loss: 1.3072628Losses:  0.2560187876224518 0.01500849798321724
MemoryTrain:  epoch  5, batch     3 | loss: 0.2710273Losses:  0.47916990518569946 0.64928138256073
MemoryTrain:  epoch  6, batch     0 | loss: 1.1284513Losses:  0.3661205768585205 0.7084397077560425
MemoryTrain:  epoch  6, batch     1 | loss: 1.0745603Losses:  0.3595018982887268 0.7525322437286377
MemoryTrain:  epoch  6, batch     2 | loss: 1.1120341Losses:  0.2921915650367737 0.048632655292749405
MemoryTrain:  epoch  6, batch     3 | loss: 0.3408242Losses:  0.3722466826438904 0.8117769956588745
MemoryTrain:  epoch  7, batch     0 | loss: 1.1840236Losses:  0.35301995277404785 0.5783759355545044
MemoryTrain:  epoch  7, batch     1 | loss: 0.9313959Losses:  0.3593023419380188 0.720832884311676
MemoryTrain:  epoch  7, batch     2 | loss: 1.0801352Losses:  0.22561219334602356 0.009004556573927402
MemoryTrain:  epoch  7, batch     3 | loss: 0.2346168Losses:  0.3770616054534912 0.7919372320175171
MemoryTrain:  epoch  8, batch     0 | loss: 1.1689988Losses:  0.30026182532310486 0.5277966260910034
MemoryTrain:  epoch  8, batch     1 | loss: 0.8280585Losses:  0.3085381090641022 0.6437594890594482
MemoryTrain:  epoch  8, batch     2 | loss: 0.9522976Losses:  0.2867025136947632 0.11937541514635086
MemoryTrain:  epoch  8, batch     3 | loss: 0.4060779Losses:  0.313485324382782 0.6997239589691162
MemoryTrain:  epoch  9, batch     0 | loss: 1.0132093Losses:  0.3153764307498932 0.7033418416976929
MemoryTrain:  epoch  9, batch     1 | loss: 1.0187182Losses:  0.32712337374687195 0.6151500344276428
MemoryTrain:  epoch  9, batch     2 | loss: 0.9422734Losses:  0.20569203794002533 0.005617581307888031
MemoryTrain:  epoch  9, batch     3 | loss: 0.2113096
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 62.30%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 60.54%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 59.38%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 58.11%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 58.06%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 59.06%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 59.76%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 60.76%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 61.28%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 61.30%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 62.12%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 62.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.33%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.29%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 89.67%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.81%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 89.47%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.69%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.35%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 87.81%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 87.69%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 87.78%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.13%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 88.20%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 88.11%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.01%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 87.99%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 87.74%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.04%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 86.75%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 86.12%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 85.56%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 85.51%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 85.18%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 84.93%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 84.62%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.31%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 83.87%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 83.31%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 82.96%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 82.55%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 82.35%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 81.89%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 81.76%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 81.31%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 80.88%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 80.51%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 80.22%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 79.99%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 79.70%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 79.36%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 78.86%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.24%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.69%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 77.05%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 76.46%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 76.06%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 75.83%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 76.66%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 77.98%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 77.81%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 77.69%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 77.55%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 77.48%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 77.65%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.42%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 77.92%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 77.54%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 77.17%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 76.94%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.70%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.43%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.55%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.71%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 76.87%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.82%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 76.36%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 75.90%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 75.57%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 75.08%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.60%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.68%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 75.23%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  170 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 75.90%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.95%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 76.18%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.28%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 76.30%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 76.55%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 76.60%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 76.74%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 76.67%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 76.48%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 76.35%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 76.22%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 76.12%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 76.05%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 76.02%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 75.77%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 75.64%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 75.42%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 75.15%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 74.85%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 74.61%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 74.38%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 74.09%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  219 | acc: 87.50%,  total acc: 74.80%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  222 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 74.97%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.31%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.57%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 76.78%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 76.85%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.10%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 76.92%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 76.84%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 76.63%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 76.45%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 76.25%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 76.03%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 75.92%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 75.69%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.67%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 75.59%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 75.47%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 75.74%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 75.77%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 75.70%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 75.65%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 75.56%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 75.47%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 75.44%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 75.20%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 75.07%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 74.87%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 74.65%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 74.59%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 74.66%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 74.70%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 74.68%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 74.64%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 74.60%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.62%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 75.38%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.54%   
cur_acc:  ['0.9484', '0.7014', '0.7252', '0.7917', '0.6935']
his_acc:  ['0.9484', '0.8215', '0.7846', '0.7768', '0.7554']
Clustering into  29  clusters
Clusters:  [ 0  4 17  7  0  0 28  0 19  3  0  0  0 21  0  3  0 23 27  1 13 24 18  0
  0 16  0 11  0 14  7 25  0  0 26 12  5  0  0 15  4 20  0  0  0 22  0  0
  6  2  0 10  0  0  1  0  0  0  8  9]
Losses:  7.114869117736816 1.4378437995910645
CurrentTrain: epoch  0, batch     0 | loss: 8.5527134Losses:  9.648804664611816 1.1346111297607422
CurrentTrain: epoch  0, batch     1 | loss: 10.7834158Losses:  8.321346282958984 1.3127450942993164
CurrentTrain: epoch  0, batch     2 | loss: 9.6340914Losses:  5.664031505584717 2.9802322387695312e-08
CurrentTrain: epoch  0, batch     3 | loss: 5.6640315Losses:  4.629132270812988 1.4851160049438477
CurrentTrain: epoch  1, batch     0 | loss: 6.1142483Losses:  3.7225382328033447 1.1681294441223145
CurrentTrain: epoch  1, batch     1 | loss: 4.8906679Losses:  3.193045139312744 1.1564100980758667
CurrentTrain: epoch  1, batch     2 | loss: 4.3494554Losses:  3.9872725009918213 0.32732683420181274
CurrentTrain: epoch  1, batch     3 | loss: 4.3145995Losses:  3.7898592948913574 1.1208754777908325
CurrentTrain: epoch  2, batch     0 | loss: 4.9107347Losses:  4.029913425445557 1.151924967765808
CurrentTrain: epoch  2, batch     1 | loss: 5.1818385Losses:  3.225268840789795 1.1552648544311523
CurrentTrain: epoch  2, batch     2 | loss: 4.3805337Losses:  4.351415157318115 0.46338358521461487
CurrentTrain: epoch  2, batch     3 | loss: 4.8147988Losses:  2.96824312210083 1.0183650255203247
CurrentTrain: epoch  3, batch     0 | loss: 3.9866080Losses:  3.6470320224761963 1.2037181854248047
CurrentTrain: epoch  3, batch     1 | loss: 4.8507500Losses:  3.6884632110595703 1.2224680185317993
CurrentTrain: epoch  3, batch     2 | loss: 4.9109311Losses:  4.647800445556641 0.16773521900177002
CurrentTrain: epoch  3, batch     3 | loss: 4.8155355Losses:  2.8532986640930176 0.844244122505188
CurrentTrain: epoch  4, batch     0 | loss: 3.6975427Losses:  3.7415785789489746 0.9682955741882324
CurrentTrain: epoch  4, batch     1 | loss: 4.7098742Losses:  3.45401930809021 1.0407041311264038
CurrentTrain: epoch  4, batch     2 | loss: 4.4947233Losses:  1.939333200454712 0.2421640157699585
CurrentTrain: epoch  4, batch     3 | loss: 2.1814971Losses:  3.0681285858154297 0.9891401529312134
CurrentTrain: epoch  5, batch     0 | loss: 4.0572686Losses:  3.236734390258789 0.774246335029602
CurrentTrain: epoch  5, batch     1 | loss: 4.0109806Losses:  2.808216094970703 0.9731744527816772
CurrentTrain: epoch  5, batch     2 | loss: 3.7813907Losses:  2.6074647903442383 0.12695910036563873
CurrentTrain: epoch  5, batch     3 | loss: 2.7344239Losses:  2.329371929168701 0.8061356544494629
CurrentTrain: epoch  6, batch     0 | loss: 3.1355076Losses:  3.0597004890441895 1.0125977993011475
CurrentTrain: epoch  6, batch     1 | loss: 4.0722980Losses:  2.890773296356201 1.0015654563903809
CurrentTrain: epoch  6, batch     2 | loss: 3.8923388Losses:  4.272519111633301 0.024520672857761383
CurrentTrain: epoch  6, batch     3 | loss: 4.2970400Losses:  2.690865993499756 0.8723649978637695
CurrentTrain: epoch  7, batch     0 | loss: 3.5632310Losses:  2.9838125705718994 0.8747665882110596
CurrentTrain: epoch  7, batch     1 | loss: 3.8585792Losses:  2.3145861625671387 0.7206593751907349
CurrentTrain: epoch  7, batch     2 | loss: 3.0352454Losses:  3.459628105163574 0.1478026807308197
CurrentTrain: epoch  7, batch     3 | loss: 3.6074307Losses:  2.340869665145874 0.5803043246269226
CurrentTrain: epoch  8, batch     0 | loss: 2.9211740Losses:  2.1639063358306885 0.6540310382843018
CurrentTrain: epoch  8, batch     1 | loss: 2.8179374Losses:  3.08317232131958 0.9102908372879028
CurrentTrain: epoch  8, batch     2 | loss: 3.9934630Losses:  1.7098429203033447 0.027493970468640327
CurrentTrain: epoch  8, batch     3 | loss: 1.7373369Losses:  2.1423325538635254 0.658240795135498
CurrentTrain: epoch  9, batch     0 | loss: 2.8005733Losses:  2.057267189025879 0.6418734788894653
CurrentTrain: epoch  9, batch     1 | loss: 2.6991405Losses:  2.6686182022094727 0.7785506248474121
CurrentTrain: epoch  9, batch     2 | loss: 3.4471688Losses:  4.331539630889893 0.40158870816230774
CurrentTrain: epoch  9, batch     3 | loss: 4.7331285
Losses:  5.993644714355469 0.7899518013000488
MemoryTrain:  epoch  0, batch     0 | loss: 6.7835965Losses:  9.176451683044434 0.9307588338851929
MemoryTrain:  epoch  0, batch     1 | loss: 10.1072102Losses:  9.456663131713867 0.7345812320709229
MemoryTrain:  epoch  0, batch     2 | loss: 10.1912441Losses:  10.865781784057617 0.6042066812515259
MemoryTrain:  epoch  0, batch     3 | loss: 11.4699888Losses:  1.1266241073608398 0.7092353105545044
MemoryTrain:  epoch  1, batch     0 | loss: 1.8358594Losses:  0.7904758453369141 0.7714443206787109
MemoryTrain:  epoch  1, batch     1 | loss: 1.5619202Losses:  1.0654723644256592 0.8716782331466675
MemoryTrain:  epoch  1, batch     2 | loss: 1.9371506Losses:  0.700289249420166 0.6459106206893921
MemoryTrain:  epoch  1, batch     3 | loss: 1.3461999Losses:  0.8174943327903748 0.8242460489273071
MemoryTrain:  epoch  2, batch     0 | loss: 1.6417403Losses:  0.6958156228065491 0.7462344169616699
MemoryTrain:  epoch  2, batch     1 | loss: 1.4420500Losses:  0.6006428003311157 0.6448040008544922
MemoryTrain:  epoch  2, batch     2 | loss: 1.2454468Losses:  0.9128791689872742 0.5257605314254761
MemoryTrain:  epoch  2, batch     3 | loss: 1.4386396Losses:  0.5627034902572632 0.6244534254074097
MemoryTrain:  epoch  3, batch     0 | loss: 1.1871569Losses:  0.4679943323135376 0.6778473854064941
MemoryTrain:  epoch  3, batch     1 | loss: 1.1458417Losses:  0.6712133884429932 0.6158649325370789
MemoryTrain:  epoch  3, batch     2 | loss: 1.2870784Losses:  0.9010686278343201 0.7473543882369995
MemoryTrain:  epoch  3, batch     3 | loss: 1.6484230Losses:  0.8259384632110596 0.9327843189239502
MemoryTrain:  epoch  4, batch     0 | loss: 1.7587228Losses:  0.4077785015106201 0.541662335395813
MemoryTrain:  epoch  4, batch     1 | loss: 0.9494408Losses:  0.44912365078926086 0.6021566390991211
MemoryTrain:  epoch  4, batch     2 | loss: 1.0512803Losses:  0.5411890745162964 0.5275739431381226
MemoryTrain:  epoch  4, batch     3 | loss: 1.0687630Losses:  0.47390657663345337 0.5587936043739319
MemoryTrain:  epoch  5, batch     0 | loss: 1.0327002Losses:  0.4499270021915436 0.8579067587852478
MemoryTrain:  epoch  5, batch     1 | loss: 1.3078338Losses:  0.5882558226585388 0.7001975774765015
MemoryTrain:  epoch  5, batch     2 | loss: 1.2884533Losses:  0.4290342330932617 0.6154351234436035
MemoryTrain:  epoch  5, batch     3 | loss: 1.0444694Losses:  0.3954352140426636 0.6397923231124878
MemoryTrain:  epoch  6, batch     0 | loss: 1.0352275Losses:  0.3726751208305359 0.6500933766365051
MemoryTrain:  epoch  6, batch     1 | loss: 1.0227685Losses:  0.5067379474639893 0.7830272912979126
MemoryTrain:  epoch  6, batch     2 | loss: 1.2897652Losses:  0.4391460418701172 0.5300461053848267
MemoryTrain:  epoch  6, batch     3 | loss: 0.9691921Losses:  0.47097763419151306 0.6741858720779419
MemoryTrain:  epoch  7, batch     0 | loss: 1.1451635Losses:  0.4331556260585785 0.751137375831604
MemoryTrain:  epoch  7, batch     1 | loss: 1.1842930Losses:  0.3457692861557007 0.6039881706237793
MemoryTrain:  epoch  7, batch     2 | loss: 0.9497575Losses:  0.39900845289230347 0.5120245218276978
MemoryTrain:  epoch  7, batch     3 | loss: 0.9110330Losses:  0.3996782600879669 0.668899416923523
MemoryTrain:  epoch  8, batch     0 | loss: 1.0685776Losses:  0.3465667963027954 0.593508243560791
MemoryTrain:  epoch  8, batch     1 | loss: 0.9400750Losses:  0.37669628858566284 0.7190945148468018
MemoryTrain:  epoch  8, batch     2 | loss: 1.0957909Losses:  0.4383849799633026 0.5904684066772461
MemoryTrain:  epoch  8, batch     3 | loss: 1.0288534Losses:  0.33052724599838257 0.5926228761672974
MemoryTrain:  epoch  9, batch     0 | loss: 0.9231501Losses:  0.4136499762535095 0.7948960065841675
MemoryTrain:  epoch  9, batch     1 | loss: 1.2085459Losses:  0.3102991580963135 0.6531089544296265
MemoryTrain:  epoch  9, batch     2 | loss: 0.9634081Losses:  0.37364840507507324 0.4743224084377289
MemoryTrain:  epoch  9, batch     3 | loss: 0.8479708
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 74.68%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 73.75%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 70.64%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 73.08%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 72.64%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 72.34%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 71.65%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.16%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 71.23%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 90.03%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.12%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 90.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.69%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.80%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.68%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 90.13%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 89.12%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 88.35%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 87.92%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 87.30%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 86.79%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 86.41%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 86.23%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 85.91%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 85.75%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 85.51%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 85.92%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 85.85%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.98%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 85.86%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 85.39%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 85.21%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 85.11%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 84.68%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 84.26%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 83.53%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 83.36%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 82.76%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 82.60%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 82.30%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 82.01%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 81.66%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 81.32%   [EVAL] batch:   92 | acc: 37.50%,  total acc: 80.85%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.32%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 80.13%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 79.82%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 79.70%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 79.27%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 78.81%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.59%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 78.25%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 77.97%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 77.82%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 77.68%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 77.54%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 77.10%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 76.50%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 75.92%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 75.28%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 74.72%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 74.33%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 74.12%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 75.47%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 75.41%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 75.41%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 75.20%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 75.10%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.90%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.80%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 75.32%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 74.87%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 74.11%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 73.90%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.69%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 73.58%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 73.94%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 74.42%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 73.97%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 73.52%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 73.08%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 72.73%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 72.26%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 71.83%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 71.86%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 72.23%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 72.63%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 72.72%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 72.68%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 72.62%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 72.38%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 72.13%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 71.93%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 71.91%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 71.94%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 72.06%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.53%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 73.23%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 73.16%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.20%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 73.12%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 73.03%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 72.98%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 72.90%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 72.84%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 72.64%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 72.56%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 72.34%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 72.12%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 71.89%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 71.67%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 71.45%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 71.20%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 72.06%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 72.24%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 73.55%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 73.74%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 73.80%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 73.86%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 73.89%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 74.14%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 74.45%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 74.25%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 74.16%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 73.96%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 73.79%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 73.60%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 73.39%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 73.30%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 73.28%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 73.20%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.08%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.07%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 72.96%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 72.94%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 72.93%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 73.39%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 73.24%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 73.18%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 73.14%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 73.01%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 72.91%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 72.85%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 72.56%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 72.41%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 72.20%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 71.95%   [EVAL] batch:  287 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 71.86%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 71.79%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 71.68%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 71.64%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 71.53%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 71.46%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.16%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 72.42%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 72.27%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 72.10%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 71.91%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 71.75%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 71.56%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 71.49%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 71.82%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 71.75%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 71.69%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.66%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 72.30%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 72.53%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 72.83%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 72.67%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 72.59%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 72.53%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 72.36%   [EVAL] batch:  355 | acc: 25.00%,  total acc: 72.23%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 72.27%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 72.63%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 72.59%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 72.54%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 72.42%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 72.34%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 72.34%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 72.31%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 72.40%   
cur_acc:  ['0.9484', '0.7014', '0.7252', '0.7917', '0.6935', '0.7123']
his_acc:  ['0.9484', '0.8215', '0.7846', '0.7768', '0.7554', '0.7240']
Clustering into  34  clusters
Clusters:  [ 0 14 21  0  0  0 28  0 18 32  0  0  0 23  0 19  0 25 31 15 27 24  2  0
  0 17  0 11  0 33 16 13  0  0 22 26 29  0  0 30 14 10  0  0  0  6  0  0
 12  5  6 20  0  7  9  0  0  0  8  4  3  0  0  2  2  1  0  0  0  0]
Losses:  6.917962074279785 1.358903169631958
CurrentTrain: epoch  0, batch     0 | loss: 8.2768650Losses:  9.464099884033203 1.493194341659546
CurrentTrain: epoch  0, batch     1 | loss: 10.9572945Losses:  7.474493026733398 1.6013455390930176
CurrentTrain: epoch  0, batch     2 | loss: 9.0758381Losses:  7.718289375305176 0.4113016128540039
CurrentTrain: epoch  0, batch     3 | loss: 8.1295910Losses:  4.159124374389648 1.6372150182724
CurrentTrain: epoch  1, batch     0 | loss: 5.7963395Losses:  3.964252471923828 1.362199306488037
CurrentTrain: epoch  1, batch     1 | loss: 5.3264518Losses:  3.5588040351867676 1.5165332555770874
CurrentTrain: epoch  1, batch     2 | loss: 5.0753374Losses:  4.273033618927002 0.41077858209609985
CurrentTrain: epoch  1, batch     3 | loss: 4.6838121Losses:  3.445596694946289 1.3126702308654785
CurrentTrain: epoch  2, batch     0 | loss: 4.7582669Losses:  3.425370931625366 1.3400212526321411
CurrentTrain: epoch  2, batch     1 | loss: 4.7653923Losses:  3.5375757217407227 1.561879277229309
CurrentTrain: epoch  2, batch     2 | loss: 5.0994549Losses:  4.643834114074707 0.43307948112487793
CurrentTrain: epoch  2, batch     3 | loss: 5.0769138Losses:  3.431854248046875 1.2511279582977295
CurrentTrain: epoch  3, batch     0 | loss: 4.6829824Losses:  3.2694239616394043 1.3571592569351196
CurrentTrain: epoch  3, batch     1 | loss: 4.6265831Losses:  2.901555299758911 1.2283426523208618
CurrentTrain: epoch  3, batch     2 | loss: 4.1298981Losses:  2.5096473693847656 0.1274939477443695
CurrentTrain: epoch  3, batch     3 | loss: 2.6371412Losses:  3.2385244369506836 1.3296293020248413
CurrentTrain: epoch  4, batch     0 | loss: 4.5681539Losses:  2.588980197906494 1.0064976215362549
CurrentTrain: epoch  4, batch     1 | loss: 3.5954778Losses:  2.911276340484619 1.2835636138916016
CurrentTrain: epoch  4, batch     2 | loss: 4.1948400Losses:  4.434073448181152 0.3278074264526367
CurrentTrain: epoch  4, batch     3 | loss: 4.7618809Losses:  3.0392544269561768 1.0651590824127197
CurrentTrain: epoch  5, batch     0 | loss: 4.1044135Losses:  2.6905617713928223 1.1267125606536865
CurrentTrain: epoch  5, batch     1 | loss: 3.8172743Losses:  2.718721866607666 1.2102049589157104
CurrentTrain: epoch  5, batch     2 | loss: 3.9289269Losses:  2.257528305053711 0.1663576066493988
CurrentTrain: epoch  5, batch     3 | loss: 2.4238858Losses:  2.886380910873413 1.126835584640503
CurrentTrain: epoch  6, batch     0 | loss: 4.0132165Losses:  2.75839900970459 0.9430464506149292
CurrentTrain: epoch  6, batch     1 | loss: 3.7014456Losses:  2.24759578704834 0.8314507603645325
CurrentTrain: epoch  6, batch     2 | loss: 3.0790465Losses:  3.6812174320220947 0.2894730567932129
CurrentTrain: epoch  6, batch     3 | loss: 3.9706905Losses:  2.2423267364501953 0.9366169571876526
CurrentTrain: epoch  7, batch     0 | loss: 3.1789436Losses:  2.717740535736084 0.9461872577667236
CurrentTrain: epoch  7, batch     1 | loss: 3.6639278Losses:  2.504340171813965 1.1852049827575684
CurrentTrain: epoch  7, batch     2 | loss: 3.6895452Losses:  3.083178997039795 0.8510289192199707
CurrentTrain: epoch  7, batch     3 | loss: 3.9342079Losses:  2.4413459300994873 1.059668779373169
CurrentTrain: epoch  8, batch     0 | loss: 3.5010147Losses:  2.419124126434326 0.7591233849525452
CurrentTrain: epoch  8, batch     1 | loss: 3.1782475Losses:  2.4634509086608887 1.1082513332366943
CurrentTrain: epoch  8, batch     2 | loss: 3.5717022Losses:  1.958857536315918 0.11397911608219147
CurrentTrain: epoch  8, batch     3 | loss: 2.0728366Losses:  2.161424160003662 1.023496150970459
CurrentTrain: epoch  9, batch     0 | loss: 3.1849203Losses:  2.5080018043518066 0.9443155527114868
CurrentTrain: epoch  9, batch     1 | loss: 3.4523172Losses:  2.308588743209839 0.8955272436141968
CurrentTrain: epoch  9, batch     2 | loss: 3.2041159Losses:  1.926231026649475 0.4521867036819458
CurrentTrain: epoch  9, batch     3 | loss: 2.3784177
Losses:  6.083354949951172 0.7268743515014648
MemoryTrain:  epoch  0, batch     0 | loss: 6.8102293Losses:  9.275075912475586 0.9025851488113403
MemoryTrain:  epoch  0, batch     1 | loss: 10.1776609Losses:  9.787044525146484 0.906941831111908
MemoryTrain:  epoch  0, batch     2 | loss: 10.6939859Losses:  10.530477523803711 0.7659968137741089
MemoryTrain:  epoch  0, batch     3 | loss: 11.2964745Losses:  10.416121482849121 0.1935894787311554
MemoryTrain:  epoch  0, batch     4 | loss: 10.6097107Losses:  0.7770308256149292 0.518458366394043
MemoryTrain:  epoch  1, batch     0 | loss: 1.2954892Losses:  1.191011905670166 0.693549394607544
MemoryTrain:  epoch  1, batch     1 | loss: 1.8845613Losses:  0.650273859500885 0.7808677554130554
MemoryTrain:  epoch  1, batch     2 | loss: 1.4311416Losses:  1.2960503101348877 0.8132522106170654
MemoryTrain:  epoch  1, batch     3 | loss: 2.1093025Losses:  0.7558571696281433 0.5755072832107544
MemoryTrain:  epoch  1, batch     4 | loss: 1.3313644Losses:  0.883857786655426 0.7059030532836914
MemoryTrain:  epoch  2, batch     0 | loss: 1.5897608Losses:  0.5712199807167053 0.7224397659301758
MemoryTrain:  epoch  2, batch     1 | loss: 1.2936597Losses:  0.824589192867279 0.8041229844093323
MemoryTrain:  epoch  2, batch     2 | loss: 1.6287122Losses:  0.8299844264984131 0.8803994059562683
MemoryTrain:  epoch  2, batch     3 | loss: 1.7103839Losses:  0.7581431269645691 0.43019136786460876
MemoryTrain:  epoch  2, batch     4 | loss: 1.1883345Losses:  0.47749388217926025 0.7999303340911865
MemoryTrain:  epoch  3, batch     0 | loss: 1.2774242Losses:  0.7228690981864929 0.7359409332275391
MemoryTrain:  epoch  3, batch     1 | loss: 1.4588101Losses:  0.6584831476211548 0.6460549831390381
MemoryTrain:  epoch  3, batch     2 | loss: 1.3045381Losses:  0.7590952515602112 0.8281523585319519
MemoryTrain:  epoch  3, batch     3 | loss: 1.5872476Losses:  0.6652284860610962 0.32402628660202026
MemoryTrain:  epoch  3, batch     4 | loss: 0.9892548Losses:  0.5005528926849365 0.5873944759368896
MemoryTrain:  epoch  4, batch     0 | loss: 1.0879474Losses:  0.5048120021820068 0.6902697086334229
MemoryTrain:  epoch  4, batch     1 | loss: 1.1950817Losses:  0.36238160729408264 0.6708400845527649
MemoryTrain:  epoch  4, batch     2 | loss: 1.0332217Losses:  0.6874690055847168 0.9281599521636963
MemoryTrain:  epoch  4, batch     3 | loss: 1.6156290Losses:  0.5121897459030151 0.4805753231048584
MemoryTrain:  epoch  4, batch     4 | loss: 0.9927651Losses:  0.46890199184417725 0.6698542833328247
MemoryTrain:  epoch  5, batch     0 | loss: 1.1387563Losses:  0.5216405391693115 0.843284010887146
MemoryTrain:  epoch  5, batch     1 | loss: 1.3649246Losses:  0.45770639181137085 0.8191921710968018
MemoryTrain:  epoch  5, batch     2 | loss: 1.2768986Losses:  0.5403906106948853 0.758033037185669
MemoryTrain:  epoch  5, batch     3 | loss: 1.2984236Losses:  0.34838852286338806 0.20469766855239868
MemoryTrain:  epoch  5, batch     4 | loss: 0.5530862Losses:  0.4637063443660736 0.7084499597549438
MemoryTrain:  epoch  6, batch     0 | loss: 1.1721563Losses:  0.4156501293182373 0.6028763651847839
MemoryTrain:  epoch  6, batch     1 | loss: 1.0185266Losses:  0.5730808973312378 0.8149092197418213
MemoryTrain:  epoch  6, batch     2 | loss: 1.3879901Losses:  0.42767295241355896 0.7125059962272644
MemoryTrain:  epoch  6, batch     3 | loss: 1.1401789Losses:  0.6569596529006958 0.3083648979663849
MemoryTrain:  epoch  6, batch     4 | loss: 0.9653245Losses:  0.4273127615451813 0.6377378106117249
MemoryTrain:  epoch  7, batch     0 | loss: 1.0650506Losses:  0.5711123943328857 0.7926641702651978
MemoryTrain:  epoch  7, batch     1 | loss: 1.3637766Losses:  0.42187562584877014 0.6892017126083374
MemoryTrain:  epoch  7, batch     2 | loss: 1.1110773Losses:  0.4303876459598541 0.6427955627441406
MemoryTrain:  epoch  7, batch     3 | loss: 1.0731832Losses:  0.34500575065612793 0.18014198541641235
MemoryTrain:  epoch  7, batch     4 | loss: 0.5251477Losses:  0.32904908061027527 0.5620095729827881
MemoryTrain:  epoch  8, batch     0 | loss: 0.8910587Losses:  0.5292929410934448 0.8687525391578674
MemoryTrain:  epoch  8, batch     1 | loss: 1.3980455Losses:  0.4518333077430725 0.7775633931159973
MemoryTrain:  epoch  8, batch     2 | loss: 1.2293967Losses:  0.4349904954433441 0.5621420741081238
MemoryTrain:  epoch  8, batch     3 | loss: 0.9971325Losses:  0.42857474088668823 0.26327478885650635
MemoryTrain:  epoch  8, batch     4 | loss: 0.6918495Losses:  0.4792224168777466 0.7769057154655457
MemoryTrain:  epoch  9, batch     0 | loss: 1.2561281Losses:  0.33713674545288086 0.6166952848434448
MemoryTrain:  epoch  9, batch     1 | loss: 0.9538320Losses:  0.42969852685928345 0.7064939737319946
MemoryTrain:  epoch  9, batch     2 | loss: 1.1361926Losses:  0.40558627247810364 0.6275730729103088
MemoryTrain:  epoch  9, batch     3 | loss: 1.0331594Losses:  0.3596348762512207 0.24052727222442627
MemoryTrain:  epoch  9, batch     4 | loss: 0.6001621
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 28.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 53.80%   [EVAL] batch:   23 | acc: 6.25%,  total acc: 51.82%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 50.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 49.04%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 47.22%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 45.54%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 43.97%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 42.71%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 41.33%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 41.99%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 43.37%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 44.12%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 45.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 46.18%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 47.30%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 48.52%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 49.36%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 50.62%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 51.68%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 52.53%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 53.49%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 54.40%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 55.57%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 55.98%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 56.77%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 57.53%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 57.97%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 58.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 59.20%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.60%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 59.16%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 59.11%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.22%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 59.27%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.73%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.54%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.89%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.22%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 87.62%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 86.72%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 85.96%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 85.02%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 83.71%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 82.94%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 82.91%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 82.86%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 82.93%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 82.70%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.28%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.25%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 82.98%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 82.55%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 82.45%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 82.36%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 82.25%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 82.01%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 81.78%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 81.55%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 81.18%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 80.53%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 80.33%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 79.99%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 79.72%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 79.14%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 78.56%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 78.06%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 77.67%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 77.58%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 77.17%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 76.69%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.49%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 76.23%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 75.91%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.78%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 75.65%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.12%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.54%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.03%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 73.41%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 72.92%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 72.35%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 73.80%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 73.76%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 73.82%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 73.90%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 73.62%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 73.35%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 73.27%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 73.23%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.39%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.54%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.55%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.67%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 73.82%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 73.38%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.08%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 72.65%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 72.27%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 72.07%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.83%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.55%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.43%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 71.96%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 71.53%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 71.19%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 70.73%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.27%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.41%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.70%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.85%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.97%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 70.54%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 70.34%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 70.03%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 69.84%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 69.57%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 69.34%   [EVAL] batch:  169 | acc: 56.25%,  total acc: 69.26%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 69.04%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 68.89%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 68.64%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 68.78%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 68.92%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 69.19%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.42%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 69.55%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 69.68%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 70.02%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 70.09%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 70.15%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 69.97%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 69.78%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 69.68%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 69.61%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 69.39%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 69.14%   [EVAL] batch:  206 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 68.66%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 68.36%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 68.12%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 67.57%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  219 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 68.61%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 68.83%   [EVAL] batch:  224 | acc: 62.50%,  total acc: 68.81%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 70.46%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.93%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  250 | acc: 25.00%,  total acc: 71.17%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 71.06%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 70.87%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 70.72%   [EVAL] batch:  254 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 70.21%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 70.20%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.20%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 70.10%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.99%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 69.90%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 69.85%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 70.31%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 70.26%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 70.11%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 70.02%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 69.90%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 69.67%   [EVAL] batch:  284 | acc: 18.75%,  total acc: 69.50%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 69.30%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 69.05%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 68.86%   [EVAL] batch:  288 | acc: 0.00%,  total acc: 68.62%   [EVAL] batch:  289 | acc: 0.00%,  total acc: 68.38%   [EVAL] batch:  290 | acc: 0.00%,  total acc: 68.15%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 67.92%   [EVAL] batch:  292 | acc: 6.25%,  total acc: 67.70%   [EVAL] batch:  293 | acc: 12.50%,  total acc: 67.52%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 67.44%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 67.24%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 67.16%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 67.10%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 68.19%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 68.10%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 67.98%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 67.80%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 67.67%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  321 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 68.17%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.99%   [EVAL] batch:  327 | acc: 50.00%,  total acc: 67.93%   [EVAL] batch:  328 | acc: 50.00%,  total acc: 67.88%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 67.82%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 68.46%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.98%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 69.16%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 68.93%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 68.77%   [EVAL] batch:  355 | acc: 31.25%,  total acc: 68.66%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 68.78%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 68.85%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 69.01%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 68.90%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 68.83%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 68.77%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 68.77%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 68.77%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 68.73%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 68.82%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 68.67%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 68.39%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 68.27%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 68.11%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 67.95%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 68.46%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 68.35%   [EVAL] batch:  395 | acc: 12.50%,  total acc: 68.21%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 68.06%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 67.95%   [EVAL] batch:  398 | acc: 6.25%,  total acc: 67.79%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 67.53%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 67.37%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 67.20%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 67.03%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 66.88%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 66.72%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 66.76%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 66.81%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 67.34%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 67.37%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  422 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 67.54%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 67.52%   [EVAL] batch:  426 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 67.58%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 67.52%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:  434 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 67.47%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 67.37%   
cur_acc:  ['0.9484', '0.7014', '0.7252', '0.7917', '0.6935', '0.7123', '0.5873']
his_acc:  ['0.9484', '0.8215', '0.7846', '0.7768', '0.7554', '0.7240', '0.6737']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38  0  0  0 25  0 37  0 23 31 36 32 27  1  0
  0 18  0 29  0 34 35 30  0  0 28 15 13  0  0 17  5 22  0  0  0  2  0  8
 14 19  2 26  0  0  6  0  0  0 20 11 16  0  0  1  1  9  0  0  0  0 12  0
 10  0  4  7  3  0  0  0]
Losses:  7.105127334594727 1.5520009994506836
CurrentTrain: epoch  0, batch     0 | loss: 8.6571283Losses:  9.3365478515625 1.416341781616211
CurrentTrain: epoch  0, batch     1 | loss: 10.7528896Losses:  7.378547668457031 1.2622106075286865
CurrentTrain: epoch  0, batch     2 | loss: 8.6407585Losses:  4.294404983520508 0.25906556844711304
CurrentTrain: epoch  0, batch     3 | loss: 4.5534706Losses:  3.9200453758239746 1.0211586952209473
CurrentTrain: epoch  1, batch     0 | loss: 4.9412041Losses:  4.106123924255371 1.3318681716918945
CurrentTrain: epoch  1, batch     1 | loss: 5.4379921Losses:  4.057585716247559 1.1173877716064453
CurrentTrain: epoch  1, batch     2 | loss: 5.1749735Losses:  2.2310147285461426 5.960464477539063e-08
CurrentTrain: epoch  1, batch     3 | loss: 2.2310147Losses:  3.5492117404937744 1.1347092390060425
CurrentTrain: epoch  2, batch     0 | loss: 4.6839209Losses:  3.8373687267303467 1.0426013469696045
CurrentTrain: epoch  2, batch     1 | loss: 4.8799701Losses:  3.6685173511505127 0.94211745262146
CurrentTrain: epoch  2, batch     2 | loss: 4.6106348Losses:  2.663623094558716 0.220399409532547
CurrentTrain: epoch  2, batch     3 | loss: 2.8840225Losses:  3.5052380561828613 1.1246721744537354
CurrentTrain: epoch  3, batch     0 | loss: 4.6299105Losses:  3.5916833877563477 0.9584583640098572
CurrentTrain: epoch  3, batch     1 | loss: 4.5501418Losses:  3.124333620071411 0.9928882122039795
CurrentTrain: epoch  3, batch     2 | loss: 4.1172218Losses:  4.286281108856201 0.9634752869606018
CurrentTrain: epoch  3, batch     3 | loss: 5.2497563Losses:  3.138679027557373 0.850142240524292
CurrentTrain: epoch  4, batch     0 | loss: 3.9888213Losses:  2.810001850128174 0.9609131813049316
CurrentTrain: epoch  4, batch     1 | loss: 3.7709150Losses:  3.6834890842437744 1.1522201299667358
CurrentTrain: epoch  4, batch     2 | loss: 4.8357091Losses:  4.931450843811035 0.19631873071193695
CurrentTrain: epoch  4, batch     3 | loss: 5.1277695Losses:  3.11104679107666 1.0703446865081787
CurrentTrain: epoch  5, batch     0 | loss: 4.1813917Losses:  3.6151938438415527 0.766061544418335
CurrentTrain: epoch  5, batch     1 | loss: 4.3812551Losses:  2.6561131477355957 0.7668895721435547
CurrentTrain: epoch  5, batch     2 | loss: 3.4230027Losses:  1.8600003719329834 0.11744499206542969
CurrentTrain: epoch  5, batch     3 | loss: 1.9774454Losses:  3.1700310707092285 1.0374510288238525
CurrentTrain: epoch  6, batch     0 | loss: 4.2074823Losses:  3.140566110610962 0.871376633644104
CurrentTrain: epoch  6, batch     1 | loss: 4.0119429Losses:  2.4437105655670166 0.5490021705627441
CurrentTrain: epoch  6, batch     2 | loss: 2.9927127Losses:  2.1487679481506348 0.2849147617816925
CurrentTrain: epoch  6, batch     3 | loss: 2.4336827Losses:  2.8936541080474854 0.9616369009017944
CurrentTrain: epoch  7, batch     0 | loss: 3.8552909Losses:  2.7698817253112793 0.6383489370346069
CurrentTrain: epoch  7, batch     1 | loss: 3.4082308Losses:  2.559025526046753 0.885263204574585
CurrentTrain: epoch  7, batch     2 | loss: 3.4442887Losses:  2.0063886642456055 0.09932048618793488
CurrentTrain: epoch  7, batch     3 | loss: 2.1057091Losses:  2.974010467529297 0.8646258115768433
CurrentTrain: epoch  8, batch     0 | loss: 3.8386364Losses:  2.5158896446228027 0.6484937071800232
CurrentTrain: epoch  8, batch     1 | loss: 3.1643834Losses:  2.2463219165802 0.6647658944129944
CurrentTrain: epoch  8, batch     2 | loss: 2.9110878Losses:  2.8157873153686523 0.07704539597034454
CurrentTrain: epoch  8, batch     3 | loss: 2.8928328Losses:  2.068887233734131 0.515012264251709
CurrentTrain: epoch  9, batch     0 | loss: 2.5838995Losses:  2.5964531898498535 0.8774840831756592
CurrentTrain: epoch  9, batch     1 | loss: 3.4739373Losses:  2.5354316234588623 0.7341365218162537
CurrentTrain: epoch  9, batch     2 | loss: 3.2695682Losses:  2.6756792068481445 0.14548148214817047
CurrentTrain: epoch  9, batch     3 | loss: 2.8211608
Losses:  6.081937789916992 0.6685327887535095
MemoryTrain:  epoch  0, batch     0 | loss: 6.7504706Losses:  8.993090629577637 0.772270679473877
MemoryTrain:  epoch  0, batch     1 | loss: 9.7653618Losses:  9.782661437988281 1.030233383178711
MemoryTrain:  epoch  0, batch     2 | loss: 10.8128948Losses:  10.280879020690918 0.733379602432251
MemoryTrain:  epoch  0, batch     3 | loss: 11.0142584Losses:  10.708898544311523 0.6200013160705566
MemoryTrain:  epoch  0, batch     4 | loss: 11.3288994Losses:  0.927391529083252 0.6993532180786133
MemoryTrain:  epoch  1, batch     0 | loss: 1.6267447Losses:  0.6723406910896301 0.61142498254776
MemoryTrain:  epoch  1, batch     1 | loss: 1.2837657Losses:  0.6239757537841797 0.7510704398155212
MemoryTrain:  epoch  1, batch     2 | loss: 1.3750463Losses:  1.1714918613433838 0.8509225845336914
MemoryTrain:  epoch  1, batch     3 | loss: 2.0224144Losses:  0.7720319032669067 0.6572122573852539
MemoryTrain:  epoch  1, batch     4 | loss: 1.4292442Losses:  0.5872405767440796 0.7089583873748779
MemoryTrain:  epoch  2, batch     0 | loss: 1.2961990Losses:  0.9432203769683838 0.8732839822769165
MemoryTrain:  epoch  2, batch     1 | loss: 1.8165044Losses:  0.5736141800880432 0.7347723841667175
MemoryTrain:  epoch  2, batch     2 | loss: 1.3083866Losses:  0.7003033757209778 0.7465131282806396
MemoryTrain:  epoch  2, batch     3 | loss: 1.4468164Losses:  0.5908842086791992 0.5239259004592896
MemoryTrain:  epoch  2, batch     4 | loss: 1.1148101Losses:  0.577285647392273 0.8373497128486633
MemoryTrain:  epoch  3, batch     0 | loss: 1.4146354Losses:  0.6266142725944519 0.7865336537361145
MemoryTrain:  epoch  3, batch     1 | loss: 1.4131479Losses:  0.5465394258499146 0.5461350083351135
MemoryTrain:  epoch  3, batch     2 | loss: 1.0926745Losses:  0.6007992029190063 0.6876566410064697
MemoryTrain:  epoch  3, batch     3 | loss: 1.2884558Losses:  0.6052318215370178 0.6061713695526123
MemoryTrain:  epoch  3, batch     4 | loss: 1.2114031Losses:  0.5604236125946045 0.6674507260322571
MemoryTrain:  epoch  4, batch     0 | loss: 1.2278743Losses:  0.5346863269805908 0.6269219517707825
MemoryTrain:  epoch  4, batch     1 | loss: 1.1616082Losses:  0.5210174918174744 0.6243318319320679
MemoryTrain:  epoch  4, batch     2 | loss: 1.1453493Losses:  0.548164427280426 0.7462140321731567
MemoryTrain:  epoch  4, batch     3 | loss: 1.2943785Losses:  0.529812216758728 0.7625092267990112
MemoryTrain:  epoch  4, batch     4 | loss: 1.2923214Losses:  0.47692006826400757 0.47041285037994385
MemoryTrain:  epoch  5, batch     0 | loss: 0.9473329Losses:  0.4389179050922394 0.76197350025177
MemoryTrain:  epoch  5, batch     1 | loss: 1.2008914Losses:  0.4985552430152893 0.7021085023880005
MemoryTrain:  epoch  5, batch     2 | loss: 1.2006638Losses:  0.5595857501029968 0.7486706972122192
MemoryTrain:  epoch  5, batch     3 | loss: 1.3082564Losses:  0.4933852255344391 0.6880368590354919
MemoryTrain:  epoch  5, batch     4 | loss: 1.1814221Losses:  0.4491288661956787 0.6948161125183105
MemoryTrain:  epoch  6, batch     0 | loss: 1.1439450Losses:  0.451873779296875 0.5167492628097534
MemoryTrain:  epoch  6, batch     1 | loss: 0.9686230Losses:  0.42718714475631714 0.7067698240280151
MemoryTrain:  epoch  6, batch     2 | loss: 1.1339569Losses:  0.605151891708374 0.8242650032043457
MemoryTrain:  epoch  6, batch     3 | loss: 1.4294169Losses:  0.45370620489120483 0.6482270359992981
MemoryTrain:  epoch  6, batch     4 | loss: 1.1019332Losses:  0.535569429397583 0.7632108926773071
MemoryTrain:  epoch  7, batch     0 | loss: 1.2987803Losses:  0.5001020431518555 0.6563549637794495
MemoryTrain:  epoch  7, batch     1 | loss: 1.1564569Losses:  0.41719189286231995 0.5683910846710205
MemoryTrain:  epoch  7, batch     2 | loss: 0.9855829Losses:  0.5104326009750366 0.7278031706809998
MemoryTrain:  epoch  7, batch     3 | loss: 1.2382357Losses:  0.4032710790634155 0.48985907435417175
MemoryTrain:  epoch  7, batch     4 | loss: 0.8931302Losses:  0.38925260305404663 0.6520631909370422
MemoryTrain:  epoch  8, batch     0 | loss: 1.0413158Losses:  0.46615707874298096 0.679914116859436
MemoryTrain:  epoch  8, batch     1 | loss: 1.1460712Losses:  0.46694332361221313 0.557400643825531
MemoryTrain:  epoch  8, batch     2 | loss: 1.0243440Losses:  0.44086313247680664 0.6236082315444946
MemoryTrain:  epoch  8, batch     3 | loss: 1.0644714Losses:  0.46677690744400024 0.6646988391876221
MemoryTrain:  epoch  8, batch     4 | loss: 1.1314757Losses:  0.4469827115535736 0.5763278007507324
MemoryTrain:  epoch  9, batch     0 | loss: 1.0233105Losses:  0.454421728849411 0.5868847370147705
MemoryTrain:  epoch  9, batch     1 | loss: 1.0413065Losses:  0.5241511464118958 0.8288930654525757
MemoryTrain:  epoch  9, batch     2 | loss: 1.3530443Losses:  0.3454127907752991 0.5167588591575623
MemoryTrain:  epoch  9, batch     3 | loss: 0.8621716Losses:  0.47250473499298096 0.643886923789978
MemoryTrain:  epoch  9, batch     4 | loss: 1.1163917
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 55.16%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 54.43%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 54.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 58.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 71.80%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 70.97%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 68.43%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 67.27%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 66.25%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 65.16%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 64.31%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 63.39%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 86.28%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.48%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.20%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 85.80%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.60%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 84.76%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 82.60%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.17%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.96%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 81.65%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 81.64%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 81.53%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 81.71%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 82.31%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.45%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 82.60%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.67%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 82.48%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 82.06%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.97%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 81.87%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 81.71%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 81.48%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 81.32%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 81.03%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 80.96%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 80.46%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 80.26%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 79.92%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 79.58%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 79.26%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 78.94%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 78.36%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 77.79%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 77.63%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 77.34%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 77.32%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 76.98%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 76.83%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 76.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.18%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 75.79%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.66%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 75.59%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.23%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.65%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 74.14%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 73.52%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 73.03%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 72.66%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 73.62%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 73.79%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 73.98%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 74.09%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 74.09%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 74.05%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 74.01%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 73.63%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 73.55%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.69%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 73.43%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 73.04%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 72.61%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 72.14%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 71.81%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 71.48%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 72.06%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 71.59%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.12%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 70.70%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 70.24%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.79%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.82%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 70.05%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 69.73%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 69.35%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 69.05%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 68.82%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 68.57%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 68.64%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 68.50%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 68.46%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 68.32%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 68.22%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 68.06%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  187 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.43%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 69.58%   [EVAL] batch:  195 | acc: 37.50%,  total acc: 69.42%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 69.19%   [EVAL] batch:  199 | acc: 50.00%,  total acc: 69.09%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 68.94%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 68.78%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 68.57%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 68.41%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 68.29%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 68.12%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 67.88%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 67.58%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 66.77%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  219 | acc: 81.25%,  total acc: 67.73%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 67.99%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  224 | acc: 56.25%,  total acc: 68.00%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  239 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 69.68%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 69.65%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 69.70%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 69.80%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 70.16%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 70.06%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 70.00%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 69.87%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 69.74%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.63%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 69.68%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 69.60%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 69.55%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 69.41%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 69.36%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 69.29%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  273 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  275 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 69.77%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 69.72%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 69.65%   [EVAL] batch:  279 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 69.37%   [EVAL] batch:  282 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 68.93%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.73%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 68.49%   [EVAL] batch:  287 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:  288 | acc: 18.75%,  total acc: 68.12%   [EVAL] batch:  289 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:  290 | acc: 12.50%,  total acc: 67.72%   [EVAL] batch:  291 | acc: 0.00%,  total acc: 67.49%   [EVAL] batch:  292 | acc: 25.00%,  total acc: 67.34%   [EVAL] batch:  293 | acc: 12.50%,  total acc: 67.16%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 67.02%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 66.79%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 66.73%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 67.64%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 67.73%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 67.56%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 67.35%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 67.13%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 66.96%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.35%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 67.22%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 66.83%   [EVAL] batch:  329 | acc: 25.00%,  total acc: 66.70%   [EVAL] batch:  330 | acc: 37.50%,  total acc: 66.62%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.68%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.23%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 67.80%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 67.97%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 67.74%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 67.59%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 67.50%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 67.84%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 67.81%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 67.75%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 67.68%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.58%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 67.57%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 67.50%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 67.36%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 67.20%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 67.05%   [EVAL] batch:  379 | acc: 6.25%,  total acc: 66.89%   [EVAL] batch:  380 | acc: 6.25%,  total acc: 66.73%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  388 | acc: 81.25%,  total acc: 67.06%   [EVAL] batch:  389 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  390 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:  394 | acc: 25.00%,  total acc: 67.17%   [EVAL] batch:  395 | acc: 18.75%,  total acc: 67.05%   [EVAL] batch:  396 | acc: 6.25%,  total acc: 66.89%   [EVAL] batch:  397 | acc: 25.00%,  total acc: 66.79%   [EVAL] batch:  398 | acc: 6.25%,  total acc: 66.64%   [EVAL] batch:  399 | acc: 12.50%,  total acc: 66.50%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 66.35%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 65.86%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 65.73%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 65.63%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 66.05%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.10%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 66.28%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  423 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 66.37%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 66.29%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 66.32%   [EVAL] batch:  432 | acc: 43.75%,  total acc: 66.27%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 66.24%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 66.21%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  437 | acc: 25.00%,  total acc: 66.11%   [EVAL] batch:  438 | acc: 0.00%,  total acc: 65.96%   [EVAL] batch:  439 | acc: 18.75%,  total acc: 65.85%   [EVAL] batch:  440 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  441 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:  442 | acc: 18.75%,  total acc: 65.60%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 65.55%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 65.60%   [EVAL] batch:  446 | acc: 37.50%,  total acc: 65.53%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  448 | acc: 50.00%,  total acc: 65.52%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  452 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  457 | acc: 37.50%,  total acc: 65.82%   [EVAL] batch:  458 | acc: 31.25%,  total acc: 65.74%   [EVAL] batch:  459 | acc: 43.75%,  total acc: 65.69%   [EVAL] batch:  460 | acc: 25.00%,  total acc: 65.60%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 65.56%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 65.68%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  475 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  477 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  480 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  481 | acc: 37.50%,  total acc: 66.65%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  483 | acc: 43.75%,  total acc: 66.53%   [EVAL] batch:  484 | acc: 31.25%,  total acc: 66.46%   [EVAL] batch:  485 | acc: 50.00%,  total acc: 66.42%   [EVAL] batch:  486 | acc: 43.75%,  total acc: 66.38%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 66.46%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  494 | acc: 0.00%,  total acc: 66.50%   [EVAL] batch:  495 | acc: 6.25%,  total acc: 66.38%   [EVAL] batch:  496 | acc: 0.00%,  total acc: 66.25%   [EVAL] batch:  497 | acc: 6.25%,  total acc: 66.13%   [EVAL] batch:  498 | acc: 6.25%,  total acc: 66.01%   [EVAL] batch:  499 | acc: 12.50%,  total acc: 65.90%   
cur_acc:  ['0.9484', '0.7014', '0.7252', '0.7917', '0.6935', '0.7123', '0.5873', '0.6339']
his_acc:  ['0.9484', '0.8215', '0.7846', '0.7768', '0.7554', '0.7240', '0.6737', '0.6590']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.830145835876465 2.1285247802734375
CurrentTrain: epoch  0, batch     0 | loss: 13.9586706Losses:  13.127245903015137 2.184195041656494
CurrentTrain: epoch  0, batch     1 | loss: 15.3114414Losses:  13.462050437927246 1.7033047676086426
CurrentTrain: epoch  0, batch     2 | loss: 15.1653557Losses:  13.59020709991455 2.0126280784606934
CurrentTrain: epoch  0, batch     3 | loss: 15.6028347Losses:  13.367920875549316 1.7004156112670898
CurrentTrain: epoch  0, batch     4 | loss: 15.0683365Losses:  13.23577880859375 1.8429386615753174
CurrentTrain: epoch  0, batch     5 | loss: 15.0787172Losses:  13.32376480102539 1.6395988464355469
CurrentTrain: epoch  0, batch     6 | loss: 14.9633636Losses:  12.496360778808594 1.4828698635101318
CurrentTrain: epoch  0, batch     7 | loss: 13.9792309Losses:  12.853193283081055 1.6261506080627441
CurrentTrain: epoch  0, batch     8 | loss: 14.4793434Losses:  12.440279006958008 1.728280782699585
CurrentTrain: epoch  0, batch     9 | loss: 14.1685600Losses:  12.626991271972656 1.6021649837493896
CurrentTrain: epoch  0, batch    10 | loss: 14.2291565Losses:  12.099175453186035 1.5808796882629395
CurrentTrain: epoch  0, batch    11 | loss: 13.6800556Losses:  11.966527938842773 1.7590153217315674
CurrentTrain: epoch  0, batch    12 | loss: 13.7255430Losses:  12.06827449798584 1.7368581295013428
CurrentTrain: epoch  0, batch    13 | loss: 13.8051329Losses:  11.450949668884277 1.3452723026275635
CurrentTrain: epoch  0, batch    14 | loss: 12.7962217Losses:  11.967366218566895 1.5837702751159668
CurrentTrain: epoch  0, batch    15 | loss: 13.5511360Losses:  11.634916305541992 1.8824331760406494
CurrentTrain: epoch  0, batch    16 | loss: 13.5173492Losses:  11.739886283874512 1.6322071552276611
CurrentTrain: epoch  0, batch    17 | loss: 13.3720932Losses:  11.257622718811035 1.6386656761169434
CurrentTrain: epoch  0, batch    18 | loss: 12.8962879Losses:  11.451600074768066 1.7168503999710083
CurrentTrain: epoch  0, batch    19 | loss: 13.1684504Losses:  11.00656509399414 1.6610589027404785
CurrentTrain: epoch  0, batch    20 | loss: 12.6676235Losses:  10.932619094848633 1.613537311553955
CurrentTrain: epoch  0, batch    21 | loss: 12.5461559Losses:  10.875045776367188 1.172759771347046
CurrentTrain: epoch  0, batch    22 | loss: 12.0478058Losses:  10.23293685913086 1.3687466382980347
CurrentTrain: epoch  0, batch    23 | loss: 11.6016836Losses:  10.717792510986328 1.4922242164611816
CurrentTrain: epoch  0, batch    24 | loss: 12.2100163Losses:  10.901498794555664 1.1797101497650146
CurrentTrain: epoch  0, batch    25 | loss: 12.0812092Losses:  10.429570198059082 1.4193251132965088
CurrentTrain: epoch  0, batch    26 | loss: 11.8488951Losses:  10.468903541564941 1.6420042514801025
CurrentTrain: epoch  0, batch    27 | loss: 12.1109076Losses:  10.367584228515625 1.3479092121124268
CurrentTrain: epoch  0, batch    28 | loss: 11.7154932Losses:  10.275592803955078 1.1961504220962524
CurrentTrain: epoch  0, batch    29 | loss: 11.4717436Losses:  9.780624389648438 1.326341152191162
CurrentTrain: epoch  0, batch    30 | loss: 11.1069660Losses:  9.80239200592041 1.6662623882293701
CurrentTrain: epoch  0, batch    31 | loss: 11.4686546Losses:  9.736530303955078 1.4544907808303833
CurrentTrain: epoch  0, batch    32 | loss: 11.1910210Losses:  9.713184356689453 1.352602243423462
CurrentTrain: epoch  0, batch    33 | loss: 11.0657864Losses:  9.188748359680176 1.2099477052688599
CurrentTrain: epoch  0, batch    34 | loss: 10.3986959Losses:  9.839923858642578 1.1216213703155518
CurrentTrain: epoch  0, batch    35 | loss: 10.9615450Losses:  10.083483695983887 1.1377768516540527
CurrentTrain: epoch  0, batch    36 | loss: 11.2212601Losses:  9.036510467529297 1.2976410388946533
CurrentTrain: epoch  0, batch    37 | loss: 10.3341513Losses:  9.135607719421387 1.251861810684204
CurrentTrain: epoch  0, batch    38 | loss: 10.3874693Losses:  8.541162490844727 1.319203495979309
CurrentTrain: epoch  0, batch    39 | loss: 9.8603659Losses:  8.642080307006836 1.257788896560669
CurrentTrain: epoch  0, batch    40 | loss: 9.8998690Losses:  8.855681419372559 1.216399073600769
CurrentTrain: epoch  0, batch    41 | loss: 10.0720806Losses:  8.396794319152832 1.0762594938278198
CurrentTrain: epoch  0, batch    42 | loss: 9.4730539Losses:  8.03099250793457 0.9938790798187256
CurrentTrain: epoch  0, batch    43 | loss: 9.0248718Losses:  8.227575302124023 1.0704236030578613
CurrentTrain: epoch  0, batch    44 | loss: 9.2979984Losses:  8.1326904296875 1.0964683294296265
CurrentTrain: epoch  0, batch    45 | loss: 9.2291584Losses:  8.448835372924805 1.1536139249801636
CurrentTrain: epoch  0, batch    46 | loss: 9.6024494Losses:  7.787393569946289 1.1273468732833862
CurrentTrain: epoch  0, batch    47 | loss: 8.9147406Losses:  7.8676838874816895 1.1070680618286133
CurrentTrain: epoch  0, batch    48 | loss: 8.9747524Losses:  7.411367416381836 1.1690770387649536
CurrentTrain: epoch  0, batch    49 | loss: 8.5804443Losses:  8.128297805786133 0.8301994204521179
CurrentTrain: epoch  0, batch    50 | loss: 8.9584970Losses:  7.281013011932373 0.9113562107086182
CurrentTrain: epoch  0, batch    51 | loss: 8.1923695Losses:  7.232476711273193 1.0721790790557861
CurrentTrain: epoch  0, batch    52 | loss: 8.3046560Losses:  7.181459426879883 0.8797974586486816
CurrentTrain: epoch  0, batch    53 | loss: 8.0612564Losses:  6.956089496612549 1.0184719562530518
CurrentTrain: epoch  0, batch    54 | loss: 7.9745617Losses:  7.48345947265625 1.2253021001815796
CurrentTrain: epoch  0, batch    55 | loss: 8.7087612Losses:  6.513228416442871 0.9968590140342712
CurrentTrain: epoch  0, batch    56 | loss: 7.5100875Losses:  6.295203685760498 0.7437750101089478
CurrentTrain: epoch  0, batch    57 | loss: 7.0389786Losses:  6.519851207733154 0.9357176423072815
CurrentTrain: epoch  0, batch    58 | loss: 7.4555688Losses:  6.94610595703125 0.9028340578079224
CurrentTrain: epoch  0, batch    59 | loss: 7.8489399Losses:  6.287077903747559 0.9015846848487854
CurrentTrain: epoch  0, batch    60 | loss: 7.1886625Losses:  6.139311790466309 0.9040497541427612
CurrentTrain: epoch  0, batch    61 | loss: 7.0433617Losses:  6.105490207672119 0.8573497533798218
CurrentTrain: epoch  0, batch    62 | loss: 6.9628401Losses:  6.282999038696289 0.8614630699157715
CurrentTrain: epoch  1, batch     0 | loss: 7.1444621Losses:  5.467805862426758 0.749612033367157
CurrentTrain: epoch  1, batch     1 | loss: 6.2174177Losses:  5.702075004577637 0.9934273958206177
CurrentTrain: epoch  1, batch     2 | loss: 6.6955023Losses:  5.74937629699707 0.8151977062225342
CurrentTrain: epoch  1, batch     3 | loss: 6.5645742Losses:  5.340459823608398 0.7924820184707642
CurrentTrain: epoch  1, batch     4 | loss: 6.1329417Losses:  5.331602096557617 0.5698598027229309
CurrentTrain: epoch  1, batch     5 | loss: 5.9014621Losses:  5.662351131439209 0.6399357318878174
CurrentTrain: epoch  1, batch     6 | loss: 6.3022871Losses:  5.813686847686768 0.9323741793632507
CurrentTrain: epoch  1, batch     7 | loss: 6.7460608Losses:  5.611386299133301 0.8858962059020996
CurrentTrain: epoch  1, batch     8 | loss: 6.4972825Losses:  5.490228652954102 0.629899263381958
CurrentTrain: epoch  1, batch     9 | loss: 6.1201277Losses:  5.588027000427246 0.8140174150466919
CurrentTrain: epoch  1, batch    10 | loss: 6.4020443Losses:  6.029108047485352 0.870615541934967
CurrentTrain: epoch  1, batch    11 | loss: 6.8997235Losses:  5.464326858520508 0.8157287836074829
CurrentTrain: epoch  1, batch    12 | loss: 6.2800555Losses:  5.855007171630859 0.8928441405296326
CurrentTrain: epoch  1, batch    13 | loss: 6.7478514Losses:  5.835630893707275 0.9286800026893616
CurrentTrain: epoch  1, batch    14 | loss: 6.7643108Losses:  5.694173812866211 0.7979576587677002
CurrentTrain: epoch  1, batch    15 | loss: 6.4921312Losses:  5.318653106689453 0.6870492696762085
CurrentTrain: epoch  1, batch    16 | loss: 6.0057025Losses:  6.00568962097168 0.7157367467880249
CurrentTrain: epoch  1, batch    17 | loss: 6.7214265Losses:  5.520366668701172 0.8020442128181458
CurrentTrain: epoch  1, batch    18 | loss: 6.3224111Losses:  5.442984580993652 0.7946650385856628
CurrentTrain: epoch  1, batch    19 | loss: 6.2376494Losses:  5.646713733673096 0.7944729328155518
CurrentTrain: epoch  1, batch    20 | loss: 6.4411869Losses:  5.535505294799805 0.6644247174263
CurrentTrain: epoch  1, batch    21 | loss: 6.1999302Losses:  5.54871940612793 0.7471730709075928
CurrentTrain: epoch  1, batch    22 | loss: 6.2958927Losses:  5.625242710113525 0.7430853843688965
CurrentTrain: epoch  1, batch    23 | loss: 6.3683281Losses:  5.665595054626465 0.7472093105316162
CurrentTrain: epoch  1, batch    24 | loss: 6.4128046Losses:  5.235037326812744 0.7553094625473022
CurrentTrain: epoch  1, batch    25 | loss: 5.9903469Losses:  5.72378396987915 0.794916570186615
CurrentTrain: epoch  1, batch    26 | loss: 6.5187006Losses:  5.042812347412109 0.4560301899909973
CurrentTrain: epoch  1, batch    27 | loss: 5.4988427Losses:  5.11528205871582 0.4653930068016052
CurrentTrain: epoch  1, batch    28 | loss: 5.5806751Losses:  5.35139274597168 0.7015019655227661
CurrentTrain: epoch  1, batch    29 | loss: 6.0528946Losses:  5.250645637512207 0.6725745797157288
CurrentTrain: epoch  1, batch    30 | loss: 5.9232202Losses:  5.6283159255981445 0.6778422594070435
CurrentTrain: epoch  1, batch    31 | loss: 6.3061581Losses:  5.077098846435547 0.38949668407440186
CurrentTrain: epoch  1, batch    32 | loss: 5.4665956Losses:  5.382965087890625 0.7032725811004639
CurrentTrain: epoch  1, batch    33 | loss: 6.0862379Losses:  5.3536224365234375 0.5373598337173462
CurrentTrain: epoch  1, batch    34 | loss: 5.8909822Losses:  5.184666633605957 0.6401312351226807
CurrentTrain: epoch  1, batch    35 | loss: 5.8247976Losses:  5.281641960144043 0.6946723461151123
CurrentTrain: epoch  1, batch    36 | loss: 5.9763145Losses:  5.277267932891846 0.7101172208786011
CurrentTrain: epoch  1, batch    37 | loss: 5.9873853Losses:  5.396029472351074 0.7083455324172974
CurrentTrain: epoch  1, batch    38 | loss: 6.1043749Losses:  4.827162742614746 0.40817928314208984
CurrentTrain: epoch  1, batch    39 | loss: 5.2353420Losses:  5.4167938232421875 0.6229404211044312
CurrentTrain: epoch  1, batch    40 | loss: 6.0397344Losses:  5.490801811218262 0.7171372175216675
CurrentTrain: epoch  1, batch    41 | loss: 6.2079391Losses:  4.994065761566162 0.6899161338806152
CurrentTrain: epoch  1, batch    42 | loss: 5.6839819Losses:  5.32676887512207 0.6329919695854187
CurrentTrain: epoch  1, batch    43 | loss: 5.9597607Losses:  5.700651168823242 0.7289276123046875
CurrentTrain: epoch  1, batch    44 | loss: 6.4295788Losses:  5.060708522796631 0.5862166285514832
CurrentTrain: epoch  1, batch    45 | loss: 5.6469250Losses:  5.123409271240234 0.5223701000213623
CurrentTrain: epoch  1, batch    46 | loss: 5.6457796Losses:  5.1068034172058105 0.5413874387741089
CurrentTrain: epoch  1, batch    47 | loss: 5.6481910Losses:  5.442725658416748 0.5463204383850098
CurrentTrain: epoch  1, batch    48 | loss: 5.9890461Losses:  4.850257873535156 0.474040687084198
CurrentTrain: epoch  1, batch    49 | loss: 5.3242984Losses:  4.928037643432617 0.5273423790931702
CurrentTrain: epoch  1, batch    50 | loss: 5.4553800Losses:  4.6307783126831055 0.2625778913497925
CurrentTrain: epoch  1, batch    51 | loss: 4.8933563Losses:  4.965951919555664 0.40425407886505127
CurrentTrain: epoch  1, batch    52 | loss: 5.3702059Losses:  4.7212982177734375 0.515630841255188
CurrentTrain: epoch  1, batch    53 | loss: 5.2369289Losses:  5.511402130126953 0.5118780136108398
CurrentTrain: epoch  1, batch    54 | loss: 6.0232801Losses:  4.683038711547852 0.26423993706703186
CurrentTrain: epoch  1, batch    55 | loss: 4.9472785Losses:  5.124429702758789 0.6170748472213745
CurrentTrain: epoch  1, batch    56 | loss: 5.7415047Losses:  5.139031887054443 0.49089938402175903
CurrentTrain: epoch  1, batch    57 | loss: 5.6299314Losses:  4.990336894989014 0.46473419666290283
CurrentTrain: epoch  1, batch    58 | loss: 5.4550710Losses:  4.901102066040039 0.584663987159729
CurrentTrain: epoch  1, batch    59 | loss: 5.4857659Losses:  4.6570024490356445 0.40837839245796204
CurrentTrain: epoch  1, batch    60 | loss: 5.0653811Losses:  5.074508190155029 0.4695966839790344
CurrentTrain: epoch  1, batch    61 | loss: 5.5441051Losses:  5.010595321655273 0.2880336344242096
CurrentTrain: epoch  1, batch    62 | loss: 5.2986288Losses:  4.787981986999512 0.43167901039123535
CurrentTrain: epoch  2, batch     0 | loss: 5.2196608Losses:  4.541031360626221 0.3643479645252228
CurrentTrain: epoch  2, batch     1 | loss: 4.9053793Losses:  4.5039448738098145 0.4011969566345215
CurrentTrain: epoch  2, batch     2 | loss: 4.9051418Losses:  4.647472381591797 0.4402094781398773
CurrentTrain: epoch  2, batch     3 | loss: 5.0876818Losses:  4.5949835777282715 0.5135246515274048
CurrentTrain: epoch  2, batch     4 | loss: 5.1085081Losses:  5.021045207977295 0.4046156406402588
CurrentTrain: epoch  2, batch     5 | loss: 5.4256611Losses:  4.5323920249938965 0.41568779945373535
CurrentTrain: epoch  2, batch     6 | loss: 4.9480801Losses:  5.015438079833984 0.6189883351325989
CurrentTrain: epoch  2, batch     7 | loss: 5.6344266Losses:  5.161533355712891 0.4949515759944916
CurrentTrain: epoch  2, batch     8 | loss: 5.6564851Losses:  4.486353874206543 0.34250497817993164
CurrentTrain: epoch  2, batch     9 | loss: 4.8288589Losses:  4.712863922119141 0.38800162076950073
CurrentTrain: epoch  2, batch    10 | loss: 5.1008654Losses:  4.537171363830566 0.4324146509170532
CurrentTrain: epoch  2, batch    11 | loss: 4.9695859Losses:  4.652274131774902 0.2841227650642395
CurrentTrain: epoch  2, batch    12 | loss: 4.9363971Losses:  4.690652847290039 0.47119688987731934
CurrentTrain: epoch  2, batch    13 | loss: 5.1618500Losses:  4.800848960876465 0.3453502655029297
CurrentTrain: epoch  2, batch    14 | loss: 5.1461992Losses:  4.564974784851074 0.32971078157424927
CurrentTrain: epoch  2, batch    15 | loss: 4.8946857Losses:  4.658594131469727 0.35552507638931274
CurrentTrain: epoch  2, batch    16 | loss: 5.0141191Losses:  4.959225654602051 0.4896358251571655
CurrentTrain: epoch  2, batch    17 | loss: 5.4488616Losses:  4.676331520080566 0.42821788787841797
CurrentTrain: epoch  2, batch    18 | loss: 5.1045494Losses:  4.770432949066162 0.3517531752586365
CurrentTrain: epoch  2, batch    19 | loss: 5.1221862Losses:  5.162416458129883 0.23918665945529938
CurrentTrain: epoch  2, batch    20 | loss: 5.4016032Losses:  4.705206871032715 0.44591423869132996
CurrentTrain: epoch  2, batch    21 | loss: 5.1511211Losses:  4.357387542724609 0.3281545042991638
CurrentTrain: epoch  2, batch    22 | loss: 4.6855421Losses:  4.625313758850098 0.33804142475128174
CurrentTrain: epoch  2, batch    23 | loss: 4.9633551Losses:  4.425099849700928 0.37269458174705505
CurrentTrain: epoch  2, batch    24 | loss: 4.7977943Losses:  4.4925665855407715 0.2214498072862625
CurrentTrain: epoch  2, batch    25 | loss: 4.7140164Losses:  4.3615264892578125 0.274579256772995
CurrentTrain: epoch  2, batch    26 | loss: 4.6361055Losses:  4.521164417266846 0.313599169254303
CurrentTrain: epoch  2, batch    27 | loss: 4.8347635Losses:  4.427974224090576 0.2762598395347595
CurrentTrain: epoch  2, batch    28 | loss: 4.7042341Losses:  4.678483009338379 0.43243512511253357
CurrentTrain: epoch  2, batch    29 | loss: 5.1109180Losses:  4.642827033996582 0.3256606459617615
CurrentTrain: epoch  2, batch    30 | loss: 4.9684877Losses:  4.7725300788879395 0.4444262981414795
CurrentTrain: epoch  2, batch    31 | loss: 5.2169561Losses:  4.5438151359558105 0.2703140377998352
CurrentTrain: epoch  2, batch    32 | loss: 4.8141294Losses:  4.704099178314209 0.5094116926193237
CurrentTrain: epoch  2, batch    33 | loss: 5.2135110Losses:  5.07730770111084 0.41132813692092896
CurrentTrain: epoch  2, batch    34 | loss: 5.4886360Losses:  4.480930328369141 0.30647459626197815
CurrentTrain: epoch  2, batch    35 | loss: 4.7874050Losses:  4.8192009925842285 0.6014479398727417
CurrentTrain: epoch  2, batch    36 | loss: 5.4206491Losses:  4.47051477432251 0.29430073499679565
CurrentTrain: epoch  2, batch    37 | loss: 4.7648153Losses:  4.64954137802124 0.36770281195640564
CurrentTrain: epoch  2, batch    38 | loss: 5.0172443Losses:  4.728566646575928 0.3934350609779358
CurrentTrain: epoch  2, batch    39 | loss: 5.1220016Losses:  4.892711639404297 0.4035065770149231
CurrentTrain: epoch  2, batch    40 | loss: 5.2962184Losses:  4.355525016784668 0.23386211693286896
CurrentTrain: epoch  2, batch    41 | loss: 4.5893869Losses:  4.512360572814941 0.3124566674232483
CurrentTrain: epoch  2, batch    42 | loss: 4.8248172Losses:  4.591976165771484 0.32203298807144165
CurrentTrain: epoch  2, batch    43 | loss: 4.9140091Losses:  4.397439002990723 0.2787196934223175
CurrentTrain: epoch  2, batch    44 | loss: 4.6761589Losses:  4.323666572570801 0.17090973258018494
CurrentTrain: epoch  2, batch    45 | loss: 4.4945765Losses:  4.4847412109375 0.3792353570461273
CurrentTrain: epoch  2, batch    46 | loss: 4.8639765Losses:  4.407778739929199 0.2695324122905731
CurrentTrain: epoch  2, batch    47 | loss: 4.6773109Losses:  4.584847450256348 0.3213488757610321
CurrentTrain: epoch  2, batch    48 | loss: 4.9061961Losses:  4.565736770629883 0.2686697542667389
CurrentTrain: epoch  2, batch    49 | loss: 4.8344064Losses:  4.397745132446289 0.2836618721485138
CurrentTrain: epoch  2, batch    50 | loss: 4.6814070Losses:  4.57865047454834 0.2837130129337311
CurrentTrain: epoch  2, batch    51 | loss: 4.8623633Losses:  4.502121448516846 0.3135097622871399
CurrentTrain: epoch  2, batch    52 | loss: 4.8156314Losses:  4.461269378662109 0.23328346014022827
CurrentTrain: epoch  2, batch    53 | loss: 4.6945529Losses:  4.633025169372559 0.2429823875427246
CurrentTrain: epoch  2, batch    54 | loss: 4.8760076Losses:  4.77952241897583 0.43847396969795227
CurrentTrain: epoch  2, batch    55 | loss: 5.2179966Losses:  4.504262447357178 0.2770219147205353
CurrentTrain: epoch  2, batch    56 | loss: 4.7812843Losses:  4.692721366882324 0.3899601101875305
CurrentTrain: epoch  2, batch    57 | loss: 5.0826817Losses:  4.364899158477783 0.22392697632312775
CurrentTrain: epoch  2, batch    58 | loss: 4.5888262Losses:  4.307123184204102 0.3035617470741272
CurrentTrain: epoch  2, batch    59 | loss: 4.6106849Losses:  4.289163589477539 0.2395843267440796
CurrentTrain: epoch  2, batch    60 | loss: 4.5287480Losses:  4.258710861206055 0.2653176188468933
CurrentTrain: epoch  2, batch    61 | loss: 4.5240283Losses:  4.309211730957031 0.16934214532375336
CurrentTrain: epoch  2, batch    62 | loss: 4.4785538Losses:  4.3389692306518555 0.27356958389282227
CurrentTrain: epoch  3, batch     0 | loss: 4.6125388Losses:  4.315032482147217 0.17260031402111053
CurrentTrain: epoch  3, batch     1 | loss: 4.4876328Losses:  4.328052520751953 0.26454439759254456
CurrentTrain: epoch  3, batch     2 | loss: 4.5925970Losses:  4.217487812042236 0.22213129699230194
CurrentTrain: epoch  3, batch     3 | loss: 4.4396191Losses:  4.314748764038086 0.1900743544101715
CurrentTrain: epoch  3, batch     4 | loss: 4.5048232Losses:  4.59143590927124 0.3265775442123413
CurrentTrain: epoch  3, batch     5 | loss: 4.9180136Losses:  4.271690368652344 0.2711290419101715
CurrentTrain: epoch  3, batch     6 | loss: 4.5428195Losses:  4.315267562866211 0.22777587175369263
CurrentTrain: epoch  3, batch     7 | loss: 4.5430436Losses:  4.365661144256592 0.24477824568748474
CurrentTrain: epoch  3, batch     8 | loss: 4.6104393Losses:  4.380982875823975 0.19316129386425018
CurrentTrain: epoch  3, batch     9 | loss: 4.5741444Losses:  4.549613952636719 0.34411734342575073
CurrentTrain: epoch  3, batch    10 | loss: 4.8937311Losses:  4.247611999511719 0.19867315888404846
CurrentTrain: epoch  3, batch    11 | loss: 4.4462852Losses:  4.379793167114258 0.27728623151779175
CurrentTrain: epoch  3, batch    12 | loss: 4.6570792Losses:  4.204050064086914 0.19691205024719238
CurrentTrain: epoch  3, batch    13 | loss: 4.4009619Losses:  4.558298110961914 0.2359725832939148
CurrentTrain: epoch  3, batch    14 | loss: 4.7942705Losses:  4.882848739624023 0.41238144040107727
CurrentTrain: epoch  3, batch    15 | loss: 5.2952304Losses:  4.4296159744262695 0.2779233157634735
CurrentTrain: epoch  3, batch    16 | loss: 4.7075391Losses:  4.263734817504883 0.11888210475444794
CurrentTrain: epoch  3, batch    17 | loss: 4.3826170Losses:  4.311698913574219 0.22042962908744812
CurrentTrain: epoch  3, batch    18 | loss: 4.5321283Losses:  4.192521095275879 0.23334857821464539
CurrentTrain: epoch  3, batch    19 | loss: 4.4258695Losses:  4.192671775817871 0.2420646995306015
CurrentTrain: epoch  3, batch    20 | loss: 4.4347363Losses:  4.454306125640869 0.20469801127910614
CurrentTrain: epoch  3, batch    21 | loss: 4.6590042Losses:  4.272012710571289 0.25095731019973755
CurrentTrain: epoch  3, batch    22 | loss: 4.5229702Losses:  4.473155498504639 0.1550598442554474
CurrentTrain: epoch  3, batch    23 | loss: 4.6282153Losses:  4.249725341796875 0.203451007604599
CurrentTrain: epoch  3, batch    24 | loss: 4.4531765Losses:  4.24384880065918 0.20371919870376587
CurrentTrain: epoch  3, batch    25 | loss: 4.4475679Losses:  4.228946208953857 0.215664803981781
CurrentTrain: epoch  3, batch    26 | loss: 4.4446111Losses:  4.408496856689453 0.33709776401519775
CurrentTrain: epoch  3, batch    27 | loss: 4.7455945Losses:  4.547444820404053 0.23672565817832947
CurrentTrain: epoch  3, batch    28 | loss: 4.7841706Losses:  4.256322860717773 0.20463016629219055
CurrentTrain: epoch  3, batch    29 | loss: 4.4609532Losses:  4.183994293212891 0.2169560343027115
CurrentTrain: epoch  3, batch    30 | loss: 4.4009504Losses:  4.450017929077148 0.27068978548049927
CurrentTrain: epoch  3, batch    31 | loss: 4.7207079Losses:  4.2587127685546875 0.21411718428134918
CurrentTrain: epoch  3, batch    32 | loss: 4.4728298Losses:  4.3823041915893555 0.27143505215644836
CurrentTrain: epoch  3, batch    33 | loss: 4.6537395Losses:  4.430459499359131 0.33018553256988525
CurrentTrain: epoch  3, batch    34 | loss: 4.7606449Losses:  4.179207801818848 0.18934950232505798
CurrentTrain: epoch  3, batch    35 | loss: 4.3685575Losses:  4.32460355758667 0.20993928611278534
CurrentTrain: epoch  3, batch    36 | loss: 4.5345430Losses:  4.148678779602051 0.1590774953365326
CurrentTrain: epoch  3, batch    37 | loss: 4.3077564Losses:  4.209299087524414 0.15657323598861694
CurrentTrain: epoch  3, batch    38 | loss: 4.3658724Losses:  4.141360282897949 0.11796470731496811
CurrentTrain: epoch  3, batch    39 | loss: 4.2593250Losses:  4.355407238006592 0.25540459156036377
CurrentTrain: epoch  3, batch    40 | loss: 4.6108117Losses:  4.455615520477295 0.20917370915412903
CurrentTrain: epoch  3, batch    41 | loss: 4.6647892Losses:  4.31469202041626 0.1953134685754776
CurrentTrain: epoch  3, batch    42 | loss: 4.5100055Losses:  4.170560836791992 0.17597439885139465
CurrentTrain: epoch  3, batch    43 | loss: 4.3465352Losses:  4.174312591552734 0.18688523769378662
CurrentTrain: epoch  3, batch    44 | loss: 4.3611979Losses:  4.348239898681641 0.28255560994148254
CurrentTrain: epoch  3, batch    45 | loss: 4.6307955Losses:  4.102492332458496 0.17064014077186584
CurrentTrain: epoch  3, batch    46 | loss: 4.2731323Losses:  4.352699279785156 0.30289703607559204
CurrentTrain: epoch  3, batch    47 | loss: 4.6555963Losses:  4.2961859703063965 0.25526902079582214
CurrentTrain: epoch  3, batch    48 | loss: 4.5514550Losses:  4.087998867034912 0.15582381188869476
CurrentTrain: epoch  3, batch    49 | loss: 4.2438226Losses:  4.445281028747559 0.3019008934497833
CurrentTrain: epoch  3, batch    50 | loss: 4.7471819Losses:  4.110227584838867 0.1918867975473404
CurrentTrain: epoch  3, batch    51 | loss: 4.3021145Losses:  4.161181449890137 0.14905154705047607
CurrentTrain: epoch  3, batch    52 | loss: 4.3102331Losses:  4.318512916564941 0.28654828667640686
CurrentTrain: epoch  3, batch    53 | loss: 4.6050611Losses:  4.276935577392578 0.190652534365654
CurrentTrain: epoch  3, batch    54 | loss: 4.4675879Losses:  4.159054756164551 0.17659106850624084
CurrentTrain: epoch  3, batch    55 | loss: 4.3356457Losses:  4.1941118240356445 0.1999925673007965
CurrentTrain: epoch  3, batch    56 | loss: 4.3941045Losses:  4.027987480163574 0.18281668424606323
CurrentTrain: epoch  3, batch    57 | loss: 4.2108040Losses:  4.229762554168701 0.23760080337524414
CurrentTrain: epoch  3, batch    58 | loss: 4.4673634Losses:  4.293275356292725 0.21620461344718933
CurrentTrain: epoch  3, batch    59 | loss: 4.5094800Losses:  4.573739051818848 0.2631164789199829
CurrentTrain: epoch  3, batch    60 | loss: 4.8368554Losses:  4.189162731170654 0.21096929907798767
CurrentTrain: epoch  3, batch    61 | loss: 4.4001322Losses:  4.281612396240234 0.15575455129146576
CurrentTrain: epoch  3, batch    62 | loss: 4.4373670Losses:  4.151392459869385 0.23067902028560638
CurrentTrain: epoch  4, batch     0 | loss: 4.3820715Losses:  4.053339004516602 0.2418256402015686
CurrentTrain: epoch  4, batch     1 | loss: 4.2951646Losses:  4.404159069061279 0.2173292338848114
CurrentTrain: epoch  4, batch     2 | loss: 4.6214881Losses:  4.2431440353393555 0.20080578327178955
CurrentTrain: epoch  4, batch     3 | loss: 4.4439497Losses:  4.1617608070373535 0.2248118668794632
CurrentTrain: epoch  4, batch     4 | loss: 4.3865728Losses:  4.288060188293457 0.15421989560127258
CurrentTrain: epoch  4, batch     5 | loss: 4.4422803Losses:  4.169926643371582 0.16039054095745087
CurrentTrain: epoch  4, batch     6 | loss: 4.3303170Losses:  4.061818599700928 0.1668682098388672
CurrentTrain: epoch  4, batch     7 | loss: 4.2286868Losses:  4.330523490905762 0.22169075906276703
CurrentTrain: epoch  4, batch     8 | loss: 4.5522141Losses:  4.2738447189331055 0.2103792428970337
CurrentTrain: epoch  4, batch     9 | loss: 4.4842238Losses:  4.118500709533691 0.19421067833900452
CurrentTrain: epoch  4, batch    10 | loss: 4.3127112Losses:  4.0586090087890625 0.14311060309410095
CurrentTrain: epoch  4, batch    11 | loss: 4.2017198Losses:  4.0463104248046875 0.19537067413330078
CurrentTrain: epoch  4, batch    12 | loss: 4.2416811Losses:  4.256163597106934 0.28092074394226074
CurrentTrain: epoch  4, batch    13 | loss: 4.5370846Losses:  4.1832990646362305 0.16939567029476166
CurrentTrain: epoch  4, batch    14 | loss: 4.3526945Losses:  4.132043838500977 0.18711654841899872
CurrentTrain: epoch  4, batch    15 | loss: 4.3191605Losses:  4.125953674316406 0.20020180940628052
CurrentTrain: epoch  4, batch    16 | loss: 4.3261557Losses:  4.153601169586182 0.16710901260375977
CurrentTrain: epoch  4, batch    17 | loss: 4.3207102Losses:  4.080585956573486 0.20101556181907654
CurrentTrain: epoch  4, batch    18 | loss: 4.2816014Losses:  4.106491565704346 0.18722380697727203
CurrentTrain: epoch  4, batch    19 | loss: 4.2937155Losses:  4.080456733703613 0.17011624574661255
CurrentTrain: epoch  4, batch    20 | loss: 4.2505732Losses:  4.000436782836914 0.12708374857902527
CurrentTrain: epoch  4, batch    21 | loss: 4.1275206Losses:  4.217209815979004 0.16867448389530182
CurrentTrain: epoch  4, batch    22 | loss: 4.3858843Losses:  4.109394073486328 0.20863711833953857
CurrentTrain: epoch  4, batch    23 | loss: 4.3180313Losses:  4.091123104095459 0.21633097529411316
CurrentTrain: epoch  4, batch    24 | loss: 4.3074541Losses:  4.094923973083496 0.22964069247245789
CurrentTrain: epoch  4, batch    25 | loss: 4.3245645Losses:  4.08177375793457 0.17608857154846191
CurrentTrain: epoch  4, batch    26 | loss: 4.2578621Losses:  4.099366664886475 0.1781051754951477
CurrentTrain: epoch  4, batch    27 | loss: 4.2774720Losses:  4.154980659484863 0.1977447271347046
CurrentTrain: epoch  4, batch    28 | loss: 4.3527255Losses:  4.084162712097168 0.08497375249862671
CurrentTrain: epoch  4, batch    29 | loss: 4.1691365Losses:  4.08780574798584 0.1368604600429535
CurrentTrain: epoch  4, batch    30 | loss: 4.2246661Losses:  4.042746543884277 0.11219654977321625
CurrentTrain: epoch  4, batch    31 | loss: 4.1549430Losses:  4.126320838928223 0.17649608850479126
CurrentTrain: epoch  4, batch    32 | loss: 4.3028169Losses:  4.148684501647949 0.1261816918849945
CurrentTrain: epoch  4, batch    33 | loss: 4.2748661Losses:  4.074756622314453 0.08371228724718094
CurrentTrain: epoch  4, batch    34 | loss: 4.1584687Losses:  4.065610885620117 0.16832995414733887
CurrentTrain: epoch  4, batch    35 | loss: 4.2339411Losses:  4.017017364501953 0.12861046195030212
CurrentTrain: epoch  4, batch    36 | loss: 4.1456280Losses:  4.203212738037109 0.17935121059417725
CurrentTrain: epoch  4, batch    37 | loss: 4.3825641Losses:  4.052644729614258 0.12870793044567108
CurrentTrain: epoch  4, batch    38 | loss: 4.1813526Losses:  4.820529937744141 0.2760578989982605
CurrentTrain: epoch  4, batch    39 | loss: 5.0965877Losses:  4.081283092498779 0.16588374972343445
CurrentTrain: epoch  4, batch    40 | loss: 4.2471666Losses:  4.0465192794799805 0.1266278773546219
CurrentTrain: epoch  4, batch    41 | loss: 4.1731472Losses:  4.195396423339844 0.15701651573181152
CurrentTrain: epoch  4, batch    42 | loss: 4.3524132Losses:  4.0377912521362305 0.18160776793956757
CurrentTrain: epoch  4, batch    43 | loss: 4.2193990Losses:  4.027544021606445 0.10789217054843903
CurrentTrain: epoch  4, batch    44 | loss: 4.1354361Losses:  4.034722328186035 0.15527498722076416
CurrentTrain: epoch  4, batch    45 | loss: 4.1899972Losses:  4.090719699859619 0.13972069323062897
CurrentTrain: epoch  4, batch    46 | loss: 4.2304406Losses:  4.049894332885742 0.0776352658867836
CurrentTrain: epoch  4, batch    47 | loss: 4.1275296Losses:  4.181936740875244 0.13627733290195465
CurrentTrain: epoch  4, batch    48 | loss: 4.3182139Losses:  4.216444969177246 0.127860426902771
CurrentTrain: epoch  4, batch    49 | loss: 4.3443055Losses:  4.017477989196777 0.1464724838733673
CurrentTrain: epoch  4, batch    50 | loss: 4.1639504Losses:  4.045658111572266 0.1522781252861023
CurrentTrain: epoch  4, batch    51 | loss: 4.1979361Losses:  4.124951362609863 0.1336928755044937
CurrentTrain: epoch  4, batch    52 | loss: 4.2586441Losses:  4.127211570739746 0.17967580258846283
CurrentTrain: epoch  4, batch    53 | loss: 4.3068871Losses:  4.0267791748046875 0.17971527576446533
CurrentTrain: epoch  4, batch    54 | loss: 4.2064943Losses:  4.198740005493164 0.16186641156673431
CurrentTrain: epoch  4, batch    55 | loss: 4.3606062Losses:  4.0107421875 0.15340112149715424
CurrentTrain: epoch  4, batch    56 | loss: 4.1641431Losses:  4.028977870941162 0.16085581481456757
CurrentTrain: epoch  4, batch    57 | loss: 4.1898336Losses:  4.193784713745117 0.24995571374893188
CurrentTrain: epoch  4, batch    58 | loss: 4.4437404Losses:  4.034910202026367 0.12291660904884338
CurrentTrain: epoch  4, batch    59 | loss: 4.1578269Losses:  4.104518413543701 0.13869047164916992
CurrentTrain: epoch  4, batch    60 | loss: 4.2432089Losses:  4.008901596069336 0.1555783748626709
CurrentTrain: epoch  4, batch    61 | loss: 4.1644802Losses:  3.996687650680542 0.04964069649577141
CurrentTrain: epoch  4, batch    62 | loss: 4.0463285Losses:  4.144048690795898 0.12533314526081085
CurrentTrain: epoch  5, batch     0 | loss: 4.2693820Losses:  4.028151512145996 0.16436068713665009
CurrentTrain: epoch  5, batch     1 | loss: 4.1925120Losses:  4.070978164672852 0.11995057016611099
CurrentTrain: epoch  5, batch     2 | loss: 4.1909289Losses:  4.03666877746582 0.12801793217658997
CurrentTrain: epoch  5, batch     3 | loss: 4.1646867Losses:  4.056134223937988 0.17846906185150146
CurrentTrain: epoch  5, batch     4 | loss: 4.2346034Losses:  3.993698835372925 0.07414154708385468
CurrentTrain: epoch  5, batch     5 | loss: 4.0678406Losses:  4.070918083190918 0.16108863055706024
CurrentTrain: epoch  5, batch     6 | loss: 4.2320065Losses:  4.0945281982421875 0.16796889901161194
CurrentTrain: epoch  5, batch     7 | loss: 4.2624969Losses:  3.9973766803741455 0.17915740609169006
CurrentTrain: epoch  5, batch     8 | loss: 4.1765342Losses:  4.066832542419434 0.11347176879644394
CurrentTrain: epoch  5, batch     9 | loss: 4.1803045Losses:  3.9891724586486816 0.10785390436649323
CurrentTrain: epoch  5, batch    10 | loss: 4.0970263Losses:  4.053403854370117 0.09379834681749344
CurrentTrain: epoch  5, batch    11 | loss: 4.1472020Losses:  3.9717369079589844 0.10679186880588531
CurrentTrain: epoch  5, batch    12 | loss: 4.0785289Losses:  3.964006185531616 0.14443153142929077
CurrentTrain: epoch  5, batch    13 | loss: 4.1084375Losses:  4.0245490074157715 0.11919708549976349
CurrentTrain: epoch  5, batch    14 | loss: 4.1437459Losses:  4.0394511222839355 0.11979468911886215
CurrentTrain: epoch  5, batch    15 | loss: 4.1592460Losses:  3.9926042556762695 0.15229326486587524
CurrentTrain: epoch  5, batch    16 | loss: 4.1448975Losses:  4.4683837890625 0.21631202101707458
CurrentTrain: epoch  5, batch    17 | loss: 4.6846957Losses:  4.051192283630371 0.1319075971841812
CurrentTrain: epoch  5, batch    18 | loss: 4.1830997Losses:  4.003377437591553 0.1532406508922577
CurrentTrain: epoch  5, batch    19 | loss: 4.1566181Losses:  4.032797336578369 0.13125796616077423
CurrentTrain: epoch  5, batch    20 | loss: 4.1640553Losses:  4.0716633796691895 0.12509065866470337
CurrentTrain: epoch  5, batch    21 | loss: 4.1967540Losses:  4.156707286834717 0.18919484317302704
CurrentTrain: epoch  5, batch    22 | loss: 4.3459020Losses:  4.146672248840332 0.11964961886405945
CurrentTrain: epoch  5, batch    23 | loss: 4.2663217Losses:  4.054593563079834 0.12307781726121902
CurrentTrain: epoch  5, batch    24 | loss: 4.1776714Losses:  4.131175518035889 0.15807907283306122
CurrentTrain: epoch  5, batch    25 | loss: 4.2892547Losses:  4.025434494018555 0.1147313266992569
CurrentTrain: epoch  5, batch    26 | loss: 4.1401658Losses:  4.0628662109375 0.14788733422756195
CurrentTrain: epoch  5, batch    27 | loss: 4.2107534Losses:  4.072436332702637 0.1035519540309906
CurrentTrain: epoch  5, batch    28 | loss: 4.1759882Losses:  3.9844439029693604 0.12969976663589478
CurrentTrain: epoch  5, batch    29 | loss: 4.1141438Losses:  3.992152690887451 0.14780375361442566
CurrentTrain: epoch  5, batch    30 | loss: 4.1399565Losses:  4.021853446960449 0.13031944632530212
CurrentTrain: epoch  5, batch    31 | loss: 4.1521730Losses:  4.034650802612305 0.1398622840642929
CurrentTrain: epoch  5, batch    32 | loss: 4.1745129Losses:  4.006361484527588 0.11339960247278214
CurrentTrain: epoch  5, batch    33 | loss: 4.1197610Losses:  4.020480632781982 0.1386329084634781
CurrentTrain: epoch  5, batch    34 | loss: 4.1591134Losses:  4.026201248168945 0.16249994933605194
CurrentTrain: epoch  5, batch    35 | loss: 4.1887012Losses:  4.0614447593688965 0.11343634128570557
CurrentTrain: epoch  5, batch    36 | loss: 4.1748810Losses:  4.034296989440918 0.11876481771469116
CurrentTrain: epoch  5, batch    37 | loss: 4.1530619Losses:  3.9986255168914795 0.11764121800661087
CurrentTrain: epoch  5, batch    38 | loss: 4.1162667Losses:  4.032498836517334 0.1320759356021881
CurrentTrain: epoch  5, batch    39 | loss: 4.1645746Losses:  4.034855842590332 0.13191547989845276
CurrentTrain: epoch  5, batch    40 | loss: 4.1667714Losses:  4.007384777069092 0.12199442088603973
CurrentTrain: epoch  5, batch    41 | loss: 4.1293793Losses:  3.9995975494384766 0.13575226068496704
CurrentTrain: epoch  5, batch    42 | loss: 4.1353498Losses:  4.04892635345459 0.1251315176486969
CurrentTrain: epoch  5, batch    43 | loss: 4.1740580Losses:  3.97706937789917 0.08692881464958191
CurrentTrain: epoch  5, batch    44 | loss: 4.0639982Losses:  3.968677043914795 0.10280394554138184
CurrentTrain: epoch  5, batch    45 | loss: 4.0714808Losses:  3.974881410598755 0.1360921561717987
CurrentTrain: epoch  5, batch    46 | loss: 4.1109734Losses:  4.227867126464844 0.18350771069526672
CurrentTrain: epoch  5, batch    47 | loss: 4.4113750Losses:  3.9696056842803955 0.14047589898109436
CurrentTrain: epoch  5, batch    48 | loss: 4.1100817Losses:  3.998030424118042 0.15086916089057922
CurrentTrain: epoch  5, batch    49 | loss: 4.1488996Losses:  4.008061408996582 0.11915034055709839
CurrentTrain: epoch  5, batch    50 | loss: 4.1272116Losses:  4.046078205108643 0.1475500464439392
CurrentTrain: epoch  5, batch    51 | loss: 4.1936283Losses:  4.002655029296875 0.14268434047698975
CurrentTrain: epoch  5, batch    52 | loss: 4.1453395Losses:  4.029407978057861 0.14357689023017883
CurrentTrain: epoch  5, batch    53 | loss: 4.1729851Losses:  4.009281158447266 0.14988304674625397
CurrentTrain: epoch  5, batch    54 | loss: 4.1591644Losses:  3.9842004776000977 0.14107367396354675
CurrentTrain: epoch  5, batch    55 | loss: 4.1252742Losses:  3.9668402671813965 0.13096104562282562
CurrentTrain: epoch  5, batch    56 | loss: 4.0978012Losses:  4.070954322814941 0.11093774437904358
CurrentTrain: epoch  5, batch    57 | loss: 4.1818919Losses:  3.990818500518799 0.0739065483212471
CurrentTrain: epoch  5, batch    58 | loss: 4.0647249Losses:  4.006035804748535 0.11467427015304565
CurrentTrain: epoch  5, batch    59 | loss: 4.1207099Losses:  4.025401592254639 0.09193690121173859
CurrentTrain: epoch  5, batch    60 | loss: 4.1173387Losses:  3.9907872676849365 0.1107882708311081
CurrentTrain: epoch  5, batch    61 | loss: 4.1015754Losses:  3.9917120933532715 0.07819715887308121
CurrentTrain: epoch  5, batch    62 | loss: 4.0699091Losses:  4.005895137786865 0.1236061379313469
CurrentTrain: epoch  6, batch     0 | loss: 4.1295013Losses:  3.9790897369384766 0.08043794333934784
CurrentTrain: epoch  6, batch     1 | loss: 4.0595279Losses:  3.9992356300354004 0.10412168502807617
CurrentTrain: epoch  6, batch     2 | loss: 4.1033573Losses:  4.038642883300781 0.133542001247406
CurrentTrain: epoch  6, batch     3 | loss: 4.1721849Losses:  3.9780492782592773 0.12465979903936386
CurrentTrain: epoch  6, batch     4 | loss: 4.1027093Losses:  3.9742608070373535 0.1162319928407669
CurrentTrain: epoch  6, batch     5 | loss: 4.0904927Losses:  3.984819173812866 0.13386067748069763
CurrentTrain: epoch  6, batch     6 | loss: 4.1186800Losses:  4.020197868347168 0.10693639516830444
CurrentTrain: epoch  6, batch     7 | loss: 4.1271343Losses:  4.034913063049316 0.12196138501167297
CurrentTrain: epoch  6, batch     8 | loss: 4.1568747Losses:  4.053071022033691 0.09902685880661011
CurrentTrain: epoch  6, batch     9 | loss: 4.1520977Losses:  4.007818698883057 0.09987539798021317
CurrentTrain: epoch  6, batch    10 | loss: 4.1076941Losses:  4.013302803039551 0.11231005936861038
CurrentTrain: epoch  6, batch    11 | loss: 4.1256127Losses:  3.9868736267089844 0.09220238775014877
CurrentTrain: epoch  6, batch    12 | loss: 4.0790758Losses:  3.993101119995117 0.1279478222131729
CurrentTrain: epoch  6, batch    13 | loss: 4.1210489Losses:  3.9636878967285156 0.11921534687280655
CurrentTrain: epoch  6, batch    14 | loss: 4.0829034Losses:  3.989227533340454 0.10424073040485382
CurrentTrain: epoch  6, batch    15 | loss: 4.0934682Losses:  4.0031280517578125 0.13341931998729706
CurrentTrain: epoch  6, batch    16 | loss: 4.1365476Losses:  3.9653449058532715 0.0957629531621933
CurrentTrain: epoch  6, batch    17 | loss: 4.0611076Losses:  4.028846740722656 0.09974174201488495
CurrentTrain: epoch  6, batch    18 | loss: 4.1285887Losses:  3.9831600189208984 0.1185523122549057
CurrentTrain: epoch  6, batch    19 | loss: 4.1017122Losses:  3.9874889850616455 0.07586086541414261
CurrentTrain: epoch  6, batch    20 | loss: 4.0633497Losses:  4.247762680053711 0.1803293377161026
CurrentTrain: epoch  6, batch    21 | loss: 4.4280920Losses:  4.038661479949951 0.11247673630714417
CurrentTrain: epoch  6, batch    22 | loss: 4.1511383Losses:  4.0041985511779785 0.07766430079936981
CurrentTrain: epoch  6, batch    23 | loss: 4.0818629Losses:  4.031172752380371 0.12148177623748779
CurrentTrain: epoch  6, batch    24 | loss: 4.1526546Losses:  3.996257781982422 0.14429856836795807
CurrentTrain: epoch  6, batch    25 | loss: 4.1405563Losses:  4.020469665527344 0.0901448130607605
CurrentTrain: epoch  6, batch    26 | loss: 4.1106143Losses:  4.018852233886719 0.09098534286022186
CurrentTrain: epoch  6, batch    27 | loss: 4.1098375Losses:  3.9878602027893066 0.1223239153623581
CurrentTrain: epoch  6, batch    28 | loss: 4.1101842Losses:  4.017490863800049 0.07979684323072433
CurrentTrain: epoch  6, batch    29 | loss: 4.0972877Losses:  4.0069708824157715 0.12963436543941498
CurrentTrain: epoch  6, batch    30 | loss: 4.1366053Losses:  3.9820480346679688 0.12095706164836884
CurrentTrain: epoch  6, batch    31 | loss: 4.1030049Losses:  4.018150329589844 0.13572241365909576
CurrentTrain: epoch  6, batch    32 | loss: 4.1538730Losses:  4.0223517417907715 0.11633437871932983
CurrentTrain: epoch  6, batch    33 | loss: 4.1386862Losses:  3.9756054878234863 0.13196690380573273
CurrentTrain: epoch  6, batch    34 | loss: 4.1075726Losses:  3.9734177589416504 0.12045791745185852
CurrentTrain: epoch  6, batch    35 | loss: 4.0938759Losses:  3.9985270500183105 0.09306745231151581
CurrentTrain: epoch  6, batch    36 | loss: 4.0915947Losses:  3.9360029697418213 0.06361605226993561
CurrentTrain: epoch  6, batch    37 | loss: 3.9996190Losses:  3.9402873516082764 0.09406401216983795
CurrentTrain: epoch  6, batch    38 | loss: 4.0343513Losses:  3.9754700660705566 0.11267435550689697
CurrentTrain: epoch  6, batch    39 | loss: 4.0881443Losses:  3.9855587482452393 0.10297152400016785
CurrentTrain: epoch  6, batch    40 | loss: 4.0885301Losses:  3.9730303287506104 0.11626644432544708
CurrentTrain: epoch  6, batch    41 | loss: 4.0892968Losses:  4.009248733520508 0.10292869806289673
CurrentTrain: epoch  6, batch    42 | loss: 4.1121774Losses:  3.987339735031128 0.09516929090023041
CurrentTrain: epoch  6, batch    43 | loss: 4.0825090Losses:  3.986551284790039 0.06951002776622772
CurrentTrain: epoch  6, batch    44 | loss: 4.0560613Losses:  4.0254926681518555 0.09604194015264511
CurrentTrain: epoch  6, batch    45 | loss: 4.1215348Losses:  3.945657730102539 0.10764646530151367
CurrentTrain: epoch  6, batch    46 | loss: 4.0533042Losses:  4.003960132598877 0.09468311816453934
CurrentTrain: epoch  6, batch    47 | loss: 4.0986433Losses:  3.9802451133728027 0.10675708204507828
CurrentTrain: epoch  6, batch    48 | loss: 4.0870023Losses:  4.032439708709717 0.07352935522794724
CurrentTrain: epoch  6, batch    49 | loss: 4.1059690Losses:  3.975748300552368 0.13319219648838043
CurrentTrain: epoch  6, batch    50 | loss: 4.1089406Losses:  3.9848461151123047 0.08806326985359192
CurrentTrain: epoch  6, batch    51 | loss: 4.0729094Losses:  3.929291248321533 0.10162529349327087
CurrentTrain: epoch  6, batch    52 | loss: 4.0309167Losses:  3.923706531524658 0.10562445223331451
CurrentTrain: epoch  6, batch    53 | loss: 4.0293312Losses:  3.9524333477020264 0.12321361899375916
CurrentTrain: epoch  6, batch    54 | loss: 4.0756469Losses:  3.9759740829467773 0.10454728454351425
CurrentTrain: epoch  6, batch    55 | loss: 4.0805216Losses:  3.9930505752563477 0.10104706138372421
CurrentTrain: epoch  6, batch    56 | loss: 4.0940976Losses:  3.970308303833008 0.0910799652338028
CurrentTrain: epoch  6, batch    57 | loss: 4.0613885Losses:  3.987579107284546 0.12299443781375885
CurrentTrain: epoch  6, batch    58 | loss: 4.1105738Losses:  3.9761359691619873 0.083272285759449
CurrentTrain: epoch  6, batch    59 | loss: 4.0594082Losses:  3.9671006202697754 0.10002346336841583
CurrentTrain: epoch  6, batch    60 | loss: 4.0671239Losses:  3.916177749633789 0.09918393939733505
CurrentTrain: epoch  6, batch    61 | loss: 4.0153618Losses:  3.9836699962615967 0.09251714497804642
CurrentTrain: epoch  6, batch    62 | loss: 4.0761871Losses:  3.9188363552093506 0.0783175528049469
CurrentTrain: epoch  7, batch     0 | loss: 3.9971540Losses:  4.0070295333862305 0.11501887440681458
CurrentTrain: epoch  7, batch     1 | loss: 4.1220484Losses:  3.9551467895507812 0.11274680495262146
CurrentTrain: epoch  7, batch     2 | loss: 4.0678935Losses:  4.015634536743164 0.0822710394859314
CurrentTrain: epoch  7, batch     3 | loss: 4.0979056Losses:  3.964876174926758 0.12032613158226013
CurrentTrain: epoch  7, batch     4 | loss: 4.0852022Losses:  3.9789319038391113 0.09011602401733398
CurrentTrain: epoch  7, batch     5 | loss: 4.0690479Losses:  3.9393839836120605 0.09121333807706833
CurrentTrain: epoch  7, batch     6 | loss: 4.0305972Losses:  3.951897144317627 0.09254230558872223
CurrentTrain: epoch  7, batch     7 | loss: 4.0444393Losses:  3.997502326965332 0.09198892116546631
CurrentTrain: epoch  7, batch     8 | loss: 4.0894914Losses:  3.9797375202178955 0.10861814022064209
CurrentTrain: epoch  7, batch     9 | loss: 4.0883555Losses:  3.9606688022613525 0.09145278483629227
CurrentTrain: epoch  7, batch    10 | loss: 4.0521216Losses:  3.983701467514038 0.09140297770500183
CurrentTrain: epoch  7, batch    11 | loss: 4.0751042Losses:  3.9594013690948486 0.07742872834205627
CurrentTrain: epoch  7, batch    12 | loss: 4.0368299Losses:  3.9600167274475098 0.11143770813941956
CurrentTrain: epoch  7, batch    13 | loss: 4.0714545Losses:  3.9667654037475586 0.10530123859643936
CurrentTrain: epoch  7, batch    14 | loss: 4.0720668Losses:  3.954087257385254 0.10451187938451767
CurrentTrain: epoch  7, batch    15 | loss: 4.0585990Losses:  3.9778754711151123 0.0657554343342781
CurrentTrain: epoch  7, batch    16 | loss: 4.0436311Losses:  3.998854875564575 0.06919622421264648
CurrentTrain: epoch  7, batch    17 | loss: 4.0680513Losses:  3.967721939086914 0.06077565252780914
CurrentTrain: epoch  7, batch    18 | loss: 4.0284977Losses:  3.9912495613098145 0.10333947837352753
CurrentTrain: epoch  7, batch    19 | loss: 4.0945892Losses:  3.939303159713745 0.09795770049095154
CurrentTrain: epoch  7, batch    20 | loss: 4.0372610Losses:  3.9461073875427246 0.07609440386295319
CurrentTrain: epoch  7, batch    21 | loss: 4.0222020Losses:  3.917161226272583 0.11262913048267365
CurrentTrain: epoch  7, batch    22 | loss: 4.0297904Losses:  3.9729866981506348 0.10864407569169998
CurrentTrain: epoch  7, batch    23 | loss: 4.0816307Losses:  3.9498653411865234 0.10647600889205933
CurrentTrain: epoch  7, batch    24 | loss: 4.0563412Losses:  3.9646193981170654 0.09835326671600342
CurrentTrain: epoch  7, batch    25 | loss: 4.0629725Losses:  3.9685208797454834 0.10700343549251556
CurrentTrain: epoch  7, batch    26 | loss: 4.0755243Losses:  4.001183032989502 0.07510539889335632
CurrentTrain: epoch  7, batch    27 | loss: 4.0762882Losses:  3.9488203525543213 0.07203087210655212
CurrentTrain: epoch  7, batch    28 | loss: 4.0208511Losses:  3.965928077697754 0.1086193174123764
CurrentTrain: epoch  7, batch    29 | loss: 4.0745473Losses:  3.948793411254883 0.11231570690870285
CurrentTrain: epoch  7, batch    30 | loss: 4.0611091Losses:  3.958928108215332 0.07485377788543701
CurrentTrain: epoch  7, batch    31 | loss: 4.0337820Losses:  3.917003631591797 0.08311483263969421
CurrentTrain: epoch  7, batch    32 | loss: 4.0001183Losses:  3.9587299823760986 0.0803564265370369
CurrentTrain: epoch  7, batch    33 | loss: 4.0390863Losses:  3.962705612182617 0.10392466187477112
CurrentTrain: epoch  7, batch    34 | loss: 4.0666304Losses:  3.933687686920166 0.0759289339184761
CurrentTrain: epoch  7, batch    35 | loss: 4.0096169Losses:  3.962341785430908 0.09055361151695251
CurrentTrain: epoch  7, batch    36 | loss: 4.0528955Losses:  3.961946487426758 0.10059909522533417
CurrentTrain: epoch  7, batch    37 | loss: 4.0625458Losses:  3.9489188194274902 0.06174787878990173
CurrentTrain: epoch  7, batch    38 | loss: 4.0106668Losses:  3.9502806663513184 0.05904015898704529
CurrentTrain: epoch  7, batch    39 | loss: 4.0093207Losses:  3.9077296257019043 0.09253235161304474
CurrentTrain: epoch  7, batch    40 | loss: 4.0002618Losses:  3.9290902614593506 0.06778302788734436
CurrentTrain: epoch  7, batch    41 | loss: 3.9968734Losses:  3.931576728820801 0.06492196023464203
CurrentTrain: epoch  7, batch    42 | loss: 3.9964986Losses:  3.9449877738952637 0.07596644759178162
CurrentTrain: epoch  7, batch    43 | loss: 4.0209541Losses:  3.995579719543457 0.07122577726840973
CurrentTrain: epoch  7, batch    44 | loss: 4.0668054Losses:  3.955432653427124 0.09186895936727524
CurrentTrain: epoch  7, batch    45 | loss: 4.0473018Losses:  3.9369542598724365 0.09589966386556625
CurrentTrain: epoch  7, batch    46 | loss: 4.0328541Losses:  3.972142457962036 0.10392829775810242
CurrentTrain: epoch  7, batch    47 | loss: 4.0760708Losses:  3.906498908996582 0.10334740579128265
CurrentTrain: epoch  7, batch    48 | loss: 4.0098462Losses:  3.9428582191467285 0.0570811852812767
CurrentTrain: epoch  7, batch    49 | loss: 3.9999394Losses:  3.964831590652466 0.08988045901060104
CurrentTrain: epoch  7, batch    50 | loss: 4.0547118Losses:  3.9457125663757324 0.08638256788253784
CurrentTrain: epoch  7, batch    51 | loss: 4.0320950Losses:  3.9625966548919678 0.0872420147061348
CurrentTrain: epoch  7, batch    52 | loss: 4.0498385Losses:  3.9359371662139893 0.09327185153961182
CurrentTrain: epoch  7, batch    53 | loss: 4.0292091Losses:  3.9798920154571533 0.09869343787431717
CurrentTrain: epoch  7, batch    54 | loss: 4.0785856Losses:  3.8637287616729736 0.07767347991466522
CurrentTrain: epoch  7, batch    55 | loss: 3.9414022Losses:  3.9727165699005127 0.08397086709737778
CurrentTrain: epoch  7, batch    56 | loss: 4.0566874Losses:  3.932011127471924 0.09525924175977707
CurrentTrain: epoch  7, batch    57 | loss: 4.0272703Losses:  3.9502272605895996 0.09563091397285461
CurrentTrain: epoch  7, batch    58 | loss: 4.0458584Losses:  3.998426675796509 0.07439042627811432
CurrentTrain: epoch  7, batch    59 | loss: 4.0728173Losses:  3.937772750854492 0.10677950829267502
CurrentTrain: epoch  7, batch    60 | loss: 4.0445523Losses:  3.934051990509033 0.10523378103971481
CurrentTrain: epoch  7, batch    61 | loss: 4.0392857Losses:  3.9745125770568848 0.049186669290065765
CurrentTrain: epoch  7, batch    62 | loss: 4.0236993Losses:  3.927772045135498 0.09371615201234818
CurrentTrain: epoch  8, batch     0 | loss: 4.0214882Losses:  3.9548592567443848 0.09488709270954132
CurrentTrain: epoch  8, batch     1 | loss: 4.0497465Losses:  3.958104133605957 0.10719817876815796
CurrentTrain: epoch  8, batch     2 | loss: 4.0653024Losses:  3.9524121284484863 0.08843483030796051
CurrentTrain: epoch  8, batch     3 | loss: 4.0408468Losses:  3.952651023864746 0.06423285603523254
CurrentTrain: epoch  8, batch     4 | loss: 4.0168839Losses:  3.9680471420288086 0.07804505527019501
CurrentTrain: epoch  8, batch     5 | loss: 4.0460920Losses:  3.914463996887207 0.10473131388425827
CurrentTrain: epoch  8, batch     6 | loss: 4.0191951Losses:  3.9310081005096436 0.09836942702531815
CurrentTrain: epoch  8, batch     7 | loss: 4.0293775Losses:  3.9654221534729004 0.05933050066232681
CurrentTrain: epoch  8, batch     8 | loss: 4.0247526Losses:  3.919743537902832 0.06383711844682693
CurrentTrain: epoch  8, batch     9 | loss: 3.9835806Losses:  3.9407005310058594 0.09166008234024048
CurrentTrain: epoch  8, batch    10 | loss: 4.0323606Losses:  3.926572799682617 0.07809033244848251
CurrentTrain: epoch  8, batch    11 | loss: 4.0046630Losses:  3.9299657344818115 0.08720932900905609
CurrentTrain: epoch  8, batch    12 | loss: 4.0171752Losses:  3.963189125061035 0.08576841652393341
CurrentTrain: epoch  8, batch    13 | loss: 4.0489573Losses:  3.994938611984253 0.07548031210899353
CurrentTrain: epoch  8, batch    14 | loss: 4.0704188Losses:  3.938638925552368 0.09804919362068176
CurrentTrain: epoch  8, batch    15 | loss: 4.0366883Losses:  3.933882236480713 0.06805934011936188
CurrentTrain: epoch  8, batch    16 | loss: 4.0019417Losses:  3.962103843688965 0.07570183277130127
CurrentTrain: epoch  8, batch    17 | loss: 4.0378056Losses:  3.943089485168457 0.0907895416021347
CurrentTrain: epoch  8, batch    18 | loss: 4.0338788Losses:  3.933398485183716 0.09848318994045258
CurrentTrain: epoch  8, batch    19 | loss: 4.0318818Losses:  3.9620351791381836 0.061391524970531464
CurrentTrain: epoch  8, batch    20 | loss: 4.0234265Losses:  3.9779014587402344 0.08032342046499252
CurrentTrain: epoch  8, batch    21 | loss: 4.0582247Losses:  3.9556384086608887 0.07760857045650482
CurrentTrain: epoch  8, batch    22 | loss: 4.0332470Losses:  3.934926748275757 0.08908914029598236
CurrentTrain: epoch  8, batch    23 | loss: 4.0240159Losses:  3.956617593765259 0.10052713751792908
CurrentTrain: epoch  8, batch    24 | loss: 4.0571446Losses:  3.993027687072754 0.08485960960388184
CurrentTrain: epoch  8, batch    25 | loss: 4.0778875Losses:  3.9092016220092773 0.09431186318397522
CurrentTrain: epoch  8, batch    26 | loss: 4.0035133Losses:  3.974222183227539 0.07993672788143158
CurrentTrain: epoch  8, batch    27 | loss: 4.0541587Losses:  3.933688163757324 0.09286810457706451
CurrentTrain: epoch  8, batch    28 | loss: 4.0265565Losses:  3.9214210510253906 0.07939814776182175
CurrentTrain: epoch  8, batch    29 | loss: 4.0008192Losses:  3.9344322681427 0.058580636978149414
CurrentTrain: epoch  8, batch    30 | loss: 3.9930129Losses:  3.9579310417175293 0.07966776192188263
CurrentTrain: epoch  8, batch    31 | loss: 4.0375986Losses:  3.9546055793762207 0.08438753336668015
CurrentTrain: epoch  8, batch    32 | loss: 4.0389929Losses:  3.8954918384552 0.07545366883277893
CurrentTrain: epoch  8, batch    33 | loss: 3.9709456Losses:  3.9441516399383545 0.07889385521411896
CurrentTrain: epoch  8, batch    34 | loss: 4.0230455Losses:  3.9448037147521973 0.08568594604730606
CurrentTrain: epoch  8, batch    35 | loss: 4.0304894Losses:  3.951712131500244 0.0679195299744606
CurrentTrain: epoch  8, batch    36 | loss: 4.0196319Losses:  4.035900592803955 0.10546538233757019
CurrentTrain: epoch  8, batch    37 | loss: 4.1413660Losses:  3.9308199882507324 0.07581740617752075
CurrentTrain: epoch  8, batch    38 | loss: 4.0066376Losses:  3.972186326980591 0.08490727841854095
CurrentTrain: epoch  8, batch    39 | loss: 4.0570936Losses:  3.9804651737213135 0.06830619275569916
CurrentTrain: epoch  8, batch    40 | loss: 4.0487714Losses:  3.9736835956573486 0.08506864309310913
CurrentTrain: epoch  8, batch    41 | loss: 4.0587521Losses:  3.9940357208251953 0.06879308074712753
CurrentTrain: epoch  8, batch    42 | loss: 4.0628290Losses:  3.9390668869018555 0.06752745062112808
CurrentTrain: epoch  8, batch    43 | loss: 4.0065942Losses:  3.9235787391662598 0.0939522385597229
CurrentTrain: epoch  8, batch    44 | loss: 4.0175309Losses:  3.9358878135681152 0.06426534056663513
CurrentTrain: epoch  8, batch    45 | loss: 4.0001531Losses:  3.9677248001098633 0.08635711669921875
CurrentTrain: epoch  8, batch    46 | loss: 4.0540819Losses:  3.9639155864715576 0.06560349464416504
CurrentTrain: epoch  8, batch    47 | loss: 4.0295191Losses:  3.9726529121398926 0.08210672438144684
CurrentTrain: epoch  8, batch    48 | loss: 4.0547595Losses:  3.959012031555176 0.0716458410024643
CurrentTrain: epoch  8, batch    49 | loss: 4.0306578Losses:  3.9570484161376953 0.070642851293087
CurrentTrain: epoch  8, batch    50 | loss: 4.0276914Losses:  3.958528995513916 0.09242875128984451
CurrentTrain: epoch  8, batch    51 | loss: 4.0509577Losses:  3.97509765625 0.06536202877759933
CurrentTrain: epoch  8, batch    52 | loss: 4.0404596Losses:  3.95550537109375 0.0999043807387352
CurrentTrain: epoch  8, batch    53 | loss: 4.0554099Losses:  3.940584659576416 0.0822032019495964
CurrentTrain: epoch  8, batch    54 | loss: 4.0227880Losses:  3.981895923614502 0.06657468527555466
CurrentTrain: epoch  8, batch    55 | loss: 4.0484705Losses:  3.961317539215088 0.05256578326225281
CurrentTrain: epoch  8, batch    56 | loss: 4.0138831Losses:  3.941277503967285 0.08195125311613083
CurrentTrain: epoch  8, batch    57 | loss: 4.0232286Losses:  3.9538562297821045 0.05269259586930275
CurrentTrain: epoch  8, batch    58 | loss: 4.0065489Losses:  3.9367384910583496 0.08028469979763031
CurrentTrain: epoch  8, batch    59 | loss: 4.0170231Losses:  3.952328681945801 0.06963211297988892
CurrentTrain: epoch  8, batch    60 | loss: 4.0219607Losses:  3.949680805206299 0.07591027021408081
CurrentTrain: epoch  8, batch    61 | loss: 4.0255909Losses:  3.9525904655456543 0.0641927644610405
CurrentTrain: epoch  8, batch    62 | loss: 4.0167832Losses:  3.962571382522583 0.08809259533882141
CurrentTrain: epoch  9, batch     0 | loss: 4.0506639Losses:  3.9320437908172607 0.07580298185348511
CurrentTrain: epoch  9, batch     1 | loss: 4.0078468Losses:  3.9459917545318604 0.08042002469301224
CurrentTrain: epoch  9, batch     2 | loss: 4.0264120Losses:  3.928652286529541 0.08207675069570541
CurrentTrain: epoch  9, batch     3 | loss: 4.0107288Losses:  3.9648022651672363 0.0649687871336937
CurrentTrain: epoch  9, batch     4 | loss: 4.0297709Losses:  3.977224349975586 0.07053665816783905
CurrentTrain: epoch  9, batch     5 | loss: 4.0477610Losses:  3.943385601043701 0.09017917513847351
CurrentTrain: epoch  9, batch     6 | loss: 4.0335646Losses:  3.9667654037475586 0.07023866474628448
CurrentTrain: epoch  9, batch     7 | loss: 4.0370040Losses:  3.9074437618255615 0.08172981441020966
CurrentTrain: epoch  9, batch     8 | loss: 3.9891737Losses:  3.959441661834717 0.08423145115375519
CurrentTrain: epoch  9, batch     9 | loss: 4.0436730Losses:  3.962130308151245 0.07264822721481323
CurrentTrain: epoch  9, batch    10 | loss: 4.0347786Losses:  3.960911750793457 0.0940607413649559
CurrentTrain: epoch  9, batch    11 | loss: 4.0549726Losses:  3.948939323425293 0.07897084951400757
CurrentTrain: epoch  9, batch    12 | loss: 4.0279102Losses:  3.9344465732574463 0.059291090816259384
CurrentTrain: epoch  9, batch    13 | loss: 3.9937377Losses:  3.9234280586242676 0.086459681391716
CurrentTrain: epoch  9, batch    14 | loss: 4.0098877Losses:  3.9662508964538574 0.08102641254663467
CurrentTrain: epoch  9, batch    15 | loss: 4.0472775Losses:  3.9378724098205566 0.09443261474370956
CurrentTrain: epoch  9, batch    16 | loss: 4.0323052Losses:  3.9390459060668945 0.07242134213447571
CurrentTrain: epoch  9, batch    17 | loss: 4.0114675Losses:  3.924482583999634 0.07524023950099945
CurrentTrain: epoch  9, batch    18 | loss: 3.9997227Losses:  3.960665464401245 0.09896066784858704
CurrentTrain: epoch  9, batch    19 | loss: 4.0596261Losses:  3.9226632118225098 0.08381383866071701
CurrentTrain: epoch  9, batch    20 | loss: 4.0064769Losses:  3.9590539932250977 0.06830444186925888
CurrentTrain: epoch  9, batch    21 | loss: 4.0273585Losses:  3.9261932373046875 0.09247282147407532
CurrentTrain: epoch  9, batch    22 | loss: 4.0186663Losses:  3.9332211017608643 0.08226216584444046
CurrentTrain: epoch  9, batch    23 | loss: 4.0154834Losses:  3.9688713550567627 0.06857508420944214
CurrentTrain: epoch  9, batch    24 | loss: 4.0374465Losses:  3.937474489212036 0.07409486174583435
CurrentTrain: epoch  9, batch    25 | loss: 4.0115695Losses:  3.9383363723754883 0.09720273315906525
CurrentTrain: epoch  9, batch    26 | loss: 4.0355392Losses:  3.9804272651672363 0.09029722213745117
CurrentTrain: epoch  9, batch    27 | loss: 4.0707245Losses:  3.931227207183838 0.08511513471603394
CurrentTrain: epoch  9, batch    28 | loss: 4.0163422Losses:  3.9375550746917725 0.06293360143899918
CurrentTrain: epoch  9, batch    29 | loss: 4.0004888Losses:  3.958179473876953 0.08664257824420929
CurrentTrain: epoch  9, batch    30 | loss: 4.0448222Losses:  3.9369070529937744 0.07691377401351929
CurrentTrain: epoch  9, batch    31 | loss: 4.0138206Losses:  3.9331154823303223 0.06468755751848221
CurrentTrain: epoch  9, batch    32 | loss: 3.9978030Losses:  3.9656808376312256 0.06240454316139221
CurrentTrain: epoch  9, batch    33 | loss: 4.0280852Losses:  3.9326562881469727 0.0709104835987091
CurrentTrain: epoch  9, batch    34 | loss: 4.0035667Losses:  3.9162707328796387 0.07360215485095978
CurrentTrain: epoch  9, batch    35 | loss: 3.9898729Losses:  3.9137284755706787 0.0548098161816597
CurrentTrain: epoch  9, batch    36 | loss: 3.9685383Losses:  3.949002504348755 0.06518776714801788
CurrentTrain: epoch  9, batch    37 | loss: 4.0141902Losses:  3.930509567260742 0.07109660655260086
CurrentTrain: epoch  9, batch    38 | loss: 4.0016060Losses:  3.9263391494750977 0.07485297322273254
CurrentTrain: epoch  9, batch    39 | loss: 4.0011921Losses:  3.9482758045196533 0.08184700459241867
CurrentTrain: epoch  9, batch    40 | loss: 4.0301228Losses:  3.8869454860687256 0.04521527886390686
CurrentTrain: epoch  9, batch    41 | loss: 3.9321609Losses:  3.9788923263549805 0.03766099363565445
CurrentTrain: epoch  9, batch    42 | loss: 4.0165534Losses:  3.958460569381714 0.07866018265485764
CurrentTrain: epoch  9, batch    43 | loss: 4.0371208Losses:  3.931088447570801 0.06564512848854065
CurrentTrain: epoch  9, batch    44 | loss: 3.9967337Losses:  3.921663761138916 0.055098600685596466
CurrentTrain: epoch  9, batch    45 | loss: 3.9767623Losses:  3.9330623149871826 0.06551525741815567
CurrentTrain: epoch  9, batch    46 | loss: 3.9985776Losses:  3.955892562866211 0.05305591970682144
CurrentTrain: epoch  9, batch    47 | loss: 4.0089483Losses:  3.956357002258301 0.060307133942842484
CurrentTrain: epoch  9, batch    48 | loss: 4.0166640Losses:  3.9326374530792236 0.04901541769504547
CurrentTrain: epoch  9, batch    49 | loss: 3.9816530Losses:  3.937922477722168 0.08909086883068085
CurrentTrain: epoch  9, batch    50 | loss: 4.0270133Losses:  3.9135584831237793 0.05009165406227112
CurrentTrain: epoch  9, batch    51 | loss: 3.9636502Losses:  3.965238094329834 0.08693373203277588
CurrentTrain: epoch  9, batch    52 | loss: 4.0521717Losses:  3.941429376602173 0.052798427641391754
CurrentTrain: epoch  9, batch    53 | loss: 3.9942279Losses:  3.9619550704956055 0.04614248126745224
CurrentTrain: epoch  9, batch    54 | loss: 4.0080976Losses:  3.972562313079834 0.07288812100887299
CurrentTrain: epoch  9, batch    55 | loss: 4.0454502Losses:  3.9198172092437744 0.07595135271549225
CurrentTrain: epoch  9, batch    56 | loss: 3.9957685Losses:  3.9642882347106934 0.055997900664806366
CurrentTrain: epoch  9, batch    57 | loss: 4.0202861Losses:  3.9505393505096436 0.0673983171582222
CurrentTrain: epoch  9, batch    58 | loss: 4.0179377Losses:  3.905266761779785 0.0742807388305664
CurrentTrain: epoch  9, batch    59 | loss: 3.9795475Losses:  3.962963581085205 0.07371536642313004
CurrentTrain: epoch  9, batch    60 | loss: 4.0366788Losses:  3.9400808811187744 0.06647482514381409
CurrentTrain: epoch  9, batch    61 | loss: 4.0065556Losses:  3.9983410835266113 0.03916049748659134
CurrentTrain: epoch  9, batch    62 | loss: 4.0375018
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.44%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.03%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.10%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.27%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.06%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.35%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.41%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.28%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.11%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.18%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.16%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.44%   
cur_acc:  ['0.9444']
his_acc:  ['0.9444']
Clustering into  9  clusters
Clusters:  [1 5 4 0 1 1 7 1 6 8 2 0 1 0 0 3 1 1 1 1]
Losses:  8.310612678527832 1.8492374420166016
CurrentTrain: epoch  0, batch     0 | loss: 10.1598501Losses:  11.250682830810547 1.7272701263427734
CurrentTrain: epoch  0, batch     1 | loss: 12.9779530Losses:  8.34434700012207 2.0745608806610107
CurrentTrain: epoch  0, batch     2 | loss: 10.4189081Losses:  6.230431079864502 0.375044584274292
CurrentTrain: epoch  0, batch     3 | loss: 6.6054754Losses:  5.1157732009887695 1.8759901523590088
CurrentTrain: epoch  1, batch     0 | loss: 6.9917631Losses:  4.626343250274658 1.7037007808685303
CurrentTrain: epoch  1, batch     1 | loss: 6.3300438Losses:  4.60460090637207 1.803693175315857
CurrentTrain: epoch  1, batch     2 | loss: 6.4082942Losses:  3.4459691047668457 0.33379286527633667
CurrentTrain: epoch  1, batch     3 | loss: 3.7797620Losses:  4.225641250610352 1.5874369144439697
CurrentTrain: epoch  2, batch     0 | loss: 5.8130779Losses:  3.889932155609131 1.6195471286773682
CurrentTrain: epoch  2, batch     1 | loss: 5.5094795Losses:  3.886037826538086 1.6741644144058228
CurrentTrain: epoch  2, batch     2 | loss: 5.5602021Losses:  5.900733947753906 0.8709935545921326
CurrentTrain: epoch  2, batch     3 | loss: 6.7717276Losses:  3.835585117340088 1.694069266319275
CurrentTrain: epoch  3, batch     0 | loss: 5.5296545Losses:  3.91701078414917 1.8218075037002563
CurrentTrain: epoch  3, batch     1 | loss: 5.7388182Losses:  3.3841440677642822 1.7074077129364014
CurrentTrain: epoch  3, batch     2 | loss: 5.0915518Losses:  3.6474878787994385 0.6380033493041992
CurrentTrain: epoch  3, batch     3 | loss: 4.2854910Losses:  3.4340097904205322 1.4461373090744019
CurrentTrain: epoch  4, batch     0 | loss: 4.8801470Losses:  3.666407585144043 1.0769257545471191
CurrentTrain: epoch  4, batch     1 | loss: 4.7433333Losses:  3.5107533931732178 1.5653653144836426
CurrentTrain: epoch  4, batch     2 | loss: 5.0761185Losses:  4.15987491607666 0.49070069193840027
CurrentTrain: epoch  4, batch     3 | loss: 4.6505756Losses:  3.820314407348633 1.3041399717330933
CurrentTrain: epoch  5, batch     0 | loss: 5.1244545Losses:  3.2202749252319336 1.6202155351638794
CurrentTrain: epoch  5, batch     1 | loss: 4.8404903Losses:  3.0997567176818848 1.4485176801681519
CurrentTrain: epoch  5, batch     2 | loss: 4.5482745Losses:  2.572922706604004 0.4662507474422455
CurrentTrain: epoch  5, batch     3 | loss: 3.0391734Losses:  3.2067482471466064 1.3894563913345337
CurrentTrain: epoch  6, batch     0 | loss: 4.5962048Losses:  3.061361789703369 1.1691688299179077
CurrentTrain: epoch  6, batch     1 | loss: 4.2305307Losses:  3.038466691970825 1.5542521476745605
CurrentTrain: epoch  6, batch     2 | loss: 4.5927191Losses:  3.4895105361938477 0.5541287064552307
CurrentTrain: epoch  6, batch     3 | loss: 4.0436392Losses:  2.9280033111572266 1.333967685699463
CurrentTrain: epoch  7, batch     0 | loss: 4.2619710Losses:  2.6584320068359375 1.2537784576416016
CurrentTrain: epoch  7, batch     1 | loss: 3.9122105Losses:  2.9766016006469727 1.3397419452667236
CurrentTrain: epoch  7, batch     2 | loss: 4.3163433Losses:  5.221025466918945 3.278256031080673e-07
CurrentTrain: epoch  7, batch     3 | loss: 5.2210259Losses:  2.976074695587158 1.4251856803894043
CurrentTrain: epoch  8, batch     0 | loss: 4.4012604Losses:  2.8652701377868652 1.3191455602645874
CurrentTrain: epoch  8, batch     1 | loss: 4.1844158Losses:  2.750464916229248 1.3837203979492188
CurrentTrain: epoch  8, batch     2 | loss: 4.1341853Losses:  2.9885289669036865 0.5519998073577881
CurrentTrain: epoch  8, batch     3 | loss: 3.5405288Losses:  2.5886523723602295 1.236403226852417
CurrentTrain: epoch  9, batch     0 | loss: 3.8250556Losses:  3.091935157775879 1.3186399936676025
CurrentTrain: epoch  9, batch     1 | loss: 4.4105749Losses:  2.4305224418640137 1.2411987781524658
CurrentTrain: epoch  9, batch     2 | loss: 3.6717212Losses:  2.759255886077881 0.5034677982330322
CurrentTrain: epoch  9, batch     3 | loss: 3.2627237
Losses:  5.121006965637207 0.9125665426254272
MemoryTrain:  epoch  0, batch     0 | loss: 6.0335736Losses:  11.076404571533203 0.6055444478988647
MemoryTrain:  epoch  0, batch     1 | loss: 11.6819487Losses:  0.6490991115570068 1.1852774620056152
MemoryTrain:  epoch  1, batch     0 | loss: 1.8343766Losses:  0.42953068017959595 0.18810661137104034
MemoryTrain:  epoch  1, batch     1 | loss: 0.6176373Losses:  0.3488085865974426 0.8412033915519714
MemoryTrain:  epoch  2, batch     0 | loss: 1.1900120Losses:  0.42090800404548645 0.7277584671974182
MemoryTrain:  epoch  2, batch     1 | loss: 1.1486665Losses:  0.33255547285079956 1.0140819549560547
MemoryTrain:  epoch  3, batch     0 | loss: 1.3466375Losses:  0.13765527307987213 0.22820700705051422
MemoryTrain:  epoch  3, batch     1 | loss: 0.3658623Losses:  0.26247772574424744 1.0223429203033447
MemoryTrain:  epoch  4, batch     0 | loss: 1.2848207Losses:  0.2086072713136673 0.15439331531524658
MemoryTrain:  epoch  4, batch     1 | loss: 0.3630006Losses:  0.21611084043979645 0.9045824408531189
MemoryTrain:  epoch  5, batch     0 | loss: 1.1206933Losses:  0.38735517859458923 0.39664629101753235
MemoryTrain:  epoch  5, batch     1 | loss: 0.7840015Losses:  0.19139349460601807 0.8598191738128662
MemoryTrain:  epoch  6, batch     0 | loss: 1.0512127Losses:  0.1770702451467514 0.27041178941726685
MemoryTrain:  epoch  6, batch     1 | loss: 0.4474820Losses:  0.14975519478321075 0.6751875877380371
MemoryTrain:  epoch  7, batch     0 | loss: 0.8249428Losses:  0.23684938251972198 0.6419440507888794
MemoryTrain:  epoch  7, batch     1 | loss: 0.8787934Losses:  0.1690872311592102 0.801355242729187
MemoryTrain:  epoch  8, batch     0 | loss: 0.9704425Losses:  0.15039516985416412 0.32391437888145447
MemoryTrain:  epoch  8, batch     1 | loss: 0.4743096Losses:  0.14825966954231262 0.6717522740364075
MemoryTrain:  epoch  9, batch     0 | loss: 0.8200120Losses:  0.16744586825370789 0.4735540747642517
MemoryTrain:  epoch  9, batch     1 | loss: 0.6409999
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 64.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 57.59%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 55.60%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 53.96%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.22%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 52.54%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 53.79%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 55.71%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 56.42%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 57.09%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 57.73%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 60.21%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 60.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 61.63%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 61.67%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 59.71%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 59.24%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 58.67%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 58.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 58.46%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 58.89%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 59.43%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 59.84%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 60.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 60.13%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 59.75%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 59.79%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.63%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 59.78%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 59.13%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 94.56%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.29%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 94.47%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 93.85%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 92.58%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 91.25%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 90.06%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 89.37%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 87.59%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 87.76%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 88.00%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 88.15%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 88.03%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 87.65%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 87.28%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 87.21%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 86.77%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 86.06%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 85.44%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 84.48%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 83.54%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 82.62%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 81.73%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 80.91%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 80.19%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 80.33%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 80.57%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.01%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 81.19%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 80.84%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 80.44%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 79.93%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 79.38%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 79.00%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 78.68%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.43%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 78.29%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 78.37%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 78.42%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 78.02%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 77.79%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 77.56%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 77.39%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 77.22%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 77.05%   
cur_acc:  ['0.9444', '0.5913']
his_acc:  ['0.9444', '0.7705']
Clustering into  14  clusters
Clusters:  [ 2 11  7  0  2  2 12  2 10  5  9  0  2  0  0  4  2  2  2  2  2  2  2  8
  2  3  2  1 13  6]
Losses:  7.345987319946289 1.3144209384918213
CurrentTrain: epoch  0, batch     0 | loss: 8.6604080Losses:  9.513262748718262 1.1751679182052612
CurrentTrain: epoch  0, batch     1 | loss: 10.6884308Losses:  6.601997375488281 1.3290992975234985
CurrentTrain: epoch  0, batch     2 | loss: 7.9310966Losses:  5.2876811027526855 0.26037752628326416
CurrentTrain: epoch  0, batch     3 | loss: 5.5480585Losses:  3.627600908279419 1.151500940322876
CurrentTrain: epoch  1, batch     0 | loss: 4.7791018Losses:  3.472297430038452 1.1691944599151611
CurrentTrain: epoch  1, batch     1 | loss: 4.6414919Losses:  3.257786750793457 1.0438637733459473
CurrentTrain: epoch  1, batch     2 | loss: 4.3016505Losses:  3.6994335651397705 0.2378264218568802
CurrentTrain: epoch  1, batch     3 | loss: 3.9372599Losses:  3.3052892684936523 0.969310998916626
CurrentTrain: epoch  2, batch     0 | loss: 4.2746000Losses:  2.707183599472046 0.8730924725532532
CurrentTrain: epoch  2, batch     1 | loss: 3.5802760Losses:  3.0523369312286377 1.0313658714294434
CurrentTrain: epoch  2, batch     2 | loss: 4.0837030Losses:  3.4510130882263184 0.23268428444862366
CurrentTrain: epoch  2, batch     3 | loss: 3.6836975Losses:  3.2491989135742188 0.8732205629348755
CurrentTrain: epoch  3, batch     0 | loss: 4.1224194Losses:  2.329432487487793 0.7420569658279419
CurrentTrain: epoch  3, batch     1 | loss: 3.0714893Losses:  2.4190640449523926 0.8064220547676086
CurrentTrain: epoch  3, batch     2 | loss: 3.2254860Losses:  3.317146062850952 0.08161281049251556
CurrentTrain: epoch  3, batch     3 | loss: 3.3987589Losses:  2.1582658290863037 0.5993633270263672
CurrentTrain: epoch  4, batch     0 | loss: 2.7576292Losses:  2.724717855453491 0.821510910987854
CurrentTrain: epoch  4, batch     1 | loss: 3.5462289Losses:  2.7229223251342773 0.7347373962402344
CurrentTrain: epoch  4, batch     2 | loss: 3.4576597Losses:  2.9199132919311523 0.3604341447353363
CurrentTrain: epoch  4, batch     3 | loss: 3.2803473Losses:  2.3425827026367188 0.6168800592422485
CurrentTrain: epoch  5, batch     0 | loss: 2.9594626Losses:  2.317840099334717 0.642842173576355
CurrentTrain: epoch  5, batch     1 | loss: 2.9606824Losses:  2.496320962905884 0.6956011056900024
CurrentTrain: epoch  5, batch     2 | loss: 3.1919222Losses:  2.157249927520752 0.08116141706705093
CurrentTrain: epoch  5, batch     3 | loss: 2.2384114Losses:  2.3187060356140137 0.6536775827407837
CurrentTrain: epoch  6, batch     0 | loss: 2.9723835Losses:  2.1015844345092773 0.7026568651199341
CurrentTrain: epoch  6, batch     1 | loss: 2.8042412Losses:  2.062373161315918 0.538798451423645
CurrentTrain: epoch  6, batch     2 | loss: 2.6011715Losses:  2.289628028869629 0.029217004776000977
CurrentTrain: epoch  6, batch     3 | loss: 2.3188450Losses:  1.988565444946289 0.5963945388793945
CurrentTrain: epoch  7, batch     0 | loss: 2.5849600Losses:  1.9205135107040405 0.629043698310852
CurrentTrain: epoch  7, batch     1 | loss: 2.5495572Losses:  2.3574421405792236 0.5506989359855652
CurrentTrain: epoch  7, batch     2 | loss: 2.9081411Losses:  2.543776273727417 0.0385548435151577
CurrentTrain: epoch  7, batch     3 | loss: 2.5823312Losses:  1.84641695022583 0.5292534828186035
CurrentTrain: epoch  8, batch     0 | loss: 2.3756704Losses:  2.0168814659118652 0.5774564743041992
CurrentTrain: epoch  8, batch     1 | loss: 2.5943379Losses:  1.9832756519317627 0.6307176947593689
CurrentTrain: epoch  8, batch     2 | loss: 2.6139934Losses:  3.148460865020752 2.9802322387695312e-08
CurrentTrain: epoch  8, batch     3 | loss: 3.1484609Losses:  1.9901043176651 0.4696427881717682
CurrentTrain: epoch  9, batch     0 | loss: 2.4597471Losses:  1.9751567840576172 0.596214771270752
CurrentTrain: epoch  9, batch     1 | loss: 2.5713716Losses:  1.929161548614502 0.45221564173698425
CurrentTrain: epoch  9, batch     2 | loss: 2.3813772Losses:  1.752840518951416 0.04082613065838814
CurrentTrain: epoch  9, batch     3 | loss: 1.7936666
Losses:  6.346168041229248 1.0299744606018066
MemoryTrain:  epoch  0, batch     0 | loss: 7.3761425Losses:  10.811702728271484 0.6885481476783752
MemoryTrain:  epoch  0, batch     1 | loss: 11.5002508Losses:  1.7157039642333984 0.8602743148803711
MemoryTrain:  epoch  1, batch     0 | loss: 2.5759783Losses:  1.1920264959335327 0.8728629946708679
MemoryTrain:  epoch  1, batch     1 | loss: 2.0648894Losses:  0.8409572839736938 0.9338022470474243
MemoryTrain:  epoch  2, batch     0 | loss: 1.7747595Losses:  1.4768589735031128 0.6910644769668579
MemoryTrain:  epoch  2, batch     1 | loss: 2.1679235Losses:  0.7233083248138428 0.8650983572006226
MemoryTrain:  epoch  3, batch     0 | loss: 1.5884067Losses:  1.0443363189697266 0.7004287242889404
MemoryTrain:  epoch  3, batch     1 | loss: 1.7447650Losses:  1.069395661354065 0.8011910319328308
MemoryTrain:  epoch  4, batch     0 | loss: 1.8705866Losses:  0.5924617648124695 0.9201067686080933
MemoryTrain:  epoch  4, batch     1 | loss: 1.5125685Losses:  0.4606708884239197 0.9576680660247803
MemoryTrain:  epoch  5, batch     0 | loss: 1.4183390Losses:  0.9830888509750366 0.7411360144615173
MemoryTrain:  epoch  5, batch     1 | loss: 1.7242248Losses:  0.6970878839492798 0.8236212730407715
MemoryTrain:  epoch  6, batch     0 | loss: 1.5207092Losses:  0.6221696734428406 0.7579418420791626
MemoryTrain:  epoch  6, batch     1 | loss: 1.3801115Losses:  0.6846761703491211 0.9792194366455078
MemoryTrain:  epoch  7, batch     0 | loss: 1.6638956Losses:  0.3815764784812927 0.6295412182807922
MemoryTrain:  epoch  7, batch     1 | loss: 1.0111177Losses:  0.6256831884384155 0.9952524900436401
MemoryTrain:  epoch  8, batch     0 | loss: 1.6209357Losses:  0.4801647663116455 0.46470698714256287
MemoryTrain:  epoch  8, batch     1 | loss: 0.9448718Losses:  0.4446313977241516 0.8665429949760437
MemoryTrain:  epoch  9, batch     0 | loss: 1.3111744Losses:  0.49152329564094543 0.5714425444602966
MemoryTrain:  epoch  9, batch     1 | loss: 1.0629659
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 95.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 94.71%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 87.81%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 74.14%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 69.96%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 69.34%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 68.39%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 68.41%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 67.76%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 67.84%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 65.03%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 64.19%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 63.52%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 63.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 69.64%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.31%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 93.53%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.64%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 93.33%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.04%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.36%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.92%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 89.52%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 88.35%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 87.22%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 86.12%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 85.42%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 85.27%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 85.33%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.36%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 85.30%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 85.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 85.31%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.52%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.73%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 85.21%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 84.71%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 84.08%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 83.46%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 82.85%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 82.11%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 81.39%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 80.48%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 79.58%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 78.71%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 77.85%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 77.08%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 76.40%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 76.58%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 76.80%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 76.89%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 77.56%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 77.63%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 77.37%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 77.06%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 76.63%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 76.38%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 76.26%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.35%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 76.43%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.37%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 75.99%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 75.72%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 75.51%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 75.30%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 75.10%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 74.95%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 75.10%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 75.25%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 76.74%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 76.72%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 76.70%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 76.78%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 76.72%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 76.71%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 76.53%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 76.47%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 76.33%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 75.83%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 75.49%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 75.16%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 74.80%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 74.40%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 73.96%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 73.81%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 73.66%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 73.52%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 73.49%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 73.46%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 73.27%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 73.32%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 73.19%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 73.13%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 73.18%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 73.11%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 72.55%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.24%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 71.97%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 71.73%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 71.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 72.29%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 73.17%   
cur_acc:  ['0.9444', '0.5913', '0.6964']
his_acc:  ['0.9444', '0.7705', '0.7317']
Clustering into  19  clusters
Clusters:  [ 1 11 15  0  1  1 16  1 12 17 10  0  1  0  0  9  1  1  1  1  1  1  1  4
  1  7  1 18 13  5  8  1 14  1  6  2  3  1  1  1]
Losses:  7.439762115478516 1.5121829509735107
CurrentTrain: epoch  0, batch     0 | loss: 8.9519453Losses:  9.785062789916992 1.5777380466461182
CurrentTrain: epoch  0, batch     1 | loss: 11.3628006Losses:  8.464603424072266 1.350982427597046
CurrentTrain: epoch  0, batch     2 | loss: 9.8155861Losses:  6.261679172515869 0.3375243544578552
CurrentTrain: epoch  0, batch     3 | loss: 6.5992036Losses:  4.752942085266113 1.5108559131622314
CurrentTrain: epoch  1, batch     0 | loss: 6.2637978Losses:  3.8639473915100098 1.4396837949752808
CurrentTrain: epoch  1, batch     1 | loss: 5.3036313Losses:  3.8126540184020996 1.3491268157958984
CurrentTrain: epoch  1, batch     2 | loss: 5.1617808Losses:  5.151030540466309 0.32940739393234253
CurrentTrain: epoch  1, batch     3 | loss: 5.4804378Losses:  3.2296369075775146 1.281207799911499
CurrentTrain: epoch  2, batch     0 | loss: 4.5108447Losses:  4.208573341369629 1.279374122619629
CurrentTrain: epoch  2, batch     1 | loss: 5.4879475Losses:  4.2428178787231445 1.267944097518921
CurrentTrain: epoch  2, batch     2 | loss: 5.5107622Losses:  2.3869547843933105 2.9802322387695312e-08
CurrentTrain: epoch  2, batch     3 | loss: 2.3869548Losses:  2.668128490447998 0.7606618404388428
CurrentTrain: epoch  3, batch     0 | loss: 3.4287903Losses:  4.095860958099365 1.1808007955551147
CurrentTrain: epoch  3, batch     1 | loss: 5.2766619Losses:  4.277757167816162 1.1510531902313232
CurrentTrain: epoch  3, batch     2 | loss: 5.4288101Losses:  2.2583818435668945 0.23605498671531677
CurrentTrain: epoch  3, batch     3 | loss: 2.4944367Losses:  3.6383166313171387 1.1756975650787354
CurrentTrain: epoch  4, batch     0 | loss: 4.8140144Losses:  3.3152356147766113 0.8273505568504333
CurrentTrain: epoch  4, batch     1 | loss: 4.1425862Losses:  3.451537847518921 1.1299779415130615
CurrentTrain: epoch  4, batch     2 | loss: 4.5815158Losses:  2.898613929748535 0.12984153628349304
CurrentTrain: epoch  4, batch     3 | loss: 3.0284555Losses:  3.5296823978424072 1.2633492946624756
CurrentTrain: epoch  5, batch     0 | loss: 4.7930317Losses:  2.9444878101348877 1.164720058441162
CurrentTrain: epoch  5, batch     1 | loss: 4.1092081Losses:  3.483820915222168 0.9569375514984131
CurrentTrain: epoch  5, batch     2 | loss: 4.4407587Losses:  1.7417097091674805 5.960464477539063e-08
CurrentTrain: epoch  5, batch     3 | loss: 1.7417097Losses:  3.2849669456481934 1.2254676818847656
CurrentTrain: epoch  6, batch     0 | loss: 4.5104346Losses:  3.2389423847198486 0.8670815229415894
CurrentTrain: epoch  6, batch     1 | loss: 4.1060238Losses:  2.91037654876709 1.0249851942062378
CurrentTrain: epoch  6, batch     2 | loss: 3.9353619Losses:  2.179807662963867 0.11239413917064667
CurrentTrain: epoch  6, batch     3 | loss: 2.2922018Losses:  3.5871686935424805 0.8702949285507202
CurrentTrain: epoch  7, batch     0 | loss: 4.4574637Losses:  2.962090015411377 1.1252074241638184
CurrentTrain: epoch  7, batch     1 | loss: 4.0872974Losses:  2.6295523643493652 0.9357937574386597
CurrentTrain: epoch  7, batch     2 | loss: 3.5653462Losses:  1.763443946838379 0.10397626459598541
CurrentTrain: epoch  7, batch     3 | loss: 1.8674202Losses:  2.783031463623047 0.9389390349388123
CurrentTrain: epoch  8, batch     0 | loss: 3.7219706Losses:  2.815206527709961 0.9762704968452454
CurrentTrain: epoch  8, batch     1 | loss: 3.7914770Losses:  3.2144393920898438 0.7615777254104614
CurrentTrain: epoch  8, batch     2 | loss: 3.9760170Losses:  1.8187804222106934 0.11341279000043869
CurrentTrain: epoch  8, batch     3 | loss: 1.9321932Losses:  2.827113151550293 0.8170203566551208
CurrentTrain: epoch  9, batch     0 | loss: 3.6441336Losses:  2.7986485958099365 0.9249036312103271
CurrentTrain: epoch  9, batch     1 | loss: 3.7235522Losses:  2.4901905059814453 0.8441831469535828
CurrentTrain: epoch  9, batch     2 | loss: 3.3343737Losses:  3.244434356689453 0.21826905012130737
CurrentTrain: epoch  9, batch     3 | loss: 3.4627035
Losses:  5.976605415344238 0.9526944756507874
MemoryTrain:  epoch  0, batch     0 | loss: 6.9292998Losses:  10.568458557128906 1.043424129486084
MemoryTrain:  epoch  0, batch     1 | loss: 11.6118832Losses:  11.128735542297363 0.4896450638771057
MemoryTrain:  epoch  0, batch     2 | loss: 11.6183805Losses:  1.1222907304763794 0.9725469350814819
MemoryTrain:  epoch  1, batch     0 | loss: 2.0948377Losses:  0.9916120767593384 0.7667269706726074
MemoryTrain:  epoch  1, batch     1 | loss: 1.7583390Losses:  0.789225161075592 0.5929186940193176
MemoryTrain:  epoch  1, batch     2 | loss: 1.3821439Losses:  0.923082709312439 0.998358964920044
MemoryTrain:  epoch  2, batch     0 | loss: 1.9214417Losses:  0.6912291049957275 0.8135287761688232
MemoryTrain:  epoch  2, batch     1 | loss: 1.5047579Losses:  0.6929658651351929 0.3457145094871521
MemoryTrain:  epoch  2, batch     2 | loss: 1.0386803Losses:  0.6968972682952881 0.9225199818611145
MemoryTrain:  epoch  3, batch     0 | loss: 1.6194172Losses:  0.6239707469940186 0.748315691947937
MemoryTrain:  epoch  3, batch     1 | loss: 1.3722864Losses:  0.7340742349624634 0.5361953973770142
MemoryTrain:  epoch  3, batch     2 | loss: 1.2702696Losses:  0.6872752904891968 1.071171760559082
MemoryTrain:  epoch  4, batch     0 | loss: 1.7584471Losses:  0.4536934494972229 0.7660967111587524
MemoryTrain:  epoch  4, batch     1 | loss: 1.2197902Losses:  0.6328601837158203 0.5147239565849304
MemoryTrain:  epoch  4, batch     2 | loss: 1.1475842Losses:  0.35261601209640503 0.6866388916969299
MemoryTrain:  epoch  5, batch     0 | loss: 1.0392549Losses:  0.44451332092285156 0.9991844892501831
MemoryTrain:  epoch  5, batch     1 | loss: 1.4436978Losses:  0.7092120051383972 0.6473093032836914
MemoryTrain:  epoch  5, batch     2 | loss: 1.3565214Losses:  0.3942077159881592 0.8561125993728638
MemoryTrain:  epoch  6, batch     0 | loss: 1.2503203Losses:  0.3758792281150818 0.7087891101837158
MemoryTrain:  epoch  6, batch     1 | loss: 1.0846684Losses:  0.4947701096534729 0.6888649463653564
MemoryTrain:  epoch  6, batch     2 | loss: 1.1836350Losses:  0.4164174199104309 0.879525899887085
MemoryTrain:  epoch  7, batch     0 | loss: 1.2959433Losses:  0.32518863677978516 0.6885056495666504
MemoryTrain:  epoch  7, batch     1 | loss: 1.0136943Losses:  0.44929707050323486 0.4894775152206421
MemoryTrain:  epoch  7, batch     2 | loss: 0.9387746Losses:  0.4009345769882202 0.6833418607711792
MemoryTrain:  epoch  8, batch     0 | loss: 1.0842764Losses:  0.4611389636993408 0.9786291122436523
MemoryTrain:  epoch  8, batch     1 | loss: 1.4397681Losses:  0.3808802664279938 0.4017848074436188
MemoryTrain:  epoch  8, batch     2 | loss: 0.7826651Losses:  0.4370375871658325 0.9994044303894043
MemoryTrain:  epoch  9, batch     0 | loss: 1.4364420Losses:  0.31392407417297363 0.639433741569519
MemoryTrain:  epoch  9, batch     1 | loss: 0.9533578Losses:  0.36590147018432617 0.35669445991516113
MemoryTrain:  epoch  9, batch     2 | loss: 0.7225959
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 54.35%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 53.39%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 51.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 53.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 55.32%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 56.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 57.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 60.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 66.92%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.61%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 64.54%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 63.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 64.34%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 64.83%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 64.48%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 63.42%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 63.10%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.40%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 93.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 93.89%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.87%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 92.71%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.42%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.67%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 90.23%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 88.94%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 87.78%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 86.75%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 85.66%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 84.96%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.22%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 85.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.23%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.44%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 85.55%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 85.21%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 84.86%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 84.38%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 84.19%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 83.72%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 83.12%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 82.53%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 81.60%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 80.69%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 79.81%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 79.01%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 78.23%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 77.53%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 78.00%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 77.94%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.19%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 78.34%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.55%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 78.30%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 77.92%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 77.44%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 77.31%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 76.95%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 76.55%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 76.37%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.35%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 76.28%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 76.16%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 75.78%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.57%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 75.31%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 75.05%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 74.85%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 74.65%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 74.80%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 74.90%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.86%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 75.94%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.98%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 75.97%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 75.78%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 75.77%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 75.55%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 75.51%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 75.46%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 75.33%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 74.83%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 74.38%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 74.02%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 73.58%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 73.15%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.68%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 72.45%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 72.31%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 72.13%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 71.99%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 71.89%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 71.80%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 71.74%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 71.61%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.29%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 70.94%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 70.64%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 70.34%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 70.04%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 69.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 71.26%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 71.64%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 71.30%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 71.02%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 70.71%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 70.48%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 70.24%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 70.10%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 70.13%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 70.02%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  200 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 70.44%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 70.16%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 69.94%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 69.73%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 69.85%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 70.99%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 70.95%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 70.89%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 70.88%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 70.91%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 70.93%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 70.73%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 70.51%   [EVAL] batch:  235 | acc: 37.50%,  total acc: 70.37%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 70.20%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 70.36%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.43%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 70.22%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 70.04%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 69.86%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 69.60%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 69.55%   
cur_acc:  ['0.9444', '0.5913', '0.6964', '0.6240']
his_acc:  ['0.9444', '0.7705', '0.7317', '0.6955']
Clustering into  24  clusters
Clusters:  [ 2  1 15  0  2  2 19  2 14 21 22  0  2  0  0 20  2  2  2  2  2  2  2  9
  2 16  2 23 13 12 18  2 11  2  7 17  8  2  2  2  1  6  2  2  2  3  2 10
  5  4]
Losses:  6.44020938873291 1.329254150390625
CurrentTrain: epoch  0, batch     0 | loss: 7.7694635Losses:  8.125953674316406 1.6032408475875854
CurrentTrain: epoch  0, batch     1 | loss: 9.7291946Losses:  6.9277544021606445 1.4661476612091064
CurrentTrain: epoch  0, batch     2 | loss: 8.3939018Losses:  3.966292381286621 0.14073985815048218
CurrentTrain: epoch  0, batch     3 | loss: 4.1070323Losses:  3.7312943935394287 1.0507534742355347
CurrentTrain: epoch  1, batch     0 | loss: 4.7820477Losses:  3.315438747406006 1.3840240240097046
CurrentTrain: epoch  1, batch     1 | loss: 4.6994629Losses:  3.122769832611084 1.197127103805542
CurrentTrain: epoch  1, batch     2 | loss: 4.3198967Losses:  2.8898229598999023 0.39007896184921265
CurrentTrain: epoch  1, batch     3 | loss: 3.2799020Losses:  3.4162471294403076 0.9887365102767944
CurrentTrain: epoch  2, batch     0 | loss: 4.4049835Losses:  3.1399693489074707 1.0612109899520874
CurrentTrain: epoch  2, batch     1 | loss: 4.2011805Losses:  2.6699981689453125 1.0171136856079102
CurrentTrain: epoch  2, batch     2 | loss: 3.6871119Losses:  4.136545658111572 0.41818708181381226
CurrentTrain: epoch  2, batch     3 | loss: 4.5547328Losses:  2.7789697647094727 1.0181726217269897
CurrentTrain: epoch  3, batch     0 | loss: 3.7971425Losses:  2.8511180877685547 0.8547316193580627
CurrentTrain: epoch  3, batch     1 | loss: 3.7058496Losses:  2.7066400051116943 0.9849302768707275
CurrentTrain: epoch  3, batch     2 | loss: 3.6915703Losses:  4.785490989685059 0.3715403974056244
CurrentTrain: epoch  3, batch     3 | loss: 5.1570315Losses:  2.5411946773529053 0.8122937679290771
CurrentTrain: epoch  4, batch     0 | loss: 3.3534884Losses:  3.022822856903076 0.8291930556297302
CurrentTrain: epoch  4, batch     1 | loss: 3.8520160Losses:  2.3983585834503174 0.987742006778717
CurrentTrain: epoch  4, batch     2 | loss: 3.3861005Losses:  3.6435132026672363 0.3120696246623993
CurrentTrain: epoch  4, batch     3 | loss: 3.9555829Losses:  2.6933600902557373 1.0506818294525146
CurrentTrain: epoch  5, batch     0 | loss: 3.7440419Losses:  2.3897554874420166 0.9690657258033752
CurrentTrain: epoch  5, batch     1 | loss: 3.3588212Losses:  2.4824016094207764 0.8596497178077698
CurrentTrain: epoch  5, batch     2 | loss: 3.3420513Losses:  2.3473522663116455 0.16719113290309906
CurrentTrain: epoch  5, batch     3 | loss: 2.5145433Losses:  2.4084134101867676 0.7840951085090637
CurrentTrain: epoch  6, batch     0 | loss: 3.1925085Losses:  2.5877435207366943 0.9353333711624146
CurrentTrain: epoch  6, batch     1 | loss: 3.5230770Losses:  2.283568859100342 0.8614261150360107
CurrentTrain: epoch  6, batch     2 | loss: 3.1449950Losses:  1.8798998594284058 0.14939065277576447
CurrentTrain: epoch  6, batch     3 | loss: 2.0292904Losses:  2.308537006378174 0.6513176560401917
CurrentTrain: epoch  7, batch     0 | loss: 2.9598546Losses:  2.295846462249756 0.6469987034797668
CurrentTrain: epoch  7, batch     1 | loss: 2.9428451Losses:  2.362196922302246 0.7949658632278442
CurrentTrain: epoch  7, batch     2 | loss: 3.1571627Losses:  2.720931053161621 0.35202306509017944
CurrentTrain: epoch  7, batch     3 | loss: 3.0729542Losses:  2.1823465824127197 0.7137787342071533
CurrentTrain: epoch  8, batch     0 | loss: 2.8961253Losses:  2.2509703636169434 0.7256748080253601
CurrentTrain: epoch  8, batch     1 | loss: 2.9766452Losses:  2.3655967712402344 0.6111741065979004
CurrentTrain: epoch  8, batch     2 | loss: 2.9767709Losses:  1.8290283679962158 0.09410671889781952
CurrentTrain: epoch  8, batch     3 | loss: 1.9231350Losses:  2.1759791374206543 0.7210512161254883
CurrentTrain: epoch  9, batch     0 | loss: 2.8970304Losses:  1.9739010334014893 0.47481781244277954
CurrentTrain: epoch  9, batch     1 | loss: 2.4487188Losses:  2.264737129211426 0.6542303562164307
CurrentTrain: epoch  9, batch     2 | loss: 2.9189675Losses:  2.3617753982543945 0.06176495552062988
CurrentTrain: epoch  9, batch     3 | loss: 2.4235404
Losses:  5.854403018951416 0.8414632081985474
MemoryTrain:  epoch  0, batch     0 | loss: 6.6958661Losses:  10.369327545166016 0.9586547613143921
MemoryTrain:  epoch  0, batch     1 | loss: 11.3279819Losses:  9.78777027130127 0.7217049598693848
MemoryTrain:  epoch  0, batch     2 | loss: 10.5094757Losses:  10.820818901062012 0.14836078882217407
MemoryTrain:  epoch  0, batch     3 | loss: 10.9691801Losses:  1.3397552967071533 0.9584132432937622
MemoryTrain:  epoch  1, batch     0 | loss: 2.2981687Losses:  0.6959511637687683 0.6940326690673828
MemoryTrain:  epoch  1, batch     1 | loss: 1.3899839Losses:  0.9795928001403809 0.8889676332473755
MemoryTrain:  epoch  1, batch     2 | loss: 1.8685604Losses:  0.7659515738487244 0.05207376182079315
MemoryTrain:  epoch  1, batch     3 | loss: 0.8180254Losses:  0.663732647895813 0.7086457014083862
MemoryTrain:  epoch  2, batch     0 | loss: 1.3723783Losses:  0.9367550611495972 0.7376230955123901
MemoryTrain:  epoch  2, batch     1 | loss: 1.6743782Losses:  1.0320332050323486 1.1058640480041504
MemoryTrain:  epoch  2, batch     2 | loss: 2.1378973Losses:  0.257644385099411 0.029234187677502632
MemoryTrain:  epoch  2, batch     3 | loss: 0.2868786Losses:  0.5987167358398438 0.8338581919670105
MemoryTrain:  epoch  3, batch     0 | loss: 1.4325750Losses:  0.8187804818153381 0.9649739265441895
MemoryTrain:  epoch  3, batch     1 | loss: 1.7837543Losses:  0.5666611194610596 0.6613315939903259
MemoryTrain:  epoch  3, batch     2 | loss: 1.2279928Losses:  0.17081227898597717 0.07015818357467651
MemoryTrain:  epoch  3, batch     3 | loss: 0.2409705Losses:  0.659959614276886 0.7947163581848145
MemoryTrain:  epoch  4, batch     0 | loss: 1.4546759Losses:  0.5573626756668091 0.7744694352149963
MemoryTrain:  epoch  4, batch     1 | loss: 1.3318322Losses:  0.46907731890678406 0.8986037373542786
MemoryTrain:  epoch  4, batch     2 | loss: 1.3676810Losses:  0.8357678651809692 0.006780401803553104
MemoryTrain:  epoch  4, batch     3 | loss: 0.8425483Losses:  0.6306349039077759 0.9356240630149841
MemoryTrain:  epoch  5, batch     0 | loss: 1.5662589Losses:  0.4568291902542114 0.756858229637146
MemoryTrain:  epoch  5, batch     1 | loss: 1.2136874Losses:  0.5067469477653503 0.800847053527832
MemoryTrain:  epoch  5, batch     2 | loss: 1.3075941Losses:  0.14168594777584076 0.016425510868430138
MemoryTrain:  epoch  5, batch     3 | loss: 0.1581115Losses:  0.3622755706310272 0.74582439661026
MemoryTrain:  epoch  6, batch     0 | loss: 1.1080999Losses:  0.39537471532821655 0.7612707614898682
MemoryTrain:  epoch  6, batch     1 | loss: 1.1566455Losses:  0.47487279772758484 0.803757905960083
MemoryTrain:  epoch  6, batch     2 | loss: 1.2786307Losses:  0.7816193103790283 0.13117004930973053
MemoryTrain:  epoch  6, batch     3 | loss: 0.9127893Losses:  0.5639676451683044 0.9068164825439453
MemoryTrain:  epoch  7, batch     0 | loss: 1.4707842Losses:  0.3139170706272125 0.701612114906311
MemoryTrain:  epoch  7, batch     1 | loss: 1.0155292Losses:  0.3888268768787384 0.6986030340194702
MemoryTrain:  epoch  7, batch     2 | loss: 1.0874299Losses:  0.45269834995269775 0.08765091001987457
MemoryTrain:  epoch  7, batch     3 | loss: 0.5403492Losses:  0.4119681119918823 0.725864052772522
MemoryTrain:  epoch  8, batch     0 | loss: 1.1378322Losses:  0.45550015568733215 0.8346052169799805
MemoryTrain:  epoch  8, batch     1 | loss: 1.2901053Losses:  0.4041880667209625 0.8323879241943359
MemoryTrain:  epoch  8, batch     2 | loss: 1.2365760Losses:  0.26235896348953247 0.013815054669976234
MemoryTrain:  epoch  8, batch     3 | loss: 0.2761740Losses:  0.39191073179244995 0.6584336757659912
MemoryTrain:  epoch  9, batch     0 | loss: 1.0503445Losses:  0.4203144311904907 0.8386474847793579
MemoryTrain:  epoch  9, batch     1 | loss: 1.2589619Losses:  0.43417298793792725 0.8104261755943298
MemoryTrain:  epoch  9, batch     2 | loss: 1.2445991Losses:  0.3698517382144928 0.043800804764032364
MemoryTrain:  epoch  9, batch     3 | loss: 0.4136525
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 43.75%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 31.25%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 73.67%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 68.61%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 67.42%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 66.80%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.31%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.06%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.25%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 84.55%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 84.76%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 84.75%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.79%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.73%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.78%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 84.23%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 82.91%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 81.73%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 80.68%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 79.76%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 78.77%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 78.08%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 77.95%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 78.08%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 78.21%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 78.63%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 78.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 78.81%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 79.19%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 79.04%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 78.69%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 78.27%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 78.09%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 77.62%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 77.08%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 76.42%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 75.63%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 74.79%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 73.97%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 73.23%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 72.51%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 71.94%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 72.11%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 72.42%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 72.64%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 73.22%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 73.77%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 73.09%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 72.48%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.93%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 71.40%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 70.76%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 70.35%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 70.33%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 70.16%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 69.99%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 69.77%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 69.51%   [EVAL] batch:  123 | acc: 43.75%,  total acc: 69.30%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 69.34%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.29%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 70.37%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 70.85%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 70.86%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 70.69%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 70.76%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 70.58%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 70.57%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 70.03%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 69.65%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 69.28%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 68.87%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 68.47%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 68.03%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 67.83%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 67.76%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 67.65%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 67.47%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 67.48%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 67.39%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 67.37%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 67.34%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 67.10%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 66.74%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 66.42%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 65.88%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 65.64%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 67.72%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:  189 | acc: 0.00%,  total acc: 67.04%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 66.69%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 65.82%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 66.01%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 66.17%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 66.03%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 65.61%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 65.40%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 67.26%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 67.27%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 67.33%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 67.07%   [EVAL] batch:  235 | acc: 31.25%,  total acc: 66.92%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 66.75%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 66.93%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 67.02%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 67.10%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 66.82%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 66.65%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 66.48%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 66.27%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 66.28%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 66.62%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 66.79%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 66.92%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  275 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:  276 | acc: 31.25%,  total acc: 67.58%   [EVAL] batch:  277 | acc: 31.25%,  total acc: 67.45%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:  279 | acc: 31.25%,  total acc: 67.23%   [EVAL] batch:  280 | acc: 43.75%,  total acc: 67.15%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 67.09%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 66.90%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 66.80%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 66.63%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 66.42%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 66.61%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 66.57%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 66.47%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 66.39%   [EVAL] batch:  297 | acc: 37.50%,  total acc: 66.30%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 66.22%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.46%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.35%   
cur_acc:  ['0.9444', '0.5913', '0.6964', '0.6240', '0.7192']
his_acc:  ['0.9444', '0.7705', '0.7317', '0.6955', '0.6735']
Clustering into  29  clusters
Clusters:  [ 0  1 21  0  0  0 25  0 15 27 16  0  0  2  2  7  0  0  0  0  0  0  0 28
  0 19  0 23 17 22 24  0 14  0 18 20  9  0  0  0  1  8  0  0  0  3  0 13
 26 11 10  4  2  0  0 12  0  6  0  5]
Losses:  6.730586051940918 1.1824798583984375
CurrentTrain: epoch  0, batch     0 | loss: 7.9130659Losses:  8.847251892089844 1.5125151872634888
CurrentTrain: epoch  0, batch     1 | loss: 10.3597670Losses:  7.139206886291504 1.6430692672729492
CurrentTrain: epoch  0, batch     2 | loss: 8.7822762Losses:  7.290653228759766 0.3173755407333374
CurrentTrain: epoch  0, batch     3 | loss: 7.6080289Losses:  3.254575729370117 1.2711074352264404
CurrentTrain: epoch  1, batch     0 | loss: 4.5256834Losses:  3.694775104522705 1.302746295928955
CurrentTrain: epoch  1, batch     1 | loss: 4.9975214Losses:  3.8373124599456787 1.7145801782608032
CurrentTrain: epoch  1, batch     2 | loss: 5.5518928Losses:  6.02503776550293 0.7125773429870605
CurrentTrain: epoch  1, batch     3 | loss: 6.7376151Losses:  3.4454846382141113 1.2783091068267822
CurrentTrain: epoch  2, batch     0 | loss: 4.7237940Losses:  3.6935794353485107 1.4094364643096924
CurrentTrain: epoch  2, batch     1 | loss: 5.1030159Losses:  3.5000064373016357 1.282743215560913
CurrentTrain: epoch  2, batch     2 | loss: 4.7827497Losses:  2.491309642791748 0.23831945657730103
CurrentTrain: epoch  2, batch     3 | loss: 2.7296290Losses:  3.1218721866607666 1.2776823043823242
CurrentTrain: epoch  3, batch     0 | loss: 4.3995543Losses:  4.043124198913574 1.3169629573822021
CurrentTrain: epoch  3, batch     1 | loss: 5.3600874Losses:  2.6120285987854004 0.8539717793464661
CurrentTrain: epoch  3, batch     2 | loss: 3.4660003Losses:  4.647856712341309 0.8421220779418945
CurrentTrain: epoch  3, batch     3 | loss: 5.4899788Losses:  3.407754898071289 1.3345391750335693
CurrentTrain: epoch  4, batch     0 | loss: 4.7422943Losses:  3.0403947830200195 1.050595760345459
CurrentTrain: epoch  4, batch     1 | loss: 4.0909905Losses:  2.631828784942627 0.8322737216949463
CurrentTrain: epoch  4, batch     2 | loss: 3.4641025Losses:  2.3840742111206055 0.5388870239257812
CurrentTrain: epoch  4, batch     3 | loss: 2.9229612Losses:  3.1237587928771973 1.011621356010437
CurrentTrain: epoch  5, batch     0 | loss: 4.1353803Losses:  3.281369924545288 1.1910169124603271
CurrentTrain: epoch  5, batch     1 | loss: 4.4723868Losses:  2.3247904777526855 0.5517591834068298
CurrentTrain: epoch  5, batch     2 | loss: 2.8765497Losses:  3.4137511253356934 0.2471161186695099
CurrentTrain: epoch  5, batch     3 | loss: 3.6608672Losses:  2.6806674003601074 0.9561786651611328
CurrentTrain: epoch  6, batch     0 | loss: 3.6368461Losses:  2.7963638305664062 0.9908931851387024
CurrentTrain: epoch  6, batch     1 | loss: 3.7872570Losses:  2.95460844039917 0.8330967426300049
CurrentTrain: epoch  6, batch     2 | loss: 3.7877052Losses:  3.794656276702881 0.10181860625743866
CurrentTrain: epoch  6, batch     3 | loss: 3.8964748Losses:  2.7676897048950195 0.9307938814163208
CurrentTrain: epoch  7, batch     0 | loss: 3.6984835Losses:  2.7312326431274414 0.9611818790435791
CurrentTrain: epoch  7, batch     1 | loss: 3.6924145Losses:  2.6654062271118164 0.8970445990562439
CurrentTrain: epoch  7, batch     2 | loss: 3.5624509Losses:  1.9880681037902832 0.0619274377822876
CurrentTrain: epoch  7, batch     3 | loss: 2.0499954Losses:  2.6973626613616943 0.9021888375282288
CurrentTrain: epoch  8, batch     0 | loss: 3.5995514Losses:  2.4979324340820312 0.6822125911712646
CurrentTrain: epoch  8, batch     1 | loss: 3.1801450Losses:  2.592968702316284 0.8293823599815369
CurrentTrain: epoch  8, batch     2 | loss: 3.4223511Losses:  2.0203967094421387 2.9802322387695312e-08
CurrentTrain: epoch  8, batch     3 | loss: 2.0203967Losses:  2.227956771850586 0.567479133605957
CurrentTrain: epoch  9, batch     0 | loss: 2.7954359Losses:  2.6822447776794434 0.8481347560882568
CurrentTrain: epoch  9, batch     1 | loss: 3.5303795Losses:  2.468357563018799 0.818089485168457
CurrentTrain: epoch  9, batch     2 | loss: 3.2864470Losses:  1.8262875080108643 0.09997963160276413
CurrentTrain: epoch  9, batch     3 | loss: 1.9262671
Losses:  6.003355026245117 0.682635486125946
MemoryTrain:  epoch  0, batch     0 | loss: 6.6859903Losses:  8.95550537109375 1.0190494060516357
MemoryTrain:  epoch  0, batch     1 | loss: 9.9745550Losses:  9.781389236450195 0.7168928384780884
MemoryTrain:  epoch  0, batch     2 | loss: 10.4982824Losses:  11.352038383483887 0.873360812664032
MemoryTrain:  epoch  0, batch     3 | loss: 12.2253990Losses:  1.0259222984313965 0.8940376043319702
MemoryTrain:  epoch  1, batch     0 | loss: 1.9199599Losses:  0.6904550790786743 0.8247065544128418
MemoryTrain:  epoch  1, batch     1 | loss: 1.5151616Losses:  1.0409435033798218 0.8960156440734863
MemoryTrain:  epoch  1, batch     2 | loss: 1.9369591Losses:  0.6642637252807617 0.522068202495575
MemoryTrain:  epoch  1, batch     3 | loss: 1.1863320Losses:  0.6226118803024292 0.9014525413513184
MemoryTrain:  epoch  2, batch     0 | loss: 1.5240644Losses:  1.0944538116455078 0.7458654046058655
MemoryTrain:  epoch  2, batch     1 | loss: 1.8403192Losses:  0.624382734298706 0.8084211349487305
MemoryTrain:  epoch  2, batch     2 | loss: 1.4328039Losses:  0.6579452753067017 0.6123316884040833
MemoryTrain:  epoch  2, batch     3 | loss: 1.2702770Losses:  0.6432625651359558 0.6847907304763794
MemoryTrain:  epoch  3, batch     0 | loss: 1.3280532Losses:  0.6744446754455566 0.9495452642440796
MemoryTrain:  epoch  3, batch     1 | loss: 1.6239899Losses:  0.7153173685073853 0.8167468309402466
MemoryTrain:  epoch  3, batch     2 | loss: 1.5320642Losses:  0.632901132106781 0.5556796789169312
MemoryTrain:  epoch  3, batch     3 | loss: 1.1885808Losses:  0.523351788520813 0.6400106549263
MemoryTrain:  epoch  4, batch     0 | loss: 1.1633625Losses:  0.6695130467414856 0.7778466939926147
MemoryTrain:  epoch  4, batch     1 | loss: 1.4473598Losses:  0.5324535369873047 0.9073103666305542
MemoryTrain:  epoch  4, batch     2 | loss: 1.4397639Losses:  0.6272656917572021 0.5680410861968994
MemoryTrain:  epoch  4, batch     3 | loss: 1.1953068Losses:  0.5966775417327881 0.7827852368354797
MemoryTrain:  epoch  5, batch     0 | loss: 1.3794627Losses:  0.5168834328651428 0.7105640172958374
MemoryTrain:  epoch  5, batch     1 | loss: 1.2274475Losses:  0.4429248571395874 0.6958872079849243
MemoryTrain:  epoch  5, batch     2 | loss: 1.1388121Losses:  0.45172280073165894 0.6784342527389526
MemoryTrain:  epoch  5, batch     3 | loss: 1.1301570Losses:  0.529694676399231 0.8480244874954224
MemoryTrain:  epoch  6, batch     0 | loss: 1.3777192Losses:  0.44487449526786804 0.7895647287368774
MemoryTrain:  epoch  6, batch     1 | loss: 1.2344393Losses:  0.41940557956695557 0.7231815457344055
MemoryTrain:  epoch  6, batch     2 | loss: 1.1425872Losses:  0.4255315065383911 0.523753821849823
MemoryTrain:  epoch  6, batch     3 | loss: 0.9492853Losses:  0.4622611403465271 0.732616662979126
MemoryTrain:  epoch  7, batch     0 | loss: 1.1948779Losses:  0.388283908367157 0.6516036987304688
MemoryTrain:  epoch  7, batch     1 | loss: 1.0398877Losses:  0.544208288192749 0.9051805734634399
MemoryTrain:  epoch  7, batch     2 | loss: 1.4493889Losses:  0.41268694400787354 0.5456091165542603
MemoryTrain:  epoch  7, batch     3 | loss: 0.9582961Losses:  0.4222602844238281 0.7999470233917236
MemoryTrain:  epoch  8, batch     0 | loss: 1.2222073Losses:  0.4064624309539795 0.5133267045021057
MemoryTrain:  epoch  8, batch     1 | loss: 0.9197891Losses:  0.4383510947227478 0.7432246208190918
MemoryTrain:  epoch  8, batch     2 | loss: 1.1815758Losses:  0.47310611605644226 0.6199851036071777
MemoryTrain:  epoch  8, batch     3 | loss: 1.0930912Losses:  0.3738507628440857 0.6559622883796692
MemoryTrain:  epoch  9, batch     0 | loss: 1.0298131Losses:  0.40911632776260376 0.5848966836929321
MemoryTrain:  epoch  9, batch     1 | loss: 0.9940130Losses:  0.3916224539279938 0.702498733997345
MemoryTrain:  epoch  9, batch     2 | loss: 1.0941212Losses:  0.5987458825111389 0.7704787254333496
MemoryTrain:  epoch  9, batch     3 | loss: 1.3692245
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 61.81%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 59.82%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 58.41%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 56.67%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 55.04%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 55.47%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 56.44%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 57.72%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 60.64%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 61.35%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 61.43%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 61.63%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 62.91%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 64.50%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 64.43%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 64.40%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 65.67%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 78.26%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.65%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 85.37%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 85.03%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 85.08%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.86%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.30%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 85.09%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.43%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 84.02%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.23%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 81.93%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 80.67%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 79.55%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 78.36%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 77.30%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 76.54%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 76.58%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 76.82%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 77.28%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 77.25%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 76.68%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 76.58%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 75.85%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 75.23%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 74.77%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 74.26%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 73.97%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 73.47%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 72.84%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 72.23%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 71.42%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 70.62%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 69.85%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 69.09%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 68.41%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 67.89%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 69.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 70.15%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 69.50%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 68.92%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 68.41%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 67.85%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 67.24%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 66.76%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 66.63%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 66.81%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 66.75%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 66.48%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 66.39%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 66.18%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 66.05%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 68.16%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 68.02%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 67.94%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 67.51%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 67.19%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 66.91%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 66.60%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 66.37%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 66.07%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 65.68%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 65.59%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 65.57%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 65.45%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 65.42%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 65.36%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 65.15%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 64.80%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 64.50%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 64.23%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 63.97%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 63.75%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 64.16%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 65.89%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 65.58%   [EVAL] batch:  189 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 64.95%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 64.65%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 64.38%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 64.18%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.23%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 64.15%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 64.14%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 64.07%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.16%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 64.21%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 64.48%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 64.33%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 64.17%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 63.96%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.77%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 63.65%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 63.59%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.50%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 65.39%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 65.35%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 65.49%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 65.32%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 65.22%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 65.08%   [EVAL] batch:  235 | acc: 43.75%,  total acc: 64.99%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 64.85%   [EVAL] batch:  237 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 64.96%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 64.78%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 64.64%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 64.38%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 64.44%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 64.65%   [EVAL] batch:  253 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 64.99%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 64.96%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 64.98%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 64.89%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.91%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 64.87%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 64.80%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 64.69%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 65.32%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 65.18%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 65.04%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 64.95%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 64.79%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 64.37%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 64.34%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 64.54%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 64.62%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 64.55%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 64.56%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 64.51%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  315 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 65.81%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 66.26%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 66.06%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 65.93%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 65.73%   [EVAL] batch:  329 | acc: 6.25%,  total acc: 65.55%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 65.35%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 65.86%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 65.81%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 65.63%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 65.46%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 65.13%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 64.96%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 64.86%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 65.35%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 65.32%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  353 | acc: 50.00%,  total acc: 65.31%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 65.33%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 65.36%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 65.66%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 65.63%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 65.64%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 65.75%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 65.92%   
cur_acc:  ['0.9444', '0.5913', '0.6964', '0.6240', '0.7192', '0.6567']
his_acc:  ['0.9444', '0.7705', '0.7317', '0.6955', '0.6735', '0.6592']
Clustering into  34  clusters
Clusters:  [ 1  2 24  1  1  1 30  1 22  0 31  1  1  3  3 19  1  1  1  1  1  1  1 32
  1  0  1 27 17 26 25  1 33  1 20 23 21  1  1  1  2 15  1  1  1  9  1 16
 29 28 14 10  3  1  1 18  1  4  1 12  8 13  1  1  6 11  7  1  1  5]
Losses:  6.742489814758301 1.5160763263702393
CurrentTrain: epoch  0, batch     0 | loss: 8.2585659Losses:  7.926079750061035 1.7071901559829712
CurrentTrain: epoch  0, batch     1 | loss: 9.6332703Losses:  6.316869735717773 1.4126113653182983
CurrentTrain: epoch  0, batch     2 | loss: 7.7294812Losses:  5.8977861404418945 0.48520734906196594
CurrentTrain: epoch  0, batch     3 | loss: 6.3829937Losses:  2.89981746673584 1.241762638092041
CurrentTrain: epoch  1, batch     0 | loss: 4.1415801Losses:  3.2189488410949707 1.044978141784668
CurrentTrain: epoch  1, batch     1 | loss: 4.2639270Losses:  2.576023578643799 1.5038132667541504
CurrentTrain: epoch  1, batch     2 | loss: 4.0798368Losses:  2.4469895362854004 0.3078988790512085
CurrentTrain: epoch  1, batch     3 | loss: 2.7548885Losses:  2.7303848266601562 1.1403675079345703
CurrentTrain: epoch  2, batch     0 | loss: 3.8707523Losses:  2.6998801231384277 0.9181892275810242
CurrentTrain: epoch  2, batch     1 | loss: 3.6180694Losses:  2.572772741317749 1.3359994888305664
CurrentTrain: epoch  2, batch     2 | loss: 3.9087722Losses:  1.6757404804229736 2.9802322387695312e-08
CurrentTrain: epoch  2, batch     3 | loss: 1.6757405Losses:  2.4112370014190674 1.1739877462387085
CurrentTrain: epoch  3, batch     0 | loss: 3.5852246Losses:  2.3377532958984375 0.954871416091919
CurrentTrain: epoch  3, batch     1 | loss: 3.2926247Losses:  2.4516470432281494 1.1962027549743652
CurrentTrain: epoch  3, batch     2 | loss: 3.6478498Losses:  2.435523509979248 0.13757023215293884
CurrentTrain: epoch  3, batch     3 | loss: 2.5730937Losses:  2.5001091957092285 0.7803103923797607
CurrentTrain: epoch  4, batch     0 | loss: 3.2804196Losses:  2.453887462615967 1.2137911319732666
CurrentTrain: epoch  4, batch     1 | loss: 3.6676786Losses:  1.9384247064590454 0.6886652708053589
CurrentTrain: epoch  4, batch     2 | loss: 2.6270900Losses:  1.8660662174224854 0.1704295426607132
CurrentTrain: epoch  4, batch     3 | loss: 2.0364957Losses:  2.3220114707946777 0.7686582803726196
CurrentTrain: epoch  5, batch     0 | loss: 3.0906696Losses:  2.0456151962280273 0.9148712158203125
CurrentTrain: epoch  5, batch     1 | loss: 2.9604864Losses:  2.117349624633789 0.573275089263916
CurrentTrain: epoch  5, batch     2 | loss: 2.6906247Losses:  2.411508083343506 0.14174365997314453
CurrentTrain: epoch  5, batch     3 | loss: 2.5532517Losses:  2.085322141647339 0.7843456268310547
CurrentTrain: epoch  6, batch     0 | loss: 2.8696678Losses:  2.145030975341797 0.8554692268371582
CurrentTrain: epoch  6, batch     1 | loss: 3.0005002Losses:  1.9882311820983887 0.6182430386543274
CurrentTrain: epoch  6, batch     2 | loss: 2.6064742Losses:  1.8809928894042969 0.07521997392177582
CurrentTrain: epoch  6, batch     3 | loss: 1.9562129Losses:  1.923823356628418 0.8341655731201172
CurrentTrain: epoch  7, batch     0 | loss: 2.7579889Losses:  1.8792133331298828 0.6934057474136353
CurrentTrain: epoch  7, batch     1 | loss: 2.5726190Losses:  2.1370511054992676 0.4934631586074829
CurrentTrain: epoch  7, batch     2 | loss: 2.6305141Losses:  2.060103178024292 0.043469518423080444
CurrentTrain: epoch  7, batch     3 | loss: 2.1035726Losses:  1.9209115505218506 0.6082744598388672
CurrentTrain: epoch  8, batch     0 | loss: 2.5291860Losses:  2.0843281745910645 0.5468948483467102
CurrentTrain: epoch  8, batch     1 | loss: 2.6312230Losses:  1.8824329376220703 0.655608057975769
CurrentTrain: epoch  8, batch     2 | loss: 2.5380411Losses:  1.831000804901123 0.07344592362642288
CurrentTrain: epoch  8, batch     3 | loss: 1.9044467Losses:  2.084414482116699 0.45278093218803406
CurrentTrain: epoch  9, batch     0 | loss: 2.5371954Losses:  1.8611043691635132 0.5710500478744507
CurrentTrain: epoch  9, batch     1 | loss: 2.4321544Losses:  1.8290436267852783 0.5930210947990417
CurrentTrain: epoch  9, batch     2 | loss: 2.4220648Losses:  1.721912145614624 0.08924885094165802
CurrentTrain: epoch  9, batch     3 | loss: 1.8111610
Losses:  6.10174560546875 0.6181150078773499
MemoryTrain:  epoch  0, batch     0 | loss: 6.7198606Losses:  9.253661155700684 0.8365538716316223
MemoryTrain:  epoch  0, batch     1 | loss: 10.0902147Losses:  9.447590827941895 0.7644740343093872
MemoryTrain:  epoch  0, batch     2 | loss: 10.2120647Losses:  10.761195182800293 0.8522421717643738
MemoryTrain:  epoch  0, batch     3 | loss: 11.6134377Losses:  10.540044784545898 0.4445607662200928
MemoryTrain:  epoch  0, batch     4 | loss: 10.9846058Losses:  0.8799819946289062 0.7506746649742126
MemoryTrain:  epoch  1, batch     0 | loss: 1.6306567Losses:  0.8389843702316284 0.6769666075706482
MemoryTrain:  epoch  1, batch     1 | loss: 1.5159509Losses:  0.770723819732666 0.6697514653205872
MemoryTrain:  epoch  1, batch     2 | loss: 1.4404752Losses:  0.5912989974021912 0.7241679430007935
MemoryTrain:  epoch  1, batch     3 | loss: 1.3154669Losses:  1.3363490104675293 0.5698530673980713
MemoryTrain:  epoch  1, batch     4 | loss: 1.9062021Losses:  0.6708518266677856 0.6829856038093567
MemoryTrain:  epoch  2, batch     0 | loss: 1.3538375Losses:  0.6483780145645142 0.6511721014976501
MemoryTrain:  epoch  2, batch     1 | loss: 1.2995501Losses:  0.6867502927780151 0.8541918992996216
MemoryTrain:  epoch  2, batch     2 | loss: 1.5409422Losses:  0.48293358087539673 0.7653251886367798
MemoryTrain:  epoch  2, batch     3 | loss: 1.2482588Losses:  0.44965991377830505 0.3328660726547241
MemoryTrain:  epoch  2, batch     4 | loss: 0.7825260Losses:  0.5422177910804749 0.5804988145828247
MemoryTrain:  epoch  3, batch     0 | loss: 1.1227167Losses:  0.4812561273574829 0.6498463153839111
MemoryTrain:  epoch  3, batch     1 | loss: 1.1311024Losses:  0.5920965075492859 0.805299699306488
MemoryTrain:  epoch  3, batch     2 | loss: 1.3973962Losses:  0.6615415215492249 0.701749861240387
MemoryTrain:  epoch  3, batch     3 | loss: 1.3632914Losses:  0.5248551368713379 0.41795241832733154
MemoryTrain:  epoch  3, batch     4 | loss: 0.9428076Losses:  0.45557475090026855 0.6426849365234375
MemoryTrain:  epoch  4, batch     0 | loss: 1.0982597Losses:  0.49093759059906006 0.717241108417511
MemoryTrain:  epoch  4, batch     1 | loss: 1.2081788Losses:  0.6085482239723206 0.8586769104003906
MemoryTrain:  epoch  4, batch     2 | loss: 1.4672251Losses:  0.5076326131820679 0.7635698318481445
MemoryTrain:  epoch  4, batch     3 | loss: 1.2712024Losses:  0.4156302809715271 0.19713088870048523
MemoryTrain:  epoch  4, batch     4 | loss: 0.6127611Losses:  0.5233105421066284 0.8658577799797058
MemoryTrain:  epoch  5, batch     0 | loss: 1.3891683Losses:  0.44513076543807983 0.5297040343284607
MemoryTrain:  epoch  5, batch     1 | loss: 0.9748348Losses:  0.5055872797966003 0.7068756818771362
MemoryTrain:  epoch  5, batch     2 | loss: 1.2124629Losses:  0.48092037439346313 0.6149924993515015
MemoryTrain:  epoch  5, batch     3 | loss: 1.0959129Losses:  0.48971056938171387 0.23838505148887634
MemoryTrain:  epoch  5, batch     4 | loss: 0.7280957Losses:  0.4662824273109436 0.7206689715385437
MemoryTrain:  epoch  6, batch     0 | loss: 1.1869514Losses:  0.4883124530315399 0.7348302602767944
MemoryTrain:  epoch  6, batch     1 | loss: 1.2231427Losses:  0.3793362081050873 0.5355545282363892
MemoryTrain:  epoch  6, batch     2 | loss: 0.9148908Losses:  0.4555208683013916 0.6678873300552368
MemoryTrain:  epoch  6, batch     3 | loss: 1.1234082Losses:  0.7725364565849304 0.2922702431678772
MemoryTrain:  epoch  6, batch     4 | loss: 1.0648067Losses:  0.46913379430770874 0.6362206935882568
MemoryTrain:  epoch  7, batch     0 | loss: 1.1053545Losses:  0.39526647329330444 0.7873736619949341
MemoryTrain:  epoch  7, batch     1 | loss: 1.1826401Losses:  0.4079260230064392 0.6477789878845215
MemoryTrain:  epoch  7, batch     2 | loss: 1.0557051Losses:  0.45479321479797363 0.5996164083480835
MemoryTrain:  epoch  7, batch     3 | loss: 1.0544096Losses:  0.4485664367675781 0.3240806758403778
MemoryTrain:  epoch  7, batch     4 | loss: 0.7726471Losses:  0.4292145371437073 0.696567714214325
MemoryTrain:  epoch  8, batch     0 | loss: 1.1257823Losses:  0.4207761883735657 0.6971768140792847
MemoryTrain:  epoch  8, batch     1 | loss: 1.1179531Losses:  0.4106137156486511 0.6162807941436768
MemoryTrain:  epoch  8, batch     2 | loss: 1.0268946Losses:  0.38659536838531494 0.6196230053901672
MemoryTrain:  epoch  8, batch     3 | loss: 1.0062184Losses:  0.39058637619018555 0.2947288155555725
MemoryTrain:  epoch  8, batch     4 | loss: 0.6853152Losses:  0.43680426478385925 0.7333080172538757
MemoryTrain:  epoch  9, batch     0 | loss: 1.1701123Losses:  0.3876006007194519 0.6152077913284302
MemoryTrain:  epoch  9, batch     1 | loss: 1.0028083Losses:  0.3611535429954529 0.6115493774414062
MemoryTrain:  epoch  9, batch     2 | loss: 0.9727029Losses:  0.46279385685920715 0.6937580108642578
MemoryTrain:  epoch  9, batch     3 | loss: 1.1565518Losses:  0.37034469842910767 0.2736355662345886
MemoryTrain:  epoch  9, batch     4 | loss: 0.6439803
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 82.48%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 82.21%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 82.31%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 82.42%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.64%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.38%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 83.06%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 82.74%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 81.77%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 81.51%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 81.00%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 81.02%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 80.59%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 80.17%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.90%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 79.82%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.74%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 79.07%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 77.83%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 76.63%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 75.57%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 74.44%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 73.44%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 72.74%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.77%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 72.98%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 73.92%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 73.77%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 73.30%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 73.05%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 72.76%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 72.26%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 71.99%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 71.50%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 71.40%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 71.08%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 70.47%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 69.82%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 69.03%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 68.26%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.51%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 66.78%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 66.13%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 66.11%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 67.93%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 67.30%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 66.74%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 66.25%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 65.65%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 65.09%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 64.70%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:  123 | acc: 43.75%,  total acc: 64.36%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 64.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 64.43%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 64.57%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 64.75%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 66.40%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 66.24%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 65.81%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 65.46%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 65.20%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 64.89%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 64.68%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 64.42%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 64.29%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 64.19%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 64.18%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 64.17%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 64.08%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 64.00%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 63.98%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 63.93%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 63.94%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 63.41%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 63.08%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 62.83%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 62.57%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 62.36%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.57%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 63.97%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 64.35%   [EVAL] batch:  189 | acc: 6.25%,  total acc: 64.05%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 63.78%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 63.48%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 63.21%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 63.04%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 63.01%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 63.01%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 63.06%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 63.18%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 63.27%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 63.39%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 63.44%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 62.80%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 62.71%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.65%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.99%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 64.63%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.80%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 64.76%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 64.59%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 64.45%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 64.28%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 64.11%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.10%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 64.37%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 64.52%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 64.38%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 64.22%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 64.11%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 63.91%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 63.88%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 64.13%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 64.40%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 64.48%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 64.42%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 64.34%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 64.34%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 64.40%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 64.35%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 64.34%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 64.31%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 64.28%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 64.25%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 64.94%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 64.79%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 64.72%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 64.69%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 64.68%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 64.48%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 64.41%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 64.25%   [EVAL] batch:  286 | acc: 6.25%,  total acc: 64.05%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 64.04%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 64.08%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 64.22%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 64.31%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 64.24%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 64.10%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 64.05%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 63.96%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 63.85%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 65.27%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 65.15%   [EVAL] batch:  318 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 65.43%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 65.53%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 65.33%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 65.01%   [EVAL] batch:  329 | acc: 6.25%,  total acc: 64.83%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 64.63%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 65.11%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 64.92%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 64.72%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 64.57%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 64.23%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 64.14%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 64.37%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 64.55%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 64.49%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 64.38%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 64.25%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  356 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 64.43%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 64.49%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 64.41%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 64.37%   [EVAL] batch:  365 | acc: 25.00%,  total acc: 64.26%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 64.23%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 64.43%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 64.51%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 64.56%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 64.86%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  396 | acc: 43.75%,  total acc: 65.14%   [EVAL] batch:  397 | acc: 56.25%,  total acc: 65.12%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  399 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  406 | acc: 87.50%,  total acc: 65.63%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 66.79%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 66.87%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 67.24%   
cur_acc:  ['0.9444', '0.5913', '0.6964', '0.6240', '0.7192', '0.6567', '0.8264']
his_acc:  ['0.9444', '0.7705', '0.7317', '0.6955', '0.6735', '0.6592', '0.6724']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 35  0  0  1  1 19  0  0  0  0  0  0  0 25
  0 34  0 23 31 36 26  0 22  0 20 12 17  0  0  0  5 29  0  0  0  2  0 37
 27  9 32 28  1  0  0 18  0 30  0 16  8 10  0  0 13 15 14  0  0  7  2  3
  0  0  6  0  0  0  4 11]
Losses:  6.9178876876831055 1.4477859735488892
CurrentTrain: epoch  0, batch     0 | loss: 8.3656740Losses:  9.262331008911133 1.4152002334594727
CurrentTrain: epoch  0, batch     1 | loss: 10.6775312Losses:  7.229878902435303 1.4344632625579834
CurrentTrain: epoch  0, batch     2 | loss: 8.6643419Losses:  3.728062152862549 0.23415742814540863
CurrentTrain: epoch  0, batch     3 | loss: 3.9622195Losses:  4.155860900878906 1.2355026006698608
CurrentTrain: epoch  1, batch     0 | loss: 5.3913636Losses:  4.04898738861084 1.3259259462356567
CurrentTrain: epoch  1, batch     1 | loss: 5.3749132Losses:  3.889364242553711 1.1755921840667725
CurrentTrain: epoch  1, batch     2 | loss: 5.0649567Losses:  2.983140230178833 0.2442527413368225
CurrentTrain: epoch  1, batch     3 | loss: 3.2273929Losses:  3.529090404510498 1.152956485748291
CurrentTrain: epoch  2, batch     0 | loss: 4.6820469Losses:  3.7559361457824707 1.005264163017273
CurrentTrain: epoch  2, batch     1 | loss: 4.7612004Losses:  3.8167519569396973 1.3510339260101318
CurrentTrain: epoch  2, batch     2 | loss: 5.1677856Losses:  3.0254030227661133 0.33781176805496216
CurrentTrain: epoch  2, batch     3 | loss: 3.3632147Losses:  3.4950175285339355 1.1211190223693848
CurrentTrain: epoch  3, batch     0 | loss: 4.6161366Losses:  3.512202262878418 1.020578384399414
CurrentTrain: epoch  3, batch     1 | loss: 4.5327806Losses:  3.199721336364746 1.133901596069336
CurrentTrain: epoch  3, batch     2 | loss: 4.3336229Losses:  2.1618492603302 0.40497568249702454
CurrentTrain: epoch  3, batch     3 | loss: 2.5668249Losses:  3.2561874389648438 1.1097526550292969
CurrentTrain: epoch  4, batch     0 | loss: 4.3659401Losses:  3.0835189819335938 1.0861616134643555
CurrentTrain: epoch  4, batch     1 | loss: 4.1696806Losses:  2.7098701000213623 0.7967172861099243
CurrentTrain: epoch  4, batch     2 | loss: 3.5065875Losses:  2.7879533767700195 0.2735123038291931
CurrentTrain: epoch  4, batch     3 | loss: 3.0614657Losses:  2.6956419944763184 0.5941894054412842
CurrentTrain: epoch  5, batch     0 | loss: 3.2898314Losses:  3.111560583114624 0.9167929887771606
CurrentTrain: epoch  5, batch     1 | loss: 4.0283537Losses:  2.8662147521972656 0.8870781660079956
CurrentTrain: epoch  5, batch     2 | loss: 3.7532930Losses:  2.8662843704223633 0.24737548828125
CurrentTrain: epoch  5, batch     3 | loss: 3.1136599Losses:  2.4953255653381348 0.7876673936843872
CurrentTrain: epoch  6, batch     0 | loss: 3.2829928Losses:  2.8256545066833496 0.8906237483024597
CurrentTrain: epoch  6, batch     1 | loss: 3.7162783Losses:  2.4402003288269043 0.7587538361549377
CurrentTrain: epoch  6, batch     2 | loss: 3.1989541Losses:  4.907026290893555 0.6530798673629761
CurrentTrain: epoch  6, batch     3 | loss: 5.5601063Losses:  2.5369040966033936 0.7536089420318604
CurrentTrain: epoch  7, batch     0 | loss: 3.2905130Losses:  2.4911131858825684 0.8265831470489502
CurrentTrain: epoch  7, batch     1 | loss: 3.3176963Losses:  2.466129779815674 0.8442815542221069
CurrentTrain: epoch  7, batch     2 | loss: 3.3104115Losses:  2.5961272716522217 0.07171055674552917
CurrentTrain: epoch  7, batch     3 | loss: 2.6678379Losses:  2.514585494995117 0.7968795299530029
CurrentTrain: epoch  8, batch     0 | loss: 3.3114650Losses:  2.2314181327819824 0.764065146446228
CurrentTrain: epoch  8, batch     1 | loss: 2.9954834Losses:  2.5742199420928955 0.5194726586341858
CurrentTrain: epoch  8, batch     2 | loss: 3.0936925Losses:  1.715922236442566 0.07212069630622864
CurrentTrain: epoch  8, batch     3 | loss: 1.7880429Losses:  2.39508056640625 0.6537829637527466
CurrentTrain: epoch  9, batch     0 | loss: 3.0488634Losses:  2.204361915588379 0.606057345867157
CurrentTrain: epoch  9, batch     1 | loss: 2.8104193Losses:  2.120403289794922 0.6973621249198914
CurrentTrain: epoch  9, batch     2 | loss: 2.8177655Losses:  2.437924861907959 0.10196927934885025
CurrentTrain: epoch  9, batch     3 | loss: 2.5398941
Losses:  6.033452033996582 0.654129683971405
MemoryTrain:  epoch  0, batch     0 | loss: 6.6875815Losses:  8.988561630249023 0.725155234336853
MemoryTrain:  epoch  0, batch     1 | loss: 9.7137165Losses:  10.038744926452637 0.7028228044509888
MemoryTrain:  epoch  0, batch     2 | loss: 10.7415676Losses:  10.205299377441406 0.7664525508880615
MemoryTrain:  epoch  0, batch     3 | loss: 10.9717522Losses:  10.503923416137695 0.7601152062416077
MemoryTrain:  epoch  0, batch     4 | loss: 11.2640390Losses:  0.923216700553894 0.6584932804107666
MemoryTrain:  epoch  1, batch     0 | loss: 1.5817100Losses:  1.0107516050338745 0.7357073426246643
MemoryTrain:  epoch  1, batch     1 | loss: 1.7464590Losses:  1.0026830434799194 0.6729406118392944
MemoryTrain:  epoch  1, batch     2 | loss: 1.6756237Losses:  1.085700273513794 0.6145035028457642
MemoryTrain:  epoch  1, batch     3 | loss: 1.7002038Losses:  0.9195458292961121 0.6997964382171631
MemoryTrain:  epoch  1, batch     4 | loss: 1.6193423Losses:  1.1770470142364502 0.7477620840072632
MemoryTrain:  epoch  2, batch     0 | loss: 1.9248091Losses:  0.7149386405944824 0.6012053489685059
MemoryTrain:  epoch  2, batch     1 | loss: 1.3161440Losses:  0.7040119171142578 0.6837601661682129
MemoryTrain:  epoch  2, batch     2 | loss: 1.3877721Losses:  0.8354728817939758 0.7481334209442139
MemoryTrain:  epoch  2, batch     3 | loss: 1.5836062Losses:  0.5777779221534729 0.6232190132141113
MemoryTrain:  epoch  2, batch     4 | loss: 1.2009969Losses:  0.45138072967529297 0.5349470376968384
MemoryTrain:  epoch  3, batch     0 | loss: 0.9863278Losses:  0.6145975589752197 0.7416328191757202
MemoryTrain:  epoch  3, batch     1 | loss: 1.3562304Losses:  0.8600741028785706 0.7328078150749207
MemoryTrain:  epoch  3, batch     2 | loss: 1.5928819Losses:  0.6724728345870972 0.6984752416610718
MemoryTrain:  epoch  3, batch     3 | loss: 1.3709481Losses:  0.6336096525192261 0.597350537776947
MemoryTrain:  epoch  3, batch     4 | loss: 1.2309601Losses:  0.6569638252258301 0.7170157432556152
MemoryTrain:  epoch  4, batch     0 | loss: 1.3739796Losses:  0.6450342535972595 0.793537437915802
MemoryTrain:  epoch  4, batch     1 | loss: 1.4385717Losses:  0.5212229490280151 0.6601359844207764
MemoryTrain:  epoch  4, batch     2 | loss: 1.1813589Losses:  0.49507254362106323 0.6427300572395325
MemoryTrain:  epoch  4, batch     3 | loss: 1.1378026Losses:  0.5259408354759216 0.5886176824569702
MemoryTrain:  epoch  4, batch     4 | loss: 1.1145585Losses:  0.5413592457771301 0.6420746445655823
MemoryTrain:  epoch  5, batch     0 | loss: 1.1834339Losses:  0.6592479944229126 0.7590922117233276
MemoryTrain:  epoch  5, batch     1 | loss: 1.4183402Losses:  0.5952184200286865 0.7325069308280945
MemoryTrain:  epoch  5, batch     2 | loss: 1.3277254Losses:  0.5215153694152832 0.6076453328132629
MemoryTrain:  epoch  5, batch     3 | loss: 1.1291606Losses:  0.47773197293281555 0.6473970413208008
MemoryTrain:  epoch  5, batch     4 | loss: 1.1251290Losses:  0.4967101216316223 0.5415588617324829
MemoryTrain:  epoch  6, batch     0 | loss: 1.0382690Losses:  0.5348197221755981 0.7029925584793091
MemoryTrain:  epoch  6, batch     1 | loss: 1.2378123Losses:  0.47161704301834106 0.6805347204208374
MemoryTrain:  epoch  6, batch     2 | loss: 1.1521518Losses:  0.45779651403427124 0.7739770412445068
MemoryTrain:  epoch  6, batch     3 | loss: 1.2317736Losses:  0.5013534426689148 0.6426879167556763
MemoryTrain:  epoch  6, batch     4 | loss: 1.1440413Losses:  0.5689985752105713 0.7695316076278687
MemoryTrain:  epoch  7, batch     0 | loss: 1.3385302Losses:  0.5242051482200623 0.6714687943458557
MemoryTrain:  epoch  7, batch     1 | loss: 1.1956739Losses:  0.482507586479187 0.6147641539573669
MemoryTrain:  epoch  7, batch     2 | loss: 1.0972717Losses:  0.411334365606308 0.6455209255218506
MemoryTrain:  epoch  7, batch     3 | loss: 1.0568553Losses:  0.4098789691925049 0.5079255104064941
MemoryTrain:  epoch  7, batch     4 | loss: 0.9178045Losses:  0.44461196660995483 0.5942215919494629
MemoryTrain:  epoch  8, batch     0 | loss: 1.0388336Losses:  0.47158217430114746 0.5827455520629883
MemoryTrain:  epoch  8, batch     1 | loss: 1.0543277Losses:  0.502937912940979 0.7592430710792542
MemoryTrain:  epoch  8, batch     2 | loss: 1.2621810Losses:  0.4595893621444702 0.6640710830688477
MemoryTrain:  epoch  8, batch     3 | loss: 1.1236604Losses:  0.4792882204055786 0.5942016243934631
MemoryTrain:  epoch  8, batch     4 | loss: 1.0734899Losses:  0.4405425786972046 0.672520101070404
MemoryTrain:  epoch  9, batch     0 | loss: 1.1130626Losses:  0.5263757705688477 0.6805117130279541
MemoryTrain:  epoch  9, batch     1 | loss: 1.2068875Losses:  0.510493278503418 0.5939537882804871
MemoryTrain:  epoch  9, batch     2 | loss: 1.1044471Losses:  0.38241100311279297 0.5632534623146057
MemoryTrain:  epoch  9, batch     3 | loss: 0.9456645Losses:  0.4371591806411743 0.67823326587677
MemoryTrain:  epoch  9, batch     4 | loss: 1.1153924
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 43.75%   [EVAL] batch:   16 | acc: 18.75%,  total acc: 42.28%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 41.32%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 41.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 44.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.73%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 48.86%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 59.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.56%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 66.52%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 65.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 68.63%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 68.28%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 67.84%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 67.00%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 67.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 66.77%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 82.05%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 81.64%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 81.38%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 81.60%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 81.36%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 80.81%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 79.85%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 79.45%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 78.89%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 77.98%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 76.76%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 75.58%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 74.53%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 73.41%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 72.43%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 71.74%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 71.92%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 72.55%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 72.58%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 72.45%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 72.20%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 71.11%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 70.71%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 70.24%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 70.07%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 69.62%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 69.04%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 68.39%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 66.94%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 66.21%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 64.85%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 64.36%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 64.54%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.71%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 65.09%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 66.82%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 66.26%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 65.71%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 65.23%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 64.75%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 64.17%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 63.94%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 64.13%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 64.48%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 64.60%   [EVAL] batch:  119 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 64.14%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 63.91%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 63.89%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 64.01%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 63.91%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 64.12%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 64.57%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 65.22%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 65.24%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 64.99%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 64.89%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 64.71%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 64.40%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 64.10%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 63.81%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 63.47%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 63.23%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 62.94%   [EVAL] batch:  156 | acc: 31.25%,  total acc: 62.74%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 62.66%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 62.58%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 62.46%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 62.46%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 62.35%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 62.31%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 62.35%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 62.35%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 62.31%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 62.31%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 62.39%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 62.43%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 62.24%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 61.92%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 61.63%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 61.38%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 61.14%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 60.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 61.15%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 61.37%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 61.80%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 62.43%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 62.64%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 62.80%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 62.97%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 63.14%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 63.03%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 62.73%   [EVAL] batch:  189 | acc: 31.25%,  total acc: 62.57%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 62.27%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 62.04%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 61.82%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 61.69%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 61.77%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 61.68%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 61.68%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 61.62%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 61.75%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 61.82%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 61.92%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 62.04%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 62.10%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 62.08%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 61.96%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 61.81%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 61.61%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 61.43%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 61.29%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 61.24%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.60%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 61.75%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 62.58%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.09%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 63.25%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 63.27%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 63.29%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 63.42%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 63.50%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:  232 | acc: 12.50%,  total acc: 63.20%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 63.03%   [EVAL] batch:  234 | acc: 6.25%,  total acc: 62.79%   [EVAL] batch:  235 | acc: 31.25%,  total acc: 62.66%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 62.47%   [EVAL] batch:  237 | acc: 50.00%,  total acc: 62.42%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 62.53%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 62.66%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 62.73%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 62.91%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 62.78%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 62.58%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 62.35%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 62.17%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 61.95%   [EVAL] batch:  249 | acc: 12.50%,  total acc: 61.75%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 61.80%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 61.86%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 61.93%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 61.98%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 62.06%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 62.18%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 62.18%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 62.21%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 62.19%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 62.12%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 62.12%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 62.21%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 62.14%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 62.17%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 62.12%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 62.08%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 62.06%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 62.13%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 62.66%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 62.93%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 62.91%   [EVAL] batch:  277 | acc: 50.00%,  total acc: 62.86%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 62.81%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 62.79%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 62.81%   [EVAL] batch:  281 | acc: 37.50%,  total acc: 62.72%   [EVAL] batch:  282 | acc: 31.25%,  total acc: 62.61%   [EVAL] batch:  283 | acc: 6.25%,  total acc: 62.41%   [EVAL] batch:  284 | acc: 6.25%,  total acc: 62.21%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 62.04%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 61.81%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 61.83%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 61.92%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 61.98%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 62.07%   [EVAL] batch:  292 | acc: 62.50%,  total acc: 62.07%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 61.97%   [EVAL] batch:  295 | acc: 43.75%,  total acc: 61.91%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 61.78%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 61.74%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 61.64%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 61.54%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 61.67%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 61.80%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 62.60%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 62.82%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 63.00%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 62.94%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 62.94%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 62.89%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 62.93%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 62.91%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 63.01%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 63.35%   [EVAL] batch:  324 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:  325 | acc: 0.00%,  total acc: 63.27%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 63.07%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 62.94%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 62.75%   [EVAL] batch:  329 | acc: 6.25%,  total acc: 62.58%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 62.91%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 62.72%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 62.54%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 62.39%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 62.23%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 62.06%   [EVAL] batch:  343 | acc: 25.00%,  total acc: 61.95%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 62.14%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 62.32%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 62.41%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 62.43%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 62.34%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 62.25%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 62.15%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 62.11%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:  356 | acc: 31.25%,  total acc: 62.01%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 61.92%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 61.82%   [EVAL] batch:  359 | acc: 31.25%,  total acc: 61.74%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 61.63%   [EVAL] batch:  361 | acc: 18.75%,  total acc: 61.52%   [EVAL] batch:  362 | acc: 37.50%,  total acc: 61.45%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 61.37%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 61.30%   [EVAL] batch:  365 | acc: 31.25%,  total acc: 61.22%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 61.22%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 61.23%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 61.21%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 61.30%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 61.39%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 61.48%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:  374 | acc: 100.00%,  total acc: 61.73%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 61.82%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 61.99%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 62.14%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 62.25%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 62.32%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 62.34%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 62.42%   [EVAL] batch:  385 | acc: 62.50%,  total acc: 62.42%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 62.45%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 62.44%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 62.45%   [EVAL] batch:  389 | acc: 50.00%,  total acc: 62.42%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 62.37%   [EVAL] batch:  391 | acc: 43.75%,  total acc: 62.32%   [EVAL] batch:  392 | acc: 37.50%,  total acc: 62.26%   [EVAL] batch:  393 | acc: 43.75%,  total acc: 62.21%   [EVAL] batch:  394 | acc: 50.00%,  total acc: 62.18%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 62.18%   [EVAL] batch:  396 | acc: 25.00%,  total acc: 62.09%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 62.06%   [EVAL] batch:  398 | acc: 37.50%,  total acc: 62.00%   [EVAL] batch:  399 | acc: 31.25%,  total acc: 61.92%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 62.21%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 62.39%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 62.48%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 62.53%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 62.61%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 62.70%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 62.71%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 62.73%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 62.76%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  425 | acc: 31.25%,  total acc: 63.72%   [EVAL] batch:  426 | acc: 62.50%,  total acc: 63.71%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 63.76%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 63.76%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 63.78%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 63.83%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 63.90%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 63.95%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 64.18%   [EVAL] batch:  438 | acc: 31.25%,  total acc: 64.11%   [EVAL] batch:  439 | acc: 12.50%,  total acc: 63.99%   [EVAL] batch:  440 | acc: 12.50%,  total acc: 63.87%   [EVAL] batch:  441 | acc: 18.75%,  total acc: 63.77%   [EVAL] batch:  442 | acc: 12.50%,  total acc: 63.66%   [EVAL] batch:  443 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 63.81%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.86%   [EVAL] batch:  450 | acc: 31.25%,  total acc: 63.79%   [EVAL] batch:  451 | acc: 6.25%,  total acc: 63.66%   [EVAL] batch:  452 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:  453 | acc: 18.75%,  total acc: 63.46%   [EVAL] batch:  454 | acc: 18.75%,  total acc: 63.37%   [EVAL] batch:  455 | acc: 18.75%,  total acc: 63.27%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  462 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  463 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:  464 | acc: 93.75%,  total acc: 63.88%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  466 | acc: 87.50%,  total acc: 64.01%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 64.34%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 64.57%   [EVAL] batch:  476 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 64.53%   [EVAL] batch:  478 | acc: 43.75%,  total acc: 64.48%   [EVAL] batch:  479 | acc: 12.50%,  total acc: 64.38%   [EVAL] batch:  480 | acc: 37.50%,  total acc: 64.32%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 64.37%   [EVAL] batch:  482 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  483 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  485 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  487 | acc: 87.50%,  total acc: 64.74%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 64.68%   [EVAL] batch:  490 | acc: 56.25%,  total acc: 64.66%   [EVAL] batch:  491 | acc: 56.25%,  total acc: 64.65%   [EVAL] batch:  492 | acc: 50.00%,  total acc: 64.62%   [EVAL] batch:  493 | acc: 43.75%,  total acc: 64.57%   [EVAL] batch:  494 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 64.60%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 64.57%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 64.61%   
cur_acc:  ['0.9444', '0.5913', '0.6964', '0.6240', '0.7192', '0.6567', '0.8264', '0.6677']
his_acc:  ['0.9444', '0.7705', '0.7317', '0.6955', '0.6735', '0.6592', '0.6724', '0.6461']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.79939079284668 2.1490416526794434
CurrentTrain: epoch  0, batch     0 | loss: 13.9484329Losses:  13.135014533996582 2.036714553833008
CurrentTrain: epoch  0, batch     1 | loss: 15.1717291Losses:  13.538808822631836 1.8613779544830322
CurrentTrain: epoch  0, batch     2 | loss: 15.4001865Losses:  13.692621231079102 2.0318353176116943
CurrentTrain: epoch  0, batch     3 | loss: 15.7244568Losses:  13.431598663330078 1.796144723892212
CurrentTrain: epoch  0, batch     4 | loss: 15.2277431Losses:  13.528339385986328 1.47483491897583
CurrentTrain: epoch  0, batch     5 | loss: 15.0031738Losses:  13.260042190551758 1.7973767518997192
CurrentTrain: epoch  0, batch     6 | loss: 15.0574188Losses:  12.95081901550293 1.7576384544372559
CurrentTrain: epoch  0, batch     7 | loss: 14.7084579Losses:  13.241971969604492 1.337550163269043
CurrentTrain: epoch  0, batch     8 | loss: 14.5795221Losses:  13.004487991333008 1.7053642272949219
CurrentTrain: epoch  0, batch     9 | loss: 14.7098522Losses:  12.148030281066895 1.33872652053833
CurrentTrain: epoch  0, batch    10 | loss: 13.4867573Losses:  11.776707649230957 1.4375925064086914
CurrentTrain: epoch  0, batch    11 | loss: 13.2143002Losses:  11.995750427246094 1.6066055297851562
CurrentTrain: epoch  0, batch    12 | loss: 13.6023560Losses:  11.972318649291992 1.4253203868865967
CurrentTrain: epoch  0, batch    13 | loss: 13.3976393Losses:  12.232800483703613 1.2151131629943848
CurrentTrain: epoch  0, batch    14 | loss: 13.4479141Losses:  11.726722717285156 1.50295090675354
CurrentTrain: epoch  0, batch    15 | loss: 13.2296734Losses:  11.551082611083984 1.810289978981018
CurrentTrain: epoch  0, batch    16 | loss: 13.3613729Losses:  11.651873588562012 1.815571904182434
CurrentTrain: epoch  0, batch    17 | loss: 13.4674454Losses:  11.475503921508789 1.7183177471160889
CurrentTrain: epoch  0, batch    18 | loss: 13.1938219Losses:  11.403352737426758 1.7043184041976929
CurrentTrain: epoch  0, batch    19 | loss: 13.1076708Losses:  11.582612037658691 1.4966694116592407
CurrentTrain: epoch  0, batch    20 | loss: 13.0792818Losses:  11.265616416931152 1.4864557981491089
CurrentTrain: epoch  0, batch    21 | loss: 12.7520723Losses:  11.030399322509766 1.6129287481307983
CurrentTrain: epoch  0, batch    22 | loss: 12.6433277Losses:  10.696142196655273 1.3585388660430908
CurrentTrain: epoch  0, batch    23 | loss: 12.0546808Losses:  10.788751602172852 1.6283870935440063
CurrentTrain: epoch  0, batch    24 | loss: 12.4171391Losses:  10.392285346984863 1.343312382698059
CurrentTrain: epoch  0, batch    25 | loss: 11.7355976Losses:  10.660462379455566 1.5457203388214111
CurrentTrain: epoch  0, batch    26 | loss: 12.2061825Losses:  10.319133758544922 1.279910922050476
CurrentTrain: epoch  0, batch    27 | loss: 11.5990448Losses:  10.474348068237305 1.4378173351287842
CurrentTrain: epoch  0, batch    28 | loss: 11.9121656Losses:  9.929996490478516 1.3417091369628906
CurrentTrain: epoch  0, batch    29 | loss: 11.2717056Losses:  9.907953262329102 1.1438217163085938
CurrentTrain: epoch  0, batch    30 | loss: 11.0517750Losses:  10.00217056274414 1.1300430297851562
CurrentTrain: epoch  0, batch    31 | loss: 11.1322136Losses:  9.421916961669922 0.9603966474533081
CurrentTrain: epoch  0, batch    32 | loss: 10.3823137Losses:  10.070603370666504 1.0290229320526123
CurrentTrain: epoch  0, batch    33 | loss: 11.0996265Losses:  9.529335021972656 1.2593902349472046
CurrentTrain: epoch  0, batch    34 | loss: 10.7887249Losses:  9.12363338470459 1.0373525619506836
CurrentTrain: epoch  0, batch    35 | loss: 10.1609859Losses:  8.810815811157227 1.0703785419464111
CurrentTrain: epoch  0, batch    36 | loss: 9.8811941Losses:  9.29232406616211 1.1952884197235107
CurrentTrain: epoch  0, batch    37 | loss: 10.4876127Losses:  8.603621482849121 1.0993132591247559
CurrentTrain: epoch  0, batch    38 | loss: 9.7029343Losses:  9.166757583618164 1.1507282257080078
CurrentTrain: epoch  0, batch    39 | loss: 10.3174858Losses:  8.856657028198242 0.6698777675628662
CurrentTrain: epoch  0, batch    40 | loss: 9.5265350Losses:  8.694526672363281 1.1007076501846313
CurrentTrain: epoch  0, batch    41 | loss: 9.7952347Losses:  8.480607986450195 1.18697190284729
CurrentTrain: epoch  0, batch    42 | loss: 9.6675797Losses:  8.375120162963867 1.3599843978881836
CurrentTrain: epoch  0, batch    43 | loss: 9.7351046Losses:  8.364694595336914 1.067576289176941
CurrentTrain: epoch  0, batch    44 | loss: 9.4322710Losses:  8.119739532470703 1.1750600337982178
CurrentTrain: epoch  0, batch    45 | loss: 9.2947998Losses:  8.067193984985352 0.8910646438598633
CurrentTrain: epoch  0, batch    46 | loss: 8.9582586Losses:  7.766533851623535 0.956616997718811
CurrentTrain: epoch  0, batch    47 | loss: 8.7231512Losses:  7.490372657775879 1.0473028421401978
CurrentTrain: epoch  0, batch    48 | loss: 8.5376759Losses:  7.519103050231934 1.116597294807434
CurrentTrain: epoch  0, batch    49 | loss: 8.6357002Losses:  7.309909820556641 1.094667911529541
CurrentTrain: epoch  0, batch    50 | loss: 8.4045773Losses:  7.103516578674316 1.025627613067627
CurrentTrain: epoch  0, batch    51 | loss: 8.1291447Losses:  6.813689708709717 1.0495898723602295
CurrentTrain: epoch  0, batch    52 | loss: 7.8632793Losses:  6.900795936584473 0.7865605354309082
CurrentTrain: epoch  0, batch    53 | loss: 7.6873565Losses:  7.000592231750488 1.0990591049194336
CurrentTrain: epoch  0, batch    54 | loss: 8.0996513Losses:  6.3716230392456055 0.8070791959762573
CurrentTrain: epoch  0, batch    55 | loss: 7.1787024Losses:  6.7136945724487305 1.1467604637145996
CurrentTrain: epoch  0, batch    56 | loss: 7.8604550Losses:  6.282364845275879 1.1527941226959229
CurrentTrain: epoch  0, batch    57 | loss: 7.4351587Losses:  6.581617832183838 1.0398523807525635
CurrentTrain: epoch  0, batch    58 | loss: 7.6214705Losses:  6.388101577758789 1.0143952369689941
CurrentTrain: epoch  0, batch    59 | loss: 7.4024968Losses:  5.646880626678467 0.8243674635887146
CurrentTrain: epoch  0, batch    60 | loss: 6.4712481Losses:  6.153753280639648 0.8808799982070923
CurrentTrain: epoch  0, batch    61 | loss: 7.0346332Losses:  5.546409606933594 0.6265982985496521
CurrentTrain: epoch  0, batch    62 | loss: 6.1730080Losses:  5.976481914520264 0.7755496501922607
CurrentTrain: epoch  1, batch     0 | loss: 6.7520313Losses:  5.5634355545043945 0.7479357123374939
CurrentTrain: epoch  1, batch     1 | loss: 6.3113713Losses:  5.495336532592773 0.8037854433059692
CurrentTrain: epoch  1, batch     2 | loss: 6.2991219Losses:  5.8077826499938965 0.8758020401000977
CurrentTrain: epoch  1, batch     3 | loss: 6.6835847Losses:  5.3018670082092285 0.7651680707931519
CurrentTrain: epoch  1, batch     4 | loss: 6.0670352Losses:  5.584531784057617 0.8494141101837158
CurrentTrain: epoch  1, batch     5 | loss: 6.4339457Losses:  5.662367343902588 0.761225700378418
CurrentTrain: epoch  1, batch     6 | loss: 6.4235930Losses:  5.6001105308532715 0.730003833770752
CurrentTrain: epoch  1, batch     7 | loss: 6.3301144Losses:  5.545459747314453 0.7936920523643494
CurrentTrain: epoch  1, batch     8 | loss: 6.3391519Losses:  5.506979942321777 0.878063440322876
CurrentTrain: epoch  1, batch     9 | loss: 6.3850431Losses:  5.328566551208496 0.6739000082015991
CurrentTrain: epoch  1, batch    10 | loss: 6.0024667Losses:  5.856596946716309 0.9039278626441956
CurrentTrain: epoch  1, batch    11 | loss: 6.7605247Losses:  5.625398635864258 0.7446045875549316
CurrentTrain: epoch  1, batch    12 | loss: 6.3700032Losses:  5.4787750244140625 0.7287602424621582
CurrentTrain: epoch  1, batch    13 | loss: 6.2075353Losses:  5.569766521453857 0.7175948023796082
CurrentTrain: epoch  1, batch    14 | loss: 6.2873611Losses:  5.22395133972168 0.6036841869354248
CurrentTrain: epoch  1, batch    15 | loss: 5.8276358Losses:  6.0789313316345215 0.7046530842781067
CurrentTrain: epoch  1, batch    16 | loss: 6.7835846Losses:  5.310314655303955 0.6587903499603271
CurrentTrain: epoch  1, batch    17 | loss: 5.9691048Losses:  5.657261371612549 0.6046798229217529
CurrentTrain: epoch  1, batch    18 | loss: 6.2619410Losses:  6.0283098220825195 0.6808996200561523
CurrentTrain: epoch  1, batch    19 | loss: 6.7092094Losses:  5.3887939453125 0.4131142497062683
CurrentTrain: epoch  1, batch    20 | loss: 5.8019080Losses:  4.934109687805176 0.5255597829818726
CurrentTrain: epoch  1, batch    21 | loss: 5.4596696Losses:  5.141468048095703 0.5532138347625732
CurrentTrain: epoch  1, batch    22 | loss: 5.6946821Losses:  5.039300918579102 0.6032942533493042
CurrentTrain: epoch  1, batch    23 | loss: 5.6425953Losses:  5.232092380523682 0.7468726634979248
CurrentTrain: epoch  1, batch    24 | loss: 5.9789648Losses:  5.300657272338867 0.6716627478599548
CurrentTrain: epoch  1, batch    25 | loss: 5.9723201Losses:  6.047152042388916 0.7511550188064575
CurrentTrain: epoch  1, batch    26 | loss: 6.7983069Losses:  5.331106185913086 0.7134579420089722
CurrentTrain: epoch  1, batch    27 | loss: 6.0445642Losses:  5.353913307189941 0.6391422748565674
CurrentTrain: epoch  1, batch    28 | loss: 5.9930553Losses:  5.576990127563477 0.9483118653297424
CurrentTrain: epoch  1, batch    29 | loss: 6.5253019Losses:  5.016027450561523 0.44126784801483154
CurrentTrain: epoch  1, batch    30 | loss: 5.4572954Losses:  5.524707794189453 0.5833455324172974
CurrentTrain: epoch  1, batch    31 | loss: 6.1080532Losses:  5.341038703918457 0.722876787185669
CurrentTrain: epoch  1, batch    32 | loss: 6.0639153Losses:  5.889116287231445 0.6634962558746338
CurrentTrain: epoch  1, batch    33 | loss: 6.5526123Losses:  5.380824089050293 0.6020424365997314
CurrentTrain: epoch  1, batch    34 | loss: 5.9828663Losses:  4.864166736602783 0.26204603910446167
CurrentTrain: epoch  1, batch    35 | loss: 5.1262126Losses:  5.292871952056885 0.6490000486373901
CurrentTrain: epoch  1, batch    36 | loss: 5.9418721Losses:  5.542333126068115 0.7776173949241638
CurrentTrain: epoch  1, batch    37 | loss: 6.3199506Losses:  5.289202690124512 0.6251125335693359
CurrentTrain: epoch  1, batch    38 | loss: 5.9143152Losses:  5.518233299255371 0.7375655174255371
CurrentTrain: epoch  1, batch    39 | loss: 6.2557988Losses:  5.109474182128906 0.6039131879806519
CurrentTrain: epoch  1, batch    40 | loss: 5.7133875Losses:  5.844795227050781 0.5761888027191162
CurrentTrain: epoch  1, batch    41 | loss: 6.4209843Losses:  5.316074371337891 0.5806003212928772
CurrentTrain: epoch  1, batch    42 | loss: 5.8966746Losses:  5.119177341461182 0.5905981659889221
CurrentTrain: epoch  1, batch    43 | loss: 5.7097754Losses:  4.958019256591797 0.417542964220047
CurrentTrain: epoch  1, batch    44 | loss: 5.3755622Losses:  5.471315383911133 0.7327122688293457
CurrentTrain: epoch  1, batch    45 | loss: 6.2040277Losses:  4.901673316955566 0.6056071519851685
CurrentTrain: epoch  1, batch    46 | loss: 5.5072803Losses:  5.435170650482178 0.5180901288986206
CurrentTrain: epoch  1, batch    47 | loss: 5.9532609Losses:  5.294189453125 0.567246675491333
CurrentTrain: epoch  1, batch    48 | loss: 5.8614359Losses:  4.812344074249268 0.34333285689353943
CurrentTrain: epoch  1, batch    49 | loss: 5.1556768Losses:  4.722867012023926 0.44679030776023865
CurrentTrain: epoch  1, batch    50 | loss: 5.1696572Losses:  5.215682029724121 0.717882513999939
CurrentTrain: epoch  1, batch    51 | loss: 5.9335647Losses:  4.86209774017334 0.49003103375434875
CurrentTrain: epoch  1, batch    52 | loss: 5.3521290Losses:  4.628582000732422 0.41468000411987305
CurrentTrain: epoch  1, batch    53 | loss: 5.0432620Losses:  4.898054122924805 0.5683345794677734
CurrentTrain: epoch  1, batch    54 | loss: 5.4663887Losses:  5.002204895019531 0.5802202224731445
CurrentTrain: epoch  1, batch    55 | loss: 5.5824251Losses:  4.672060966491699 0.43131840229034424
CurrentTrain: epoch  1, batch    56 | loss: 5.1033792Losses:  4.882296085357666 0.4557964503765106
CurrentTrain: epoch  1, batch    57 | loss: 5.3380923Losses:  4.981295585632324 0.4929645359516144
CurrentTrain: epoch  1, batch    58 | loss: 5.4742603Losses:  4.843790054321289 0.5432406663894653
CurrentTrain: epoch  1, batch    59 | loss: 5.3870306Losses:  5.2131242752075195 0.5443547964096069
CurrentTrain: epoch  1, batch    60 | loss: 5.7574792Losses:  4.671250343322754 0.4938693642616272
CurrentTrain: epoch  1, batch    61 | loss: 5.1651196Losses:  4.504258155822754 0.3335145115852356
CurrentTrain: epoch  1, batch    62 | loss: 4.8377728Losses:  4.874604225158691 0.49287235736846924
CurrentTrain: epoch  2, batch     0 | loss: 5.3674765Losses:  4.746933937072754 0.45631659030914307
CurrentTrain: epoch  2, batch     1 | loss: 5.2032504Losses:  5.646594047546387 0.49294087290763855
CurrentTrain: epoch  2, batch     2 | loss: 6.1395350Losses:  4.614910125732422 0.31449058651924133
CurrentTrain: epoch  2, batch     3 | loss: 4.9294009Losses:  4.520150184631348 0.28965139389038086
CurrentTrain: epoch  2, batch     4 | loss: 4.8098016Losses:  4.740063190460205 0.48520955443382263
CurrentTrain: epoch  2, batch     5 | loss: 5.2252727Losses:  4.541611671447754 0.3977072834968567
CurrentTrain: epoch  2, batch     6 | loss: 4.9393191Losses:  4.8384246826171875 0.4604077935218811
CurrentTrain: epoch  2, batch     7 | loss: 5.2988324Losses:  4.773031234741211 0.5246553421020508
CurrentTrain: epoch  2, batch     8 | loss: 5.2976866Losses:  4.67923641204834 0.37734875082969666
CurrentTrain: epoch  2, batch     9 | loss: 5.0565853Losses:  4.486794948577881 0.34268221259117126
CurrentTrain: epoch  2, batch    10 | loss: 4.8294773Losses:  5.001972675323486 0.506332516670227
CurrentTrain: epoch  2, batch    11 | loss: 5.5083051Losses:  4.541832447052002 0.3261599838733673
CurrentTrain: epoch  2, batch    12 | loss: 4.8679924Losses:  4.415765285491943 0.3790232539176941
CurrentTrain: epoch  2, batch    13 | loss: 4.7947884Losses:  4.379671096801758 0.27186086773872375
CurrentTrain: epoch  2, batch    14 | loss: 4.6515322Losses:  4.810824394226074 0.48333582282066345
CurrentTrain: epoch  2, batch    15 | loss: 5.2941604Losses:  4.643322944641113 0.3104237914085388
CurrentTrain: epoch  2, batch    16 | loss: 4.9537468Losses:  4.94033145904541 0.47759151458740234
CurrentTrain: epoch  2, batch    17 | loss: 5.4179230Losses:  4.737804412841797 0.34930336475372314
CurrentTrain: epoch  2, batch    18 | loss: 5.0871077Losses:  4.341361999511719 0.3175086975097656
CurrentTrain: epoch  2, batch    19 | loss: 4.6588707Losses:  4.400664806365967 0.3213691711425781
CurrentTrain: epoch  2, batch    20 | loss: 4.7220340Losses:  5.026143550872803 0.42357033491134644
CurrentTrain: epoch  2, batch    21 | loss: 5.4497137Losses:  4.707583427429199 0.30977392196655273
CurrentTrain: epoch  2, batch    22 | loss: 5.0173573Losses:  5.024051666259766 0.5109811425209045
CurrentTrain: epoch  2, batch    23 | loss: 5.5350327Losses:  4.887673377990723 0.43696022033691406
CurrentTrain: epoch  2, batch    24 | loss: 5.3246336Losses:  4.706613063812256 0.47242414951324463
CurrentTrain: epoch  2, batch    25 | loss: 5.1790371Losses:  4.392967700958252 0.3633385896682739
CurrentTrain: epoch  2, batch    26 | loss: 4.7563062Losses:  4.748258590698242 0.2974339723587036
CurrentTrain: epoch  2, batch    27 | loss: 5.0456924Losses:  4.749128818511963 0.31198179721832275
CurrentTrain: epoch  2, batch    28 | loss: 5.0611105Losses:  4.933758735656738 0.39010828733444214
CurrentTrain: epoch  2, batch    29 | loss: 5.3238668Losses:  4.32904052734375 0.2845415472984314
CurrentTrain: epoch  2, batch    30 | loss: 4.6135821Losses:  4.396352767944336 0.3097642660140991
CurrentTrain: epoch  2, batch    31 | loss: 4.7061172Losses:  4.329398155212402 0.2489667385816574
CurrentTrain: epoch  2, batch    32 | loss: 4.5783648Losses:  4.741698741912842 0.2727345824241638
CurrentTrain: epoch  2, batch    33 | loss: 5.0144334Losses:  4.445314407348633 0.3833483159542084
CurrentTrain: epoch  2, batch    34 | loss: 4.8286629Losses:  4.489758491516113 0.35485053062438965
CurrentTrain: epoch  2, batch    35 | loss: 4.8446093Losses:  4.68373441696167 0.27477243542671204
CurrentTrain: epoch  2, batch    36 | loss: 4.9585071Losses:  4.594605445861816 0.3685908913612366
CurrentTrain: epoch  2, batch    37 | loss: 4.9631963Losses:  4.456446170806885 0.2746623158454895
CurrentTrain: epoch  2, batch    38 | loss: 4.7311087Losses:  4.486339092254639 0.2928508520126343
CurrentTrain: epoch  2, batch    39 | loss: 4.7791901Losses:  4.561305999755859 0.2558165192604065
CurrentTrain: epoch  2, batch    40 | loss: 4.8171225Losses:  4.306580543518066 0.29536545276641846
CurrentTrain: epoch  2, batch    41 | loss: 4.6019459Losses:  4.420886039733887 0.27529335021972656
CurrentTrain: epoch  2, batch    42 | loss: 4.6961794Losses:  4.4066267013549805 0.2528421878814697
CurrentTrain: epoch  2, batch    43 | loss: 4.6594687Losses:  4.715524673461914 0.4388968348503113
CurrentTrain: epoch  2, batch    44 | loss: 5.1544213Losses:  4.487715244293213 0.3489380478858948
CurrentTrain: epoch  2, batch    45 | loss: 4.8366532Losses:  4.414911270141602 0.24858234822750092
CurrentTrain: epoch  2, batch    46 | loss: 4.6634936Losses:  4.4915924072265625 0.3388885259628296
CurrentTrain: epoch  2, batch    47 | loss: 4.8304811Losses:  4.589353561401367 0.33509695529937744
CurrentTrain: epoch  2, batch    48 | loss: 4.9244504Losses:  4.466716766357422 0.23563608527183533
CurrentTrain: epoch  2, batch    49 | loss: 4.7023530Losses:  4.823638916015625 0.4110982120037079
CurrentTrain: epoch  2, batch    50 | loss: 5.2347369Losses:  4.33453369140625 0.26629960536956787
CurrentTrain: epoch  2, batch    51 | loss: 4.6008334Losses:  4.41423225402832 0.19228196144104004
CurrentTrain: epoch  2, batch    52 | loss: 4.6065140Losses:  4.2993011474609375 0.336402952671051
CurrentTrain: epoch  2, batch    53 | loss: 4.6357040Losses:  4.527871131896973 0.40260568261146545
CurrentTrain: epoch  2, batch    54 | loss: 4.9304767Losses:  4.308202266693115 0.2746902406215668
CurrentTrain: epoch  2, batch    55 | loss: 4.5828924Losses:  4.5028862953186035 0.318393349647522
CurrentTrain: epoch  2, batch    56 | loss: 4.8212795Losses:  4.778253078460693 0.3961184322834015
CurrentTrain: epoch  2, batch    57 | loss: 5.1743717Losses:  4.527118682861328 0.3439375162124634
CurrentTrain: epoch  2, batch    58 | loss: 4.8710561Losses:  4.356742858886719 0.2947491407394409
CurrentTrain: epoch  2, batch    59 | loss: 4.6514921Losses:  4.55255126953125 0.3236143887042999
CurrentTrain: epoch  2, batch    60 | loss: 4.8761659Losses:  4.359519958496094 0.2499411404132843
CurrentTrain: epoch  2, batch    61 | loss: 4.6094613Losses:  4.106374263763428 0.12090998888015747
CurrentTrain: epoch  2, batch    62 | loss: 4.2272844Losses:  4.340793609619141 0.2806466221809387
CurrentTrain: epoch  3, batch     0 | loss: 4.6214404Losses:  4.2147979736328125 0.23961469531059265
CurrentTrain: epoch  3, batch     1 | loss: 4.4544125Losses:  4.458704471588135 0.30638062953948975
CurrentTrain: epoch  3, batch     2 | loss: 4.7650852Losses:  4.366041660308838 0.3173292875289917
CurrentTrain: epoch  3, batch     3 | loss: 4.6833711Losses:  4.21756649017334 0.2845118045806885
CurrentTrain: epoch  3, batch     4 | loss: 4.5020781Losses:  4.437564849853516 0.15976029634475708
CurrentTrain: epoch  3, batch     5 | loss: 4.5973253Losses:  4.256924629211426 0.18265196681022644
CurrentTrain: epoch  3, batch     6 | loss: 4.4395766Losses:  4.397488594055176 0.2520520091056824
CurrentTrain: epoch  3, batch     7 | loss: 4.6495404Losses:  4.242992401123047 0.2662685513496399
CurrentTrain: epoch  3, batch     8 | loss: 4.5092611Losses:  4.405083656311035 0.2681888937950134
CurrentTrain: epoch  3, batch     9 | loss: 4.6732726Losses:  4.231095790863037 0.21152877807617188
CurrentTrain: epoch  3, batch    10 | loss: 4.4426246Losses:  4.176476001739502 0.23538900911808014
CurrentTrain: epoch  3, batch    11 | loss: 4.4118652Losses:  4.479578018188477 0.25686728954315186
CurrentTrain: epoch  3, batch    12 | loss: 4.7364454Losses:  4.435734748840332 0.26279735565185547
CurrentTrain: epoch  3, batch    13 | loss: 4.6985321Losses:  4.581240653991699 0.26909077167510986
CurrentTrain: epoch  3, batch    14 | loss: 4.8503313Losses:  4.368051528930664 0.20635750889778137
CurrentTrain: epoch  3, batch    15 | loss: 4.5744090Losses:  4.415894508361816 0.21962249279022217
CurrentTrain: epoch  3, batch    16 | loss: 4.6355171Losses:  4.421711444854736 0.25091058015823364
CurrentTrain: epoch  3, batch    17 | loss: 4.6726222Losses:  4.175120830535889 0.20324021577835083
CurrentTrain: epoch  3, batch    18 | loss: 4.3783612Losses:  4.319880485534668 0.27311426401138306
CurrentTrain: epoch  3, batch    19 | loss: 4.5929947Losses:  4.259576797485352 0.231692373752594
CurrentTrain: epoch  3, batch    20 | loss: 4.4912691Losses:  4.3361945152282715 0.23069563508033752
CurrentTrain: epoch  3, batch    21 | loss: 4.5668902Losses:  4.334393501281738 0.21302960813045502
CurrentTrain: epoch  3, batch    22 | loss: 4.5474229Losses:  4.281726837158203 0.168308824300766
CurrentTrain: epoch  3, batch    23 | loss: 4.4500356Losses:  4.207432746887207 0.1468813419342041
CurrentTrain: epoch  3, batch    24 | loss: 4.3543139Losses:  4.323592662811279 0.20825441181659698
CurrentTrain: epoch  3, batch    25 | loss: 4.5318470Losses:  4.38578462600708 0.1767175942659378
CurrentTrain: epoch  3, batch    26 | loss: 4.5625024Losses:  4.508646488189697 0.20773915946483612
CurrentTrain: epoch  3, batch    27 | loss: 4.7163858Losses:  4.234861373901367 0.30962592363357544
CurrentTrain: epoch  3, batch    28 | loss: 4.5444875Losses:  4.150848865509033 0.1747339516878128
CurrentTrain: epoch  3, batch    29 | loss: 4.3255830Losses:  4.162999153137207 0.1533103734254837
CurrentTrain: epoch  3, batch    30 | loss: 4.3163095Losses:  4.15535306930542 0.21557173132896423
CurrentTrain: epoch  3, batch    31 | loss: 4.3709249Losses:  4.251369476318359 0.2269030213356018
CurrentTrain: epoch  3, batch    32 | loss: 4.4782724Losses:  4.228972434997559 0.24886858463287354
CurrentTrain: epoch  3, batch    33 | loss: 4.4778409Losses:  4.130214691162109 0.23496805131435394
CurrentTrain: epoch  3, batch    34 | loss: 4.3651829Losses:  4.269754409790039 0.27921706438064575
CurrentTrain: epoch  3, batch    35 | loss: 4.5489717Losses:  4.272543430328369 0.2885211110115051
CurrentTrain: epoch  3, batch    36 | loss: 4.5610647Losses:  4.091281890869141 0.21109089255332947
CurrentTrain: epoch  3, batch    37 | loss: 4.3023729Losses:  4.80402135848999 0.3864160180091858
CurrentTrain: epoch  3, batch    38 | loss: 5.1904373Losses:  4.540805816650391 0.2077898383140564
CurrentTrain: epoch  3, batch    39 | loss: 4.7485957Losses:  4.194157123565674 0.14436975121498108
CurrentTrain: epoch  3, batch    40 | loss: 4.3385267Losses:  4.136012077331543 0.10682958364486694
CurrentTrain: epoch  3, batch    41 | loss: 4.2428417Losses:  4.832311153411865 0.1959267109632492
CurrentTrain: epoch  3, batch    42 | loss: 5.0282378Losses:  4.19069766998291 0.202741801738739
CurrentTrain: epoch  3, batch    43 | loss: 4.3934393Losses:  4.248934745788574 0.21025919914245605
CurrentTrain: epoch  3, batch    44 | loss: 4.4591942Losses:  4.5873122215271 0.202946275472641
CurrentTrain: epoch  3, batch    45 | loss: 4.7902584Losses:  4.259701728820801 0.18630477786064148
CurrentTrain: epoch  3, batch    46 | loss: 4.4460063Losses:  4.186301231384277 0.26192706823349
CurrentTrain: epoch  3, batch    47 | loss: 4.4482284Losses:  4.197713851928711 0.17483435571193695
CurrentTrain: epoch  3, batch    48 | loss: 4.3725481Losses:  4.132214546203613 0.1972278654575348
CurrentTrain: epoch  3, batch    49 | loss: 4.3294425Losses:  4.169757843017578 0.20313981175422668
CurrentTrain: epoch  3, batch    50 | loss: 4.3728976Losses:  4.152730941772461 0.17484791576862335
CurrentTrain: epoch  3, batch    51 | loss: 4.3275790Losses:  4.2148051261901855 0.194003164768219
CurrentTrain: epoch  3, batch    52 | loss: 4.4088082Losses:  4.206071376800537 0.19284844398498535
CurrentTrain: epoch  3, batch    53 | loss: 4.3989201Losses:  4.2481369972229 0.21760475635528564
CurrentTrain: epoch  3, batch    54 | loss: 4.4657416Losses:  4.380157470703125 0.3164956569671631
CurrentTrain: epoch  3, batch    55 | loss: 4.6966534Losses:  4.156091690063477 0.18906672298908234
CurrentTrain: epoch  3, batch    56 | loss: 4.3451586Losses:  4.189087867736816 0.19626010954380035
CurrentTrain: epoch  3, batch    57 | loss: 4.3853478Losses:  4.514957904815674 0.3286168575286865
CurrentTrain: epoch  3, batch    58 | loss: 4.8435745Losses:  4.125964641571045 0.1783149391412735
CurrentTrain: epoch  3, batch    59 | loss: 4.3042798Losses:  4.327181339263916 0.19598841667175293
CurrentTrain: epoch  3, batch    60 | loss: 4.5231695Losses:  4.199917316436768 0.23154261708259583
CurrentTrain: epoch  3, batch    61 | loss: 4.4314599Losses:  4.145301818847656 0.09239911288022995
CurrentTrain: epoch  3, batch    62 | loss: 4.2377009Losses:  4.098542213439941 0.1382817029953003
CurrentTrain: epoch  4, batch     0 | loss: 4.2368240Losses:  4.06251859664917 0.19861045479774475
CurrentTrain: epoch  4, batch     1 | loss: 4.2611289Losses:  4.2082672119140625 0.2237023115158081
CurrentTrain: epoch  4, batch     2 | loss: 4.4319696Losses:  4.177341461181641 0.18674272298812866
CurrentTrain: epoch  4, batch     3 | loss: 4.3640842Losses:  4.099479675292969 0.1611582338809967
CurrentTrain: epoch  4, batch     4 | loss: 4.2606378Losses:  4.213037490844727 0.18622075021266937
CurrentTrain: epoch  4, batch     5 | loss: 4.3992581Losses:  4.08505916595459 0.11422836780548096
CurrentTrain: epoch  4, batch     6 | loss: 4.1992874Losses:  4.2280402183532715 0.17222732305526733
CurrentTrain: epoch  4, batch     7 | loss: 4.4002676Losses:  4.027395248413086 0.14526790380477905
CurrentTrain: epoch  4, batch     8 | loss: 4.1726632Losses:  4.06641960144043 0.19710862636566162
CurrentTrain: epoch  4, batch     9 | loss: 4.2635283Losses:  4.14958381652832 0.2054968625307083
CurrentTrain: epoch  4, batch    10 | loss: 4.3550806Losses:  4.110134601593018 0.17145326733589172
CurrentTrain: epoch  4, batch    11 | loss: 4.2815881Losses:  4.168482780456543 0.13475990295410156
CurrentTrain: epoch  4, batch    12 | loss: 4.3032427Losses:  4.034790992736816 0.1319311112165451
CurrentTrain: epoch  4, batch    13 | loss: 4.1667223Losses:  4.154772758483887 0.167344868183136
CurrentTrain: epoch  4, batch    14 | loss: 4.3221178Losses:  4.103638648986816 0.14757803082466125
CurrentTrain: epoch  4, batch    15 | loss: 4.2512169Losses:  4.214145660400391 0.1291588991880417
CurrentTrain: epoch  4, batch    16 | loss: 4.3433046Losses:  4.217387676239014 0.2663436830043793
CurrentTrain: epoch  4, batch    17 | loss: 4.4837313Losses:  4.319116115570068 0.1523628979921341
CurrentTrain: epoch  4, batch    18 | loss: 4.4714789Losses:  4.0689849853515625 0.18707677721977234
CurrentTrain: epoch  4, batch    19 | loss: 4.2560616Losses:  4.209744453430176 0.2054072618484497
CurrentTrain: epoch  4, batch    20 | loss: 4.4151516Losses:  4.087517261505127 0.20406240224838257
CurrentTrain: epoch  4, batch    21 | loss: 4.2915797Losses:  4.12968111038208 0.11080494523048401
CurrentTrain: epoch  4, batch    22 | loss: 4.2404861Losses:  4.083690166473389 0.18395669758319855
CurrentTrain: epoch  4, batch    23 | loss: 4.2676468Losses:  4.1223859786987305 0.14993257820606232
CurrentTrain: epoch  4, batch    24 | loss: 4.2723184Losses:  4.0609588623046875 0.18350109457969666
CurrentTrain: epoch  4, batch    25 | loss: 4.2444601Losses:  4.079728603363037 0.19549842178821564
CurrentTrain: epoch  4, batch    26 | loss: 4.2752271Losses:  4.128602504730225 0.15196344256401062
CurrentTrain: epoch  4, batch    27 | loss: 4.2805657Losses:  4.075002670288086 0.18586844205856323
CurrentTrain: epoch  4, batch    28 | loss: 4.2608709Losses:  4.250506401062012 0.17216384410858154
CurrentTrain: epoch  4, batch    29 | loss: 4.4226704Losses:  4.191527366638184 0.14667603373527527
CurrentTrain: epoch  4, batch    30 | loss: 4.3382034Losses:  4.11997127532959 0.10923290252685547
CurrentTrain: epoch  4, batch    31 | loss: 4.2292042Losses:  4.039176940917969 0.19752228260040283
CurrentTrain: epoch  4, batch    32 | loss: 4.2366991Losses:  4.104744911193848 0.12485088407993317
CurrentTrain: epoch  4, batch    33 | loss: 4.2295957Losses:  4.059200286865234 0.1700069010257721
CurrentTrain: epoch  4, batch    34 | loss: 4.2292070Losses:  4.154771327972412 0.20405688881874084
CurrentTrain: epoch  4, batch    35 | loss: 4.3588281Losses:  4.103781700134277 0.15238186717033386
CurrentTrain: epoch  4, batch    36 | loss: 4.2561636Losses:  4.106358528137207 0.17287874221801758
CurrentTrain: epoch  4, batch    37 | loss: 4.2792373Losses:  4.094862937927246 0.13813358545303345
CurrentTrain: epoch  4, batch    38 | loss: 4.2329965Losses:  4.664228439331055 0.36543336510658264
CurrentTrain: epoch  4, batch    39 | loss: 5.0296617Losses:  4.063103199005127 0.17823933064937592
CurrentTrain: epoch  4, batch    40 | loss: 4.2413425Losses:  4.179826259613037 0.18845075368881226
CurrentTrain: epoch  4, batch    41 | loss: 4.3682771Losses:  4.141448974609375 0.09254573285579681
CurrentTrain: epoch  4, batch    42 | loss: 4.2339945Losses:  4.058438301086426 0.17789852619171143
CurrentTrain: epoch  4, batch    43 | loss: 4.2363367Losses:  4.0619659423828125 0.1287643164396286
CurrentTrain: epoch  4, batch    44 | loss: 4.1907301Losses:  4.05482292175293 0.13247829675674438
CurrentTrain: epoch  4, batch    45 | loss: 4.1873012Losses:  4.119741916656494 0.16325396299362183
CurrentTrain: epoch  4, batch    46 | loss: 4.2829957Losses:  4.131312847137451 0.18564602732658386
CurrentTrain: epoch  4, batch    47 | loss: 4.3169589Losses:  4.033991813659668 0.1355455368757248
CurrentTrain: epoch  4, batch    48 | loss: 4.1695375Losses:  4.012999534606934 0.15823300182819366
CurrentTrain: epoch  4, batch    49 | loss: 4.1712327Losses:  4.046084403991699 0.14105655252933502
CurrentTrain: epoch  4, batch    50 | loss: 4.1871409Losses:  4.020395278930664 0.09665554761886597
CurrentTrain: epoch  4, batch    51 | loss: 4.1170506Losses:  4.155879020690918 0.15294037759304047
CurrentTrain: epoch  4, batch    52 | loss: 4.3088193Losses:  4.035780429840088 0.09916970133781433
CurrentTrain: epoch  4, batch    53 | loss: 4.1349502Losses:  4.067583084106445 0.16993585228919983
CurrentTrain: epoch  4, batch    54 | loss: 4.2375188Losses:  4.1743621826171875 0.211500883102417
CurrentTrain: epoch  4, batch    55 | loss: 4.3858633Losses:  4.022329807281494 0.06290203332901001
CurrentTrain: epoch  4, batch    56 | loss: 4.0852318Losses:  4.080947399139404 0.14601382613182068
CurrentTrain: epoch  4, batch    57 | loss: 4.2269611Losses:  4.028378009796143 0.17548790574073792
CurrentTrain: epoch  4, batch    58 | loss: 4.2038660Losses:  4.063817501068115 0.17929232120513916
CurrentTrain: epoch  4, batch    59 | loss: 4.2431097Losses:  4.062110424041748 0.10462383925914764
CurrentTrain: epoch  4, batch    60 | loss: 4.1667342Losses:  4.103731155395508 0.09331223368644714
CurrentTrain: epoch  4, batch    61 | loss: 4.1970434Losses:  3.9975268840789795 0.07218475639820099
CurrentTrain: epoch  4, batch    62 | loss: 4.0697117Losses:  4.008716583251953 0.14948876202106476
CurrentTrain: epoch  5, batch     0 | loss: 4.1582055Losses:  3.976393938064575 0.07604920864105225
CurrentTrain: epoch  5, batch     1 | loss: 4.0524430Losses:  4.097785949707031 0.14738424122333527
CurrentTrain: epoch  5, batch     2 | loss: 4.2451701Losses:  4.064713478088379 0.16019894182682037
CurrentTrain: epoch  5, batch     3 | loss: 4.2249126Losses:  4.041908264160156 0.09717337042093277
CurrentTrain: epoch  5, batch     4 | loss: 4.1390815Losses:  4.086373329162598 0.10734033584594727
CurrentTrain: epoch  5, batch     5 | loss: 4.1937137Losses:  3.9994373321533203 0.16891473531723022
CurrentTrain: epoch  5, batch     6 | loss: 4.1683521Losses:  4.068120956420898 0.16195349395275116
CurrentTrain: epoch  5, batch     7 | loss: 4.2300744Losses:  4.042058944702148 0.15573647618293762
CurrentTrain: epoch  5, batch     8 | loss: 4.1977954Losses:  4.052534580230713 0.1317003071308136
CurrentTrain: epoch  5, batch     9 | loss: 4.1842351Losses:  4.042568683624268 0.0916920155286789
CurrentTrain: epoch  5, batch    10 | loss: 4.1342607Losses:  4.082354545593262 0.1514807790517807
CurrentTrain: epoch  5, batch    11 | loss: 4.2338352Losses:  3.9886422157287598 0.11586080491542816
CurrentTrain: epoch  5, batch    12 | loss: 4.1045032Losses:  4.010434150695801 0.11356201767921448
CurrentTrain: epoch  5, batch    13 | loss: 4.1239963Losses:  4.163726806640625 0.07770057022571564
CurrentTrain: epoch  5, batch    14 | loss: 4.2414274Losses:  4.015397548675537 0.12446295469999313
CurrentTrain: epoch  5, batch    15 | loss: 4.1398606Losses:  4.01223087310791 0.1165289431810379
CurrentTrain: epoch  5, batch    16 | loss: 4.1287599Losses:  4.067084312438965 0.13959649205207825
CurrentTrain: epoch  5, batch    17 | loss: 4.2066808Losses:  4.041851997375488 0.0781654566526413
CurrentTrain: epoch  5, batch    18 | loss: 4.1200175Losses:  4.032088756561279 0.15176820755004883
CurrentTrain: epoch  5, batch    19 | loss: 4.1838570Losses:  4.036357402801514 0.13656258583068848
CurrentTrain: epoch  5, batch    20 | loss: 4.1729202Losses:  4.099390983581543 0.1726093739271164
CurrentTrain: epoch  5, batch    21 | loss: 4.2720003Losses:  3.995901107788086 0.10291760414838791
CurrentTrain: epoch  5, batch    22 | loss: 4.0988188Losses:  4.024925231933594 0.13068462908267975
CurrentTrain: epoch  5, batch    23 | loss: 4.1556101Losses:  4.080434799194336 0.15636414289474487
CurrentTrain: epoch  5, batch    24 | loss: 4.2367988Losses:  4.036579132080078 0.1094169020652771
CurrentTrain: epoch  5, batch    25 | loss: 4.1459961Losses:  4.003849506378174 0.12861590087413788
CurrentTrain: epoch  5, batch    26 | loss: 4.1324654Losses:  3.995828628540039 0.08498407155275345
CurrentTrain: epoch  5, batch    27 | loss: 4.0808129Losses:  3.993912935256958 0.11769722402095795
CurrentTrain: epoch  5, batch    28 | loss: 4.1116099Losses:  4.005911350250244 0.1357523798942566
CurrentTrain: epoch  5, batch    29 | loss: 4.1416636Losses:  4.009783744812012 0.12862923741340637
CurrentTrain: epoch  5, batch    30 | loss: 4.1384130Losses:  4.020571708679199 0.09035612642765045
CurrentTrain: epoch  5, batch    31 | loss: 4.1109281Losses:  3.9726595878601074 0.10000808537006378
CurrentTrain: epoch  5, batch    32 | loss: 4.0726676Losses:  4.033267498016357 0.12114674597978592
CurrentTrain: epoch  5, batch    33 | loss: 4.1544142Losses:  4.084290027618408 0.1466723382472992
CurrentTrain: epoch  5, batch    34 | loss: 4.2309623Losses:  4.048647880554199 0.1499153971672058
CurrentTrain: epoch  5, batch    35 | loss: 4.1985631Losses:  4.208324432373047 0.20567476749420166
CurrentTrain: epoch  5, batch    36 | loss: 4.4139991Losses:  3.9890928268432617 0.13126179575920105
CurrentTrain: epoch  5, batch    37 | loss: 4.1203547Losses:  3.994194269180298 0.1248348206281662
CurrentTrain: epoch  5, batch    38 | loss: 4.1190290Losses:  4.035431861877441 0.13905464112758636
CurrentTrain: epoch  5, batch    39 | loss: 4.1744866Losses:  3.9994773864746094 0.17134973406791687
CurrentTrain: epoch  5, batch    40 | loss: 4.1708269Losses:  3.9444665908813477 0.14640569686889648
CurrentTrain: epoch  5, batch    41 | loss: 4.0908723Losses:  4.084745407104492 0.16702347993850708
CurrentTrain: epoch  5, batch    42 | loss: 4.2517691Losses:  4.006229400634766 0.15161675214767456
CurrentTrain: epoch  5, batch    43 | loss: 4.1578460Losses:  4.009923458099365 0.1076890379190445
CurrentTrain: epoch  5, batch    44 | loss: 4.1176124Losses:  4.005662441253662 0.1328357458114624
CurrentTrain: epoch  5, batch    45 | loss: 4.1384983Losses:  3.9840312004089355 0.09707379341125488
CurrentTrain: epoch  5, batch    46 | loss: 4.0811052Losses:  3.9541454315185547 0.14395081996917725
CurrentTrain: epoch  5, batch    47 | loss: 4.0980964Losses:  4.057737827301025 0.14611336588859558
CurrentTrain: epoch  5, batch    48 | loss: 4.2038512Losses:  3.998844623565674 0.14125925302505493
CurrentTrain: epoch  5, batch    49 | loss: 4.1401038Losses:  3.9961328506469727 0.16558831930160522
CurrentTrain: epoch  5, batch    50 | loss: 4.1617212Losses:  3.9618048667907715 0.1279258131980896
CurrentTrain: epoch  5, batch    51 | loss: 4.0897307Losses:  4.002378463745117 0.0997273325920105
CurrentTrain: epoch  5, batch    52 | loss: 4.1021056Losses:  3.972463369369507 0.15024425089359283
CurrentTrain: epoch  5, batch    53 | loss: 4.1227078Losses:  4.00360107421875 0.10912973433732986
CurrentTrain: epoch  5, batch    54 | loss: 4.1127310Losses:  3.9803149700164795 0.09121866524219513
CurrentTrain: epoch  5, batch    55 | loss: 4.0715337Losses:  3.9878668785095215 0.1141468957066536
CurrentTrain: epoch  5, batch    56 | loss: 4.1020136Losses:  4.034758567810059 0.11032246053218842
CurrentTrain: epoch  5, batch    57 | loss: 4.1450810Losses:  4.019228935241699 0.09103327989578247
CurrentTrain: epoch  5, batch    58 | loss: 4.1102624Losses:  3.9575083255767822 0.09968919306993484
CurrentTrain: epoch  5, batch    59 | loss: 4.0571976Losses:  3.9720828533172607 0.12169472873210907
CurrentTrain: epoch  5, batch    60 | loss: 4.0937777Losses:  3.982487916946411 0.12971943616867065
CurrentTrain: epoch  5, batch    61 | loss: 4.1122074Losses:  3.967252016067505 0.09510951489210129
CurrentTrain: epoch  5, batch    62 | loss: 4.0623617Losses:  4.00405740737915 0.13617901504039764
CurrentTrain: epoch  6, batch     0 | loss: 4.1402364Losses:  3.985755443572998 0.1203802227973938
CurrentTrain: epoch  6, batch     1 | loss: 4.1061358Losses:  3.9853358268737793 0.10998378694057465
CurrentTrain: epoch  6, batch     2 | loss: 4.0953197Losses:  3.952238082885742 0.11830280721187592
CurrentTrain: epoch  6, batch     3 | loss: 4.0705409Losses:  4.021113395690918 0.11264464259147644
CurrentTrain: epoch  6, batch     4 | loss: 4.1337581Losses:  3.947296619415283 0.10155433416366577
CurrentTrain: epoch  6, batch     5 | loss: 4.0488510Losses:  3.9845027923583984 0.08801442384719849
CurrentTrain: epoch  6, batch     6 | loss: 4.0725174Losses:  3.9768528938293457 0.13477230072021484
CurrentTrain: epoch  6, batch     7 | loss: 4.1116252Losses:  4.035194396972656 0.14700372517108917
CurrentTrain: epoch  6, batch     8 | loss: 4.1821980Losses:  4.026725769042969 0.10016423463821411
CurrentTrain: epoch  6, batch     9 | loss: 4.1268902Losses:  3.9294185638427734 0.0852411538362503
CurrentTrain: epoch  6, batch    10 | loss: 4.0146599Losses:  4.037004470825195 0.13246984779834747
CurrentTrain: epoch  6, batch    11 | loss: 4.1694741Losses:  3.99432635307312 0.10707645863294601
CurrentTrain: epoch  6, batch    12 | loss: 4.1014028Losses:  4.003005504608154 0.0854247659444809
CurrentTrain: epoch  6, batch    13 | loss: 4.0884304Losses:  3.9764857292175293 0.07880426198244095
CurrentTrain: epoch  6, batch    14 | loss: 4.0552902Losses:  3.9602274894714355 0.1258956789970398
CurrentTrain: epoch  6, batch    15 | loss: 4.0861230Losses:  3.9915719032287598 0.09663721919059753
CurrentTrain: epoch  6, batch    16 | loss: 4.0882092Losses:  4.009081840515137 0.10138367116451263
CurrentTrain: epoch  6, batch    17 | loss: 4.1104655Losses:  4.00194787979126 0.10622411966323853
CurrentTrain: epoch  6, batch    18 | loss: 4.1081719Losses:  4.000326156616211 0.11156754195690155
CurrentTrain: epoch  6, batch    19 | loss: 4.1118937Losses:  3.9941859245300293 0.1467714011669159
CurrentTrain: epoch  6, batch    20 | loss: 4.1409574Losses:  3.9022364616394043 0.13007502257823944
CurrentTrain: epoch  6, batch    21 | loss: 4.0323114Losses:  3.95847749710083 0.13666850328445435
CurrentTrain: epoch  6, batch    22 | loss: 4.0951462Losses:  3.959587812423706 0.06929196417331696
CurrentTrain: epoch  6, batch    23 | loss: 4.0288796Losses:  3.9315109252929688 0.1266898363828659
CurrentTrain: epoch  6, batch    24 | loss: 4.0582008Losses:  3.9794058799743652 0.12559305131435394
CurrentTrain: epoch  6, batch    25 | loss: 4.1049991Losses:  3.969501256942749 0.10648686438798904
CurrentTrain: epoch  6, batch    26 | loss: 4.0759883Losses:  4.036312580108643 0.10545551776885986
CurrentTrain: epoch  6, batch    27 | loss: 4.1417680Losses:  4.048617839813232 0.09807498008012772
CurrentTrain: epoch  6, batch    28 | loss: 4.1466928Losses:  3.9380857944488525 0.08385033905506134
CurrentTrain: epoch  6, batch    29 | loss: 4.0219359Losses:  3.9966070652008057 0.09845148772001266
CurrentTrain: epoch  6, batch    30 | loss: 4.0950584Losses:  3.9764404296875 0.11349254846572876
CurrentTrain: epoch  6, batch    31 | loss: 4.0899329Losses:  3.9986822605133057 0.09916572272777557
CurrentTrain: epoch  6, batch    32 | loss: 4.0978479Losses:  3.994814872741699 0.10752374678850174
CurrentTrain: epoch  6, batch    33 | loss: 4.1023388Losses:  3.987001657485962 0.09148503094911575
CurrentTrain: epoch  6, batch    34 | loss: 4.0784869Losses:  4.006631851196289 0.11947578191757202
CurrentTrain: epoch  6, batch    35 | loss: 4.1261077Losses:  4.017330169677734 0.0896511822938919
CurrentTrain: epoch  6, batch    36 | loss: 4.1069813Losses:  3.9712424278259277 0.12169571220874786
CurrentTrain: epoch  6, batch    37 | loss: 4.0929379Losses:  3.9659674167633057 0.12795238196849823
CurrentTrain: epoch  6, batch    38 | loss: 4.0939198Losses:  3.9784789085388184 0.13695435225963593
CurrentTrain: epoch  6, batch    39 | loss: 4.1154332Losses:  3.940821409225464 0.08945973217487335
CurrentTrain: epoch  6, batch    40 | loss: 4.0302811Losses:  4.004663467407227 0.06636898964643478
CurrentTrain: epoch  6, batch    41 | loss: 4.0710325Losses:  3.992699146270752 0.10119796544313431
CurrentTrain: epoch  6, batch    42 | loss: 4.0938973Losses:  3.937666177749634 0.11957177519798279
CurrentTrain: epoch  6, batch    43 | loss: 4.0572381Losses:  3.9830193519592285 0.06709801405668259
CurrentTrain: epoch  6, batch    44 | loss: 4.0501175Losses:  3.9524707794189453 0.11650723218917847
CurrentTrain: epoch  6, batch    45 | loss: 4.0689778Losses:  3.9555583000183105 0.12849512696266174
CurrentTrain: epoch  6, batch    46 | loss: 4.0840535Losses:  3.9922285079956055 0.0943194031715393
CurrentTrain: epoch  6, batch    47 | loss: 4.0865479Losses:  4.0014214515686035 0.10854623466730118
CurrentTrain: epoch  6, batch    48 | loss: 4.1099677Losses:  3.970323085784912 0.11291414499282837
CurrentTrain: epoch  6, batch    49 | loss: 4.0832372Losses:  3.9648966789245605 0.0739133432507515
CurrentTrain: epoch  6, batch    50 | loss: 4.0388103Losses:  3.9100728034973145 0.09008089452981949
CurrentTrain: epoch  6, batch    51 | loss: 4.0001535Losses:  4.001114845275879 0.08879134058952332
CurrentTrain: epoch  6, batch    52 | loss: 4.0899062Losses:  3.9719576835632324 0.10750603675842285
CurrentTrain: epoch  6, batch    53 | loss: 4.0794640Losses:  3.956808090209961 0.12588852643966675
CurrentTrain: epoch  6, batch    54 | loss: 4.0826964Losses:  3.9805171489715576 0.10295194387435913
CurrentTrain: epoch  6, batch    55 | loss: 4.0834689Losses:  3.987206220626831 0.06065090000629425
CurrentTrain: epoch  6, batch    56 | loss: 4.0478573Losses:  4.004730224609375 0.13453538715839386
CurrentTrain: epoch  6, batch    57 | loss: 4.1392655Losses:  3.978525161743164 0.10533429682254791
CurrentTrain: epoch  6, batch    58 | loss: 4.0838594Losses:  4.028649806976318 0.1515173316001892
CurrentTrain: epoch  6, batch    59 | loss: 4.1801672Losses:  3.9611563682556152 0.07754140347242355
CurrentTrain: epoch  6, batch    60 | loss: 4.0386977Losses:  4.009102821350098 0.07572352886199951
CurrentTrain: epoch  6, batch    61 | loss: 4.0848265Losses:  3.935253858566284 0.06654398143291473
CurrentTrain: epoch  6, batch    62 | loss: 4.0017977Losses:  4.054388046264648 0.12552089989185333
CurrentTrain: epoch  7, batch     0 | loss: 4.1799088Losses:  3.983616352081299 0.10258732736110687
CurrentTrain: epoch  7, batch     1 | loss: 4.0862036Losses:  3.9862756729125977 0.07795926928520203
CurrentTrain: epoch  7, batch     2 | loss: 4.0642347Losses:  3.9669551849365234 0.10088545829057693
CurrentTrain: epoch  7, batch     3 | loss: 4.0678406Losses:  3.9581985473632812 0.11077546328306198
CurrentTrain: epoch  7, batch     4 | loss: 4.0689740Losses:  3.9697067737579346 0.11933819949626923
CurrentTrain: epoch  7, batch     5 | loss: 4.0890450Losses:  3.9536595344543457 0.08587789535522461
CurrentTrain: epoch  7, batch     6 | loss: 4.0395374Losses:  3.9860830307006836 0.1026255190372467
CurrentTrain: epoch  7, batch     7 | loss: 4.0887084Losses:  3.9751486778259277 0.08914469182491302
CurrentTrain: epoch  7, batch     8 | loss: 4.0642934Losses:  3.929043769836426 0.07154949009418488
CurrentTrain: epoch  7, batch     9 | loss: 4.0005932Losses:  4.014848709106445 0.11181369423866272
CurrentTrain: epoch  7, batch    10 | loss: 4.1266623Losses:  3.979566812515259 0.10432417690753937
CurrentTrain: epoch  7, batch    11 | loss: 4.0838909Losses:  3.972080945968628 0.07114146649837494
CurrentTrain: epoch  7, batch    12 | loss: 4.0432224Losses:  3.96360445022583 0.07270831614732742
CurrentTrain: epoch  7, batch    13 | loss: 4.0363126Losses:  3.9533133506774902 0.11528200656175613
CurrentTrain: epoch  7, batch    14 | loss: 4.0685954Losses:  3.9224982261657715 0.09361071139574051
CurrentTrain: epoch  7, batch    15 | loss: 4.0161090Losses:  4.024406433105469 0.08888566493988037
CurrentTrain: epoch  7, batch    16 | loss: 4.1132922Losses:  3.986142635345459 0.1164536327123642
CurrentTrain: epoch  7, batch    17 | loss: 4.1025963Losses:  3.963024139404297 0.10311637073755264
CurrentTrain: epoch  7, batch    18 | loss: 4.0661407Losses:  4.018683433532715 0.0790095254778862
CurrentTrain: epoch  7, batch    19 | loss: 4.0976930Losses:  3.9831511974334717 0.06631726771593094
CurrentTrain: epoch  7, batch    20 | loss: 4.0494685Losses:  3.9757890701293945 0.07765726745128632
CurrentTrain: epoch  7, batch    21 | loss: 4.0534463Losses:  3.92787504196167 0.07791507989168167
CurrentTrain: epoch  7, batch    22 | loss: 4.0057902Losses:  3.943425416946411 0.11764241755008698
CurrentTrain: epoch  7, batch    23 | loss: 4.0610681Losses:  3.991065502166748 0.07728752493858337
CurrentTrain: epoch  7, batch    24 | loss: 4.0683532Losses:  3.948850393295288 0.07371188700199127
CurrentTrain: epoch  7, batch    25 | loss: 4.0225625Losses:  3.950291872024536 0.10770472139120102
CurrentTrain: epoch  7, batch    26 | loss: 4.0579967Losses:  3.994946002960205 0.09495509415864944
CurrentTrain: epoch  7, batch    27 | loss: 4.0899010Losses:  4.00653076171875 0.07477977871894836
CurrentTrain: epoch  7, batch    28 | loss: 4.0813107Losses:  3.991464138031006 0.08556224405765533
CurrentTrain: epoch  7, batch    29 | loss: 4.0770264Losses:  3.9376912117004395 0.09180405735969543
CurrentTrain: epoch  7, batch    30 | loss: 4.0294952Losses:  3.9626855850219727 0.11064508557319641
CurrentTrain: epoch  7, batch    31 | loss: 4.0733309Losses:  3.993967294692993 0.10244372487068176
CurrentTrain: epoch  7, batch    32 | loss: 4.0964112Losses:  3.9399280548095703 0.09025420248508453
CurrentTrain: epoch  7, batch    33 | loss: 4.0301824Losses:  3.960693836212158 0.05213269591331482
CurrentTrain: epoch  7, batch    34 | loss: 4.0128264Losses:  3.986175537109375 0.07316755503416061
CurrentTrain: epoch  7, batch    35 | loss: 4.0593429Losses:  4.012755870819092 0.10033787786960602
CurrentTrain: epoch  7, batch    36 | loss: 4.1130939Losses:  3.9954802989959717 0.08602484315633774
CurrentTrain: epoch  7, batch    37 | loss: 4.0815053Losses:  3.977895975112915 0.05405046045780182
CurrentTrain: epoch  7, batch    38 | loss: 4.0319467Losses:  4.1181182861328125 0.0861569344997406
CurrentTrain: epoch  7, batch    39 | loss: 4.2042751Losses:  4.018941879272461 0.06992685794830322
CurrentTrain: epoch  7, batch    40 | loss: 4.0888686Losses:  3.9522156715393066 0.08802592009305954
CurrentTrain: epoch  7, batch    41 | loss: 4.0402417Losses:  3.9492287635803223 0.11863218992948532
CurrentTrain: epoch  7, batch    42 | loss: 4.0678611Losses:  3.9673688411712646 0.0753331184387207
CurrentTrain: epoch  7, batch    43 | loss: 4.0427017Losses:  3.9631075859069824 0.10210995376110077
CurrentTrain: epoch  7, batch    44 | loss: 4.0652175Losses:  3.9617600440979004 0.08589370548725128
CurrentTrain: epoch  7, batch    45 | loss: 4.0476537Losses:  3.9714441299438477 0.059124015271663666
CurrentTrain: epoch  7, batch    46 | loss: 4.0305681Losses:  3.9936952590942383 0.06664348393678665
CurrentTrain: epoch  7, batch    47 | loss: 4.0603390Losses:  4.000790596008301 0.08509290963411331
CurrentTrain: epoch  7, batch    48 | loss: 4.0858836Losses:  3.911752462387085 0.07761531323194504
CurrentTrain: epoch  7, batch    49 | loss: 3.9893677Losses:  3.9130804538726807 0.06743748486042023
CurrentTrain: epoch  7, batch    50 | loss: 3.9805179Losses:  3.9704437255859375 0.08581241965293884
CurrentTrain: epoch  7, batch    51 | loss: 4.0562563Losses:  4.01572322845459 0.08045640587806702
CurrentTrain: epoch  7, batch    52 | loss: 4.0961795Losses:  3.9594504833221436 0.11046332120895386
CurrentTrain: epoch  7, batch    53 | loss: 4.0699139Losses:  3.982395648956299 0.09855810552835464
CurrentTrain: epoch  7, batch    54 | loss: 4.0809536Losses:  3.974520206451416 0.10042648762464523
CurrentTrain: epoch  7, batch    55 | loss: 4.0749469Losses:  3.9763777256011963 0.09494612365961075
CurrentTrain: epoch  7, batch    56 | loss: 4.0713239Losses:  3.9739911556243896 0.1029587835073471
CurrentTrain: epoch  7, batch    57 | loss: 4.0769501Losses:  3.96083402633667 0.10325472056865692
CurrentTrain: epoch  7, batch    58 | loss: 4.0640888Losses:  3.9281814098358154 0.09036921709775925
CurrentTrain: epoch  7, batch    59 | loss: 4.0185504Losses:  3.939547538757324 0.08806047588586807
CurrentTrain: epoch  7, batch    60 | loss: 4.0276079Losses:  4.020589351654053 0.08402836322784424
CurrentTrain: epoch  7, batch    61 | loss: 4.1046176Losses:  3.9360647201538086 0.05300122871994972
CurrentTrain: epoch  7, batch    62 | loss: 3.9890659Losses:  4.0000410079956055 0.05146205797791481
CurrentTrain: epoch  8, batch     0 | loss: 4.0515032Losses:  3.915060043334961 0.09122419357299805
CurrentTrain: epoch  8, batch     1 | loss: 4.0062842Losses:  3.9333622455596924 0.0904730036854744
CurrentTrain: epoch  8, batch     2 | loss: 4.0238352Losses:  3.9511802196502686 0.10155463218688965
CurrentTrain: epoch  8, batch     3 | loss: 4.0527349Losses:  3.9427080154418945 0.05778949707746506
CurrentTrain: epoch  8, batch     4 | loss: 4.0004973Losses:  3.960482597351074 0.08301717042922974
CurrentTrain: epoch  8, batch     5 | loss: 4.0434999Losses:  3.9706807136535645 0.0854368656873703
CurrentTrain: epoch  8, batch     6 | loss: 4.0561175Losses:  3.994284152984619 0.09854374825954437
CurrentTrain: epoch  8, batch     7 | loss: 4.0928278Losses:  3.975658893585205 0.06413795053958893
CurrentTrain: epoch  8, batch     8 | loss: 4.0397968Losses:  3.9528422355651855 0.07558464258909225
CurrentTrain: epoch  8, batch     9 | loss: 4.0284266Losses:  3.9577724933624268 0.06419552862644196
CurrentTrain: epoch  8, batch    10 | loss: 4.0219679Losses:  3.9827146530151367 0.09391419589519501
CurrentTrain: epoch  8, batch    11 | loss: 4.0766287Losses:  3.966761589050293 0.1061844527721405
CurrentTrain: epoch  8, batch    12 | loss: 4.0729461Losses:  3.9992733001708984 0.07024182379245758
CurrentTrain: epoch  8, batch    13 | loss: 4.0695152Losses:  3.964874744415283 0.10352468490600586
CurrentTrain: epoch  8, batch    14 | loss: 4.0683994Losses:  3.9845151901245117 0.08834730088710785
CurrentTrain: epoch  8, batch    15 | loss: 4.0728626Losses:  3.9423813819885254 0.079264335334301
CurrentTrain: epoch  8, batch    16 | loss: 4.0216455Losses:  3.9374465942382812 0.07135530561208725
CurrentTrain: epoch  8, batch    17 | loss: 4.0088019Losses:  3.9433481693267822 0.10290855914354324
CurrentTrain: epoch  8, batch    18 | loss: 4.0462565Losses:  3.9415040016174316 0.10941694676876068
CurrentTrain: epoch  8, batch    19 | loss: 4.0509210Losses:  3.9697046279907227 0.04806125909090042
CurrentTrain: epoch  8, batch    20 | loss: 4.0177660Losses:  3.9555439949035645 0.0771016925573349
CurrentTrain: epoch  8, batch    21 | loss: 4.0326457Losses:  3.955331802368164 0.07902903854846954
CurrentTrain: epoch  8, batch    22 | loss: 4.0343609Losses:  3.952258348464966 0.08629416674375534
CurrentTrain: epoch  8, batch    23 | loss: 4.0385523Losses:  3.965087413787842 0.09835933148860931
CurrentTrain: epoch  8, batch    24 | loss: 4.0634465Losses:  3.959597587585449 0.0915059745311737
CurrentTrain: epoch  8, batch    25 | loss: 4.0511036Losses:  3.9685492515563965 0.06344617903232574
CurrentTrain: epoch  8, batch    26 | loss: 4.0319953Losses:  3.9592044353485107 0.09696986526250839
CurrentTrain: epoch  8, batch    27 | loss: 4.0561743Losses:  3.9422717094421387 0.06925366818904877
CurrentTrain: epoch  8, batch    28 | loss: 4.0115252Losses:  3.9438533782958984 0.04246130585670471
CurrentTrain: epoch  8, batch    29 | loss: 3.9863148Losses:  3.956444501876831 0.08103589713573456
CurrentTrain: epoch  8, batch    30 | loss: 4.0374804Losses:  3.961214065551758 0.08626139163970947
CurrentTrain: epoch  8, batch    31 | loss: 4.0474753Losses:  3.923863649368286 0.05786451697349548
CurrentTrain: epoch  8, batch    32 | loss: 3.9817281Losses:  3.8950138092041016 0.09112048149108887
CurrentTrain: epoch  8, batch    33 | loss: 3.9861343Losses:  3.9431796073913574 0.09071294963359833
CurrentTrain: epoch  8, batch    34 | loss: 4.0338926Losses:  3.961379289627075 0.06007843464612961
CurrentTrain: epoch  8, batch    35 | loss: 4.0214577Losses:  3.9466214179992676 0.0976487547159195
CurrentTrain: epoch  8, batch    36 | loss: 4.0442700Losses:  3.9985544681549072 0.09052307158708572
CurrentTrain: epoch  8, batch    37 | loss: 4.0890775Losses:  3.8784518241882324 0.06693121790885925
CurrentTrain: epoch  8, batch    38 | loss: 3.9453831Losses:  3.954226016998291 0.08310450613498688
CurrentTrain: epoch  8, batch    39 | loss: 4.0373306Losses:  3.9492413997650146 0.05849788710474968
CurrentTrain: epoch  8, batch    40 | loss: 4.0077391Losses:  3.9315319061279297 0.08346911519765854
CurrentTrain: epoch  8, batch    41 | loss: 4.0150008Losses:  3.951535224914551 0.07469296455383301
CurrentTrain: epoch  8, batch    42 | loss: 4.0262280Losses:  3.948756217956543 0.09831531345844269
CurrentTrain: epoch  8, batch    43 | loss: 4.0470715Losses:  3.94718599319458 0.09362915903329849
CurrentTrain: epoch  8, batch    44 | loss: 4.0408154Losses:  4.330004692077637 0.15938912332057953
CurrentTrain: epoch  8, batch    45 | loss: 4.4893937Losses:  3.9347777366638184 0.08425804227590561
CurrentTrain: epoch  8, batch    46 | loss: 4.0190358Losses:  3.9450061321258545 0.0703253298997879
CurrentTrain: epoch  8, batch    47 | loss: 4.0153313Losses:  4.012085914611816 0.07903540134429932
CurrentTrain: epoch  8, batch    48 | loss: 4.0911212Losses:  3.9178647994995117 0.0804050862789154
CurrentTrain: epoch  8, batch    49 | loss: 3.9982698Losses:  3.949571132659912 0.07952658832073212
CurrentTrain: epoch  8, batch    50 | loss: 4.0290976Losses:  3.92012882232666 0.0577683262526989
CurrentTrain: epoch  8, batch    51 | loss: 3.9778972Losses:  3.927510976791382 0.0945533812046051
CurrentTrain: epoch  8, batch    52 | loss: 4.0220642Losses:  3.9443230628967285 0.09385599195957184
CurrentTrain: epoch  8, batch    53 | loss: 4.0381789Losses:  3.9656925201416016 0.07515008002519608
CurrentTrain: epoch  8, batch    54 | loss: 4.0408425Losses:  3.9846746921539307 0.0800493136048317
CurrentTrain: epoch  8, batch    55 | loss: 4.0647240Losses:  3.9603123664855957 0.08709469437599182
CurrentTrain: epoch  8, batch    56 | loss: 4.0474072Losses:  3.9363207817077637 0.09445954114198685
CurrentTrain: epoch  8, batch    57 | loss: 4.0307803Losses:  3.960360527038574 0.08234477043151855
CurrentTrain: epoch  8, batch    58 | loss: 4.0427055Losses:  3.965118408203125 0.08425047993659973
CurrentTrain: epoch  8, batch    59 | loss: 4.0493689Losses:  3.9560399055480957 0.08355239778757095
CurrentTrain: epoch  8, batch    60 | loss: 4.0395923Losses:  3.967905044555664 0.09186369180679321
CurrentTrain: epoch  8, batch    61 | loss: 4.0597687Losses:  4.032453536987305 0.05409899353981018
CurrentTrain: epoch  8, batch    62 | loss: 4.0865526Losses:  3.9604029655456543 0.08258190006017685
CurrentTrain: epoch  9, batch     0 | loss: 4.0429850Losses:  3.998408555984497 0.0746336430311203
CurrentTrain: epoch  9, batch     1 | loss: 4.0730424Losses:  3.9632797241210938 0.05687675252556801
CurrentTrain: epoch  9, batch     2 | loss: 4.0201564Losses:  3.914153575897217 0.052993591874837875
CurrentTrain: epoch  9, batch     3 | loss: 3.9671471Losses:  3.9954750537872314 0.06154609099030495
CurrentTrain: epoch  9, batch     4 | loss: 4.0570211Losses:  3.9697318077087402 0.08076018840074539
CurrentTrain: epoch  9, batch     5 | loss: 4.0504918Losses:  3.9437692165374756 0.08579356968402863
CurrentTrain: epoch  9, batch     6 | loss: 4.0295630Losses:  3.969723701477051 0.07869689166545868
CurrentTrain: epoch  9, batch     7 | loss: 4.0484204Losses:  3.962495803833008 0.04149226099252701
CurrentTrain: epoch  9, batch     8 | loss: 4.0039883Losses:  3.962226390838623 0.0683174878358841
CurrentTrain: epoch  9, batch     9 | loss: 4.0305438Losses:  3.966580390930176 0.08095306158065796
CurrentTrain: epoch  9, batch    10 | loss: 4.0475335Losses:  3.9566538333892822 0.06973620504140854
CurrentTrain: epoch  9, batch    11 | loss: 4.0263901Losses:  3.9799368381500244 0.05283089727163315
CurrentTrain: epoch  9, batch    12 | loss: 4.0327678Losses:  3.938384532928467 0.09802219271659851
CurrentTrain: epoch  9, batch    13 | loss: 4.0364065Losses:  4.025729179382324 0.0751199945807457
CurrentTrain: epoch  9, batch    14 | loss: 4.1008492Losses:  3.940829277038574 0.08802257478237152
CurrentTrain: epoch  9, batch    15 | loss: 4.0288520Losses:  3.9682886600494385 0.07885270565748215
CurrentTrain: epoch  9, batch    16 | loss: 4.0471416Losses:  3.9526445865631104 0.0848013311624527
CurrentTrain: epoch  9, batch    17 | loss: 4.0374460Losses:  3.978179454803467 0.0540495365858078
CurrentTrain: epoch  9, batch    18 | loss: 4.0322289Losses:  3.9401001930236816 0.0908692255616188
CurrentTrain: epoch  9, batch    19 | loss: 4.0309696Losses:  3.94769287109375 0.08507315814495087
CurrentTrain: epoch  9, batch    20 | loss: 4.0327659Losses:  3.9657764434814453 0.08871955424547195
CurrentTrain: epoch  9, batch    21 | loss: 4.0544958Losses:  3.961202621459961 0.07911105453968048
CurrentTrain: epoch  9, batch    22 | loss: 4.0403137Losses:  3.9288265705108643 0.06390512734651566
CurrentTrain: epoch  9, batch    23 | loss: 3.9927318Losses:  3.948603391647339 0.08954696357250214
CurrentTrain: epoch  9, batch    24 | loss: 4.0381503Losses:  3.926685333251953 0.0895436704158783
CurrentTrain: epoch  9, batch    25 | loss: 4.0162292Losses:  3.9451165199279785 0.09008372575044632
CurrentTrain: epoch  9, batch    26 | loss: 4.0352001Losses:  3.9974308013916016 0.08122830092906952
CurrentTrain: epoch  9, batch    27 | loss: 4.0786591Losses:  3.9566469192504883 0.08672304451465607
CurrentTrain: epoch  9, batch    28 | loss: 4.0433698Losses:  3.949786424636841 0.0800183117389679
CurrentTrain: epoch  9, batch    29 | loss: 4.0298047Losses:  3.9667582511901855 0.06576637923717499
CurrentTrain: epoch  9, batch    30 | loss: 4.0325246Losses:  4.0075578689575195 0.049722105264663696
CurrentTrain: epoch  9, batch    31 | loss: 4.0572801Losses:  3.929530620574951 0.05280913785099983
CurrentTrain: epoch  9, batch    32 | loss: 3.9823399Losses:  3.9627535343170166 0.04275422915816307
CurrentTrain: epoch  9, batch    33 | loss: 4.0055079Losses:  3.957373857498169 0.06441638618707657
CurrentTrain: epoch  9, batch    34 | loss: 4.0217900Losses:  3.949611186981201 0.08400292694568634
CurrentTrain: epoch  9, batch    35 | loss: 4.0336142Losses:  3.944227695465088 0.08355392515659332
CurrentTrain: epoch  9, batch    36 | loss: 4.0277815Losses:  3.9305105209350586 0.05791143700480461
CurrentTrain: epoch  9, batch    37 | loss: 3.9884219Losses:  3.9857337474823 0.06985355913639069
CurrentTrain: epoch  9, batch    38 | loss: 4.0555873Losses:  3.944039821624756 0.0760023221373558
CurrentTrain: epoch  9, batch    39 | loss: 4.0200419Losses:  3.9478158950805664 0.06639627367258072
CurrentTrain: epoch  9, batch    40 | loss: 4.0142121Losses:  3.93882417678833 0.09287989139556885
CurrentTrain: epoch  9, batch    41 | loss: 4.0317039Losses:  3.9483156204223633 0.05837070196866989
CurrentTrain: epoch  9, batch    42 | loss: 4.0066862Losses:  3.9322667121887207 0.06806078553199768
CurrentTrain: epoch  9, batch    43 | loss: 4.0003276Losses:  3.9604392051696777 0.0681314468383789
CurrentTrain: epoch  9, batch    44 | loss: 4.0285707Losses:  3.9689083099365234 0.07652278244495392
CurrentTrain: epoch  9, batch    45 | loss: 4.0454311Losses:  3.900094985961914 0.05106315761804581
CurrentTrain: epoch  9, batch    46 | loss: 3.9511580Losses:  3.9680581092834473 0.06024293228983879
CurrentTrain: epoch  9, batch    47 | loss: 4.0283012Losses:  3.9505691528320312 0.0699867233633995
CurrentTrain: epoch  9, batch    48 | loss: 4.0205560Losses:  3.9785351753234863 0.06440581381320953
CurrentTrain: epoch  9, batch    49 | loss: 4.0429411Losses:  3.9348716735839844 0.06771939247846603
CurrentTrain: epoch  9, batch    50 | loss: 4.0025911Losses:  3.880270004272461 0.08336691558361053
CurrentTrain: epoch  9, batch    51 | loss: 3.9636369Losses:  3.972224473953247 0.07936178147792816
CurrentTrain: epoch  9, batch    52 | loss: 4.0515862Losses:  3.919099807739258 0.06723818182945251
CurrentTrain: epoch  9, batch    53 | loss: 3.9863379Losses:  3.9582388401031494 0.08597305417060852
CurrentTrain: epoch  9, batch    54 | loss: 4.0442119Losses:  3.9437644481658936 0.05908452719449997
CurrentTrain: epoch  9, batch    55 | loss: 4.0028491Losses:  3.9279167652130127 0.061816923320293427
CurrentTrain: epoch  9, batch    56 | loss: 3.9897337Losses:  3.9459071159362793 0.04118695855140686
CurrentTrain: epoch  9, batch    57 | loss: 3.9870942Losses:  3.9596948623657227 0.07468271255493164
CurrentTrain: epoch  9, batch    58 | loss: 4.0343776Losses:  3.955198049545288 0.04252960532903671
CurrentTrain: epoch  9, batch    59 | loss: 3.9977276Losses:  3.9405336380004883 0.08047327399253845
CurrentTrain: epoch  9, batch    60 | loss: 4.0210071Losses:  3.983421564102173 0.09053324162960052
CurrentTrain: epoch  9, batch    61 | loss: 4.0739546Losses:  3.914860725402832 0.024602340534329414
CurrentTrain: epoch  9, batch    62 | loss: 3.9394631
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.90%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 94.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.05%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.24%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.21%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.28%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.34%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 95.07%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 94.94%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.02%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 94.90%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 94.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.76%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.05%   
cur_acc:  ['0.9405']
his_acc:  ['0.9405']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  8.552061080932617 1.7923359870910645
CurrentTrain: epoch  0, batch     0 | loss: 10.3443966Losses:  11.114227294921875 1.6579569578170776
CurrentTrain: epoch  0, batch     1 | loss: 12.7721844Losses:  9.274795532226562 1.6827378273010254
CurrentTrain: epoch  0, batch     2 | loss: 10.9575329Losses:  4.951962471008301 0.17593127489089966
CurrentTrain: epoch  0, batch     3 | loss: 5.1278939Losses:  5.029152870178223 1.6592262983322144
CurrentTrain: epoch  1, batch     0 | loss: 6.6883793Losses:  4.426527976989746 1.6378746032714844
CurrentTrain: epoch  1, batch     1 | loss: 6.0644026Losses:  4.609887599945068 1.6448190212249756
CurrentTrain: epoch  1, batch     2 | loss: 6.2547064Losses:  3.500779390335083 0.3705829381942749
CurrentTrain: epoch  1, batch     3 | loss: 3.8713622Losses:  4.071297645568848 1.1871405839920044
CurrentTrain: epoch  2, batch     0 | loss: 5.2584381Losses:  4.599665641784668 1.4928338527679443
CurrentTrain: epoch  2, batch     1 | loss: 6.0924997Losses:  4.130426406860352 1.5875244140625
CurrentTrain: epoch  2, batch     2 | loss: 5.7179508Losses:  3.0894131660461426 0.2181599736213684
CurrentTrain: epoch  2, batch     3 | loss: 3.3075731Losses:  4.194301605224609 0.9655625820159912
CurrentTrain: epoch  3, batch     0 | loss: 5.1598644Losses:  3.596555709838867 1.1791770458221436
CurrentTrain: epoch  3, batch     1 | loss: 4.7757330Losses:  4.139729022979736 1.6332671642303467
CurrentTrain: epoch  3, batch     2 | loss: 5.7729959Losses:  3.984557628631592 0.17131322622299194
CurrentTrain: epoch  3, batch     3 | loss: 4.1558709Losses:  3.1890034675598145 1.0304789543151855
CurrentTrain: epoch  4, batch     0 | loss: 4.2194824Losses:  4.471671104431152 1.3061840534210205
CurrentTrain: epoch  4, batch     1 | loss: 5.7778549Losses:  3.309223175048828 1.1134748458862305
CurrentTrain: epoch  4, batch     2 | loss: 4.4226980Losses:  6.071612358093262 8.94069742685133e-08
CurrentTrain: epoch  4, batch     3 | loss: 6.0716124Losses:  3.5604047775268555 1.148108720779419
CurrentTrain: epoch  5, batch     0 | loss: 4.7085133Losses:  3.296682119369507 1.1446363925933838
CurrentTrain: epoch  5, batch     1 | loss: 4.4413185Losses:  4.172027111053467 1.4460750818252563
CurrentTrain: epoch  5, batch     2 | loss: 5.6181021Losses:  1.826225996017456 0.11292581260204315
CurrentTrain: epoch  5, batch     3 | loss: 1.9391518Losses:  3.5452420711517334 1.2592799663543701
CurrentTrain: epoch  6, batch     0 | loss: 4.8045220Losses:  3.361100196838379 1.1117475032806396
CurrentTrain: epoch  6, batch     1 | loss: 4.4728479Losses:  3.3104772567749023 1.145599365234375
CurrentTrain: epoch  6, batch     2 | loss: 4.4560766Losses:  2.9872562885284424 1.1920930376163597e-07
CurrentTrain: epoch  6, batch     3 | loss: 2.9872565Losses:  3.85736083984375 1.0973048210144043
CurrentTrain: epoch  7, batch     0 | loss: 4.9546657Losses:  3.175990104675293 1.2663580179214478
CurrentTrain: epoch  7, batch     1 | loss: 4.4423480Losses:  2.7116613388061523 1.0143237113952637
CurrentTrain: epoch  7, batch     2 | loss: 3.7259851Losses:  3.7317094802856445 0.38011062145233154
CurrentTrain: epoch  7, batch     3 | loss: 4.1118202Losses:  2.915680408477783 1.200622797012329
CurrentTrain: epoch  8, batch     0 | loss: 4.1163034Losses:  3.0558664798736572 1.0725959539413452
CurrentTrain: epoch  8, batch     1 | loss: 4.1284623Losses:  3.5770981311798096 0.8236366510391235
CurrentTrain: epoch  8, batch     2 | loss: 4.4007349Losses:  1.8236544132232666 0.1480773389339447
CurrentTrain: epoch  8, batch     3 | loss: 1.9717318Losses:  3.028657913208008 0.9024068117141724
CurrentTrain: epoch  9, batch     0 | loss: 3.9310646Losses:  3.354816198348999 1.0519745349884033
CurrentTrain: epoch  9, batch     1 | loss: 4.4067907Losses:  2.9136624336242676 0.7218981385231018
CurrentTrain: epoch  9, batch     2 | loss: 3.6355605Losses:  1.8971991539001465 0.11730340868234634
CurrentTrain: epoch  9, batch     3 | loss: 2.0145025
Losses:  5.555479049682617 1.0509634017944336
MemoryTrain:  epoch  0, batch     0 | loss: 6.6064425Losses:  10.521920204162598 0.3458424210548401
MemoryTrain:  epoch  0, batch     1 | loss: 10.8677626Losses:  1.0348126888275146 1.024937629699707
MemoryTrain:  epoch  1, batch     0 | loss: 2.0597503Losses:  0.269072562456131 0.35737496614456177
MemoryTrain:  epoch  1, batch     1 | loss: 0.6264476Losses:  0.816295325756073 1.103163719177246
MemoryTrain:  epoch  2, batch     0 | loss: 1.9194591Losses:  0.2147338092327118 0.13636291027069092
MemoryTrain:  epoch  2, batch     1 | loss: 0.3510967Losses:  0.6482143402099609 0.9769523739814758
MemoryTrain:  epoch  3, batch     0 | loss: 1.6251667Losses:  0.24638964235782623 0.11245331168174744
MemoryTrain:  epoch  3, batch     1 | loss: 0.3588430Losses:  0.39234301447868347 0.8896307945251465
MemoryTrain:  epoch  4, batch     0 | loss: 1.2819738Losses:  0.7153921127319336 0.1353221833705902
MemoryTrain:  epoch  4, batch     1 | loss: 0.8507143Losses:  0.36952540278434753 0.8425407409667969
MemoryTrain:  epoch  5, batch     0 | loss: 1.2120662Losses:  0.567487895488739 0.14165741205215454
MemoryTrain:  epoch  5, batch     1 | loss: 0.7091453Losses:  0.4031819701194763 0.750689685344696
MemoryTrain:  epoch  6, batch     0 | loss: 1.1538717Losses:  0.3575626611709595 0.419066846370697
MemoryTrain:  epoch  6, batch     1 | loss: 0.7766295Losses:  0.3411406874656677 0.9221026301383972
MemoryTrain:  epoch  7, batch     0 | loss: 1.2632433Losses:  0.106047123670578 0.15872272849082947
MemoryTrain:  epoch  7, batch     1 | loss: 0.2647699Losses:  0.20686961710453033 0.8107680678367615
MemoryTrain:  epoch  8, batch     0 | loss: 1.0176377Losses:  0.20675286650657654 0.14443886280059814
MemoryTrain:  epoch  8, batch     1 | loss: 0.3511917Losses:  0.27636879682540894 0.8748476505279541
MemoryTrain:  epoch  9, batch     0 | loss: 1.1512165Losses:  0.1560364067554474 0.04686448723077774
MemoryTrain:  epoch  9, batch     1 | loss: 0.2029009
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 78.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 79.09%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.36%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.80%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 78.95%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 78.45%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 77.29%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 76.95%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 76.01%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.27%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.36%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 94.08%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 93.44%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 93.04%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 91.89%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 91.25%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 90.62%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 90.02%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 89.34%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 88.28%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 88.18%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 88.09%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.08%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 87.99%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.98%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 87.97%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 87.81%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 87.73%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 87.58%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 87.28%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 87.06%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 87.06%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 87.07%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.36%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 87.36%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 88.08%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 88.20%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 88.24%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 88.11%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 87.99%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 87.91%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 87.44%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 87.21%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 87.16%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 86.99%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 86.66%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 86.73%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 86.68%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 86.80%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.86%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 86.71%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 86.25%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 85.90%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 85.66%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 85.16%   [EVAL] batch:  123 | acc: 43.75%,  total acc: 84.83%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 84.50%   
cur_acc:  ['0.9405', '0.7530']
his_acc:  ['0.9405', '0.8450']
Clustering into  14  clusters
Clusters:  [ 2  9 13  2  2  2  0  2 10  1  0  2  4  2 11  6  7  2  2  2  2  2  2  8
  2  1  2  3 12  5]
Losses:  7.516772270202637 1.4380075931549072
CurrentTrain: epoch  0, batch     0 | loss: 8.9547796Losses:  8.386148452758789 1.3730883598327637
CurrentTrain: epoch  0, batch     1 | loss: 9.7592373Losses:  8.54096508026123 1.116741418838501
CurrentTrain: epoch  0, batch     2 | loss: 9.6577063Losses:  5.312700271606445 0.10774555802345276
CurrentTrain: epoch  0, batch     3 | loss: 5.4204459Losses:  3.8864216804504395 0.8583136200904846
CurrentTrain: epoch  1, batch     0 | loss: 4.7447352Losses:  3.782543182373047 1.1331443786621094
CurrentTrain: epoch  1, batch     1 | loss: 4.9156876Losses:  3.0284552574157715 1.0581876039505005
CurrentTrain: epoch  1, batch     2 | loss: 4.0866427Losses:  4.258079528808594 0.24376548826694489
CurrentTrain: epoch  1, batch     3 | loss: 4.5018449Losses:  3.1496729850769043 1.055717945098877
CurrentTrain: epoch  2, batch     0 | loss: 4.2053909Losses:  3.0842385292053223 0.9372156858444214
CurrentTrain: epoch  2, batch     1 | loss: 4.0214543Losses:  3.2216978073120117 0.867478609085083
CurrentTrain: epoch  2, batch     2 | loss: 4.0891762Losses:  2.5787010192871094 0.15769460797309875
CurrentTrain: epoch  2, batch     3 | loss: 2.7363956Losses:  2.9105677604675293 0.8547847270965576
CurrentTrain: epoch  3, batch     0 | loss: 3.7653525Losses:  2.903212070465088 0.9044977426528931
CurrentTrain: epoch  3, batch     1 | loss: 3.8077097Losses:  2.7555806636810303 0.7079419493675232
CurrentTrain: epoch  3, batch     2 | loss: 3.4635227Losses:  2.675753355026245 0.17789879441261292
CurrentTrain: epoch  3, batch     3 | loss: 2.8536522Losses:  2.6125407218933105 0.724174976348877
CurrentTrain: epoch  4, batch     0 | loss: 3.3367157Losses:  2.615513324737549 0.787665605545044
CurrentTrain: epoch  4, batch     1 | loss: 3.4031789Losses:  2.4986231327056885 0.6505204439163208
CurrentTrain: epoch  4, batch     2 | loss: 3.1491437Losses:  1.9783202409744263 0.04876158386468887
CurrentTrain: epoch  4, batch     3 | loss: 2.0270817Losses:  2.2098207473754883 0.7145940065383911
CurrentTrain: epoch  5, batch     0 | loss: 2.9244146Losses:  2.388676166534424 0.6288236379623413
CurrentTrain: epoch  5, batch     1 | loss: 3.0174999Losses:  2.4118049144744873 0.5795137286186218
CurrentTrain: epoch  5, batch     2 | loss: 2.9913187Losses:  1.918287992477417 0.2715321481227875
CurrentTrain: epoch  5, batch     3 | loss: 2.1898201Losses:  2.5515289306640625 0.7307466864585876
CurrentTrain: epoch  6, batch     0 | loss: 3.2822757Losses:  2.142886161804199 0.6312698721885681
CurrentTrain: epoch  6, batch     1 | loss: 2.7741561Losses:  1.932159423828125 0.5583242177963257
CurrentTrain: epoch  6, batch     2 | loss: 2.4904838Losses:  1.7717524766921997 0.1972673535346985
CurrentTrain: epoch  6, batch     3 | loss: 1.9690199Losses:  1.9506402015686035 0.5373038053512573
CurrentTrain: epoch  7, batch     0 | loss: 2.4879441Losses:  2.1542367935180664 0.6356456279754639
CurrentTrain: epoch  7, batch     1 | loss: 2.7898824Losses:  2.273956298828125 0.5955743789672852
CurrentTrain: epoch  7, batch     2 | loss: 2.8695307Losses:  1.9088599681854248 0.06392417103052139
CurrentTrain: epoch  7, batch     3 | loss: 1.9727842Losses:  1.8854864835739136 0.508051335811615
CurrentTrain: epoch  8, batch     0 | loss: 2.3935378Losses:  2.080188274383545 0.5520755052566528
CurrentTrain: epoch  8, batch     1 | loss: 2.6322637Losses:  2.065950870513916 0.655746340751648
CurrentTrain: epoch  8, batch     2 | loss: 2.7216973Losses:  2.151404857635498 0.22840632498264313
CurrentTrain: epoch  8, batch     3 | loss: 2.3798113Losses:  1.8028385639190674 0.38775205612182617
CurrentTrain: epoch  9, batch     0 | loss: 2.1905906Losses:  2.1176719665527344 0.46458977460861206
CurrentTrain: epoch  9, batch     1 | loss: 2.5822618Losses:  1.958235740661621 0.6542530655860901
CurrentTrain: epoch  9, batch     2 | loss: 2.6124887Losses:  2.0525314807891846 0.23548239469528198
CurrentTrain: epoch  9, batch     3 | loss: 2.2880139
Losses:  5.973406791687012 0.8736871480941772
MemoryTrain:  epoch  0, batch     0 | loss: 6.8470941Losses:  10.076547622680664 0.8164727687835693
MemoryTrain:  epoch  0, batch     1 | loss: 10.8930206Losses:  1.2981727123260498 0.7664263844490051
MemoryTrain:  epoch  1, batch     0 | loss: 2.0645990Losses:  1.5699725151062012 0.9600406885147095
MemoryTrain:  epoch  1, batch     1 | loss: 2.5300131Losses:  1.5523796081542969 0.9353674054145813
MemoryTrain:  epoch  2, batch     0 | loss: 2.4877470Losses:  1.011296272277832 0.6487445831298828
MemoryTrain:  epoch  2, batch     1 | loss: 1.6600409Losses:  1.176086187362671 0.7201409935951233
MemoryTrain:  epoch  3, batch     0 | loss: 1.8962271Losses:  1.0071452856063843 0.9561314582824707
MemoryTrain:  epoch  3, batch     1 | loss: 1.9632767Losses:  1.0369250774383545 0.8039079904556274
MemoryTrain:  epoch  4, batch     0 | loss: 1.8408331Losses:  0.6806920170783997 0.6460248827934265
MemoryTrain:  epoch  4, batch     1 | loss: 1.3267169Losses:  0.8071359395980835 0.8537390828132629
MemoryTrain:  epoch  5, batch     0 | loss: 1.6608751Losses:  0.5676926374435425 0.6733928918838501
MemoryTrain:  epoch  5, batch     1 | loss: 1.2410855Losses:  0.7076472043991089 0.7576435804367065
MemoryTrain:  epoch  6, batch     0 | loss: 1.4652908Losses:  0.42856109142303467 0.7587843537330627
MemoryTrain:  epoch  6, batch     1 | loss: 1.1873455Losses:  0.439363956451416 0.7802180647850037
MemoryTrain:  epoch  7, batch     0 | loss: 1.2195821Losses:  0.6528313755989075 0.6999370455741882
MemoryTrain:  epoch  7, batch     1 | loss: 1.3527684Losses:  0.5351992249488831 0.6719889640808105
MemoryTrain:  epoch  8, batch     0 | loss: 1.2071881Losses:  0.395169198513031 0.8062231540679932
MemoryTrain:  epoch  8, batch     1 | loss: 1.2013924Losses:  0.39211952686309814 0.5131289958953857
MemoryTrain:  epoch  9, batch     0 | loss: 0.9052485Losses:  0.5119456648826599 0.8412728905677795
MemoryTrain:  epoch  9, batch     1 | loss: 1.3532186
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 73.52%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 66.15%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.73%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 91.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.48%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.10%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.26%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.53%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.09%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.37%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 91.98%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 91.60%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 90.97%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 90.14%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 89.52%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 88.73%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 88.25%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 87.59%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 87.23%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 87.14%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 86.46%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 85.87%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 85.63%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.58%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.60%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 85.31%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 85.21%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 84.79%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 84.23%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 83.82%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 83.36%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 83.05%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 83.03%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 83.45%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 83.91%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 84.08%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 84.65%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 84.13%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 84.20%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 84.05%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 83.45%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 83.08%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 82.78%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 82.60%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 82.25%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 82.19%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 82.46%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 81.82%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 81.20%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 80.58%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 80.03%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 79.44%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 78.95%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 78.87%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 78.79%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 78.71%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 78.78%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 78.96%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 79.29%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 79.27%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 79.48%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 79.54%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 79.42%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 79.48%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 79.40%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 79.33%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 79.35%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 79.37%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 79.21%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 79.10%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 79.03%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 78.88%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 78.94%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 78.63%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 78.37%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 78.26%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 78.16%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 78.07%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 77.89%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 77.80%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 77.82%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 77.52%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 77.39%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 77.18%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 77.02%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 76.66%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 76.36%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 76.06%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 75.73%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 75.40%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 75.11%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 74.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  180 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 75.76%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 76.20%   
cur_acc:  ['0.9405', '0.7530', '0.7073']
his_acc:  ['0.9405', '0.8450', '0.7620']
Clustering into  19  clusters
Clusters:  [ 2  9 13  2  2  2 17  2 18  0 14  2 11  2 12 16 15  2  2  2  2  2  2 10
  2  0  2  7  5  8  2  3  2  2  4  2  2  2  1  6]
Losses:  7.422013282775879 1.638427495956421
CurrentTrain: epoch  0, batch     0 | loss: 9.0604410Losses:  8.775192260742188 1.5579771995544434
CurrentTrain: epoch  0, batch     1 | loss: 10.3331699Losses:  7.7757978439331055 1.465460181236267
CurrentTrain: epoch  0, batch     2 | loss: 9.2412577Losses:  7.733613967895508 0.5989906787872314
CurrentTrain: epoch  0, batch     3 | loss: 8.3326044Losses:  4.323768138885498 1.6508352756500244
CurrentTrain: epoch  1, batch     0 | loss: 5.9746037Losses:  4.367063045501709 1.4981870651245117
CurrentTrain: epoch  1, batch     1 | loss: 5.8652501Losses:  3.2333168983459473 1.3606157302856445
CurrentTrain: epoch  1, batch     2 | loss: 4.5939326Losses:  5.172101974487305 0.3047720789909363
CurrentTrain: epoch  1, batch     3 | loss: 5.4768739Losses:  3.9552712440490723 1.349675178527832
CurrentTrain: epoch  2, batch     0 | loss: 5.3049464Losses:  3.6686501502990723 1.1728644371032715
CurrentTrain: epoch  2, batch     1 | loss: 4.8415146Losses:  3.1143949031829834 1.1215723752975464
CurrentTrain: epoch  2, batch     2 | loss: 4.2359672Losses:  3.7906851768493652 0.5598763227462769
CurrentTrain: epoch  2, batch     3 | loss: 4.3505616Losses:  3.205380439758301 1.3176815509796143
CurrentTrain: epoch  3, batch     0 | loss: 4.5230618Losses:  3.275454044342041 1.2178566455841064
CurrentTrain: epoch  3, batch     1 | loss: 4.4933109Losses:  3.6738340854644775 1.2801036834716797
CurrentTrain: epoch  3, batch     2 | loss: 4.9539375Losses:  3.889714479446411 0.4788806736469269
CurrentTrain: epoch  3, batch     3 | loss: 4.3685951Losses:  2.918639659881592 1.0685673952102661
CurrentTrain: epoch  4, batch     0 | loss: 3.9872069Losses:  3.6045947074890137 1.1901087760925293
CurrentTrain: epoch  4, batch     1 | loss: 4.7947035Losses:  2.986105442047119 1.0549635887145996
CurrentTrain: epoch  4, batch     2 | loss: 4.0410690Losses:  4.542069435119629 0.5003604888916016
CurrentTrain: epoch  4, batch     3 | loss: 5.0424299Losses:  2.9191527366638184 1.024597406387329
CurrentTrain: epoch  5, batch     0 | loss: 3.9437501Losses:  3.0373916625976562 0.9761434197425842
CurrentTrain: epoch  5, batch     1 | loss: 4.0135350Losses:  3.4160237312316895 1.3065403699874878
CurrentTrain: epoch  5, batch     2 | loss: 4.7225642Losses:  2.4743378162384033 0.04253581538796425
CurrentTrain: epoch  5, batch     3 | loss: 2.5168736Losses:  3.6720523834228516 1.0794827938079834
CurrentTrain: epoch  6, batch     0 | loss: 4.7515354Losses:  2.521152973175049 0.8102092146873474
CurrentTrain: epoch  6, batch     1 | loss: 3.3313622Losses:  2.2859573364257812 0.8647168874740601
CurrentTrain: epoch  6, batch     2 | loss: 3.1506743Losses:  3.983276844024658 0.65537029504776
CurrentTrain: epoch  6, batch     3 | loss: 4.6386471Losses:  2.950815439224243 0.9760184288024902
CurrentTrain: epoch  7, batch     0 | loss: 3.9268339Losses:  2.9086174964904785 0.885253369808197
CurrentTrain: epoch  7, batch     1 | loss: 3.7938709Losses:  2.3145580291748047 0.8867056965827942
CurrentTrain: epoch  7, batch     2 | loss: 3.2012637Losses:  2.0785202980041504 0.10683664679527283
CurrentTrain: epoch  7, batch     3 | loss: 2.1853569Losses:  2.5011978149414062 0.7125147581100464
CurrentTrain: epoch  8, batch     0 | loss: 3.2137127Losses:  2.844399929046631 0.9735867977142334
CurrentTrain: epoch  8, batch     1 | loss: 3.8179867Losses:  2.270442485809326 0.8498521447181702
CurrentTrain: epoch  8, batch     2 | loss: 3.1202946Losses:  1.9229300022125244 8.94069742685133e-08
CurrentTrain: epoch  8, batch     3 | loss: 1.9229301Losses:  1.9150279760360718 0.5080516934394836
CurrentTrain: epoch  9, batch     0 | loss: 2.4230797Losses:  2.5501322746276855 0.8105674982070923
CurrentTrain: epoch  9, batch     1 | loss: 3.3606997Losses:  2.8473849296569824 0.8789805173873901
CurrentTrain: epoch  9, batch     2 | loss: 3.7263656Losses:  1.8138577938079834 0.03373679146170616
CurrentTrain: epoch  9, batch     3 | loss: 1.8475946
Losses:  5.784677505493164 0.6478445529937744
MemoryTrain:  epoch  0, batch     0 | loss: 6.4325218Losses:  9.631908416748047 1.2171108722686768
MemoryTrain:  epoch  0, batch     1 | loss: 10.8490191Losses:  10.545132637023926 0.5076410174369812
MemoryTrain:  epoch  0, batch     2 | loss: 11.0527735Losses:  1.27559494972229 1.0600924491882324
MemoryTrain:  epoch  1, batch     0 | loss: 2.3356874Losses:  0.6702958345413208 0.6817646026611328
MemoryTrain:  epoch  1, batch     1 | loss: 1.3520604Losses:  0.9235318303108215 0.7190049290657043
MemoryTrain:  epoch  1, batch     2 | loss: 1.6425368Losses:  0.5298916101455688 0.7642337679862976
MemoryTrain:  epoch  2, batch     0 | loss: 1.2941253Losses:  0.7499470710754395 0.7553622126579285
MemoryTrain:  epoch  2, batch     1 | loss: 1.5053093Losses:  0.8486666679382324 1.0105011463165283
MemoryTrain:  epoch  2, batch     2 | loss: 1.8591678Losses:  0.6681839227676392 0.8444368839263916
MemoryTrain:  epoch  3, batch     0 | loss: 1.5126208Losses:  0.4772969186306 0.8722833395004272
MemoryTrain:  epoch  3, batch     1 | loss: 1.3495803Losses:  0.6178725361824036 0.49004027247428894
MemoryTrain:  epoch  3, batch     2 | loss: 1.1079128Losses:  0.5115881562232971 0.7677250504493713
MemoryTrain:  epoch  4, batch     0 | loss: 1.2793132Losses:  0.5585960149765015 0.9028719663619995
MemoryTrain:  epoch  4, batch     1 | loss: 1.4614680Losses:  0.47801637649536133 0.5458285212516785
MemoryTrain:  epoch  4, batch     2 | loss: 1.0238450Losses:  0.5717678070068359 1.0676259994506836
MemoryTrain:  epoch  5, batch     0 | loss: 1.6393938Losses:  0.36254772543907166 0.672627329826355
MemoryTrain:  epoch  5, batch     1 | loss: 1.0351751Losses:  0.34063804149627686 0.5189211964607239
MemoryTrain:  epoch  5, batch     2 | loss: 0.8595592Losses:  0.34590011835098267 1.0049103498458862
MemoryTrain:  epoch  6, batch     0 | loss: 1.3508105Losses:  0.4224647879600525 0.853653073310852
MemoryTrain:  epoch  6, batch     1 | loss: 1.2761178Losses:  0.267500638961792 0.2890254259109497
MemoryTrain:  epoch  6, batch     2 | loss: 0.5565261Losses:  0.34484732151031494 0.7887393236160278
MemoryTrain:  epoch  7, batch     0 | loss: 1.1335866Losses:  0.3795740306377411 0.8781679272651672
MemoryTrain:  epoch  7, batch     1 | loss: 1.2577419Losses:  0.3020314574241638 0.3859054148197174
MemoryTrain:  epoch  7, batch     2 | loss: 0.6879369Losses:  0.3407544791698456 0.7013582587242126
MemoryTrain:  epoch  8, batch     0 | loss: 1.0421127Losses:  0.34420305490493774 0.9558795094490051
MemoryTrain:  epoch  8, batch     1 | loss: 1.3000826Losses:  0.29022276401519775 0.3729708790779114
MemoryTrain:  epoch  8, batch     2 | loss: 0.6631936Losses:  0.3088473677635193 0.8743200898170471
MemoryTrain:  epoch  9, batch     0 | loss: 1.1831675Losses:  0.3316323161125183 0.8268327713012695
MemoryTrain:  epoch  9, batch     1 | loss: 1.1584651Losses:  0.2646966278553009 0.3031800389289856
MemoryTrain:  epoch  9, batch     2 | loss: 0.5678767
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 19.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 44.53%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 41.91%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 40.62%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 40.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 43.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 46.13%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 48.30%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 50.54%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 54.00%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 55.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 59.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 66.62%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 66.37%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 68.42%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 68.64%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.95%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.04%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.34%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.09%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.16%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.19%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 91.81%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 90.06%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 89.82%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 89.29%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 88.38%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 87.60%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 86.74%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 85.82%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 85.02%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 84.42%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.29%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 84.07%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 83.68%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 83.30%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.01%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 82.73%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 82.77%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 82.38%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 81.69%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 81.54%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.87%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.63%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 83.29%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 83.15%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 82.89%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.92%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 82.90%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 82.65%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.06%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 81.59%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 81.08%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 80.74%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 80.30%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.14%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.51%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 79.90%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 79.29%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 78.69%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 78.10%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.52%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 77.00%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 76.57%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 76.32%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 76.21%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 76.11%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 76.05%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.14%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 76.48%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 76.61%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 76.66%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 76.52%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 76.55%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 76.50%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 76.28%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 76.06%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 76.01%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 75.92%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 75.82%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 75.77%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 75.48%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 75.24%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 75.12%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 74.92%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 74.72%   [EVAL] batch:  159 | acc: 37.50%,  total acc: 74.49%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 74.11%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 73.89%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 73.74%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.67%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 73.53%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 73.39%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 73.29%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 73.19%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 72.90%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 72.55%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.24%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 71.93%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 71.62%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 71.39%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.03%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 72.84%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 72.52%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 72.20%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 71.86%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 71.52%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 71.28%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 71.01%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  200 | acc: 0.00%,  total acc: 71.46%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 71.13%   [EVAL] batch:  202 | acc: 12.50%,  total acc: 70.84%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 70.56%   [EVAL] batch:  204 | acc: 6.25%,  total acc: 70.24%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 70.02%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.85%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.95%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.04%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 72.10%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 71.94%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 71.90%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 71.83%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 71.63%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:  242 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 71.93%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 71.95%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 72.10%   
cur_acc:  ['0.9405', '0.7530', '0.7073', '0.6895']
his_acc:  ['0.9405', '0.8450', '0.7620', '0.7210']
Clustering into  24  clusters
Clusters:  [ 3 23 16  4  3  3 22  3 18  1 17  3 14  3 15 21  8  3  3  3  3  3  3 12
  3  1  3 19 13  0  3 20  3  3  0  3  3  3 11  9  4 10  3  3  5  6  2  3
  3  7]
Losses:  6.711575508117676 1.2763303518295288
CurrentTrain: epoch  0, batch     0 | loss: 7.9879060Losses:  8.087746620178223 1.8716003894805908
CurrentTrain: epoch  0, batch     1 | loss: 9.9593468Losses:  6.739516258239746 1.4662468433380127
CurrentTrain: epoch  0, batch     2 | loss: 8.2057629Losses:  6.249427795410156 0.9883372783660889
CurrentTrain: epoch  0, batch     3 | loss: 7.2377653Losses:  2.926182508468628 1.330803394317627
CurrentTrain: epoch  1, batch     0 | loss: 4.2569857Losses:  2.7674267292022705 1.476928472518921
CurrentTrain: epoch  1, batch     1 | loss: 4.2443552Losses:  3.2032337188720703 1.1327170133590698
CurrentTrain: epoch  1, batch     2 | loss: 4.3359509Losses:  1.9155123233795166 0.4629344344139099
CurrentTrain: epoch  1, batch     3 | loss: 2.3784468Losses:  2.668095111846924 0.9758043885231018
CurrentTrain: epoch  2, batch     0 | loss: 3.6438994Losses:  2.6408145427703857 1.0923378467559814
CurrentTrain: epoch  2, batch     1 | loss: 3.7331524Losses:  2.856653928756714 1.0600669384002686
CurrentTrain: epoch  2, batch     2 | loss: 3.9167209Losses:  1.9809024333953857 0.03878225386142731
CurrentTrain: epoch  2, batch     3 | loss: 2.0196848Losses:  2.984445095062256 1.2262842655181885
CurrentTrain: epoch  3, batch     0 | loss: 4.2107296Losses:  2.3251993656158447 1.0857607126235962
CurrentTrain: epoch  3, batch     1 | loss: 3.4109602Losses:  2.182750940322876 1.1035783290863037
CurrentTrain: epoch  3, batch     2 | loss: 3.2863293Losses:  1.835705280303955 0.0494009405374527
CurrentTrain: epoch  3, batch     3 | loss: 1.8851062Losses:  2.000525712966919 0.6484559774398804
CurrentTrain: epoch  4, batch     0 | loss: 2.6489816Losses:  2.2308497428894043 0.9047765135765076
CurrentTrain: epoch  4, batch     1 | loss: 3.1356263Losses:  2.4140524864196777 1.0523924827575684
CurrentTrain: epoch  4, batch     2 | loss: 3.4664450Losses:  2.87929105758667 5.960465188081798e-08
CurrentTrain: epoch  4, batch     3 | loss: 2.8792911Losses:  2.1414666175842285 1.010066032409668
CurrentTrain: epoch  5, batch     0 | loss: 3.1515326Losses:  2.34647798538208 0.7305665016174316
CurrentTrain: epoch  5, batch     1 | loss: 3.0770445Losses:  1.9921050071716309 0.7435985803604126
CurrentTrain: epoch  5, batch     2 | loss: 2.7357035Losses:  1.9938101768493652 0.38533294200897217
CurrentTrain: epoch  5, batch     3 | loss: 2.3791432Losses:  2.346318483352661 0.7512028813362122
CurrentTrain: epoch  6, batch     0 | loss: 3.0975213Losses:  1.9864387512207031 0.8274118900299072
CurrentTrain: epoch  6, batch     1 | loss: 2.8138506Losses:  1.9369821548461914 0.613922655582428
CurrentTrain: epoch  6, batch     2 | loss: 2.5509048Losses:  1.81257963180542 0.23297014832496643
CurrentTrain: epoch  6, batch     3 | loss: 2.0455499Losses:  1.915010929107666 0.7627362608909607
CurrentTrain: epoch  7, batch     0 | loss: 2.6777472Losses:  2.0564651489257812 0.9379040002822876
CurrentTrain: epoch  7, batch     1 | loss: 2.9943690Losses:  1.9864474534988403 0.6135616898536682
CurrentTrain: epoch  7, batch     2 | loss: 2.6000092Losses:  2.4183077812194824 0.05825220048427582
CurrentTrain: epoch  7, batch     3 | loss: 2.4765599Losses:  2.0574944019317627 0.8336595892906189
CurrentTrain: epoch  8, batch     0 | loss: 2.8911541Losses:  1.8913118839263916 0.5287835597991943
CurrentTrain: epoch  8, batch     1 | loss: 2.4200954Losses:  1.824308156967163 0.7060218453407288
CurrentTrain: epoch  8, batch     2 | loss: 2.5303299Losses:  1.7478225231170654 0.09874516725540161
CurrentTrain: epoch  8, batch     3 | loss: 1.8465676Losses:  1.8914451599121094 0.6042206883430481
CurrentTrain: epoch  9, batch     0 | loss: 2.4956658Losses:  2.033876895904541 0.73995041847229
CurrentTrain: epoch  9, batch     1 | loss: 2.7738273Losses:  1.823355793952942 0.6415483355522156
CurrentTrain: epoch  9, batch     2 | loss: 2.4649041Losses:  1.74143385887146 0.026720233261585236
CurrentTrain: epoch  9, batch     3 | loss: 1.7681541
Losses:  5.932248115539551 0.8522568345069885
MemoryTrain:  epoch  0, batch     0 | loss: 6.7845049Losses:  9.246305465698242 0.8147976398468018
MemoryTrain:  epoch  0, batch     1 | loss: 10.0611029Losses:  10.384880065917969 0.9896107316017151
MemoryTrain:  epoch  0, batch     2 | loss: 11.3744907Losses:  10.237133026123047 0.017544139176607132
MemoryTrain:  epoch  0, batch     3 | loss: 10.2546768Losses:  0.7964074611663818 0.6699094176292419
MemoryTrain:  epoch  1, batch     0 | loss: 1.4663169Losses:  1.0915982723236084 0.780686616897583
MemoryTrain:  epoch  1, batch     1 | loss: 1.8722849Losses:  0.7799468040466309 0.8975564241409302
MemoryTrain:  epoch  1, batch     2 | loss: 1.6775032Losses:  0.566552996635437 0.33645185828208923
MemoryTrain:  epoch  1, batch     3 | loss: 0.9030049Losses:  0.7057333588600159 0.7634168267250061
MemoryTrain:  epoch  2, batch     0 | loss: 1.4691502Losses:  0.8695875406265259 0.7870192527770996
MemoryTrain:  epoch  2, batch     1 | loss: 1.6566068Losses:  0.5634586811065674 0.8399177193641663
MemoryTrain:  epoch  2, batch     2 | loss: 1.4033763Losses:  0.5714453458786011 0.052239712327718735
MemoryTrain:  epoch  2, batch     3 | loss: 0.6236851Losses:  0.5978806018829346 0.7179651260375977
MemoryTrain:  epoch  3, batch     0 | loss: 1.3158457Losses:  0.5588489770889282 0.6851102113723755
MemoryTrain:  epoch  3, batch     1 | loss: 1.2439592Losses:  0.5963455438613892 0.7701812386512756
MemoryTrain:  epoch  3, batch     2 | loss: 1.3665268Losses:  1.4798088073730469 0.3410867154598236
MemoryTrain:  epoch  3, batch     3 | loss: 1.8208956Losses:  0.3858380913734436 0.7013218402862549
MemoryTrain:  epoch  4, batch     0 | loss: 1.0871599Losses:  0.4225252866744995 0.8691941499710083
MemoryTrain:  epoch  4, batch     1 | loss: 1.2917194Losses:  0.7298657298088074 0.7995004653930664
MemoryTrain:  epoch  4, batch     2 | loss: 1.5293663Losses:  0.284431129693985 0.019111275672912598
MemoryTrain:  epoch  4, batch     3 | loss: 0.3035424Losses:  0.418231338262558 0.853984534740448
MemoryTrain:  epoch  5, batch     0 | loss: 1.2722158Losses:  0.5148541927337646 0.7704306840896606
MemoryTrain:  epoch  5, batch     1 | loss: 1.2852849Losses:  0.3805691599845886 0.6038238406181335
MemoryTrain:  epoch  5, batch     2 | loss: 0.9843930Losses:  0.6828595399856567 0.01905602216720581
MemoryTrain:  epoch  5, batch     3 | loss: 0.7019156Losses:  0.42808228731155396 0.6028504371643066
MemoryTrain:  epoch  6, batch     0 | loss: 1.0309327Losses:  0.34275367856025696 0.8028229475021362
MemoryTrain:  epoch  6, batch     1 | loss: 1.1455766Losses:  0.4768933057785034 0.8814351558685303
MemoryTrain:  epoch  6, batch     2 | loss: 1.3583285Losses:  0.31570470333099365 0.0505334809422493
MemoryTrain:  epoch  6, batch     3 | loss: 0.3662382Losses:  0.40463435649871826 1.01972234249115
MemoryTrain:  epoch  7, batch     0 | loss: 1.4243567Losses:  0.4871842563152313 0.7287287712097168
MemoryTrain:  epoch  7, batch     1 | loss: 1.2159131Losses:  0.31933021545410156 0.4372401833534241
MemoryTrain:  epoch  7, batch     2 | loss: 0.7565704Losses:  0.295454740524292 0.0581088662147522
MemoryTrain:  epoch  7, batch     3 | loss: 0.3535636Losses:  0.3362780213356018 0.6808886528015137
MemoryTrain:  epoch  8, batch     0 | loss: 1.0171666Losses:  0.373430997133255 0.5883261561393738
MemoryTrain:  epoch  8, batch     1 | loss: 0.9617572Losses:  0.3914486765861511 0.7172133922576904
MemoryTrain:  epoch  8, batch     2 | loss: 1.1086621Losses:  0.5159300565719604 0.2555990219116211
MemoryTrain:  epoch  8, batch     3 | loss: 0.7715291Losses:  0.2929440140724182 0.6023350954055786
MemoryTrain:  epoch  9, batch     0 | loss: 0.8952791Losses:  0.39708322286605835 0.9174424409866333
MemoryTrain:  epoch  9, batch     1 | loss: 1.3145256Losses:  0.31147706508636475 0.6579232215881348
MemoryTrain:  epoch  9, batch     2 | loss: 0.9694003Losses:  0.22268988192081451 0.025857359170913696
MemoryTrain:  epoch  9, batch     3 | loss: 0.2485472
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 0.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 67.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.54%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 80.60%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.75%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 88.79%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 90.43%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.56%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.86%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 90.02%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 89.33%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.70%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 87.40%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 86.72%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 86.08%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 85.35%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 84.65%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 84.33%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.20%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 83.98%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 83.22%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 83.02%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 83.14%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 83.01%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 83.05%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 83.31%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 82.91%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 82.29%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 81.84%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 81.47%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.32%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.70%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.05%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 83.37%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 83.29%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 83.37%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 83.06%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 82.52%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 82.11%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 81.53%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 81.31%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 80.69%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.53%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.76%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 80.93%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 80.26%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 79.65%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 79.10%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 78.51%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 77.92%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 77.35%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 77.13%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 76.92%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 76.61%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 76.50%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 76.35%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 76.19%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 76.28%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.71%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 76.98%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 76.84%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 76.87%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 76.55%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 76.46%   [EVAL] batch:  146 | acc: 18.75%,  total acc: 76.06%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 75.97%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 75.76%   [EVAL] batch:  149 | acc: 31.25%,  total acc: 75.46%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 75.25%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 75.21%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 75.08%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 74.96%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 74.64%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 74.36%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 74.24%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 74.13%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 73.98%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 73.79%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 73.68%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 73.46%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 73.24%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 73.11%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 73.00%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 72.94%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 72.88%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 72.78%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 72.15%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 71.77%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 71.42%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 71.12%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 70.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 72.06%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 72.41%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 72.06%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 71.74%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 71.43%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 71.09%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 70.76%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 70.49%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 71.17%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  200 | acc: 18.75%,  total acc: 71.05%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 70.73%   [EVAL] batch:  202 | acc: 6.25%,  total acc: 70.41%   [EVAL] batch:  203 | acc: 12.50%,  total acc: 70.13%   [EVAL] batch:  204 | acc: 12.50%,  total acc: 69.85%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 69.63%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  219 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 71.41%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 71.64%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 71.63%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 71.56%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 71.36%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 71.90%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 71.84%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 71.67%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 71.68%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 71.65%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 71.53%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  256 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 72.57%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 72.63%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 72.51%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 72.40%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 72.29%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 72.24%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 72.06%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 71.93%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 71.71%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 71.45%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 71.30%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 71.14%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.45%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 71.89%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 71.90%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 72.01%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 73.05%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 73.05%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 73.11%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 73.14%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 73.16%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 73.19%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 73.23%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 73.50%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 73.38%   
cur_acc:  ['0.9405', '0.7530', '0.7073', '0.6895', '0.8075']
his_acc:  ['0.9405', '0.8450', '0.7620', '0.7210', '0.7338']
Clustering into  29  clusters
Clusters:  [ 0 27 18  0  0  0 23  0 21 28 14  0 15  0 16 20 19  0  0  0  0  0  0 25
  0  9  0 24  7 26  0 22  0 13 17  0  0  0 10  4  8 11  0  0 12  6  5  0
  0  2  3  0  0  0  0  1  0  0  0  0]
Losses:  7.143024444580078 1.6560413837432861
CurrentTrain: epoch  0, batch     0 | loss: 8.7990656Losses:  8.913139343261719 1.9324300289154053
CurrentTrain: epoch  0, batch     1 | loss: 10.8455696Losses:  7.305240631103516 1.6923390626907349
CurrentTrain: epoch  0, batch     2 | loss: 8.9975796Losses:  5.841155052185059 0.3907291889190674
CurrentTrain: epoch  0, batch     3 | loss: 6.2318840Losses:  4.335381507873535 1.7062216997146606
CurrentTrain: epoch  1, batch     0 | loss: 6.0416031Losses:  3.918121337890625 1.5026085376739502
CurrentTrain: epoch  1, batch     1 | loss: 5.4207296Losses:  3.710416316986084 1.356147050857544
CurrentTrain: epoch  1, batch     2 | loss: 5.0665636Losses:  3.640322685241699 0.5957311391830444
CurrentTrain: epoch  1, batch     3 | loss: 4.2360539Losses:  3.5527567863464355 1.6318321228027344
CurrentTrain: epoch  2, batch     0 | loss: 5.1845889Losses:  3.3548741340637207 1.344573736190796
CurrentTrain: epoch  2, batch     1 | loss: 4.6994476Losses:  3.461775541305542 1.5955699682235718
CurrentTrain: epoch  2, batch     2 | loss: 5.0573454Losses:  4.78257942199707 0.6691226959228516
CurrentTrain: epoch  2, batch     3 | loss: 5.4517021Losses:  3.3258752822875977 1.479060173034668
CurrentTrain: epoch  3, batch     0 | loss: 4.8049355Losses:  2.913632869720459 1.32573664188385
CurrentTrain: epoch  3, batch     1 | loss: 4.2393694Losses:  3.683734178543091 1.3131405115127563
CurrentTrain: epoch  3, batch     2 | loss: 4.9968748Losses:  3.7456235885620117 0.34155014157295227
CurrentTrain: epoch  3, batch     3 | loss: 4.0871739Losses:  3.0439951419830322 1.3539965152740479
CurrentTrain: epoch  4, batch     0 | loss: 4.3979917Losses:  3.3183422088623047 1.2696970701217651
CurrentTrain: epoch  4, batch     1 | loss: 4.5880394Losses:  3.030771255493164 1.2051615715026855
CurrentTrain: epoch  4, batch     2 | loss: 4.2359328Losses:  2.353055953979492 0.2132362723350525
CurrentTrain: epoch  4, batch     3 | loss: 2.5662923Losses:  2.8256711959838867 1.2297627925872803
CurrentTrain: epoch  5, batch     0 | loss: 4.0554342Losses:  2.7746543884277344 1.0770578384399414
CurrentTrain: epoch  5, batch     1 | loss: 3.8517122Losses:  2.8869516849517822 1.2238857746124268
CurrentTrain: epoch  5, batch     2 | loss: 4.1108375Losses:  4.323716163635254 0.42735496163368225
CurrentTrain: epoch  5, batch     3 | loss: 4.7510710Losses:  3.075840950012207 1.229912281036377
CurrentTrain: epoch  6, batch     0 | loss: 4.3057532Losses:  2.550260543823242 1.1747246980667114
CurrentTrain: epoch  6, batch     1 | loss: 3.7249851Losses:  2.5702965259552 0.9989352822303772
CurrentTrain: epoch  6, batch     2 | loss: 3.5692317Losses:  3.0104188919067383 0.5044264793395996
CurrentTrain: epoch  6, batch     3 | loss: 3.5148454Losses:  2.536625862121582 0.8641825914382935
CurrentTrain: epoch  7, batch     0 | loss: 3.4008083Losses:  2.309410572052002 0.8994022607803345
CurrentTrain: epoch  7, batch     1 | loss: 3.2088127Losses:  2.826742172241211 1.1591413021087646
CurrentTrain: epoch  7, batch     2 | loss: 3.9858835Losses:  2.5761444568634033 0.19040416181087494
CurrentTrain: epoch  7, batch     3 | loss: 2.7665486Losses:  2.94777512550354 1.128835678100586
CurrentTrain: epoch  8, batch     0 | loss: 4.0766106Losses:  2.3230667114257812 0.8730264902114868
CurrentTrain: epoch  8, batch     1 | loss: 3.1960931Losses:  2.169205665588379 0.8864121437072754
CurrentTrain: epoch  8, batch     2 | loss: 3.0556178Losses:  2.209601879119873 0.2934585511684418
CurrentTrain: epoch  8, batch     3 | loss: 2.5030603Losses:  2.5965957641601562 0.9069279432296753
CurrentTrain: epoch  9, batch     0 | loss: 3.5035238Losses:  2.494720935821533 1.0493147373199463
CurrentTrain: epoch  9, batch     1 | loss: 3.5440357Losses:  2.052098035812378 0.8737704753875732
CurrentTrain: epoch  9, batch     2 | loss: 2.9258685Losses:  2.6751532554626465 0.4851737916469574
CurrentTrain: epoch  9, batch     3 | loss: 3.1603270
Losses:  5.815308570861816 0.9498196840286255
MemoryTrain:  epoch  0, batch     0 | loss: 6.7651281Losses:  8.87662124633789 0.814475417137146
MemoryTrain:  epoch  0, batch     1 | loss: 9.6910963Losses:  10.210092544555664 0.8733046650886536
MemoryTrain:  epoch  0, batch     2 | loss: 11.0833969Losses:  10.632326126098633 0.7330695986747742
MemoryTrain:  epoch  0, batch     3 | loss: 11.3653955Losses:  0.7403463125228882 0.7244296073913574
MemoryTrain:  epoch  1, batch     0 | loss: 1.4647759Losses:  1.0688074827194214 0.9755831956863403
MemoryTrain:  epoch  1, batch     1 | loss: 2.0443907Losses:  0.7141069173812866 0.6906706094741821
MemoryTrain:  epoch  1, batch     2 | loss: 1.4047775Losses:  0.675416886806488 0.8089451789855957
MemoryTrain:  epoch  1, batch     3 | loss: 1.4843621Losses:  0.7910575866699219 0.8301774263381958
MemoryTrain:  epoch  2, batch     0 | loss: 1.6212350Losses:  0.513125479221344 0.8246030807495117
MemoryTrain:  epoch  2, batch     1 | loss: 1.3377285Losses:  0.7847625613212585 0.9323142766952515
MemoryTrain:  epoch  2, batch     2 | loss: 1.7170768Losses:  0.5434330701828003 0.4796019196510315
MemoryTrain:  epoch  2, batch     3 | loss: 1.0230350Losses:  0.6763854622840881 0.713847815990448
MemoryTrain:  epoch  3, batch     0 | loss: 1.3902333Losses:  0.6053054928779602 0.8069621324539185
MemoryTrain:  epoch  3, batch     1 | loss: 1.4122677Losses:  0.5187875628471375 0.7096525430679321
MemoryTrain:  epoch  3, batch     2 | loss: 1.2284400Losses:  0.5930173397064209 0.8501702547073364
MemoryTrain:  epoch  3, batch     3 | loss: 1.4431876Losses:  0.6942984461784363 1.0881669521331787
MemoryTrain:  epoch  4, batch     0 | loss: 1.7824655Losses:  0.37371399998664856 0.6278011202812195
MemoryTrain:  epoch  4, batch     1 | loss: 1.0015152Losses:  0.39822715520858765 0.6839185953140259
MemoryTrain:  epoch  4, batch     2 | loss: 1.0821457Losses:  0.42778071761131287 0.6179580092430115
MemoryTrain:  epoch  4, batch     3 | loss: 1.0457387Losses:  0.5221049785614014 0.7922689914703369
MemoryTrain:  epoch  5, batch     0 | loss: 1.3143740Losses:  0.4653761386871338 0.9736328125
MemoryTrain:  epoch  5, batch     1 | loss: 1.4390090Losses:  0.4269011616706848 0.6819517016410828
MemoryTrain:  epoch  5, batch     2 | loss: 1.1088529Losses:  0.4508321285247803 0.5314713716506958
MemoryTrain:  epoch  5, batch     3 | loss: 0.9823035Losses:  0.40430521965026855 0.6879924535751343
MemoryTrain:  epoch  6, batch     0 | loss: 1.0922977Losses:  0.44226083159446716 0.8843435049057007
MemoryTrain:  epoch  6, batch     1 | loss: 1.3266044Losses:  0.41614484786987305 0.7326163053512573
MemoryTrain:  epoch  6, batch     2 | loss: 1.1487612Losses:  0.3372599482536316 0.5511312484741211
MemoryTrain:  epoch  6, batch     3 | loss: 0.8883912Losses:  0.39724045991897583 0.7305591702461243
MemoryTrain:  epoch  7, batch     0 | loss: 1.1277996Losses:  0.3420115113258362 0.5877984166145325
MemoryTrain:  epoch  7, batch     1 | loss: 0.9298099Losses:  0.47070008516311646 0.8583052158355713
MemoryTrain:  epoch  7, batch     2 | loss: 1.3290052Losses:  0.43909889459609985 0.6465365886688232
MemoryTrain:  epoch  7, batch     3 | loss: 1.0856354Losses:  0.4229774475097656 0.771398663520813
MemoryTrain:  epoch  8, batch     0 | loss: 1.1943761Losses:  0.3793291747570038 0.6428397297859192
MemoryTrain:  epoch  8, batch     1 | loss: 1.0221689Losses:  0.3649040758609772 0.5154349207878113
MemoryTrain:  epoch  8, batch     2 | loss: 0.8803390Losses:  0.45979413390159607 0.6514646410942078
MemoryTrain:  epoch  8, batch     3 | loss: 1.1112587Losses:  0.3657853603363037 0.6294692754745483
MemoryTrain:  epoch  9, batch     0 | loss: 0.9952546Losses:  0.334449827671051 0.6523346900939941
MemoryTrain:  epoch  9, batch     1 | loss: 0.9867845Losses:  0.4632590413093567 0.8370524644851685
MemoryTrain:  epoch  9, batch     2 | loss: 1.3003116Losses:  0.36475953459739685 0.4938341975212097
MemoryTrain:  epoch  9, batch     3 | loss: 0.8585937
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 44.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 0.00%,  total acc: 57.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 55.05%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 53.24%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 51.34%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 49.57%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 48.54%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 46.98%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 47.46%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 48.67%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 49.45%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 50.71%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 51.39%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 52.36%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 53.29%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 54.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.31%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 56.99%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 57.99%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 58.95%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 59.31%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 59.51%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 59.57%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 59.57%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 59.75%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 59.56%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 60.10%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 60.38%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 60.88%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 60.80%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 61.16%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 60.96%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 60.56%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 60.38%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 60.21%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 60.18%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.62%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.14%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 89.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.30%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.39%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 88.93%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 88.15%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 87.71%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 87.09%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.79%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 85.84%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 85.67%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 85.32%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 84.70%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 84.10%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 83.79%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 83.63%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 83.16%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 82.62%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 82.26%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 82.33%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 82.07%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 81.49%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 80.93%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 80.70%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 80.08%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 79.63%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 79.12%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 78.31%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 77.60%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 76.91%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 76.31%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 75.65%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 75.50%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 78.43%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 78.34%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 78.48%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 78.15%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 77.55%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 77.06%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.48%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 75.73%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 76.21%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 75.57%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 74.95%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 74.44%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 73.88%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 73.34%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 72.85%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 72.62%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 72.22%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 72.09%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 71.85%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 72.96%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 72.59%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 72.65%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 72.67%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 72.60%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 72.37%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 72.39%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 72.11%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 71.85%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 71.52%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 71.38%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.12%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 70.82%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 70.48%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 70.23%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 69.93%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 69.76%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 69.56%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 69.36%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 69.32%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 69.28%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 69.12%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 68.49%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 68.17%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 67.85%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 67.56%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 67.32%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 68.72%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 68.95%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 68.62%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 68.36%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 68.10%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 67.77%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 67.49%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 67.24%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  200 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 67.67%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 67.43%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  204 | acc: 12.50%,  total acc: 66.98%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 66.81%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.29%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 67.72%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 69.28%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 69.22%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 69.10%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 68.89%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 69.70%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 69.61%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 69.48%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 69.26%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 69.13%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 69.13%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 69.16%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.08%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 69.05%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 69.88%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 69.92%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 69.87%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 69.67%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 69.57%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 69.43%   [EVAL] batch:  268 | acc: 31.25%,  total acc: 69.28%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 69.07%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 68.70%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 68.48%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 68.29%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 68.11%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 69.10%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:  301 | acc: 68.75%,  total acc: 70.30%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 70.47%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 70.51%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 70.73%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 70.67%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 70.52%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 70.38%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 70.25%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 70.13%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 69.97%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 70.18%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  325 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  326 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:  327 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 70.50%   [EVAL] batch:  332 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:  333 | acc: 25.00%,  total acc: 70.25%   [EVAL] batch:  334 | acc: 37.50%,  total acc: 70.15%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 70.01%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 69.84%   [EVAL] batch:  337 | acc: 6.25%,  total acc: 69.66%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 69.45%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 69.26%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 69.06%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 68.90%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.71%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 68.57%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.61%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 68.68%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 68.79%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 69.33%   [EVAL] batch:  358 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  359 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 69.25%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 69.23%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.21%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 69.29%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 69.21%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 69.15%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 69.09%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 69.02%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 68.98%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.98%   
cur_acc:  ['0.9405', '0.7530', '0.7073', '0.6895', '0.8075', '0.5962']
his_acc:  ['0.9405', '0.8450', '0.7620', '0.7210', '0.7338', '0.6898']
Clustering into  34  clusters
Clusters:  [ 0 23 24  0  0  0 28  0 19 33 26  0 20  0 17 11  9  0  0  0  0  0  0 25
  0 16  0 27 31 32  0 29  0  0 22  0  0  0 12 21 18  5  0  0 13 30 14  0
  0 15 10  0  0  2  2  8  0  0  0  0  4  7  2  0  0  3  0  6  0  1]
Losses:  6.813741207122803 1.4263243675231934
CurrentTrain: epoch  0, batch     0 | loss: 8.2400656Losses:  7.913851737976074 1.566058874130249
CurrentTrain: epoch  0, batch     1 | loss: 9.4799109Losses:  6.869919776916504 1.5279008150100708
CurrentTrain: epoch  0, batch     2 | loss: 8.3978205Losses:  6.0059661865234375 0.6598320603370667
CurrentTrain: epoch  0, batch     3 | loss: 6.6657982Losses:  4.1822509765625 1.0391641855239868
CurrentTrain: epoch  1, batch     0 | loss: 5.2214150Losses:  3.896702766418457 1.15403413772583
CurrentTrain: epoch  1, batch     1 | loss: 5.0507369Losses:  3.402879476547241 1.0736274719238281
CurrentTrain: epoch  1, batch     2 | loss: 4.4765072Losses:  3.1330273151397705 0.39864882826805115
CurrentTrain: epoch  1, batch     3 | loss: 3.5316761Losses:  3.566037893295288 1.097419261932373
CurrentTrain: epoch  2, batch     0 | loss: 4.6634569Losses:  3.3253819942474365 1.1535879373550415
CurrentTrain: epoch  2, batch     1 | loss: 4.4789701Losses:  3.624616861343384 1.085696816444397
CurrentTrain: epoch  2, batch     2 | loss: 4.7103138Losses:  5.314993858337402 0.5435246825218201
CurrentTrain: epoch  2, batch     3 | loss: 5.8585186Losses:  3.6331496238708496 1.1157290935516357
CurrentTrain: epoch  3, batch     0 | loss: 4.7488785Losses:  2.888126850128174 0.941359281539917
CurrentTrain: epoch  3, batch     1 | loss: 3.8294861Losses:  3.5235891342163086 0.8867486715316772
CurrentTrain: epoch  3, batch     2 | loss: 4.4103379Losses:  2.906607151031494 0.22838447988033295
CurrentTrain: epoch  3, batch     3 | loss: 3.1349916Losses:  2.9231820106506348 0.8991782665252686
CurrentTrain: epoch  4, batch     0 | loss: 3.8223603Losses:  3.5339865684509277 1.0107371807098389
CurrentTrain: epoch  4, batch     1 | loss: 4.5447235Losses:  2.7637228965759277 0.8557381629943848
CurrentTrain: epoch  4, batch     2 | loss: 3.6194611Losses:  3.837684154510498 0.2743035852909088
CurrentTrain: epoch  4, batch     3 | loss: 4.1119876Losses:  3.1692681312561035 0.9934862852096558
CurrentTrain: epoch  5, batch     0 | loss: 4.1627545Losses:  2.7153568267822266 0.8078693151473999
CurrentTrain: epoch  5, batch     1 | loss: 3.5232263Losses:  2.9328625202178955 0.8528413772583008
CurrentTrain: epoch  5, batch     2 | loss: 3.7857039Losses:  3.0121102333068848 0.22557902336120605
CurrentTrain: epoch  5, batch     3 | loss: 3.2376893Losses:  2.789623498916626 0.8173266649246216
CurrentTrain: epoch  6, batch     0 | loss: 3.6069503Losses:  2.3527379035949707 0.7179961204528809
CurrentTrain: epoch  6, batch     1 | loss: 3.0707340Losses:  3.0850391387939453 0.902573823928833
CurrentTrain: epoch  6, batch     2 | loss: 3.9876130Losses:  4.170080184936523 0.2886333167552948
CurrentTrain: epoch  6, batch     3 | loss: 4.4587135Losses:  2.2984845638275146 0.6082589626312256
CurrentTrain: epoch  7, batch     0 | loss: 2.9067435Losses:  2.673748254776001 0.8465911149978638
CurrentTrain: epoch  7, batch     1 | loss: 3.5203395Losses:  3.049910545349121 0.726691722869873
CurrentTrain: epoch  7, batch     2 | loss: 3.7766023Losses:  2.4984822273254395 2.0861628513557662e-07
CurrentTrain: epoch  7, batch     3 | loss: 2.4984825Losses:  2.79207706451416 0.8099099397659302
CurrentTrain: epoch  8, batch     0 | loss: 3.6019869Losses:  2.261995792388916 0.5748773813247681
CurrentTrain: epoch  8, batch     1 | loss: 2.8368731Losses:  2.664637565612793 0.7373325824737549
CurrentTrain: epoch  8, batch     2 | loss: 3.4019701Losses:  3.0536723136901855 0.09497391432523727
CurrentTrain: epoch  8, batch     3 | loss: 3.1486461Losses:  2.5482821464538574 0.7454534769058228
CurrentTrain: epoch  9, batch     0 | loss: 3.2937355Losses:  2.647581100463867 0.7437945604324341
CurrentTrain: epoch  9, batch     1 | loss: 3.3913755Losses:  2.297914981842041 0.6853256225585938
CurrentTrain: epoch  9, batch     2 | loss: 2.9832406Losses:  1.9515330791473389 0.08088332414627075
CurrentTrain: epoch  9, batch     3 | loss: 2.0324163
Losses:  6.29655647277832 0.7299660444259644
MemoryTrain:  epoch  0, batch     0 | loss: 7.0265226Losses:  9.957132339477539 0.8027169704437256
MemoryTrain:  epoch  0, batch     1 | loss: 10.7598495Losses:  10.069095611572266 0.779937744140625
MemoryTrain:  epoch  0, batch     2 | loss: 10.8490334Losses:  9.989311218261719 0.6795728206634521
MemoryTrain:  epoch  0, batch     3 | loss: 10.6688843Losses:  11.041991233825684 0.4638153910636902
MemoryTrain:  epoch  0, batch     4 | loss: 11.5058069Losses:  0.7363097667694092 0.6212953925132751
MemoryTrain:  epoch  1, batch     0 | loss: 1.3576052Losses:  1.134336233139038 0.7950303554534912
MemoryTrain:  epoch  1, batch     1 | loss: 1.9293666Losses:  0.8522403836250305 0.9345057606697083
MemoryTrain:  epoch  1, batch     2 | loss: 1.7867461Losses:  0.6994479894638062 0.6620764136314392
MemoryTrain:  epoch  1, batch     3 | loss: 1.3615243Losses:  0.5979323387145996 0.30245187878608704
MemoryTrain:  epoch  1, batch     4 | loss: 0.9003842Losses:  0.7667467594146729 0.7832449674606323
MemoryTrain:  epoch  2, batch     0 | loss: 1.5499917Losses:  0.5387200117111206 0.8887326717376709
MemoryTrain:  epoch  2, batch     1 | loss: 1.4274527Losses:  0.6549320220947266 0.5997125506401062
MemoryTrain:  epoch  2, batch     2 | loss: 1.2546446Losses:  0.6488881707191467 0.7653182744979858
MemoryTrain:  epoch  2, batch     3 | loss: 1.4142065Losses:  1.0771288871765137 0.355819433927536
MemoryTrain:  epoch  2, batch     4 | loss: 1.4329484Losses:  0.6340844631195068 0.7125344276428223
MemoryTrain:  epoch  3, batch     0 | loss: 1.3466189Losses:  0.4971085786819458 0.5730841755867004
MemoryTrain:  epoch  3, batch     1 | loss: 1.0701928Losses:  0.5311898589134216 0.8595039248466492
MemoryTrain:  epoch  3, batch     2 | loss: 1.3906938Losses:  0.615354061126709 0.789228618144989
MemoryTrain:  epoch  3, batch     3 | loss: 1.4045827Losses:  0.9566587805747986 0.293923944234848
MemoryTrain:  epoch  3, batch     4 | loss: 1.2505827Losses:  0.5545472502708435 0.6557444334030151
MemoryTrain:  epoch  4, batch     0 | loss: 1.2102916Losses:  0.4396136999130249 0.6728781461715698
MemoryTrain:  epoch  4, batch     1 | loss: 1.1124918Losses:  0.7105972766876221 0.7226337790489197
MemoryTrain:  epoch  4, batch     2 | loss: 1.4332311Losses:  0.47188401222229004 0.5918570756912231
MemoryTrain:  epoch  4, batch     3 | loss: 1.0637411Losses:  0.7484428286552429 0.5740698575973511
MemoryTrain:  epoch  4, batch     4 | loss: 1.3225126Losses:  0.39943045377731323 0.6438678503036499
MemoryTrain:  epoch  5, batch     0 | loss: 1.0432982Losses:  0.43908873200416565 0.6398568153381348
MemoryTrain:  epoch  5, batch     1 | loss: 1.0789455Losses:  0.533620297908783 0.6583575010299683
MemoryTrain:  epoch  5, batch     2 | loss: 1.1919777Losses:  0.5537747144699097 0.7573275566101074
MemoryTrain:  epoch  5, batch     3 | loss: 1.3111023Losses:  0.41150495409965515 0.35079389810562134
MemoryTrain:  epoch  5, batch     4 | loss: 0.7622988Losses:  0.4288650155067444 0.5829927921295166
MemoryTrain:  epoch  6, batch     0 | loss: 1.0118577Losses:  0.4439483880996704 0.7773326635360718
MemoryTrain:  epoch  6, batch     1 | loss: 1.2212811Losses:  0.4698636829853058 0.766843318939209
MemoryTrain:  epoch  6, batch     2 | loss: 1.2367070Losses:  0.42882710695266724 0.6897075176239014
MemoryTrain:  epoch  6, batch     3 | loss: 1.1185346Losses:  0.32596737146377563 0.23246455192565918
MemoryTrain:  epoch  6, batch     4 | loss: 0.5584319Losses:  0.44422101974487305 0.7263952493667603
MemoryTrain:  epoch  7, batch     0 | loss: 1.1706163Losses:  0.48244091868400574 0.6724196672439575
MemoryTrain:  epoch  7, batch     1 | loss: 1.1548606Losses:  0.38143330812454224 0.597181499004364
MemoryTrain:  epoch  7, batch     2 | loss: 0.9786148Losses:  0.38103020191192627 0.654179036617279
MemoryTrain:  epoch  7, batch     3 | loss: 1.0352092Losses:  0.495919406414032 0.2716961205005646
MemoryTrain:  epoch  7, batch     4 | loss: 0.7676156Losses:  0.38677287101745605 0.7120476961135864
MemoryTrain:  epoch  8, batch     0 | loss: 1.0988206Losses:  0.39900529384613037 0.6324885487556458
MemoryTrain:  epoch  8, batch     1 | loss: 1.0314939Losses:  0.49319973587989807 0.8883255124092102
MemoryTrain:  epoch  8, batch     2 | loss: 1.3815253Losses:  0.42575427889823914 0.5639593601226807
MemoryTrain:  epoch  8, batch     3 | loss: 0.9897137Losses:  0.3117618262767792 0.19886645674705505
MemoryTrain:  epoch  8, batch     4 | loss: 0.5106283Losses:  0.34701773524284363 0.5666703581809998
MemoryTrain:  epoch  9, batch     0 | loss: 0.9136881Losses:  0.3586941957473755 0.5413424968719482
MemoryTrain:  epoch  9, batch     1 | loss: 0.9000367Losses:  0.420976459980011 0.6588181257247925
MemoryTrain:  epoch  9, batch     2 | loss: 1.0797946Losses:  0.4461696147918701 0.7082223296165466
MemoryTrain:  epoch  9, batch     3 | loss: 1.1543920Losses:  0.4633771777153015 0.4453775882720947
MemoryTrain:  epoch  9, batch     4 | loss: 0.9087548
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 59.72%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 57.59%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.82%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 53.32%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 54.36%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.79%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 57.64%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 57.85%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 57.34%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 56.71%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 56.10%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 55.97%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 56.67%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 57.47%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 58.24%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 58.46%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 59.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 59.31%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 59.62%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 59.67%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 59.84%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 60.34%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 61.31%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 61.86%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 63.00%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 89.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.34%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.50%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 88.71%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 87.72%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 87.18%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 86.48%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 86.19%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 85.71%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 84.96%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 84.62%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 84.56%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 83.86%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 83.27%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 82.88%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 82.48%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 82.03%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 81.42%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 81.08%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.17%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 80.67%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 79.95%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 79.65%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 79.19%   [EVAL] batch:   79 | acc: 25.00%,  total acc: 78.52%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 78.09%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 77.36%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 76.43%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 75.60%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 74.71%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 73.84%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 73.06%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.80%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.70%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 76.11%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 76.15%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 76.42%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 75.64%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 75.23%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.66%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 74.44%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 73.94%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 73.73%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 73.79%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 74.03%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 74.32%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 73.70%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 73.09%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 72.59%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 72.05%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 71.52%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 70.67%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 70.41%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 70.20%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 70.10%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 70.04%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 71.04%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 70.89%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 70.97%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 70.99%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 70.94%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 70.92%   [EVAL] batch:  144 | acc: 43.75%,  total acc: 70.73%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 70.45%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 70.40%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 70.26%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 70.04%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 69.74%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 69.49%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 69.16%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 68.87%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 68.63%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 68.55%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  159 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:  160 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 68.09%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 67.91%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:  164 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 67.70%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 67.64%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 66.64%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 66.33%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 65.82%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 67.18%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 67.54%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.23%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 66.69%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 65.82%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  200 | acc: 25.00%,  total acc: 66.57%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 66.27%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 65.87%   [EVAL] batch:  204 | acc: 12.50%,  total acc: 65.61%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 66.59%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 67.40%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  228 | acc: 56.25%,  total acc: 67.33%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 67.17%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.99%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 67.89%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 67.76%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 67.69%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 67.44%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 67.42%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.53%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 68.02%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 68.02%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.03%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 68.10%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 68.13%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 68.08%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 68.02%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 67.95%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 67.79%   [EVAL] batch:  266 | acc: 31.25%,  total acc: 67.65%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 67.54%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 67.43%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 67.27%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 67.11%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 66.91%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 66.67%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 66.54%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 66.36%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  301 | acc: 62.50%,  total acc: 68.63%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 68.71%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 68.67%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 68.79%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 68.87%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 68.91%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 68.69%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 68.51%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 68.33%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 68.16%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 67.94%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 67.83%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 68.13%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  327 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 68.15%   [EVAL] batch:  332 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:  333 | acc: 31.25%,  total acc: 67.95%   [EVAL] batch:  334 | acc: 43.75%,  total acc: 67.87%   [EVAL] batch:  335 | acc: 25.00%,  total acc: 67.75%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 67.62%   [EVAL] batch:  337 | acc: 12.50%,  total acc: 67.46%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 67.26%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 67.08%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 66.88%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 66.70%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 66.53%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 66.39%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 66.43%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 66.66%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  356 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  357 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  358 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 67.05%   [EVAL] batch:  360 | acc: 50.00%,  total acc: 67.00%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 66.97%   [EVAL] batch:  362 | acc: 43.75%,  total acc: 66.91%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 66.86%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 66.90%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 66.90%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 66.78%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 66.73%   [EVAL] batch:  372 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 66.62%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 66.52%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  387 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:  388 | acc: 0.00%,  total acc: 66.71%   [EVAL] batch:  389 | acc: 6.25%,  total acc: 66.55%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 66.42%   [EVAL] batch:  391 | acc: 0.00%,  total acc: 66.25%   [EVAL] batch:  392 | acc: 0.00%,  total acc: 66.08%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 66.32%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 66.15%   [EVAL] batch:  402 | acc: 0.00%,  total acc: 65.99%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 65.73%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 65.57%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 65.94%   [EVAL] batch:  413 | acc: 6.25%,  total acc: 65.79%   [EVAL] batch:  414 | acc: 37.50%,  total acc: 65.72%   [EVAL] batch:  415 | acc: 31.25%,  total acc: 65.64%   [EVAL] batch:  416 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:  417 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:  418 | acc: 50.00%,  total acc: 65.50%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  420 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  421 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 65.69%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 65.74%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 65.76%   [EVAL] batch:  427 | acc: 62.50%,  total acc: 65.76%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 66.10%   
cur_acc:  ['0.9405', '0.7530', '0.7073', '0.6895', '0.8075', '0.5962', '0.6300']
his_acc:  ['0.9405', '0.8450', '0.7620', '0.7210', '0.7338', '0.6898', '0.6610']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0  0  0  0 12
  0 37  0 23 31 36  2 34  0  0 30  0  0  0 27 11 18 29  0  0 28 32 13  0
  0 17 16  0  0  1  1 20  0  0  0  0 15  9  1  0  0  8  0 14  0  7  5 10
  0  0  0  2  0  3  6  4]
Losses:  6.851944923400879 1.293574571609497
CurrentTrain: epoch  0, batch     0 | loss: 8.1455193Losses:  8.032365798950195 1.195467472076416
CurrentTrain: epoch  0, batch     1 | loss: 9.2278328Losses:  6.698724746704102 1.4049203395843506
CurrentTrain: epoch  0, batch     2 | loss: 8.1036453Losses:  5.3454766273498535 0.1164759024977684
CurrentTrain: epoch  0, batch     3 | loss: 5.4619527Losses:  3.450222969055176 1.0744731426239014
CurrentTrain: epoch  1, batch     0 | loss: 4.5246964Losses:  3.4723992347717285 1.1486884355545044
CurrentTrain: epoch  1, batch     1 | loss: 4.6210876Losses:  3.301935911178589 1.1773791313171387
CurrentTrain: epoch  1, batch     2 | loss: 4.4793148Losses:  2.9861550331115723 0.09505316615104675
CurrentTrain: epoch  1, batch     3 | loss: 3.0812082Losses:  4.074097633361816 1.0635462999343872
CurrentTrain: epoch  2, batch     0 | loss: 5.1376438Losses:  2.5248303413391113 0.8745991587638855
CurrentTrain: epoch  2, batch     1 | loss: 3.3994296Losses:  3.027287483215332 1.120368480682373
CurrentTrain: epoch  2, batch     2 | loss: 4.1476560Losses:  2.2173941135406494 0.0276072695851326
CurrentTrain: epoch  2, batch     3 | loss: 2.2450013Losses:  3.055488109588623 0.9002482295036316
CurrentTrain: epoch  3, batch     0 | loss: 3.9557364Losses:  2.908987045288086 0.8662091493606567
CurrentTrain: epoch  3, batch     1 | loss: 3.7751961Losses:  3.000579833984375 1.0070395469665527
CurrentTrain: epoch  3, batch     2 | loss: 4.0076194Losses:  1.825199842453003 0.18462985754013062
CurrentTrain: epoch  3, batch     3 | loss: 2.0098298Losses:  3.200679302215576 0.76943039894104
CurrentTrain: epoch  4, batch     0 | loss: 3.9701097Losses:  2.323699951171875 0.7274794578552246
CurrentTrain: epoch  4, batch     1 | loss: 3.0511794Losses:  2.457441806793213 0.8522543907165527
CurrentTrain: epoch  4, batch     2 | loss: 3.3096962Losses:  4.263361930847168 0.3000366687774658
CurrentTrain: epoch  4, batch     3 | loss: 4.5633984Losses:  2.514659881591797 0.8518633842468262
CurrentTrain: epoch  5, batch     0 | loss: 3.3665233Losses:  2.596097469329834 0.875464677810669
CurrentTrain: epoch  5, batch     1 | loss: 3.4715621Losses:  2.4714584350585938 0.7746231555938721
CurrentTrain: epoch  5, batch     2 | loss: 3.2460816Losses:  2.4629900455474854 0.23252639174461365
CurrentTrain: epoch  5, batch     3 | loss: 2.6955163Losses:  2.5951390266418457 0.7865206003189087
CurrentTrain: epoch  6, batch     0 | loss: 3.3816595Losses:  2.1336922645568848 0.6728134155273438
CurrentTrain: epoch  6, batch     1 | loss: 2.8065057Losses:  2.3909125328063965 0.6603503227233887
CurrentTrain: epoch  6, batch     2 | loss: 3.0512629Losses:  1.9148385524749756 0.10921711474657059
CurrentTrain: epoch  6, batch     3 | loss: 2.0240557Losses:  2.551788806915283 0.5016777515411377
CurrentTrain: epoch  7, batch     0 | loss: 3.0534666Losses:  2.064879894256592 0.6500239372253418
CurrentTrain: epoch  7, batch     1 | loss: 2.7149038Losses:  1.9987845420837402 0.5554683208465576
CurrentTrain: epoch  7, batch     2 | loss: 2.5542529Losses:  2.8304436206817627 0.18105043470859528
CurrentTrain: epoch  7, batch     3 | loss: 3.0114942Losses:  2.0776526927948 0.4631766080856323
CurrentTrain: epoch  8, batch     0 | loss: 2.5408292Losses:  2.078294277191162 0.6250094175338745
CurrentTrain: epoch  8, batch     1 | loss: 2.7033038Losses:  2.301546096801758 0.6536381244659424
CurrentTrain: epoch  8, batch     2 | loss: 2.9551842Losses:  1.7399921417236328 0.16880816221237183
CurrentTrain: epoch  8, batch     3 | loss: 1.9088004Losses:  1.883172631263733 0.48005974292755127
CurrentTrain: epoch  9, batch     0 | loss: 2.3632324Losses:  2.1311910152435303 0.5288334488868713
CurrentTrain: epoch  9, batch     1 | loss: 2.6600244Losses:  2.0813634395599365 0.6854041218757629
CurrentTrain: epoch  9, batch     2 | loss: 2.7667675Losses:  1.9884538650512695 0.19678178429603577
CurrentTrain: epoch  9, batch     3 | loss: 2.1852357
Losses:  6.297398567199707 0.7492086887359619
MemoryTrain:  epoch  0, batch     0 | loss: 7.0466070Losses:  9.433384895324707 0.6068393588066101
MemoryTrain:  epoch  0, batch     1 | loss: 10.0402241Losses:  10.261388778686523 0.5785927176475525
MemoryTrain:  epoch  0, batch     2 | loss: 10.8399811Losses:  10.938766479492188 0.9088840484619141
MemoryTrain:  epoch  0, batch     3 | loss: 11.8476505Losses:  10.858043670654297 0.6758885383605957
MemoryTrain:  epoch  0, batch     4 | loss: 11.5339317Losses:  1.0332317352294922 0.7400640249252319
MemoryTrain:  epoch  1, batch     0 | loss: 1.7732958Losses:  0.8432562351226807 0.614141047000885
MemoryTrain:  epoch  1, batch     1 | loss: 1.4573972Losses:  0.66340172290802 0.45732295513153076
MemoryTrain:  epoch  1, batch     2 | loss: 1.1207247Losses:  1.2507919073104858 0.9571370482444763
MemoryTrain:  epoch  1, batch     3 | loss: 2.2079289Losses:  1.450354814529419 0.8217775821685791
MemoryTrain:  epoch  1, batch     4 | loss: 2.2721324Losses:  0.7678965926170349 0.742516279220581
MemoryTrain:  epoch  2, batch     0 | loss: 1.5104129Losses:  0.7845032811164856 0.741212010383606
MemoryTrain:  epoch  2, batch     1 | loss: 1.5257154Losses:  0.9751759171485901 0.8083945512771606
MemoryTrain:  epoch  2, batch     2 | loss: 1.7835705Losses:  0.6045955419540405 0.5373489856719971
MemoryTrain:  epoch  2, batch     3 | loss: 1.1419445Losses:  0.8894187211990356 0.6431354284286499
MemoryTrain:  epoch  2, batch     4 | loss: 1.5325541Losses:  0.5054730772972107 0.6237021684646606
MemoryTrain:  epoch  3, batch     0 | loss: 1.1291752Losses:  0.7406952977180481 0.6994249820709229
MemoryTrain:  epoch  3, batch     1 | loss: 1.4401202Losses:  0.6015588045120239 0.6423025131225586
MemoryTrain:  epoch  3, batch     2 | loss: 1.2438613Losses:  0.7639392614364624 0.8819363117218018
MemoryTrain:  epoch  3, batch     3 | loss: 1.6458756Losses:  0.5277021527290344 0.5870766639709473
MemoryTrain:  epoch  3, batch     4 | loss: 1.1147788Losses:  0.6226359009742737 0.7130318880081177
MemoryTrain:  epoch  4, batch     0 | loss: 1.3356678Losses:  0.5942532420158386 0.6679017543792725
MemoryTrain:  epoch  4, batch     1 | loss: 1.2621551Losses:  0.4597626328468323 0.5870522856712341
MemoryTrain:  epoch  4, batch     2 | loss: 1.0468149Losses:  0.508625864982605 0.619917631149292
MemoryTrain:  epoch  4, batch     3 | loss: 1.1285435Losses:  0.5773100256919861 0.7703192234039307
MemoryTrain:  epoch  4, batch     4 | loss: 1.3476293Losses:  0.45912978053092957 0.5920220613479614
MemoryTrain:  epoch  5, batch     0 | loss: 1.0511519Losses:  0.5688799023628235 0.8365099430084229
MemoryTrain:  epoch  5, batch     1 | loss: 1.4053898Losses:  0.5020338892936707 0.719147801399231
MemoryTrain:  epoch  5, batch     2 | loss: 1.2211816Losses:  0.5585564374923706 0.7139447927474976
MemoryTrain:  epoch  5, batch     3 | loss: 1.2725012Losses:  0.37878209352493286 0.4991265535354614
MemoryTrain:  epoch  5, batch     4 | loss: 0.8779086Losses:  0.4954485297203064 0.7239509224891663
MemoryTrain:  epoch  6, batch     0 | loss: 1.2193995Losses:  0.4268070161342621 0.6072044372558594
MemoryTrain:  epoch  6, batch     1 | loss: 1.0340115Losses:  0.49828290939331055 0.6285858750343323
MemoryTrain:  epoch  6, batch     2 | loss: 1.1268687Losses:  0.4169391095638275 0.5959219932556152
MemoryTrain:  epoch  6, batch     3 | loss: 1.0128611Losses:  0.5074290633201599 0.8222506046295166
MemoryTrain:  epoch  6, batch     4 | loss: 1.3296797Losses:  0.46351224184036255 0.7068610787391663
MemoryTrain:  epoch  7, batch     0 | loss: 1.1703733Losses:  0.4774619936943054 0.806458592414856
MemoryTrain:  epoch  7, batch     1 | loss: 1.2839205Losses:  0.4783020615577698 0.6003409624099731
MemoryTrain:  epoch  7, batch     2 | loss: 1.0786431Losses:  0.4124446213245392 0.515099048614502
MemoryTrain:  epoch  7, batch     3 | loss: 0.9275436Losses:  0.48505470156669617 0.6353893280029297
MemoryTrain:  epoch  7, batch     4 | loss: 1.1204441Losses:  0.4301040470600128 0.5618639588356018
MemoryTrain:  epoch  8, batch     0 | loss: 0.9919680Losses:  0.663638174533844 0.703469455242157
MemoryTrain:  epoch  8, batch     1 | loss: 1.3671076Losses:  0.4443127512931824 0.6074317693710327
MemoryTrain:  epoch  8, batch     2 | loss: 1.0517445Losses:  0.5163565278053284 0.6085635423660278
MemoryTrain:  epoch  8, batch     3 | loss: 1.1249201Losses:  0.40522855520248413 0.6199252605438232
MemoryTrain:  epoch  8, batch     4 | loss: 1.0251539Losses:  0.4425848722457886 0.7935538291931152
MemoryTrain:  epoch  9, batch     0 | loss: 1.2361387Losses:  0.6638821363449097 0.6527894139289856
MemoryTrain:  epoch  9, batch     1 | loss: 1.3166716Losses:  0.4111440181732178 0.5039660930633545
MemoryTrain:  epoch  9, batch     2 | loss: 0.9151101Losses:  0.3486751616001129 0.49004754424095154
MemoryTrain:  epoch  9, batch     3 | loss: 0.8387227Losses:  0.4865889251232147 0.700735867023468
MemoryTrain:  epoch  9, batch     4 | loss: 1.1873248
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 68.93%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 66.15%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 65.03%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 64.54%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 70.34%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.72%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.07%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 84.14%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.45%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.75%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 79.88%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 79.13%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 78.50%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 77.71%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 76.93%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 76.54%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 76.32%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 75.87%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 75.51%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 75.25%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 75.33%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 75.08%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 74.59%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 74.28%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 73.91%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 73.77%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 73.09%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 72.29%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 71.43%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 70.66%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 69.84%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 69.11%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.78%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 72.71%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 72.73%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 72.63%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 72.72%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 72.78%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 72.28%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 71.73%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 71.25%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 71.00%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 71.03%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 70.96%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 70.42%   [EVAL] batch:  120 | acc: 0.00%,  total acc: 69.83%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 69.36%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 68.85%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 68.35%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 67.90%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 67.66%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 67.29%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 66.98%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 67.14%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 67.99%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 67.81%   [EVAL] batch:  140 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 67.87%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 67.74%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 67.59%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 67.39%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 67.40%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 67.25%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 67.09%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 66.94%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 66.71%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 66.40%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 66.09%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 65.83%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 65.61%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 65.51%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 65.41%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 65.24%   [EVAL] batch:  162 | acc: 37.50%,  total acc: 65.07%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 64.74%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 64.10%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 63.80%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 63.54%   [EVAL] batch:  174 | acc: 25.00%,  total acc: 63.32%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.74%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.54%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 65.36%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 65.15%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 64.90%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 64.59%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 64.32%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 64.05%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 63.89%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 64.04%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  196 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  200 | acc: 25.00%,  total acc: 64.71%   [EVAL] batch:  201 | acc: 6.25%,  total acc: 64.42%   [EVAL] batch:  202 | acc: 18.75%,  total acc: 64.19%   [EVAL] batch:  203 | acc: 18.75%,  total acc: 63.97%   [EVAL] batch:  204 | acc: 12.50%,  total acc: 63.72%   [EVAL] batch:  205 | acc: 18.75%,  total acc: 63.50%   [EVAL] batch:  206 | acc: 81.25%,  total acc: 63.59%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:  213 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 64.59%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 64.67%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 64.82%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 65.58%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 65.49%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.53%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 66.09%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 66.00%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 65.73%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 65.79%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 65.80%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 66.46%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 66.37%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 66.21%   [EVAL] batch:  266 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 66.00%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 65.89%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 65.69%   [EVAL] batch:  270 | acc: 12.50%,  total acc: 65.50%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 65.30%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 65.06%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 64.90%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 64.70%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.83%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 65.68%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 65.69%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 67.05%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.68%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 67.69%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 67.48%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 67.28%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 67.09%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 66.92%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 66.71%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 66.61%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 66.64%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 67.02%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 67.03%   [EVAL] batch:  327 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 67.04%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 66.89%   [EVAL] batch:  333 | acc: 31.25%,  total acc: 66.79%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 66.68%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 66.52%   [EVAL] batch:  336 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  337 | acc: 12.50%,  total acc: 66.20%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 66.00%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 65.44%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 65.29%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 65.15%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  349 | acc: 75.00%,  total acc: 65.45%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 65.71%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 65.55%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 65.38%   [EVAL] batch:  360 | acc: 6.25%,  total acc: 65.22%   [EVAL] batch:  361 | acc: 0.00%,  total acc: 65.04%   [EVAL] batch:  362 | acc: 37.50%,  total acc: 64.96%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 64.93%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 65.01%   [EVAL] batch:  369 | acc: 37.50%,  total acc: 64.93%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 64.91%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 64.83%   [EVAL] batch:  373 | acc: 56.25%,  total acc: 64.81%   [EVAL] batch:  374 | acc: 56.25%,  total acc: 64.78%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 64.74%   [EVAL] batch:  377 | acc: 56.25%,  total acc: 64.72%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 64.75%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  387 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  388 | acc: 0.00%,  total acc: 64.97%   [EVAL] batch:  389 | acc: 6.25%,  total acc: 64.82%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 64.69%   [EVAL] batch:  391 | acc: 0.00%,  total acc: 64.52%   [EVAL] batch:  392 | acc: 6.25%,  total acc: 64.38%   [EVAL] batch:  393 | acc: 25.00%,  total acc: 64.28%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 64.65%   [EVAL] batch:  401 | acc: 0.00%,  total acc: 64.49%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 64.35%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 64.26%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 64.10%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 63.95%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 64.05%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 64.21%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 64.28%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 64.37%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  413 | acc: 12.50%,  total acc: 64.28%   [EVAL] batch:  414 | acc: 43.75%,  total acc: 64.23%   [EVAL] batch:  415 | acc: 37.50%,  total acc: 64.17%   [EVAL] batch:  416 | acc: 37.50%,  total acc: 64.10%   [EVAL] batch:  417 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 64.04%   [EVAL] batch:  419 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  420 | acc: 93.75%,  total acc: 64.16%   [EVAL] batch:  421 | acc: 93.75%,  total acc: 64.23%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 64.26%   [EVAL] batch:  423 | acc: 87.50%,  total acc: 64.31%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 64.37%   [EVAL] batch:  425 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 64.36%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 64.37%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 64.38%   [EVAL] batch:  429 | acc: 68.75%,  total acc: 64.39%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 64.41%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 64.48%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  436 | acc: 81.25%,  total acc: 64.70%   [EVAL] batch:  437 | acc: 87.50%,  total acc: 64.75%   [EVAL] batch:  438 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:  439 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  440 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  441 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:  442 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 64.98%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  445 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  448 | acc: 31.25%,  total acc: 64.89%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  451 | acc: 50.00%,  total acc: 64.93%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 64.94%   [EVAL] batch:  453 | acc: 37.50%,  total acc: 64.88%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:  455 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:  463 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  464 | acc: 50.00%,  total acc: 65.27%   [EVAL] batch:  465 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  466 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  467 | acc: 62.50%,  total acc: 65.22%   [EVAL] batch:  468 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  469 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:  470 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  471 | acc: 18.75%,  total acc: 65.00%   [EVAL] batch:  472 | acc: 18.75%,  total acc: 64.90%   [EVAL] batch:  473 | acc: 18.75%,  total acc: 64.81%   [EVAL] batch:  474 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 64.77%   [EVAL] batch:  477 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 64.87%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 64.87%   [EVAL] batch:  482 | acc: 37.50%,  total acc: 64.82%   [EVAL] batch:  483 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  484 | acc: 37.50%,  total acc: 64.76%   [EVAL] batch:  485 | acc: 56.25%,  total acc: 64.74%   [EVAL] batch:  486 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 65.33%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 65.51%   
cur_acc:  ['0.9405', '0.7530', '0.7073', '0.6895', '0.8075', '0.5962', '0.6300', '0.7034']
his_acc:  ['0.9405', '0.8450', '0.7620', '0.7210', '0.7338', '0.6898', '0.6610', '0.6551']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.725239753723145 1.7866168022155762
CurrentTrain: epoch  0, batch     0 | loss: 13.5118561Losses:  13.38376235961914 1.7751171588897705
CurrentTrain: epoch  0, batch     1 | loss: 15.1588793Losses:  13.819381713867188 1.5740278959274292
CurrentTrain: epoch  0, batch     2 | loss: 15.3934097Losses:  13.58057975769043 1.8535072803497314
CurrentTrain: epoch  0, batch     3 | loss: 15.4340868Losses:  12.955269813537598 1.8357983827590942
CurrentTrain: epoch  0, batch     4 | loss: 14.7910681Losses:  13.391741752624512 1.6984502077102661
CurrentTrain: epoch  0, batch     5 | loss: 15.0901918Losses:  12.692693710327148 1.6014541387557983
CurrentTrain: epoch  0, batch     6 | loss: 14.2941475Losses:  12.794824600219727 1.6582653522491455
CurrentTrain: epoch  0, batch     7 | loss: 14.4530897Losses:  12.856782913208008 1.6576907634735107
CurrentTrain: epoch  0, batch     8 | loss: 14.5144739Losses:  12.910018920898438 1.6190485954284668
CurrentTrain: epoch  0, batch     9 | loss: 14.5290680Losses:  12.551483154296875 1.613991141319275
CurrentTrain: epoch  0, batch    10 | loss: 14.1654739Losses:  12.20489501953125 1.5713727474212646
CurrentTrain: epoch  0, batch    11 | loss: 13.7762680Losses:  11.868359565734863 1.5437997579574585
CurrentTrain: epoch  0, batch    12 | loss: 13.4121590Losses:  11.927678108215332 1.6538012027740479
CurrentTrain: epoch  0, batch    13 | loss: 13.5814791Losses:  11.570840835571289 1.627005696296692
CurrentTrain: epoch  0, batch    14 | loss: 13.1978464Losses:  11.306148529052734 1.6669212579727173
CurrentTrain: epoch  0, batch    15 | loss: 12.9730701Losses:  12.016668319702148 1.547483205795288
CurrentTrain: epoch  0, batch    16 | loss: 13.5641518Losses:  11.607784271240234 1.8174854516983032
CurrentTrain: epoch  0, batch    17 | loss: 13.4252701Losses:  12.17730712890625 1.5314624309539795
CurrentTrain: epoch  0, batch    18 | loss: 13.7087698Losses:  10.934189796447754 1.4151498079299927
CurrentTrain: epoch  0, batch    19 | loss: 12.3493395Losses:  11.510461807250977 1.7555782794952393
CurrentTrain: epoch  0, batch    20 | loss: 13.2660398Losses:  11.207008361816406 1.7271027565002441
CurrentTrain: epoch  0, batch    21 | loss: 12.9341106Losses:  10.777278900146484 1.3248999118804932
CurrentTrain: epoch  0, batch    22 | loss: 12.1021786Losses:  10.606435775756836 1.2347450256347656
CurrentTrain: epoch  0, batch    23 | loss: 11.8411808Losses:  11.198270797729492 1.480238914489746
CurrentTrain: epoch  0, batch    24 | loss: 12.6785097Losses:  10.779748916625977 1.0967652797698975
CurrentTrain: epoch  0, batch    25 | loss: 11.8765144Losses:  10.293206214904785 1.3346716165542603
CurrentTrain: epoch  0, batch    26 | loss: 11.6278782Losses:  10.825725555419922 1.403538703918457
CurrentTrain: epoch  0, batch    27 | loss: 12.2292643Losses:  10.277344703674316 1.4431788921356201
CurrentTrain: epoch  0, batch    28 | loss: 11.7205238Losses:  9.693235397338867 1.1003501415252686
CurrentTrain: epoch  0, batch    29 | loss: 10.7935858Losses:  10.328608512878418 1.32513427734375
CurrentTrain: epoch  0, batch    30 | loss: 11.6537428Losses:  9.803899765014648 1.0867587327957153
CurrentTrain: epoch  0, batch    31 | loss: 10.8906584Losses:  9.562626838684082 1.1092562675476074
CurrentTrain: epoch  0, batch    32 | loss: 10.6718826Losses:  9.771185874938965 1.2634083032608032
CurrentTrain: epoch  0, batch    33 | loss: 11.0345945Losses:  9.568532943725586 1.0482264757156372
CurrentTrain: epoch  0, batch    34 | loss: 10.6167593Losses:  9.360849380493164 1.0397436618804932
CurrentTrain: epoch  0, batch    35 | loss: 10.4005928Losses:  9.253833770751953 1.2568023204803467
CurrentTrain: epoch  0, batch    36 | loss: 10.5106363Losses:  9.22528076171875 1.0915398597717285
CurrentTrain: epoch  0, batch    37 | loss: 10.3168201Losses:  9.086726188659668 1.1584508419036865
CurrentTrain: epoch  0, batch    38 | loss: 10.2451773Losses:  8.78765869140625 1.3078219890594482
CurrentTrain: epoch  0, batch    39 | loss: 10.0954809Losses:  8.68784236907959 1.4031163454055786
CurrentTrain: epoch  0, batch    40 | loss: 10.0909586Losses:  8.513450622558594 1.0962178707122803
CurrentTrain: epoch  0, batch    41 | loss: 9.6096687Losses:  8.157376289367676 0.9417352676391602
CurrentTrain: epoch  0, batch    42 | loss: 9.0991116Losses:  8.23287582397461 1.0897222757339478
CurrentTrain: epoch  0, batch    43 | loss: 9.3225985Losses:  8.090011596679688 1.185286045074463
CurrentTrain: epoch  0, batch    44 | loss: 9.2752972Losses:  7.887392997741699 0.8862744569778442
CurrentTrain: epoch  0, batch    45 | loss: 8.7736673Losses:  8.169342041015625 0.9590014219284058
CurrentTrain: epoch  0, batch    46 | loss: 9.1283436Losses:  7.713382720947266 1.0247597694396973
CurrentTrain: epoch  0, batch    47 | loss: 8.7381420Losses:  7.278357028961182 1.1697967052459717
CurrentTrain: epoch  0, batch    48 | loss: 8.4481535Losses:  7.582134246826172 1.0868289470672607
CurrentTrain: epoch  0, batch    49 | loss: 8.6689634Losses:  7.551000595092773 0.9775404930114746
CurrentTrain: epoch  0, batch    50 | loss: 8.5285416Losses:  7.0114898681640625 1.0587949752807617
CurrentTrain: epoch  0, batch    51 | loss: 8.0702848Losses:  7.256622314453125 0.9360079169273376
CurrentTrain: epoch  0, batch    52 | loss: 8.1926298Losses:  7.193320274353027 0.9121042490005493
CurrentTrain: epoch  0, batch    53 | loss: 8.1054249Losses:  6.641526699066162 0.9008869528770447
CurrentTrain: epoch  0, batch    54 | loss: 7.5424137Losses:  6.627254009246826 1.0036853551864624
CurrentTrain: epoch  0, batch    55 | loss: 7.6309395Losses:  6.829817771911621 0.921980619430542
CurrentTrain: epoch  0, batch    56 | loss: 7.7517986Losses:  6.242756366729736 0.7360515594482422
CurrentTrain: epoch  0, batch    57 | loss: 6.9788079Losses:  6.468041896820068 0.9059863090515137
CurrentTrain: epoch  0, batch    58 | loss: 7.3740282Losses:  6.232226371765137 1.0352927446365356
CurrentTrain: epoch  0, batch    59 | loss: 7.2675190Losses:  6.2757368087768555 0.9486324787139893
CurrentTrain: epoch  0, batch    60 | loss: 7.2243690Losses:  6.166767120361328 0.9187448024749756
CurrentTrain: epoch  0, batch    61 | loss: 7.0855122Losses:  5.961428642272949 0.6664285659790039
CurrentTrain: epoch  0, batch    62 | loss: 6.6278572Losses:  5.641903877258301 0.7519152164459229
CurrentTrain: epoch  1, batch     0 | loss: 6.3938189Losses:  5.966142177581787 0.9878761768341064
CurrentTrain: epoch  1, batch     1 | loss: 6.9540186Losses:  5.555847644805908 0.7995933294296265
CurrentTrain: epoch  1, batch     2 | loss: 6.3554411Losses:  5.803632736206055 0.8554701209068298
CurrentTrain: epoch  1, batch     3 | loss: 6.6591029Losses:  5.55744743347168 0.7536971569061279
CurrentTrain: epoch  1, batch     4 | loss: 6.3111448Losses:  5.775596618652344 0.7199965715408325
CurrentTrain: epoch  1, batch     5 | loss: 6.4955931Losses:  5.951249599456787 0.8774837851524353
CurrentTrain: epoch  1, batch     6 | loss: 6.8287334Losses:  5.625682353973389 0.7271145582199097
CurrentTrain: epoch  1, batch     7 | loss: 6.3527970Losses:  5.649514198303223 0.890448272228241
CurrentTrain: epoch  1, batch     8 | loss: 6.5399623Losses:  5.697039604187012 0.8277413845062256
CurrentTrain: epoch  1, batch     9 | loss: 6.5247812Losses:  5.455550193786621 0.8111228942871094
CurrentTrain: epoch  1, batch    10 | loss: 6.2666731Losses:  5.449038505554199 0.8543598651885986
CurrentTrain: epoch  1, batch    11 | loss: 6.3033981Losses:  5.911690711975098 0.8533545732498169
CurrentTrain: epoch  1, batch    12 | loss: 6.7650452Losses:  5.922776222229004 0.9040239453315735
CurrentTrain: epoch  1, batch    13 | loss: 6.8268003Losses:  5.361121654510498 0.6118682622909546
CurrentTrain: epoch  1, batch    14 | loss: 5.9729900Losses:  5.578119277954102 0.7847726345062256
CurrentTrain: epoch  1, batch    15 | loss: 6.3628922Losses:  5.195296287536621 0.6854719519615173
CurrentTrain: epoch  1, batch    16 | loss: 5.8807683Losses:  4.934398651123047 0.5723729133605957
CurrentTrain: epoch  1, batch    17 | loss: 5.5067716Losses:  5.261943817138672 0.6727380752563477
CurrentTrain: epoch  1, batch    18 | loss: 5.9346819Losses:  5.415658473968506 0.7799121141433716
CurrentTrain: epoch  1, batch    19 | loss: 6.1955705Losses:  5.369668960571289 0.6682332754135132
CurrentTrain: epoch  1, batch    20 | loss: 6.0379024Losses:  5.200220108032227 0.7287184000015259
CurrentTrain: epoch  1, batch    21 | loss: 5.9289384Losses:  5.311181545257568 0.7843971848487854
CurrentTrain: epoch  1, batch    22 | loss: 6.0955787Losses:  5.203729152679443 0.5219196081161499
CurrentTrain: epoch  1, batch    23 | loss: 5.7256489Losses:  5.165066719055176 0.6209802627563477
CurrentTrain: epoch  1, batch    24 | loss: 5.7860470Losses:  5.68941593170166 0.781591534614563
CurrentTrain: epoch  1, batch    25 | loss: 6.4710073Losses:  5.346195697784424 0.5521974563598633
CurrentTrain: epoch  1, batch    26 | loss: 5.8983932Losses:  5.486227035522461 0.8237597942352295
CurrentTrain: epoch  1, batch    27 | loss: 6.3099871Losses:  5.4648613929748535 0.47632482647895813
CurrentTrain: epoch  1, batch    28 | loss: 5.9411864Losses:  5.366947174072266 0.7160981893539429
CurrentTrain: epoch  1, batch    29 | loss: 6.0830455Losses:  5.1672892570495605 0.5152794122695923
CurrentTrain: epoch  1, batch    30 | loss: 5.6825686Losses:  5.310940742492676 0.7248347401618958
CurrentTrain: epoch  1, batch    31 | loss: 6.0357757Losses:  4.876511096954346 0.47209930419921875
CurrentTrain: epoch  1, batch    32 | loss: 5.3486104Losses:  5.23300838470459 0.5482800006866455
CurrentTrain: epoch  1, batch    33 | loss: 5.7812881Losses:  5.380141258239746 0.5752196311950684
CurrentTrain: epoch  1, batch    34 | loss: 5.9553609Losses:  5.302148342132568 0.6330922245979309
CurrentTrain: epoch  1, batch    35 | loss: 5.9352407Losses:  4.983634948730469 0.5406614542007446
CurrentTrain: epoch  1, batch    36 | loss: 5.5242963Losses:  4.968282699584961 0.4789612591266632
CurrentTrain: epoch  1, batch    37 | loss: 5.4472442Losses:  5.491370677947998 0.6170090436935425
CurrentTrain: epoch  1, batch    38 | loss: 6.1083798Losses:  5.055385112762451 0.6764808297157288
CurrentTrain: epoch  1, batch    39 | loss: 5.7318659Losses:  4.994895935058594 0.6056930422782898
CurrentTrain: epoch  1, batch    40 | loss: 5.6005888Losses:  4.985198020935059 0.6137840151786804
CurrentTrain: epoch  1, batch    41 | loss: 5.5989819Losses:  5.130082130432129 0.46753406524658203
CurrentTrain: epoch  1, batch    42 | loss: 5.5976162Losses:  4.916703224182129 0.41099292039871216
CurrentTrain: epoch  1, batch    43 | loss: 5.3276963Losses:  5.427172660827637 0.5075432062149048
CurrentTrain: epoch  1, batch    44 | loss: 5.9347157Losses:  5.617374897003174 0.6719489097595215
CurrentTrain: epoch  1, batch    45 | loss: 6.2893238Losses:  5.432382583618164 0.6454302072525024
CurrentTrain: epoch  1, batch    46 | loss: 6.0778127Losses:  5.16274356842041 0.6707595586776733
CurrentTrain: epoch  1, batch    47 | loss: 5.8335032Losses:  5.125500679016113 0.5938163995742798
CurrentTrain: epoch  1, batch    48 | loss: 5.7193170Losses:  5.090446472167969 0.6586129665374756
CurrentTrain: epoch  1, batch    49 | loss: 5.7490597Losses:  5.1201958656311035 0.4221635162830353
CurrentTrain: epoch  1, batch    50 | loss: 5.5423594Losses:  5.3028717041015625 0.6285095810890198
CurrentTrain: epoch  1, batch    51 | loss: 5.9313812Losses:  4.829898834228516 0.43875235319137573
CurrentTrain: epoch  1, batch    52 | loss: 5.2686510Losses:  5.235852241516113 0.5946546792984009
CurrentTrain: epoch  1, batch    53 | loss: 5.8305068Losses:  4.840644836425781 0.3817545175552368
CurrentTrain: epoch  1, batch    54 | loss: 5.2223992Losses:  5.083932876586914 0.44654300808906555
CurrentTrain: epoch  1, batch    55 | loss: 5.5304761Losses:  5.116201400756836 0.6144125461578369
CurrentTrain: epoch  1, batch    56 | loss: 5.7306137Losses:  5.304645538330078 0.5318610668182373
CurrentTrain: epoch  1, batch    57 | loss: 5.8365068Losses:  4.926689147949219 0.49499544501304626
CurrentTrain: epoch  1, batch    58 | loss: 5.4216847Losses:  5.068543434143066 0.4666477143764496
CurrentTrain: epoch  1, batch    59 | loss: 5.5351911Losses:  4.701323986053467 0.42495185136795044
CurrentTrain: epoch  1, batch    60 | loss: 5.1262760Losses:  4.805360794067383 0.5077647566795349
CurrentTrain: epoch  1, batch    61 | loss: 5.3131256Losses:  5.6418609619140625 0.4693405330181122
CurrentTrain: epoch  1, batch    62 | loss: 6.1112013Losses:  4.522256851196289 0.3345697522163391
CurrentTrain: epoch  2, batch     0 | loss: 4.8568268Losses:  4.787083625793457 0.45041990280151367
CurrentTrain: epoch  2, batch     1 | loss: 5.2375035Losses:  4.53969144821167 0.4580720067024231
CurrentTrain: epoch  2, batch     2 | loss: 4.9977636Losses:  4.85355806350708 0.27314096689224243
CurrentTrain: epoch  2, batch     3 | loss: 5.1266990Losses:  4.73598051071167 0.44087594747543335
CurrentTrain: epoch  2, batch     4 | loss: 5.1768565Losses:  4.866735458374023 0.4028717279434204
CurrentTrain: epoch  2, batch     5 | loss: 5.2696071Losses:  4.623645782470703 0.3310222327709198
CurrentTrain: epoch  2, batch     6 | loss: 4.9546680Losses:  4.523009300231934 0.4601617157459259
CurrentTrain: epoch  2, batch     7 | loss: 4.9831710Losses:  4.747517108917236 0.4230620861053467
CurrentTrain: epoch  2, batch     8 | loss: 5.1705790Losses:  4.561063289642334 0.3631324768066406
CurrentTrain: epoch  2, batch     9 | loss: 4.9241958Losses:  4.8920135498046875 0.6035299301147461
CurrentTrain: epoch  2, batch    10 | loss: 5.4955435Losses:  4.710618495941162 0.30368584394454956
CurrentTrain: epoch  2, batch    11 | loss: 5.0143042Losses:  4.542588233947754 0.346378892660141
CurrentTrain: epoch  2, batch    12 | loss: 4.8889670Losses:  4.820656776428223 0.4000815749168396
CurrentTrain: epoch  2, batch    13 | loss: 5.2207384Losses:  4.97712516784668 0.3590635657310486
CurrentTrain: epoch  2, batch    14 | loss: 5.3361888Losses:  4.63146448135376 0.31460997462272644
CurrentTrain: epoch  2, batch    15 | loss: 4.9460745Losses:  4.6570587158203125 0.41554394364356995
CurrentTrain: epoch  2, batch    16 | loss: 5.0726027Losses:  4.4422607421875 0.32015472650527954
CurrentTrain: epoch  2, batch    17 | loss: 4.7624154Losses:  4.7793684005737305 0.4306628406047821
CurrentTrain: epoch  2, batch    18 | loss: 5.2100310Losses:  4.631217002868652 0.35974711179733276
CurrentTrain: epoch  2, batch    19 | loss: 4.9909639Losses:  5.368518352508545 0.6038922071456909
CurrentTrain: epoch  2, batch    20 | loss: 5.9724107Losses:  5.129356384277344 0.48019328713417053
CurrentTrain: epoch  2, batch    21 | loss: 5.6095495Losses:  4.737427234649658 0.5541324019432068
CurrentTrain: epoch  2, batch    22 | loss: 5.2915597Losses:  4.75865364074707 0.31232982873916626
CurrentTrain: epoch  2, batch    23 | loss: 5.0709834Losses:  4.379802703857422 0.3004976511001587
CurrentTrain: epoch  2, batch    24 | loss: 4.6803002Losses:  4.500784873962402 0.4710356593132019
CurrentTrain: epoch  2, batch    25 | loss: 4.9718204Losses:  4.526715278625488 0.4454377293586731
CurrentTrain: epoch  2, batch    26 | loss: 4.9721532Losses:  4.695000171661377 0.3583254814147949
CurrentTrain: epoch  2, batch    27 | loss: 5.0533257Losses:  4.629688739776611 0.37747621536254883
CurrentTrain: epoch  2, batch    28 | loss: 5.0071650Losses:  4.583139419555664 0.32702282071113586
CurrentTrain: epoch  2, batch    29 | loss: 4.9101624Losses:  4.662745952606201 0.37461233139038086
CurrentTrain: epoch  2, batch    30 | loss: 5.0373583Losses:  4.9668779373168945 0.45767080783843994
CurrentTrain: epoch  2, batch    31 | loss: 5.4245486Losses:  4.444310188293457 0.2847973704338074
CurrentTrain: epoch  2, batch    32 | loss: 4.7291074Losses:  4.726518630981445 0.303220272064209
CurrentTrain: epoch  2, batch    33 | loss: 5.0297389Losses:  5.021211624145508 0.4649721086025238
CurrentTrain: epoch  2, batch    34 | loss: 5.4861836Losses:  4.452587127685547 0.29358696937561035
CurrentTrain: epoch  2, batch    35 | loss: 4.7461739Losses:  4.549238204956055 0.2683166265487671
CurrentTrain: epoch  2, batch    36 | loss: 4.8175550Losses:  4.385263442993164 0.3094497621059418
CurrentTrain: epoch  2, batch    37 | loss: 4.6947131Losses:  4.480062484741211 0.22562819719314575
CurrentTrain: epoch  2, batch    38 | loss: 4.7056909Losses:  4.6773271560668945 0.3576768636703491
CurrentTrain: epoch  2, batch    39 | loss: 5.0350041Losses:  4.742850303649902 0.25057825446128845
CurrentTrain: epoch  2, batch    40 | loss: 4.9934287Losses:  4.381545543670654 0.32821711897850037
CurrentTrain: epoch  2, batch    41 | loss: 4.7097626Losses:  4.724520683288574 0.2814885377883911
CurrentTrain: epoch  2, batch    42 | loss: 5.0060091Losses:  4.528389930725098 0.3022029399871826
CurrentTrain: epoch  2, batch    43 | loss: 4.8305931Losses:  4.457089900970459 0.24214863777160645
CurrentTrain: epoch  2, batch    44 | loss: 4.6992388Losses:  4.487978935241699 0.3159624934196472
CurrentTrain: epoch  2, batch    45 | loss: 4.8039412Losses:  4.483977794647217 0.35045915842056274
CurrentTrain: epoch  2, batch    46 | loss: 4.8344369Losses:  4.379595756530762 0.23518499732017517
CurrentTrain: epoch  2, batch    47 | loss: 4.6147809Losses:  4.123849868774414 0.15968775749206543
CurrentTrain: epoch  2, batch    48 | loss: 4.2835379Losses:  4.445523262023926 0.2699960768222809
CurrentTrain: epoch  2, batch    49 | loss: 4.7155194Losses:  4.354787826538086 0.2616897225379944
CurrentTrain: epoch  2, batch    50 | loss: 4.6164775Losses:  4.214406967163086 0.2213210165500641
CurrentTrain: epoch  2, batch    51 | loss: 4.4357281Losses:  4.488189697265625 0.25157177448272705
CurrentTrain: epoch  2, batch    52 | loss: 4.7397614Losses:  4.5703935623168945 0.3343591094017029
CurrentTrain: epoch  2, batch    53 | loss: 4.9047527Losses:  4.317205429077148 0.2978210151195526
CurrentTrain: epoch  2, batch    54 | loss: 4.6150265Losses:  4.509505271911621 0.22132742404937744
CurrentTrain: epoch  2, batch    55 | loss: 4.7308326Losses:  4.304817199707031 0.26113444566726685
CurrentTrain: epoch  2, batch    56 | loss: 4.5659518Losses:  4.617582321166992 0.2977513074874878
CurrentTrain: epoch  2, batch    57 | loss: 4.9153337Losses:  4.897858619689941 0.3089120388031006
CurrentTrain: epoch  2, batch    58 | loss: 5.2067709Losses:  4.202363967895508 0.21974200010299683
CurrentTrain: epoch  2, batch    59 | loss: 4.4221058Losses:  4.499362945556641 0.2613309621810913
CurrentTrain: epoch  2, batch    60 | loss: 4.7606940Losses:  4.674709320068359 0.3257705271244049
CurrentTrain: epoch  2, batch    61 | loss: 5.0004797Losses:  4.386331558227539 0.21112839877605438
CurrentTrain: epoch  2, batch    62 | loss: 4.5974598Losses:  4.308122158050537 0.2020343840122223
CurrentTrain: epoch  3, batch     0 | loss: 4.5101566Losses:  4.447271823883057 0.19836744666099548
CurrentTrain: epoch  3, batch     1 | loss: 4.6456394Losses:  4.839450359344482 0.3558894395828247
CurrentTrain: epoch  3, batch     2 | loss: 5.1953397Losses:  4.476263046264648 0.2552548050880432
CurrentTrain: epoch  3, batch     3 | loss: 4.7315178Losses:  4.483996391296387 0.266225129365921
CurrentTrain: epoch  3, batch     4 | loss: 4.7502217Losses:  4.210648059844971 0.17318308353424072
CurrentTrain: epoch  3, batch     5 | loss: 4.3838310Losses:  4.283389091491699 0.2751002609729767
CurrentTrain: epoch  3, batch     6 | loss: 4.5584893Losses:  4.566921710968018 0.2491634339094162
CurrentTrain: epoch  3, batch     7 | loss: 4.8160853Losses:  4.469083309173584 0.2975650727748871
CurrentTrain: epoch  3, batch     8 | loss: 4.7666483Losses:  4.293712139129639 0.25542181730270386
CurrentTrain: epoch  3, batch     9 | loss: 4.5491338Losses:  4.594408988952637 0.28111785650253296
CurrentTrain: epoch  3, batch    10 | loss: 4.8755269Losses:  4.252654075622559 0.19831180572509766
CurrentTrain: epoch  3, batch    11 | loss: 4.4509659Losses:  4.325819492340088 0.12657025456428528
CurrentTrain: epoch  3, batch    12 | loss: 4.4523897Losses:  4.685988426208496 0.360507607460022
CurrentTrain: epoch  3, batch    13 | loss: 5.0464959Losses:  4.4834489822387695 0.22028431296348572
CurrentTrain: epoch  3, batch    14 | loss: 4.7037334Losses:  4.324272155761719 0.24975243210792542
CurrentTrain: epoch  3, batch    15 | loss: 4.5740247Losses:  4.614545822143555 0.3317159414291382
CurrentTrain: epoch  3, batch    16 | loss: 4.9462619Losses:  4.264299392700195 0.17480120062828064
CurrentTrain: epoch  3, batch    17 | loss: 4.4391007Losses:  4.323183059692383 0.2764321267604828
CurrentTrain: epoch  3, batch    18 | loss: 4.5996151Losses:  4.277990341186523 0.16510112583637238
CurrentTrain: epoch  3, batch    19 | loss: 4.4430914Losses:  4.192312717437744 0.22253623604774475
CurrentTrain: epoch  3, batch    20 | loss: 4.4148488Losses:  4.36320686340332 0.23028439283370972
CurrentTrain: epoch  3, batch    21 | loss: 4.5934911Losses:  4.292723655700684 0.23411321640014648
CurrentTrain: epoch  3, batch    22 | loss: 4.5268369Losses:  4.341095447540283 0.31912195682525635
CurrentTrain: epoch  3, batch    23 | loss: 4.6602173Losses:  4.255826473236084 0.24982628226280212
CurrentTrain: epoch  3, batch    24 | loss: 4.5056529Losses:  4.282769203186035 0.2798179090023041
CurrentTrain: epoch  3, batch    25 | loss: 4.5625873Losses:  4.427831649780273 0.18297018110752106
CurrentTrain: epoch  3, batch    26 | loss: 4.6108017Losses:  4.186107635498047 0.23591817915439606
CurrentTrain: epoch  3, batch    27 | loss: 4.4220257Losses:  4.386532783508301 0.22289299964904785
CurrentTrain: epoch  3, batch    28 | loss: 4.6094255Losses:  4.326484680175781 0.26868316531181335
CurrentTrain: epoch  3, batch    29 | loss: 4.5951676Losses:  4.161835193634033 0.24552276730537415
CurrentTrain: epoch  3, batch    30 | loss: 4.4073582Losses:  4.2589945793151855 0.2601052522659302
CurrentTrain: epoch  3, batch    31 | loss: 4.5190997Losses:  4.213987350463867 0.20374958217144012
CurrentTrain: epoch  3, batch    32 | loss: 4.4177370Losses:  4.448211193084717 0.24012061953544617
CurrentTrain: epoch  3, batch    33 | loss: 4.6883316Losses:  4.283276081085205 0.220974862575531
CurrentTrain: epoch  3, batch    34 | loss: 4.5042510Losses:  4.372330188751221 0.30081722140312195
CurrentTrain: epoch  3, batch    35 | loss: 4.6731472Losses:  4.492364406585693 0.3449709415435791
CurrentTrain: epoch  3, batch    36 | loss: 4.8373356Losses:  4.203688621520996 0.24555157124996185
CurrentTrain: epoch  3, batch    37 | loss: 4.4492402Losses:  4.122448921203613 0.23590406775474548
CurrentTrain: epoch  3, batch    38 | loss: 4.3583531Losses:  4.29819393157959 0.20009011030197144
CurrentTrain: epoch  3, batch    39 | loss: 4.4982839Losses:  4.257937431335449 0.1324956715106964
CurrentTrain: epoch  3, batch    40 | loss: 4.3904333Losses:  4.143277168273926 0.17160868644714355
CurrentTrain: epoch  3, batch    41 | loss: 4.3148861Losses:  4.239287376403809 0.19357962906360626
CurrentTrain: epoch  3, batch    42 | loss: 4.4328671Losses:  4.142825126647949 0.18039818108081818
CurrentTrain: epoch  3, batch    43 | loss: 4.3232231Losses:  4.292187690734863 0.22650188207626343
CurrentTrain: epoch  3, batch    44 | loss: 4.5186896Losses:  4.15994930267334 0.20937040448188782
CurrentTrain: epoch  3, batch    45 | loss: 4.3693199Losses:  4.162084102630615 0.2125156819820404
CurrentTrain: epoch  3, batch    46 | loss: 4.3745999Losses:  4.151761054992676 0.17146220803260803
CurrentTrain: epoch  3, batch    47 | loss: 4.3232231Losses:  4.587246894836426 0.3038499653339386
CurrentTrain: epoch  3, batch    48 | loss: 4.8910971Losses:  4.225890159606934 0.22628039121627808
CurrentTrain: epoch  3, batch    49 | loss: 4.4521704Losses:  4.393429756164551 0.1572043001651764
CurrentTrain: epoch  3, batch    50 | loss: 4.5506339Losses:  4.1572418212890625 0.2084602415561676
CurrentTrain: epoch  3, batch    51 | loss: 4.3657022Losses:  4.128260612487793 0.19437330961227417
CurrentTrain: epoch  3, batch    52 | loss: 4.3226337Losses:  4.330212116241455 0.1367679238319397
CurrentTrain: epoch  3, batch    53 | loss: 4.4669800Losses:  4.575229644775391 0.15667682886123657
CurrentTrain: epoch  3, batch    54 | loss: 4.7319064Losses:  4.093292713165283 0.20473243296146393
CurrentTrain: epoch  3, batch    55 | loss: 4.2980251Losses:  4.2814531326293945 0.25023049116134644
CurrentTrain: epoch  3, batch    56 | loss: 4.5316834Losses:  4.223265171051025 0.2559002637863159
CurrentTrain: epoch  3, batch    57 | loss: 4.4791656Losses:  4.214077949523926 0.19989171624183655
CurrentTrain: epoch  3, batch    58 | loss: 4.4139695Losses:  4.10601282119751 0.2016594111919403
CurrentTrain: epoch  3, batch    59 | loss: 4.3076720Losses:  4.129359245300293 0.2515295147895813
CurrentTrain: epoch  3, batch    60 | loss: 4.3808889Losses:  4.340864658355713 0.33319586515426636
CurrentTrain: epoch  3, batch    61 | loss: 4.6740603Losses:  4.1404924392700195 0.11591626703739166
CurrentTrain: epoch  3, batch    62 | loss: 4.2564087Losses:  4.108136177062988 0.16807138919830322
CurrentTrain: epoch  4, batch     0 | loss: 4.2762074Losses:  4.165535926818848 0.19650015234947205
CurrentTrain: epoch  4, batch     1 | loss: 4.3620362Losses:  4.181057929992676 0.1860838532447815
CurrentTrain: epoch  4, batch     2 | loss: 4.3671417Losses:  4.160794734954834 0.22459805011749268
CurrentTrain: epoch  4, batch     3 | loss: 4.3853927Losses:  4.187010288238525 0.21273718774318695
CurrentTrain: epoch  4, batch     4 | loss: 4.3997474Losses:  4.189265251159668 0.16396832466125488
CurrentTrain: epoch  4, batch     5 | loss: 4.3532333Losses:  4.084660530090332 0.16578227281570435
CurrentTrain: epoch  4, batch     6 | loss: 4.2504430Losses:  4.299309730529785 0.2398369163274765
CurrentTrain: epoch  4, batch     7 | loss: 4.5391464Losses:  4.220805644989014 0.18746116757392883
CurrentTrain: epoch  4, batch     8 | loss: 4.4082670Losses:  4.152812957763672 0.23154208064079285
CurrentTrain: epoch  4, batch     9 | loss: 4.3843551Losses:  4.085748672485352 0.18566255271434784
CurrentTrain: epoch  4, batch    10 | loss: 4.2714114Losses:  4.132352828979492 0.13233786821365356
CurrentTrain: epoch  4, batch    11 | loss: 4.2646909Losses:  4.113204479217529 0.21228939294815063
CurrentTrain: epoch  4, batch    12 | loss: 4.3254938Losses:  4.120115280151367 0.19131878018379211
CurrentTrain: epoch  4, batch    13 | loss: 4.3114343Losses:  4.143352031707764 0.15308184921741486
CurrentTrain: epoch  4, batch    14 | loss: 4.2964339Losses:  4.165607929229736 0.21721935272216797
CurrentTrain: epoch  4, batch    15 | loss: 4.3828273Losses:  4.643697738647461 0.19891856610774994
CurrentTrain: epoch  4, batch    16 | loss: 4.8426161Losses:  4.117715358734131 0.13153976202011108
CurrentTrain: epoch  4, batch    17 | loss: 4.2492552Losses:  4.259132385253906 0.22120948135852814
CurrentTrain: epoch  4, batch    18 | loss: 4.4803419Losses:  4.146461486816406 0.19460856914520264
CurrentTrain: epoch  4, batch    19 | loss: 4.3410702Losses:  4.13472843170166 0.18959186971187592
CurrentTrain: epoch  4, batch    20 | loss: 4.3243203Losses:  4.171911239624023 0.13736850023269653
CurrentTrain: epoch  4, batch    21 | loss: 4.3092799Losses:  4.226232528686523 0.201142817735672
CurrentTrain: epoch  4, batch    22 | loss: 4.4273753Losses:  4.1783342361450195 0.19125643372535706
CurrentTrain: epoch  4, batch    23 | loss: 4.3695908Losses:  4.08139705657959 0.13079646229743958
CurrentTrain: epoch  4, batch    24 | loss: 4.2121935Losses:  4.33475399017334 0.1910121589899063
CurrentTrain: epoch  4, batch    25 | loss: 4.5257664Losses:  4.2499494552612305 0.17384013533592224
CurrentTrain: epoch  4, batch    26 | loss: 4.4237895Losses:  4.228073596954346 0.14566433429718018
CurrentTrain: epoch  4, batch    27 | loss: 4.3737378Losses:  4.10966682434082 0.14492742717266083
CurrentTrain: epoch  4, batch    28 | loss: 4.2545943Losses:  4.068630218505859 0.17283634841442108
CurrentTrain: epoch  4, batch    29 | loss: 4.2414665Losses:  4.096310615539551 0.1380654275417328
CurrentTrain: epoch  4, batch    30 | loss: 4.2343760Losses:  4.187599182128906 0.18515686690807343
CurrentTrain: epoch  4, batch    31 | loss: 4.3727560Losses:  4.151697158813477 0.15001189708709717
CurrentTrain: epoch  4, batch    32 | loss: 4.3017092Losses:  4.037740707397461 0.11369380354881287
CurrentTrain: epoch  4, batch    33 | loss: 4.1514344Losses:  4.209185600280762 0.21259711682796478
CurrentTrain: epoch  4, batch    34 | loss: 4.4217825Losses:  4.058415412902832 0.19536007940769196
CurrentTrain: epoch  4, batch    35 | loss: 4.2537756Losses:  4.1501569747924805 0.1285252869129181
CurrentTrain: epoch  4, batch    36 | loss: 4.2786822Losses:  4.145888328552246 0.13532856106758118
CurrentTrain: epoch  4, batch    37 | loss: 4.2812171Losses:  4.1446533203125 0.19458656013011932
CurrentTrain: epoch  4, batch    38 | loss: 4.3392401Losses:  4.194589614868164 0.1890973448753357
CurrentTrain: epoch  4, batch    39 | loss: 4.3836870Losses:  4.051628589630127 0.11982347816228867
CurrentTrain: epoch  4, batch    40 | loss: 4.1714520Losses:  4.082026481628418 0.16518165171146393
CurrentTrain: epoch  4, batch    41 | loss: 4.2472081Losses:  4.143406867980957 0.14686675369739532
CurrentTrain: epoch  4, batch    42 | loss: 4.2902737Losses:  4.393129825592041 0.2027406096458435
CurrentTrain: epoch  4, batch    43 | loss: 4.5958705Losses:  4.08304500579834 0.12915179133415222
CurrentTrain: epoch  4, batch    44 | loss: 4.2121968Losses:  4.093728065490723 0.0822184681892395
CurrentTrain: epoch  4, batch    45 | loss: 4.1759467Losses:  4.053756237030029 0.14921803772449493
CurrentTrain: epoch  4, batch    46 | loss: 4.2029743Losses:  4.1258320808410645 0.1837923526763916
CurrentTrain: epoch  4, batch    47 | loss: 4.3096247Losses:  4.121554374694824 0.1446419209241867
CurrentTrain: epoch  4, batch    48 | loss: 4.2661963Losses:  4.46391487121582 0.2532048523426056
CurrentTrain: epoch  4, batch    49 | loss: 4.7171197Losses:  4.088598251342773 0.14382201433181763
CurrentTrain: epoch  4, batch    50 | loss: 4.2324204Losses:  4.0970869064331055 0.13022275269031525
CurrentTrain: epoch  4, batch    51 | loss: 4.2273097Losses:  4.1472697257995605 0.15233507752418518
CurrentTrain: epoch  4, batch    52 | loss: 4.2996049Losses:  4.060312747955322 0.09532012045383453
CurrentTrain: epoch  4, batch    53 | loss: 4.1556330Losses:  4.1518144607543945 0.18498337268829346
CurrentTrain: epoch  4, batch    54 | loss: 4.3367977Losses:  4.04768180847168 0.16268882155418396
CurrentTrain: epoch  4, batch    55 | loss: 4.2103705Losses:  4.092009544372559 0.19657978415489197
CurrentTrain: epoch  4, batch    56 | loss: 4.2885895Losses:  4.07307767868042 0.12435054779052734
CurrentTrain: epoch  4, batch    57 | loss: 4.1974282Losses:  3.9439611434936523 0.1627010703086853
CurrentTrain: epoch  4, batch    58 | loss: 4.1066623Losses:  4.235653400421143 0.11225683987140656
CurrentTrain: epoch  4, batch    59 | loss: 4.3479104Losses:  4.064103603363037 0.17295221984386444
CurrentTrain: epoch  4, batch    60 | loss: 4.2370558Losses:  4.009641170501709 0.11134275794029236
CurrentTrain: epoch  4, batch    61 | loss: 4.1209841Losses:  4.1162567138671875 0.12183986604213715
CurrentTrain: epoch  4, batch    62 | loss: 4.2380967Losses:  4.088412284851074 0.14916706085205078
CurrentTrain: epoch  5, batch     0 | loss: 4.2375793Losses:  4.069455146789551 0.13381677865982056
CurrentTrain: epoch  5, batch     1 | loss: 4.2032719Losses:  4.050845146179199 0.19980472326278687
CurrentTrain: epoch  5, batch     2 | loss: 4.2506499Losses:  4.128649711608887 0.16367879509925842
CurrentTrain: epoch  5, batch     3 | loss: 4.2923284Losses:  4.074601650238037 0.14824920892715454
CurrentTrain: epoch  5, batch     4 | loss: 4.2228508Losses:  4.044422626495361 0.12010306864976883
CurrentTrain: epoch  5, batch     5 | loss: 4.1645255Losses:  4.065052032470703 0.15825621783733368
CurrentTrain: epoch  5, batch     6 | loss: 4.2233081Losses:  4.064903259277344 0.16960938274860382
CurrentTrain: epoch  5, batch     7 | loss: 4.2345128Losses:  4.053197860717773 0.13689109683036804
CurrentTrain: epoch  5, batch     8 | loss: 4.1900887Losses:  4.01458215713501 0.1550402045249939
CurrentTrain: epoch  5, batch     9 | loss: 4.1696224Losses:  4.153302192687988 0.08818529546260834
CurrentTrain: epoch  5, batch    10 | loss: 4.2414875Losses:  4.04209041595459 0.089694544672966
CurrentTrain: epoch  5, batch    11 | loss: 4.1317849Losses:  4.011913299560547 0.14489495754241943
CurrentTrain: epoch  5, batch    12 | loss: 4.1568084Losses:  4.0174713134765625 0.12113751471042633
CurrentTrain: epoch  5, batch    13 | loss: 4.1386089Losses:  4.091975212097168 0.13574647903442383
CurrentTrain: epoch  5, batch    14 | loss: 4.2277217Losses:  4.065555572509766 0.16875584423542023
CurrentTrain: epoch  5, batch    15 | loss: 4.2343116Losses:  4.065671920776367 0.14847689867019653
CurrentTrain: epoch  5, batch    16 | loss: 4.2141490Losses:  3.986238479614258 0.16557951271533966
CurrentTrain: epoch  5, batch    17 | loss: 4.1518178Losses:  4.056260585784912 0.13455992937088013
CurrentTrain: epoch  5, batch    18 | loss: 4.1908207Losses:  4.110300064086914 0.13485023379325867
CurrentTrain: epoch  5, batch    19 | loss: 4.2451501Losses:  4.016875267028809 0.12012814730405807
CurrentTrain: epoch  5, batch    20 | loss: 4.1370034Losses:  4.091785430908203 0.09904491156339645
CurrentTrain: epoch  5, batch    21 | loss: 4.1908302Losses:  4.033576488494873 0.16754093766212463
CurrentTrain: epoch  5, batch    22 | loss: 4.2011175Losses:  4.0826640129089355 0.11444850265979767
CurrentTrain: epoch  5, batch    23 | loss: 4.1971126Losses:  4.081494331359863 0.1553839147090912
CurrentTrain: epoch  5, batch    24 | loss: 4.2368784Losses:  4.044052600860596 0.08769791573286057
CurrentTrain: epoch  5, batch    25 | loss: 4.1317506Losses:  4.075013160705566 0.15926381945610046
CurrentTrain: epoch  5, batch    26 | loss: 4.2342768Losses:  4.03373908996582 0.1727162003517151
CurrentTrain: epoch  5, batch    27 | loss: 4.2064552Losses:  4.05751895904541 0.13898491859436035
CurrentTrain: epoch  5, batch    28 | loss: 4.1965036Losses:  4.050270080566406 0.14643579721450806
CurrentTrain: epoch  5, batch    29 | loss: 4.1967058Losses:  4.098407745361328 0.14858397841453552
CurrentTrain: epoch  5, batch    30 | loss: 4.2469916Losses:  4.062722682952881 0.10587060451507568
CurrentTrain: epoch  5, batch    31 | loss: 4.1685934Losses:  3.996256113052368 0.12330546975135803
CurrentTrain: epoch  5, batch    32 | loss: 4.1195617Losses:  3.9888405799865723 0.0692054033279419
CurrentTrain: epoch  5, batch    33 | loss: 4.0580459Losses:  4.076394081115723 0.11446041613817215
CurrentTrain: epoch  5, batch    34 | loss: 4.1908545Losses:  4.015270233154297 0.13812017440795898
CurrentTrain: epoch  5, batch    35 | loss: 4.1533904Losses:  4.006438732147217 0.09689938277006149
CurrentTrain: epoch  5, batch    36 | loss: 4.1033382Losses:  3.9941766262054443 0.11683444678783417
CurrentTrain: epoch  5, batch    37 | loss: 4.1110110Losses:  4.009555816650391 0.1211211085319519
CurrentTrain: epoch  5, batch    38 | loss: 4.1306767Losses:  3.9509940147399902 0.11120715737342834
CurrentTrain: epoch  5, batch    39 | loss: 4.0622010Losses:  4.687982559204102 0.2958783209323883
CurrentTrain: epoch  5, batch    40 | loss: 4.9838610Losses:  4.083956241607666 0.11337482929229736
CurrentTrain: epoch  5, batch    41 | loss: 4.1973310Losses:  4.0016655921936035 0.14817336201667786
CurrentTrain: epoch  5, batch    42 | loss: 4.1498389Losses:  4.075912952423096 0.15049585700035095
CurrentTrain: epoch  5, batch    43 | loss: 4.2264090Losses:  4.074257850646973 0.12945440411567688
CurrentTrain: epoch  5, batch    44 | loss: 4.2037125Losses:  3.9948558807373047 0.1398182213306427
CurrentTrain: epoch  5, batch    45 | loss: 4.1346741Losses:  4.152171611785889 0.12684404850006104
CurrentTrain: epoch  5, batch    46 | loss: 4.2790155Losses:  4.048925876617432 0.10357418656349182
CurrentTrain: epoch  5, batch    47 | loss: 4.1525002Losses:  3.998249053955078 0.11364888399839401
CurrentTrain: epoch  5, batch    48 | loss: 4.1118979Losses:  4.110668659210205 0.10549280047416687
CurrentTrain: epoch  5, batch    49 | loss: 4.2161613Losses:  3.976412296295166 0.129075825214386
CurrentTrain: epoch  5, batch    50 | loss: 4.1054883Losses:  4.076767444610596 0.10530149936676025
CurrentTrain: epoch  5, batch    51 | loss: 4.1820688Losses:  4.044894218444824 0.10044988989830017
CurrentTrain: epoch  5, batch    52 | loss: 4.1453443Losses:  4.0106682777404785 0.1709720939397812
CurrentTrain: epoch  5, batch    53 | loss: 4.1816401Losses:  4.05789852142334 0.1387399137020111
CurrentTrain: epoch  5, batch    54 | loss: 4.1966386Losses:  4.054450988769531 0.14200884103775024
CurrentTrain: epoch  5, batch    55 | loss: 4.1964598Losses:  3.983502149581909 0.12127849459648132
CurrentTrain: epoch  5, batch    56 | loss: 4.1047807Losses:  4.059484004974365 0.09579353779554367
CurrentTrain: epoch  5, batch    57 | loss: 4.1552777Losses:  4.071867942810059 0.12321034073829651
CurrentTrain: epoch  5, batch    58 | loss: 4.1950784Losses:  3.9938840866088867 0.13734020292758942
CurrentTrain: epoch  5, batch    59 | loss: 4.1312242Losses:  4.177783012390137 0.17802593111991882
CurrentTrain: epoch  5, batch    60 | loss: 4.3558087Losses:  4.02175760269165 0.08553068339824677
CurrentTrain: epoch  5, batch    61 | loss: 4.1072884Losses:  3.969958543777466 0.052040014415979385
CurrentTrain: epoch  5, batch    62 | loss: 4.0219984Losses:  4.001153945922852 0.08050093054771423
CurrentTrain: epoch  6, batch     0 | loss: 4.0816550Losses:  4.075257301330566 0.08865871280431747
CurrentTrain: epoch  6, batch     1 | loss: 4.1639161Losses:  4.010472297668457 0.10899174958467484
CurrentTrain: epoch  6, batch     2 | loss: 4.1194639Losses:  3.9919540882110596 0.06471123546361923
CurrentTrain: epoch  6, batch     3 | loss: 4.0566654Losses:  4.0290045738220215 0.09305655211210251
CurrentTrain: epoch  6, batch     4 | loss: 4.1220613Losses:  3.9927847385406494 0.1388377994298935
CurrentTrain: epoch  6, batch     5 | loss: 4.1316223Losses:  4.017093181610107 0.11914125829935074
CurrentTrain: epoch  6, batch     6 | loss: 4.1362343Losses:  3.9900667667388916 0.08182178437709808
CurrentTrain: epoch  6, batch     7 | loss: 4.0718884Losses:  3.9884204864501953 0.13460087776184082
CurrentTrain: epoch  6, batch     8 | loss: 4.1230211Losses:  4.006504535675049 0.07619445770978928
CurrentTrain: epoch  6, batch     9 | loss: 4.0826988Losses:  3.9592909812927246 0.09459348022937775
CurrentTrain: epoch  6, batch    10 | loss: 4.0538845Losses:  4.00347900390625 0.06892423331737518
CurrentTrain: epoch  6, batch    11 | loss: 4.0724034Losses:  4.002339839935303 0.07218471169471741
CurrentTrain: epoch  6, batch    12 | loss: 4.0745244Losses:  4.011551856994629 0.08671920001506805
CurrentTrain: epoch  6, batch    13 | loss: 4.0982709Losses:  3.9318184852600098 0.1040986180305481
CurrentTrain: epoch  6, batch    14 | loss: 4.0359173Losses:  4.0432939529418945 0.1505059450864792
CurrentTrain: epoch  6, batch    15 | loss: 4.1938000Losses:  4.071718215942383 0.12004491686820984
CurrentTrain: epoch  6, batch    16 | loss: 4.1917629Losses:  4.00236701965332 0.143429234623909
CurrentTrain: epoch  6, batch    17 | loss: 4.1457963Losses:  4.041152477264404 0.10780671238899231
CurrentTrain: epoch  6, batch    18 | loss: 4.1489592Losses:  3.967750072479248 0.13842558860778809
CurrentTrain: epoch  6, batch    19 | loss: 4.1061754Losses:  4.064874172210693 0.1318800449371338
CurrentTrain: epoch  6, batch    20 | loss: 4.1967545Losses:  4.069113731384277 0.11033712327480316
CurrentTrain: epoch  6, batch    21 | loss: 4.1794510Losses:  4.004414081573486 0.13742955029010773
CurrentTrain: epoch  6, batch    22 | loss: 4.1418438Losses:  4.041981220245361 0.07621804624795914
CurrentTrain: epoch  6, batch    23 | loss: 4.1181993Losses:  4.019077301025391 0.12388458847999573
CurrentTrain: epoch  6, batch    24 | loss: 4.1429620Losses:  4.0124831199646 0.12106121331453323
CurrentTrain: epoch  6, batch    25 | loss: 4.1335444Losses:  3.977597236633301 0.08749355375766754
CurrentTrain: epoch  6, batch    26 | loss: 4.0650907Losses:  3.9444236755371094 0.07540994882583618
CurrentTrain: epoch  6, batch    27 | loss: 4.0198336Losses:  4.00197696685791 0.12507900595664978
CurrentTrain: epoch  6, batch    28 | loss: 4.1270561Losses:  4.073919296264648 0.17473089694976807
CurrentTrain: epoch  6, batch    29 | loss: 4.2486501Losses:  3.9672341346740723 0.09566488862037659
CurrentTrain: epoch  6, batch    30 | loss: 4.0628991Losses:  4.01609992980957 0.06865935027599335
CurrentTrain: epoch  6, batch    31 | loss: 4.0847592Losses:  3.9787309169769287 0.10191082954406738
CurrentTrain: epoch  6, batch    32 | loss: 4.0806417Losses:  3.956517219543457 0.11437226086854935
CurrentTrain: epoch  6, batch    33 | loss: 4.0708895Losses:  3.9858450889587402 0.1159636601805687
CurrentTrain: epoch  6, batch    34 | loss: 4.1018085Losses:  3.9861698150634766 0.11185403913259506
CurrentTrain: epoch  6, batch    35 | loss: 4.0980239Losses:  3.9486083984375 0.0995938628911972
CurrentTrain: epoch  6, batch    36 | loss: 4.0482020Losses:  3.991518974304199 0.13012945652008057
CurrentTrain: epoch  6, batch    37 | loss: 4.1216483Losses:  3.997682571411133 0.13140006363391876
CurrentTrain: epoch  6, batch    38 | loss: 4.1290827Losses:  3.9037225246429443 0.08717069029808044
CurrentTrain: epoch  6, batch    39 | loss: 3.9908931Losses:  3.9579875469207764 0.11000774055719376
CurrentTrain: epoch  6, batch    40 | loss: 4.0679951Losses:  3.9868881702423096 0.09790343046188354
CurrentTrain: epoch  6, batch    41 | loss: 4.0847917Losses:  3.988459825515747 0.09595033526420593
CurrentTrain: epoch  6, batch    42 | loss: 4.0844102Losses:  3.982855796813965 0.08062459528446198
CurrentTrain: epoch  6, batch    43 | loss: 4.0634804Losses:  3.9862728118896484 0.12394218146800995
CurrentTrain: epoch  6, batch    44 | loss: 4.1102152Losses:  3.98551344871521 0.08818753063678741
CurrentTrain: epoch  6, batch    45 | loss: 4.0737009Losses:  3.9835476875305176 0.13824164867401123
CurrentTrain: epoch  6, batch    46 | loss: 4.1217895Losses:  3.998991012573242 0.07116080075502396
CurrentTrain: epoch  6, batch    47 | loss: 4.0701518Losses:  3.928067684173584 0.10055150091648102
CurrentTrain: epoch  6, batch    48 | loss: 4.0286193Losses:  3.9401087760925293 0.10115790367126465
CurrentTrain: epoch  6, batch    49 | loss: 4.0412664Losses:  4.003485202789307 0.11636220663785934
CurrentTrain: epoch  6, batch    50 | loss: 4.1198473Losses:  3.9808156490325928 0.06752316653728485
CurrentTrain: epoch  6, batch    51 | loss: 4.0483389Losses:  4.018862724304199 0.10008285194635391
CurrentTrain: epoch  6, batch    52 | loss: 4.1189456Losses:  3.9697909355163574 0.11974166333675385
CurrentTrain: epoch  6, batch    53 | loss: 4.0895324Losses:  3.951838970184326 0.082829549908638
CurrentTrain: epoch  6, batch    54 | loss: 4.0346684Losses:  3.9854061603546143 0.10028713941574097
CurrentTrain: epoch  6, batch    55 | loss: 4.0856934Losses:  3.9774374961853027 0.0592665895819664
CurrentTrain: epoch  6, batch    56 | loss: 4.0367041Losses:  3.969886541366577 0.11597980558872223
CurrentTrain: epoch  6, batch    57 | loss: 4.0858665Losses:  3.9493300914764404 0.09925492107868195
CurrentTrain: epoch  6, batch    58 | loss: 4.0485849Losses:  3.9729514122009277 0.1109405905008316
CurrentTrain: epoch  6, batch    59 | loss: 4.0838919Losses:  3.9777774810791016 0.08632181584835052
CurrentTrain: epoch  6, batch    60 | loss: 4.0640993Losses:  3.931791067123413 0.09830152988433838
CurrentTrain: epoch  6, batch    61 | loss: 4.0300927Losses:  3.9205169677734375 0.058399979025125504
CurrentTrain: epoch  6, batch    62 | loss: 3.9789169Losses:  3.9563496112823486 0.10525096952915192
CurrentTrain: epoch  7, batch     0 | loss: 4.0616007Losses:  3.991790294647217 0.08869124203920364
CurrentTrain: epoch  7, batch     1 | loss: 4.0804815Losses:  3.954131841659546 0.07300571352243423
CurrentTrain: epoch  7, batch     2 | loss: 4.0271378Losses:  3.980698823928833 0.09203556180000305
CurrentTrain: epoch  7, batch     3 | loss: 4.0727344Losses:  3.9668428897857666 0.1213768720626831
CurrentTrain: epoch  7, batch     4 | loss: 4.0882196Losses:  3.965121269226074 0.055758897215127945
CurrentTrain: epoch  7, batch     5 | loss: 4.0208802Losses:  3.9559693336486816 0.08720093965530396
CurrentTrain: epoch  7, batch     6 | loss: 4.0431705Losses:  3.937859535217285 0.07750838994979858
CurrentTrain: epoch  7, batch     7 | loss: 4.0153680Losses:  3.9565131664276123 0.10027129203081131
CurrentTrain: epoch  7, batch     8 | loss: 4.0567846Losses:  3.9981822967529297 0.09463663399219513
CurrentTrain: epoch  7, batch     9 | loss: 4.0928187Losses:  3.970928192138672 0.1238977387547493
CurrentTrain: epoch  7, batch    10 | loss: 4.0948257Losses:  3.979158878326416 0.08850385248661041
CurrentTrain: epoch  7, batch    11 | loss: 4.0676627Losses:  4.000195503234863 0.11681945621967316
CurrentTrain: epoch  7, batch    12 | loss: 4.1170149Losses:  4.016087532043457 0.12663859128952026
CurrentTrain: epoch  7, batch    13 | loss: 4.1427259Losses:  3.9588463306427 0.11173389852046967
CurrentTrain: epoch  7, batch    14 | loss: 4.0705800Losses:  3.97011137008667 0.080184206366539
CurrentTrain: epoch  7, batch    15 | loss: 4.0502954Losses:  4.014384746551514 0.11310026049613953
CurrentTrain: epoch  7, batch    16 | loss: 4.1274848Losses:  3.977717876434326 0.08534955978393555
CurrentTrain: epoch  7, batch    17 | loss: 4.0630674Losses:  3.95866060256958 0.08205492049455643
CurrentTrain: epoch  7, batch    18 | loss: 4.0407157Losses:  3.9693443775177 0.10135003179311752
CurrentTrain: epoch  7, batch    19 | loss: 4.0706944Losses:  3.948758840560913 0.08732950687408447
CurrentTrain: epoch  7, batch    20 | loss: 4.0360885Losses:  3.984771966934204 0.06661306321620941
CurrentTrain: epoch  7, batch    21 | loss: 4.0513849Losses:  3.9679579734802246 0.06602311879396439
CurrentTrain: epoch  7, batch    22 | loss: 4.0339813Losses:  3.977598190307617 0.0625310018658638
CurrentTrain: epoch  7, batch    23 | loss: 4.0401292Losses:  3.90670108795166 0.08659765124320984
CurrentTrain: epoch  7, batch    24 | loss: 3.9932988Losses:  3.971055030822754 0.12069322168827057
CurrentTrain: epoch  7, batch    25 | loss: 4.0917482Losses:  3.908358573913574 0.030968651175498962
CurrentTrain: epoch  7, batch    26 | loss: 3.9393272Losses:  3.9661664962768555 0.11153870820999146
CurrentTrain: epoch  7, batch    27 | loss: 4.0777054Losses:  3.962005615234375 0.10558585822582245
CurrentTrain: epoch  7, batch    28 | loss: 4.0675917Losses:  3.9541594982147217 0.11022020876407623
CurrentTrain: epoch  7, batch    29 | loss: 4.0643797Losses:  3.979403018951416 0.10969214886426926
CurrentTrain: epoch  7, batch    30 | loss: 4.0890951Losses:  3.9943346977233887 0.06104319170117378
CurrentTrain: epoch  7, batch    31 | loss: 4.0553780Losses:  3.99464750289917 0.12322556227445602
CurrentTrain: epoch  7, batch    32 | loss: 4.1178732Losses:  3.8879528045654297 0.062473997473716736
CurrentTrain: epoch  7, batch    33 | loss: 3.9504268Losses:  3.9860212802886963 0.0765092745423317
CurrentTrain: epoch  7, batch    34 | loss: 4.0625305Losses:  3.9328558444976807 0.1046748012304306
CurrentTrain: epoch  7, batch    35 | loss: 4.0375304Losses:  3.983933448791504 0.08300728350877762
CurrentTrain: epoch  7, batch    36 | loss: 4.0669408Losses:  3.9690005779266357 0.08011236041784286
CurrentTrain: epoch  7, batch    37 | loss: 4.0491128Losses:  3.985447883605957 0.0758545994758606
CurrentTrain: epoch  7, batch    38 | loss: 4.0613027Losses:  3.9677987098693848 0.11278653144836426
CurrentTrain: epoch  7, batch    39 | loss: 4.0805855Losses:  3.990699052810669 0.09778213500976562
CurrentTrain: epoch  7, batch    40 | loss: 4.0884809Losses:  3.9645419120788574 0.09175212681293488
CurrentTrain: epoch  7, batch    41 | loss: 4.0562940Losses:  3.927260160446167 0.10302741080522537
CurrentTrain: epoch  7, batch    42 | loss: 4.0302877Losses:  3.9492151737213135 0.09423528611660004
CurrentTrain: epoch  7, batch    43 | loss: 4.0434504Losses:  3.9523749351501465 0.08634530007839203
CurrentTrain: epoch  7, batch    44 | loss: 4.0387201Losses:  3.9435536861419678 0.0984182357788086
CurrentTrain: epoch  7, batch    45 | loss: 4.0419722Losses:  3.9753079414367676 0.0910857543349266
CurrentTrain: epoch  7, batch    46 | loss: 4.0663939Losses:  3.974938154220581 0.08201895654201508
CurrentTrain: epoch  7, batch    47 | loss: 4.0569572Losses:  3.99013352394104 0.10865596681833267
CurrentTrain: epoch  7, batch    48 | loss: 4.0987897Losses:  3.937833786010742 0.10812728106975555
CurrentTrain: epoch  7, batch    49 | loss: 4.0459609Losses:  3.986022472381592 0.09189286828041077
CurrentTrain: epoch  7, batch    50 | loss: 4.0779152Losses:  4.002270698547363 0.07864443957805634
CurrentTrain: epoch  7, batch    51 | loss: 4.0809150Losses:  3.926771879196167 0.05925282463431358
CurrentTrain: epoch  7, batch    52 | loss: 3.9860246Losses:  3.9536168575286865 0.07504003494977951
CurrentTrain: epoch  7, batch    53 | loss: 4.0286570Losses:  3.9788148403167725 0.1100599467754364
CurrentTrain: epoch  7, batch    54 | loss: 4.0888748Losses:  3.9794578552246094 0.09808281064033508
CurrentTrain: epoch  7, batch    55 | loss: 4.0775409Losses:  3.9786131381988525 0.08646322786808014
CurrentTrain: epoch  7, batch    56 | loss: 4.0650764Losses:  3.948335886001587 0.10610850155353546
CurrentTrain: epoch  7, batch    57 | loss: 4.0544443Losses:  3.953662395477295 0.1007019430398941
CurrentTrain: epoch  7, batch    58 | loss: 4.0543642Losses:  3.949538230895996 0.07808127254247665
CurrentTrain: epoch  7, batch    59 | loss: 4.0276194Losses:  3.953542709350586 0.12980353832244873
CurrentTrain: epoch  7, batch    60 | loss: 4.0833464Losses:  3.978917360305786 0.08654431998729706
CurrentTrain: epoch  7, batch    61 | loss: 4.0654616Losses:  3.97621488571167 0.05704863369464874
CurrentTrain: epoch  7, batch    62 | loss: 4.0332637Losses:  3.9616942405700684 0.10476565361022949
CurrentTrain: epoch  8, batch     0 | loss: 4.0664597Losses:  3.964040517807007 0.1003846675157547
CurrentTrain: epoch  8, batch     1 | loss: 4.0644250Losses:  3.98651123046875 0.07338766753673553
CurrentTrain: epoch  8, batch     2 | loss: 4.0598989Losses:  4.01393985748291 0.11846053600311279
CurrentTrain: epoch  8, batch     3 | loss: 4.1324005Losses:  3.989819049835205 0.07629042118787766
CurrentTrain: epoch  8, batch     4 | loss: 4.0661097Losses:  3.9338743686676025 0.0933031365275383
CurrentTrain: epoch  8, batch     5 | loss: 4.0271773Losses:  3.9711508750915527 0.06184661388397217
CurrentTrain: epoch  8, batch     6 | loss: 4.0329976Losses:  3.9565093517303467 0.10109378397464752
CurrentTrain: epoch  8, batch     7 | loss: 4.0576034Losses:  3.9514245986938477 0.11074388772249222
CurrentTrain: epoch  8, batch     8 | loss: 4.0621686Losses:  3.9787254333496094 0.10121569037437439
CurrentTrain: epoch  8, batch     9 | loss: 4.0799413Losses:  3.9647607803344727 0.09586505591869354
CurrentTrain: epoch  8, batch    10 | loss: 4.0606260Losses:  3.953165054321289 0.09980487078428268
CurrentTrain: epoch  8, batch    11 | loss: 4.0529699Losses:  3.972385883331299 0.05804479122161865
CurrentTrain: epoch  8, batch    12 | loss: 4.0304308Losses:  3.965658187866211 0.10288810729980469
CurrentTrain: epoch  8, batch    13 | loss: 4.0685463Losses:  3.9596805572509766 0.08878403156995773
CurrentTrain: epoch  8, batch    14 | loss: 4.0484648Losses:  3.9675235748291016 0.0944981575012207
CurrentTrain: epoch  8, batch    15 | loss: 4.0620217Losses:  3.9657323360443115 0.0904340073466301
CurrentTrain: epoch  8, batch    16 | loss: 4.0561662Losses:  3.9713237285614014 0.06865338981151581
CurrentTrain: epoch  8, batch    17 | loss: 4.0399771Losses:  3.9969029426574707 0.05742398649454117
CurrentTrain: epoch  8, batch    18 | loss: 4.0543270Losses:  3.977459192276001 0.07941992580890656
CurrentTrain: epoch  8, batch    19 | loss: 4.0568790Losses:  3.9217517375946045 0.07802142947912216
CurrentTrain: epoch  8, batch    20 | loss: 3.9997733Losses:  3.971344470977783 0.06899400055408478
CurrentTrain: epoch  8, batch    21 | loss: 4.0403385Losses:  3.9684996604919434 0.0827980637550354
CurrentTrain: epoch  8, batch    22 | loss: 4.0512977Losses:  3.9469475746154785 0.09604906290769577
CurrentTrain: epoch  8, batch    23 | loss: 4.0429964Losses:  3.923722267150879 0.08158884942531586
CurrentTrain: epoch  8, batch    24 | loss: 4.0053110Losses:  3.9721717834472656 0.10686969757080078
CurrentTrain: epoch  8, batch    25 | loss: 4.0790415Losses:  3.970886707305908 0.09389197081327438
CurrentTrain: epoch  8, batch    26 | loss: 4.0647788Losses:  3.958900213241577 0.09049803018569946
CurrentTrain: epoch  8, batch    27 | loss: 4.0493984Losses:  3.9243991374969482 0.08553176373243332
CurrentTrain: epoch  8, batch    28 | loss: 4.0099311Losses:  3.93711519241333 0.09331682324409485
CurrentTrain: epoch  8, batch    29 | loss: 4.0304322Losses:  3.91605806350708 0.09198145568370819
CurrentTrain: epoch  8, batch    30 | loss: 4.0080395Losses:  3.9364709854125977 0.08686208724975586
CurrentTrain: epoch  8, batch    31 | loss: 4.0233331Losses:  3.9534976482391357 0.10284097492694855
CurrentTrain: epoch  8, batch    32 | loss: 4.0563388Losses:  3.951045513153076 0.09502924233675003
CurrentTrain: epoch  8, batch    33 | loss: 4.0460749Losses:  3.9635865688323975 0.09937383234500885
CurrentTrain: epoch  8, batch    34 | loss: 4.0629606Losses:  3.938647508621216 0.0698702409863472
CurrentTrain: epoch  8, batch    35 | loss: 4.0085177Losses:  3.9707117080688477 0.05131614953279495
CurrentTrain: epoch  8, batch    36 | loss: 4.0220280Losses:  3.9155466556549072 0.09690085053443909
CurrentTrain: epoch  8, batch    37 | loss: 4.0124474Losses:  3.950660228729248 0.09332390129566193
CurrentTrain: epoch  8, batch    38 | loss: 4.0439839Losses:  3.9095540046691895 0.07170988619327545
CurrentTrain: epoch  8, batch    39 | loss: 3.9812639Losses:  3.9722390174865723 0.08503274619579315
CurrentTrain: epoch  8, batch    40 | loss: 4.0572720Losses:  3.985839366912842 0.06608617305755615
CurrentTrain: epoch  8, batch    41 | loss: 4.0519257Losses:  3.943044424057007 0.05378124117851257
CurrentTrain: epoch  8, batch    42 | loss: 3.9968257Losses:  3.9588711261749268 0.08310246467590332
CurrentTrain: epoch  8, batch    43 | loss: 4.0419736Losses:  3.980959415435791 0.08371533453464508
CurrentTrain: epoch  8, batch    44 | loss: 4.0646749Losses:  3.96464204788208 0.08140884339809418
CurrentTrain: epoch  8, batch    45 | loss: 4.0460510Losses:  3.9328644275665283 0.07267167419195175
CurrentTrain: epoch  8, batch    46 | loss: 4.0055361Losses:  3.9704675674438477 0.06132078915834427
CurrentTrain: epoch  8, batch    47 | loss: 4.0317883Losses:  3.985419511795044 0.07754889130592346
CurrentTrain: epoch  8, batch    48 | loss: 4.0629683Losses:  3.924102306365967 0.057716138660907745
CurrentTrain: epoch  8, batch    49 | loss: 3.9818184Losses:  3.9404265880584717 0.09307920932769775
CurrentTrain: epoch  8, batch    50 | loss: 4.0335059Losses:  3.9567246437072754 0.06334200501441956
CurrentTrain: epoch  8, batch    51 | loss: 4.0200667Losses:  3.986057758331299 0.09341885149478912
CurrentTrain: epoch  8, batch    52 | loss: 4.0794768Losses:  3.9376707077026367 0.05714130401611328
CurrentTrain: epoch  8, batch    53 | loss: 3.9948120Losses:  3.9682610034942627 0.07354886829853058
CurrentTrain: epoch  8, batch    54 | loss: 4.0418100Losses:  3.944526195526123 0.07835683226585388
CurrentTrain: epoch  8, batch    55 | loss: 4.0228829Losses:  3.955244541168213 0.08909624069929123
CurrentTrain: epoch  8, batch    56 | loss: 4.0443406Losses:  3.9727578163146973 0.08674784749746323
CurrentTrain: epoch  8, batch    57 | loss: 4.0595055Losses:  3.927499771118164 0.08853079378604889
CurrentTrain: epoch  8, batch    58 | loss: 4.0160308Losses:  3.9923317432403564 0.05044557526707649
CurrentTrain: epoch  8, batch    59 | loss: 4.0427775Losses:  3.9290056228637695 0.06236959993839264
CurrentTrain: epoch  8, batch    60 | loss: 3.9913752Losses:  3.924933433532715 0.09166540205478668
CurrentTrain: epoch  8, batch    61 | loss: 4.0165987Losses:  3.9542815685272217 0.05094229802489281
CurrentTrain: epoch  8, batch    62 | loss: 4.0052238Losses:  3.908806324005127 0.04736891761422157
CurrentTrain: epoch  9, batch     0 | loss: 3.9561753Losses:  3.957108497619629 0.0906820222735405
CurrentTrain: epoch  9, batch     1 | loss: 4.0477905Losses:  3.9589145183563232 0.07391046732664108
CurrentTrain: epoch  9, batch     2 | loss: 4.0328250Losses:  3.917056083679199 0.08045094460248947
CurrentTrain: epoch  9, batch     3 | loss: 3.9975071Losses:  3.912398099899292 0.07992572337388992
CurrentTrain: epoch  9, batch     4 | loss: 3.9923239Losses:  3.980236768722534 0.07099686563014984
CurrentTrain: epoch  9, batch     5 | loss: 4.0512338Losses:  3.9216055870056152 0.05946078896522522
CurrentTrain: epoch  9, batch     6 | loss: 3.9810665Losses:  3.925579786300659 0.09205713123083115
CurrentTrain: epoch  9, batch     7 | loss: 4.0176368Losses:  3.937070846557617 0.09811817109584808
CurrentTrain: epoch  9, batch     8 | loss: 4.0351892Losses:  3.9820258617401123 0.0776238813996315
CurrentTrain: epoch  9, batch     9 | loss: 4.0596499Losses:  3.9856250286102295 0.09176905453205109
CurrentTrain: epoch  9, batch    10 | loss: 4.0773940Losses:  3.9566915035247803 0.08994782716035843
CurrentTrain: epoch  9, batch    11 | loss: 4.0466394Losses:  3.9665589332580566 0.07194870710372925
CurrentTrain: epoch  9, batch    12 | loss: 4.0385075Losses:  3.958315134048462 0.07813552021980286
CurrentTrain: epoch  9, batch    13 | loss: 4.0364509Losses:  3.9450957775115967 0.07500582188367844
CurrentTrain: epoch  9, batch    14 | loss: 4.0201015Losses:  3.9966025352478027 0.04342886060476303
CurrentTrain: epoch  9, batch    15 | loss: 4.0400314Losses:  3.9480209350585938 0.09016934037208557
CurrentTrain: epoch  9, batch    16 | loss: 4.0381904Losses:  3.956878662109375 0.07142481207847595
CurrentTrain: epoch  9, batch    17 | loss: 4.0283036Losses:  3.942054033279419 0.04269418492913246
CurrentTrain: epoch  9, batch    18 | loss: 3.9847481Losses:  3.9406538009643555 0.05314358323812485
CurrentTrain: epoch  9, batch    19 | loss: 3.9937973Losses:  3.957754611968994 0.07406826317310333
CurrentTrain: epoch  9, batch    20 | loss: 4.0318227Losses:  3.9511797428131104 0.07764044404029846
CurrentTrain: epoch  9, batch    21 | loss: 4.0288200Losses:  3.9422435760498047 0.06357906013727188
CurrentTrain: epoch  9, batch    22 | loss: 4.0058227Losses:  3.9854860305786133 0.0640629306435585
CurrentTrain: epoch  9, batch    23 | loss: 4.0495491Losses:  3.8785176277160645 0.06082761660218239
CurrentTrain: epoch  9, batch    24 | loss: 3.9393454Losses:  3.9915030002593994 0.06559184193611145
CurrentTrain: epoch  9, batch    25 | loss: 4.0570951Losses:  3.9554572105407715 0.06016230583190918
CurrentTrain: epoch  9, batch    26 | loss: 4.0156193Losses:  3.8714821338653564 0.08976709097623825
CurrentTrain: epoch  9, batch    27 | loss: 3.9612491Losses:  3.966498851776123 0.0857415646314621
CurrentTrain: epoch  9, batch    28 | loss: 4.0522404Losses:  3.9592156410217285 0.07229772955179214
CurrentTrain: epoch  9, batch    29 | loss: 4.0315132Losses:  3.9679036140441895 0.08897051215171814
CurrentTrain: epoch  9, batch    30 | loss: 4.0568743Losses:  3.9606988430023193 0.08431626856327057
CurrentTrain: epoch  9, batch    31 | loss: 4.0450153Losses:  3.9774837493896484 0.05439770221710205
CurrentTrain: epoch  9, batch    32 | loss: 4.0318813Losses:  3.942519187927246 0.09119872748851776
CurrentTrain: epoch  9, batch    33 | loss: 4.0337181Losses:  3.951153516769409 0.06637696921825409
CurrentTrain: epoch  9, batch    34 | loss: 4.0175304Losses:  3.9489755630493164 0.06453549116849899
CurrentTrain: epoch  9, batch    35 | loss: 4.0135112Losses:  3.9148759841918945 0.04876983165740967
CurrentTrain: epoch  9, batch    36 | loss: 3.9636459Losses:  3.956075668334961 0.07061323523521423
CurrentTrain: epoch  9, batch    37 | loss: 4.0266891Losses:  3.9403886795043945 0.08294719457626343
CurrentTrain: epoch  9, batch    38 | loss: 4.0233359Losses:  3.956298828125 0.08012715727090836
CurrentTrain: epoch  9, batch    39 | loss: 4.0364261Losses:  3.961146831512451 0.07968834042549133
CurrentTrain: epoch  9, batch    40 | loss: 4.0408354Losses:  3.9344546794891357 0.08629706501960754
CurrentTrain: epoch  9, batch    41 | loss: 4.0207520Losses:  3.9505081176757812 0.08856044709682465
CurrentTrain: epoch  9, batch    42 | loss: 4.0390687Losses:  3.911228656768799 0.08406208455562592
CurrentTrain: epoch  9, batch    43 | loss: 3.9952908Losses:  3.935558319091797 0.07641630619764328
CurrentTrain: epoch  9, batch    44 | loss: 4.0119748Losses:  3.9303412437438965 0.0754861980676651
CurrentTrain: epoch  9, batch    45 | loss: 4.0058274Losses:  3.930474281311035 0.06714870780706406
CurrentTrain: epoch  9, batch    46 | loss: 3.9976230Losses:  3.945852756500244 0.08330148458480835
CurrentTrain: epoch  9, batch    47 | loss: 4.0291543Losses:  3.9373817443847656 0.07386747002601624
CurrentTrain: epoch  9, batch    48 | loss: 4.0112491Losses:  3.9619293212890625 0.07704997062683105
CurrentTrain: epoch  9, batch    49 | loss: 4.0389795Losses:  3.942596435546875 0.07863310724496841
CurrentTrain: epoch  9, batch    50 | loss: 4.0212297Losses:  3.9606404304504395 0.05721982195973396
CurrentTrain: epoch  9, batch    51 | loss: 4.0178604Losses:  3.9565601348876953 0.08343487232923508
CurrentTrain: epoch  9, batch    52 | loss: 4.0399952Losses:  3.9109201431274414 0.10254265367984772
CurrentTrain: epoch  9, batch    53 | loss: 4.0134630Losses:  3.9177236557006836 0.09069167077541351
CurrentTrain: epoch  9, batch    54 | loss: 4.0084152Losses:  3.94793963432312 0.06631787121295929
CurrentTrain: epoch  9, batch    55 | loss: 4.0142574Losses:  3.957090377807617 0.0710529014468193
CurrentTrain: epoch  9, batch    56 | loss: 4.0281434Losses:  3.945478916168213 0.07219808548688889
CurrentTrain: epoch  9, batch    57 | loss: 4.0176768Losses:  3.9373116493225098 0.08564493060112
CurrentTrain: epoch  9, batch    58 | loss: 4.0229564Losses:  3.9250550270080566 0.07825598120689392
CurrentTrain: epoch  9, batch    59 | loss: 4.0033112Losses:  3.9309699535369873 0.0702258050441742
CurrentTrain: epoch  9, batch    60 | loss: 4.0011959Losses:  3.9319820404052734 0.08017290383577347
CurrentTrain: epoch  9, batch    61 | loss: 4.0121551Losses:  3.907346248626709 0.027292940765619278
CurrentTrain: epoch  9, batch    62 | loss: 3.9346392
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 94.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.75%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 94.55%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 94.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 94.35%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.34%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.20%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.39%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.75%   
cur_acc:  ['0.9375']
his_acc:  ['0.9375']
Clustering into  9  clusters
Clusters:  [0 5 4 0 0 0 0 0 8 7 0 3 0 0 0 6 0 1 0 2]
Losses:  8.1256742477417 1.4026823043823242
CurrentTrain: epoch  0, batch     0 | loss: 9.5283566Losses:  10.308032989501953 1.86864173412323
CurrentTrain: epoch  0, batch     1 | loss: 12.1766748Losses:  7.890231132507324 1.7150180339813232
CurrentTrain: epoch  0, batch     2 | loss: 9.6052494Losses:  4.932927131652832 0.4702807068824768
CurrentTrain: epoch  0, batch     3 | loss: 5.4032078Losses:  4.511703968048096 1.7994704246520996
CurrentTrain: epoch  1, batch     0 | loss: 6.3111744Losses:  4.448551177978516 1.1858031749725342
CurrentTrain: epoch  1, batch     1 | loss: 5.6343546Losses:  4.227754592895508 1.466921091079712
CurrentTrain: epoch  1, batch     2 | loss: 5.6946754Losses:  6.844067573547363 0.35021132230758667
CurrentTrain: epoch  1, batch     3 | loss: 7.1942787Losses:  3.8846917152404785 1.3073394298553467
CurrentTrain: epoch  2, batch     0 | loss: 5.1920309Losses:  3.50864315032959 1.3524141311645508
CurrentTrain: epoch  2, batch     1 | loss: 4.8610573Losses:  4.721669673919678 1.3050771951675415
CurrentTrain: epoch  2, batch     2 | loss: 6.0267467Losses:  3.729304552078247 0.24863819777965546
CurrentTrain: epoch  2, batch     3 | loss: 3.9779427Losses:  3.309803009033203 1.3819222450256348
CurrentTrain: epoch  3, batch     0 | loss: 4.6917253Losses:  3.5472664833068848 1.2690765857696533
CurrentTrain: epoch  3, batch     1 | loss: 4.8163433Losses:  3.7212798595428467 1.1856964826583862
CurrentTrain: epoch  3, batch     2 | loss: 4.9069762Losses:  3.295867443084717 0.46989503502845764
CurrentTrain: epoch  3, batch     3 | loss: 3.7657626Losses:  3.3711085319519043 1.3363546133041382
CurrentTrain: epoch  4, batch     0 | loss: 4.7074633Losses:  3.754211664199829 1.4096230268478394
CurrentTrain: epoch  4, batch     1 | loss: 5.1638346Losses:  2.826181173324585 1.0384114980697632
CurrentTrain: epoch  4, batch     2 | loss: 3.8645926Losses:  3.9371161460876465 0.21026280522346497
CurrentTrain: epoch  4, batch     3 | loss: 4.1473789Losses:  3.423053026199341 1.3649187088012695
CurrentTrain: epoch  5, batch     0 | loss: 4.7879715Losses:  3.2275986671447754 1.2587435245513916
CurrentTrain: epoch  5, batch     1 | loss: 4.4863424Losses:  2.938173770904541 1.1375946998596191
CurrentTrain: epoch  5, batch     2 | loss: 4.0757685Losses:  1.747829794883728 8.94069742685133e-08
CurrentTrain: epoch  5, batch     3 | loss: 1.7478299Losses:  2.624215602874756 0.9687137603759766
CurrentTrain: epoch  6, batch     0 | loss: 3.5929294Losses:  3.044151544570923 1.2754172086715698
CurrentTrain: epoch  6, batch     1 | loss: 4.3195686Losses:  3.1759324073791504 1.2078120708465576
CurrentTrain: epoch  6, batch     2 | loss: 4.3837442Losses:  3.013484001159668 0.2149336338043213
CurrentTrain: epoch  6, batch     3 | loss: 3.2284176Losses:  2.249709129333496 0.5832170248031616
CurrentTrain: epoch  7, batch     0 | loss: 2.8329263Losses:  3.0575456619262695 1.0712178945541382
CurrentTrain: epoch  7, batch     1 | loss: 4.1287637Losses:  3.193727731704712 1.2353856563568115
CurrentTrain: epoch  7, batch     2 | loss: 4.4291134Losses:  2.5995826721191406 0.18392197787761688
CurrentTrain: epoch  7, batch     3 | loss: 2.7835047Losses:  2.7094783782958984 1.0018290281295776
CurrentTrain: epoch  8, batch     0 | loss: 3.7113075Losses:  2.766777992248535 1.0859707593917847
CurrentTrain: epoch  8, batch     1 | loss: 3.8527489Losses:  2.7158725261688232 0.8530368804931641
CurrentTrain: epoch  8, batch     2 | loss: 3.5689094Losses:  1.7669751644134521 5.960464477539063e-08
CurrentTrain: epoch  8, batch     3 | loss: 1.7669752Losses:  2.248643636703491 0.9171103239059448
CurrentTrain: epoch  9, batch     0 | loss: 3.1657538Losses:  2.8863697052001953 1.0393571853637695
CurrentTrain: epoch  9, batch     1 | loss: 3.9257269Losses:  2.629333734512329 0.9505817890167236
CurrentTrain: epoch  9, batch     2 | loss: 3.5799155Losses:  2.72701096534729 0.09108437597751617
CurrentTrain: epoch  9, batch     3 | loss: 2.8180954
Losses:  5.240950584411621 0.9260385632514954
MemoryTrain:  epoch  0, batch     0 | loss: 6.1669893Losses:  10.13895320892334 0.1627882570028305
MemoryTrain:  epoch  0, batch     1 | loss: 10.3017416Losses:  0.5106918811798096 0.9443382024765015
MemoryTrain:  epoch  1, batch     0 | loss: 1.4550301Losses:  0.9575126767158508 0.21715320646762848
MemoryTrain:  epoch  1, batch     1 | loss: 1.1746659Losses:  0.5206873416900635 0.764046311378479
MemoryTrain:  epoch  2, batch     0 | loss: 1.2847337Losses:  0.20837102830410004 0.2269723266363144
MemoryTrain:  epoch  2, batch     1 | loss: 0.4353434Losses:  0.3524166941642761 0.8175762295722961
MemoryTrain:  epoch  3, batch     0 | loss: 1.1699929Losses:  0.12358447909355164 0.19323527812957764
MemoryTrain:  epoch  3, batch     1 | loss: 0.3168198Losses:  0.22643816471099854 0.7850743532180786
MemoryTrain:  epoch  4, batch     0 | loss: 1.0115125Losses:  0.20339688658714294 0.16823537647724152
MemoryTrain:  epoch  4, batch     1 | loss: 0.3716323Losses:  0.17522171139717102 0.7923294305801392
MemoryTrain:  epoch  5, batch     0 | loss: 0.9675511Losses:  0.13917309045791626 0.07166831195354462
MemoryTrain:  epoch  5, batch     1 | loss: 0.2108414Losses:  0.19084800779819489 0.7697709798812866
MemoryTrain:  epoch  6, batch     0 | loss: 0.9606190Losses:  0.1313198059797287 0.11178357899188995
MemoryTrain:  epoch  6, batch     1 | loss: 0.2431034Losses:  0.15691640973091125 0.5916018486022949
MemoryTrain:  epoch  7, batch     0 | loss: 0.7485182Losses:  0.2818683981895447 0.33429694175720215
MemoryTrain:  epoch  7, batch     1 | loss: 0.6161653Losses:  0.17637653648853302 0.7739014625549316
MemoryTrain:  epoch  8, batch     0 | loss: 0.9502780Losses:  0.09221354126930237 0.17195245623588562
MemoryTrain:  epoch  8, batch     1 | loss: 0.2641660Losses:  0.13583751022815704 0.6986877918243408
MemoryTrain:  epoch  9, batch     0 | loss: 0.8345253Losses:  0.1160062700510025 0.08006661385297775
MemoryTrain:  epoch  9, batch     1 | loss: 0.1960729
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 66.29%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 64.66%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 61.09%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 61.52%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 62.31%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 63.05%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 64.41%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 69.81%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 70.91%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 71.62%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.23%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.85%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.49%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.81%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.90%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.98%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.95%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 95.05%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.87%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.74%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 94.49%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.48%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 94.26%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 93.55%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 93.08%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 92.90%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 92.26%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 92.10%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 92.12%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 92.17%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 91.53%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 90.67%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 89.74%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 89.00%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 88.36%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 87.50%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 87.35%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 87.86%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 88.00%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 87.64%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 86.80%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 86.04%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 85.30%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 84.51%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 83.67%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 83.18%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 83.27%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 83.31%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 83.29%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.60%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 84.29%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 84.14%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 84.00%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 83.64%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 83.39%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 83.11%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 82.97%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 83.00%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 82.93%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 83.03%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 83.01%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 83.05%   
cur_acc:  ['0.9375', '0.7123']
his_acc:  ['0.9375', '0.8305']
Clustering into  14  clusters
Clusters:  [ 2 11  9  2  2  2  0  2  8  7  0 13 10  2  2  6  2  1  2 12  2  5  2  2
  1  2  2  2  3  4]
Losses:  7.278506278991699 1.3179534673690796
CurrentTrain: epoch  0, batch     0 | loss: 8.5964594Losses:  10.217328071594238 1.2802983522415161
CurrentTrain: epoch  0, batch     1 | loss: 11.4976263Losses:  8.731912612915039 1.3858176469802856
CurrentTrain: epoch  0, batch     2 | loss: 10.1177301Losses:  5.085504531860352 0.6103782057762146
CurrentTrain: epoch  0, batch     3 | loss: 5.6958828Losses:  4.804843902587891 1.3374115228652954
CurrentTrain: epoch  1, batch     0 | loss: 6.1422553Losses:  4.220905303955078 1.2470557689666748
CurrentTrain: epoch  1, batch     1 | loss: 5.4679613Losses:  3.577042579650879 1.1758546829223633
CurrentTrain: epoch  1, batch     2 | loss: 4.7528973Losses:  4.162747383117676 1.4901162614933128e-07
CurrentTrain: epoch  1, batch     3 | loss: 4.1627474Losses:  3.8212292194366455 1.4610393047332764
CurrentTrain: epoch  2, batch     0 | loss: 5.2822685Losses:  4.231579780578613 1.5066064596176147
CurrentTrain: epoch  2, batch     1 | loss: 5.7381864Losses:  3.919140577316284 1.429189920425415
CurrentTrain: epoch  2, batch     2 | loss: 5.3483305Losses:  2.5539298057556152 0.3660280108451843
CurrentTrain: epoch  2, batch     3 | loss: 2.9199579Losses:  3.6263551712036133 1.2627513408660889
CurrentTrain: epoch  3, batch     0 | loss: 4.8891068Losses:  3.441542148590088 1.3151500225067139
CurrentTrain: epoch  3, batch     1 | loss: 4.7566919Losses:  3.7733826637268066 1.1450743675231934
CurrentTrain: epoch  3, batch     2 | loss: 4.9184570Losses:  3.867922782897949 0.3619517683982849
CurrentTrain: epoch  3, batch     3 | loss: 4.2298746Losses:  3.584606885910034 1.0648406744003296
CurrentTrain: epoch  4, batch     0 | loss: 4.6494474Losses:  3.2803077697753906 1.063335657119751
CurrentTrain: epoch  4, batch     1 | loss: 4.3436432Losses:  3.2436094284057617 1.2013746500015259
CurrentTrain: epoch  4, batch     2 | loss: 4.4449840Losses:  5.226261138916016 0.40661418437957764
CurrentTrain: epoch  4, batch     3 | loss: 5.6328754Losses:  3.401440143585205 1.058079719543457
CurrentTrain: epoch  5, batch     0 | loss: 4.4595199Losses:  3.51467227935791 1.177046775817871
CurrentTrain: epoch  5, batch     1 | loss: 4.6917191Losses:  2.6807491779327393 0.7788478136062622
CurrentTrain: epoch  5, batch     2 | loss: 3.4595971Losses:  3.069286823272705 0.19685396552085876
CurrentTrain: epoch  5, batch     3 | loss: 3.2661407Losses:  2.8215842247009277 0.9219570755958557
CurrentTrain: epoch  6, batch     0 | loss: 3.7435412Losses:  3.0548086166381836 1.2288264036178589
CurrentTrain: epoch  6, batch     1 | loss: 4.2836351Losses:  3.143857002258301 1.051910400390625
CurrentTrain: epoch  6, batch     2 | loss: 4.1957674Losses:  2.2574853897094727 0.18691062927246094
CurrentTrain: epoch  6, batch     3 | loss: 2.4443960Losses:  3.1326799392700195 1.1491765975952148
CurrentTrain: epoch  7, batch     0 | loss: 4.2818565Losses:  3.103297233581543 1.1028926372528076
CurrentTrain: epoch  7, batch     1 | loss: 4.2061901Losses:  2.16349196434021 0.7543609142303467
CurrentTrain: epoch  7, batch     2 | loss: 2.9178529Losses:  3.7310798168182373 8.94069742685133e-08
CurrentTrain: epoch  7, batch     3 | loss: 3.7310798Losses:  3.1479787826538086 0.9619896411895752
CurrentTrain: epoch  8, batch     0 | loss: 4.1099682Losses:  2.515507698059082 0.804908275604248
CurrentTrain: epoch  8, batch     1 | loss: 3.3204160Losses:  2.706547498703003 1.0562591552734375
CurrentTrain: epoch  8, batch     2 | loss: 3.7628067Losses:  1.9165053367614746 0.16112755239009857
CurrentTrain: epoch  8, batch     3 | loss: 2.0776329Losses:  3.147085189819336 0.9601855874061584
CurrentTrain: epoch  9, batch     0 | loss: 4.1072707Losses:  2.3335041999816895 0.8210722208023071
CurrentTrain: epoch  9, batch     1 | loss: 3.1545763Losses:  2.378941535949707 0.924527645111084
CurrentTrain: epoch  9, batch     2 | loss: 3.3034692Losses:  2.612365245819092 1.1920930376163597e-07
CurrentTrain: epoch  9, batch     3 | loss: 2.6123655
Losses:  5.648261070251465 1.1859045028686523
MemoryTrain:  epoch  0, batch     0 | loss: 6.8341656Losses:  9.967733383178711 0.590427815914154
MemoryTrain:  epoch  0, batch     1 | loss: 10.5581608Losses:  1.049149513244629 1.0673198699951172
MemoryTrain:  epoch  1, batch     0 | loss: 2.1164694Losses:  0.6775903105735779 0.6269590854644775
MemoryTrain:  epoch  1, batch     1 | loss: 1.3045495Losses:  0.5367477536201477 0.8601171970367432
MemoryTrain:  epoch  2, batch     0 | loss: 1.3968649Losses:  0.65285724401474 0.8033115267753601
MemoryTrain:  epoch  2, batch     1 | loss: 1.4561688Losses:  0.4292854964733124 0.7551088333129883
MemoryTrain:  epoch  3, batch     0 | loss: 1.1843944Losses:  0.551213264465332 0.8701663613319397
MemoryTrain:  epoch  3, batch     1 | loss: 1.4213796Losses:  0.42337194085121155 0.946323812007904
MemoryTrain:  epoch  4, batch     0 | loss: 1.3696958Losses:  0.339131236076355 0.6555683016777039
MemoryTrain:  epoch  4, batch     1 | loss: 0.9946995Losses:  0.28559795022010803 0.6392183303833008
MemoryTrain:  epoch  5, batch     0 | loss: 0.9248163Losses:  0.3961447477340698 1.0382065773010254
MemoryTrain:  epoch  5, batch     1 | loss: 1.4343513Losses:  0.2627812922000885 0.8161932229995728
MemoryTrain:  epoch  6, batch     0 | loss: 1.0789745Losses:  0.3763805329799652 0.6572517156600952
MemoryTrain:  epoch  6, batch     1 | loss: 1.0336323Losses:  0.2548168897628784 0.7766623497009277
MemoryTrain:  epoch  7, batch     0 | loss: 1.0314792Losses:  0.2936677038669586 0.7005526423454285
MemoryTrain:  epoch  7, batch     1 | loss: 0.9942204Losses:  0.2506190240383148 0.7432326078414917
MemoryTrain:  epoch  8, batch     0 | loss: 0.9938517Losses:  0.27783772349357605 0.7317137122154236
MemoryTrain:  epoch  8, batch     1 | loss: 1.0095514Losses:  0.2711787223815918 0.6524279117584229
MemoryTrain:  epoch  9, batch     0 | loss: 0.9236066Losses:  0.24874071776866913 0.7150387167930603
MemoryTrain:  epoch  9, batch     1 | loss: 0.9637794
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 77.76%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 78.67%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 79.78%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 78.97%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 78.18%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 77.78%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 77.01%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 76.54%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 76.29%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 76.06%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.99%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.95%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.60%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.14%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.30%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.86%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 92.56%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 92.06%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 91.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.50%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 91.13%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 90.97%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 89.81%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 89.27%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 89.15%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 89.22%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 89.35%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 87.99%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 87.10%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 86.39%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 85.78%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 85.03%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 84.91%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 85.09%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 85.37%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 85.54%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 85.70%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 85.30%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 84.48%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 83.68%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 82.90%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 82.13%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 81.32%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 80.78%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 80.92%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 80.93%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 81.12%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 81.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 81.37%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 81.66%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 81.13%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 80.68%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 80.17%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 79.73%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 79.13%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 78.93%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 78.73%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 78.72%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.79%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.73%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.85%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 79.23%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 79.07%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 78.74%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 78.32%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 78.00%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 77.50%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 77.19%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 77.23%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.99%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 77.83%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 77.68%   [EVAL] batch:  140 | acc: 50.00%,  total acc: 77.48%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 77.38%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 77.32%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 77.17%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 77.48%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 77.70%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 78.21%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 78.78%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 78.97%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.15%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 79.27%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 79.20%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 79.22%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 79.19%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 78.98%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 78.99%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 79.20%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 79.64%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 79.51%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 79.27%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 79.04%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 78.66%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 78.50%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 78.42%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 78.33%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 78.34%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 78.36%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 78.44%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 78.26%   
cur_acc:  ['0.9375', '0.7123', '0.7599']
his_acc:  ['0.9375', '0.8305', '0.7826']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15 17 16  2  1  1 12  1 18  1  8  1 14  1  1
  7  1  1  1  3  6  2  1  5  1 10  0  4  1  1  1]
Losses:  7.1564202308654785 1.5702605247497559
CurrentTrain: epoch  0, batch     0 | loss: 8.7266808Losses:  8.5572509765625 1.297723412513733
CurrentTrain: epoch  0, batch     1 | loss: 9.8549747Losses:  9.317621231079102 1.435060739517212
CurrentTrain: epoch  0, batch     2 | loss: 10.7526817Losses:  4.010819911956787 0.2919021248817444
CurrentTrain: epoch  0, batch     3 | loss: 4.3027220Losses:  4.194843292236328 1.1990783214569092
CurrentTrain: epoch  1, batch     0 | loss: 5.3939219Losses:  3.728299617767334 1.4407618045806885
CurrentTrain: epoch  1, batch     1 | loss: 5.1690617Losses:  4.0312957763671875 1.4265446662902832
CurrentTrain: epoch  1, batch     2 | loss: 5.4578404Losses:  3.6572413444519043 0.20155975222587585
CurrentTrain: epoch  1, batch     3 | loss: 3.8588011Losses:  3.950265884399414 1.2388325929641724
CurrentTrain: epoch  2, batch     0 | loss: 5.1890984Losses:  3.746265172958374 1.4583442211151123
CurrentTrain: epoch  2, batch     1 | loss: 5.2046094Losses:  3.4378914833068848 1.1844158172607422
CurrentTrain: epoch  2, batch     2 | loss: 4.6223073Losses:  3.888023853302002 0.24590390920639038
CurrentTrain: epoch  2, batch     3 | loss: 4.1339278Losses:  4.083556652069092 1.0670628547668457
CurrentTrain: epoch  3, batch     0 | loss: 5.1506195Losses:  3.2790136337280273 1.1548423767089844
CurrentTrain: epoch  3, batch     1 | loss: 4.4338560Losses:  2.9766619205474854 0.9885140061378479
CurrentTrain: epoch  3, batch     2 | loss: 3.9651759Losses:  3.227243661880493 0.3517846465110779
CurrentTrain: epoch  3, batch     3 | loss: 3.5790284Losses:  3.8851377964019775 1.2200981378555298
CurrentTrain: epoch  4, batch     0 | loss: 5.1052361Losses:  2.9588305950164795 0.9087146520614624
CurrentTrain: epoch  4, batch     1 | loss: 3.8675451Losses:  3.2028918266296387 1.0611789226531982
CurrentTrain: epoch  4, batch     2 | loss: 4.2640705Losses:  1.6791751384735107 2.9802322387695312e-08
CurrentTrain: epoch  4, batch     3 | loss: 1.6791751Losses:  3.1522164344787598 0.8053044080734253
CurrentTrain: epoch  5, batch     0 | loss: 3.9575210Losses:  3.3669631481170654 1.0621308088302612
CurrentTrain: epoch  5, batch     1 | loss: 4.4290938Losses:  2.5839974880218506 0.7045605182647705
CurrentTrain: epoch  5, batch     2 | loss: 3.2885580Losses:  4.590112686157227 2.0861628513557662e-07
CurrentTrain: epoch  5, batch     3 | loss: 4.5901127Losses:  2.8274612426757812 0.8309043645858765
CurrentTrain: epoch  6, batch     0 | loss: 3.6583657Losses:  2.565046787261963 0.8405797481536865
CurrentTrain: epoch  6, batch     1 | loss: 3.4056265Losses:  3.465482711791992 0.9902284145355225
CurrentTrain: epoch  6, batch     2 | loss: 4.4557114Losses:  2.039699077606201 0.1370852291584015
CurrentTrain: epoch  6, batch     3 | loss: 2.1767843Losses:  3.1129941940307617 0.8954166173934937
CurrentTrain: epoch  7, batch     0 | loss: 4.0084109Losses:  2.4099783897399902 0.704565167427063
CurrentTrain: epoch  7, batch     1 | loss: 3.1145434Losses:  2.8542733192443848 0.8941957354545593
CurrentTrain: epoch  7, batch     2 | loss: 3.7484691Losses:  2.251894950866699 0.11222484707832336
CurrentTrain: epoch  7, batch     3 | loss: 2.3641198Losses:  2.6855037212371826 0.9147980213165283
CurrentTrain: epoch  8, batch     0 | loss: 3.6003017Losses:  2.671973705291748 0.7935687899589539
CurrentTrain: epoch  8, batch     1 | loss: 3.4655426Losses:  2.6182408332824707 0.7882874011993408
CurrentTrain: epoch  8, batch     2 | loss: 3.4065282Losses:  1.8027660846710205 0.08775167912244797
CurrentTrain: epoch  8, batch     3 | loss: 1.8905177Losses:  2.5855565071105957 0.8866155743598938
CurrentTrain: epoch  9, batch     0 | loss: 3.4721720Losses:  2.26723313331604 0.7719874382019043
CurrentTrain: epoch  9, batch     1 | loss: 3.0392206Losses:  2.8925399780273438 0.918542206287384
CurrentTrain: epoch  9, batch     2 | loss: 3.8110821Losses:  1.732177734375 0.11269091814756393
CurrentTrain: epoch  9, batch     3 | loss: 1.8448687
Losses:  5.9549713134765625 1.0766942501068115
MemoryTrain:  epoch  0, batch     0 | loss: 7.0316658Losses:  10.31466293334961 0.859707236289978
MemoryTrain:  epoch  0, batch     1 | loss: 11.1743698Losses:  10.64787483215332 0.27219948172569275
MemoryTrain:  epoch  0, batch     2 | loss: 10.9200745Losses:  0.9459861516952515 0.8564854860305786
MemoryTrain:  epoch  1, batch     0 | loss: 1.8024716Losses:  1.1568045616149902 0.7436308860778809
MemoryTrain:  epoch  1, batch     1 | loss: 1.9004354Losses:  1.0499686002731323 0.6227680444717407
MemoryTrain:  epoch  1, batch     2 | loss: 1.6727366Losses:  0.7304831743240356 0.9169072508811951
MemoryTrain:  epoch  2, batch     0 | loss: 1.6473904Losses:  0.8055238723754883 0.6592203974723816
MemoryTrain:  epoch  2, batch     1 | loss: 1.4647443Losses:  0.9106047749519348 0.5507745146751404
MemoryTrain:  epoch  2, batch     2 | loss: 1.4613793Losses:  0.6639211773872375 0.7847501039505005
MemoryTrain:  epoch  3, batch     0 | loss: 1.4486713Losses:  0.7400939464569092 1.0915393829345703
MemoryTrain:  epoch  3, batch     1 | loss: 1.8316333Losses:  0.34187594056129456 0.28661882877349854
MemoryTrain:  epoch  3, batch     2 | loss: 0.6284947Losses:  0.5135540962219238 0.8176953792572021
MemoryTrain:  epoch  4, batch     0 | loss: 1.3312495Losses:  0.5776350498199463 0.7537069916725159
MemoryTrain:  epoch  4, batch     1 | loss: 1.3313420Losses:  0.5239292979240417 0.6445077061653137
MemoryTrain:  epoch  4, batch     2 | loss: 1.1684370Losses:  0.42450183629989624 0.9097476601600647
MemoryTrain:  epoch  5, batch     0 | loss: 1.3342495Losses:  0.40769118070602417 0.7197496891021729
MemoryTrain:  epoch  5, batch     1 | loss: 1.1274409Losses:  0.4714846611022949 0.46031448245048523
MemoryTrain:  epoch  5, batch     2 | loss: 0.9317992Losses:  0.40205061435699463 0.8163067102432251
MemoryTrain:  epoch  6, batch     0 | loss: 1.2183573Losses:  0.4084054231643677 0.828558087348938
MemoryTrain:  epoch  6, batch     1 | loss: 1.2369635Losses:  0.4100034236907959 0.39340832829475403
MemoryTrain:  epoch  6, batch     2 | loss: 0.8034117Losses:  0.32837551832199097 0.7179795503616333
MemoryTrain:  epoch  7, batch     0 | loss: 1.0463550Losses:  0.3304274082183838 0.8118430376052856
MemoryTrain:  epoch  7, batch     1 | loss: 1.1422704Losses:  0.3348378539085388 0.4956507682800293
MemoryTrain:  epoch  7, batch     2 | loss: 0.8304886Losses:  0.2728934586048126 0.6187441349029541
MemoryTrain:  epoch  8, batch     0 | loss: 0.8916376Losses:  0.3286457657814026 0.6813409328460693
MemoryTrain:  epoch  8, batch     1 | loss: 1.0099866Losses:  0.4393535256385803 0.6139287352561951
MemoryTrain:  epoch  8, batch     2 | loss: 1.0532823Losses:  0.2795591950416565 0.5426910519599915
MemoryTrain:  epoch  9, batch     0 | loss: 0.8222502Losses:  0.3820415735244751 0.9964480400085449
MemoryTrain:  epoch  9, batch     1 | loss: 1.3784896Losses:  0.26714634895324707 0.38214775919914246
MemoryTrain:  epoch  9, batch     2 | loss: 0.6492941
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 79.12%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.80%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 77.72%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 76.60%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 74.87%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 74.26%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 74.76%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 75.11%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 74.35%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 73.83%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 72.81%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 72.54%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 71.77%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 71.03%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 88.92%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.87%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.36%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.68%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.95%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.84%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.08%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 91.70%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 91.00%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 90.73%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 90.27%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.92%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 89.78%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 89.16%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 88.56%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 88.35%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 88.06%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 87.87%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 87.94%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 87.42%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 86.53%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 85.58%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 84.89%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 84.30%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 83.49%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 83.66%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 84.02%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 83.15%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 82.29%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 81.59%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 80.91%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 80.04%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 79.52%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 79.67%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.64%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 80.58%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.89%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 80.72%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 80.32%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.87%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 79.55%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 79.22%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.74%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 78.43%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 78.18%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 77.99%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 77.69%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 77.62%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 77.49%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 77.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.72%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 77.83%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 77.46%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 76.95%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 76.60%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 76.06%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 75.67%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 75.75%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.28%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 76.13%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 75.67%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 75.22%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 74.73%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 74.25%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 73.86%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 73.52%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 74.92%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 75.12%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.63%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.81%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 75.50%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 75.27%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 75.04%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 74.78%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 74.44%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 74.37%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 74.93%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 74.75%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 74.65%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 74.48%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 74.31%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 74.21%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 74.15%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 74.03%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 73.97%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 73.98%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 73.99%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 73.80%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 73.71%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 73.78%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 73.79%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 73.86%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 73.93%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 74.10%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 74.14%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 74.23%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 74.27%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 74.36%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 74.28%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 74.13%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 73.81%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 73.61%   [EVAL] batch:  211 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 73.59%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 73.90%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 74.23%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 75.03%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 75.08%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 75.24%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 75.13%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 74.92%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 74.79%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 74.57%   [EVAL] batch:  235 | acc: 37.50%,  total acc: 74.42%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 74.18%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 74.16%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 74.33%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 74.49%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 74.31%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 74.09%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 73.96%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 73.79%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 73.67%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 73.52%   
cur_acc:  ['0.9375', '0.7123', '0.7599', '0.7103']
his_acc:  ['0.9375', '0.8305', '0.7826', '0.7352']
Clustering into  24  clusters
Clusters:  [ 4 23 19  4  4  4  0  4 15 16  1 21  3  4  4 12  4 22  4 10  4 17  4  4
 11  4  4  4 20 18  3  4 13  4 14  0  6  4  4  4  7  8  4  4  5  1  9  4
  4  2]
Losses:  6.965822219848633 1.6084072589874268
CurrentTrain: epoch  0, batch     0 | loss: 8.5742292Losses:  8.281391143798828 1.2865498065948486
CurrentTrain: epoch  0, batch     1 | loss: 9.5679407Losses:  6.672276020050049 1.7473745346069336
CurrentTrain: epoch  0, batch     2 | loss: 8.4196510Losses:  3.8008408546447754 0.3495280146598816
CurrentTrain: epoch  0, batch     3 | loss: 4.1503687Losses:  3.1195993423461914 1.1351068019866943
CurrentTrain: epoch  1, batch     0 | loss: 4.2547064Losses:  2.750275135040283 0.9426504969596863
CurrentTrain: epoch  1, batch     1 | loss: 3.6929257Losses:  3.0683932304382324 1.1542259454727173
CurrentTrain: epoch  1, batch     2 | loss: 4.2226191Losses:  3.4348981380462646 0.4059714078903198
CurrentTrain: epoch  1, batch     3 | loss: 3.8408694Losses:  2.774345874786377 1.2764835357666016
CurrentTrain: epoch  2, batch     0 | loss: 4.0508294Losses:  3.0499703884124756 1.4154561758041382
CurrentTrain: epoch  2, batch     1 | loss: 4.4654264Losses:  2.2480931282043457 1.0108639001846313
CurrentTrain: epoch  2, batch     2 | loss: 3.2589569Losses:  2.0234575271606445 0.1527649313211441
CurrentTrain: epoch  2, batch     3 | loss: 2.1762226Losses:  2.44761323928833 1.275681972503662
CurrentTrain: epoch  3, batch     0 | loss: 3.7232952Losses:  2.505133628845215 1.0467395782470703
CurrentTrain: epoch  3, batch     1 | loss: 3.5518732Losses:  2.2170472145080566 0.7958833575248718
CurrentTrain: epoch  3, batch     2 | loss: 3.0129306Losses:  1.9394950866699219 0.35547447204589844
CurrentTrain: epoch  3, batch     3 | loss: 2.2949696Losses:  2.303347587585449 0.9722442626953125
CurrentTrain: epoch  4, batch     0 | loss: 3.2755919Losses:  1.9792816638946533 0.9859161972999573
CurrentTrain: epoch  4, batch     1 | loss: 2.9651978Losses:  2.3513219356536865 0.6915774941444397
CurrentTrain: epoch  4, batch     2 | loss: 3.0428994Losses:  2.1716346740722656 0.16464175283908844
CurrentTrain: epoch  4, batch     3 | loss: 2.3362765Losses:  2.3003435134887695 0.614875316619873
CurrentTrain: epoch  5, batch     0 | loss: 2.9152188Losses:  1.943979263305664 0.6819908022880554
CurrentTrain: epoch  5, batch     1 | loss: 2.6259701Losses:  2.05749773979187 0.9205141663551331
CurrentTrain: epoch  5, batch     2 | loss: 2.9780118Losses:  1.772632122039795 0.17586873471736908
CurrentTrain: epoch  5, batch     3 | loss: 1.9485009Losses:  1.8468073606491089 0.7522323727607727
CurrentTrain: epoch  6, batch     0 | loss: 2.5990398Losses:  2.0252528190612793 0.6970224380493164
CurrentTrain: epoch  6, batch     1 | loss: 2.7222753Losses:  2.0608818531036377 0.6776256561279297
CurrentTrain: epoch  6, batch     2 | loss: 2.7385075Losses:  2.6144981384277344 0.1691683530807495
CurrentTrain: epoch  6, batch     3 | loss: 2.7836666Losses:  1.9278746843338013 0.6513010263442993
CurrentTrain: epoch  7, batch     0 | loss: 2.5791757Losses:  1.881293535232544 0.762880265712738
CurrentTrain: epoch  7, batch     1 | loss: 2.6441739Losses:  1.8773739337921143 0.5369430184364319
CurrentTrain: epoch  7, batch     2 | loss: 2.4143169Losses:  1.8230465650558472 0.14218772947788239
CurrentTrain: epoch  7, batch     3 | loss: 1.9652343Losses:  1.8722596168518066 0.6870731711387634
CurrentTrain: epoch  8, batch     0 | loss: 2.5593328Losses:  1.8291993141174316 0.6485681533813477
CurrentTrain: epoch  8, batch     1 | loss: 2.4777675Losses:  1.876036524772644 0.5752999186515808
CurrentTrain: epoch  8, batch     2 | loss: 2.4513364Losses:  1.6657135486602783 0.0
CurrentTrain: epoch  8, batch     3 | loss: 1.6657135Losses:  1.8722453117370605 0.5689758658409119
CurrentTrain: epoch  9, batch     0 | loss: 2.4412212Losses:  1.7668187618255615 0.5240293741226196
CurrentTrain: epoch  9, batch     1 | loss: 2.2908483Losses:  1.8280885219573975 0.6081915497779846
CurrentTrain: epoch  9, batch     2 | loss: 2.4362800Losses:  1.9101992845535278 0.060908690094947815
CurrentTrain: epoch  9, batch     3 | loss: 1.9711080
Losses:  5.766000747680664 0.7529217004776001
MemoryTrain:  epoch  0, batch     0 | loss: 6.5189223Losses:  9.066154479980469 1.015830397605896
MemoryTrain:  epoch  0, batch     1 | loss: 10.0819845Losses:  10.150707244873047 0.7298179864883423
MemoryTrain:  epoch  0, batch     2 | loss: 10.8805256Losses:  11.965656280517578 0.01749569922685623
MemoryTrain:  epoch  0, batch     3 | loss: 11.9831524Losses:  0.570818305015564 0.5330871343612671
MemoryTrain:  epoch  1, batch     0 | loss: 1.1039054Losses:  1.0074214935302734 0.9578864574432373
MemoryTrain:  epoch  1, batch     1 | loss: 1.9653080Losses:  0.8923507928848267 0.7559677362442017
MemoryTrain:  epoch  1, batch     2 | loss: 1.6483185Losses:  0.5467828512191772 0.05817569047212601
MemoryTrain:  epoch  1, batch     3 | loss: 0.6049585Losses:  0.6390203237533569 0.8448803424835205
MemoryTrain:  epoch  2, batch     0 | loss: 1.4839007Losses:  0.5977999567985535 0.6163022518157959
MemoryTrain:  epoch  2, batch     1 | loss: 1.2141023Losses:  0.6900856494903564 0.8020854592323303
MemoryTrain:  epoch  2, batch     2 | loss: 1.4921710Losses:  0.7110066413879395 0.07204356789588928
MemoryTrain:  epoch  2, batch     3 | loss: 0.7830502Losses:  0.854107677936554 0.8502565026283264
MemoryTrain:  epoch  3, batch     0 | loss: 1.7043642Losses:  0.3843182623386383 0.6264054775238037
MemoryTrain:  epoch  3, batch     1 | loss: 1.0107237Losses:  0.500226616859436 0.7601335644721985
MemoryTrain:  epoch  3, batch     2 | loss: 1.2603602Losses:  0.6250503659248352 0.06407492607831955
MemoryTrain:  epoch  3, batch     3 | loss: 0.6891253Losses:  0.5938039422035217 0.8774576783180237
MemoryTrain:  epoch  4, batch     0 | loss: 1.4712616Losses:  0.4431607723236084 0.6864092350006104
MemoryTrain:  epoch  4, batch     1 | loss: 1.1295700Losses:  0.48813745379447937 0.5971576571464539
MemoryTrain:  epoch  4, batch     2 | loss: 1.0852951Losses:  0.2922636866569519 0.09416615217924118
MemoryTrain:  epoch  4, batch     3 | loss: 0.3864298Losses:  0.3726254403591156 0.6751536130905151
MemoryTrain:  epoch  5, batch     0 | loss: 1.0477791Losses:  0.4894377589225769 0.6921714544296265
MemoryTrain:  epoch  5, batch     1 | loss: 1.1816092Losses:  0.4972047507762909 0.6301862001419067
MemoryTrain:  epoch  5, batch     2 | loss: 1.1273910Losses:  0.689335823059082 0.1729552149772644
MemoryTrain:  epoch  5, batch     3 | loss: 0.8622910Losses:  0.37182214856147766 0.6821897029876709
MemoryTrain:  epoch  6, batch     0 | loss: 1.0540118Losses:  0.4705975353717804 0.6264303922653198
MemoryTrain:  epoch  6, batch     1 | loss: 1.0970279Losses:  0.394107460975647 0.7675322890281677
MemoryTrain:  epoch  6, batch     2 | loss: 1.1616397Losses:  0.6076666116714478 0.18082912266254425
MemoryTrain:  epoch  6, batch     3 | loss: 0.7884957Losses:  0.31962957978248596 0.5048826932907104
MemoryTrain:  epoch  7, batch     0 | loss: 0.8245122Losses:  0.42827466130256653 0.7728791832923889
MemoryTrain:  epoch  7, batch     1 | loss: 1.2011539Losses:  0.42964011430740356 0.6794036030769348
MemoryTrain:  epoch  7, batch     2 | loss: 1.1090437Losses:  0.5385023355484009 0.04866774007678032
MemoryTrain:  epoch  7, batch     3 | loss: 0.5871701Losses:  0.40715137124061584 0.8227056264877319
MemoryTrain:  epoch  8, batch     0 | loss: 1.2298570Losses:  0.2823786437511444 0.5192961096763611
MemoryTrain:  epoch  8, batch     1 | loss: 0.8016747Losses:  0.3112790584564209 0.6632542610168457
MemoryTrain:  epoch  8, batch     2 | loss: 0.9745333Losses:  0.48274004459381104 0.0613858588039875
MemoryTrain:  epoch  8, batch     3 | loss: 0.5441259Losses:  0.35692018270492554 0.733830451965332
MemoryTrain:  epoch  9, batch     0 | loss: 1.0907507Losses:  0.33039936423301697 0.7514686584472656
MemoryTrain:  epoch  9, batch     1 | loss: 1.0818681Losses:  0.34337118268013 0.651969313621521
MemoryTrain:  epoch  9, batch     2 | loss: 0.9953405Losses:  0.16718551516532898 0.00815361738204956
MemoryTrain:  epoch  9, batch     3 | loss: 0.1753391
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 73.65%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 79.90%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 79.72%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 79.63%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.15%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 80.46%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.49%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 89.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.54%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 89.47%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.24%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 87.81%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 87.40%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 86.91%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 86.15%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 85.98%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 85.45%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 85.20%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.09%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.98%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 85.22%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 84.79%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 83.93%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 83.09%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 82.36%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 81.80%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 81.02%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 81.02%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 81.69%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 81.75%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 80.83%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 79.93%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 79.19%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 78.53%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 77.69%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 77.13%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 77.58%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 77.54%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 77.33%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 76.70%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 76.14%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 75.71%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 75.47%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 75.23%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 74.94%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 74.71%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 74.43%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 74.21%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 73.56%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 73.25%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 72.99%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 72.63%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 72.25%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 72.52%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 72.64%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 72.87%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 72.44%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 71.97%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 71.61%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 71.11%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 70.75%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 70.74%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 71.13%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 70.67%   [EVAL] batch:  140 | acc: 0.00%,  total acc: 70.17%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 69.67%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 70.56%   [EVAL] batch:  152 | acc: 87.50%,  total acc: 70.67%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 70.89%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.36%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 71.65%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 71.52%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 71.39%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 71.15%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 70.94%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 71.24%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 71.89%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 71.84%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 71.49%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 71.34%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 71.15%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 71.08%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 71.10%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 71.19%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 71.04%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 71.03%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 71.15%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 71.22%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 71.39%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 71.47%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 71.60%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 71.86%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 71.72%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 71.24%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 71.21%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 72.80%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 72.76%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 72.59%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 72.44%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 72.23%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 71.94%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 71.90%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.97%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 72.12%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 72.14%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 71.93%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 71.76%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 71.57%   [EVAL] batch:  248 | acc: 50.00%,  total acc: 71.49%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 71.35%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 71.59%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  256 | acc: 93.75%,  total acc: 71.94%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 72.26%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 72.11%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 71.92%   [EVAL] batch:  268 | acc: 50.00%,  total acc: 71.84%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 71.60%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 71.47%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 71.28%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 71.04%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 70.85%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 70.68%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.79%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 71.37%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 71.65%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 72.28%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 72.80%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 73.06%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 73.18%   
cur_acc:  ['0.9375', '0.7123', '0.7599', '0.7103', '0.8046']
his_acc:  ['0.9375', '0.8305', '0.7826', '0.7352', '0.7318']
Clustering into  28  clusters
Clusters:  [ 5 19 22  5  5  5 26  5 18  1  0 24  4  5  5 15  5 25  5 27  5 20  5  5
  2  5  5  5 17 21  4  5 16  5  8 14  7  5  5  5  3 10  5  5 13  0 23  5
  5  9  5  5  5 12  5  1  5  6 11  2]
Losses:  7.062543869018555 1.3794658184051514
CurrentTrain: epoch  0, batch     0 | loss: 8.4420099Losses:  8.836187362670898 1.3700131177902222
CurrentTrain: epoch  0, batch     1 | loss: 10.2062006Losses:  7.805049896240234 1.047291874885559
CurrentTrain: epoch  0, batch     2 | loss: 8.8523417Losses:  6.032471656799316 0.42698943614959717
CurrentTrain: epoch  0, batch     3 | loss: 6.4594612Losses:  3.6823627948760986 1.0637576580047607
CurrentTrain: epoch  1, batch     0 | loss: 4.7461205Losses:  3.905034065246582 1.0576813220977783
CurrentTrain: epoch  1, batch     1 | loss: 4.9627151Losses:  3.2995119094848633 0.9779357314109802
CurrentTrain: epoch  1, batch     2 | loss: 4.2774477Losses:  2.8164401054382324 0.08215545862913132
CurrentTrain: epoch  1, batch     3 | loss: 2.8985956Losses:  3.5839948654174805 0.9034630656242371
CurrentTrain: epoch  2, batch     0 | loss: 4.4874578Losses:  3.1571195125579834 0.8151842355728149
CurrentTrain: epoch  2, batch     1 | loss: 3.9723039Losses:  2.7162904739379883 0.8261445760726929
CurrentTrain: epoch  2, batch     2 | loss: 3.5424352Losses:  4.062839508056641 0.34231337904930115
CurrentTrain: epoch  2, batch     3 | loss: 4.4051528Losses:  3.1114277839660645 1.0628374814987183
CurrentTrain: epoch  3, batch     0 | loss: 4.1742654Losses:  2.7013614177703857 0.5759258270263672
CurrentTrain: epoch  3, batch     1 | loss: 3.2772872Losses:  2.8180603981018066 0.9614448547363281
CurrentTrain: epoch  3, batch     2 | loss: 3.7795053Losses:  3.7779541015625 0.2317114919424057
CurrentTrain: epoch  3, batch     3 | loss: 4.0096655Losses:  2.969296932220459 0.9731301665306091
CurrentTrain: epoch  4, batch     0 | loss: 3.9424272Losses:  2.756511688232422 0.670299232006073
CurrentTrain: epoch  4, batch     1 | loss: 3.4268110Losses:  2.5957236289978027 0.6085729002952576
CurrentTrain: epoch  4, batch     2 | loss: 3.2042966Losses:  1.7799313068389893 0.03625095635652542
CurrentTrain: epoch  4, batch     3 | loss: 1.8161823Losses:  2.5061678886413574 0.5879369974136353
CurrentTrain: epoch  5, batch     0 | loss: 3.0941048Losses:  2.5168375968933105 0.7984153032302856
CurrentTrain: epoch  5, batch     1 | loss: 3.3152528Losses:  2.4357352256774902 0.7023651599884033
CurrentTrain: epoch  5, batch     2 | loss: 3.1381004Losses:  2.384396553039551 0.12433356046676636
CurrentTrain: epoch  5, batch     3 | loss: 2.5087302Losses:  2.763371229171753 0.6499022245407104
CurrentTrain: epoch  6, batch     0 | loss: 3.4132733Losses:  2.123873233795166 0.6408122181892395
CurrentTrain: epoch  6, batch     1 | loss: 2.7646854Losses:  2.1415929794311523 0.6093558073043823
CurrentTrain: epoch  6, batch     2 | loss: 2.7509489Losses:  1.998626470565796 0.15408509969711304
CurrentTrain: epoch  6, batch     3 | loss: 2.1527116Losses:  2.285153388977051 0.6882145404815674
CurrentTrain: epoch  7, batch     0 | loss: 2.9733679Losses:  2.340869426727295 0.3852372467517853
CurrentTrain: epoch  7, batch     1 | loss: 2.7261066Losses:  2.032102584838867 0.5065401792526245
CurrentTrain: epoch  7, batch     2 | loss: 2.5386429Losses:  1.891039490699768 0.1981753408908844
CurrentTrain: epoch  7, batch     3 | loss: 2.0892148Losses:  2.008732795715332 0.5001420974731445
CurrentTrain: epoch  8, batch     0 | loss: 2.5088749Losses:  2.3288896083831787 0.6895791888237
CurrentTrain: epoch  8, batch     1 | loss: 3.0184689Losses:  2.0393404960632324 0.5227280855178833
CurrentTrain: epoch  8, batch     2 | loss: 2.5620685Losses:  2.041764736175537 0.09486241638660431
CurrentTrain: epoch  8, batch     3 | loss: 2.1366272Losses:  1.9788761138916016 0.47992047667503357
CurrentTrain: epoch  9, batch     0 | loss: 2.4587965Losses:  2.10660982131958 0.5112236142158508
CurrentTrain: epoch  9, batch     1 | loss: 2.6178334Losses:  2.1367685794830322 0.6174566745758057
CurrentTrain: epoch  9, batch     2 | loss: 2.7542253Losses:  1.755698561668396 0.024735229089856148
CurrentTrain: epoch  9, batch     3 | loss: 1.7804338
Losses:  6.031614303588867 0.7203956842422485
MemoryTrain:  epoch  0, batch     0 | loss: 6.7520099Losses:  8.862213134765625 0.8559008836746216
MemoryTrain:  epoch  0, batch     1 | loss: 9.7181139Losses:  10.040910720825195 0.6472552418708801
MemoryTrain:  epoch  0, batch     2 | loss: 10.6881657Losses:  11.027616500854492 0.6451047658920288
MemoryTrain:  epoch  0, batch     3 | loss: 11.6727209Losses:  1.5105640888214111 0.7664076685905457
MemoryTrain:  epoch  1, batch     0 | loss: 2.2769718Losses:  0.7034125924110413 0.640419602394104
MemoryTrain:  epoch  1, batch     1 | loss: 1.3438323Losses:  1.0502350330352783 0.6083592176437378
MemoryTrain:  epoch  1, batch     2 | loss: 1.6585943Losses:  0.7292070388793945 0.6558238863945007
MemoryTrain:  epoch  1, batch     3 | loss: 1.3850310Losses:  0.8425053358078003 0.7408586740493774
MemoryTrain:  epoch  2, batch     0 | loss: 1.5833640Losses:  0.6292259097099304 0.67474365234375
MemoryTrain:  epoch  2, batch     1 | loss: 1.3039696Losses:  0.7802944183349609 0.5793668031692505
MemoryTrain:  epoch  2, batch     2 | loss: 1.3596612Losses:  1.0875353813171387 0.6285325288772583
MemoryTrain:  epoch  2, batch     3 | loss: 1.7160679Losses:  0.6771801710128784 0.7167043089866638
MemoryTrain:  epoch  3, batch     0 | loss: 1.3938844Losses:  0.7067011594772339 0.5989156365394592
MemoryTrain:  epoch  3, batch     1 | loss: 1.3056169Losses:  0.6548913717269897 0.8066996335983276
MemoryTrain:  epoch  3, batch     2 | loss: 1.4615910Losses:  0.5727035999298096 0.5386106967926025
MemoryTrain:  epoch  3, batch     3 | loss: 1.1113143Losses:  0.5674993991851807 0.5999503135681152
MemoryTrain:  epoch  4, batch     0 | loss: 1.1674497Losses:  0.4547973871231079 0.6634649634361267
MemoryTrain:  epoch  4, batch     1 | loss: 1.1182623Losses:  0.9872337579727173 0.866234302520752
MemoryTrain:  epoch  4, batch     2 | loss: 1.8534681Losses:  0.45030349493026733 0.4127906262874603
MemoryTrain:  epoch  4, batch     3 | loss: 0.8630941Losses:  0.5194780826568604 0.7141416072845459
MemoryTrain:  epoch  5, batch     0 | loss: 1.2336197Losses:  0.5259849429130554 0.7814898490905762
MemoryTrain:  epoch  5, batch     1 | loss: 1.3074749Losses:  0.463735431432724 0.5852166414260864
MemoryTrain:  epoch  5, batch     2 | loss: 1.0489521Losses:  0.39099591970443726 0.4945250153541565
MemoryTrain:  epoch  5, batch     3 | loss: 0.8855209Losses:  0.46096375584602356 0.8640251159667969
MemoryTrain:  epoch  6, batch     0 | loss: 1.3249888Losses:  0.4053771495819092 0.47105589509010315
MemoryTrain:  epoch  6, batch     1 | loss: 0.8764330Losses:  0.5006154775619507 0.6549815535545349
MemoryTrain:  epoch  6, batch     2 | loss: 1.1555970Losses:  0.37001216411590576 0.5659388899803162
MemoryTrain:  epoch  6, batch     3 | loss: 0.9359511Losses:  0.4321349859237671 0.5840340256690979
MemoryTrain:  epoch  7, batch     0 | loss: 1.0161691Losses:  0.45397430658340454 0.7521502375602722
MemoryTrain:  epoch  7, batch     1 | loss: 1.2061245Losses:  0.4185265898704529 0.6572937369346619
MemoryTrain:  epoch  7, batch     2 | loss: 1.0758203Losses:  0.34049615263938904 0.4317012429237366
MemoryTrain:  epoch  7, batch     3 | loss: 0.7721974Losses:  0.34802770614624023 0.5645138025283813
MemoryTrain:  epoch  8, batch     0 | loss: 0.9125415Losses:  0.42837488651275635 0.6525919437408447
MemoryTrain:  epoch  8, batch     1 | loss: 1.0809668Losses:  0.36007454991340637 0.8222901225090027
MemoryTrain:  epoch  8, batch     2 | loss: 1.1823647Losses:  0.3520701825618744 0.4211394190788269
MemoryTrain:  epoch  8, batch     3 | loss: 0.7732096Losses:  0.4661059081554413 0.6757200360298157
MemoryTrain:  epoch  9, batch     0 | loss: 1.1418259Losses:  0.3116507828235626 0.6033002138137817
MemoryTrain:  epoch  9, batch     1 | loss: 0.9149510Losses:  0.40362703800201416 0.7501046657562256
MemoryTrain:  epoch  9, batch     2 | loss: 1.1537317Losses:  0.3167845904827118 0.3585194945335388
MemoryTrain:  epoch  9, batch     3 | loss: 0.6753041
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 69.17%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 67.95%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 64.54%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 62.24%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 61.22%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 60.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.36%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.93%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 88.03%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.77%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.52%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.62%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 87.94%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 87.07%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 86.65%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 86.35%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 85.86%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 84.86%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 84.13%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 84.00%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 83.40%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 83.18%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 83.01%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.02%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 82.40%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 81.57%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 80.77%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 80.06%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 79.61%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 78.94%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 78.96%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 79.71%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 79.94%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 79.76%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 78.86%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 77.99%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 77.34%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 76.63%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 75.81%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 75.27%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 75.84%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 76.18%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 76.10%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 75.61%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 75.18%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 74.94%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 74.76%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 74.59%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 74.37%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 74.15%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 73.99%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 73.56%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 73.08%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 72.66%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 72.14%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 71.74%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 71.40%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 71.17%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 71.72%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 71.95%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 72.13%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 71.46%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 71.00%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 70.54%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 70.05%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 69.66%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 69.55%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 70.27%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.46%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 69.02%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.53%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 67.97%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.58%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 70.32%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.74%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 70.54%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 70.30%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 70.11%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 69.80%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 69.53%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 69.45%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 70.24%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 70.02%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 69.80%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:  179 | acc: 31.25%,  total acc: 69.48%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 69.27%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 69.02%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 69.22%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 69.11%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 69.17%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 69.29%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 69.23%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 69.13%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 69.42%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 69.14%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 68.87%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 68.78%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 68.81%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 70.56%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 70.66%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 70.44%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 70.27%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 70.11%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 70.02%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 69.80%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 69.75%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 69.97%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 69.77%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 69.56%   [EVAL] batch:  247 | acc: 12.50%,  total acc: 69.33%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 68.97%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 69.95%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 69.78%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 69.68%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 69.59%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 69.38%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 69.09%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 68.84%   [EVAL] batch:  273 | acc: 37.50%,  total acc: 68.73%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 68.57%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 69.43%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 69.62%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.56%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  300 | acc: 31.25%,  total acc: 70.72%   [EVAL] batch:  301 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  303 | acc: 62.50%,  total acc: 70.64%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 70.59%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 71.07%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 71.00%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 70.77%   [EVAL] batch:  318 | acc: 68.75%,  total acc: 70.77%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 71.14%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 71.19%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 71.17%   [EVAL] batch:  333 | acc: 81.25%,  total acc: 71.20%   [EVAL] batch:  334 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:  338 | acc: 56.25%,  total acc: 71.11%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 71.07%   [EVAL] batch:  340 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:  341 | acc: 43.75%,  total acc: 70.96%   [EVAL] batch:  342 | acc: 31.25%,  total acc: 70.85%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 70.73%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:  345 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 70.76%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 70.79%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 70.77%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 70.73%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 70.63%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 70.59%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 70.52%   [EVAL] batch:  357 | acc: 0.00%,  total acc: 70.32%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 70.14%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 69.98%   [EVAL] batch:  360 | acc: 12.50%,  total acc: 69.82%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 69.72%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 69.66%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.99%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  370 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 70.55%   
cur_acc:  ['0.9375', '0.7123', '0.7599', '0.7103', '0.8046', '0.6736']
his_acc:  ['0.9375', '0.8305', '0.7826', '0.7352', '0.7318', '0.7055']
Clustering into  34  clusters
Clusters:  [ 0  9 20  3  0  0 29  0 22  1 31 27  4  0  0 17  0 28  0 33  0 24  0  0
 16  0  0  0 21 25  4  0 18  0 19 26  8  0  0  0  3 13  0  0 30 14 23  0
  0 32  0  0  0 10  0  1  0  6 12 15  9 11  0  0  0  5  0  0  2  7]
Losses:  6.9767351150512695 1.080986738204956
CurrentTrain: epoch  0, batch     0 | loss: 8.0577221Losses:  8.363314628601074 1.3146553039550781
CurrentTrain: epoch  0, batch     1 | loss: 9.6779699Losses:  7.131516933441162 1.0250989198684692
CurrentTrain: epoch  0, batch     2 | loss: 8.1566162Losses:  5.791327476501465 0.6066741943359375
CurrentTrain: epoch  0, batch     3 | loss: 6.3980017Losses:  3.788177490234375 1.1715457439422607
CurrentTrain: epoch  1, batch     0 | loss: 4.9597235Losses:  3.1710567474365234 0.86897873878479
CurrentTrain: epoch  1, batch     1 | loss: 4.0400352Losses:  3.119314670562744 1.2073417901992798
CurrentTrain: epoch  1, batch     2 | loss: 4.3266563Losses:  2.151172637939453 0.19223108887672424
CurrentTrain: epoch  1, batch     3 | loss: 2.3434038Losses:  2.994722366333008 0.802709698677063
CurrentTrain: epoch  2, batch     0 | loss: 3.7974319Losses:  3.016934394836426 1.1006581783294678
CurrentTrain: epoch  2, batch     1 | loss: 4.1175928Losses:  3.02337646484375 1.0322757959365845
CurrentTrain: epoch  2, batch     2 | loss: 4.0556521Losses:  2.3296942710876465 0.19917786121368408
CurrentTrain: epoch  2, batch     3 | loss: 2.5288720Losses:  2.907406806945801 1.0380733013153076
CurrentTrain: epoch  3, batch     0 | loss: 3.9454801Losses:  2.8961095809936523 0.9749922156333923
CurrentTrain: epoch  3, batch     1 | loss: 3.8711019Losses:  2.5958924293518066 0.9005392789840698
CurrentTrain: epoch  3, batch     2 | loss: 3.4964318Losses:  2.364591598510742 0.08516202867031097
CurrentTrain: epoch  3, batch     3 | loss: 2.4497535Losses:  2.9958155155181885 1.027652382850647
CurrentTrain: epoch  4, batch     0 | loss: 4.0234680Losses:  2.255232810974121 0.7770066261291504
CurrentTrain: epoch  4, batch     1 | loss: 3.0322394Losses:  2.483322858810425 0.680674135684967
CurrentTrain: epoch  4, batch     2 | loss: 3.1639969Losses:  2.4995832443237305 0.10229013860225677
CurrentTrain: epoch  4, batch     3 | loss: 2.6018734Losses:  2.2675223350524902 0.6328403949737549
CurrentTrain: epoch  5, batch     0 | loss: 2.9003627Losses:  2.320732593536377 0.7316451668739319
CurrentTrain: epoch  5, batch     1 | loss: 3.0523777Losses:  2.476262331008911 0.8406361937522888
CurrentTrain: epoch  5, batch     2 | loss: 3.3168986Losses:  4.620393753051758 8.94069742685133e-08
CurrentTrain: epoch  5, batch     3 | loss: 4.6203938Losses:  2.3460512161254883 0.7680345177650452
CurrentTrain: epoch  6, batch     0 | loss: 3.1140857Losses:  2.1918702125549316 0.6882649064064026
CurrentTrain: epoch  6, batch     1 | loss: 2.8801351Losses:  2.16402530670166 0.716934323310852
CurrentTrain: epoch  6, batch     2 | loss: 2.8809595Losses:  3.2804360389709473 0.28225237131118774
CurrentTrain: epoch  6, batch     3 | loss: 3.5626884Losses:  2.1235616207122803 0.727303147315979
CurrentTrain: epoch  7, batch     0 | loss: 2.8508649Losses:  2.36594295501709 0.7370370030403137
CurrentTrain: epoch  7, batch     1 | loss: 3.1029799Losses:  2.0634217262268066 0.5615872144699097
CurrentTrain: epoch  7, batch     2 | loss: 2.6250091Losses:  1.9012267589569092 0.07707656919956207
CurrentTrain: epoch  7, batch     3 | loss: 1.9783033Losses:  2.0998966693878174 0.6184251308441162
CurrentTrain: epoch  8, batch     0 | loss: 2.7183218Losses:  2.0263655185699463 0.5158495903015137
CurrentTrain: epoch  8, batch     1 | loss: 2.5422151Losses:  2.0876200199127197 0.651580810546875
CurrentTrain: epoch  8, batch     2 | loss: 2.7392008Losses:  2.490941047668457 0.11964194476604462
CurrentTrain: epoch  8, batch     3 | loss: 2.6105831Losses:  1.982416033744812 0.6601704359054565
CurrentTrain: epoch  9, batch     0 | loss: 2.6425865Losses:  1.9360382556915283 0.6049262285232544
CurrentTrain: epoch  9, batch     1 | loss: 2.5409646Losses:  2.0401549339294434 0.5983791351318359
CurrentTrain: epoch  9, batch     2 | loss: 2.6385341Losses:  1.8062260150909424 0.060783497989177704
CurrentTrain: epoch  9, batch     3 | loss: 1.8670095
Losses:  6.112720966339111 0.6957024931907654
MemoryTrain:  epoch  0, batch     0 | loss: 6.8084235Losses:  8.745031356811523 0.6028053760528564
MemoryTrain:  epoch  0, batch     1 | loss: 9.3478365Losses:  10.553092956542969 0.8639780282974243
MemoryTrain:  epoch  0, batch     2 | loss: 11.4170713Losses:  11.716286659240723 0.6201820373535156
MemoryTrain:  epoch  0, batch     3 | loss: 12.3364687Losses:  10.128667831420898 0.2519382834434509
MemoryTrain:  epoch  0, batch     4 | loss: 10.3806057Losses:  0.7324657440185547 0.6702880263328552
MemoryTrain:  epoch  1, batch     0 | loss: 1.4027538Losses:  1.1145719289779663 0.5900805592536926
MemoryTrain:  epoch  1, batch     1 | loss: 1.7046525Losses:  1.032531976699829 0.8083130121231079
MemoryTrain:  epoch  1, batch     2 | loss: 1.8408450Losses:  1.632076621055603 0.7683851718902588
MemoryTrain:  epoch  1, batch     3 | loss: 2.4004617Losses:  1.2038315534591675 0.24155454337596893
MemoryTrain:  epoch  1, batch     4 | loss: 1.4453861Losses:  0.8390061259269714 0.7339508533477783
MemoryTrain:  epoch  2, batch     0 | loss: 1.5729570Losses:  0.8818501830101013 0.6784812211990356
MemoryTrain:  epoch  2, batch     1 | loss: 1.5603313Losses:  1.178362250328064 0.7036950588226318
MemoryTrain:  epoch  2, batch     2 | loss: 1.8820573Losses:  0.8229721784591675 0.5612529516220093
MemoryTrain:  epoch  2, batch     3 | loss: 1.3842251Losses:  0.7988874316215515 0.6350980997085571
MemoryTrain:  epoch  2, batch     4 | loss: 1.4339855Losses:  0.9284186363220215 0.6130545139312744
MemoryTrain:  epoch  3, batch     0 | loss: 1.5414732Losses:  0.5339324474334717 0.6937121152877808
MemoryTrain:  epoch  3, batch     1 | loss: 1.2276446Losses:  0.8377090692520142 0.8538565635681152
MemoryTrain:  epoch  3, batch     2 | loss: 1.6915656Losses:  0.593116819858551 0.6677518486976624
MemoryTrain:  epoch  3, batch     3 | loss: 1.2608687Losses:  0.6229874491691589 0.24234627187252045
MemoryTrain:  epoch  3, batch     4 | loss: 0.8653337Losses:  0.550615668296814 0.8007452487945557
MemoryTrain:  epoch  4, batch     0 | loss: 1.3513609Losses:  0.609673023223877 0.7580212354660034
MemoryTrain:  epoch  4, batch     1 | loss: 1.3676943Losses:  0.5403462052345276 0.47734642028808594
MemoryTrain:  epoch  4, batch     2 | loss: 1.0176926Losses:  0.6065155863761902 0.5663461685180664
MemoryTrain:  epoch  4, batch     3 | loss: 1.1728618Losses:  0.616252064704895 0.3406224250793457
MemoryTrain:  epoch  4, batch     4 | loss: 0.9568745Losses:  0.5570192337036133 0.6621348857879639
MemoryTrain:  epoch  5, batch     0 | loss: 1.2191541Losses:  0.508190393447876 0.599743127822876
MemoryTrain:  epoch  5, batch     1 | loss: 1.1079335Losses:  0.5157082080841064 0.6440905332565308
MemoryTrain:  epoch  5, batch     2 | loss: 1.1597987Losses:  0.5614383816719055 0.614267110824585
MemoryTrain:  epoch  5, batch     3 | loss: 1.1757054Losses:  0.42250245809555054 0.37364205718040466
MemoryTrain:  epoch  5, batch     4 | loss: 0.7961445Losses:  0.5362882614135742 0.7146562337875366
MemoryTrain:  epoch  6, batch     0 | loss: 1.2509445Losses:  0.461603045463562 0.5012675523757935
MemoryTrain:  epoch  6, batch     1 | loss: 0.9628706Losses:  0.42397835850715637 0.6545608043670654
MemoryTrain:  epoch  6, batch     2 | loss: 1.0785391Losses:  0.42721545696258545 0.6677991151809692
MemoryTrain:  epoch  6, batch     3 | loss: 1.0950146Losses:  0.4643583297729492 0.3325337767601013
MemoryTrain:  epoch  6, batch     4 | loss: 0.7968921Losses:  0.441054105758667 0.6110324859619141
MemoryTrain:  epoch  7, batch     0 | loss: 1.0520866Losses:  0.42985957860946655 0.6601693034172058
MemoryTrain:  epoch  7, batch     1 | loss: 1.0900289Losses:  0.3607196807861328 0.6337963938713074
MemoryTrain:  epoch  7, batch     2 | loss: 0.9945161Losses:  0.44534242153167725 0.6078791618347168
MemoryTrain:  epoch  7, batch     3 | loss: 1.0532216Losses:  0.40715324878692627 0.3152909576892853
MemoryTrain:  epoch  7, batch     4 | loss: 0.7224442Losses:  0.3746560215950012 0.6383853554725647
MemoryTrain:  epoch  8, batch     0 | loss: 1.0130414Losses:  0.3915846049785614 0.5379692316055298
MemoryTrain:  epoch  8, batch     1 | loss: 0.9295539Losses:  0.4600580930709839 0.6414291858673096
MemoryTrain:  epoch  8, batch     2 | loss: 1.1014873Losses:  0.3869613707065582 0.5861418843269348
MemoryTrain:  epoch  8, batch     3 | loss: 0.9731033Losses:  0.6154204607009888 0.5608853101730347
MemoryTrain:  epoch  8, batch     4 | loss: 1.1763058Losses:  0.45772525668144226 0.633558988571167
MemoryTrain:  epoch  9, batch     0 | loss: 1.0912843Losses:  0.3322053551673889 0.40646934509277344
MemoryTrain:  epoch  9, batch     1 | loss: 0.7386747Losses:  0.41017651557922363 0.7694261074066162
MemoryTrain:  epoch  9, batch     2 | loss: 1.1796026Losses:  0.37907105684280396 0.5214610695838928
MemoryTrain:  epoch  9, batch     3 | loss: 0.9005321Losses:  0.5807920694351196 0.47523558139801025
MemoryTrain:  epoch  9, batch     4 | loss: 1.0560277
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 70.62%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 69.76%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 64.24%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 62.84%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 62.82%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 63.87%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 63.72%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 62.88%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.74%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.65%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.69%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.17%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.11%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 85.03%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.73%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 85.53%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.16%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 83.84%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 83.47%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.89%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.66%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 82.23%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 81.06%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 80.80%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 80.80%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 80.81%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 80.74%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.92%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 80.26%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 79.46%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 78.61%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 77.93%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 77.42%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 76.70%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 77.70%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 76.83%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 75.97%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 75.14%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 74.39%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 73.59%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 73.07%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.57%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 73.78%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 74.24%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 74.26%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 74.20%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 73.85%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 73.50%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 73.33%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 73.01%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 72.86%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 72.76%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 72.56%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 72.41%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 72.21%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 71.96%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 71.49%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 71.14%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 70.58%   [EVAL] batch:  116 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:  117 | acc: 25.00%,  total acc: 69.76%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 69.59%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 70.44%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 70.03%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 69.58%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 69.04%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 68.56%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 68.13%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 68.04%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 68.53%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 68.08%   [EVAL] batch:  140 | acc: 0.00%,  total acc: 67.60%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 67.17%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 66.83%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 69.32%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 69.09%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 68.90%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 68.60%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 68.30%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 68.31%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 69.07%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 68.82%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 68.61%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 68.40%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 67.99%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 67.79%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 67.83%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 67.80%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 67.84%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 67.82%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 67.69%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 67.64%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 67.55%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 67.36%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 67.30%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 67.40%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 67.38%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 67.29%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 67.20%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.56%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  206 | acc: 43.75%,  total acc: 67.57%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 67.43%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 67.25%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 66.97%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 68.80%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 68.64%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 68.51%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 68.25%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 68.20%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.39%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  243 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 68.22%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 68.04%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 67.84%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 67.70%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.99%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  258 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 68.51%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 68.51%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 68.49%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 68.28%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 68.19%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 68.10%   [EVAL] batch:  269 | acc: 18.75%,  total acc: 67.92%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 67.85%   [EVAL] batch:  271 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 67.51%   [EVAL] batch:  273 | acc: 37.50%,  total acc: 67.40%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.51%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 68.04%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 68.27%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 69.47%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.57%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.76%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 70.01%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 69.76%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 69.66%   [EVAL] batch:  317 | acc: 43.75%,  total acc: 69.58%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 69.51%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 69.72%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 69.94%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.96%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 70.04%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 70.05%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 70.11%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 70.12%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 70.13%   [EVAL] batch:  335 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 70.16%   [EVAL] batch:  338 | acc: 56.25%,  total acc: 70.11%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 70.09%   [EVAL] batch:  340 | acc: 56.25%,  total acc: 70.05%   [EVAL] batch:  341 | acc: 50.00%,  total acc: 69.99%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 69.80%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  345 | acc: 68.75%,  total acc: 69.80%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 69.76%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 69.62%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 69.58%   [EVAL] batch:  354 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 69.54%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 69.45%   [EVAL] batch:  357 | acc: 0.00%,  total acc: 69.26%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 68.91%   [EVAL] batch:  360 | acc: 6.25%,  total acc: 68.73%   [EVAL] batch:  361 | acc: 25.00%,  total acc: 68.61%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 68.56%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 69.45%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  377 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  380 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 69.55%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 69.55%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 69.46%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:  388 | acc: 37.50%,  total acc: 69.44%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 69.39%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 69.75%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 69.67%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 69.65%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 69.54%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 69.47%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 69.41%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 69.25%   [EVAL] batch:  409 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 68.99%   [EVAL] batch:  411 | acc: 12.50%,  total acc: 68.86%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 68.83%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  415 | acc: 81.25%,  total acc: 68.90%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 68.94%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 68.87%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 68.76%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 68.69%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 68.79%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  431 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 69.21%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 69.31%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.34%   
cur_acc:  ['0.9375', '0.7123', '0.7599', '0.7103', '0.8046', '0.6736', '0.6865']
his_acc:  ['0.9375', '0.8305', '0.7826', '0.7352', '0.7318', '0.7055', '0.6934']
Clustering into  38  clusters
Clusters:  [ 1  5 24  1  1  1 33  1 21  0 32 27  3  1  1 37  1 29  1 34  2 25  1  1
 36  1  1  1 20 23 26  1 22  1 19 31 35  1  1  1 17 30  1  1 28 15 11  1
  1 18  1  1  1 12  1  0  1 13 16 14  5 10  1  1  1  2  1  8  6  9  7  1
  1  3  3  4  1  1  1  1]
Losses:  7.034584999084473 1.644545078277588
CurrentTrain: epoch  0, batch     0 | loss: 8.6791306Losses:  8.495697975158691 1.853065013885498
CurrentTrain: epoch  0, batch     1 | loss: 10.3487625Losses:  7.168710708618164 1.522268533706665
CurrentTrain: epoch  0, batch     2 | loss: 8.6909790Losses:  7.574457168579102 0.4211168885231018
CurrentTrain: epoch  0, batch     3 | loss: 7.9955740Losses:  4.446571350097656 1.6319668292999268
CurrentTrain: epoch  1, batch     0 | loss: 6.0785379Losses:  3.679985761642456 1.4995055198669434
CurrentTrain: epoch  1, batch     1 | loss: 5.1794910Losses:  3.5266926288604736 1.454883098602295
CurrentTrain: epoch  1, batch     2 | loss: 4.9815760Losses:  4.400468826293945 0.39416074752807617
CurrentTrain: epoch  1, batch     3 | loss: 4.7946296Losses:  3.9575111865997314 1.2928481101989746
CurrentTrain: epoch  2, batch     0 | loss: 5.2503595Losses:  3.619302749633789 1.5194636583328247
CurrentTrain: epoch  2, batch     1 | loss: 5.1387663Losses:  3.3096365928649902 1.4589720964431763
CurrentTrain: epoch  2, batch     2 | loss: 4.7686086Losses:  2.5199642181396484 0.1939772069454193
CurrentTrain: epoch  2, batch     3 | loss: 2.7139413Losses:  3.0413784980773926 1.1421302556991577
CurrentTrain: epoch  3, batch     0 | loss: 4.1835089Losses:  3.4953155517578125 1.391340970993042
CurrentTrain: epoch  3, batch     1 | loss: 4.8866568Losses:  3.2409534454345703 1.147043228149414
CurrentTrain: epoch  3, batch     2 | loss: 4.3879967Losses:  2.6265034675598145 0.27498987317085266
CurrentTrain: epoch  3, batch     3 | loss: 2.9014933Losses:  2.7160756587982178 1.1571582555770874
CurrentTrain: epoch  4, batch     0 | loss: 3.8732338Losses:  3.3324904441833496 1.401474118232727
CurrentTrain: epoch  4, batch     1 | loss: 4.7339644Losses:  3.3009066581726074 1.080918788909912
CurrentTrain: epoch  4, batch     2 | loss: 4.3818254Losses:  2.50946307182312 0.10354658961296082
CurrentTrain: epoch  4, batch     3 | loss: 2.6130097Losses:  2.7004482746124268 1.0137898921966553
CurrentTrain: epoch  5, batch     0 | loss: 3.7142382Losses:  2.701218605041504 0.8754311203956604
CurrentTrain: epoch  5, batch     1 | loss: 3.5766497Losses:  3.2813985347747803 1.2401126623153687
CurrentTrain: epoch  5, batch     2 | loss: 4.5215111Losses:  2.1204562187194824 0.19394278526306152
CurrentTrain: epoch  5, batch     3 | loss: 2.3143990Losses:  2.927494525909424 1.002436876296997
CurrentTrain: epoch  6, batch     0 | loss: 3.9299314Losses:  3.039602279663086 0.978036105632782
CurrentTrain: epoch  6, batch     1 | loss: 4.0176382Losses:  2.463083505630493 1.0047070980072021
CurrentTrain: epoch  6, batch     2 | loss: 3.4677906Losses:  2.1662087440490723 0.10331474244594574
CurrentTrain: epoch  6, batch     3 | loss: 2.2695234Losses:  2.8204126358032227 1.034380316734314
CurrentTrain: epoch  7, batch     0 | loss: 3.8547931Losses:  2.3961124420166016 0.8120445013046265
CurrentTrain: epoch  7, batch     1 | loss: 3.2081571Losses:  2.7639596462249756 0.8369575142860413
CurrentTrain: epoch  7, batch     2 | loss: 3.6009171Losses:  1.8335251808166504 0.3814804255962372
CurrentTrain: epoch  7, batch     3 | loss: 2.2150056Losses:  2.687387228012085 0.9259313941001892
CurrentTrain: epoch  8, batch     0 | loss: 3.6133187Losses:  2.1349058151245117 0.8991872072219849
CurrentTrain: epoch  8, batch     1 | loss: 3.0340929Losses:  2.5259251594543457 1.0789899826049805
CurrentTrain: epoch  8, batch     2 | loss: 3.6049151Losses:  2.3859715461730957 0.1171790137887001
CurrentTrain: epoch  8, batch     3 | loss: 2.5031505Losses:  2.614053964614868 0.9698801040649414
CurrentTrain: epoch  9, batch     0 | loss: 3.5839341Losses:  2.0588133335113525 0.8605830073356628
CurrentTrain: epoch  9, batch     1 | loss: 2.9193964Losses:  2.3352746963500977 0.8983815908432007
CurrentTrain: epoch  9, batch     2 | loss: 3.2336564Losses:  3.846907138824463 0.20412090420722961
CurrentTrain: epoch  9, batch     3 | loss: 4.0510283
Losses:  5.97186279296875 0.6083856225013733
MemoryTrain:  epoch  0, batch     0 | loss: 6.5802484Losses:  8.45413589477539 0.8894816637039185
MemoryTrain:  epoch  0, batch     1 | loss: 9.3436174Losses:  9.647068977355957 0.7719262838363647
MemoryTrain:  epoch  0, batch     2 | loss: 10.4189949Losses:  10.757450103759766 0.6782746911048889
MemoryTrain:  epoch  0, batch     3 | loss: 11.4357252Losses:  11.011068344116211 0.6335502862930298
MemoryTrain:  epoch  0, batch     4 | loss: 11.6446190Losses:  0.762273371219635 0.6575605869293213
MemoryTrain:  epoch  1, batch     0 | loss: 1.4198339Losses:  0.7557607889175415 0.5253486633300781
MemoryTrain:  epoch  1, batch     1 | loss: 1.2811095Losses:  1.1302326917648315 0.7212662696838379
MemoryTrain:  epoch  1, batch     2 | loss: 1.8514990Losses:  0.940462589263916 0.757006049156189
MemoryTrain:  epoch  1, batch     3 | loss: 1.6974686Losses:  1.0945768356323242 0.8068509101867676
MemoryTrain:  epoch  1, batch     4 | loss: 1.9014277Losses:  0.9566841125488281 0.812191903591156
MemoryTrain:  epoch  2, batch     0 | loss: 1.7688761Losses:  0.9862228035926819 0.5801822543144226
MemoryTrain:  epoch  2, batch     1 | loss: 1.5664051Losses:  0.5463135242462158 0.6830328106880188
MemoryTrain:  epoch  2, batch     2 | loss: 1.2293463Losses:  1.0298941135406494 0.6032252311706543
MemoryTrain:  epoch  2, batch     3 | loss: 1.6331193Losses:  0.6370410919189453 0.8694010972976685
MemoryTrain:  epoch  2, batch     4 | loss: 1.5064422Losses:  0.6245372295379639 0.5334798097610474
MemoryTrain:  epoch  3, batch     0 | loss: 1.1580170Losses:  0.508880615234375 0.6692138910293579
MemoryTrain:  epoch  3, batch     1 | loss: 1.1780945Losses:  0.9533891081809998 0.8493378758430481
MemoryTrain:  epoch  3, batch     2 | loss: 1.8027270Losses:  0.8527727127075195 0.6937853097915649
MemoryTrain:  epoch  3, batch     3 | loss: 1.5465580Losses:  0.8229619860649109 0.6532763838768005
MemoryTrain:  epoch  3, batch     4 | loss: 1.4762384Losses:  1.137820839881897 0.8729345798492432
MemoryTrain:  epoch  4, batch     0 | loss: 2.0107555Losses:  0.41538000106811523 0.6133283376693726
MemoryTrain:  epoch  4, batch     1 | loss: 1.0287083Losses:  0.6476043462753296 0.8065731525421143
MemoryTrain:  epoch  4, batch     2 | loss: 1.4541775Losses:  0.4765814244747162 0.6294809579849243
MemoryTrain:  epoch  4, batch     3 | loss: 1.1060624Losses:  0.4287846088409424 0.514915943145752
MemoryTrain:  epoch  4, batch     4 | loss: 0.9437006Losses:  0.6238619089126587 0.7219730615615845
MemoryTrain:  epoch  5, batch     0 | loss: 1.3458350Losses:  0.5081718564033508 0.6638002395629883
MemoryTrain:  epoch  5, batch     1 | loss: 1.1719720Losses:  0.4933825731277466 0.7199997901916504
MemoryTrain:  epoch  5, batch     2 | loss: 1.2133824Losses:  0.6202159523963928 0.6502703428268433
MemoryTrain:  epoch  5, batch     3 | loss: 1.2704864Losses:  0.47253382205963135 0.621349573135376
MemoryTrain:  epoch  5, batch     4 | loss: 1.0938834Losses:  0.48878026008605957 0.6134355068206787
MemoryTrain:  epoch  6, batch     0 | loss: 1.1022158Losses:  0.5816167593002319 0.612978458404541
MemoryTrain:  epoch  6, batch     1 | loss: 1.1945952Losses:  0.42182421684265137 0.7161573171615601
MemoryTrain:  epoch  6, batch     2 | loss: 1.1379815Losses:  0.6012523770332336 0.8093815445899963
MemoryTrain:  epoch  6, batch     3 | loss: 1.4106339Losses:  0.46261441707611084 0.5736554861068726
MemoryTrain:  epoch  6, batch     4 | loss: 1.0362699Losses:  0.5396848320960999 0.6611411571502686
MemoryTrain:  epoch  7, batch     0 | loss: 1.2008259Losses:  0.42539849877357483 0.6629554033279419
MemoryTrain:  epoch  7, batch     1 | loss: 1.0883539Losses:  0.5191450119018555 0.664219856262207
MemoryTrain:  epoch  7, batch     2 | loss: 1.1833649Losses:  0.46004411578178406 0.5030255317687988
MemoryTrain:  epoch  7, batch     3 | loss: 0.9630697Losses:  0.5047796964645386 0.6959556341171265
MemoryTrain:  epoch  7, batch     4 | loss: 1.2007353Losses:  0.43522924184799194 0.6061556339263916
MemoryTrain:  epoch  8, batch     0 | loss: 1.0413849Losses:  0.541988730430603 0.7724452614784241
MemoryTrain:  epoch  8, batch     1 | loss: 1.3144341Losses:  0.4456154704093933 0.5392554998397827
MemoryTrain:  epoch  8, batch     2 | loss: 0.9848710Losses:  0.5944651365280151 0.7167466282844543
MemoryTrain:  epoch  8, batch     3 | loss: 1.3112118Losses:  0.3649429976940155 0.5635963082313538
MemoryTrain:  epoch  8, batch     4 | loss: 0.9285393Losses:  0.43483293056488037 0.5201750993728638
MemoryTrain:  epoch  9, batch     0 | loss: 0.9550080Losses:  0.4907819628715515 0.7665094137191772
MemoryTrain:  epoch  9, batch     1 | loss: 1.2572913Losses:  0.4551926851272583 0.5899909138679504
MemoryTrain:  epoch  9, batch     2 | loss: 1.0451837Losses:  0.5100716948509216 0.6387614011764526
MemoryTrain:  epoch  9, batch     3 | loss: 1.1488330Losses:  0.4268614947795868 0.44320929050445557
MemoryTrain:  epoch  9, batch     4 | loss: 0.8700708
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 59.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 57.21%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 55.09%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 53.12%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 51.29%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 50.00%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 48.39%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 48.83%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 50.19%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 50.92%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 51.96%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 52.78%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 53.55%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 55.45%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 56.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 57.47%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 58.18%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.16%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 59.17%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 58.02%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 57.18%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 56.38%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 55.99%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 55.25%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 54.78%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 55.17%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 55.54%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 55.90%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 55.91%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 56.36%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 56.14%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 56.46%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 56.35%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 56.65%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 56.15%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 56.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 82.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.98%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 83.64%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 82.89%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 82.11%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.78%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 81.05%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 80.00%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 79.29%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 79.32%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 78.99%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 79.05%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 79.14%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 78.45%   [EVAL] batch:   76 | acc: 18.75%,  total acc: 77.68%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 76.84%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 76.19%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 75.70%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 74.92%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 76.36%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 76.07%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 75.21%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 74.38%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 73.63%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 72.96%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 72.18%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 71.68%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 72.07%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 72.23%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 72.39%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 72.15%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 71.63%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 71.00%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 69.64%   [EVAL] batch:  105 | acc: 6.25%,  total acc: 69.04%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 68.87%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 68.81%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 68.58%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 68.03%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 67.65%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 67.34%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 66.86%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 66.18%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 66.85%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 67.26%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 66.88%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 66.46%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 65.99%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 65.53%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 65.17%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 65.10%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 65.32%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 65.78%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 65.45%   [EVAL] batch:  140 | acc: 0.00%,  total acc: 64.98%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 64.61%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 64.29%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 64.06%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  151 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 65.85%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.91%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.25%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 66.86%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 66.64%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 66.35%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 66.03%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 66.83%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 66.63%   [EVAL] batch:  177 | acc: 18.75%,  total acc: 66.36%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 65.94%   [EVAL] batch:  180 | acc: 18.75%,  total acc: 65.68%   [EVAL] batch:  181 | acc: 25.00%,  total acc: 65.45%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 65.39%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 65.31%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 65.31%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 65.27%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 65.16%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.24%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 64.96%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 64.92%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 64.99%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 65.04%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 65.12%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.24%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 65.17%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 65.01%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 64.91%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 64.72%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 64.62%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 66.99%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 66.89%   [EVAL] batch:  232 | acc: 12.50%,  total acc: 66.66%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 66.48%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.33%   [EVAL] batch:  235 | acc: 43.75%,  total acc: 66.23%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 66.03%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 66.08%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 65.92%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 65.75%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 65.59%   [EVAL] batch:  249 | acc: 31.25%,  total acc: 65.45%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.18%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 66.33%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.39%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 66.34%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 66.17%   [EVAL] batch:  266 | acc: 31.25%,  total acc: 66.03%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:  268 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 65.60%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 65.38%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 65.17%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 64.93%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 64.76%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 64.55%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:  284 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 65.60%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 65.61%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.00%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  300 | acc: 37.50%,  total acc: 66.90%   [EVAL] batch:  301 | acc: 56.25%,  total acc: 66.87%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 66.93%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 67.00%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.13%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 67.39%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 67.26%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:  316 | acc: 43.75%,  total acc: 67.11%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 67.06%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 67.03%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 67.27%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 67.44%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 67.48%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 67.43%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  332 | acc: 50.00%,  total acc: 67.42%   [EVAL] batch:  333 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 67.46%   [EVAL] batch:  335 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 67.49%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:  338 | acc: 50.00%,  total acc: 67.40%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 67.30%   [EVAL] batch:  340 | acc: 43.75%,  total acc: 67.23%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 67.14%   [EVAL] batch:  342 | acc: 18.75%,  total acc: 67.00%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 66.90%   [EVAL] batch:  345 | acc: 75.00%,  total acc: 66.93%   [EVAL] batch:  346 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 66.95%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 66.99%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 66.95%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:  351 | acc: 56.25%,  total acc: 66.85%   [EVAL] batch:  352 | acc: 37.50%,  total acc: 66.77%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  354 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 66.61%   [EVAL] batch:  357 | acc: 0.00%,  total acc: 66.43%   [EVAL] batch:  358 | acc: 6.25%,  total acc: 66.26%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 66.09%   [EVAL] batch:  360 | acc: 6.25%,  total acc: 65.93%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 65.83%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 65.79%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  375 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 66.73%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 66.74%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 66.66%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:  388 | acc: 31.25%,  total acc: 66.63%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 66.62%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 66.55%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 66.49%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 66.55%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.70%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 66.99%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 66.90%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 66.89%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 66.79%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 66.73%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 66.68%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 66.55%   [EVAL] batch:  409 | acc: 25.00%,  total acc: 66.45%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 66.33%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 66.22%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 66.18%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  415 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 66.26%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 66.16%   [EVAL] batch:  420 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  421 | acc: 37.50%,  total acc: 66.05%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  423 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 66.69%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 66.70%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 66.62%   [EVAL] batch:  440 | acc: 18.75%,  total acc: 66.51%   [EVAL] batch:  441 | acc: 6.25%,  total acc: 66.37%   [EVAL] batch:  442 | acc: 6.25%,  total acc: 66.24%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 66.20%   [EVAL] batch:  444 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 66.31%   [EVAL] batch:  448 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:  450 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  452 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  453 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 66.64%   [EVAL] batch:  457 | acc: 50.00%,  total acc: 66.61%   [EVAL] batch:  458 | acc: 31.25%,  total acc: 66.53%   [EVAL] batch:  459 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  460 | acc: 43.75%,  total acc: 66.47%   [EVAL] batch:  461 | acc: 31.25%,  total acc: 66.40%   [EVAL] batch:  462 | acc: 37.50%,  total acc: 66.33%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 66.19%   [EVAL] batch:  464 | acc: 0.00%,  total acc: 66.05%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 65.91%   [EVAL] batch:  466 | acc: 6.25%,  total acc: 65.78%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 65.65%   [EVAL] batch:  468 | acc: 18.75%,  total acc: 65.55%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 65.66%   [EVAL] batch:  471 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  473 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  474 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 65.96%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  481 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.03%   [EVAL] batch:  483 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  484 | acc: 12.50%,  total acc: 65.82%   [EVAL] batch:  485 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  486 | acc: 25.00%,  total acc: 65.66%   [EVAL] batch:  487 | acc: 31.25%,  total acc: 65.59%   [EVAL] batch:  488 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  489 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  491 | acc: 62.50%,  total acc: 65.59%   [EVAL] batch:  492 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 65.60%   [EVAL] batch:  494 | acc: 43.75%,  total acc: 65.56%   [EVAL] batch:  495 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:  496 | acc: 56.25%,  total acc: 65.54%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 65.54%   [EVAL] batch:  498 | acc: 56.25%,  total acc: 65.52%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 65.53%   
cur_acc:  ['0.9375', '0.7123', '0.7599', '0.7103', '0.8046', '0.6736', '0.6865', '0.5615']
his_acc:  ['0.9375', '0.8305', '0.7826', '0.7352', '0.7318', '0.7055', '0.6934', '0.6552']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.62559700012207 1.7023675441741943
CurrentTrain: epoch  0, batch     0 | loss: 13.3279648Losses:  13.498717308044434 1.8431247472763062
CurrentTrain: epoch  0, batch     1 | loss: 15.3418417Losses:  13.390158653259277 2.0560107231140137
CurrentTrain: epoch  0, batch     2 | loss: 15.4461689Losses:  13.307865142822266 1.6393992900848389
CurrentTrain: epoch  0, batch     3 | loss: 14.9472647Losses:  13.652499198913574 1.5304219722747803
CurrentTrain: epoch  0, batch     4 | loss: 15.1829214Losses:  13.400910377502441 1.9921904802322388
CurrentTrain: epoch  0, batch     5 | loss: 15.3931007Losses:  12.952593803405762 1.4154820442199707
CurrentTrain: epoch  0, batch     6 | loss: 14.3680763Losses:  12.937982559204102 1.63289213180542
CurrentTrain: epoch  0, batch     7 | loss: 14.5708752Losses:  12.950897216796875 1.559765338897705
CurrentTrain: epoch  0, batch     8 | loss: 14.5106621Losses:  12.67097282409668 1.7219935655593872
CurrentTrain: epoch  0, batch     9 | loss: 14.3929663Losses:  12.376504898071289 1.5919270515441895
CurrentTrain: epoch  0, batch    10 | loss: 13.9684315Losses:  12.077855110168457 1.4010505676269531
CurrentTrain: epoch  0, batch    11 | loss: 13.4789057Losses:  11.9238862991333 1.7233200073242188
CurrentTrain: epoch  0, batch    12 | loss: 13.6472063Losses:  11.807908058166504 1.5143680572509766
CurrentTrain: epoch  0, batch    13 | loss: 13.3222761Losses:  12.074760437011719 1.5809065103530884
CurrentTrain: epoch  0, batch    14 | loss: 13.6556673Losses:  12.005670547485352 1.49664306640625
CurrentTrain: epoch  0, batch    15 | loss: 13.5023136Losses:  11.732261657714844 1.6502923965454102
CurrentTrain: epoch  0, batch    16 | loss: 13.3825541Losses:  12.051519393920898 1.8170582056045532
CurrentTrain: epoch  0, batch    17 | loss: 13.8685780Losses:  11.57599925994873 1.9326075315475464
CurrentTrain: epoch  0, batch    18 | loss: 13.5086069Losses:  11.577896118164062 1.653204083442688
CurrentTrain: epoch  0, batch    19 | loss: 13.2311001Losses:  11.067866325378418 1.2260369062423706
CurrentTrain: epoch  0, batch    20 | loss: 12.2939034Losses:  10.97927474975586 1.4214942455291748
CurrentTrain: epoch  0, batch    21 | loss: 12.4007692Losses:  11.312378883361816 1.4264371395111084
CurrentTrain: epoch  0, batch    22 | loss: 12.7388163Losses:  11.061026573181152 1.3398288488388062
CurrentTrain: epoch  0, batch    23 | loss: 12.4008551Losses:  10.740283966064453 1.3831493854522705
CurrentTrain: epoch  0, batch    24 | loss: 12.1234331Losses:  10.84387493133545 1.286555290222168
CurrentTrain: epoch  0, batch    25 | loss: 12.1304302Losses:  10.412851333618164 1.2923346757888794
CurrentTrain: epoch  0, batch    26 | loss: 11.7051859Losses:  10.506355285644531 1.5026249885559082
CurrentTrain: epoch  0, batch    27 | loss: 12.0089798Losses:  9.994482040405273 1.1779708862304688
CurrentTrain: epoch  0, batch    28 | loss: 11.1724529Losses:  9.94234848022461 1.4268078804016113
CurrentTrain: epoch  0, batch    29 | loss: 11.3691559Losses:  9.984707832336426 1.274395227432251
CurrentTrain: epoch  0, batch    30 | loss: 11.2591028Losses:  10.033937454223633 1.4983890056610107
CurrentTrain: epoch  0, batch    31 | loss: 11.5323267Losses:  9.650261878967285 1.2085819244384766
CurrentTrain: epoch  0, batch    32 | loss: 10.8588438Losses:  9.844869613647461 1.406563401222229
CurrentTrain: epoch  0, batch    33 | loss: 11.2514334Losses:  9.637239456176758 1.208786964416504
CurrentTrain: epoch  0, batch    34 | loss: 10.8460264Losses:  9.286603927612305 1.3148982524871826
CurrentTrain: epoch  0, batch    35 | loss: 10.6015024Losses:  9.143235206604004 1.1002377271652222
CurrentTrain: epoch  0, batch    36 | loss: 10.2434731Losses:  9.065690040588379 1.2812484502792358
CurrentTrain: epoch  0, batch    37 | loss: 10.3469381Losses:  8.948051452636719 1.2610691785812378
CurrentTrain: epoch  0, batch    38 | loss: 10.2091208Losses:  9.115221977233887 1.2827584743499756
CurrentTrain: epoch  0, batch    39 | loss: 10.3979807Losses:  8.54700756072998 0.9407044649124146
CurrentTrain: epoch  0, batch    40 | loss: 9.4877119Losses:  8.722166061401367 1.1926705837249756
CurrentTrain: epoch  0, batch    41 | loss: 9.9148369Losses:  8.879159927368164 1.2388224601745605
CurrentTrain: epoch  0, batch    42 | loss: 10.1179829Losses:  8.417320251464844 1.3209261894226074
CurrentTrain: epoch  0, batch    43 | loss: 9.7382469Losses:  8.272051811218262 1.1288292407989502
CurrentTrain: epoch  0, batch    44 | loss: 9.4008808Losses:  8.35158634185791 1.0670430660247803
CurrentTrain: epoch  0, batch    45 | loss: 9.4186296Losses:  7.60491943359375 1.1108430624008179
CurrentTrain: epoch  0, batch    46 | loss: 8.7157621Losses:  7.664181709289551 1.0635192394256592
CurrentTrain: epoch  0, batch    47 | loss: 8.7277012Losses:  7.955803394317627 1.0758891105651855
CurrentTrain: epoch  0, batch    48 | loss: 9.0316925Losses:  7.748286247253418 0.9279705882072449
CurrentTrain: epoch  0, batch    49 | loss: 8.6762571Losses:  7.442619323730469 1.0248041152954102
CurrentTrain: epoch  0, batch    50 | loss: 8.4674234Losses:  7.249152660369873 1.0829137563705444
CurrentTrain: epoch  0, batch    51 | loss: 8.3320665Losses:  6.8071441650390625 1.0327122211456299
CurrentTrain: epoch  0, batch    52 | loss: 7.8398561Losses:  6.436059474945068 1.1778556108474731
CurrentTrain: epoch  0, batch    53 | loss: 7.6139150Losses:  7.209569931030273 1.062218189239502
CurrentTrain: epoch  0, batch    54 | loss: 8.2717876Losses:  6.771362781524658 0.9205223321914673
CurrentTrain: epoch  0, batch    55 | loss: 7.6918850Losses:  6.402800559997559 0.7752687931060791
CurrentTrain: epoch  0, batch    56 | loss: 7.1780691Losses:  6.339524269104004 0.820095419883728
CurrentTrain: epoch  0, batch    57 | loss: 7.1596198Losses:  6.301473617553711 1.1236369609832764
CurrentTrain: epoch  0, batch    58 | loss: 7.4251108Losses:  6.273965835571289 1.0389809608459473
CurrentTrain: epoch  0, batch    59 | loss: 7.3129468Losses:  6.558820724487305 1.0341391563415527
CurrentTrain: epoch  0, batch    60 | loss: 7.5929599Losses:  5.687191963195801 0.7403433322906494
CurrentTrain: epoch  0, batch    61 | loss: 6.4275351Losses:  6.2003326416015625 0.45144152641296387
CurrentTrain: epoch  0, batch    62 | loss: 6.6517744Losses:  6.019719123840332 1.0530037879943848
CurrentTrain: epoch  1, batch     0 | loss: 7.0727229Losses:  5.572214126586914 0.9689668416976929
CurrentTrain: epoch  1, batch     1 | loss: 6.5411811Losses:  6.017858982086182 1.1798062324523926
CurrentTrain: epoch  1, batch     2 | loss: 7.1976652Losses:  5.679951190948486 0.8188413381576538
CurrentTrain: epoch  1, batch     3 | loss: 6.4987926Losses:  5.5201520919799805 0.5753377676010132
CurrentTrain: epoch  1, batch     4 | loss: 6.0954900Losses:  5.681604385375977 0.6943726539611816
CurrentTrain: epoch  1, batch     5 | loss: 6.3759770Losses:  5.439298152923584 0.7717609405517578
CurrentTrain: epoch  1, batch     6 | loss: 6.2110591Losses:  5.7502946853637695 0.7136340141296387
CurrentTrain: epoch  1, batch     7 | loss: 6.4639287Losses:  5.49167537689209 0.7717518210411072
CurrentTrain: epoch  1, batch     8 | loss: 6.2634273Losses:  5.572575569152832 0.7429969906806946
CurrentTrain: epoch  1, batch     9 | loss: 6.3155727Losses:  5.733434677124023 0.8284940123558044
CurrentTrain: epoch  1, batch    10 | loss: 6.5619287Losses:  5.427626609802246 0.6773414015769958
CurrentTrain: epoch  1, batch    11 | loss: 6.1049681Losses:  5.940102577209473 0.9607361555099487
CurrentTrain: epoch  1, batch    12 | loss: 6.9008389Losses:  5.6059393882751465 0.8021091818809509
CurrentTrain: epoch  1, batch    13 | loss: 6.4080486Losses:  5.446768760681152 0.7331290245056152
CurrentTrain: epoch  1, batch    14 | loss: 6.1798978Losses:  5.507376670837402 0.6924020051956177
CurrentTrain: epoch  1, batch    15 | loss: 6.1997786Losses:  5.805556297302246 0.7952450513839722
CurrentTrain: epoch  1, batch    16 | loss: 6.6008015Losses:  5.956508636474609 0.9842137098312378
CurrentTrain: epoch  1, batch    17 | loss: 6.9407225Losses:  6.097369194030762 0.9728673100471497
CurrentTrain: epoch  1, batch    18 | loss: 7.0702367Losses:  5.666250228881836 0.7201097011566162
CurrentTrain: epoch  1, batch    19 | loss: 6.3863602Losses:  5.40023136138916 0.5402698516845703
CurrentTrain: epoch  1, batch    20 | loss: 5.9405012Losses:  5.5889997482299805 0.7900174856185913
CurrentTrain: epoch  1, batch    21 | loss: 6.3790174Losses:  5.2085065841674805 0.6151272654533386
CurrentTrain: epoch  1, batch    22 | loss: 5.8236337Losses:  5.528729438781738 0.7787898182868958
CurrentTrain: epoch  1, batch    23 | loss: 6.3075194Losses:  5.6223649978637695 0.7259020805358887
CurrentTrain: epoch  1, batch    24 | loss: 6.3482671Losses:  5.416857719421387 0.626960277557373
CurrentTrain: epoch  1, batch    25 | loss: 6.0438180Losses:  5.584234237670898 0.6943950653076172
CurrentTrain: epoch  1, batch    26 | loss: 6.2786293Losses:  5.342180252075195 0.6953546404838562
CurrentTrain: epoch  1, batch    27 | loss: 6.0375347Losses:  5.095656394958496 0.5341081619262695
CurrentTrain: epoch  1, batch    28 | loss: 5.6297646Losses:  5.461443901062012 0.6952333450317383
CurrentTrain: epoch  1, batch    29 | loss: 6.1566772Losses:  5.458711624145508 0.5640550851821899
CurrentTrain: epoch  1, batch    30 | loss: 6.0227666Losses:  5.18192195892334 0.6208711266517639
CurrentTrain: epoch  1, batch    31 | loss: 5.8027930Losses:  5.087895393371582 0.5667343139648438
CurrentTrain: epoch  1, batch    32 | loss: 5.6546297Losses:  4.934282302856445 0.5880557894706726
CurrentTrain: epoch  1, batch    33 | loss: 5.5223379Losses:  4.834620475769043 0.5189832448959351
CurrentTrain: epoch  1, batch    34 | loss: 5.3536038Losses:  5.16066837310791 0.5729935765266418
CurrentTrain: epoch  1, batch    35 | loss: 5.7336621Losses:  5.282951831817627 0.7427873015403748
CurrentTrain: epoch  1, batch    36 | loss: 6.0257392Losses:  5.605765342712402 0.6225435733795166
CurrentTrain: epoch  1, batch    37 | loss: 6.2283087Losses:  5.051342487335205 0.525317907333374
CurrentTrain: epoch  1, batch    38 | loss: 5.5766602Losses:  5.45052433013916 0.5457637310028076
CurrentTrain: epoch  1, batch    39 | loss: 5.9962883Losses:  5.394072532653809 0.6808657050132751
CurrentTrain: epoch  1, batch    40 | loss: 6.0749383Losses:  5.553153038024902 0.5921210050582886
CurrentTrain: epoch  1, batch    41 | loss: 6.1452742Losses:  5.414288520812988 0.7136958837509155
CurrentTrain: epoch  1, batch    42 | loss: 6.1279845Losses:  5.053642272949219 0.4541317820549011
CurrentTrain: epoch  1, batch    43 | loss: 5.5077739Losses:  5.153350353240967 0.5751453638076782
CurrentTrain: epoch  1, batch    44 | loss: 5.7284956Losses:  5.083749771118164 0.4658679962158203
CurrentTrain: epoch  1, batch    45 | loss: 5.5496178Losses:  5.358327865600586 0.6828714609146118
CurrentTrain: epoch  1, batch    46 | loss: 6.0411992Losses:  4.754137992858887 0.37566837668418884
CurrentTrain: epoch  1, batch    47 | loss: 5.1298065Losses:  4.888659954071045 0.5581432580947876
CurrentTrain: epoch  1, batch    48 | loss: 5.4468031Losses:  4.903054237365723 0.6929704546928406
CurrentTrain: epoch  1, batch    49 | loss: 5.5960245Losses:  4.746429443359375 0.47862643003463745
CurrentTrain: epoch  1, batch    50 | loss: 5.2250557Losses:  5.227116107940674 0.4726683497428894
CurrentTrain: epoch  1, batch    51 | loss: 5.6997843Losses:  4.797316074371338 0.5313543081283569
CurrentTrain: epoch  1, batch    52 | loss: 5.3286705Losses:  4.958488941192627 0.544288694858551
CurrentTrain: epoch  1, batch    53 | loss: 5.5027776Losses:  4.866033554077148 0.44781574606895447
CurrentTrain: epoch  1, batch    54 | loss: 5.3138494Losses:  4.780342102050781 0.4395824372768402
CurrentTrain: epoch  1, batch    55 | loss: 5.2199244Losses:  5.088887691497803 0.5511989593505859
CurrentTrain: epoch  1, batch    56 | loss: 5.6400867Losses:  5.153301239013672 0.5914884209632874
CurrentTrain: epoch  1, batch    57 | loss: 5.7447896Losses:  5.16070556640625 0.45210784673690796
CurrentTrain: epoch  1, batch    58 | loss: 5.6128135Losses:  4.527662754058838 0.3837112486362457
CurrentTrain: epoch  1, batch    59 | loss: 4.9113741Losses:  4.728957176208496 0.47358474135398865
CurrentTrain: epoch  1, batch    60 | loss: 5.2025418Losses:  5.006132125854492 0.5202390551567078
CurrentTrain: epoch  1, batch    61 | loss: 5.5263710Losses:  4.51344108581543 0.40889060497283936
CurrentTrain: epoch  1, batch    62 | loss: 4.9223318Losses:  4.902261257171631 0.4807908236980438
CurrentTrain: epoch  2, batch     0 | loss: 5.3830519Losses:  4.647307395935059 0.33290889859199524
CurrentTrain: epoch  2, batch     1 | loss: 4.9802165Losses:  4.820380210876465 0.4933468699455261
CurrentTrain: epoch  2, batch     2 | loss: 5.3137269Losses:  4.4897613525390625 0.4001220762729645
CurrentTrain: epoch  2, batch     3 | loss: 4.8898835Losses:  4.904035568237305 0.440514475107193
CurrentTrain: epoch  2, batch     4 | loss: 5.3445501Losses:  4.585653781890869 0.4182540774345398
CurrentTrain: epoch  2, batch     5 | loss: 5.0039077Losses:  4.771581649780273 0.4745408594608307
CurrentTrain: epoch  2, batch     6 | loss: 5.2461224Losses:  4.482346534729004 0.42561084032058716
CurrentTrain: epoch  2, batch     7 | loss: 4.9079576Losses:  4.771384239196777 0.4768809676170349
CurrentTrain: epoch  2, batch     8 | loss: 5.2482653Losses:  4.852306365966797 0.3999013304710388
CurrentTrain: epoch  2, batch     9 | loss: 5.2522078Losses:  4.576434135437012 0.36001718044281006
CurrentTrain: epoch  2, batch    10 | loss: 4.9364514Losses:  4.876896858215332 0.4811477065086365
CurrentTrain: epoch  2, batch    11 | loss: 5.3580446Losses:  4.965578556060791 0.4113152027130127
CurrentTrain: epoch  2, batch    12 | loss: 5.3768940Losses:  4.592746734619141 0.43938887119293213
CurrentTrain: epoch  2, batch    13 | loss: 5.0321355Losses:  4.635031223297119 0.2551496624946594
CurrentTrain: epoch  2, batch    14 | loss: 4.8901811Losses:  4.735663414001465 0.3819652199745178
CurrentTrain: epoch  2, batch    15 | loss: 5.1176286Losses:  4.525671005249023 0.3257283568382263
CurrentTrain: epoch  2, batch    16 | loss: 4.8513994Losses:  4.78206729888916 0.3737736940383911
CurrentTrain: epoch  2, batch    17 | loss: 5.1558409Losses:  4.663015365600586 0.3400081396102905
CurrentTrain: epoch  2, batch    18 | loss: 5.0030236Losses:  4.609981060028076 0.32530462741851807
CurrentTrain: epoch  2, batch    19 | loss: 4.9352856Losses:  4.393475532531738 0.2450573444366455
CurrentTrain: epoch  2, batch    20 | loss: 4.6385326Losses:  4.666594982147217 0.3785340487957001
CurrentTrain: epoch  2, batch    21 | loss: 5.0451288Losses:  4.691488742828369 0.35872241854667664
CurrentTrain: epoch  2, batch    22 | loss: 5.0502110Losses:  4.699724197387695 0.33345016837120056
CurrentTrain: epoch  2, batch    23 | loss: 5.0331745Losses:  4.729564189910889 0.292019784450531
CurrentTrain: epoch  2, batch    24 | loss: 5.0215840Losses:  4.4630889892578125 0.3208182156085968
CurrentTrain: epoch  2, batch    25 | loss: 4.7839074Losses:  4.445954322814941 0.30488184094429016
CurrentTrain: epoch  2, batch    26 | loss: 4.7508364Losses:  5.015575885772705 0.38253089785575867
CurrentTrain: epoch  2, batch    27 | loss: 5.3981066Losses:  4.379237174987793 0.34769952297210693
CurrentTrain: epoch  2, batch    28 | loss: 4.7269368Losses:  4.486575126647949 0.3701065182685852
CurrentTrain: epoch  2, batch    29 | loss: 4.8566818Losses:  4.874933242797852 0.33371788263320923
CurrentTrain: epoch  2, batch    30 | loss: 5.2086511Losses:  4.642003059387207 0.42832720279693604
CurrentTrain: epoch  2, batch    31 | loss: 5.0703301Losses:  4.698666572570801 0.2829912304878235
CurrentTrain: epoch  2, batch    32 | loss: 4.9816580Losses:  4.404048919677734 0.22732490301132202
CurrentTrain: epoch  2, batch    33 | loss: 4.6313739Losses:  4.665234565734863 0.325204074382782
CurrentTrain: epoch  2, batch    34 | loss: 4.9904385Losses:  4.80141019821167 0.35918325185775757
CurrentTrain: epoch  2, batch    35 | loss: 5.1605935Losses:  4.492427825927734 0.3442780077457428
CurrentTrain: epoch  2, batch    36 | loss: 4.8367057Losses:  4.723167419433594 0.43532121181488037
CurrentTrain: epoch  2, batch    37 | loss: 5.1584888Losses:  4.555672645568848 0.34385064244270325
CurrentTrain: epoch  2, batch    38 | loss: 4.8995233Losses:  4.471489906311035 0.2630298137664795
CurrentTrain: epoch  2, batch    39 | loss: 4.7345200Losses:  4.5140838623046875 0.2721686065196991
CurrentTrain: epoch  2, batch    40 | loss: 4.7862525Losses:  5.075010299682617 0.5577179193496704
CurrentTrain: epoch  2, batch    41 | loss: 5.6327281Losses:  4.314276695251465 0.22067788243293762
CurrentTrain: epoch  2, batch    42 | loss: 4.5349545Losses:  4.290045261383057 0.2719239592552185
CurrentTrain: epoch  2, batch    43 | loss: 4.5619693Losses:  4.232320308685303 0.2163831889629364
CurrentTrain: epoch  2, batch    44 | loss: 4.4487033Losses:  4.502179145812988 0.22583623230457306
CurrentTrain: epoch  2, batch    45 | loss: 4.7280154Losses:  4.47780179977417 0.3383291959762573
CurrentTrain: epoch  2, batch    46 | loss: 4.8161311Losses:  4.925719261169434 0.5806876420974731
CurrentTrain: epoch  2, batch    47 | loss: 5.5064068Losses:  4.535471439361572 0.30204907059669495
CurrentTrain: epoch  2, batch    48 | loss: 4.8375206Losses:  4.6501383781433105 0.3425918221473694
CurrentTrain: epoch  2, batch    49 | loss: 4.9927301Losses:  4.657218933105469 0.25848010182380676
CurrentTrain: epoch  2, batch    50 | loss: 4.9156990Losses:  4.50258207321167 0.35213929414749146
CurrentTrain: epoch  2, batch    51 | loss: 4.8547215Losses:  4.641235828399658 0.31559818983078003
CurrentTrain: epoch  2, batch    52 | loss: 4.9568338Losses:  4.496740818023682 0.2118542343378067
CurrentTrain: epoch  2, batch    53 | loss: 4.7085953Losses:  4.312288284301758 0.1661066710948944
CurrentTrain: epoch  2, batch    54 | loss: 4.4783950Losses:  4.6231279373168945 0.3534640669822693
CurrentTrain: epoch  2, batch    55 | loss: 4.9765921Losses:  4.38344669342041 0.27151772379875183
CurrentTrain: epoch  2, batch    56 | loss: 4.6549644Losses:  4.514984607696533 0.3471182882785797
CurrentTrain: epoch  2, batch    57 | loss: 4.8621030Losses:  4.779064178466797 0.49302029609680176
CurrentTrain: epoch  2, batch    58 | loss: 5.2720842Losses:  4.556277751922607 0.24493935704231262
CurrentTrain: epoch  2, batch    59 | loss: 4.8012171Losses:  4.24123477935791 0.2920854389667511
CurrentTrain: epoch  2, batch    60 | loss: 4.5333204Losses:  4.514581680297852 0.23248779773712158
CurrentTrain: epoch  2, batch    61 | loss: 4.7470694Losses:  4.3530449867248535 0.15652990341186523
CurrentTrain: epoch  2, batch    62 | loss: 4.5095749Losses:  4.151369094848633 0.24127334356307983
CurrentTrain: epoch  3, batch     0 | loss: 4.3926425Losses:  4.687390327453613 0.33525991439819336
CurrentTrain: epoch  3, batch     1 | loss: 5.0226502Losses:  4.634919166564941 0.338539719581604
CurrentTrain: epoch  3, batch     2 | loss: 4.9734588Losses:  4.483107566833496 0.26341676712036133
CurrentTrain: epoch  3, batch     3 | loss: 4.7465243Losses:  4.172591686248779 0.2526462972164154
CurrentTrain: epoch  3, batch     4 | loss: 4.4252381Losses:  4.361215114593506 0.2852010428905487
CurrentTrain: epoch  3, batch     5 | loss: 4.6464162Losses:  4.282737731933594 0.21911445260047913
CurrentTrain: epoch  3, batch     6 | loss: 4.5018520Losses:  4.348133087158203 0.2504219114780426
CurrentTrain: epoch  3, batch     7 | loss: 4.5985551Losses:  4.257576942443848 0.23894479870796204
CurrentTrain: epoch  3, batch     8 | loss: 4.4965219Losses:  4.399872303009033 0.24980641901493073
CurrentTrain: epoch  3, batch     9 | loss: 4.6496787Losses:  4.531294345855713 0.24912555515766144
CurrentTrain: epoch  3, batch    10 | loss: 4.7804198Losses:  4.486778259277344 0.31793054938316345
CurrentTrain: epoch  3, batch    11 | loss: 4.8047090Losses:  4.647040367126465 0.1443936675786972
CurrentTrain: epoch  3, batch    12 | loss: 4.7914338Losses:  4.380938529968262 0.19260907173156738
CurrentTrain: epoch  3, batch    13 | loss: 4.5735474Losses:  4.282596111297607 0.28087884187698364
CurrentTrain: epoch  3, batch    14 | loss: 4.5634751Losses:  4.538041114807129 0.3393639028072357
CurrentTrain: epoch  3, batch    15 | loss: 4.8774052Losses:  4.247954845428467 0.23643721640110016
CurrentTrain: epoch  3, batch    16 | loss: 4.4843922Losses:  4.367893218994141 0.3070327043533325
CurrentTrain: epoch  3, batch    17 | loss: 4.6749258Losses:  4.252269744873047 0.282246857881546
CurrentTrain: epoch  3, batch    18 | loss: 4.5345168Losses:  4.2120490074157715 0.20399850606918335
CurrentTrain: epoch  3, batch    19 | loss: 4.4160476Losses:  4.460671424865723 0.3053201735019684
CurrentTrain: epoch  3, batch    20 | loss: 4.7659917Losses:  4.240546703338623 0.22446617484092712
CurrentTrain: epoch  3, batch    21 | loss: 4.4650130Losses:  4.241825103759766 0.2527158260345459
CurrentTrain: epoch  3, batch    22 | loss: 4.4945412Losses:  4.139719009399414 0.16158050298690796
CurrentTrain: epoch  3, batch    23 | loss: 4.3012996Losses:  4.313738822937012 0.29033443331718445
CurrentTrain: epoch  3, batch    24 | loss: 4.6040730Losses:  4.195364475250244 0.11980007588863373
CurrentTrain: epoch  3, batch    25 | loss: 4.3151646Losses:  4.286064624786377 0.2241211235523224
CurrentTrain: epoch  3, batch    26 | loss: 4.5101857Losses:  4.2760186195373535 0.2882702350616455
CurrentTrain: epoch  3, batch    27 | loss: 4.5642891Losses:  4.12185525894165 0.21826303005218506
CurrentTrain: epoch  3, batch    28 | loss: 4.3401184Losses:  4.22273063659668 0.18977828323841095
CurrentTrain: epoch  3, batch    29 | loss: 4.4125090Losses:  4.231950283050537 0.22545355558395386
CurrentTrain: epoch  3, batch    30 | loss: 4.4574037Losses:  4.382150650024414 0.2833048701286316
CurrentTrain: epoch  3, batch    31 | loss: 4.6654553Losses:  4.173798561096191 0.16366884112358093
CurrentTrain: epoch  3, batch    32 | loss: 4.3374672Losses:  4.227529525756836 0.24590419232845306
CurrentTrain: epoch  3, batch    33 | loss: 4.4734335Losses:  4.3096771240234375 0.18951672315597534
CurrentTrain: epoch  3, batch    34 | loss: 4.4991937Losses:  4.3619384765625 0.16055448353290558
CurrentTrain: epoch  3, batch    35 | loss: 4.5224929Losses:  4.257448196411133 0.2881680130958557
CurrentTrain: epoch  3, batch    36 | loss: 4.5456161Losses:  4.145102500915527 0.18227028846740723
CurrentTrain: epoch  3, batch    37 | loss: 4.3273726Losses:  4.257783889770508 0.2378867268562317
CurrentTrain: epoch  3, batch    38 | loss: 4.4956708Losses:  4.318263053894043 0.24693544209003448
CurrentTrain: epoch  3, batch    39 | loss: 4.5651984Losses:  4.239169597625732 0.18371936678886414
CurrentTrain: epoch  3, batch    40 | loss: 4.4228888Losses:  4.139740467071533 0.18900850415229797
CurrentTrain: epoch  3, batch    41 | loss: 4.3287492Losses:  4.135457992553711 0.1745680570602417
CurrentTrain: epoch  3, batch    42 | loss: 4.3100262Losses:  4.137348175048828 0.1149628534913063
CurrentTrain: epoch  3, batch    43 | loss: 4.2523112Losses:  4.222571849822998 0.1537168323993683
CurrentTrain: epoch  3, batch    44 | loss: 4.3762889Losses:  4.3175048828125 0.24370378255844116
CurrentTrain: epoch  3, batch    45 | loss: 4.5612087Losses:  4.201208114624023 0.19872814416885376
CurrentTrain: epoch  3, batch    46 | loss: 4.3999362Losses:  4.124199390411377 0.21066126227378845
CurrentTrain: epoch  3, batch    47 | loss: 4.3348608Losses:  4.3056230545043945 0.23342972993850708
CurrentTrain: epoch  3, batch    48 | loss: 4.5390530Losses:  4.223695278167725 0.11185336112976074
CurrentTrain: epoch  3, batch    49 | loss: 4.3355484Losses:  4.091848373413086 0.12818098068237305
CurrentTrain: epoch  3, batch    50 | loss: 4.2200294Losses:  4.101133823394775 0.19292566180229187
CurrentTrain: epoch  3, batch    51 | loss: 4.2940593Losses:  4.140707969665527 0.175927072763443
CurrentTrain: epoch  3, batch    52 | loss: 4.3166351Losses:  4.098716735839844 0.19447512924671173
CurrentTrain: epoch  3, batch    53 | loss: 4.2931919Losses:  4.182048797607422 0.21896056830883026
CurrentTrain: epoch  3, batch    54 | loss: 4.4010096Losses:  4.4399213790893555 0.29754987359046936
CurrentTrain: epoch  3, batch    55 | loss: 4.7374711Losses:  4.1847686767578125 0.14465336501598358
CurrentTrain: epoch  3, batch    56 | loss: 4.3294220Losses:  4.2196879386901855 0.20671021938323975
CurrentTrain: epoch  3, batch    57 | loss: 4.4263983Losses:  4.437726974487305 0.3003532886505127
CurrentTrain: epoch  3, batch    58 | loss: 4.7380800Losses:  4.298696517944336 0.26785317063331604
CurrentTrain: epoch  3, batch    59 | loss: 4.5665498Losses:  4.133697986602783 0.2534294128417969
CurrentTrain: epoch  3, batch    60 | loss: 4.3871274Losses:  4.368731498718262 0.19081784784793854
CurrentTrain: epoch  3, batch    61 | loss: 4.5595493Losses:  4.046871185302734 0.08443233370780945
CurrentTrain: epoch  3, batch    62 | loss: 4.1313033Losses:  4.204565048217773 0.22025218605995178
CurrentTrain: epoch  4, batch     0 | loss: 4.4248171Losses:  4.1482038497924805 0.20717455446720123
CurrentTrain: epoch  4, batch     1 | loss: 4.3553786Losses:  4.381969451904297 0.34391918778419495
CurrentTrain: epoch  4, batch     2 | loss: 4.7258887Losses:  4.155503273010254 0.18234112858772278
CurrentTrain: epoch  4, batch     3 | loss: 4.3378444Losses:  4.253282070159912 0.2799470126628876
CurrentTrain: epoch  4, batch     4 | loss: 4.5332289Losses:  4.125906944274902 0.22671645879745483
CurrentTrain: epoch  4, batch     5 | loss: 4.3526235Losses:  4.12735652923584 0.18354369699954987
CurrentTrain: epoch  4, batch     6 | loss: 4.3109002Losses:  4.115685939788818 0.16446766257286072
CurrentTrain: epoch  4, batch     7 | loss: 4.2801538Losses:  4.248244285583496 0.16598576307296753
CurrentTrain: epoch  4, batch     8 | loss: 4.4142299Losses:  4.128284454345703 0.16714981198310852
CurrentTrain: epoch  4, batch     9 | loss: 4.2954345Losses:  4.182456970214844 0.15698008239269257
CurrentTrain: epoch  4, batch    10 | loss: 4.3394370Losses:  4.068571090698242 0.15210917592048645
CurrentTrain: epoch  4, batch    11 | loss: 4.2206802Losses:  4.0863938331604 0.1501772403717041
CurrentTrain: epoch  4, batch    12 | loss: 4.2365713Losses:  4.064232349395752 0.1826682984828949
CurrentTrain: epoch  4, batch    13 | loss: 4.2469006Losses:  4.142961025238037 0.1355544626712799
CurrentTrain: epoch  4, batch    14 | loss: 4.2785153Losses:  4.10915470123291 0.19908563792705536
CurrentTrain: epoch  4, batch    15 | loss: 4.3082404Losses:  4.200411796569824 0.21843524277210236
CurrentTrain: epoch  4, batch    16 | loss: 4.4188471Losses:  4.073185920715332 0.1047109067440033
CurrentTrain: epoch  4, batch    17 | loss: 4.1778970Losses:  4.094630718231201 0.18836304545402527
CurrentTrain: epoch  4, batch    18 | loss: 4.2829938Losses:  4.089877605438232 0.1713225096464157
CurrentTrain: epoch  4, batch    19 | loss: 4.2612000Losses:  4.101186275482178 0.17362698912620544
CurrentTrain: epoch  4, batch    20 | loss: 4.2748132Losses:  4.0970330238342285 0.14580722153186798
CurrentTrain: epoch  4, batch    21 | loss: 4.2428403Losses:  4.015920639038086 0.14137166738510132
CurrentTrain: epoch  4, batch    22 | loss: 4.1572924Losses:  4.130255699157715 0.12572751939296722
CurrentTrain: epoch  4, batch    23 | loss: 4.2559834Losses:  4.075836181640625 0.21499916911125183
CurrentTrain: epoch  4, batch    24 | loss: 4.2908354Losses:  4.075122356414795 0.11654496192932129
CurrentTrain: epoch  4, batch    25 | loss: 4.1916676Losses:  4.117814540863037 0.13731099665164948
CurrentTrain: epoch  4, batch    26 | loss: 4.2551255Losses:  4.122322082519531 0.18771055340766907
CurrentTrain: epoch  4, batch    27 | loss: 4.3100328Losses:  4.111654281616211 0.12732729315757751
CurrentTrain: epoch  4, batch    28 | loss: 4.2389817Losses:  4.077727317810059 0.15319740772247314
CurrentTrain: epoch  4, batch    29 | loss: 4.2309246Losses:  4.064193248748779 0.17814598977565765
CurrentTrain: epoch  4, batch    30 | loss: 4.2423391Losses:  4.117736339569092 0.18236297369003296
CurrentTrain: epoch  4, batch    31 | loss: 4.3000994Losses:  4.045182228088379 0.13907477259635925
CurrentTrain: epoch  4, batch    32 | loss: 4.1842570Losses:  4.135047912597656 0.15872617065906525
CurrentTrain: epoch  4, batch    33 | loss: 4.2937741Losses:  4.060291767120361 0.16046249866485596
CurrentTrain: epoch  4, batch    34 | loss: 4.2207541Losses:  4.033071517944336 0.12638935446739197
CurrentTrain: epoch  4, batch    35 | loss: 4.1594610Losses:  4.034181118011475 0.1578870415687561
CurrentTrain: epoch  4, batch    36 | loss: 4.1920681Losses:  4.066552639007568 0.06538990139961243
CurrentTrain: epoch  4, batch    37 | loss: 4.1319427Losses:  4.062263011932373 0.15157422423362732
CurrentTrain: epoch  4, batch    38 | loss: 4.2138371Losses:  4.082159996032715 0.19775746762752533
CurrentTrain: epoch  4, batch    39 | loss: 4.2799172Losses:  4.0394816398620605 0.14942516386508942
CurrentTrain: epoch  4, batch    40 | loss: 4.1889067Losses:  4.068264484405518 0.14745473861694336
CurrentTrain: epoch  4, batch    41 | loss: 4.2157192Losses:  4.060174465179443 0.18091395497322083
CurrentTrain: epoch  4, batch    42 | loss: 4.2410884Losses:  4.089472770690918 0.18843457102775574
CurrentTrain: epoch  4, batch    43 | loss: 4.2779074Losses:  4.086385726928711 0.1189250499010086
CurrentTrain: epoch  4, batch    44 | loss: 4.2053108Losses:  4.1146955490112305 0.08179166913032532
CurrentTrain: epoch  4, batch    45 | loss: 4.1964874Losses:  4.182099342346191 0.17217135429382324
CurrentTrain: epoch  4, batch    46 | loss: 4.3542709Losses:  4.016023635864258 0.16034598648548126
CurrentTrain: epoch  4, batch    47 | loss: 4.1763697Losses:  4.115940093994141 0.12721103429794312
CurrentTrain: epoch  4, batch    48 | loss: 4.2431512Losses:  4.020037651062012 0.175136536359787
CurrentTrain: epoch  4, batch    49 | loss: 4.1951742Losses:  4.043186187744141 0.16016250848770142
CurrentTrain: epoch  4, batch    50 | loss: 4.2033486Losses:  4.265463352203369 0.2500196397304535
CurrentTrain: epoch  4, batch    51 | loss: 4.5154829Losses:  4.146291732788086 0.22198370099067688
CurrentTrain: epoch  4, batch    52 | loss: 4.3682756Losses:  4.056240558624268 0.1260031908750534
CurrentTrain: epoch  4, batch    53 | loss: 4.1822438Losses:  4.075055122375488 0.1476532518863678
CurrentTrain: epoch  4, batch    54 | loss: 4.2227082Losses:  4.097463607788086 0.08068274706602097
CurrentTrain: epoch  4, batch    55 | loss: 4.1781464Losses:  4.053061485290527 0.12085393816232681
CurrentTrain: epoch  4, batch    56 | loss: 4.1739154Losses:  3.9834060668945312 0.15357887744903564
CurrentTrain: epoch  4, batch    57 | loss: 4.1369848Losses:  4.008404731750488 0.1407851278781891
CurrentTrain: epoch  4, batch    58 | loss: 4.1491899Losses:  4.077394485473633 0.1801627278327942
CurrentTrain: epoch  4, batch    59 | loss: 4.2575574Losses:  4.032408714294434 0.1105734258890152
CurrentTrain: epoch  4, batch    60 | loss: 4.1429820Losses:  4.026609420776367 0.17074623703956604
CurrentTrain: epoch  4, batch    61 | loss: 4.1973557Losses:  4.037222385406494 0.06503238528966904
CurrentTrain: epoch  4, batch    62 | loss: 4.1022549Losses:  4.019046783447266 0.10490197688341141
CurrentTrain: epoch  5, batch     0 | loss: 4.1239486Losses:  4.037283897399902 0.11816422641277313
CurrentTrain: epoch  5, batch     1 | loss: 4.1554480Losses:  4.010184288024902 0.14129424095153809
CurrentTrain: epoch  5, batch     2 | loss: 4.1514788Losses:  4.034882545471191 0.14572495222091675
CurrentTrain: epoch  5, batch     3 | loss: 4.1806073Losses:  4.0416412353515625 0.1312236487865448
CurrentTrain: epoch  5, batch     4 | loss: 4.1728649Losses:  4.001002788543701 0.16125625371932983
CurrentTrain: epoch  5, batch     5 | loss: 4.1622591Losses:  4.009655475616455 0.11800559610128403
CurrentTrain: epoch  5, batch     6 | loss: 4.1276612Losses:  4.083457946777344 0.12709426879882812
CurrentTrain: epoch  5, batch     7 | loss: 4.2105522Losses:  4.067793846130371 0.12645748257637024
CurrentTrain: epoch  5, batch     8 | loss: 4.1942515Losses:  4.048169136047363 0.1558704674243927
CurrentTrain: epoch  5, batch     9 | loss: 4.2040396Losses:  4.035863876342773 0.1697186827659607
CurrentTrain: epoch  5, batch    10 | loss: 4.2055826Losses:  4.006490230560303 0.1318538784980774
CurrentTrain: epoch  5, batch    11 | loss: 4.1383443Losses:  4.0146684646606445 0.16579486429691315
CurrentTrain: epoch  5, batch    12 | loss: 4.1804633Losses:  4.035927772521973 0.11187714338302612
CurrentTrain: epoch  5, batch    13 | loss: 4.1478047Losses:  4.021564483642578 0.17283311486244202
CurrentTrain: epoch  5, batch    14 | loss: 4.1943974Losses:  3.9955148696899414 0.16561275720596313
CurrentTrain: epoch  5, batch    15 | loss: 4.1611276Losses:  4.031834125518799 0.13611334562301636
CurrentTrain: epoch  5, batch    16 | loss: 4.1679473Losses:  4.020603179931641 0.13099008798599243
CurrentTrain: epoch  5, batch    17 | loss: 4.1515932Losses:  4.0087361335754395 0.12040217220783234
CurrentTrain: epoch  5, batch    18 | loss: 4.1291385Losses:  3.965379238128662 0.11950461566448212
CurrentTrain: epoch  5, batch    19 | loss: 4.0848837Losses:  4.062653541564941 0.11089450865983963
CurrentTrain: epoch  5, batch    20 | loss: 4.1735482Losses:  4.040102958679199 0.13504517078399658
CurrentTrain: epoch  5, batch    21 | loss: 4.1751480Losses:  4.048961639404297 0.15784680843353271
CurrentTrain: epoch  5, batch    22 | loss: 4.2068086Losses:  4.007116794586182 0.1041385680437088
CurrentTrain: epoch  5, batch    23 | loss: 4.1112552Losses:  3.9237313270568848 0.14561091363430023
CurrentTrain: epoch  5, batch    24 | loss: 4.0693421Losses:  3.9704935550689697 0.13471198081970215
CurrentTrain: epoch  5, batch    25 | loss: 4.1052055Losses:  4.0110673904418945 0.15155228972434998
CurrentTrain: epoch  5, batch    26 | loss: 4.1626196Losses:  4.033827781677246 0.12604668736457825
CurrentTrain: epoch  5, batch    27 | loss: 4.1598744Losses:  4.063215255737305 0.09675120562314987
CurrentTrain: epoch  5, batch    28 | loss: 4.1599665Losses:  4.025928497314453 0.15274587273597717
CurrentTrain: epoch  5, batch    29 | loss: 4.1786742Losses:  4.004575729370117 0.10470706224441528
CurrentTrain: epoch  5, batch    30 | loss: 4.1092830Losses:  4.006119728088379 0.09459632635116577
CurrentTrain: epoch  5, batch    31 | loss: 4.1007161Losses:  4.005666255950928 0.14403894543647766
CurrentTrain: epoch  5, batch    32 | loss: 4.1497054Losses:  4.004697799682617 0.10438112169504166
CurrentTrain: epoch  5, batch    33 | loss: 4.1090789Losses:  4.046652317047119 0.15572971105575562
CurrentTrain: epoch  5, batch    34 | loss: 4.2023821Losses:  4.0687103271484375 0.15443482995033264
CurrentTrain: epoch  5, batch    35 | loss: 4.2231450Losses:  4.187221050262451 0.2107073813676834
CurrentTrain: epoch  5, batch    36 | loss: 4.3979282Losses:  4.031619071960449 0.06951465457677841
CurrentTrain: epoch  5, batch    37 | loss: 4.1011338Losses:  3.982600688934326 0.12650933861732483
CurrentTrain: epoch  5, batch    38 | loss: 4.1091099Losses:  4.00801944732666 0.12535667419433594
CurrentTrain: epoch  5, batch    39 | loss: 4.1333761Losses:  3.9894938468933105 0.12793755531311035
CurrentTrain: epoch  5, batch    40 | loss: 4.1174316Losses:  3.9648940563201904 0.10568571835756302
CurrentTrain: epoch  5, batch    41 | loss: 4.0705800Losses:  4.023714542388916 0.15160603821277618
CurrentTrain: epoch  5, batch    42 | loss: 4.1753206Losses:  4.000382423400879 0.09955880045890808
CurrentTrain: epoch  5, batch    43 | loss: 4.0999413Losses:  4.028724670410156 0.11619371175765991
CurrentTrain: epoch  5, batch    44 | loss: 4.1449184Losses:  4.0395588874816895 0.10996149480342865
CurrentTrain: epoch  5, batch    45 | loss: 4.1495204Losses:  4.040499687194824 0.09800390154123306
CurrentTrain: epoch  5, batch    46 | loss: 4.1385036Losses:  4.028830528259277 0.12725293636322021
CurrentTrain: epoch  5, batch    47 | loss: 4.1560836Losses:  3.9806408882141113 0.11950571835041046
CurrentTrain: epoch  5, batch    48 | loss: 4.1001468Losses:  4.036762714385986 0.13657601177692413
CurrentTrain: epoch  5, batch    49 | loss: 4.1733389Losses:  3.97528076171875 0.11499841511249542
CurrentTrain: epoch  5, batch    50 | loss: 4.0902791Losses:  4.057181358337402 0.1044389009475708
CurrentTrain: epoch  5, batch    51 | loss: 4.1616201Losses:  4.011922836303711 0.1353151500225067
CurrentTrain: epoch  5, batch    52 | loss: 4.1472378Losses:  4.0421881675720215 0.1345566362142563
CurrentTrain: epoch  5, batch    53 | loss: 4.1767449Losses:  3.9923925399780273 0.11147482693195343
CurrentTrain: epoch  5, batch    54 | loss: 4.1038675Losses:  4.0078125 0.10141463577747345
CurrentTrain: epoch  5, batch    55 | loss: 4.1092272Losses:  3.9634177684783936 0.09406247735023499
CurrentTrain: epoch  5, batch    56 | loss: 4.0574803Losses:  3.980264902114868 0.10484833270311356
CurrentTrain: epoch  5, batch    57 | loss: 4.0851130Losses:  4.068984508514404 0.10115432739257812
CurrentTrain: epoch  5, batch    58 | loss: 4.1701388Losses:  4.023608684539795 0.13857874274253845
CurrentTrain: epoch  5, batch    59 | loss: 4.1621876Losses:  3.9874978065490723 0.10382401943206787
CurrentTrain: epoch  5, batch    60 | loss: 4.0913219Losses:  3.988215923309326 0.1067567765712738
CurrentTrain: epoch  5, batch    61 | loss: 4.0949726Losses:  3.9233014583587646 0.1312885284423828
CurrentTrain: epoch  5, batch    62 | loss: 4.0545902Losses:  4.011711597442627 0.14289310574531555
CurrentTrain: epoch  6, batch     0 | loss: 4.1546049Losses:  4.026034355163574 0.10986579954624176
CurrentTrain: epoch  6, batch     1 | loss: 4.1359000Losses:  3.986393451690674 0.09470658749341965
CurrentTrain: epoch  6, batch     2 | loss: 4.0811000Losses:  3.998166084289551 0.1055937260389328
CurrentTrain: epoch  6, batch     3 | loss: 4.1037598Losses:  4.026933670043945 0.11886720359325409
CurrentTrain: epoch  6, batch     4 | loss: 4.1458011Losses:  3.983579635620117 0.10802961885929108
CurrentTrain: epoch  6, batch     5 | loss: 4.0916095Losses:  3.9838950634002686 0.13486286997795105
CurrentTrain: epoch  6, batch     6 | loss: 4.1187577Losses:  3.974327802658081 0.11935190856456757
CurrentTrain: epoch  6, batch     7 | loss: 4.0936799Losses:  3.9930875301361084 0.13286791741847992
CurrentTrain: epoch  6, batch     8 | loss: 4.1259556Losses:  4.02328634262085 0.07910943031311035
CurrentTrain: epoch  6, batch     9 | loss: 4.1023960Losses:  4.001160621643066 0.09883768856525421
CurrentTrain: epoch  6, batch    10 | loss: 4.0999985Losses:  3.9723780155181885 0.09494911134243011
CurrentTrain: epoch  6, batch    11 | loss: 4.0673270Losses:  3.9839439392089844 0.13145719468593597
CurrentTrain: epoch  6, batch    12 | loss: 4.1154013Losses:  3.9770469665527344 0.11998875439167023
CurrentTrain: epoch  6, batch    13 | loss: 4.0970359Losses:  3.992410659790039 0.13487869501113892
CurrentTrain: epoch  6, batch    14 | loss: 4.1272893Losses:  3.997663736343384 0.08240079879760742
CurrentTrain: epoch  6, batch    15 | loss: 4.0800648Losses:  3.949648857116699 0.1305314302444458
CurrentTrain: epoch  6, batch    16 | loss: 4.0801802Losses:  3.992511749267578 0.13082575798034668
CurrentTrain: epoch  6, batch    17 | loss: 4.1233377Losses:  4.000425338745117 0.08325308561325073
CurrentTrain: epoch  6, batch    18 | loss: 4.0836782Losses:  3.9779787063598633 0.10173974931240082
CurrentTrain: epoch  6, batch    19 | loss: 4.0797186Losses:  3.9969210624694824 0.14162299036979675
CurrentTrain: epoch  6, batch    20 | loss: 4.1385441Losses:  3.9627041816711426 0.10735073685646057
CurrentTrain: epoch  6, batch    21 | loss: 4.0700550Losses:  3.986049175262451 0.10419496893882751
CurrentTrain: epoch  6, batch    22 | loss: 4.0902443Losses:  3.992053270339966 0.10472876578569412
CurrentTrain: epoch  6, batch    23 | loss: 4.0967822Losses:  3.950199842453003 0.11780823022127151
CurrentTrain: epoch  6, batch    24 | loss: 4.0680079Losses:  4.014256000518799 0.09098818153142929
CurrentTrain: epoch  6, batch    25 | loss: 4.1052442Losses:  3.987539768218994 0.09306851774454117
CurrentTrain: epoch  6, batch    26 | loss: 4.0806084Losses:  3.9755163192749023 0.13940086960792542
CurrentTrain: epoch  6, batch    27 | loss: 4.1149173Losses:  4.032081604003906 0.08396347612142563
CurrentTrain: epoch  6, batch    28 | loss: 4.1160450Losses:  4.006952285766602 0.1012905091047287
CurrentTrain: epoch  6, batch    29 | loss: 4.1082430Losses:  3.984572410583496 0.1012662947177887
CurrentTrain: epoch  6, batch    30 | loss: 4.0858388Losses:  4.003780841827393 0.12119390815496445
CurrentTrain: epoch  6, batch    31 | loss: 4.1249747Losses:  3.8924758434295654 0.09254996478557587
CurrentTrain: epoch  6, batch    32 | loss: 3.9850259Losses:  4.02377986907959 0.08618029952049255
CurrentTrain: epoch  6, batch    33 | loss: 4.1099601Losses:  4.012755393981934 0.08464378118515015
CurrentTrain: epoch  6, batch    34 | loss: 4.0973992Losses:  3.977447509765625 0.07019162178039551
CurrentTrain: epoch  6, batch    35 | loss: 4.0476389Losses:  3.924640655517578 0.09778487682342529
CurrentTrain: epoch  6, batch    36 | loss: 4.0224257Losses:  4.003362655639648 0.0713055282831192
CurrentTrain: epoch  6, batch    37 | loss: 4.0746684Losses:  3.9633569717407227 0.09949252009391785
CurrentTrain: epoch  6, batch    38 | loss: 4.0628495Losses:  4.011782169342041 0.08361437916755676
CurrentTrain: epoch  6, batch    39 | loss: 4.0953965Losses:  3.9774434566497803 0.1089727133512497
CurrentTrain: epoch  6, batch    40 | loss: 4.0864162Losses:  4.000204086303711 0.10477877408266068
CurrentTrain: epoch  6, batch    41 | loss: 4.1049829Losses:  3.969111919403076 0.10788320004940033
CurrentTrain: epoch  6, batch    42 | loss: 4.0769949Losses:  3.961996555328369 0.11384615302085876
CurrentTrain: epoch  6, batch    43 | loss: 4.0758429Losses:  3.961226224899292 0.08819293230772018
CurrentTrain: epoch  6, batch    44 | loss: 4.0494189Losses:  3.982832431793213 0.05409267544746399
CurrentTrain: epoch  6, batch    45 | loss: 4.0369253Losses:  3.9814493656158447 0.12874534726142883
CurrentTrain: epoch  6, batch    46 | loss: 4.1101947Losses:  4.203420639038086 0.2122202217578888
CurrentTrain: epoch  6, batch    47 | loss: 4.4156408Losses:  3.979384422302246 0.14644508063793182
CurrentTrain: epoch  6, batch    48 | loss: 4.1258297Losses:  3.9498023986816406 0.11037543416023254
CurrentTrain: epoch  6, batch    49 | loss: 4.0601778Losses:  4.006569862365723 0.12879124283790588
CurrentTrain: epoch  6, batch    50 | loss: 4.1353612Losses:  3.994495391845703 0.08678264915943146
CurrentTrain: epoch  6, batch    51 | loss: 4.0812778Losses:  3.990415573120117 0.10536382347345352
CurrentTrain: epoch  6, batch    52 | loss: 4.0957794Losses:  3.9883475303649902 0.10285435616970062
CurrentTrain: epoch  6, batch    53 | loss: 4.0912018Losses:  3.984935998916626 0.07579079270362854
CurrentTrain: epoch  6, batch    54 | loss: 4.0607266Losses:  3.941840887069702 0.10617358982563019
CurrentTrain: epoch  6, batch    55 | loss: 4.0480146Losses:  3.9542007446289062 0.08214147388935089
CurrentTrain: epoch  6, batch    56 | loss: 4.0363421Losses:  3.930807590484619 0.07611517608165741
CurrentTrain: epoch  6, batch    57 | loss: 4.0069227Losses:  3.9940414428710938 0.0822194516658783
CurrentTrain: epoch  6, batch    58 | loss: 4.0762610Losses:  3.9780917167663574 0.10403146594762802
CurrentTrain: epoch  6, batch    59 | loss: 4.0821233Losses:  3.922187566757202 0.11183394491672516
CurrentTrain: epoch  6, batch    60 | loss: 4.0340214Losses:  3.9569811820983887 0.08710719645023346
CurrentTrain: epoch  6, batch    61 | loss: 4.0440884Losses:  3.909921884536743 0.06804364919662476
CurrentTrain: epoch  6, batch    62 | loss: 3.9779656Losses:  3.9507992267608643 0.09219606220722198
CurrentTrain: epoch  7, batch     0 | loss: 4.0429955Losses:  3.9929323196411133 0.09288899600505829
CurrentTrain: epoch  7, batch     1 | loss: 4.0858212Losses:  3.962618827819824 0.05989938601851463
CurrentTrain: epoch  7, batch     2 | loss: 4.0225182Losses:  3.9615445137023926 0.09749966114759445
CurrentTrain: epoch  7, batch     3 | loss: 4.0590444Losses:  3.9673333168029785 0.1386246681213379
CurrentTrain: epoch  7, batch     4 | loss: 4.1059580Losses:  3.9722752571105957 0.10497105121612549
CurrentTrain: epoch  7, batch     5 | loss: 4.0772462Losses:  3.858754873275757 0.08194021880626678
CurrentTrain: epoch  7, batch     6 | loss: 3.9406950Losses:  3.9991660118103027 0.09094682335853577
CurrentTrain: epoch  7, batch     7 | loss: 4.0901127Losses:  3.981271505355835 0.06521938741207123
CurrentTrain: epoch  7, batch     8 | loss: 4.0464907Losses:  3.974264144897461 0.10771733522415161
CurrentTrain: epoch  7, batch     9 | loss: 4.0819817Losses:  3.9340622425079346 0.1126575767993927
CurrentTrain: epoch  7, batch    10 | loss: 4.0467200Losses:  3.9693503379821777 0.11414367705583572
CurrentTrain: epoch  7, batch    11 | loss: 4.0834942Losses:  4.0032196044921875 0.09312807768583298
CurrentTrain: epoch  7, batch    12 | loss: 4.0963478Losses:  3.9726169109344482 0.08431439101696014
CurrentTrain: epoch  7, batch    13 | loss: 4.0569315Losses:  3.964341640472412 0.10469590872526169
CurrentTrain: epoch  7, batch    14 | loss: 4.0690374Losses:  3.9490761756896973 0.0809093788266182
CurrentTrain: epoch  7, batch    15 | loss: 4.0299854Losses:  3.9698522090911865 0.10123030096292496
CurrentTrain: epoch  7, batch    16 | loss: 4.0710826Losses:  3.9464502334594727 0.1164250448346138
CurrentTrain: epoch  7, batch    17 | loss: 4.0628753Losses:  3.958256244659424 0.11135253310203552
CurrentTrain: epoch  7, batch    18 | loss: 4.0696087Losses:  3.996514320373535 0.10963708162307739
CurrentTrain: epoch  7, batch    19 | loss: 4.1061516Losses:  3.9598662853240967 0.07783892005681992
CurrentTrain: epoch  7, batch    20 | loss: 4.0377054Losses:  3.97238826751709 0.11521537601947784
CurrentTrain: epoch  7, batch    21 | loss: 4.0876036Losses:  3.9800167083740234 0.09843365103006363
CurrentTrain: epoch  7, batch    22 | loss: 4.0784502Losses:  3.97062349319458 0.11321030557155609
CurrentTrain: epoch  7, batch    23 | loss: 4.0838337Losses:  3.9153902530670166 0.07361287623643875
CurrentTrain: epoch  7, batch    24 | loss: 3.9890032Losses:  3.9673075675964355 0.11532069742679596
CurrentTrain: epoch  7, batch    25 | loss: 4.0826283Losses:  3.9411020278930664 0.05821167305111885
CurrentTrain: epoch  7, batch    26 | loss: 3.9993136Losses:  3.980947494506836 0.11983862519264221
CurrentTrain: epoch  7, batch    27 | loss: 4.1007862Losses:  3.9436023235321045 0.0924309492111206
CurrentTrain: epoch  7, batch    28 | loss: 4.0360332Losses:  3.958083391189575 0.11545440554618835
CurrentTrain: epoch  7, batch    29 | loss: 4.0735378Losses:  3.958660364151001 0.0917050689458847
CurrentTrain: epoch  7, batch    30 | loss: 4.0503654Losses:  3.9523773193359375 0.07349544763565063
CurrentTrain: epoch  7, batch    31 | loss: 4.0258727Losses:  3.9589219093322754 0.108245350420475
CurrentTrain: epoch  7, batch    32 | loss: 4.0671673Losses:  3.937187433242798 0.06264916807413101
CurrentTrain: epoch  7, batch    33 | loss: 3.9998367Losses:  3.9727001190185547 0.06961165368556976
CurrentTrain: epoch  7, batch    34 | loss: 4.0423117Losses:  3.9797251224517822 0.09205356240272522
CurrentTrain: epoch  7, batch    35 | loss: 4.0717788Losses:  3.9732518196105957 0.10807737708091736
CurrentTrain: epoch  7, batch    36 | loss: 4.0813293Losses:  3.930779457092285 0.09305122494697571
CurrentTrain: epoch  7, batch    37 | loss: 4.0238309Losses:  3.959129571914673 0.10384246706962585
CurrentTrain: epoch  7, batch    38 | loss: 4.0629721Losses:  3.989734172821045 0.07008329778909683
CurrentTrain: epoch  7, batch    39 | loss: 4.0598173Losses:  3.9325578212738037 0.09572867304086685
CurrentTrain: epoch  7, batch    40 | loss: 4.0282865Losses:  3.9086544513702393 0.10309994220733643
CurrentTrain: epoch  7, batch    41 | loss: 4.0117545Losses:  3.964761257171631 0.09194640070199966
CurrentTrain: epoch  7, batch    42 | loss: 4.0567079Losses:  3.948880195617676 0.07770104706287384
CurrentTrain: epoch  7, batch    43 | loss: 4.0265813Losses:  3.9878041744232178 0.10698604583740234
CurrentTrain: epoch  7, batch    44 | loss: 4.0947905Losses:  3.949699878692627 0.06558489054441452
CurrentTrain: epoch  7, batch    45 | loss: 4.0152845Losses:  3.9838144779205322 0.09641575068235397
CurrentTrain: epoch  7, batch    46 | loss: 4.0802302Losses:  3.945711612701416 0.10300292819738388
CurrentTrain: epoch  7, batch    47 | loss: 4.0487146Losses:  3.9894492626190186 0.04662472754716873
CurrentTrain: epoch  7, batch    48 | loss: 4.0360742Losses:  3.939378023147583 0.08235594630241394
CurrentTrain: epoch  7, batch    49 | loss: 4.0217338Losses:  3.955801010131836 0.06573694944381714
CurrentTrain: epoch  7, batch    50 | loss: 4.0215378Losses:  3.9603753089904785 0.08486415445804596
CurrentTrain: epoch  7, batch    51 | loss: 4.0452394Losses:  3.9602532386779785 0.09818844497203827
CurrentTrain: epoch  7, batch    52 | loss: 4.0584416Losses:  3.981353759765625 0.08193853497505188
CurrentTrain: epoch  7, batch    53 | loss: 4.0632925Losses:  3.9544930458068848 0.09010660648345947
CurrentTrain: epoch  7, batch    54 | loss: 4.0445995Losses:  3.951571464538574 0.08372770249843597
CurrentTrain: epoch  7, batch    55 | loss: 4.0352993Losses:  3.9358861446380615 0.10057508200407028
CurrentTrain: epoch  7, batch    56 | loss: 4.0364614Losses:  3.9337587356567383 0.07934081554412842
CurrentTrain: epoch  7, batch    57 | loss: 4.0130997Losses:  3.9391818046569824 0.0666438490152359
CurrentTrain: epoch  7, batch    58 | loss: 4.0058255Losses:  3.990880012512207 0.05468820780515671
CurrentTrain: epoch  7, batch    59 | loss: 4.0455680Losses:  3.9404163360595703 0.07138250023126602
CurrentTrain: epoch  7, batch    60 | loss: 4.0117989Losses:  3.9798388481140137 0.07518351078033447
CurrentTrain: epoch  7, batch    61 | loss: 4.0550222Losses:  3.9216301441192627 0.06286386400461197
CurrentTrain: epoch  7, batch    62 | loss: 3.9844940Losses:  3.935086250305176 0.101466603577137
CurrentTrain: epoch  8, batch     0 | loss: 4.0365529Losses:  3.9420547485351562 0.09190745651721954
CurrentTrain: epoch  8, batch     1 | loss: 4.0339622Losses:  3.9543871879577637 0.07322470843791962
CurrentTrain: epoch  8, batch     2 | loss: 4.0276117Losses:  3.990359306335449 0.04789038747549057
CurrentTrain: epoch  8, batch     3 | loss: 4.0382495Losses:  3.965989828109741 0.08717337995767593
CurrentTrain: epoch  8, batch     4 | loss: 4.0531631Losses:  3.989487409591675 0.079189732670784
CurrentTrain: epoch  8, batch     5 | loss: 4.0686769Losses:  3.967942237854004 0.09071497619152069
CurrentTrain: epoch  8, batch     6 | loss: 4.0586572Losses:  3.9731101989746094 0.08880369365215302
CurrentTrain: epoch  8, batch     7 | loss: 4.0619140Losses:  3.922872543334961 0.05463232100009918
CurrentTrain: epoch  8, batch     8 | loss: 3.9775050Losses:  3.948320150375366 0.090338334441185
CurrentTrain: epoch  8, batch     9 | loss: 4.0386586Losses:  3.9384677410125732 0.05921824648976326
CurrentTrain: epoch  8, batch    10 | loss: 3.9976859Losses:  3.9730401039123535 0.09966780245304108
CurrentTrain: epoch  8, batch    11 | loss: 4.0727081Losses:  3.9294309616088867 0.07095103710889816
CurrentTrain: epoch  8, batch    12 | loss: 4.0003819Losses:  3.9768457412719727 0.07674934715032578
CurrentTrain: epoch  8, batch    13 | loss: 4.0535951Losses:  3.9686241149902344 0.05730393901467323
CurrentTrain: epoch  8, batch    14 | loss: 4.0259280Losses:  3.9160871505737305 0.06999705731868744
CurrentTrain: epoch  8, batch    15 | loss: 3.9860842Losses:  3.9560861587524414 0.07655157893896103
CurrentTrain: epoch  8, batch    16 | loss: 4.0326376Losses:  3.9106979370117188 0.051519688218832016
CurrentTrain: epoch  8, batch    17 | loss: 3.9622176Losses:  3.9600510597229004 0.08573184162378311
CurrentTrain: epoch  8, batch    18 | loss: 4.0457830Losses:  3.9627158641815186 0.0864001214504242
CurrentTrain: epoch  8, batch    19 | loss: 4.0491161Losses:  3.957960605621338 0.08802279829978943
CurrentTrain: epoch  8, batch    20 | loss: 4.0459833Losses:  3.930903911590576 0.05565570667386055
CurrentTrain: epoch  8, batch    21 | loss: 3.9865596Losses:  3.9368553161621094 0.07028605043888092
CurrentTrain: epoch  8, batch    22 | loss: 4.0071416Losses:  4.009932994842529 0.09036477655172348
CurrentTrain: epoch  8, batch    23 | loss: 4.1002979Losses:  3.9595513343811035 0.10501717031002045
CurrentTrain: epoch  8, batch    24 | loss: 4.0645685Losses:  3.950538396835327 0.10065805912017822
CurrentTrain: epoch  8, batch    25 | loss: 4.0511966Losses:  3.955043315887451 0.08613963425159454
CurrentTrain: epoch  8, batch    26 | loss: 4.0411830Losses:  3.957481861114502 0.0720968097448349
CurrentTrain: epoch  8, batch    27 | loss: 4.0295787Losses:  3.969428777694702 0.0945412889122963
CurrentTrain: epoch  8, batch    28 | loss: 4.0639701Losses:  3.930971622467041 0.0878678485751152
CurrentTrain: epoch  8, batch    29 | loss: 4.0188394Losses:  3.974271535873413 0.0787496417760849
CurrentTrain: epoch  8, batch    30 | loss: 4.0530210Losses:  3.940260887145996 0.07793384790420532
CurrentTrain: epoch  8, batch    31 | loss: 4.0181947Losses:  3.9652867317199707 0.06938763707876205
CurrentTrain: epoch  8, batch    32 | loss: 4.0346742Losses:  3.8576903343200684 0.08434925973415375
CurrentTrain: epoch  8, batch    33 | loss: 3.9420395Losses:  3.9555697441101074 0.0755453109741211
CurrentTrain: epoch  8, batch    34 | loss: 4.0311151Losses:  3.9205923080444336 0.08498378843069077
CurrentTrain: epoch  8, batch    35 | loss: 4.0055761Losses:  3.9277524948120117 0.0950247049331665
CurrentTrain: epoch  8, batch    36 | loss: 4.0227771Losses:  3.915767192840576 0.08326147496700287
CurrentTrain: epoch  8, batch    37 | loss: 3.9990287Losses:  3.953618049621582 0.09406581521034241
CurrentTrain: epoch  8, batch    38 | loss: 4.0476837Losses:  3.944340229034424 0.07357525080442429
CurrentTrain: epoch  8, batch    39 | loss: 4.0179152Losses:  3.9711203575134277 0.058989740908145905
CurrentTrain: epoch  8, batch    40 | loss: 4.0301099Losses:  3.915811538696289 0.06661813706159592
CurrentTrain: epoch  8, batch    41 | loss: 3.9824297Losses:  3.946225643157959 0.05810607224702835
CurrentTrain: epoch  8, batch    42 | loss: 4.0043316Losses:  3.956756353378296 0.06755243241786957
CurrentTrain: epoch  8, batch    43 | loss: 4.0243087Losses:  3.960742235183716 0.08152365684509277
CurrentTrain: epoch  8, batch    44 | loss: 4.0422659Losses:  3.9574661254882812 0.10270729660987854
CurrentTrain: epoch  8, batch    45 | loss: 4.0601735Losses:  3.951746940612793 0.09653311222791672
CurrentTrain: epoch  8, batch    46 | loss: 4.0482802Losses:  3.9732487201690674 0.07505951821804047
CurrentTrain: epoch  8, batch    47 | loss: 4.0483084Losses:  3.9655070304870605 0.057702820748090744
CurrentTrain: epoch  8, batch    48 | loss: 4.0232100Losses:  3.8927621841430664 0.05988166481256485
CurrentTrain: epoch  8, batch    49 | loss: 3.9526439Losses:  3.9443106651306152 0.07387960702180862
CurrentTrain: epoch  8, batch    50 | loss: 4.0181904Losses:  3.9351649284362793 0.06528912484645844
CurrentTrain: epoch  8, batch    51 | loss: 4.0004539Losses:  3.9631569385528564 0.0811213031411171
CurrentTrain: epoch  8, batch    52 | loss: 4.0442781Losses:  3.935487985610962 0.08022750914096832
CurrentTrain: epoch  8, batch    53 | loss: 4.0157156Losses:  3.96268892288208 0.06598129868507385
CurrentTrain: epoch  8, batch    54 | loss: 4.0286703Losses:  3.95930552482605 0.07467062026262283
CurrentTrain: epoch  8, batch    55 | loss: 4.0339761Losses:  4.003663539886475 0.08640018105506897
CurrentTrain: epoch  8, batch    56 | loss: 4.0900636Losses:  3.970641851425171 0.07811195403337479
CurrentTrain: epoch  8, batch    57 | loss: 4.0487537Losses:  3.9460787773132324 0.10002045333385468
CurrentTrain: epoch  8, batch    58 | loss: 4.0460992Losses:  3.9225940704345703 0.10015197098255157
CurrentTrain: epoch  8, batch    59 | loss: 4.0227461Losses:  3.9575159549713135 0.07785491645336151
CurrentTrain: epoch  8, batch    60 | loss: 4.0353708Losses:  3.965858221054077 0.0900358036160469
CurrentTrain: epoch  8, batch    61 | loss: 4.0558939Losses:  3.9464077949523926 0.0497589036822319
CurrentTrain: epoch  8, batch    62 | loss: 3.9961667Losses:  3.94185733795166 0.08111179620027542
CurrentTrain: epoch  9, batch     0 | loss: 4.0229692Losses:  3.918583393096924 0.08920527994632721
CurrentTrain: epoch  9, batch     1 | loss: 4.0077887Losses:  3.946044445037842 0.06485926359891891
CurrentTrain: epoch  9, batch     2 | loss: 4.0109038Losses:  3.948834180831909 0.0918329730629921
CurrentTrain: epoch  9, batch     3 | loss: 4.0406671Losses:  3.955608367919922 0.07682780176401138
CurrentTrain: epoch  9, batch     4 | loss: 4.0324364Losses:  3.9367990493774414 0.08025485277175903
CurrentTrain: epoch  9, batch     5 | loss: 4.0170541Losses:  3.940093755722046 0.09780260920524597
CurrentTrain: epoch  9, batch     6 | loss: 4.0378962Losses:  3.964791774749756 0.055160969495773315
CurrentTrain: epoch  9, batch     7 | loss: 4.0199528Losses:  3.851652145385742 0.06403128802776337
CurrentTrain: epoch  9, batch     8 | loss: 3.9156835Losses:  3.9418721199035645 0.05259225517511368
CurrentTrain: epoch  9, batch     9 | loss: 3.9944644Losses:  3.9730303287506104 0.06928608566522598
CurrentTrain: epoch  9, batch    10 | loss: 4.0423164Losses:  3.9502737522125244 0.07603023946285248
CurrentTrain: epoch  9, batch    11 | loss: 4.0263038Losses:  3.9591996669769287 0.06541313976049423
CurrentTrain: epoch  9, batch    12 | loss: 4.0246129Losses:  3.9527101516723633 0.08250604569911957
CurrentTrain: epoch  9, batch    13 | loss: 4.0352163Losses:  3.9457435607910156 0.09319256246089935
CurrentTrain: epoch  9, batch    14 | loss: 4.0389361Losses:  3.97523832321167 0.060142241418361664
CurrentTrain: epoch  9, batch    15 | loss: 4.0353804Losses:  3.928589105606079 0.07570423185825348
CurrentTrain: epoch  9, batch    16 | loss: 4.0042934Losses:  3.924614429473877 0.08184832334518433
CurrentTrain: epoch  9, batch    17 | loss: 4.0064626Losses:  3.945730447769165 0.07993724197149277
CurrentTrain: epoch  9, batch    18 | loss: 4.0256677Losses:  3.9921388626098633 0.06795517355203629
CurrentTrain: epoch  9, batch    19 | loss: 4.0600939Losses:  3.928091049194336 0.0622544027864933
CurrentTrain: epoch  9, batch    20 | loss: 3.9903455Losses:  3.9391775131225586 0.07274555414915085
CurrentTrain: epoch  9, batch    21 | loss: 4.0119228Losses:  3.9795308113098145 0.05525785684585571
CurrentTrain: epoch  9, batch    22 | loss: 4.0347886Losses:  3.9513213634490967 0.08305811882019043
CurrentTrain: epoch  9, batch    23 | loss: 4.0343795Losses:  3.910167694091797 0.0840127021074295
CurrentTrain: epoch  9, batch    24 | loss: 3.9941804Losses:  3.929715633392334 0.0901603102684021
CurrentTrain: epoch  9, batch    25 | loss: 4.0198760Losses:  3.8985657691955566 0.07987809926271439
CurrentTrain: epoch  9, batch    26 | loss: 3.9784439Losses:  3.957815408706665 0.08526304364204407
CurrentTrain: epoch  9, batch    27 | loss: 4.0430784Losses:  3.928858757019043 0.0770845040678978
CurrentTrain: epoch  9, batch    28 | loss: 4.0059433Losses:  3.946848154067993 0.08308899402618408
CurrentTrain: epoch  9, batch    29 | loss: 4.0299373Losses:  3.9695565700531006 0.07343658059835434
CurrentTrain: epoch  9, batch    30 | loss: 4.0429931Losses:  3.9413514137268066 0.10104302316904068
CurrentTrain: epoch  9, batch    31 | loss: 4.0423946Losses:  3.929633378982544 0.07241221517324448
CurrentTrain: epoch  9, batch    32 | loss: 4.0020456Losses:  3.952626943588257 0.04789465665817261
CurrentTrain: epoch  9, batch    33 | loss: 4.0005217Losses:  3.9141063690185547 0.08044757694005966
CurrentTrain: epoch  9, batch    34 | loss: 3.9945540Losses:  3.933298110961914 0.05178675800561905
CurrentTrain: epoch  9, batch    35 | loss: 3.9850848Losses:  3.9547247886657715 0.06357060372829437
CurrentTrain: epoch  9, batch    36 | loss: 4.0182953Losses:  3.927518606185913 0.06660109758377075
CurrentTrain: epoch  9, batch    37 | loss: 3.9941196Losses:  3.984342336654663 0.07767151296138763
CurrentTrain: epoch  9, batch    38 | loss: 4.0620136Losses:  3.9400508403778076 0.07894164323806763
CurrentTrain: epoch  9, batch    39 | loss: 4.0189924Losses:  3.9700608253479004 0.06348887085914612
CurrentTrain: epoch  9, batch    40 | loss: 4.0335498Losses:  3.937919855117798 0.055538855493068695
CurrentTrain: epoch  9, batch    41 | loss: 3.9934587Losses:  3.9583740234375 0.0890560895204544
CurrentTrain: epoch  9, batch    42 | loss: 4.0474300Losses:  3.9037973880767822 0.05929172784090042
CurrentTrain: epoch  9, batch    43 | loss: 3.9630892Losses:  3.906432628631592 0.08186540007591248
CurrentTrain: epoch  9, batch    44 | loss: 3.9882979Losses:  3.9637112617492676 0.047125738114118576
CurrentTrain: epoch  9, batch    45 | loss: 4.0108371Losses:  3.9566471576690674 0.058564089238643646
CurrentTrain: epoch  9, batch    46 | loss: 4.0152111Losses:  3.9562883377075195 0.07785046100616455
CurrentTrain: epoch  9, batch    47 | loss: 4.0341387Losses:  3.9711947441101074 0.09317494928836823
CurrentTrain: epoch  9, batch    48 | loss: 4.0643697Losses:  3.9097068309783936 0.0790177434682846
CurrentTrain: epoch  9, batch    49 | loss: 3.9887245Losses:  3.9582598209381104 0.06198485568165779
CurrentTrain: epoch  9, batch    50 | loss: 4.0202446Losses:  3.938567638397217 0.07854322344064713
CurrentTrain: epoch  9, batch    51 | loss: 4.0171108Losses:  3.956912040710449 0.07723157107830048
CurrentTrain: epoch  9, batch    52 | loss: 4.0341434Losses:  3.908310651779175 0.048815757036209106
CurrentTrain: epoch  9, batch    53 | loss: 3.9571264Losses:  3.9524524211883545 0.07019408047199249
CurrentTrain: epoch  9, batch    54 | loss: 4.0226464Losses:  3.982273578643799 0.05618298798799515
CurrentTrain: epoch  9, batch    55 | loss: 4.0384564Losses:  3.928102970123291 0.05404728651046753
CurrentTrain: epoch  9, batch    56 | loss: 3.9821503Losses:  3.955005645751953 0.06270802021026611
CurrentTrain: epoch  9, batch    57 | loss: 4.0177135Losses:  3.9311752319335938 0.060213737189769745
CurrentTrain: epoch  9, batch    58 | loss: 3.9913890Losses:  3.9194998741149902 0.07697096467018127
CurrentTrain: epoch  9, batch    59 | loss: 3.9964709Losses:  3.934358835220337 0.07796499133110046
CurrentTrain: epoch  9, batch    60 | loss: 4.0123239Losses:  3.950913906097412 0.07233147323131561
CurrentTrain: epoch  9, batch    61 | loss: 4.0232453Losses:  3.935415029525757 0.03775068372488022
CurrentTrain: epoch  9, batch    62 | loss: 3.9731658
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 92.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.29%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.32%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.09%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.20%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.44%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 95.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.29%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.37%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [2 7 8 1 2 2 2 2 0 5 1 6 2 2 2 0 4 2 2 3]
Losses:  7.968810558319092 1.5676720142364502
CurrentTrain: epoch  0, batch     0 | loss: 9.5364828Losses:  9.483846664428711 1.55401611328125
CurrentTrain: epoch  0, batch     1 | loss: 11.0378628Losses:  7.613946437835693 1.879612922668457
CurrentTrain: epoch  0, batch     2 | loss: 9.4935589Losses:  4.375144958496094 0.41181695461273193
CurrentTrain: epoch  0, batch     3 | loss: 4.7869620Losses:  3.9470763206481934 1.6427541971206665
CurrentTrain: epoch  1, batch     0 | loss: 5.5898304Losses:  3.933547019958496 1.5469247102737427
CurrentTrain: epoch  1, batch     1 | loss: 5.4804716Losses:  3.2608397006988525 1.0918171405792236
CurrentTrain: epoch  1, batch     2 | loss: 4.3526568Losses:  2.909289836883545 0.20436416566371918
CurrentTrain: epoch  1, batch     3 | loss: 3.1136539Losses:  3.404439687728882 1.1663918495178223
CurrentTrain: epoch  2, batch     0 | loss: 4.5708313Losses:  3.0243582725524902 1.436539888381958
CurrentTrain: epoch  2, batch     1 | loss: 4.4608984Losses:  2.6188244819641113 1.3339024782180786
CurrentTrain: epoch  2, batch     2 | loss: 3.9527268Losses:  3.7131457328796387 0.5513134598731995
CurrentTrain: epoch  2, batch     3 | loss: 4.2644591Losses:  2.772202253341675 1.099091649055481
CurrentTrain: epoch  3, batch     0 | loss: 3.8712940Losses:  2.828152894973755 1.3838601112365723
CurrentTrain: epoch  3, batch     1 | loss: 4.2120132Losses:  2.2474288940429688 1.0396478176116943
CurrentTrain: epoch  3, batch     2 | loss: 3.2870767Losses:  3.15725040435791 0.32358318567276
CurrentTrain: epoch  3, batch     3 | loss: 3.4808335Losses:  2.79240083694458 1.3820216655731201
CurrentTrain: epoch  4, batch     0 | loss: 4.1744223Losses:  2.474912166595459 1.0969738960266113
CurrentTrain: epoch  4, batch     1 | loss: 3.5718861Losses:  2.3455729484558105 0.9400322437286377
CurrentTrain: epoch  4, batch     2 | loss: 3.2856052Losses:  3.2307515144348145 0.3400806188583374
CurrentTrain: epoch  4, batch     3 | loss: 3.5708323Losses:  2.5200698375701904 0.825813889503479
CurrentTrain: epoch  5, batch     0 | loss: 3.3458838Losses:  2.214414119720459 1.0913585424423218
CurrentTrain: epoch  5, batch     1 | loss: 3.3057728Losses:  2.4999172687530518 1.0265206098556519
CurrentTrain: epoch  5, batch     2 | loss: 3.5264378Losses:  2.382706642150879 0.15060728788375854
CurrentTrain: epoch  5, batch     3 | loss: 2.5333140Losses:  2.3611674308776855 1.1788126230239868
CurrentTrain: epoch  6, batch     0 | loss: 3.5399799Losses:  2.0810158252716064 0.9185288548469543
CurrentTrain: epoch  6, batch     1 | loss: 2.9995446Losses:  2.2521960735321045 0.7652451992034912
CurrentTrain: epoch  6, batch     2 | loss: 3.0174413Losses:  2.8585205078125 0.19596457481384277
CurrentTrain: epoch  6, batch     3 | loss: 3.0544851Losses:  2.3035974502563477 1.047788143157959
CurrentTrain: epoch  7, batch     0 | loss: 3.3513856Losses:  2.0432920455932617 0.8628549575805664
CurrentTrain: epoch  7, batch     1 | loss: 2.9061470Losses:  2.300264596939087 0.9927152395248413
CurrentTrain: epoch  7, batch     2 | loss: 3.2929797Losses:  1.7554116249084473 0.055441468954086304
CurrentTrain: epoch  7, batch     3 | loss: 1.8108531Losses:  2.2250142097473145 0.7988349199295044
CurrentTrain: epoch  8, batch     0 | loss: 3.0238490Losses:  1.982222318649292 0.6239275932312012
CurrentTrain: epoch  8, batch     1 | loss: 2.6061499Losses:  2.0036566257476807 0.6889528036117554
CurrentTrain: epoch  8, batch     2 | loss: 2.6926093Losses:  2.6861612796783447 0.4583936929702759
CurrentTrain: epoch  8, batch     3 | loss: 3.1445551Losses:  2.120532989501953 0.6550023555755615
CurrentTrain: epoch  9, batch     0 | loss: 2.7755353Losses:  1.9322830438613892 0.7530945539474487
CurrentTrain: epoch  9, batch     1 | loss: 2.6853776Losses:  1.9382104873657227 0.47823458909988403
CurrentTrain: epoch  9, batch     2 | loss: 2.4164450Losses:  1.6591169834136963 0.16763827204704285
CurrentTrain: epoch  9, batch     3 | loss: 1.8267553
Losses:  5.476564407348633 0.8240683078765869
MemoryTrain:  epoch  0, batch     0 | loss: 6.3006325Losses:  11.362285614013672 0.05666661262512207
MemoryTrain:  epoch  0, batch     1 | loss: 11.4189520Losses:  1.324081540107727 0.8342952728271484
MemoryTrain:  epoch  1, batch     0 | loss: 2.1583767Losses:  0.3961677849292755 0.04749765247106552
MemoryTrain:  epoch  1, batch     1 | loss: 0.4436654Losses:  0.9690847396850586 0.7447119951248169
MemoryTrain:  epoch  2, batch     0 | loss: 1.7137967Losses:  1.1950244903564453 0.24290519952774048
MemoryTrain:  epoch  2, batch     1 | loss: 1.4379296Losses:  0.8943496942520142 0.6798090934753418
MemoryTrain:  epoch  3, batch     0 | loss: 1.5741588Losses:  0.894024133682251 0.15708526968955994
MemoryTrain:  epoch  3, batch     1 | loss: 1.0511094Losses:  0.8120152950286865 0.8403518199920654
MemoryTrain:  epoch  4, batch     0 | loss: 1.6523671Losses:  0.09782315790653229 0.027647629380226135
MemoryTrain:  epoch  4, batch     1 | loss: 0.1254708Losses:  0.40183204412460327 0.6114787459373474
MemoryTrain:  epoch  5, batch     0 | loss: 1.0133108Losses:  0.6965675354003906 0.5312186479568481
MemoryTrain:  epoch  5, batch     1 | loss: 1.2277862Losses:  0.408345103263855 0.7373705506324768
MemoryTrain:  epoch  6, batch     0 | loss: 1.1457157Losses:  0.14354896545410156 0.2774149179458618
MemoryTrain:  epoch  6, batch     1 | loss: 0.4209639Losses:  0.3617348372936249 0.8464369773864746
MemoryTrain:  epoch  7, batch     0 | loss: 1.2081718Losses:  0.15565574169158936 0.11011231690645218
MemoryTrain:  epoch  7, batch     1 | loss: 0.2657681Losses:  0.17903906106948853 0.6234952211380005
MemoryTrain:  epoch  8, batch     0 | loss: 0.8025343Losses:  0.32309690117836 0.2350606769323349
MemoryTrain:  epoch  8, batch     1 | loss: 0.5581576Losses:  0.19956150650978088 0.7463040947914124
MemoryTrain:  epoch  9, batch     0 | loss: 0.9458656Losses:  0.18224269151687622 0.16617166996002197
MemoryTrain:  epoch  9, batch     1 | loss: 0.3484144
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 84.54%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 6.25%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.92%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 82.65%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 81.94%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.65%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.26%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.29%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.94%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.96%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.87%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 92.69%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 92.44%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 92.57%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 92.59%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 92.69%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 92.81%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.82%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.83%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 92.52%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.45%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 92.39%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 91.77%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 91.64%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 91.36%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 90.47%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 89.53%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 88.69%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 87.79%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 87.06%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 86.21%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 85.87%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 86.77%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 86.71%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 86.66%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 86.54%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 86.44%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 86.45%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.65%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 86.97%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 87.09%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 87.21%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 87.44%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 87.56%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 87.72%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 87.77%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 87.97%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 87.96%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 87.96%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 87.80%   
cur_acc:  ['0.9484', '0.8194']
his_acc:  ['0.9484', '0.8780']
Clustering into  14  clusters
Clusters:  [ 0  7  9  0  0  0 11  0 10 12  8  3  0  0  5 13  4  0  0  6  2  0  0  0
  0  1  0  0  0  0]
Losses:  7.579229354858398 1.7162764072418213
CurrentTrain: epoch  0, batch     0 | loss: 9.2955055Losses:  9.53520393371582 1.7346954345703125
CurrentTrain: epoch  0, batch     1 | loss: 11.2698994Losses:  8.277836799621582 1.9469327926635742
CurrentTrain: epoch  0, batch     2 | loss: 10.2247696Losses:  4.473169326782227 0.4581979215145111
CurrentTrain: epoch  0, batch     3 | loss: 4.9313674Losses:  4.328145503997803 1.8625601530075073
CurrentTrain: epoch  1, batch     0 | loss: 6.1907058Losses:  4.039812088012695 1.798208475112915
CurrentTrain: epoch  1, batch     1 | loss: 5.8380203Losses:  3.846360683441162 1.851836919784546
CurrentTrain: epoch  1, batch     2 | loss: 5.6981974Losses:  4.1251630783081055 0.29281920194625854
CurrentTrain: epoch  1, batch     3 | loss: 4.4179821Losses:  3.797729015350342 1.5438768863677979
CurrentTrain: epoch  2, batch     0 | loss: 5.3416061Losses:  3.861006259918213 1.8161447048187256
CurrentTrain: epoch  2, batch     1 | loss: 5.6771507Losses:  3.2925944328308105 1.4989490509033203
CurrentTrain: epoch  2, batch     2 | loss: 4.7915435Losses:  3.803739547729492 0.5507704019546509
CurrentTrain: epoch  2, batch     3 | loss: 4.3545098Losses:  3.3684468269348145 1.3344616889953613
CurrentTrain: epoch  3, batch     0 | loss: 4.7029085Losses:  3.532149314880371 1.5735286474227905
CurrentTrain: epoch  3, batch     1 | loss: 5.1056781Losses:  3.077148914337158 1.575729250907898
CurrentTrain: epoch  3, batch     2 | loss: 4.6528783Losses:  3.995190382003784 0.5274039506912231
CurrentTrain: epoch  3, batch     3 | loss: 4.5225945Losses:  3.619704246520996 1.5505342483520508
CurrentTrain: epoch  4, batch     0 | loss: 5.1702385Losses:  2.979703664779663 1.4990214109420776
CurrentTrain: epoch  4, batch     1 | loss: 4.4787250Losses:  2.887608051300049 1.573540449142456
CurrentTrain: epoch  4, batch     2 | loss: 4.4611483Losses:  2.602588653564453 0.4121914505958557
CurrentTrain: epoch  4, batch     3 | loss: 3.0147800Losses:  3.1655173301696777 1.4858064651489258
CurrentTrain: epoch  5, batch     0 | loss: 4.6513238Losses:  2.8232688903808594 1.2824153900146484
CurrentTrain: epoch  5, batch     1 | loss: 4.1056843Losses:  2.949826717376709 1.3828908205032349
CurrentTrain: epoch  5, batch     2 | loss: 4.3327174Losses:  2.1742987632751465 0.22981463372707367
CurrentTrain: epoch  5, batch     3 | loss: 2.4041133Losses:  2.8276937007904053 1.5040669441223145
CurrentTrain: epoch  6, batch     0 | loss: 4.3317604Losses:  2.6390416622161865 1.2787145376205444
CurrentTrain: epoch  6, batch     1 | loss: 3.9177561Losses:  2.895996570587158 1.233486533164978
CurrentTrain: epoch  6, batch     2 | loss: 4.1294832Losses:  2.4357080459594727 0.3601717948913574
CurrentTrain: epoch  6, batch     3 | loss: 2.7958798Losses:  2.6194093227386475 1.3386859893798828
CurrentTrain: epoch  7, batch     0 | loss: 3.9580953Losses:  2.6398050785064697 1.1450169086456299
CurrentTrain: epoch  7, batch     1 | loss: 3.7848220Losses:  2.736584186553955 1.3350894451141357
CurrentTrain: epoch  7, batch     2 | loss: 4.0716734Losses:  3.7024550437927246 0.2967037558555603
CurrentTrain: epoch  7, batch     3 | loss: 3.9991589Losses:  2.641007423400879 1.3327562808990479
CurrentTrain: epoch  8, batch     0 | loss: 3.9737637Losses:  2.762387752532959 0.9803779125213623
CurrentTrain: epoch  8, batch     1 | loss: 3.7427657Losses:  2.5729470252990723 1.1929521560668945
CurrentTrain: epoch  8, batch     2 | loss: 3.7658992Losses:  1.8686838150024414 0.3325478434562683
CurrentTrain: epoch  8, batch     3 | loss: 2.2012317Losses:  2.3590281009674072 1.1345182657241821
CurrentTrain: epoch  9, batch     0 | loss: 3.4935465Losses:  2.655958652496338 1.3244643211364746
CurrentTrain: epoch  9, batch     1 | loss: 3.9804230Losses:  2.514941692352295 1.162247896194458
CurrentTrain: epoch  9, batch     2 | loss: 3.6771896Losses:  1.9872045516967773 0.13128706812858582
CurrentTrain: epoch  9, batch     3 | loss: 2.1184916
Losses:  5.757157802581787 1.2707037925720215
MemoryTrain:  epoch  0, batch     0 | loss: 7.0278616Losses:  8.908555030822754 0.7528483867645264
MemoryTrain:  epoch  0, batch     1 | loss: 9.6614037Losses:  1.0023715496063232 1.0055413246154785
MemoryTrain:  epoch  1, batch     0 | loss: 2.0079129Losses:  0.4430731534957886 0.8202688097953796
MemoryTrain:  epoch  1, batch     1 | loss: 1.2633419Losses:  0.6746997833251953 0.8815131187438965
MemoryTrain:  epoch  2, batch     0 | loss: 1.5562129Losses:  0.42938363552093506 0.9041548371315002
MemoryTrain:  epoch  2, batch     1 | loss: 1.3335385Losses:  0.5579005479812622 0.9523419141769409
MemoryTrain:  epoch  3, batch     0 | loss: 1.5102425Losses:  0.3554178774356842 0.6941481828689575
MemoryTrain:  epoch  3, batch     1 | loss: 1.0495660Losses:  0.3029480278491974 0.8388831615447998
MemoryTrain:  epoch  4, batch     0 | loss: 1.1418312Losses:  0.38922902941703796 0.9193516373634338
MemoryTrain:  epoch  4, batch     1 | loss: 1.3085806Losses:  0.2832779288291931 0.995287299156189
MemoryTrain:  epoch  5, batch     0 | loss: 1.2785652Losses:  0.2971979081630707 0.6435750126838684
MemoryTrain:  epoch  5, batch     1 | loss: 0.9407729Losses:  0.2695654332637787 0.8570675849914551
MemoryTrain:  epoch  6, batch     0 | loss: 1.1266330Losses:  0.26386895775794983 0.6949283480644226
MemoryTrain:  epoch  6, batch     1 | loss: 0.9587973Losses:  0.18487811088562012 0.6708885431289673
MemoryTrain:  epoch  7, batch     0 | loss: 0.8557667Losses:  0.29015275835990906 0.9454627633094788
MemoryTrain:  epoch  7, batch     1 | loss: 1.2356155Losses:  0.24377776682376862 0.7675991654396057
MemoryTrain:  epoch  8, batch     0 | loss: 1.0113770Losses:  0.19114430248737335 0.7906938195228577
MemoryTrain:  epoch  8, batch     1 | loss: 0.9818381Losses:  0.3154831528663635 1.0097365379333496
MemoryTrain:  epoch  9, batch     0 | loss: 1.3252196Losses:  0.18311883509159088 0.5278962254524231
MemoryTrain:  epoch  9, batch     1 | loss: 0.7110150
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 19.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 56.03%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 54.37%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.62%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 54.36%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 55.15%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.43%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 57.12%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 57.94%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 58.72%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 61.13%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 63.21%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 63.52%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 64.80%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 64.12%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 63.67%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 63.23%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 62.70%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.10%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.89%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.54%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.94%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.07%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.25%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.60%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.50%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.29%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 91.36%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.97%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 90.79%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 90.67%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 90.21%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 89.94%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 89.58%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 88.69%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 88.36%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 87.96%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 87.20%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 86.30%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 85.27%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 84.34%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 83.43%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 82.47%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 82.24%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.83%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.02%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 83.36%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.46%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 83.44%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 83.35%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 83.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.58%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.74%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 84.43%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 84.85%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 84.99%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 85.03%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 85.05%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 85.04%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 85.01%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 85.08%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 85.12%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 85.33%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 85.30%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 84.72%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 84.25%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 83.74%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 83.24%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 82.84%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 82.30%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 82.28%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 82.63%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 82.62%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 82.65%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 82.77%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 82.91%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 82.63%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 82.36%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 82.36%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 82.26%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 81.96%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 81.37%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 80.84%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 80.31%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 79.79%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 79.31%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 78.81%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 78.74%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 79.00%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 79.10%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 79.33%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 79.49%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 79.42%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 79.32%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 79.23%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 79.12%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 79.13%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 79.14%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 79.16%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 79.10%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 79.07%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 78.88%   [EVAL] batch:  182 | acc: 25.00%,  total acc: 78.59%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 78.36%   [EVAL] batch:  184 | acc: 37.50%,  total acc: 78.14%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 77.89%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 77.53%   
cur_acc:  ['0.9484', '0.8194', '0.6210']
his_acc:  ['0.9484', '0.8780', '0.7753']
Clustering into  19  clusters
Clusters:  [ 0  3 12  1  0  0 16  0 13 11 10 17  1  0 14  6 15  0  0 18  7  1  0  0
  0  9  0  0  0  0  3  8  1  1  0  5  0  0  4  2]
Losses:  6.762264251708984 1.015911340713501
CurrentTrain: epoch  0, batch     0 | loss: 7.7781754Losses:  9.276878356933594 1.4788262844085693
CurrentTrain: epoch  0, batch     1 | loss: 10.7557049Losses:  7.6495208740234375 1.4842844009399414
CurrentTrain: epoch  0, batch     2 | loss: 9.1338053Losses:  4.999550819396973 0.4016619920730591
CurrentTrain: epoch  0, batch     3 | loss: 5.4012127Losses:  3.42057466506958 1.5241594314575195
CurrentTrain: epoch  1, batch     0 | loss: 4.9447341Losses:  4.250807762145996 1.3228681087493896
CurrentTrain: epoch  1, batch     1 | loss: 5.5736761Losses:  3.2928733825683594 1.150357961654663
CurrentTrain: epoch  1, batch     2 | loss: 4.4432316Losses:  2.1008167266845703 0.07812347263097763
CurrentTrain: epoch  1, batch     3 | loss: 2.1789403Losses:  2.5893607139587402 1.014782190322876
CurrentTrain: epoch  2, batch     0 | loss: 3.6041429Losses:  3.446671724319458 0.9427436590194702
CurrentTrain: epoch  2, batch     1 | loss: 4.3894153Losses:  3.515389919281006 1.1827971935272217
CurrentTrain: epoch  2, batch     2 | loss: 4.6981869Losses:  2.632145643234253 0.21785035729408264
CurrentTrain: epoch  2, batch     3 | loss: 2.8499961Losses:  2.9467716217041016 1.110152006149292
CurrentTrain: epoch  3, batch     0 | loss: 4.0569239Losses:  2.70947265625 0.9751046299934387
CurrentTrain: epoch  3, batch     1 | loss: 3.6845772Losses:  3.1157357692718506 1.0297143459320068
CurrentTrain: epoch  3, batch     2 | loss: 4.1454501Losses:  4.20828914642334 0.3856526017189026
CurrentTrain: epoch  3, batch     3 | loss: 4.5939417Losses:  2.5957465171813965 0.8336807489395142
CurrentTrain: epoch  4, batch     0 | loss: 3.4294271Losses:  3.1139745712280273 1.1355777978897095
CurrentTrain: epoch  4, batch     1 | loss: 4.2495522Losses:  2.6273252964019775 1.0625507831573486
CurrentTrain: epoch  4, batch     2 | loss: 3.6898761Losses:  2.5394020080566406 0.12233035266399384
CurrentTrain: epoch  4, batch     3 | loss: 2.6617324Losses:  2.317159652709961 0.8826231956481934
CurrentTrain: epoch  5, batch     0 | loss: 3.1997828Losses:  2.5272226333618164 0.8415608406066895
CurrentTrain: epoch  5, batch     1 | loss: 3.3687835Losses:  2.9808316230773926 0.8609591722488403
CurrentTrain: epoch  5, batch     2 | loss: 3.8417907Losses:  2.7377800941467285 0.3936908543109894
CurrentTrain: epoch  5, batch     3 | loss: 3.1314709Losses:  2.2611634731292725 0.8478301763534546
CurrentTrain: epoch  6, batch     0 | loss: 3.1089935Losses:  2.548886775970459 0.8555641770362854
CurrentTrain: epoch  6, batch     1 | loss: 3.4044509Losses:  2.521425247192383 0.971934974193573
CurrentTrain: epoch  6, batch     2 | loss: 3.4933603Losses:  2.745659828186035 0.21370062232017517
CurrentTrain: epoch  6, batch     3 | loss: 2.9593604Losses:  2.5051352977752686 0.8613073825836182
CurrentTrain: epoch  7, batch     0 | loss: 3.3664427Losses:  2.342391014099121 0.6380061507225037
CurrentTrain: epoch  7, batch     1 | loss: 2.9803972Losses:  2.163177251815796 0.7956560850143433
CurrentTrain: epoch  7, batch     2 | loss: 2.9588332Losses:  2.4268550872802734 0.21546922624111176
CurrentTrain: epoch  7, batch     3 | loss: 2.6423242Losses:  2.480424404144287 0.7864305973052979
CurrentTrain: epoch  8, batch     0 | loss: 3.2668550Losses:  2.16314697265625 0.5141843557357788
CurrentTrain: epoch  8, batch     1 | loss: 2.6773314Losses:  1.9531465768814087 0.5885781049728394
CurrentTrain: epoch  8, batch     2 | loss: 2.5417247Losses:  3.5446133613586426 5.960465188081798e-08
CurrentTrain: epoch  8, batch     3 | loss: 3.5446134Losses:  2.1350011825561523 0.7148517370223999
CurrentTrain: epoch  9, batch     0 | loss: 2.8498530Losses:  2.1008706092834473 0.7227826118469238
CurrentTrain: epoch  9, batch     1 | loss: 2.8236532Losses:  2.092069149017334 0.7141392230987549
CurrentTrain: epoch  9, batch     2 | loss: 2.8062084Losses:  2.3669872283935547 0.09598519653081894
CurrentTrain: epoch  9, batch     3 | loss: 2.4629724
Losses:  5.715497016906738 0.9822074174880981
MemoryTrain:  epoch  0, batch     0 | loss: 6.6977043Losses:  9.721233367919922 0.6739243268966675
MemoryTrain:  epoch  0, batch     1 | loss: 10.3951578Losses:  10.192131042480469 0.5577013492584229
MemoryTrain:  epoch  0, batch     2 | loss: 10.7498322Losses:  1.2236770391464233 0.9286564588546753
MemoryTrain:  epoch  1, batch     0 | loss: 2.1523335Losses:  0.7867732048034668 0.6750657558441162
MemoryTrain:  epoch  1, batch     1 | loss: 1.4618390Losses:  0.46067264676094055 0.661693274974823
MemoryTrain:  epoch  1, batch     2 | loss: 1.1223660Losses:  0.7143867015838623 0.7329695820808411
MemoryTrain:  epoch  2, batch     0 | loss: 1.4473562Losses:  0.9941651821136475 0.9114317893981934
MemoryTrain:  epoch  2, batch     1 | loss: 1.9055970Losses:  0.31975236535072327 0.5101636648178101
MemoryTrain:  epoch  2, batch     2 | loss: 0.8299160Losses:  0.5531768202781677 0.6793540120124817
MemoryTrain:  epoch  3, batch     0 | loss: 1.2325308Losses:  0.5062304735183716 0.8449398279190063
MemoryTrain:  epoch  3, batch     1 | loss: 1.3511703Losses:  0.81432044506073 0.5611538887023926
MemoryTrain:  epoch  3, batch     2 | loss: 1.3754743Losses:  0.7258602976799011 0.8854215741157532
MemoryTrain:  epoch  4, batch     0 | loss: 1.6112819Losses:  0.39401137828826904 0.671268880367279
MemoryTrain:  epoch  4, batch     1 | loss: 1.0652802Losses:  0.5851273536682129 0.6354802846908569
MemoryTrain:  epoch  4, batch     2 | loss: 1.2206076Losses:  0.4065282344818115 0.7730530500411987
MemoryTrain:  epoch  5, batch     0 | loss: 1.1795813Losses:  0.35421431064605713 0.8383484482765198
MemoryTrain:  epoch  5, batch     1 | loss: 1.1925628Losses:  0.22239166498184204 0.5056472420692444
MemoryTrain:  epoch  5, batch     2 | loss: 0.7280389Losses:  0.44371986389160156 0.8465023040771484
MemoryTrain:  epoch  6, batch     0 | loss: 1.2902222Losses:  0.39334070682525635 0.8563461303710938
MemoryTrain:  epoch  6, batch     1 | loss: 1.2496868Losses:  0.297756552696228 0.36128658056259155
MemoryTrain:  epoch  6, batch     2 | loss: 0.6590431Losses:  0.3895597457885742 0.8954455256462097
MemoryTrain:  epoch  7, batch     0 | loss: 1.2850053Losses:  0.27887147665023804 0.5895622372627258
MemoryTrain:  epoch  7, batch     1 | loss: 0.8684337Losses:  0.28252577781677246 0.49135488271713257
MemoryTrain:  epoch  7, batch     2 | loss: 0.7738807Losses:  0.40817689895629883 0.9369815587997437
MemoryTrain:  epoch  8, batch     0 | loss: 1.3451585Losses:  0.23442810773849487 0.5448927283287048
MemoryTrain:  epoch  8, batch     1 | loss: 0.7793208Losses:  0.3141850531101227 0.46530628204345703
MemoryTrain:  epoch  8, batch     2 | loss: 0.7794913Losses:  0.2510863244533539 0.8024662733078003
MemoryTrain:  epoch  9, batch     0 | loss: 1.0535526Losses:  0.27659741044044495 0.755696713924408
MemoryTrain:  epoch  9, batch     1 | loss: 1.0322942Losses:  0.3612789809703827 0.5180177688598633
MemoryTrain:  epoch  9, batch     2 | loss: 0.8792968
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 77.27%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.37%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 72.44%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 73.10%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 73.27%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 73.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 53.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 61.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.44%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.29%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 83.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 83.46%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.41%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.22%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 82.84%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 82.48%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 82.65%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 83.43%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 83.40%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 83.02%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 83.18%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 83.01%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 83.11%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.17%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 82.73%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 82.31%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 82.05%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 81.25%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 80.94%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 80.48%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 79.80%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 78.99%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 76.53%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 75.65%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 75.43%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 76.84%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 76.95%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 77.00%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 76.91%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 76.83%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 79.18%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 79.27%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 79.70%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 79.76%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.15%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 79.61%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 79.18%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 78.76%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 78.29%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 77.93%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 77.58%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 77.56%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 77.58%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 77.75%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.87%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 78.06%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:  139 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 78.46%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 78.48%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 78.39%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 78.23%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 77.95%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 77.87%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 77.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 77.11%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 76.60%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 76.10%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 75.61%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 75.16%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 74.68%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 74.60%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 74.76%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.04%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 75.12%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 75.19%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 75.74%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 75.44%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 74.67%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 74.31%   [EVAL] batch:  173 | acc: 18.75%,  total acc: 73.99%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 73.64%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:  176 | acc: 93.75%,  total acc: 73.69%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 73.74%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 73.81%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 73.82%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 73.90%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:  182 | acc: 31.25%,  total acc: 73.57%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 73.34%   [EVAL] batch:  184 | acc: 43.75%,  total acc: 73.18%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 72.98%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 72.88%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 72.89%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 73.23%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.11%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 72.90%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 73.32%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.52%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 74.07%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 73.90%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 73.84%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 73.68%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 73.57%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 73.49%   [EVAL] batch:  220 | acc: 37.50%,  total acc: 73.33%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 73.23%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 73.04%   [EVAL] batch:  223 | acc: 37.50%,  total acc: 72.88%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 72.69%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 72.75%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 72.88%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 72.89%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 72.90%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 72.80%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 72.89%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 72.90%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 72.93%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 72.97%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 73.00%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.30%   
cur_acc:  ['0.9484', '0.8194', '0.6210', '0.7817']
his_acc:  ['0.9484', '0.8780', '0.7753', '0.7430']
Clustering into  24  clusters
Clusters:  [ 0  1 17  2  0  0 21  0 18 23 12 11  2  0 19 14 15  0  0 22  8  2  0  0
  0 20  0  0  0  0  1 13  2  0  0  6  0  0 10  9 16  0  4  0  5  7  3  0
  0  0]
Losses:  7.089989185333252 1.482649564743042
CurrentTrain: epoch  0, batch     0 | loss: 8.5726385Losses:  8.912260055541992 1.5148110389709473
CurrentTrain: epoch  0, batch     1 | loss: 10.4270706Losses:  8.292352676391602 1.3604296445846558
CurrentTrain: epoch  0, batch     2 | loss: 9.6527824Losses:  5.895553112030029 0.3153000473976135
CurrentTrain: epoch  0, batch     3 | loss: 6.2108531Losses:  4.542423248291016 1.407702922821045
CurrentTrain: epoch  1, batch     0 | loss: 5.9501262Losses:  3.4546618461608887 1.2162587642669678
CurrentTrain: epoch  1, batch     1 | loss: 4.6709204Losses:  4.013228416442871 1.3449904918670654
CurrentTrain: epoch  1, batch     2 | loss: 5.3582191Losses:  2.786165237426758 0.18115264177322388
CurrentTrain: epoch  1, batch     3 | loss: 2.9673178Losses:  3.6790807247161865 1.3655496835708618
CurrentTrain: epoch  2, batch     0 | loss: 5.0446305Losses:  3.82855486869812 1.3219778537750244
CurrentTrain: epoch  2, batch     1 | loss: 5.1505327Losses:  3.6893205642700195 1.1765046119689941
CurrentTrain: epoch  2, batch     2 | loss: 4.8658252Losses:  2.1759462356567383 0.2315722554922104
CurrentTrain: epoch  2, batch     3 | loss: 2.4075184Losses:  2.9749927520751953 0.8901087045669556
CurrentTrain: epoch  3, batch     0 | loss: 3.8651013Losses:  3.8543713092803955 1.189762830734253
CurrentTrain: epoch  3, batch     1 | loss: 5.0441341Losses:  3.778578996658325 1.2482502460479736
CurrentTrain: epoch  3, batch     2 | loss: 5.0268292Losses:  3.9880564212799072 0.75719153881073
CurrentTrain: epoch  3, batch     3 | loss: 4.7452478Losses:  3.6537210941314697 1.0431395769119263
CurrentTrain: epoch  4, batch     0 | loss: 4.6968608Losses:  3.0908050537109375 1.0118412971496582
CurrentTrain: epoch  4, batch     1 | loss: 4.1026464Losses:  3.081233024597168 1.119907259941101
CurrentTrain: epoch  4, batch     2 | loss: 4.2011404Losses:  3.555271625518799 0.07944326102733612
CurrentTrain: epoch  4, batch     3 | loss: 3.6347148Losses:  3.8384571075439453 0.9614846706390381
CurrentTrain: epoch  5, batch     0 | loss: 4.7999420Losses:  2.770148754119873 0.9947143793106079
CurrentTrain: epoch  5, batch     1 | loss: 3.7648630Losses:  2.3770039081573486 0.8910948038101196
CurrentTrain: epoch  5, batch     2 | loss: 3.2680988Losses:  3.5037150382995605 0.24404700100421906
CurrentTrain: epoch  5, batch     3 | loss: 3.7477620Losses:  3.0703558921813965 0.7578568458557129
CurrentTrain: epoch  6, batch     0 | loss: 3.8282127Losses:  2.4792935848236084 0.8149067163467407
CurrentTrain: epoch  6, batch     1 | loss: 3.2942004Losses:  3.007930040359497 1.0425862073898315
CurrentTrain: epoch  6, batch     2 | loss: 4.0505161Losses:  2.82318377494812 0.1619344800710678
CurrentTrain: epoch  6, batch     3 | loss: 2.9851182Losses:  2.576348304748535 0.8890106081962585
CurrentTrain: epoch  7, batch     0 | loss: 3.4653590Losses:  3.062063694000244 1.040552020072937
CurrentTrain: epoch  7, batch     1 | loss: 4.1026158Losses:  2.6507678031921387 0.9874559044837952
CurrentTrain: epoch  7, batch     2 | loss: 3.6382236Losses:  1.7939960956573486 0.11021943390369415
CurrentTrain: epoch  7, batch     3 | loss: 1.9042156Losses:  2.5512609481811523 0.8476201295852661
CurrentTrain: epoch  8, batch     0 | loss: 3.3988810Losses:  2.4238085746765137 0.6891347765922546
CurrentTrain: epoch  8, batch     1 | loss: 3.1129434Losses:  2.7742462158203125 0.8817739486694336
CurrentTrain: epoch  8, batch     2 | loss: 3.6560202Losses:  3.486541271209717 0.17151641845703125
CurrentTrain: epoch  8, batch     3 | loss: 3.6580577Losses:  2.4138662815093994 0.5940473675727844
CurrentTrain: epoch  9, batch     0 | loss: 3.0079136Losses:  2.7281017303466797 0.778542160987854
CurrentTrain: epoch  9, batch     1 | loss: 3.5066438Losses:  2.3121185302734375 0.7655820846557617
CurrentTrain: epoch  9, batch     2 | loss: 3.0777006Losses:  2.8868343830108643 0.19450463354587555
CurrentTrain: epoch  9, batch     3 | loss: 3.0813391
Losses:  5.727166175842285 0.6609954833984375
MemoryTrain:  epoch  0, batch     0 | loss: 6.3881617Losses:  8.96200180053711 0.9346047639846802
MemoryTrain:  epoch  0, batch     1 | loss: 9.8966064Losses:  10.35861587524414 0.9213874936103821
MemoryTrain:  epoch  0, batch     2 | loss: 11.2800035Losses:  12.848047256469727 0.11921869963407516
MemoryTrain:  epoch  0, batch     3 | loss: 12.9672661Losses:  0.9930700063705444 0.8721238970756531
MemoryTrain:  epoch  1, batch     0 | loss: 1.8651938Losses:  0.869752049446106 0.8077476024627686
MemoryTrain:  epoch  1, batch     1 | loss: 1.6774997Losses:  0.9654250144958496 0.973301351070404
MemoryTrain:  epoch  1, batch     2 | loss: 1.9387264Losses:  0.2524728775024414 0.052875012159347534
MemoryTrain:  epoch  1, batch     3 | loss: 0.3053479Losses:  0.7626149654388428 0.9182507991790771
MemoryTrain:  epoch  2, batch     0 | loss: 1.6808658Losses:  0.6768937110900879 0.7782196998596191
MemoryTrain:  epoch  2, batch     1 | loss: 1.4551134Losses:  0.4615553319454193 0.7076975703239441
MemoryTrain:  epoch  2, batch     2 | loss: 1.1692529Losses:  0.8406728506088257 0.12317249178886414
MemoryTrain:  epoch  2, batch     3 | loss: 0.9638454Losses:  0.6075637340545654 0.8211565613746643
MemoryTrain:  epoch  3, batch     0 | loss: 1.4287202Losses:  0.6698784232139587 0.8855482935905457
MemoryTrain:  epoch  3, batch     1 | loss: 1.5554267Losses:  0.41398143768310547 0.7621392011642456
MemoryTrain:  epoch  3, batch     2 | loss: 1.1761206Losses:  0.5695993900299072 0.06268783658742905
MemoryTrain:  epoch  3, batch     3 | loss: 0.6322872Losses:  0.44130122661590576 0.8053521513938904
MemoryTrain:  epoch  4, batch     0 | loss: 1.2466533Losses:  0.4494418501853943 0.7069119811058044
MemoryTrain:  epoch  4, batch     1 | loss: 1.1563538Losses:  0.661400318145752 0.9810987710952759
MemoryTrain:  epoch  4, batch     2 | loss: 1.6424991Losses:  0.4114639461040497 0.12890829145908356
MemoryTrain:  epoch  4, batch     3 | loss: 0.5403723Losses:  0.482837438583374 0.8218225240707397
MemoryTrain:  epoch  5, batch     0 | loss: 1.3046600Losses:  0.3996923267841339 0.6559722423553467
MemoryTrain:  epoch  5, batch     1 | loss: 1.0556645Losses:  0.5603637099266052 0.8731429576873779
MemoryTrain:  epoch  5, batch     2 | loss: 1.4335067Losses:  0.2245446890592575 0.035005491226911545
MemoryTrain:  epoch  5, batch     3 | loss: 0.2595502Losses:  0.5069903135299683 0.833277702331543
MemoryTrain:  epoch  6, batch     0 | loss: 1.3402680Losses:  0.44505929946899414 0.7778369188308716
MemoryTrain:  epoch  6, batch     1 | loss: 1.2228962Losses:  0.42360806465148926 0.7997943162918091
MemoryTrain:  epoch  6, batch     2 | loss: 1.2234024Losses:  0.3627502918243408 0.05317052826285362
MemoryTrain:  epoch  6, batch     3 | loss: 0.4159208Losses:  0.39160722494125366 0.8003032207489014
MemoryTrain:  epoch  7, batch     0 | loss: 1.1919105Losses:  0.5062655210494995 0.8681102991104126
MemoryTrain:  epoch  7, batch     1 | loss: 1.3743758Losses:  0.32131582498550415 0.6732729077339172
MemoryTrain:  epoch  7, batch     2 | loss: 0.9945887Losses:  0.20839908719062805 0.02133110538125038
MemoryTrain:  epoch  7, batch     3 | loss: 0.2297302Losses:  0.4354252815246582 0.8214161396026611
MemoryTrain:  epoch  8, batch     0 | loss: 1.2568414Losses:  0.31749045848846436 0.5990427732467651
MemoryTrain:  epoch  8, batch     1 | loss: 0.9165332Losses:  0.39166802167892456 0.8787165284156799
MemoryTrain:  epoch  8, batch     2 | loss: 1.2703846Losses:  0.33135563135147095 0.07029979676008224
MemoryTrain:  epoch  8, batch     3 | loss: 0.4016554Losses:  0.3646170198917389 0.8135648369789124
MemoryTrain:  epoch  9, batch     0 | loss: 1.1781819Losses:  0.4159107804298401 0.7091301679611206
MemoryTrain:  epoch  9, batch     1 | loss: 1.1250410Losses:  0.3337133228778839 0.7665920257568359
MemoryTrain:  epoch  9, batch     2 | loss: 1.1003053Losses:  0.3714666962623596 0.07720275223255157
MemoryTrain:  epoch  9, batch     3 | loss: 0.4486694
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 70.94%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 79.53%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 79.42%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.94%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 77.85%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 77.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.89%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 78.18%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.35%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.07%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 76.88%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 76.31%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.69%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 6.25%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.36%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 81.65%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 81.51%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 81.86%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.71%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 81.36%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 81.76%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 81.96%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 82.04%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 81.92%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 82.01%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 81.90%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.17%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 82.23%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 82.22%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 82.43%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 81.58%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 80.84%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 80.29%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 79.51%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 79.14%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 78.70%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 78.05%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 77.11%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 76.19%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 75.37%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 74.56%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 73.71%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 73.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.39%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 75.51%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 75.51%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 76.87%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 77.70%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 77.88%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 77.69%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 77.72%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 77.75%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 77.56%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.81%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.89%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 78.15%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 77.88%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 77.51%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 77.10%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 76.74%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 76.49%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 76.19%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 76.27%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 76.40%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 76.53%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 76.61%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 76.73%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 76.89%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 76.96%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 77.16%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 76.90%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 76.58%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 76.57%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 76.34%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 76.33%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 75.87%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 75.41%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 74.92%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 74.43%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 74.03%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 73.56%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 73.49%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.62%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 73.66%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 74.04%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 74.20%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 74.67%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 74.27%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 74.06%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 73.70%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 73.53%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 73.29%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 73.12%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 72.99%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 73.00%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 72.80%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 72.64%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 72.35%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 72.21%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.63%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 72.58%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 72.51%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 72.30%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 72.26%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 72.21%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 72.10%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.09%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 72.16%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.53%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 72.80%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 72.66%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:  215 | acc: 56.25%,  total acc: 72.42%   [EVAL] batch:  216 | acc: 25.00%,  total acc: 72.21%   [EVAL] batch:  217 | acc: 56.25%,  total acc: 72.13%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 72.09%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 72.05%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 71.86%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 71.76%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 71.58%   [EVAL] batch:  223 | acc: 18.75%,  total acc: 71.34%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 71.19%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 71.18%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 71.19%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 71.28%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 71.12%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 71.06%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 71.10%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 71.06%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 71.03%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 72.31%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 72.07%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 72.03%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 71.97%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 72.00%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.98%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 71.91%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 72.16%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 72.31%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 72.27%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 72.21%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 72.17%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 72.16%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 72.95%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 73.37%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 73.41%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 73.43%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 73.27%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 73.27%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 73.22%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 73.14%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 73.19%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 73.37%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 73.47%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 73.42%   [EVAL] batch:  308 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:  309 | acc: 37.50%,  total acc: 73.29%   [EVAL] batch:  310 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  311 | acc: 43.75%,  total acc: 73.20%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 73.08%   
cur_acc:  ['0.9484', '0.8194', '0.6210', '0.7817', '0.7569']
his_acc:  ['0.9484', '0.8780', '0.7753', '0.7430', '0.7308']
Clustering into  29  clusters
Clusters:  [ 0  2 19  0  0  0 24  0 20 15 25 23  0  0 11 27  9  0  0 26 22  0  0  0
  0 17  0  0  0  0  2 21  0  0  0  1  0 28 10 12 18  0 14  0 16  8 13  0
  0  0  1  4  0  6  7  0  0  0  5  3]
Losses:  7.108080863952637 1.1377160549163818
CurrentTrain: epoch  0, batch     0 | loss: 8.2457972Losses:  9.813790321350098 1.590576410293579
CurrentTrain: epoch  0, batch     1 | loss: 11.4043665Losses:  6.951876640319824 1.4852995872497559
CurrentTrain: epoch  0, batch     2 | loss: 8.4371758Losses:  4.091701030731201 0.4125901758670807
CurrentTrain: epoch  0, batch     3 | loss: 4.5042911Losses:  4.2853922843933105 1.4081493616104126
CurrentTrain: epoch  1, batch     0 | loss: 5.6935415Losses:  3.597160816192627 1.3069182634353638
CurrentTrain: epoch  1, batch     1 | loss: 4.9040790Losses:  3.9390294551849365 1.1183243989944458
CurrentTrain: epoch  1, batch     2 | loss: 5.0573540Losses:  2.0013885498046875 0.0
CurrentTrain: epoch  1, batch     3 | loss: 2.0013885Losses:  3.6927201747894287 1.203148603439331
CurrentTrain: epoch  2, batch     0 | loss: 4.8958688Losses:  3.531135320663452 1.2644697427749634
CurrentTrain: epoch  2, batch     1 | loss: 4.7956052Losses:  3.4763906002044678 1.0750536918640137
CurrentTrain: epoch  2, batch     2 | loss: 4.5514441Losses:  2.24884033203125 0.2815248668193817
CurrentTrain: epoch  2, batch     3 | loss: 2.5303652Losses:  3.4192678928375244 1.2263635396957397
CurrentTrain: epoch  3, batch     0 | loss: 4.6456313Losses:  3.5847208499908447 1.186077356338501
CurrentTrain: epoch  3, batch     1 | loss: 4.7707982Losses:  3.1700496673583984 1.1505051851272583
CurrentTrain: epoch  3, batch     2 | loss: 4.3205547Losses:  1.9846444129943848 0.15726839005947113
CurrentTrain: epoch  3, batch     3 | loss: 2.1419127Losses:  3.222780704498291 1.1820707321166992
CurrentTrain: epoch  4, batch     0 | loss: 4.4048514Losses:  3.0480265617370605 1.104466199874878
CurrentTrain: epoch  4, batch     1 | loss: 4.1524925Losses:  3.1318724155426025 0.8074141144752502
CurrentTrain: epoch  4, batch     2 | loss: 3.9392865Losses:  2.644744634628296 0.18807858228683472
CurrentTrain: epoch  4, batch     3 | loss: 2.8328233Losses:  3.215860366821289 0.9524139165878296
CurrentTrain: epoch  5, batch     0 | loss: 4.1682744Losses:  2.742302417755127 0.8444401025772095
CurrentTrain: epoch  5, batch     1 | loss: 3.5867424Losses:  2.691904067993164 0.9168455600738525
CurrentTrain: epoch  5, batch     2 | loss: 3.6087496Losses:  1.8114725351333618 0.05979859456419945
CurrentTrain: epoch  5, batch     3 | loss: 1.8712711Losses:  3.107229232788086 1.0737298727035522
CurrentTrain: epoch  6, batch     0 | loss: 4.1809592Losses:  2.373979091644287 0.8818809986114502
CurrentTrain: epoch  6, batch     1 | loss: 3.2558601Losses:  2.571186065673828 0.7658915519714355
CurrentTrain: epoch  6, batch     2 | loss: 3.3370776Losses:  2.899249792098999 0.2934691309928894
CurrentTrain: epoch  6, batch     3 | loss: 3.1927190Losses:  2.3802924156188965 0.7683776021003723
CurrentTrain: epoch  7, batch     0 | loss: 3.1486700Losses:  2.7494277954101562 0.9034598469734192
CurrentTrain: epoch  7, batch     1 | loss: 3.6528876Losses:  2.602654218673706 0.7939319014549255
CurrentTrain: epoch  7, batch     2 | loss: 3.3965862Losses:  1.8064765930175781 0.12307587265968323
CurrentTrain: epoch  7, batch     3 | loss: 1.9295524Losses:  2.201508045196533 0.7790222764015198
CurrentTrain: epoch  8, batch     0 | loss: 2.9805303Losses:  2.341097354888916 0.7734214067459106
CurrentTrain: epoch  8, batch     1 | loss: 3.1145186Losses:  2.784695625305176 0.9302542805671692
CurrentTrain: epoch  8, batch     2 | loss: 3.7149498Losses:  1.8063647747039795 0.14012905955314636
CurrentTrain: epoch  8, batch     3 | loss: 1.9464939Losses:  2.6353695392608643 0.8196790218353271
CurrentTrain: epoch  9, batch     0 | loss: 3.4550486Losses:  2.232842206954956 0.6454599499702454
CurrentTrain: epoch  9, batch     1 | loss: 2.8783021Losses:  2.1865811347961426 0.7401913404464722
CurrentTrain: epoch  9, batch     2 | loss: 2.9267726Losses:  1.766119122505188 0.11959132552146912
CurrentTrain: epoch  9, batch     3 | loss: 1.8857105
Losses:  5.78848123550415 0.6318379044532776
MemoryTrain:  epoch  0, batch     0 | loss: 6.4203191Losses:  8.981241226196289 0.9114954471588135
MemoryTrain:  epoch  0, batch     1 | loss: 9.8927364Losses:  9.653707504272461 0.9163873791694641
MemoryTrain:  epoch  0, batch     2 | loss: 10.5700951Losses:  11.263704299926758 0.8460581302642822
MemoryTrain:  epoch  0, batch     3 | loss: 12.1097622Losses:  0.8858815431594849 0.8384112119674683
MemoryTrain:  epoch  1, batch     0 | loss: 1.7242928Losses:  0.9451931118965149 0.8236058950424194
MemoryTrain:  epoch  1, batch     1 | loss: 1.7687991Losses:  0.8195064067840576 0.8207932710647583
MemoryTrain:  epoch  1, batch     2 | loss: 1.6402997Losses:  0.6209506988525391 0.5487653017044067
MemoryTrain:  epoch  1, batch     3 | loss: 1.1697160Losses:  0.7780545949935913 0.865731418132782
MemoryTrain:  epoch  2, batch     0 | loss: 1.6437860Losses:  0.48355719447135925 0.744550347328186
MemoryTrain:  epoch  2, batch     1 | loss: 1.2281076Losses:  0.6953778862953186 0.8542564511299133
MemoryTrain:  epoch  2, batch     2 | loss: 1.5496343Losses:  0.5430695414543152 0.5581669807434082
MemoryTrain:  epoch  2, batch     3 | loss: 1.1012366Losses:  0.5699141025543213 0.7990495562553406
MemoryTrain:  epoch  3, batch     0 | loss: 1.3689637Losses:  0.5651417970657349 0.6255989074707031
MemoryTrain:  epoch  3, batch     1 | loss: 1.1907407Losses:  0.5355212688446045 0.8652275800704956
MemoryTrain:  epoch  3, batch     2 | loss: 1.4007488Losses:  0.7473171949386597 0.6898283362388611
MemoryTrain:  epoch  3, batch     3 | loss: 1.4371455Losses:  0.531434178352356 0.6417615413665771
MemoryTrain:  epoch  4, batch     0 | loss: 1.1731957Losses:  0.6279539465904236 1.0377366542816162
MemoryTrain:  epoch  4, batch     1 | loss: 1.6656907Losses:  0.46921977400779724 0.7613515853881836
MemoryTrain:  epoch  4, batch     2 | loss: 1.2305714Losses:  0.484746515750885 0.5715216398239136
MemoryTrain:  epoch  4, batch     3 | loss: 1.0562682Losses:  0.5087054371833801 0.6950340270996094
MemoryTrain:  epoch  5, batch     0 | loss: 1.2037394Losses:  0.423473984003067 0.7230328321456909
MemoryTrain:  epoch  5, batch     1 | loss: 1.1465068Losses:  0.6285433769226074 0.6068704128265381
MemoryTrain:  epoch  5, batch     2 | loss: 1.2354138Losses:  0.5601110458374023 0.8510646224021912
MemoryTrain:  epoch  5, batch     3 | loss: 1.4111757Losses:  0.41155603528022766 0.6600667238235474
MemoryTrain:  epoch  6, batch     0 | loss: 1.0716227Losses:  0.4783482551574707 0.8021175265312195
MemoryTrain:  epoch  6, batch     1 | loss: 1.2804658Losses:  0.4169987738132477 0.779488205909729
MemoryTrain:  epoch  6, batch     2 | loss: 1.1964869Losses:  0.5396569967269897 0.5706851482391357
MemoryTrain:  epoch  6, batch     3 | loss: 1.1103421Losses:  0.45079347491264343 0.8234593272209167
MemoryTrain:  epoch  7, batch     0 | loss: 1.2742528Losses:  0.4326692223548889 0.5680686235427856
MemoryTrain:  epoch  7, batch     1 | loss: 1.0007379Losses:  0.3661879301071167 0.7511932849884033
MemoryTrain:  epoch  7, batch     2 | loss: 1.1173812Losses:  0.4702065885066986 0.5423278212547302
MemoryTrain:  epoch  7, batch     3 | loss: 1.0125344Losses:  0.45045071840286255 0.6476006507873535
MemoryTrain:  epoch  8, batch     0 | loss: 1.0980513Losses:  0.41279858350753784 0.6737822890281677
MemoryTrain:  epoch  8, batch     1 | loss: 1.0865809Losses:  0.41088563203811646 0.748165488243103
MemoryTrain:  epoch  8, batch     2 | loss: 1.1590512Losses:  0.42750462889671326 0.5427082180976868
MemoryTrain:  epoch  8, batch     3 | loss: 0.9702128Losses:  0.3783094882965088 0.6338061094284058
MemoryTrain:  epoch  9, batch     0 | loss: 1.0121156Losses:  0.43304896354675293 0.8333791494369507
MemoryTrain:  epoch  9, batch     1 | loss: 1.2664281Losses:  0.4439263939857483 0.6752967834472656
MemoryTrain:  epoch  9, batch     2 | loss: 1.1192231Losses:  0.4964684844017029 0.525618314743042
MemoryTrain:  epoch  9, batch     3 | loss: 1.0220869
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 52.60%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 43.09%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 45.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.51%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 57.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 68.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 67.95%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 67.52%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 67.00%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 67.13%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 67.06%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 66.67%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.23%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.45%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 84.51%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.31%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.31%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 84.25%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 84.32%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 83.15%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 82.89%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 81.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 81.35%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 81.16%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 81.52%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 81.16%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.34%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 81.17%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 80.51%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 79.87%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 79.33%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 78.48%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 77.70%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 77.06%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 76.13%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 75.22%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 74.41%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 73.69%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 72.84%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 74.87%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 77.25%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 77.28%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 77.32%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 77.14%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 77.12%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 77.15%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.43%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 77.51%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 77.77%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 77.38%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 76.61%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 76.21%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 76.01%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 75.57%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 75.52%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 75.56%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 75.59%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 75.63%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 75.67%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 75.65%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 75.47%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 75.17%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 75.13%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 74.96%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 74.96%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 74.50%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 74.01%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.53%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 73.05%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 72.66%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 72.20%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 72.13%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.65%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.83%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 73.24%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 73.45%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 73.24%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 72.84%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.53%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 72.18%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 71.91%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 71.57%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 71.34%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 71.42%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 71.51%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 71.42%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 71.29%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 71.06%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 71.01%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 70.86%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 70.81%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.93%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 71.36%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 71.33%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 71.24%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 71.04%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 71.05%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 70.87%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 70.73%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 70.69%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 71.45%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 71.38%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 71.22%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 71.05%   [EVAL] batch:  217 | acc: 56.25%,  total acc: 70.99%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 70.92%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 70.82%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 70.62%   [EVAL] batch:  221 | acc: 25.00%,  total acc: 70.41%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 70.18%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 69.89%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 69.69%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 69.74%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  229 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 69.81%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 69.69%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 69.61%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 69.52%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 69.39%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 69.17%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.12%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 70.39%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 70.41%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 70.30%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 70.22%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 70.16%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 70.33%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 70.44%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 70.29%   [EVAL] batch:  272 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 70.10%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 71.47%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.60%   [EVAL] batch:  294 | acc: 18.75%,  total acc: 71.42%   [EVAL] batch:  295 | acc: 6.25%,  total acc: 71.20%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 70.99%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 70.86%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 71.04%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 71.07%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  306 | acc: 31.25%,  total acc: 71.05%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 70.90%   [EVAL] batch:  308 | acc: 25.00%,  total acc: 70.75%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 70.54%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  311 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 70.21%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 70.02%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 69.86%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 69.70%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 69.52%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 69.38%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 69.24%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 69.54%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 69.34%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 69.15%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 69.02%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 68.84%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 68.71%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 68.77%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 69.87%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 70.16%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 70.03%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 70.01%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 69.93%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 69.79%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 69.70%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.75%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.81%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 70.11%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 70.02%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.96%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.90%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 69.83%   [EVAL] batch:  369 | acc: 56.25%,  total acc: 69.80%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 69.81%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.76%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 69.77%   
cur_acc:  ['0.9484', '0.8194', '0.6210', '0.7817', '0.7569', '0.6667']
his_acc:  ['0.9484', '0.8780', '0.7753', '0.7430', '0.7308', '0.6977']
Clustering into  34  clusters
Clusters:  [ 0  5 22  0  0  0 27  0 23 31 33 28  0  0 26 32 24  0  0 29 15  0  0  0
  0 19  0  0  0  0  5 21  0  0  0  2  0 16 25 13 20  0 17  0 18  9  8  0
  0  0  2 11  0  7 30  0  0  0 10  4  0  0  0 14  0 12  0  6  3  1]
Losses:  7.186060905456543 1.2388644218444824
CurrentTrain: epoch  0, batch     0 | loss: 8.4249249Losses:  9.57852840423584 1.2134363651275635
CurrentTrain: epoch  0, batch     1 | loss: 10.7919645Losses:  7.666243553161621 1.2642121315002441
CurrentTrain: epoch  0, batch     2 | loss: 8.9304562Losses:  5.02828311920166 5.960465188081798e-08
CurrentTrain: epoch  0, batch     3 | loss: 5.0282831Losses:  3.3455114364624023 0.9634916186332703
CurrentTrain: epoch  1, batch     0 | loss: 4.3090029Losses:  3.5660135746002197 1.0846619606018066
CurrentTrain: epoch  1, batch     1 | loss: 4.6506758Losses:  3.9050097465515137 1.1612378358840942
CurrentTrain: epoch  1, batch     2 | loss: 5.0662475Losses:  3.7078466415405273 0.12302440404891968
CurrentTrain: epoch  1, batch     3 | loss: 3.8308711Losses:  3.525627851486206 1.1083005666732788
CurrentTrain: epoch  2, batch     0 | loss: 4.6339283Losses:  3.2086029052734375 0.8684216141700745
CurrentTrain: epoch  2, batch     1 | loss: 4.0770245Losses:  3.0798864364624023 0.7575811147689819
CurrentTrain: epoch  2, batch     2 | loss: 3.8374677Losses:  3.308692455291748 0.03676004707813263
CurrentTrain: epoch  2, batch     3 | loss: 3.3454525Losses:  3.0050241947174072 0.9124301671981812
CurrentTrain: epoch  3, batch     0 | loss: 3.9174542Losses:  3.0700278282165527 0.7190864086151123
CurrentTrain: epoch  3, batch     1 | loss: 3.7891142Losses:  2.9651477336883545 0.7681448459625244
CurrentTrain: epoch  3, batch     2 | loss: 3.7332926Losses:  2.9973442554473877 8.94069742685133e-08
CurrentTrain: epoch  3, batch     3 | loss: 2.9973443Losses:  3.1038453578948975 0.7390424013137817
CurrentTrain: epoch  4, batch     0 | loss: 3.8428879Losses:  2.6507668495178223 0.7671912908554077
CurrentTrain: epoch  4, batch     1 | loss: 3.4179583Losses:  2.4381606578826904 0.6267508268356323
CurrentTrain: epoch  4, batch     2 | loss: 3.0649114Losses:  2.1589417457580566 8.94069742685133e-08
CurrentTrain: epoch  4, batch     3 | loss: 2.1589417Losses:  2.547201633453369 0.6141002774238586
CurrentTrain: epoch  5, batch     0 | loss: 3.1613019Losses:  2.3700151443481445 0.6803531050682068
CurrentTrain: epoch  5, batch     1 | loss: 3.0503683Losses:  2.596438407897949 0.7375560998916626
CurrentTrain: epoch  5, batch     2 | loss: 3.3339944Losses:  2.1777968406677246 0.10630927979946136
CurrentTrain: epoch  5, batch     3 | loss: 2.2841060Losses:  2.176516532897949 0.5732415318489075
CurrentTrain: epoch  6, batch     0 | loss: 2.7497580Losses:  2.196868896484375 0.48361361026763916
CurrentTrain: epoch  6, batch     1 | loss: 2.6804824Losses:  2.617286205291748 0.4634573459625244
CurrentTrain: epoch  6, batch     2 | loss: 3.0807436Losses:  2.129645824432373 0.05792822316288948
CurrentTrain: epoch  6, batch     3 | loss: 2.1875741Losses:  2.3779196739196777 0.6148461103439331
CurrentTrain: epoch  7, batch     0 | loss: 2.9927659Losses:  1.99104642868042 0.5588230490684509
CurrentTrain: epoch  7, batch     1 | loss: 2.5498695Losses:  1.96048104763031 0.4246746003627777
CurrentTrain: epoch  7, batch     2 | loss: 2.3851557Losses:  1.8762435913085938 0.05630762130022049
CurrentTrain: epoch  7, batch     3 | loss: 1.9325513Losses:  2.120680332183838 0.5823506712913513
CurrentTrain: epoch  8, batch     0 | loss: 2.7030311Losses:  1.8624659776687622 0.430905282497406
CurrentTrain: epoch  8, batch     1 | loss: 2.2933712Losses:  2.0517501831054688 0.5752503275871277
CurrentTrain: epoch  8, batch     2 | loss: 2.6270006Losses:  1.7338554859161377 0.04905249923467636
CurrentTrain: epoch  8, batch     3 | loss: 1.7829080Losses:  2.1095082759857178 0.41730543971061707
CurrentTrain: epoch  9, batch     0 | loss: 2.5268137Losses:  1.9384801387786865 0.5083461999893188
CurrentTrain: epoch  9, batch     1 | loss: 2.4468265Losses:  1.8436977863311768 0.4420729875564575
CurrentTrain: epoch  9, batch     2 | loss: 2.2857709Losses:  1.7047486305236816 0.07978777587413788
CurrentTrain: epoch  9, batch     3 | loss: 1.7845364
Losses:  6.15431022644043 0.9861426949501038
MemoryTrain:  epoch  0, batch     0 | loss: 7.1404529Losses:  9.063629150390625 0.6148046255111694
MemoryTrain:  epoch  0, batch     1 | loss: 9.6784334Losses:  10.082457542419434 0.7996945381164551
MemoryTrain:  epoch  0, batch     2 | loss: 10.8821526Losses:  10.506738662719727 0.6370418071746826
MemoryTrain:  epoch  0, batch     3 | loss: 11.1437807Losses:  11.94791316986084 0.26110926270484924
MemoryTrain:  epoch  0, batch     4 | loss: 12.2090225Losses:  1.5865319967269897 0.7763835191726685
MemoryTrain:  epoch  1, batch     0 | loss: 2.3629155Losses:  0.7442193031311035 0.7639923095703125
MemoryTrain:  epoch  1, batch     1 | loss: 1.5082116Losses:  1.1215770244598389 0.7493293285369873
MemoryTrain:  epoch  1, batch     2 | loss: 1.8709064Losses:  0.9503036141395569 0.784092903137207
MemoryTrain:  epoch  1, batch     3 | loss: 1.7343965Losses:  0.9947624206542969 0.21247538924217224
MemoryTrain:  epoch  1, batch     4 | loss: 1.2072378Losses:  0.7715597152709961 0.6332817077636719
MemoryTrain:  epoch  2, batch     0 | loss: 1.4048414Losses:  0.8587554097175598 0.7090115547180176
MemoryTrain:  epoch  2, batch     1 | loss: 1.5677669Losses:  1.2780247926712036 0.8431355953216553
MemoryTrain:  epoch  2, batch     2 | loss: 2.1211605Losses:  0.7348195314407349 0.7579972743988037
MemoryTrain:  epoch  2, batch     3 | loss: 1.4928168Losses:  0.7730082273483276 0.22031259536743164
MemoryTrain:  epoch  2, batch     4 | loss: 0.9933208Losses:  0.843486487865448 0.8442814350128174
MemoryTrain:  epoch  3, batch     0 | loss: 1.6877680Losses:  0.6467655897140503 0.622144341468811
MemoryTrain:  epoch  3, batch     1 | loss: 1.2689099Losses:  0.9759759306907654 0.8426247239112854
MemoryTrain:  epoch  3, batch     2 | loss: 1.8186007Losses:  0.6046122312545776 0.7748181223869324
MemoryTrain:  epoch  3, batch     3 | loss: 1.3794303Losses:  0.3611704707145691 0.26550859212875366
MemoryTrain:  epoch  3, batch     4 | loss: 0.6266791Losses:  0.52557373046875 0.6733977198600769
MemoryTrain:  epoch  4, batch     0 | loss: 1.1989715Losses:  0.5808354020118713 0.749638557434082
MemoryTrain:  epoch  4, batch     1 | loss: 1.3304739Losses:  0.6953782439231873 0.5702091455459595
MemoryTrain:  epoch  4, batch     2 | loss: 1.2655873Losses:  0.7946648597717285 0.8307533264160156
MemoryTrain:  epoch  4, batch     3 | loss: 1.6254182Losses:  0.8843891620635986 0.41344237327575684
MemoryTrain:  epoch  4, batch     4 | loss: 1.2978315Losses:  0.5502904057502747 0.763496458530426
MemoryTrain:  epoch  5, batch     0 | loss: 1.3137869Losses:  0.7301371693611145 0.7637976408004761
MemoryTrain:  epoch  5, batch     1 | loss: 1.4939349Losses:  0.6018801927566528 0.6487635970115662
MemoryTrain:  epoch  5, batch     2 | loss: 1.2506437Losses:  0.5182771682739258 0.6110479235649109
MemoryTrain:  epoch  5, batch     3 | loss: 1.1293252Losses:  0.4631747901439667 0.25895214080810547
MemoryTrain:  epoch  5, batch     4 | loss: 0.7221270Losses:  0.5019816756248474 0.5616739392280579
MemoryTrain:  epoch  6, batch     0 | loss: 1.0636556Losses:  0.4865410327911377 0.8590599894523621
MemoryTrain:  epoch  6, batch     1 | loss: 1.3456011Losses:  0.6097625494003296 0.6551604270935059
MemoryTrain:  epoch  6, batch     2 | loss: 1.2649230Losses:  0.47932693362236023 0.6192224621772766
MemoryTrain:  epoch  6, batch     3 | loss: 1.0985494Losses:  0.6400283575057983 0.3570847511291504
MemoryTrain:  epoch  6, batch     4 | loss: 0.9971131Losses:  0.5442379713058472 0.8762521147727966
MemoryTrain:  epoch  7, batch     0 | loss: 1.4204900Losses:  0.6039518117904663 0.6908888816833496
MemoryTrain:  epoch  7, batch     1 | loss: 1.2948407Losses:  0.5325993299484253 0.5891925692558289
MemoryTrain:  epoch  7, batch     2 | loss: 1.1217918Losses:  0.3935772776603699 0.5451987385749817
MemoryTrain:  epoch  7, batch     3 | loss: 0.9387760Losses:  0.38951700925827026 0.22136792540550232
MemoryTrain:  epoch  7, batch     4 | loss: 0.6108849Losses:  0.4763525426387787 0.4861786365509033
MemoryTrain:  epoch  8, batch     0 | loss: 0.9625312Losses:  0.5104401111602783 0.7107052803039551
MemoryTrain:  epoch  8, batch     1 | loss: 1.2211454Losses:  0.556711733341217 0.7435741424560547
MemoryTrain:  epoch  8, batch     2 | loss: 1.3002858Losses:  0.4862836003303528 0.7284544706344604
MemoryTrain:  epoch  8, batch     3 | loss: 1.2147381Losses:  0.3406175374984741 0.21427075564861298
MemoryTrain:  epoch  8, batch     4 | loss: 0.5548883Losses:  0.5081653594970703 0.6781355142593384
MemoryTrain:  epoch  9, batch     0 | loss: 1.1863009Losses:  0.4198770523071289 0.5987645387649536
MemoryTrain:  epoch  9, batch     1 | loss: 1.0186416Losses:  0.5022695660591125 0.9054677486419678
MemoryTrain:  epoch  9, batch     2 | loss: 1.4077373Losses:  0.4328058660030365 0.5275342464447021
MemoryTrain:  epoch  9, batch     3 | loss: 0.9603401Losses:  0.4496200978755951 0.2088325023651123
MemoryTrain:  epoch  9, batch     4 | loss: 0.6584526
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 60.65%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 58.19%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 56.88%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 55.65%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 55.47%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 55.49%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 55.71%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 55.41%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 55.10%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 55.29%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 55.31%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 55.79%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 55.95%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 56.54%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 56.68%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 55.83%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 54.76%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 53.86%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 53.12%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 52.42%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 52.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 52.94%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 53.85%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 54.72%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 55.56%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 56.36%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 57.89%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.41%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 58.79%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 60.48%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 60.02%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 84.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.44%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 83.26%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.78%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 81.66%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 81.45%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 81.45%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 81.54%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 81.54%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 81.78%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 81.93%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 81.76%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 81.75%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 81.09%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 80.36%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 79.81%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 79.03%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 78.52%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 78.01%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 77.36%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 76.43%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 75.52%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 74.71%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 73.98%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 73.20%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.31%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 74.80%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 74.93%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 75.06%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.13%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 74.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 76.13%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 77.54%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 77.36%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 77.39%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 77.37%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 77.19%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 77.17%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 77.21%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.48%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.66%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 77.48%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 76.92%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 76.42%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 75.87%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 75.38%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 74.86%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 74.76%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 74.81%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.91%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 74.91%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.95%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 75.04%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 75.09%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.18%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.22%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 75.04%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 74.70%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 74.36%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 74.06%   [EVAL] batch:  147 | acc: 6.25%,  total acc: 73.61%   [EVAL] batch:  148 | acc: 6.25%,  total acc: 73.15%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 72.71%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.27%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 71.79%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.32%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 70.86%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.48%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.03%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.27%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.38%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 71.22%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.18%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 70.76%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 70.42%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 70.05%   [EVAL] batch:  173 | acc: 12.50%,  total acc: 69.72%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 69.39%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 69.25%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 69.31%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 69.45%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 69.27%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 69.19%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 68.92%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  188 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 68.98%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 69.00%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 68.97%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 68.87%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.81%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 68.63%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 68.57%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.66%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  213 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  214 | acc: 25.00%,  total acc: 69.10%   [EVAL] batch:  215 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:  216 | acc: 25.00%,  total acc: 68.78%   [EVAL] batch:  217 | acc: 31.25%,  total acc: 68.61%   [EVAL] batch:  218 | acc: 37.50%,  total acc: 68.46%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 68.38%   [EVAL] batch:  220 | acc: 18.75%,  total acc: 68.16%   [EVAL] batch:  221 | acc: 12.50%,  total acc: 67.91%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 67.68%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 67.41%   [EVAL] batch:  224 | acc: 12.50%,  total acc: 67.17%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 67.48%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 67.31%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 67.15%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.11%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 66.98%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 66.99%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 68.38%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 68.16%   [EVAL] batch:  254 | acc: 56.25%,  total acc: 68.11%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 68.07%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 68.07%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.08%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 68.01%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  265 | acc: 93.75%,  total acc: 68.35%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 68.40%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 68.18%   [EVAL] batch:  273 | acc: 37.50%,  total acc: 68.07%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 68.05%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 69.48%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 69.65%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:  294 | acc: 25.00%,  total acc: 69.49%   [EVAL] batch:  295 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 69.13%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 68.96%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 69.18%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 69.03%   [EVAL] batch:  308 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 68.69%   [EVAL] batch:  310 | acc: 37.50%,  total acc: 68.59%   [EVAL] batch:  311 | acc: 18.75%,  total acc: 68.43%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 68.33%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 68.19%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.04%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 67.84%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 67.69%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 67.55%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 67.40%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 67.46%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 67.49%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 67.57%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.63%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 67.50%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 67.30%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  328 | acc: 6.25%,  total acc: 66.93%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 66.76%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 66.66%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 67.84%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 68.12%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 68.14%   [EVAL] batch:  351 | acc: 18.75%,  total acc: 68.00%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 67.94%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 67.78%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 67.91%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 68.20%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 68.13%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 68.03%   [EVAL] batch:  365 | acc: 18.75%,  total acc: 67.90%   [EVAL] batch:  366 | acc: 31.25%,  total acc: 67.80%   [EVAL] batch:  367 | acc: 31.25%,  total acc: 67.70%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 67.60%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 67.49%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 67.34%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 67.23%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 67.08%   [EVAL] batch:  379 | acc: 37.50%,  total acc: 67.01%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 66.90%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 67.14%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  387 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 67.34%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 67.41%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 67.37%   [EVAL] batch:  395 | acc: 68.75%,  total acc: 67.38%   [EVAL] batch:  396 | acc: 50.00%,  total acc: 67.33%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 67.36%   [EVAL] batch:  399 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 67.25%   [EVAL] batch:  401 | acc: 25.00%,  total acc: 67.15%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 67.04%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 66.94%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:  405 | acc: 18.75%,  total acc: 66.70%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 66.66%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 66.63%   [EVAL] batch:  409 | acc: 56.25%,  total acc: 66.60%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 66.59%   [EVAL] batch:  411 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:  412 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 66.46%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 66.43%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 66.44%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 66.47%   [EVAL] batch:  419 | acc: 18.75%,  total acc: 66.35%   [EVAL] batch:  420 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  421 | acc: 12.50%,  total acc: 66.08%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 65.97%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 65.86%   [EVAL] batch:  424 | acc: 31.25%,  total acc: 65.78%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  433 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 66.52%   
cur_acc:  ['0.9484', '0.8194', '0.6210', '0.7817', '0.7569', '0.6667', '0.6002']
his_acc:  ['0.9484', '0.8780', '0.7753', '0.7430', '0.7308', '0.6977', '0.6652']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 37 29  0  0 28 32 23  0  0 35 34  0  0  1
  1 19  0  0  0  0  5 22  0  0  0  2  0 18 27 20 26  0 10  0  9 25 36  0
  0  0  2 16  0  0 17  0  0  0 13 11  0  0  0 12  0 31  0 30 15 14  7  4
  1  0  0  8  0  6  0  3]
Losses:  7.020047187805176 1.5016365051269531
CurrentTrain: epoch  0, batch     0 | loss: 8.5216837Losses:  8.078546524047852 1.2867300510406494
CurrentTrain: epoch  0, batch     1 | loss: 9.3652763Losses:  6.83560848236084 1.4532538652420044
CurrentTrain: epoch  0, batch     2 | loss: 8.2888622Losses:  3.852201223373413 0.2417587786912918
CurrentTrain: epoch  0, batch     3 | loss: 4.0939598Losses:  4.0136003494262695 1.09987473487854
CurrentTrain: epoch  1, batch     0 | loss: 5.1134748Losses:  4.107504844665527 1.1309418678283691
CurrentTrain: epoch  1, batch     1 | loss: 5.2384467Losses:  3.108970880508423 1.0352948904037476
CurrentTrain: epoch  1, batch     2 | loss: 4.1442657Losses:  4.349359035491943 0.5890216827392578
CurrentTrain: epoch  1, batch     3 | loss: 4.9383807Losses:  3.48696231842041 1.174506425857544
CurrentTrain: epoch  2, batch     0 | loss: 4.6614685Losses:  3.4481089115142822 1.111916422843933
CurrentTrain: epoch  2, batch     1 | loss: 4.5600252Losses:  3.6076879501342773 1.1084176301956177
CurrentTrain: epoch  2, batch     2 | loss: 4.7161055Losses:  2.310783863067627 0.1689797341823578
CurrentTrain: epoch  2, batch     3 | loss: 2.4797635Losses:  4.016793727874756 1.0005457401275635
CurrentTrain: epoch  3, batch     0 | loss: 5.0173397Losses:  2.9067485332489014 0.824397623538971
CurrentTrain: epoch  3, batch     1 | loss: 3.7311461Losses:  2.889893054962158 0.8138494491577148
CurrentTrain: epoch  3, batch     2 | loss: 3.7037425Losses:  1.919161319732666 0.0793028473854065
CurrentTrain: epoch  3, batch     3 | loss: 1.9984641Losses:  3.106304168701172 0.8647687435150146
CurrentTrain: epoch  4, batch     0 | loss: 3.9710729Losses:  3.0577712059020996 0.8413906097412109
CurrentTrain: epoch  4, batch     1 | loss: 3.8991618Losses:  2.733651638031006 0.8495155572891235
CurrentTrain: epoch  4, batch     2 | loss: 3.5831671Losses:  2.939570665359497 0.08720970153808594
CurrentTrain: epoch  4, batch     3 | loss: 3.0267804Losses:  2.6819422245025635 0.708961009979248
CurrentTrain: epoch  5, batch     0 | loss: 3.3909032Losses:  2.8068995475769043 0.711589515209198
CurrentTrain: epoch  5, batch     1 | loss: 3.5184891Losses:  2.840062379837036 0.7294940948486328
CurrentTrain: epoch  5, batch     2 | loss: 3.5695565Losses:  2.8680801391601562 0.1479441225528717
CurrentTrain: epoch  5, batch     3 | loss: 3.0160244Losses:  2.6668713092803955 0.7749348878860474
CurrentTrain: epoch  6, batch     0 | loss: 3.4418063Losses:  3.005664348602295 0.7925971150398254
CurrentTrain: epoch  6, batch     1 | loss: 3.7982614Losses:  2.3940553665161133 0.6292417645454407
CurrentTrain: epoch  6, batch     2 | loss: 3.0232971Losses:  2.5331497192382812 0.19236493110656738
CurrentTrain: epoch  6, batch     3 | loss: 2.7255147Losses:  2.492518186569214 0.656944751739502
CurrentTrain: epoch  7, batch     0 | loss: 3.1494629Losses:  2.7603704929351807 0.8212112188339233
CurrentTrain: epoch  7, batch     1 | loss: 3.5815816Losses:  2.5931851863861084 0.8030141592025757
CurrentTrain: epoch  7, batch     2 | loss: 3.3961992Losses:  1.8005194664001465 5.960464477539063e-08
CurrentTrain: epoch  7, batch     3 | loss: 1.8005195Losses:  2.2015480995178223 0.5671201348304749
CurrentTrain: epoch  8, batch     0 | loss: 2.7686682Losses:  2.9407830238342285 0.7027237415313721
CurrentTrain: epoch  8, batch     1 | loss: 3.6435068Losses:  2.3570027351379395 0.6812509298324585
CurrentTrain: epoch  8, batch     2 | loss: 3.0382538Losses:  1.702853798866272 0.05241471529006958
CurrentTrain: epoch  8, batch     3 | loss: 1.7552686Losses:  2.235654830932617 0.6123785972595215
CurrentTrain: epoch  9, batch     0 | loss: 2.8480334Losses:  2.1476807594299316 0.5519405603408813
CurrentTrain: epoch  9, batch     1 | loss: 2.6996212Losses:  2.8524069786071777 0.6495672464370728
CurrentTrain: epoch  9, batch     2 | loss: 3.5019741Losses:  1.6586543321609497 0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.6586543
Losses:  6.288403034210205 0.7327768802642822
MemoryTrain:  epoch  0, batch     0 | loss: 7.0211802Losses:  9.484837532043457 0.9084810614585876
MemoryTrain:  epoch  0, batch     1 | loss: 10.3933182Losses:  10.206802368164062 0.4633866846561432
MemoryTrain:  epoch  0, batch     2 | loss: 10.6701889Losses:  10.130834579467773 0.7409340143203735
MemoryTrain:  epoch  0, batch     3 | loss: 10.8717690Losses:  10.663902282714844 0.7200226783752441
MemoryTrain:  epoch  0, batch     4 | loss: 11.3839245Losses:  0.846284031867981 0.5997024178504944
MemoryTrain:  epoch  1, batch     0 | loss: 1.4459865Losses:  1.156410813331604 0.6804367303848267
MemoryTrain:  epoch  1, batch     1 | loss: 1.8368475Losses:  0.9426507949829102 0.6400372982025146
MemoryTrain:  epoch  1, batch     2 | loss: 1.5826881Losses:  0.935560405254364 0.8340911865234375
MemoryTrain:  epoch  1, batch     3 | loss: 1.7696517Losses:  1.2383852005004883 0.7242993116378784
MemoryTrain:  epoch  1, batch     4 | loss: 1.9626845Losses:  0.873388409614563 0.6421971321105957
MemoryTrain:  epoch  2, batch     0 | loss: 1.5155855Losses:  0.5934221744537354 0.5973761081695557
MemoryTrain:  epoch  2, batch     1 | loss: 1.1907983Losses:  0.8255966901779175 0.6036179065704346
MemoryTrain:  epoch  2, batch     2 | loss: 1.4292146Losses:  0.8116025924682617 0.8785581588745117
MemoryTrain:  epoch  2, batch     3 | loss: 1.6901608Losses:  1.135265588760376 0.7420957684516907
MemoryTrain:  epoch  2, batch     4 | loss: 1.8773613Losses:  0.5446363687515259 0.6384279727935791
MemoryTrain:  epoch  3, batch     0 | loss: 1.1830643Losses:  0.6806846857070923 0.7385609745979309
MemoryTrain:  epoch  3, batch     1 | loss: 1.4192457Losses:  0.9499732255935669 0.7769902944564819
MemoryTrain:  epoch  3, batch     2 | loss: 1.7269635Losses:  1.024754524230957 0.692397952079773
MemoryTrain:  epoch  3, batch     3 | loss: 1.7171525Losses:  0.6165304183959961 0.7261961698532104
MemoryTrain:  epoch  3, batch     4 | loss: 1.3427266Losses:  0.5857305526733398 0.7173488140106201
MemoryTrain:  epoch  4, batch     0 | loss: 1.3030794Losses:  0.5732110738754272 0.6749202609062195
MemoryTrain:  epoch  4, batch     1 | loss: 1.2481313Losses:  0.860970139503479 0.6895849108695984
MemoryTrain:  epoch  4, batch     2 | loss: 1.5505550Losses:  0.6530632376670837 0.6524338722229004
MemoryTrain:  epoch  4, batch     3 | loss: 1.3054972Losses:  0.5116697549819946 0.6187141537666321
MemoryTrain:  epoch  4, batch     4 | loss: 1.1303840Losses:  0.5344252586364746 0.6671962738037109
MemoryTrain:  epoch  5, batch     0 | loss: 1.2016215Losses:  0.657649040222168 0.7657840847969055
MemoryTrain:  epoch  5, batch     1 | loss: 1.4234331Losses:  0.5221794843673706 0.652011513710022
MemoryTrain:  epoch  5, batch     2 | loss: 1.1741910Losses:  0.7842673063278198 0.7556798458099365
MemoryTrain:  epoch  5, batch     3 | loss: 1.5399472Losses:  0.42875948548316956 0.5162914991378784
MemoryTrain:  epoch  5, batch     4 | loss: 0.9450510Losses:  0.537312388420105 0.6456094980239868
MemoryTrain:  epoch  6, batch     0 | loss: 1.1829219Losses:  0.5233222246170044 0.5675572156906128
MemoryTrain:  epoch  6, batch     1 | loss: 1.0908794Losses:  0.45805054903030396 0.5976136326789856
MemoryTrain:  epoch  6, batch     2 | loss: 1.0556642Losses:  0.633860170841217 0.7929359078407288
MemoryTrain:  epoch  6, batch     3 | loss: 1.4267961Losses:  0.5421572923660278 0.6301275491714478
MemoryTrain:  epoch  6, batch     4 | loss: 1.1722848Losses:  0.46221721172332764 0.5815165042877197
MemoryTrain:  epoch  7, batch     0 | loss: 1.0437337Losses:  0.46115443110466003 0.6323649287223816
MemoryTrain:  epoch  7, batch     1 | loss: 1.0935193Losses:  0.5758122205734253 0.6132187843322754
MemoryTrain:  epoch  7, batch     2 | loss: 1.1890310Losses:  0.480027437210083 0.6344479322433472
MemoryTrain:  epoch  7, batch     3 | loss: 1.1144754Losses:  0.5959292054176331 0.728542685508728
MemoryTrain:  epoch  7, batch     4 | loss: 1.3244720Losses:  0.4443199634552002 0.4965907633304596
MemoryTrain:  epoch  8, batch     0 | loss: 0.9409107Losses:  0.44668763875961304 0.5516282320022583
MemoryTrain:  epoch  8, batch     1 | loss: 0.9983159Losses:  0.578997015953064 0.7172670364379883
MemoryTrain:  epoch  8, batch     2 | loss: 1.2962641Losses:  0.565017580986023 0.6668651103973389
MemoryTrain:  epoch  8, batch     3 | loss: 1.2318827Losses:  0.4879199266433716 0.659606397151947
MemoryTrain:  epoch  8, batch     4 | loss: 1.1475263Losses:  0.5401443243026733 0.5483745336532593
MemoryTrain:  epoch  9, batch     0 | loss: 1.0885189Losses:  0.48839235305786133 0.6375331878662109
MemoryTrain:  epoch  9, batch     1 | loss: 1.1259255Losses:  0.4182606339454651 0.541663646697998
MemoryTrain:  epoch  9, batch     2 | loss: 0.9599243Losses:  0.5030428767204285 0.6184760928153992
MemoryTrain:  epoch  9, batch     3 | loss: 1.1215190Losses:  0.4866423010826111 0.6749825477600098
MemoryTrain:  epoch  9, batch     4 | loss: 1.1616249
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 61.54%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 59.49%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 57.37%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 54.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 52.82%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 53.52%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 54.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 55.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 57.99%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 59.12%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 59.87%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 60.12%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 60.47%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 61.53%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 63.15%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 64.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 64.43%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 66.17%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.84%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 85.19%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.66%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.10%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 83.30%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 82.94%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 82.60%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 82.38%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.16%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 82.04%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 82.13%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 82.21%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 82.28%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 82.52%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 81.88%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 81.16%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 80.22%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 79.56%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 79.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 78.37%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 77.84%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 77.40%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 76.25%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 75.77%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 75.08%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 74.17%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 73.29%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 72.50%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 71.80%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 70.98%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 70.81%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 73.02%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 74.35%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 75.72%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 75.59%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 75.43%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 75.32%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 75.37%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 76.07%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 76.16%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 75.69%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 75.15%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 74.56%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 73.98%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 73.41%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 72.85%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 72.74%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.76%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.87%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 72.89%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 72.89%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 72.77%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 72.78%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 72.62%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 72.51%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.22%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 71.85%   [EVAL] batch:  145 | acc: 0.00%,  total acc: 71.36%   [EVAL] batch:  146 | acc: 6.25%,  total acc: 70.92%   [EVAL] batch:  147 | acc: 0.00%,  total acc: 70.44%   [EVAL] batch:  148 | acc: 6.25%,  total acc: 70.01%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 69.58%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 69.16%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 68.71%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.26%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 67.82%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 67.42%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 66.99%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 67.10%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 68.57%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 68.31%   [EVAL] batch:  170 | acc: 0.00%,  total acc: 67.91%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 67.55%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 67.20%   [EVAL] batch:  173 | acc: 12.50%,  total acc: 66.88%   [EVAL] batch:  174 | acc: 0.00%,  total acc: 66.50%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 66.26%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 66.31%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 66.39%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 66.40%   [EVAL] batch:  181 | acc: 31.25%,  total acc: 66.21%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:  185 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 66.49%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 66.51%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 66.36%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  200 | acc: 31.25%,  total acc: 66.32%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 66.15%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 65.73%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 65.73%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.55%   [EVAL] batch:  213 | acc: 50.00%,  total acc: 66.47%   [EVAL] batch:  214 | acc: 31.25%,  total acc: 66.31%   [EVAL] batch:  215 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 66.13%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 65.85%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 65.67%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  222 | acc: 18.75%,  total acc: 65.25%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 64.98%   [EVAL] batch:  224 | acc: 12.50%,  total acc: 64.75%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  229 | acc: 75.00%,  total acc: 65.14%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 65.05%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  234 | acc: 37.50%,  total acc: 64.84%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 64.66%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 66.01%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  269 | acc: 56.25%,  total acc: 66.25%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 66.12%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 66.04%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  273 | acc: 37.50%,  total acc: 65.78%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  281 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 67.24%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:  292 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 67.39%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:  295 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 66.93%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 66.85%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  300 | acc: 68.75%,  total acc: 66.90%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 67.04%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 67.17%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:  306 | acc: 25.00%,  total acc: 67.10%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 66.96%   [EVAL] batch:  308 | acc: 18.75%,  total acc: 66.81%   [EVAL] batch:  309 | acc: 6.25%,  total acc: 66.61%   [EVAL] batch:  310 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:  311 | acc: 12.50%,  total acc: 66.35%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 66.14%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 65.97%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 65.78%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 65.63%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 65.51%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 65.40%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 65.59%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 65.63%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  325 | acc: 25.00%,  total acc: 65.64%   [EVAL] batch:  326 | acc: 0.00%,  total acc: 65.44%   [EVAL] batch:  327 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:  328 | acc: 0.00%,  total acc: 65.06%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 64.92%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 64.78%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 64.83%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 64.94%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  343 | acc: 81.25%,  total acc: 65.61%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.03%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 66.04%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 65.85%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 65.72%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  361 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 66.17%   [EVAL] batch:  363 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 65.97%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 65.89%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 65.83%   [EVAL] batch:  368 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:  369 | acc: 62.50%,  total acc: 65.69%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  371 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 65.55%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 65.32%   [EVAL] batch:  377 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 65.09%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 65.03%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 64.99%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 65.41%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 65.42%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 65.47%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  394 | acc: 43.75%,  total acc: 65.46%   [EVAL] batch:  395 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:  397 | acc: 62.50%,  total acc: 65.39%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:  399 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 65.35%   [EVAL] batch:  401 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:  403 | acc: 56.25%,  total acc: 65.35%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 65.18%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 65.12%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 65.12%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 65.03%   [EVAL] batch:  413 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  414 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  415 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 64.97%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:  418 | acc: 62.50%,  total acc: 65.01%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 64.93%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 64.80%   [EVAL] batch:  421 | acc: 12.50%,  total acc: 64.68%   [EVAL] batch:  422 | acc: 18.75%,  total acc: 64.57%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 64.46%   [EVAL] batch:  424 | acc: 25.00%,  total acc: 64.37%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 64.95%   [EVAL] batch:  433 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 65.11%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 65.17%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:  441 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  443 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 65.30%   [EVAL] batch:  446 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  448 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 65.53%   [EVAL] batch:  450 | acc: 0.00%,  total acc: 65.38%   [EVAL] batch:  451 | acc: 0.00%,  total acc: 65.24%   [EVAL] batch:  452 | acc: 18.75%,  total acc: 65.14%   [EVAL] batch:  453 | acc: 0.00%,  total acc: 64.99%   [EVAL] batch:  454 | acc: 0.00%,  total acc: 64.85%   [EVAL] batch:  455 | acc: 0.00%,  total acc: 64.71%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 64.72%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  462 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  463 | acc: 6.25%,  total acc: 64.92%   [EVAL] batch:  464 | acc: 0.00%,  total acc: 64.78%   [EVAL] batch:  465 | acc: 12.50%,  total acc: 64.67%   [EVAL] batch:  466 | acc: 12.50%,  total acc: 64.56%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 64.44%   [EVAL] batch:  468 | acc: 25.00%,  total acc: 64.35%   [EVAL] batch:  469 | acc: 93.75%,  total acc: 64.41%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 64.48%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 64.60%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 64.68%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  478 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  479 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  481 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  482 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  483 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  485 | acc: 75.00%,  total acc: 65.02%   [EVAL] batch:  486 | acc: 87.50%,  total acc: 65.07%   [EVAL] batch:  487 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:  488 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  490 | acc: 62.50%,  total acc: 65.07%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:  492 | acc: 62.50%,  total acc: 65.10%   [EVAL] batch:  493 | acc: 62.50%,  total acc: 65.09%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 65.38%   
cur_acc:  ['0.9484', '0.8194', '0.6210', '0.7817', '0.7569', '0.6667', '0.6002', '0.6617']
his_acc:  ['0.9484', '0.8780', '0.7753', '0.7430', '0.7308', '0.6977', '0.6652', '0.6538']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  4  clusters
Clusters:  [0 2 3 0 0 0 0 0 0 1]
Losses:  11.750505447387695 1.883446455001831
CurrentTrain: epoch  0, batch     0 | loss: 13.6339521Losses:  13.44505500793457 2.083732843399048
CurrentTrain: epoch  0, batch     1 | loss: 15.5287876Losses:  13.424779891967773 1.8788721561431885
CurrentTrain: epoch  0, batch     2 | loss: 15.3036518Losses:  14.147151947021484 1.5113201141357422
CurrentTrain: epoch  0, batch     3 | loss: 15.6584721Losses:  13.82172679901123 1.7769490480422974
CurrentTrain: epoch  0, batch     4 | loss: 15.5986757Losses:  13.336402893066406 1.6963768005371094
CurrentTrain: epoch  0, batch     5 | loss: 15.0327797Losses:  13.251510620117188 1.9816447496414185
CurrentTrain: epoch  0, batch     6 | loss: 15.2331553Losses:  12.606642723083496 1.464857578277588
CurrentTrain: epoch  0, batch     7 | loss: 14.0715008Losses:  12.710229873657227 1.483517050743103
CurrentTrain: epoch  0, batch     8 | loss: 14.1937466Losses:  12.535954475402832 1.9171453714370728
CurrentTrain: epoch  0, batch     9 | loss: 14.4531002Losses:  12.427706718444824 1.80112624168396
CurrentTrain: epoch  0, batch    10 | loss: 14.2288332Losses:  12.483304977416992 1.5643447637557983
CurrentTrain: epoch  0, batch    11 | loss: 14.0476494Losses:  11.676076889038086 1.4467766284942627
CurrentTrain: epoch  0, batch    12 | loss: 13.1228533Losses:  11.958768844604492 1.778187870979309
CurrentTrain: epoch  0, batch    13 | loss: 13.7369566Losses:  11.867365837097168 1.4805755615234375
CurrentTrain: epoch  0, batch    14 | loss: 13.3479414Losses:  12.146184921264648 1.7900481224060059
CurrentTrain: epoch  0, batch    15 | loss: 13.9362335Losses:  11.793037414550781 1.8405423164367676
CurrentTrain: epoch  0, batch    16 | loss: 13.6335793Losses:  11.602933883666992 1.7093994617462158
CurrentTrain: epoch  0, batch    17 | loss: 13.3123331Losses:  11.686845779418945 1.8791406154632568
CurrentTrain: epoch  0, batch    18 | loss: 13.5659866Losses:  11.777259826660156 1.6554346084594727
CurrentTrain: epoch  0, batch    19 | loss: 13.4326944Losses:  11.144606590270996 1.326587438583374
CurrentTrain: epoch  0, batch    20 | loss: 12.4711943Losses:  11.301645278930664 1.5554450750350952
CurrentTrain: epoch  0, batch    21 | loss: 12.8570900Losses:  10.893016815185547 1.462087631225586
CurrentTrain: epoch  0, batch    22 | loss: 12.3551044Losses:  10.980966567993164 1.1558208465576172
CurrentTrain: epoch  0, batch    23 | loss: 12.1367874Losses:  10.88125228881836 1.4258437156677246
CurrentTrain: epoch  0, batch    24 | loss: 12.3070965Losses:  10.744757652282715 1.4097362756729126
CurrentTrain: epoch  0, batch    25 | loss: 12.1544943Losses:  10.940603256225586 1.4801135063171387
CurrentTrain: epoch  0, batch    26 | loss: 12.4207172Losses:  10.131721496582031 1.0950279235839844
CurrentTrain: epoch  0, batch    27 | loss: 11.2267494Losses:  9.974258422851562 1.2774943113327026
CurrentTrain: epoch  0, batch    28 | loss: 11.2517529Losses:  9.724161148071289 1.1721370220184326
CurrentTrain: epoch  0, batch    29 | loss: 10.8962984Losses:  10.001635551452637 1.3141603469848633
CurrentTrain: epoch  0, batch    30 | loss: 11.3157959Losses:  9.502354621887207 1.211491346359253
CurrentTrain: epoch  0, batch    31 | loss: 10.7138462Losses:  9.848424911499023 1.3239197731018066
CurrentTrain: epoch  0, batch    32 | loss: 11.1723442Losses:  9.705816268920898 1.2670292854309082
CurrentTrain: epoch  0, batch    33 | loss: 10.9728451Losses:  9.587986946105957 1.4602527618408203
CurrentTrain: epoch  0, batch    34 | loss: 11.0482397Losses:  9.061088562011719 1.0335721969604492
CurrentTrain: epoch  0, batch    35 | loss: 10.0946608Losses:  9.237630844116211 1.1174014806747437
CurrentTrain: epoch  0, batch    36 | loss: 10.3550320Losses:  9.144083023071289 1.2852210998535156
CurrentTrain: epoch  0, batch    37 | loss: 10.4293041Losses:  9.027496337890625 1.1628296375274658
CurrentTrain: epoch  0, batch    38 | loss: 10.1903257Losses:  8.785603523254395 0.9464243054389954
CurrentTrain: epoch  0, batch    39 | loss: 9.7320280Losses:  8.802717208862305 1.1694071292877197
CurrentTrain: epoch  0, batch    40 | loss: 9.9721241Losses:  8.720087051391602 0.8378504514694214
CurrentTrain: epoch  0, batch    41 | loss: 9.5579376Losses:  8.009811401367188 0.9748711585998535
CurrentTrain: epoch  0, batch    42 | loss: 8.9846821Losses:  9.008496284484863 1.1563056707382202
CurrentTrain: epoch  0, batch    43 | loss: 10.1648016Losses:  8.102450370788574 1.3195159435272217
CurrentTrain: epoch  0, batch    44 | loss: 9.4219666Losses:  8.146204948425293 1.2600173950195312
CurrentTrain: epoch  0, batch    45 | loss: 9.4062223Losses:  8.625020980834961 1.25496244430542
CurrentTrain: epoch  0, batch    46 | loss: 9.8799839Losses:  8.030293464660645 0.986137330532074
CurrentTrain: epoch  0, batch    47 | loss: 9.0164309Losses:  7.239686965942383 0.7320135831832886
CurrentTrain: epoch  0, batch    48 | loss: 7.9717007Losses:  7.609379768371582 1.0857700109481812
CurrentTrain: epoch  0, batch    49 | loss: 8.6951494Losses:  7.306190490722656 0.8393034934997559
CurrentTrain: epoch  0, batch    50 | loss: 8.1454945Losses:  7.341623783111572 0.9975601434707642
CurrentTrain: epoch  0, batch    51 | loss: 8.3391838Losses:  7.097373962402344 1.1095285415649414
CurrentTrain: epoch  0, batch    52 | loss: 8.2069025Losses:  7.3157854080200195 0.9094792604446411
CurrentTrain: epoch  0, batch    53 | loss: 8.2252645Losses:  6.715043544769287 0.9402742385864258
CurrentTrain: epoch  0, batch    54 | loss: 7.6553178Losses:  6.720210075378418 1.0572926998138428
CurrentTrain: epoch  0, batch    55 | loss: 7.7775030Losses:  6.879716396331787 1.1876919269561768
CurrentTrain: epoch  0, batch    56 | loss: 8.0674086Losses:  6.89451789855957 1.2200663089752197
CurrentTrain: epoch  0, batch    57 | loss: 8.1145840Losses:  6.429247856140137 0.9293152093887329
CurrentTrain: epoch  0, batch    58 | loss: 7.3585629Losses:  5.9827070236206055 1.024263620376587
CurrentTrain: epoch  0, batch    59 | loss: 7.0069704Losses:  6.098133087158203 1.051980972290039
CurrentTrain: epoch  0, batch    60 | loss: 7.1501141Losses:  6.019794464111328 0.8461506962776184
CurrentTrain: epoch  0, batch    61 | loss: 6.8659453Losses:  5.791085243225098 0.7714923620223999
CurrentTrain: epoch  0, batch    62 | loss: 6.5625777Losses:  5.6688947677612305 0.7921323180198669
CurrentTrain: epoch  1, batch     0 | loss: 6.4610271Losses:  5.6190290451049805 1.058419942855835
CurrentTrain: epoch  1, batch     1 | loss: 6.6774492Losses:  5.851609706878662 0.8187830448150635
CurrentTrain: epoch  1, batch     2 | loss: 6.6703930Losses:  5.686445236206055 0.6834068894386292
CurrentTrain: epoch  1, batch     3 | loss: 6.3698521Losses:  5.904866695404053 1.0490152835845947
CurrentTrain: epoch  1, batch     4 | loss: 6.9538822Losses:  5.698148250579834 0.8678337335586548
CurrentTrain: epoch  1, batch     5 | loss: 6.5659819Losses:  5.850013732910156 0.7300270795822144
CurrentTrain: epoch  1, batch     6 | loss: 6.5800409Losses:  5.754825592041016 0.7252762317657471
CurrentTrain: epoch  1, batch     7 | loss: 6.4801016Losses:  6.056766986846924 0.998245358467102
CurrentTrain: epoch  1, batch     8 | loss: 7.0550122Losses:  5.792128562927246 0.8826814889907837
CurrentTrain: epoch  1, batch     9 | loss: 6.6748099Losses:  5.596473693847656 0.8021658062934875
CurrentTrain: epoch  1, batch    10 | loss: 6.3986397Losses:  5.402587413787842 0.6727184057235718
CurrentTrain: epoch  1, batch    11 | loss: 6.0753059Losses:  5.610784530639648 0.7809292078018188
CurrentTrain: epoch  1, batch    12 | loss: 6.3917136Losses:  5.763838768005371 0.6154065132141113
CurrentTrain: epoch  1, batch    13 | loss: 6.3792453Losses:  5.660135269165039 0.726142168045044
CurrentTrain: epoch  1, batch    14 | loss: 6.3862772Losses:  5.688938617706299 0.6124036908149719
CurrentTrain: epoch  1, batch    15 | loss: 6.3013425Losses:  5.75716495513916 0.7148415446281433
CurrentTrain: epoch  1, batch    16 | loss: 6.4720063Losses:  5.217813968658447 0.4935033321380615
CurrentTrain: epoch  1, batch    17 | loss: 5.7113171Losses:  5.5904717445373535 0.914493203163147
CurrentTrain: epoch  1, batch    18 | loss: 6.5049648Losses:  5.508913993835449 0.823199450969696
CurrentTrain: epoch  1, batch    19 | loss: 6.3321133Losses:  5.5908684730529785 0.770719051361084
CurrentTrain: epoch  1, batch    20 | loss: 6.3615875Losses:  5.172250747680664 0.44729921221733093
CurrentTrain: epoch  1, batch    21 | loss: 5.6195498Losses:  5.372130870819092 0.8434985876083374
CurrentTrain: epoch  1, batch    22 | loss: 6.2156296Losses:  5.449295520782471 0.6207648515701294
CurrentTrain: epoch  1, batch    23 | loss: 6.0700603Losses:  5.466796875 0.8499530553817749
CurrentTrain: epoch  1, batch    24 | loss: 6.3167500Losses:  5.334095001220703 0.6497447490692139
CurrentTrain: epoch  1, batch    25 | loss: 5.9838400Losses:  5.346848487854004 0.5949751734733582
CurrentTrain: epoch  1, batch    26 | loss: 5.9418235Losses:  5.813592910766602 0.6054993271827698
CurrentTrain: epoch  1, batch    27 | loss: 6.4190922Losses:  5.577284812927246 0.8469908237457275
CurrentTrain: epoch  1, batch    28 | loss: 6.4242754Losses:  5.725035190582275 0.7863648533821106
CurrentTrain: epoch  1, batch    29 | loss: 6.5114002Losses:  5.399765968322754 0.6705002784729004
CurrentTrain: epoch  1, batch    30 | loss: 6.0702662Losses:  5.218535900115967 0.45763951539993286
CurrentTrain: epoch  1, batch    31 | loss: 5.6761756Losses:  5.27681827545166 0.606417715549469
CurrentTrain: epoch  1, batch    32 | loss: 5.8832359Losses:  5.4364213943481445 0.7445621490478516
CurrentTrain: epoch  1, batch    33 | loss: 6.1809835Losses:  5.126039028167725 0.5864419937133789
CurrentTrain: epoch  1, batch    34 | loss: 5.7124810Losses:  5.551333427429199 0.7885620594024658
CurrentTrain: epoch  1, batch    35 | loss: 6.3398952Losses:  5.131475925445557 0.5320343971252441
CurrentTrain: epoch  1, batch    36 | loss: 5.6635103Losses:  5.726170539855957 0.6464211940765381
CurrentTrain: epoch  1, batch    37 | loss: 6.3725920Losses:  5.4499921798706055 0.5889275074005127
CurrentTrain: epoch  1, batch    38 | loss: 6.0389194Losses:  5.122199535369873 0.4375506341457367
CurrentTrain: epoch  1, batch    39 | loss: 5.5597501Losses:  5.073695182800293 0.5656476020812988
CurrentTrain: epoch  1, batch    40 | loss: 5.6393428Losses:  5.140692234039307 0.5400660037994385
CurrentTrain: epoch  1, batch    41 | loss: 5.6807585Losses:  5.305603981018066 0.6307446360588074
CurrentTrain: epoch  1, batch    42 | loss: 5.9363484Losses:  5.225161552429199 0.572834849357605
CurrentTrain: epoch  1, batch    43 | loss: 5.7979965Losses:  5.331062316894531 0.5759313702583313
CurrentTrain: epoch  1, batch    44 | loss: 5.9069939Losses:  5.240570068359375 0.4893430471420288
CurrentTrain: epoch  1, batch    45 | loss: 5.7299132Losses:  5.083156108856201 0.6311677098274231
CurrentTrain: epoch  1, batch    46 | loss: 5.7143240Losses:  5.825803279876709 0.7601712942123413
CurrentTrain: epoch  1, batch    47 | loss: 6.5859747Losses:  4.994845390319824 0.5881263017654419
CurrentTrain: epoch  1, batch    48 | loss: 5.5829716Losses:  5.035819053649902 0.5539103150367737
CurrentTrain: epoch  1, batch    49 | loss: 5.5897293Losses:  5.217090129852295 0.5759433507919312
CurrentTrain: epoch  1, batch    50 | loss: 5.7930336Losses:  4.872211933135986 0.4820265769958496
CurrentTrain: epoch  1, batch    51 | loss: 5.3542385Losses:  4.8205366134643555 0.47992902994155884
CurrentTrain: epoch  1, batch    52 | loss: 5.3004656Losses:  4.904038429260254 0.509613037109375
CurrentTrain: epoch  1, batch    53 | loss: 5.4136515Losses:  5.052426338195801 0.6063058376312256
CurrentTrain: epoch  1, batch    54 | loss: 5.6587324Losses:  4.808810234069824 0.5164161920547485
CurrentTrain: epoch  1, batch    55 | loss: 5.3252263Losses:  4.9076385498046875 0.5020323991775513
CurrentTrain: epoch  1, batch    56 | loss: 5.4096708Losses:  4.775191783905029 0.48331591486930847
CurrentTrain: epoch  1, batch    57 | loss: 5.2585077Losses:  5.023092269897461 0.38702067732810974
CurrentTrain: epoch  1, batch    58 | loss: 5.4101129Losses:  5.2084550857543945 0.5438907146453857
CurrentTrain: epoch  1, batch    59 | loss: 5.7523460Losses:  4.467597484588623 0.4561862349510193
CurrentTrain: epoch  1, batch    60 | loss: 4.9237838Losses:  4.630577087402344 0.3567240238189697
CurrentTrain: epoch  1, batch    61 | loss: 4.9873009Losses:  4.415241241455078 0.20227299630641937
CurrentTrain: epoch  1, batch    62 | loss: 4.6175141Losses:  5.062594413757324 0.575753927230835
CurrentTrain: epoch  2, batch     0 | loss: 5.6383486Losses:  4.9326171875 0.5627768039703369
CurrentTrain: epoch  2, batch     1 | loss: 5.4953938Losses:  4.833612442016602 0.41605907678604126
CurrentTrain: epoch  2, batch     2 | loss: 5.2496715Losses:  5.022322177886963 0.4130406975746155
CurrentTrain: epoch  2, batch     3 | loss: 5.4353628Losses:  4.534174919128418 0.4698612689971924
CurrentTrain: epoch  2, batch     4 | loss: 5.0040359Losses:  5.256781578063965 0.3918476104736328
CurrentTrain: epoch  2, batch     5 | loss: 5.6486292Losses:  4.920063018798828 0.4678897261619568
CurrentTrain: epoch  2, batch     6 | loss: 5.3879528Losses:  4.645709037780762 0.44933611154556274
CurrentTrain: epoch  2, batch     7 | loss: 5.0950451Losses:  4.402079105377197 0.2801656424999237
CurrentTrain: epoch  2, batch     8 | loss: 4.6822448Losses:  4.613329887390137 0.28518369793891907
CurrentTrain: epoch  2, batch     9 | loss: 4.8985138Losses:  4.600790500640869 0.30245184898376465
CurrentTrain: epoch  2, batch    10 | loss: 4.9032421Losses:  4.4590582847595215 0.4100286662578583
CurrentTrain: epoch  2, batch    11 | loss: 4.8690867Losses:  4.994494438171387 0.3693169951438904
CurrentTrain: epoch  2, batch    12 | loss: 5.3638115Losses:  4.773983001708984 0.4055575430393219
CurrentTrain: epoch  2, batch    13 | loss: 5.1795406Losses:  4.458614826202393 0.30005893111228943
CurrentTrain: epoch  2, batch    14 | loss: 4.7586737Losses:  4.614801406860352 0.41474634408950806
CurrentTrain: epoch  2, batch    15 | loss: 5.0295477Losses:  4.805007457733154 0.4665336608886719
CurrentTrain: epoch  2, batch    16 | loss: 5.2715411Losses:  4.606544494628906 0.4429171085357666
CurrentTrain: epoch  2, batch    17 | loss: 5.0494614Losses:  4.55166482925415 0.35668885707855225
CurrentTrain: epoch  2, batch    18 | loss: 4.9083538Losses:  4.72882604598999 0.39544668793678284
CurrentTrain: epoch  2, batch    19 | loss: 5.1242728Losses:  5.0360493659973145 0.5117617845535278
CurrentTrain: epoch  2, batch    20 | loss: 5.5478110Losses:  4.579183101654053 0.31709715723991394
CurrentTrain: epoch  2, batch    21 | loss: 4.8962803Losses:  4.765317440032959 0.41871756315231323
CurrentTrain: epoch  2, batch    22 | loss: 5.1840348Losses:  4.737031936645508 0.375666081905365
CurrentTrain: epoch  2, batch    23 | loss: 5.1126981Losses:  4.714688301086426 0.2925230860710144
CurrentTrain: epoch  2, batch    24 | loss: 5.0072112Losses:  4.666190147399902 0.41529595851898193
CurrentTrain: epoch  2, batch    25 | loss: 5.0814862Losses:  4.777292251586914 0.3934098482131958
CurrentTrain: epoch  2, batch    26 | loss: 5.1707020Losses:  4.326298713684082 0.3297685384750366
CurrentTrain: epoch  2, batch    27 | loss: 4.6560674Losses:  4.90199089050293 0.3332911431789398
CurrentTrain: epoch  2, batch    28 | loss: 5.2352819Losses:  4.778289318084717 0.37225276231765747
CurrentTrain: epoch  2, batch    29 | loss: 5.1505423Losses:  4.569046497344971 0.3607056140899658
CurrentTrain: epoch  2, batch    30 | loss: 4.9297523Losses:  4.791486740112305 0.34114354848861694
CurrentTrain: epoch  2, batch    31 | loss: 5.1326303Losses:  4.467296600341797 0.24271084368228912
CurrentTrain: epoch  2, batch    32 | loss: 4.7100077Losses:  4.628446102142334 0.3102899193763733
CurrentTrain: epoch  2, batch    33 | loss: 4.9387360Losses:  4.517359733581543 0.267039954662323
CurrentTrain: epoch  2, batch    34 | loss: 4.7843995Losses:  4.534826278686523 0.350011944770813
CurrentTrain: epoch  2, batch    35 | loss: 4.8848381Losses:  4.782426834106445 0.4472244083881378
CurrentTrain: epoch  2, batch    36 | loss: 5.2296515Losses:  4.470631122589111 0.3698429465293884
CurrentTrain: epoch  2, batch    37 | loss: 4.8404741Losses:  4.52640438079834 0.4018489420413971
CurrentTrain: epoch  2, batch    38 | loss: 4.9282532Losses:  4.403855323791504 0.2651212811470032
CurrentTrain: epoch  2, batch    39 | loss: 4.6689768Losses:  4.533551216125488 0.2999407649040222
CurrentTrain: epoch  2, batch    40 | loss: 4.8334918Losses:  4.438714027404785 0.26823854446411133
CurrentTrain: epoch  2, batch    41 | loss: 4.7069526Losses:  4.615634918212891 0.3988969624042511
CurrentTrain: epoch  2, batch    42 | loss: 5.0145321Losses:  4.500455856323242 0.19777800142765045
CurrentTrain: epoch  2, batch    43 | loss: 4.6982341Losses:  4.676401615142822 0.32704800367355347
CurrentTrain: epoch  2, batch    44 | loss: 5.0034494Losses:  4.62242317199707 0.36879950761795044
CurrentTrain: epoch  2, batch    45 | loss: 4.9912229Losses:  4.728058815002441 0.4355107843875885
CurrentTrain: epoch  2, batch    46 | loss: 5.1635695Losses:  4.377375602722168 0.26136139035224915
CurrentTrain: epoch  2, batch    47 | loss: 4.6387372Losses:  4.375018119812012 0.28319066762924194
CurrentTrain: epoch  2, batch    48 | loss: 4.6582088Losses:  4.481269359588623 0.29291433095932007
CurrentTrain: epoch  2, batch    49 | loss: 4.7741838Losses:  5.189326286315918 0.36606884002685547
CurrentTrain: epoch  2, batch    50 | loss: 5.5553951Losses:  4.843662738800049 0.32265347242355347
CurrentTrain: epoch  2, batch    51 | loss: 5.1663160Losses:  4.36259651184082 0.3571292459964752
CurrentTrain: epoch  2, batch    52 | loss: 4.7197256Losses:  4.616220474243164 0.31657958030700684
CurrentTrain: epoch  2, batch    53 | loss: 4.9328003Losses:  4.541925430297852 0.3064393401145935
CurrentTrain: epoch  2, batch    54 | loss: 4.8483648Losses:  4.338590621948242 0.24837204813957214
CurrentTrain: epoch  2, batch    55 | loss: 4.5869627Losses:  4.796632766723633 0.3224022686481476
CurrentTrain: epoch  2, batch    56 | loss: 5.1190352Losses:  4.447232723236084 0.313751220703125
CurrentTrain: epoch  2, batch    57 | loss: 4.7609839Losses:  4.367255687713623 0.19048216938972473
CurrentTrain: epoch  2, batch    58 | loss: 4.5577378Losses:  4.345712661743164 0.15801414847373962
CurrentTrain: epoch  2, batch    59 | loss: 4.5037270Losses:  4.446037292480469 0.2322942614555359
CurrentTrain: epoch  2, batch    60 | loss: 4.6783314Losses:  4.476258754730225 0.2906780242919922
CurrentTrain: epoch  2, batch    61 | loss: 4.7669368Losses:  4.372559070587158 0.2650151252746582
CurrentTrain: epoch  2, batch    62 | loss: 4.6375742Losses:  4.264011383056641 0.29871684312820435
CurrentTrain: epoch  3, batch     0 | loss: 4.5627284Losses:  4.37515115737915 0.30855104327201843
CurrentTrain: epoch  3, batch     1 | loss: 4.6837020Losses:  4.345142841339111 0.20120960474014282
CurrentTrain: epoch  3, batch     2 | loss: 4.5463524Losses:  4.394797325134277 0.21992555260658264
CurrentTrain: epoch  3, batch     3 | loss: 4.6147227Losses:  4.240560531616211 0.25893810391426086
CurrentTrain: epoch  3, batch     4 | loss: 4.4994988Losses:  4.329065322875977 0.2507072389125824
CurrentTrain: epoch  3, batch     5 | loss: 4.5797725Losses:  4.347785949707031 0.27144289016723633
CurrentTrain: epoch  3, batch     6 | loss: 4.6192288Losses:  4.660021781921387 0.2604343891143799
CurrentTrain: epoch  3, batch     7 | loss: 4.9204559Losses:  4.31256103515625 0.2629733085632324
CurrentTrain: epoch  3, batch     8 | loss: 4.5755343Losses:  4.326791763305664 0.28532874584198
CurrentTrain: epoch  3, batch     9 | loss: 4.6121206Losses:  4.255724906921387 0.2378399670124054
CurrentTrain: epoch  3, batch    10 | loss: 4.4935651Losses:  4.534163475036621 0.3202188313007355
CurrentTrain: epoch  3, batch    11 | loss: 4.8543825Losses:  4.274245262145996 0.23905260860919952
CurrentTrain: epoch  3, batch    12 | loss: 4.5132980Losses:  4.242372512817383 0.2694711983203888
CurrentTrain: epoch  3, batch    13 | loss: 4.5118437Losses:  4.239372253417969 0.22760458290576935
CurrentTrain: epoch  3, batch    14 | loss: 4.4669766Losses:  4.301199436187744 0.1657894402742386
CurrentTrain: epoch  3, batch    15 | loss: 4.4669890Losses:  4.275899887084961 0.21137000620365143
CurrentTrain: epoch  3, batch    16 | loss: 4.4872699Losses:  4.3792572021484375 0.27440693974494934
CurrentTrain: epoch  3, batch    17 | loss: 4.6536641Losses:  4.787286281585693 0.4195019602775574
CurrentTrain: epoch  3, batch    18 | loss: 5.2067881Losses:  4.20309591293335 0.22560158371925354
CurrentTrain: epoch  3, batch    19 | loss: 4.4286976Losses:  4.2636518478393555 0.2737294137477875
CurrentTrain: epoch  3, batch    20 | loss: 4.5373812Losses:  4.247082710266113 0.29763543605804443
CurrentTrain: epoch  3, batch    21 | loss: 4.5447183Losses:  4.18812370300293 0.2806721329689026
CurrentTrain: epoch  3, batch    22 | loss: 4.4687958Losses:  4.181314945220947 0.13454535603523254
CurrentTrain: epoch  3, batch    23 | loss: 4.3158603Losses:  4.354485511779785 0.2557142972946167
CurrentTrain: epoch  3, batch    24 | loss: 4.6101999Losses:  4.081040382385254 0.12784576416015625
CurrentTrain: epoch  3, batch    25 | loss: 4.2088861Losses:  4.195269584655762 0.25161212682724
CurrentTrain: epoch  3, batch    26 | loss: 4.4468818Losses:  4.312530517578125 0.29383790493011475
CurrentTrain: epoch  3, batch    27 | loss: 4.6063685Losses:  4.167087554931641 0.22858306765556335
CurrentTrain: epoch  3, batch    28 | loss: 4.3956704Losses:  4.341815948486328 0.26847973465919495
CurrentTrain: epoch  3, batch    29 | loss: 4.6102958Losses:  4.2810139656066895 0.24035578966140747
CurrentTrain: epoch  3, batch    30 | loss: 4.5213699Losses:  4.176589488983154 0.1878950595855713
CurrentTrain: epoch  3, batch    31 | loss: 4.3644848Losses:  4.393322944641113 0.2549992799758911
CurrentTrain: epoch  3, batch    32 | loss: 4.6483221Losses:  4.2273054122924805 0.19768542051315308
CurrentTrain: epoch  3, batch    33 | loss: 4.4249907Losses:  4.165146827697754 0.24687765538692474
CurrentTrain: epoch  3, batch    34 | loss: 4.4120245Losses:  4.211008071899414 0.12322692573070526
CurrentTrain: epoch  3, batch    35 | loss: 4.3342352Losses:  4.3726959228515625 0.2382185459136963
CurrentTrain: epoch  3, batch    36 | loss: 4.6109142Losses:  4.406108856201172 0.26605284214019775
CurrentTrain: epoch  3, batch    37 | loss: 4.6721616Losses:  4.534659385681152 0.2976624369621277
CurrentTrain: epoch  3, batch    38 | loss: 4.8323216Losses:  4.193360328674316 0.22970780730247498
CurrentTrain: epoch  3, batch    39 | loss: 4.4230680Losses:  4.582887172698975 0.36226314306259155
CurrentTrain: epoch  3, batch    40 | loss: 4.9451504Losses:  4.19028902053833 0.1634221225976944
CurrentTrain: epoch  3, batch    41 | loss: 4.3537111Losses:  4.358870506286621 0.21294647455215454
CurrentTrain: epoch  3, batch    42 | loss: 4.5718169Losses:  4.261890411376953 0.17624393105506897
CurrentTrain: epoch  3, batch    43 | loss: 4.4381342Losses:  4.182689666748047 0.1577971875667572
CurrentTrain: epoch  3, batch    44 | loss: 4.3404870Losses:  4.390218734741211 0.29124873876571655
CurrentTrain: epoch  3, batch    45 | loss: 4.6814675Losses:  4.2113847732543945 0.2041337937116623
CurrentTrain: epoch  3, batch    46 | loss: 4.4155188Losses:  4.3066253662109375 0.21911609172821045
CurrentTrain: epoch  3, batch    47 | loss: 4.5257416Losses:  4.12462043762207 0.21791484951972961
CurrentTrain: epoch  3, batch    48 | loss: 4.3425355Losses:  4.226280212402344 0.21803894639015198
CurrentTrain: epoch  3, batch    49 | loss: 4.4443192Losses:  4.179800033569336 0.18253019452095032
CurrentTrain: epoch  3, batch    50 | loss: 4.3623304Losses:  4.152268409729004 0.19276145100593567
CurrentTrain: epoch  3, batch    51 | loss: 4.3450298Losses:  4.163956642150879 0.23449113965034485
CurrentTrain: epoch  3, batch    52 | loss: 4.3984480Losses:  4.2548723220825195 0.18842530250549316
CurrentTrain: epoch  3, batch    53 | loss: 4.4432974Losses:  4.266815662384033 0.2323111593723297
CurrentTrain: epoch  3, batch    54 | loss: 4.4991269Losses:  4.085859298706055 0.11102291941642761
CurrentTrain: epoch  3, batch    55 | loss: 4.1968822Losses:  4.04590368270874 0.12314958870410919
CurrentTrain: epoch  3, batch    56 | loss: 4.1690531Losses:  4.135843276977539 0.148984894156456
CurrentTrain: epoch  3, batch    57 | loss: 4.2848282Losses:  4.695788383483887 0.2187308967113495
CurrentTrain: epoch  3, batch    58 | loss: 4.9145193Losses:  4.135494232177734 0.1947539746761322
CurrentTrain: epoch  3, batch    59 | loss: 4.3302484Losses:  4.017401695251465 0.18681344389915466
CurrentTrain: epoch  3, batch    60 | loss: 4.2042150Losses:  4.525122165679932 0.287824809551239
CurrentTrain: epoch  3, batch    61 | loss: 4.8129468Losses:  4.191017150878906 0.1873016059398651
CurrentTrain: epoch  3, batch    62 | loss: 4.3783188Losses:  4.2064208984375 0.20369203388690948
CurrentTrain: epoch  4, batch     0 | loss: 4.4101129Losses:  4.068681716918945 0.15215221047401428
CurrentTrain: epoch  4, batch     1 | loss: 4.2208338Losses:  4.122176170349121 0.13807226717472076
CurrentTrain: epoch  4, batch     2 | loss: 4.2602487Losses:  4.211784362792969 0.23195861279964447
CurrentTrain: epoch  4, batch     3 | loss: 4.4437428Losses:  4.143482208251953 0.19667094945907593
CurrentTrain: epoch  4, batch     4 | loss: 4.3401532Losses:  4.074036598205566 0.16145342588424683
CurrentTrain: epoch  4, batch     5 | loss: 4.2354898Losses:  4.185131072998047 0.24324779212474823
CurrentTrain: epoch  4, batch     6 | loss: 4.4283791Losses:  4.145808219909668 0.17621725797653198
CurrentTrain: epoch  4, batch     7 | loss: 4.3220253Losses:  4.130956649780273 0.1761144995689392
CurrentTrain: epoch  4, batch     8 | loss: 4.3070712Losses:  4.245339870452881 0.1981820911169052
CurrentTrain: epoch  4, batch     9 | loss: 4.4435220Losses:  4.064561367034912 0.12454728037118912
CurrentTrain: epoch  4, batch    10 | loss: 4.1891088Losses:  4.103034019470215 0.21957647800445557
CurrentTrain: epoch  4, batch    11 | loss: 4.3226104Losses:  4.34799861907959 0.17788180708885193
CurrentTrain: epoch  4, batch    12 | loss: 4.5258803Losses:  4.154657363891602 0.17292362451553345
CurrentTrain: epoch  4, batch    13 | loss: 4.3275809Losses:  4.169981956481934 0.18996667861938477
CurrentTrain: epoch  4, batch    14 | loss: 4.3599486Losses:  4.465485572814941 0.2227795273065567
CurrentTrain: epoch  4, batch    15 | loss: 4.6882653Losses:  4.255107879638672 0.17047488689422607
CurrentTrain: epoch  4, batch    16 | loss: 4.4255829Losses:  4.09502649307251 0.17768630385398865
CurrentTrain: epoch  4, batch    17 | loss: 4.2727127Losses:  4.3172993659973145 0.2011394500732422
CurrentTrain: epoch  4, batch    18 | loss: 4.5184388Losses:  4.157509803771973 0.2130284309387207
CurrentTrain: epoch  4, batch    19 | loss: 4.3705382Losses:  4.086246013641357 0.15739694237709045
CurrentTrain: epoch  4, batch    20 | loss: 4.2436428Losses:  4.093245506286621 0.12193536013364792
CurrentTrain: epoch  4, batch    21 | loss: 4.2151809Losses:  4.064969539642334 0.11134704947471619
CurrentTrain: epoch  4, batch    22 | loss: 4.1763167Losses:  4.225602149963379 0.16816483438014984
CurrentTrain: epoch  4, batch    23 | loss: 4.3937669Losses:  4.136841297149658 0.19970671832561493
CurrentTrain: epoch  4, batch    24 | loss: 4.3365479Losses:  4.111008167266846 0.1884320080280304
CurrentTrain: epoch  4, batch    25 | loss: 4.2994404Losses:  4.027146339416504 0.11130722612142563
CurrentTrain: epoch  4, batch    26 | loss: 4.1384535Losses:  4.110611438751221 0.18090711534023285
CurrentTrain: epoch  4, batch    27 | loss: 4.2915187Losses:  4.168559551239014 0.2039172351360321
CurrentTrain: epoch  4, batch    28 | loss: 4.3724766Losses:  4.054503917694092 0.13433998823165894
CurrentTrain: epoch  4, batch    29 | loss: 4.1888437Losses:  4.213312149047852 0.14558696746826172
CurrentTrain: epoch  4, batch    30 | loss: 4.3588991Losses:  4.895235538482666 0.37067317962646484
CurrentTrain: epoch  4, batch    31 | loss: 5.2659087Losses:  4.112773418426514 0.18033823370933533
CurrentTrain: epoch  4, batch    32 | loss: 4.2931118Losses:  4.30598258972168 0.17470046877861023
CurrentTrain: epoch  4, batch    33 | loss: 4.4806828Losses:  4.190377235412598 0.1803945004940033
CurrentTrain: epoch  4, batch    34 | loss: 4.3707719Losses:  4.101120948791504 0.12548859417438507
CurrentTrain: epoch  4, batch    35 | loss: 4.2266097Losses:  4.063078880310059 0.20631910860538483
CurrentTrain: epoch  4, batch    36 | loss: 4.2693982Losses:  4.0479536056518555 0.14852765202522278
CurrentTrain: epoch  4, batch    37 | loss: 4.1964812Losses:  4.056190490722656 0.1725291609764099
CurrentTrain: epoch  4, batch    38 | loss: 4.2287197Losses:  4.042618751525879 0.1931198239326477
CurrentTrain: epoch  4, batch    39 | loss: 4.2357388Losses:  4.010976791381836 0.16285717487335205
CurrentTrain: epoch  4, batch    40 | loss: 4.1738338Losses:  4.027984619140625 0.11602933704853058
CurrentTrain: epoch  4, batch    41 | loss: 4.1440139Losses:  4.1245222091674805 0.16279913485050201
CurrentTrain: epoch  4, batch    42 | loss: 4.2873216Losses:  3.998277187347412 0.07497675716876984
CurrentTrain: epoch  4, batch    43 | loss: 4.0732541Losses:  4.073319435119629 0.15407271683216095
CurrentTrain: epoch  4, batch    44 | loss: 4.2273922Losses:  4.076488494873047 0.14888674020767212
CurrentTrain: epoch  4, batch    45 | loss: 4.2253752Losses:  4.068833351135254 0.13192883133888245
CurrentTrain: epoch  4, batch    46 | loss: 4.2007623Losses:  3.996556520462036 0.1435311734676361
CurrentTrain: epoch  4, batch    47 | loss: 4.1400876Losses:  4.078778266906738 0.15558265149593353
CurrentTrain: epoch  4, batch    48 | loss: 4.2343607Losses:  4.048684120178223 0.13391631841659546
CurrentTrain: epoch  4, batch    49 | loss: 4.1826005Losses:  4.068391799926758 0.14761583507061005
CurrentTrain: epoch  4, batch    50 | loss: 4.2160077Losses:  4.034855365753174 0.10634120553731918
CurrentTrain: epoch  4, batch    51 | loss: 4.1411967Losses:  4.286749839782715 0.1832801103591919
CurrentTrain: epoch  4, batch    52 | loss: 4.4700298Losses:  4.046380996704102 0.08099392801523209
CurrentTrain: epoch  4, batch    53 | loss: 4.1273751Losses:  3.9686803817749023 0.12167908251285553
CurrentTrain: epoch  4, batch    54 | loss: 4.0903597Losses:  4.095455169677734 0.14961451292037964
CurrentTrain: epoch  4, batch    55 | loss: 4.2450695Losses:  4.050516128540039 0.12959891557693481
CurrentTrain: epoch  4, batch    56 | loss: 4.1801152Losses:  4.051462173461914 0.13456447422504425
CurrentTrain: epoch  4, batch    57 | loss: 4.1860266Losses:  4.111136436462402 0.14025522768497467
CurrentTrain: epoch  4, batch    58 | loss: 4.2513919Losses:  4.183140277862549 0.11014057695865631
CurrentTrain: epoch  4, batch    59 | loss: 4.2932811Losses:  4.1325459480285645 0.10615531355142593
CurrentTrain: epoch  4, batch    60 | loss: 4.2387013Losses:  4.032566547393799 0.12365885823965073
CurrentTrain: epoch  4, batch    61 | loss: 4.1562252Losses:  4.047357559204102 0.08559836447238922
CurrentTrain: epoch  4, batch    62 | loss: 4.1329560Losses:  4.191902160644531 0.16353440284729004
CurrentTrain: epoch  5, batch     0 | loss: 4.3554363Losses:  3.999429702758789 0.09949159622192383
CurrentTrain: epoch  5, batch     1 | loss: 4.0989213Losses:  3.9848687648773193 0.12263902276754379
CurrentTrain: epoch  5, batch     2 | loss: 4.1075077Losses:  4.105544090270996 0.12419643998146057
CurrentTrain: epoch  5, batch     3 | loss: 4.2297406Losses:  4.06043815612793 0.16074176132678986
CurrentTrain: epoch  5, batch     4 | loss: 4.2211800Losses:  3.999021530151367 0.11268937587738037
CurrentTrain: epoch  5, batch     5 | loss: 4.1117110Losses:  3.9890341758728027 0.13574576377868652
CurrentTrain: epoch  5, batch     6 | loss: 4.1247797Losses:  4.044693946838379 0.0683695450425148
CurrentTrain: epoch  5, batch     7 | loss: 4.1130633Losses:  3.988774538040161 0.1787845939397812
CurrentTrain: epoch  5, batch     8 | loss: 4.1675591Losses:  4.017072677612305 0.07846374809741974
CurrentTrain: epoch  5, batch     9 | loss: 4.0955362Losses:  4.030792236328125 0.11007058620452881
CurrentTrain: epoch  5, batch    10 | loss: 4.1408629Losses:  4.152514457702637 0.11515115201473236
CurrentTrain: epoch  5, batch    11 | loss: 4.2676654Losses:  4.086676120758057 0.10263709723949432
CurrentTrain: epoch  5, batch    12 | loss: 4.1893134Losses:  4.147809028625488 0.13726668059825897
CurrentTrain: epoch  5, batch    13 | loss: 4.2850757Losses:  4.013360977172852 0.11858144402503967
CurrentTrain: epoch  5, batch    14 | loss: 4.1319423Losses:  4.062780380249023 0.16505885124206543
CurrentTrain: epoch  5, batch    15 | loss: 4.2278395Losses:  4.0013322830200195 0.15916161239147186
CurrentTrain: epoch  5, batch    16 | loss: 4.1604939Losses:  4.106614112854004 0.14429731667041779
CurrentTrain: epoch  5, batch    17 | loss: 4.2509112Losses:  3.998028039932251 0.14223173260688782
CurrentTrain: epoch  5, batch    18 | loss: 4.1402597Losses:  4.005226135253906 0.11306701600551605
CurrentTrain: epoch  5, batch    19 | loss: 4.1182933Losses:  4.097036361694336 0.11161477118730545
CurrentTrain: epoch  5, batch    20 | loss: 4.2086511Losses:  4.4217119216918945 0.2552942633628845
CurrentTrain: epoch  5, batch    21 | loss: 4.6770062Losses:  3.9962284564971924 0.09148390591144562
CurrentTrain: epoch  5, batch    22 | loss: 4.0877123Losses:  4.019015789031982 0.1541881263256073
CurrentTrain: epoch  5, batch    23 | loss: 4.1732039Losses:  4.092343807220459 0.12585990130901337
CurrentTrain: epoch  5, batch    24 | loss: 4.2182035Losses:  4.029233455657959 0.16218188405036926
CurrentTrain: epoch  5, batch    25 | loss: 4.1914153Losses:  4.013204097747803 0.14738692343235016
CurrentTrain: epoch  5, batch    26 | loss: 4.1605911Losses:  4.002041816711426 0.15624751150608063
CurrentTrain: epoch  5, batch    27 | loss: 4.1582894Losses:  3.9864766597747803 0.10720117390155792
CurrentTrain: epoch  5, batch    28 | loss: 4.0936780Losses:  4.010106086730957 0.1263200044631958
CurrentTrain: epoch  5, batch    29 | loss: 4.1364260Losses:  4.002583980560303 0.09669511020183563
CurrentTrain: epoch  5, batch    30 | loss: 4.0992789Losses:  4.143655776977539 0.1495395004749298
CurrentTrain: epoch  5, batch    31 | loss: 4.2931952Losses:  4.0085577964782715 0.15841877460479736
CurrentTrain: epoch  5, batch    32 | loss: 4.1669765Losses:  4.121761322021484 0.1793009638786316
CurrentTrain: epoch  5, batch    33 | loss: 4.3010621Losses:  4.077038764953613 0.11702775955200195
CurrentTrain: epoch  5, batch    34 | loss: 4.1940665Losses:  4.018445014953613 0.08408688753843307
CurrentTrain: epoch  5, batch    35 | loss: 4.1025319Losses:  4.0156731605529785 0.11760623753070831
CurrentTrain: epoch  5, batch    36 | loss: 4.1332793Losses:  3.9752750396728516 0.12748950719833374
CurrentTrain: epoch  5, batch    37 | loss: 4.1027646Losses:  4.109833240509033 0.14400190114974976
CurrentTrain: epoch  5, batch    38 | loss: 4.2538352Losses:  4.032320499420166 0.15082469582557678
CurrentTrain: epoch  5, batch    39 | loss: 4.1831450Losses:  3.969759941101074 0.0967600867152214
CurrentTrain: epoch  5, batch    40 | loss: 4.0665202Losses:  3.983273506164551 0.07097664475440979
CurrentTrain: epoch  5, batch    41 | loss: 4.0542502Losses:  4.006906032562256 0.11756323277950287
CurrentTrain: epoch  5, batch    42 | loss: 4.1244693Losses:  4.051408767700195 0.10471120476722717
CurrentTrain: epoch  5, batch    43 | loss: 4.1561198Losses:  4.015999794006348 0.08939141780138016
CurrentTrain: epoch  5, batch    44 | loss: 4.1053910Losses:  4.0014262199401855 0.1350577026605606
CurrentTrain: epoch  5, batch    45 | loss: 4.1364841Losses:  3.9834256172180176 0.1333046704530716
CurrentTrain: epoch  5, batch    46 | loss: 4.1167302Losses:  3.9320473670959473 0.08489175140857697
CurrentTrain: epoch  5, batch    47 | loss: 4.0169392Losses:  4.043603897094727 0.12415628135204315
CurrentTrain: epoch  5, batch    48 | loss: 4.1677604Losses:  4.002530097961426 0.1344570517539978
CurrentTrain: epoch  5, batch    49 | loss: 4.1369872Losses:  3.9773523807525635 0.11827422678470612
CurrentTrain: epoch  5, batch    50 | loss: 4.0956268Losses:  3.9414403438568115 0.1014762818813324
CurrentTrain: epoch  5, batch    51 | loss: 4.0429168Losses:  4.054737091064453 0.15471023321151733
CurrentTrain: epoch  5, batch    52 | loss: 4.2094474Losses:  4.0159525871276855 0.11020439863204956
CurrentTrain: epoch  5, batch    53 | loss: 4.1261568Losses:  4.039764881134033 0.1404525339603424
CurrentTrain: epoch  5, batch    54 | loss: 4.1802173Losses:  4.019672393798828 0.12168459594249725
CurrentTrain: epoch  5, batch    55 | loss: 4.1413569Losses:  4.007493019104004 0.09832547605037689
CurrentTrain: epoch  5, batch    56 | loss: 4.1058183Losses:  3.9845378398895264 0.16382120549678802
CurrentTrain: epoch  5, batch    57 | loss: 4.1483588Losses:  4.027645587921143 0.1376340687274933
CurrentTrain: epoch  5, batch    58 | loss: 4.1652799Losses:  3.9455318450927734 0.09865036606788635
CurrentTrain: epoch  5, batch    59 | loss: 4.0441823Losses:  4.002098560333252 0.08010236918926239
CurrentTrain: epoch  5, batch    60 | loss: 4.0822010Losses:  3.92313289642334 0.10988812148571014
CurrentTrain: epoch  5, batch    61 | loss: 4.0330210Losses:  4.019258975982666 0.08018326759338379
CurrentTrain: epoch  5, batch    62 | loss: 4.0994425Losses:  3.9796178340911865 0.13219475746154785
CurrentTrain: epoch  6, batch     0 | loss: 4.1118126Losses:  3.985668659210205 0.15298572182655334
CurrentTrain: epoch  6, batch     1 | loss: 4.1386542Losses:  3.9128530025482178 0.060917019844055176
CurrentTrain: epoch  6, batch     2 | loss: 3.9737701Losses:  3.9956040382385254 0.12525677680969238
CurrentTrain: epoch  6, batch     3 | loss: 4.1208611Losses:  4.001605987548828 0.13325101137161255
CurrentTrain: epoch  6, batch     4 | loss: 4.1348572Losses:  3.9403176307678223 0.07514132559299469
CurrentTrain: epoch  6, batch     5 | loss: 4.0154591Losses:  4.051174640655518 0.09739690274000168
CurrentTrain: epoch  6, batch     6 | loss: 4.1485715Losses:  3.9869611263275146 0.10828989744186401
CurrentTrain: epoch  6, batch     7 | loss: 4.0952511Losses:  4.0268754959106445 0.11513790488243103
CurrentTrain: epoch  6, batch     8 | loss: 4.1420135Losses:  3.9745500087738037 0.11580047011375427
CurrentTrain: epoch  6, batch     9 | loss: 4.0903506Losses:  3.978811740875244 0.09269331395626068
CurrentTrain: epoch  6, batch    10 | loss: 4.0715051Losses:  4.008472442626953 0.1355236917734146
CurrentTrain: epoch  6, batch    11 | loss: 4.1439962Losses:  3.9888088703155518 0.10733689367771149
CurrentTrain: epoch  6, batch    12 | loss: 4.0961456Losses:  3.9783711433410645 0.1179826483130455
CurrentTrain: epoch  6, batch    13 | loss: 4.0963540Losses:  3.9667835235595703 0.10593955218791962
CurrentTrain: epoch  6, batch    14 | loss: 4.0727229Losses:  4.022373676300049 0.07221050560474396
CurrentTrain: epoch  6, batch    15 | loss: 4.0945840Losses:  4.00359582901001 0.1164289116859436
CurrentTrain: epoch  6, batch    16 | loss: 4.1200247Losses:  3.987788677215576 0.11650989949703217
CurrentTrain: epoch  6, batch    17 | loss: 4.1042986Losses:  4.00587272644043 0.11437089741230011
CurrentTrain: epoch  6, batch    18 | loss: 4.1202435Losses:  3.978642463684082 0.12413571774959564
CurrentTrain: epoch  6, batch    19 | loss: 4.1027780Losses:  4.001772880554199 0.11622124910354614
CurrentTrain: epoch  6, batch    20 | loss: 4.1179943Losses:  4.00774621963501 0.12788435816764832
CurrentTrain: epoch  6, batch    21 | loss: 4.1356306Losses:  4.005197525024414 0.13307760655879974
CurrentTrain: epoch  6, batch    22 | loss: 4.1382751Losses:  4.024371147155762 0.09946757555007935
CurrentTrain: epoch  6, batch    23 | loss: 4.1238389Losses:  3.994548797607422 0.0889892429113388
CurrentTrain: epoch  6, batch    24 | loss: 4.0835381Losses:  3.994875907897949 0.09310603141784668
CurrentTrain: epoch  6, batch    25 | loss: 4.0879822Losses:  3.9762814044952393 0.12352433800697327
CurrentTrain: epoch  6, batch    26 | loss: 4.0998058Losses:  3.976821184158325 0.10764613002538681
CurrentTrain: epoch  6, batch    27 | loss: 4.0844674Losses:  4.026659965515137 0.11902859061956406
CurrentTrain: epoch  6, batch    28 | loss: 4.1456885Losses:  4.001868724822998 0.08421401679515839
CurrentTrain: epoch  6, batch    29 | loss: 4.0860829Losses:  4.027398586273193 0.13620948791503906
CurrentTrain: epoch  6, batch    30 | loss: 4.1636081Losses:  3.991508960723877 0.09319797903299332
CurrentTrain: epoch  6, batch    31 | loss: 4.0847068Losses:  3.988096237182617 0.11762439459562302
CurrentTrain: epoch  6, batch    32 | loss: 4.1057205Losses:  3.993781805038452 0.10977281630039215
CurrentTrain: epoch  6, batch    33 | loss: 4.1035547Losses:  4.02603006362915 0.07979346066713333
CurrentTrain: epoch  6, batch    34 | loss: 4.1058235Losses:  3.976161003112793 0.09965559095144272
CurrentTrain: epoch  6, batch    35 | loss: 4.0758166Losses:  4.004880428314209 0.09317577630281448
CurrentTrain: epoch  6, batch    36 | loss: 4.0980563Losses:  3.936234474182129 0.14130984246730804
CurrentTrain: epoch  6, batch    37 | loss: 4.0775442Losses:  3.992602586746216 0.08407297730445862
CurrentTrain: epoch  6, batch    38 | loss: 4.0766754Losses:  3.96340012550354 0.11363227665424347
CurrentTrain: epoch  6, batch    39 | loss: 4.0770326Losses:  3.9636926651000977 0.08838415145874023
CurrentTrain: epoch  6, batch    40 | loss: 4.0520768Losses:  3.9857029914855957 0.1442202627658844
CurrentTrain: epoch  6, batch    41 | loss: 4.1299233Losses:  3.966841220855713 0.1430322527885437
CurrentTrain: epoch  6, batch    42 | loss: 4.1098733Losses:  3.968679904937744 0.12068445980548859
CurrentTrain: epoch  6, batch    43 | loss: 4.0893645Losses:  3.98833966255188 0.08640053868293762
CurrentTrain: epoch  6, batch    44 | loss: 4.0747404Losses:  3.9488635063171387 0.09611006081104279
CurrentTrain: epoch  6, batch    45 | loss: 4.0449734Losses:  3.9964704513549805 0.09476338326931
CurrentTrain: epoch  6, batch    46 | loss: 4.0912337Losses:  3.9672017097473145 0.07537385076284409
CurrentTrain: epoch  6, batch    47 | loss: 4.0425754Losses:  3.9652822017669678 0.11678700894117355
CurrentTrain: epoch  6, batch    48 | loss: 4.0820694Losses:  3.9921183586120605 0.11865890026092529
CurrentTrain: epoch  6, batch    49 | loss: 4.1107774Losses:  3.998814105987549 0.11002252995967865
CurrentTrain: epoch  6, batch    50 | loss: 4.1088367Losses:  3.939645528793335 0.10137279331684113
CurrentTrain: epoch  6, batch    51 | loss: 4.0410185Losses:  3.960519313812256 0.06879815459251404
CurrentTrain: epoch  6, batch    52 | loss: 4.0293174Losses:  3.9833855628967285 0.09813608229160309
CurrentTrain: epoch  6, batch    53 | loss: 4.0815215Losses:  3.957338809967041 0.07771715521812439
CurrentTrain: epoch  6, batch    54 | loss: 4.0350561Losses:  3.9626736640930176 0.10263097286224365
CurrentTrain: epoch  6, batch    55 | loss: 4.0653048Losses:  3.9964210987091064 0.10314317047595978
CurrentTrain: epoch  6, batch    56 | loss: 4.0995641Losses:  3.9510996341705322 0.08456364274024963
CurrentTrain: epoch  6, batch    57 | loss: 4.0356631Losses:  3.9537529945373535 0.1194077581167221
CurrentTrain: epoch  6, batch    58 | loss: 4.0731606Losses:  3.927403450012207 0.11427398025989532
CurrentTrain: epoch  6, batch    59 | loss: 4.0416775Losses:  3.926086902618408 0.07493297755718231
CurrentTrain: epoch  6, batch    60 | loss: 4.0010200Losses:  3.97749924659729 0.12691619992256165
CurrentTrain: epoch  6, batch    61 | loss: 4.1044154Losses:  3.98696231842041 0.05852188169956207
CurrentTrain: epoch  6, batch    62 | loss: 4.0454841Losses:  3.959804058074951 0.1254759281873703
CurrentTrain: epoch  7, batch     0 | loss: 4.0852799Losses:  3.972269058227539 0.1155199408531189
CurrentTrain: epoch  7, batch     1 | loss: 4.0877891Losses:  3.9787139892578125 0.12008707225322723
CurrentTrain: epoch  7, batch     2 | loss: 4.0988011Losses:  3.9685745239257812 0.09968559443950653
CurrentTrain: epoch  7, batch     3 | loss: 4.0682602Losses:  3.9504904747009277 0.10278618335723877
CurrentTrain: epoch  7, batch     4 | loss: 4.0532765Losses:  3.9840168952941895 0.08656235039234161
CurrentTrain: epoch  7, batch     5 | loss: 4.0705791Losses:  3.960848331451416 0.059598762542009354
CurrentTrain: epoch  7, batch     6 | loss: 4.0204473Losses:  3.9552488327026367 0.0742497369647026
CurrentTrain: epoch  7, batch     7 | loss: 4.0294986Losses:  3.9357213973999023 0.09420964121818542
CurrentTrain: epoch  7, batch     8 | loss: 4.0299311Losses:  3.954102039337158 0.10890789330005646
CurrentTrain: epoch  7, batch     9 | loss: 4.0630097Losses:  3.9564685821533203 0.10680526494979858
CurrentTrain: epoch  7, batch    10 | loss: 4.0632739Losses:  3.9687955379486084 0.11249247193336487
CurrentTrain: epoch  7, batch    11 | loss: 4.0812879Losses:  3.971339702606201 0.05599302425980568
CurrentTrain: epoch  7, batch    12 | loss: 4.0273328Losses:  3.9260449409484863 0.10544022917747498
CurrentTrain: epoch  7, batch    13 | loss: 4.0314851Losses:  3.961254596710205 0.11536708474159241
CurrentTrain: epoch  7, batch    14 | loss: 4.0766215Losses:  3.933866262435913 0.06238939240574837
CurrentTrain: epoch  7, batch    15 | loss: 3.9962556Losses:  3.9701383113861084 0.0806770771741867
CurrentTrain: epoch  7, batch    16 | loss: 4.0508156Losses:  3.9891066551208496 0.12576784193515778
CurrentTrain: epoch  7, batch    17 | loss: 4.1148744Losses:  3.987562417984009 0.05497929826378822
CurrentTrain: epoch  7, batch    18 | loss: 4.0425415Losses:  3.9595394134521484 0.12771865725517273
CurrentTrain: epoch  7, batch    19 | loss: 4.0872579Losses:  4.002943992614746 0.10013103485107422
CurrentTrain: epoch  7, batch    20 | loss: 4.1030750Losses:  3.9394211769104004 0.08890733867883682
CurrentTrain: epoch  7, batch    21 | loss: 4.0283284Losses:  3.9098634719848633 0.11068273335695267
CurrentTrain: epoch  7, batch    22 | loss: 4.0205464Losses:  3.9526658058166504 0.08437328785657883
CurrentTrain: epoch  7, batch    23 | loss: 4.0370393Losses:  3.949465274810791 0.0986337959766388
CurrentTrain: epoch  7, batch    24 | loss: 4.0480990Losses:  3.9466686248779297 0.08301892131567001
CurrentTrain: epoch  7, batch    25 | loss: 4.0296874Losses:  3.9840574264526367 0.09487462788820267
CurrentTrain: epoch  7, batch    26 | loss: 4.0789323Losses:  3.9617323875427246 0.07135747373104095
CurrentTrain: epoch  7, batch    27 | loss: 4.0330896Losses:  3.9595894813537598 0.11154511570930481
CurrentTrain: epoch  7, batch    28 | loss: 4.0711346Losses:  4.008710861206055 0.07622022181749344
CurrentTrain: epoch  7, batch    29 | loss: 4.0849309Losses:  3.9426827430725098 0.05971521511673927
CurrentTrain: epoch  7, batch    30 | loss: 4.0023980Losses:  3.973914623260498 0.10567253828048706
CurrentTrain: epoch  7, batch    31 | loss: 4.0795870Losses:  3.9576611518859863 0.09771531820297241
CurrentTrain: epoch  7, batch    32 | loss: 4.0553765Losses:  3.9383935928344727 0.09788960218429565
CurrentTrain: epoch  7, batch    33 | loss: 4.0362830Losses:  3.9739184379577637 0.09691137820482254
CurrentTrain: epoch  7, batch    34 | loss: 4.0708299Losses:  3.951320171356201 0.08889399468898773
CurrentTrain: epoch  7, batch    35 | loss: 4.0402141Losses:  3.961005449295044 0.050133444368839264
CurrentTrain: epoch  7, batch    36 | loss: 4.0111389Losses:  3.9717330932617188 0.09986647963523865
CurrentTrain: epoch  7, batch    37 | loss: 4.0715995Losses:  3.951411724090576 0.10187596082687378
CurrentTrain: epoch  7, batch    38 | loss: 4.0532875Losses:  3.934420585632324 0.11020982265472412
CurrentTrain: epoch  7, batch    39 | loss: 4.0446305Losses:  3.920816659927368 0.11358055472373962
CurrentTrain: epoch  7, batch    40 | loss: 4.0343971Losses:  3.9439754486083984 0.09114691615104675
CurrentTrain: epoch  7, batch    41 | loss: 4.0351224Losses:  3.9759068489074707 0.08602505177259445
CurrentTrain: epoch  7, batch    42 | loss: 4.0619321Losses:  3.945173740386963 0.07295361161231995
CurrentTrain: epoch  7, batch    43 | loss: 4.0181274Losses:  3.9435536861419678 0.10183944553136826
CurrentTrain: epoch  7, batch    44 | loss: 4.0453930Losses:  3.9363579750061035 0.06753164529800415
CurrentTrain: epoch  7, batch    45 | loss: 4.0038896Losses:  3.947413444519043 0.06553716957569122
CurrentTrain: epoch  7, batch    46 | loss: 4.0129504Losses:  3.9743332862854004 0.10927961766719818
CurrentTrain: epoch  7, batch    47 | loss: 4.0836129Losses:  3.967705726623535 0.0950794368982315
CurrentTrain: epoch  7, batch    48 | loss: 4.0627851Losses:  3.923704147338867 0.07614851742982864
CurrentTrain: epoch  7, batch    49 | loss: 3.9998527Losses:  4.030758857727051 0.08600132167339325
CurrentTrain: epoch  7, batch    50 | loss: 4.1167603Losses:  3.912522792816162 0.07334751635789871
CurrentTrain: epoch  7, batch    51 | loss: 3.9858704Losses:  3.916855812072754 0.06214476376771927
CurrentTrain: epoch  7, batch    52 | loss: 3.9790006Losses:  3.882869243621826 0.07306042313575745
CurrentTrain: epoch  7, batch    53 | loss: 3.9559298Losses:  4.009024143218994 0.05650720000267029
CurrentTrain: epoch  7, batch    54 | loss: 4.0655313Losses:  4.038447380065918 0.08650077134370804
CurrentTrain: epoch  7, batch    55 | loss: 4.1249480Losses:  3.963214159011841 0.06396166235208511
CurrentTrain: epoch  7, batch    56 | loss: 4.0271759Losses:  3.962576150894165 0.07158161699771881
CurrentTrain: epoch  7, batch    57 | loss: 4.0341578Losses:  3.9684195518493652 0.09508548676967621
CurrentTrain: epoch  7, batch    58 | loss: 4.0635052Losses:  3.9197354316711426 0.10168643295764923
CurrentTrain: epoch  7, batch    59 | loss: 4.0214219Losses:  3.971811294555664 0.1227821558713913
CurrentTrain: epoch  7, batch    60 | loss: 4.0945935Losses:  3.943875789642334 0.07596316188573837
CurrentTrain: epoch  7, batch    61 | loss: 4.0198388Losses:  3.907060384750366 0.03825840726494789
CurrentTrain: epoch  7, batch    62 | loss: 3.9453187Losses:  3.950258255004883 0.0874834954738617
CurrentTrain: epoch  8, batch     0 | loss: 4.0377417Losses:  3.961040735244751 0.07128562033176422
CurrentTrain: epoch  8, batch     1 | loss: 4.0323262Losses:  3.949375629425049 0.0848199725151062
CurrentTrain: epoch  8, batch     2 | loss: 4.0341954Losses:  3.9540903568267822 0.11536446213722229
CurrentTrain: epoch  8, batch     3 | loss: 4.0694547Losses:  3.964221954345703 0.07784789055585861
CurrentTrain: epoch  8, batch     4 | loss: 4.0420699Losses:  3.988773822784424 0.09243075549602509
CurrentTrain: epoch  8, batch     5 | loss: 4.0812044Losses:  3.9600586891174316 0.10413622111082077
CurrentTrain: epoch  8, batch     6 | loss: 4.0641947Losses:  3.965839385986328 0.0951957106590271
CurrentTrain: epoch  8, batch     7 | loss: 4.0610352Losses:  3.9113659858703613 0.08561447262763977
CurrentTrain: epoch  8, batch     8 | loss: 3.9969804Losses:  4.001713752746582 0.08411269634962082
CurrentTrain: epoch  8, batch     9 | loss: 4.0858264Losses:  3.9893696308135986 0.09300290048122406
CurrentTrain: epoch  8, batch    10 | loss: 4.0823727Losses:  3.9397575855255127 0.08536967635154724
CurrentTrain: epoch  8, batch    11 | loss: 4.0251274Losses:  3.8796305656433105 0.06138169765472412
CurrentTrain: epoch  8, batch    12 | loss: 3.9410124Losses:  3.97161602973938 0.10509862750768661
CurrentTrain: epoch  8, batch    13 | loss: 4.0767145Losses:  3.8802528381347656 0.06415338069200516
CurrentTrain: epoch  8, batch    14 | loss: 3.9444063Losses:  3.944582939147949 0.09655272960662842
CurrentTrain: epoch  8, batch    15 | loss: 4.0411358Losses:  3.9358129501342773 0.09887787699699402
CurrentTrain: epoch  8, batch    16 | loss: 4.0346909Losses:  3.9039244651794434 0.06570778042078018
CurrentTrain: epoch  8, batch    17 | loss: 3.9696321Losses:  3.942497730255127 0.08455934375524521
CurrentTrain: epoch  8, batch    18 | loss: 4.0270572Losses:  3.9302515983581543 0.09981346130371094
CurrentTrain: epoch  8, batch    19 | loss: 4.0300651Losses:  3.959733009338379 0.07805362343788147
CurrentTrain: epoch  8, batch    20 | loss: 4.0377865Losses:  3.969406843185425 0.08216339349746704
CurrentTrain: epoch  8, batch    21 | loss: 4.0515704Losses:  3.9393019676208496 0.06978143751621246
CurrentTrain: epoch  8, batch    22 | loss: 4.0090833Losses:  3.9468648433685303 0.06593354791402817
CurrentTrain: epoch  8, batch    23 | loss: 4.0127983Losses:  3.9760844707489014 0.06964422017335892
CurrentTrain: epoch  8, batch    24 | loss: 4.0457287Losses:  3.9592080116271973 0.05624952167272568
CurrentTrain: epoch  8, batch    25 | loss: 4.0154576Losses:  4.0100016593933105 0.04949893057346344
CurrentTrain: epoch  8, batch    26 | loss: 4.0595007Losses:  3.910153388977051 0.06657347083091736
CurrentTrain: epoch  8, batch    27 | loss: 3.9767268Losses:  3.9675025939941406 0.062320295721292496
CurrentTrain: epoch  8, batch    28 | loss: 4.0298228Losses:  3.907533884048462 0.09436971694231033
CurrentTrain: epoch  8, batch    29 | loss: 4.0019035Losses:  3.951535940170288 0.08880969882011414
CurrentTrain: epoch  8, batch    30 | loss: 4.0403457Losses:  3.940532922744751 0.09006989747285843
CurrentTrain: epoch  8, batch    31 | loss: 4.0306029Losses:  3.92624568939209 0.08918645977973938
CurrentTrain: epoch  8, batch    32 | loss: 4.0154324Losses:  3.9294605255126953 0.0780300498008728
CurrentTrain: epoch  8, batch    33 | loss: 4.0074906Losses:  3.9191107749938965 0.07323169708251953
CurrentTrain: epoch  8, batch    34 | loss: 3.9923425Losses:  3.966005563735962 0.06393730640411377
CurrentTrain: epoch  8, batch    35 | loss: 4.0299430Losses:  3.9849119186401367 0.10153809189796448
CurrentTrain: epoch  8, batch    36 | loss: 4.0864501Losses:  3.9370973110198975 0.0969911739230156
CurrentTrain: epoch  8, batch    37 | loss: 4.0340886Losses:  3.960725784301758 0.07891970872879028
CurrentTrain: epoch  8, batch    38 | loss: 4.0396457Losses:  3.9363629817962646 0.0560651496052742
CurrentTrain: epoch  8, batch    39 | loss: 3.9924281Losses:  3.979918956756592 0.07455898076295853
CurrentTrain: epoch  8, batch    40 | loss: 4.0544782Losses:  3.9063241481781006 0.09972377121448517
CurrentTrain: epoch  8, batch    41 | loss: 4.0060477Losses:  3.935241937637329 0.08319106698036194
CurrentTrain: epoch  8, batch    42 | loss: 4.0184331Losses:  3.9595534801483154 0.08028436452150345
CurrentTrain: epoch  8, batch    43 | loss: 4.0398378Losses:  3.964576244354248 0.04933948069810867
CurrentTrain: epoch  8, batch    44 | loss: 4.0139155Losses:  4.009199142456055 0.06169570982456207
CurrentTrain: epoch  8, batch    45 | loss: 4.0708947Losses:  3.9817798137664795 0.07345764338970184
CurrentTrain: epoch  8, batch    46 | loss: 4.0552373Losses:  3.949784994125366 0.06625942140817642
CurrentTrain: epoch  8, batch    47 | loss: 4.0160446Losses:  3.9462790489196777 0.06716689467430115
CurrentTrain: epoch  8, batch    48 | loss: 4.0134459Losses:  3.944323778152466 0.0988096296787262
CurrentTrain: epoch  8, batch    49 | loss: 4.0431333Losses:  3.9674618244171143 0.08318926393985748
CurrentTrain: epoch  8, batch    50 | loss: 4.0506511Losses:  3.965536117553711 0.08825399726629257
CurrentTrain: epoch  8, batch    51 | loss: 4.0537901Losses:  3.95277738571167 0.08177231252193451
CurrentTrain: epoch  8, batch    52 | loss: 4.0345497Losses:  3.9311347007751465 0.06584206968545914
CurrentTrain: epoch  8, batch    53 | loss: 3.9969769Losses:  3.9531469345092773 0.06989976763725281
CurrentTrain: epoch  8, batch    54 | loss: 4.0230465Losses:  3.9115407466888428 0.053215254098176956
CurrentTrain: epoch  8, batch    55 | loss: 3.9647560Losses:  3.9860706329345703 0.050900429487228394
CurrentTrain: epoch  8, batch    56 | loss: 4.0369711Losses:  3.9067749977111816 0.07932953536510468
CurrentTrain: epoch  8, batch    57 | loss: 3.9861045Losses:  3.9414727687835693 0.08529644459486008
CurrentTrain: epoch  8, batch    58 | loss: 4.0267692Losses:  3.921529531478882 0.07707303762435913
CurrentTrain: epoch  8, batch    59 | loss: 3.9986026Losses:  3.9583635330200195 0.08670082688331604
CurrentTrain: epoch  8, batch    60 | loss: 4.0450644Losses:  3.915694236755371 0.09079287201166153
CurrentTrain: epoch  8, batch    61 | loss: 4.0064869Losses:  3.9628195762634277 0.018529441207647324
CurrentTrain: epoch  8, batch    62 | loss: 3.9813490Losses:  3.9910550117492676 0.051510270684957504
CurrentTrain: epoch  9, batch     0 | loss: 4.0425653Losses:  3.930543899536133 0.09156958758831024
CurrentTrain: epoch  9, batch     1 | loss: 4.0221133Losses:  3.9747400283813477 0.07937249541282654
CurrentTrain: epoch  9, batch     2 | loss: 4.0541124Losses:  3.939621686935425 0.07155180722475052
CurrentTrain: epoch  9, batch     3 | loss: 4.0111737Losses:  3.9490418434143066 0.08776959776878357
CurrentTrain: epoch  9, batch     4 | loss: 4.0368114Losses:  3.9699182510375977 0.06732675433158875
CurrentTrain: epoch  9, batch     5 | loss: 4.0372448Losses:  3.9399070739746094 0.06657382845878601
CurrentTrain: epoch  9, batch     6 | loss: 4.0064807Losses:  3.932847261428833 0.07062234729528427
CurrentTrain: epoch  9, batch     7 | loss: 4.0034695Losses:  3.9694571495056152 0.09756232798099518
CurrentTrain: epoch  9, batch     8 | loss: 4.0670195Losses:  3.9250991344451904 0.07561197876930237
CurrentTrain: epoch  9, batch     9 | loss: 4.0007110Losses:  3.972686529159546 0.08907903730869293
CurrentTrain: epoch  9, batch    10 | loss: 4.0617657Losses:  3.944800853729248 0.04949934035539627
CurrentTrain: epoch  9, batch    11 | loss: 3.9943001Losses:  3.9594240188598633 0.08096937090158463
CurrentTrain: epoch  9, batch    12 | loss: 4.0403934Losses:  3.9764809608459473 0.10837019234895706
CurrentTrain: epoch  9, batch    13 | loss: 4.0848513Losses:  3.946929931640625 0.10355319827795029
CurrentTrain: epoch  9, batch    14 | loss: 4.0504832Losses:  3.945878028869629 0.06638044863939285
CurrentTrain: epoch  9, batch    15 | loss: 4.0122585Losses:  3.941087484359741 0.08310556411743164
CurrentTrain: epoch  9, batch    16 | loss: 4.0241928Losses:  3.972630500793457 0.05384339392185211
CurrentTrain: epoch  9, batch    17 | loss: 4.0264740Losses:  3.9509594440460205 0.05670282244682312
CurrentTrain: epoch  9, batch    18 | loss: 4.0076623Losses:  3.8649802207946777 0.09334465116262436
CurrentTrain: epoch  9, batch    19 | loss: 3.9583249Losses:  3.933427095413208 0.05601343885064125
CurrentTrain: epoch  9, batch    20 | loss: 3.9894404Losses:  3.9141032695770264 0.05764590948820114
CurrentTrain: epoch  9, batch    21 | loss: 3.9717491Losses:  3.9519429206848145 0.060202281922101974
CurrentTrain: epoch  9, batch    22 | loss: 4.0121450Losses:  3.996100902557373 0.048066817224025726
CurrentTrain: epoch  9, batch    23 | loss: 4.0441675Losses:  3.9593124389648438 0.08710657060146332
CurrentTrain: epoch  9, batch    24 | loss: 4.0464191Losses:  3.932339906692505 0.0921325832605362
CurrentTrain: epoch  9, batch    25 | loss: 4.0244727Losses:  3.9398303031921387 0.06919200718402863
CurrentTrain: epoch  9, batch    26 | loss: 4.0090222Losses:  3.945028305053711 0.06401216983795166
CurrentTrain: epoch  9, batch    27 | loss: 4.0090404Losses:  3.967597007751465 0.07146049290895462
CurrentTrain: epoch  9, batch    28 | loss: 4.0390577Losses:  3.949105739593506 0.07669611275196075
CurrentTrain: epoch  9, batch    29 | loss: 4.0258017Losses:  3.915637254714966 0.08977966755628586
CurrentTrain: epoch  9, batch    30 | loss: 4.0054169Losses:  3.92014217376709 0.06229081749916077
CurrentTrain: epoch  9, batch    31 | loss: 3.9824331Losses:  3.926368236541748 0.09654206037521362
CurrentTrain: epoch  9, batch    32 | loss: 4.0229101Losses:  3.960536241531372 0.0732150673866272
CurrentTrain: epoch  9, batch    33 | loss: 4.0337515Losses:  3.925638198852539 0.08489219844341278
CurrentTrain: epoch  9, batch    34 | loss: 4.0105305Losses:  3.9479477405548096 0.09771591424942017
CurrentTrain: epoch  9, batch    35 | loss: 4.0456638Losses:  3.94514799118042 0.08564843237400055
CurrentTrain: epoch  9, batch    36 | loss: 4.0307965Losses:  3.92919659614563 0.07063806802034378
CurrentTrain: epoch  9, batch    37 | loss: 3.9998348Losses:  3.9352798461914062 0.085796058177948
CurrentTrain: epoch  9, batch    38 | loss: 4.0210757Losses:  3.975712299346924 0.09413112699985504
CurrentTrain: epoch  9, batch    39 | loss: 4.0698433Losses:  3.9621801376342773 0.08133232593536377
CurrentTrain: epoch  9, batch    40 | loss: 4.0435123Losses:  3.9325437545776367 0.08821967989206314
CurrentTrain: epoch  9, batch    41 | loss: 4.0207634Losses:  3.960977792739868 0.06904269754886627
CurrentTrain: epoch  9, batch    42 | loss: 4.0300207Losses:  3.904919147491455 0.07075395435094833
CurrentTrain: epoch  9, batch    43 | loss: 3.9756732Losses:  3.966252326965332 0.07510168850421906
CurrentTrain: epoch  9, batch    44 | loss: 4.0413542Losses:  3.95995831489563 0.08838212490081787
CurrentTrain: epoch  9, batch    45 | loss: 4.0483403Losses:  3.927300214767456 0.054602377116680145
CurrentTrain: epoch  9, batch    46 | loss: 3.9819026Losses:  3.9702134132385254 0.05742993950843811
CurrentTrain: epoch  9, batch    47 | loss: 4.0276432Losses:  3.952442169189453 0.07047918438911438
CurrentTrain: epoch  9, batch    48 | loss: 4.0229216Losses:  3.924839496612549 0.06527771055698395
CurrentTrain: epoch  9, batch    49 | loss: 3.9901173Losses:  3.972283363342285 0.05969782546162605
CurrentTrain: epoch  9, batch    50 | loss: 4.0319810Losses:  3.9568991661071777 0.0715750902891159
CurrentTrain: epoch  9, batch    51 | loss: 4.0284743Losses:  3.9723806381225586 0.08721763640642166
CurrentTrain: epoch  9, batch    52 | loss: 4.0595984Losses:  3.9519920349121094 0.06042536348104477
CurrentTrain: epoch  9, batch    53 | loss: 4.0124173Losses:  3.9503464698791504 0.08949580043554306
CurrentTrain: epoch  9, batch    54 | loss: 4.0398421Losses:  3.9576518535614014 0.05958610028028488
CurrentTrain: epoch  9, batch    55 | loss: 4.0172381Losses:  3.9353604316711426 0.05088004842400551
CurrentTrain: epoch  9, batch    56 | loss: 3.9862404Losses:  3.907399892807007 0.07724444568157196
CurrentTrain: epoch  9, batch    57 | loss: 3.9846444Losses:  3.981621265411377 0.05198104679584503
CurrentTrain: epoch  9, batch    58 | loss: 4.0336022Losses:  3.9231081008911133 0.07426522672176361
CurrentTrain: epoch  9, batch    59 | loss: 3.9973733Losses:  3.9518041610717773 0.08930550515651703
CurrentTrain: epoch  9, batch    60 | loss: 4.0411096Losses:  3.935767650604248 0.059426311403512955
CurrentTrain: epoch  9, batch    61 | loss: 3.9951940Losses:  3.944793462753296 0.04732942208647728
CurrentTrain: epoch  9, batch    62 | loss: 3.9921229
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 93.47%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.34%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.93%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.90%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.74%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.70%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.64%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.45%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [0 5 7 0 0 0 1 0 6 4 1 0 2 0 8 0 3 0 0 0]
Losses:  8.537141799926758 1.7237416505813599
CurrentTrain: epoch  0, batch     0 | loss: 10.2608833Losses:  10.380023956298828 2.1494288444519043
CurrentTrain: epoch  0, batch     1 | loss: 12.5294533Losses:  9.048494338989258 1.5503301620483398
CurrentTrain: epoch  0, batch     2 | loss: 10.5988245Losses:  6.821474075317383 0.6818467974662781
CurrentTrain: epoch  0, batch     3 | loss: 7.5033207Losses:  4.342643737792969 1.5266960859298706
CurrentTrain: epoch  1, batch     0 | loss: 5.8693399Losses:  4.921524524688721 1.8260300159454346
CurrentTrain: epoch  1, batch     1 | loss: 6.7475548Losses:  4.615820407867432 1.3370503187179565
CurrentTrain: epoch  1, batch     2 | loss: 5.9528708Losses:  6.771613121032715 0.7727535963058472
CurrentTrain: epoch  1, batch     3 | loss: 7.5443668Losses:  4.312483787536621 1.3693476915359497
CurrentTrain: epoch  2, batch     0 | loss: 5.6818314Losses:  4.1880083084106445 1.5798203945159912
CurrentTrain: epoch  2, batch     1 | loss: 5.7678289Losses:  4.466017246246338 1.3441332578659058
CurrentTrain: epoch  2, batch     2 | loss: 5.8101506Losses:  4.580852508544922 0.27221450209617615
CurrentTrain: epoch  2, batch     3 | loss: 4.8530669Losses:  4.004491806030273 1.279105305671692
CurrentTrain: epoch  3, batch     0 | loss: 5.2835970Losses:  3.3243155479431152 1.1436536312103271
CurrentTrain: epoch  3, batch     1 | loss: 4.4679689Losses:  4.382390975952148 1.2779343128204346
CurrentTrain: epoch  3, batch     2 | loss: 5.6603251Losses:  4.733668327331543 0.4429841637611389
CurrentTrain: epoch  3, batch     3 | loss: 5.1766524Losses:  3.6509764194488525 1.4049999713897705
CurrentTrain: epoch  4, batch     0 | loss: 5.0559764Losses:  3.9901747703552246 1.2676218748092651
CurrentTrain: epoch  4, batch     1 | loss: 5.2577968Losses:  4.2394118309021 1.340347409248352
CurrentTrain: epoch  4, batch     2 | loss: 5.5797591Losses:  2.1895358562469482 0.3311808109283447
CurrentTrain: epoch  4, batch     3 | loss: 2.5207167Losses:  3.983266830444336 1.333716869354248
CurrentTrain: epoch  5, batch     0 | loss: 5.3169837Losses:  4.005003929138184 1.1810998916625977
CurrentTrain: epoch  5, batch     1 | loss: 5.1861038Losses:  3.564836025238037 1.1403923034667969
CurrentTrain: epoch  5, batch     2 | loss: 4.7052283Losses:  1.9479706287384033 0.188419908285141
CurrentTrain: epoch  5, batch     3 | loss: 2.1363904Losses:  3.176844358444214 1.0950393676757812
CurrentTrain: epoch  6, batch     0 | loss: 4.2718840Losses:  3.6600499153137207 1.034919023513794
CurrentTrain: epoch  6, batch     1 | loss: 4.6949692Losses:  3.6125328540802 1.1742770671844482
CurrentTrain: epoch  6, batch     2 | loss: 4.7868099Losses:  4.062978267669678 0.14629381895065308
CurrentTrain: epoch  6, batch     3 | loss: 4.2092719Losses:  3.585407257080078 1.2122513055801392
CurrentTrain: epoch  7, batch     0 | loss: 4.7976584Losses:  3.6544933319091797 1.0111125707626343
CurrentTrain: epoch  7, batch     1 | loss: 4.6656060Losses:  3.179887056350708 1.0726361274719238
CurrentTrain: epoch  7, batch     2 | loss: 4.2525234Losses:  2.549434185028076 0.08322128653526306
CurrentTrain: epoch  7, batch     3 | loss: 2.6326554Losses:  3.6529972553253174 1.225820541381836
CurrentTrain: epoch  8, batch     0 | loss: 4.8788176Losses:  3.2288856506347656 1.0944479703903198
CurrentTrain: epoch  8, batch     1 | loss: 4.3233337Losses:  3.2013607025146484 0.8056991696357727
CurrentTrain: epoch  8, batch     2 | loss: 4.0070601Losses:  1.6529536247253418 8.94069742685133e-08
CurrentTrain: epoch  8, batch     3 | loss: 1.6529537Losses:  2.972478151321411 0.8678402900695801
CurrentTrain: epoch  9, batch     0 | loss: 3.8403184Losses:  3.3482742309570312 1.0688199996948242
CurrentTrain: epoch  9, batch     1 | loss: 4.4170942Losses:  3.051421880722046 0.9003943204879761
CurrentTrain: epoch  9, batch     2 | loss: 3.9518161Losses:  4.075772762298584 0.4511185884475708
CurrentTrain: epoch  9, batch     3 | loss: 4.5268912
Losses:  5.527892112731934 1.105652093887329
MemoryTrain:  epoch  0, batch     0 | loss: 6.6335440Losses:  9.633587837219238 0.14634472131729126
MemoryTrain:  epoch  0, batch     1 | loss: 9.7799330Losses:  0.7831147313117981 0.9817147850990295
MemoryTrain:  epoch  1, batch     0 | loss: 1.7648295Losses:  0.4904050827026367 0.10299824923276901
MemoryTrain:  epoch  1, batch     1 | loss: 0.5934033Losses:  0.6352871656417847 0.84401535987854
MemoryTrain:  epoch  2, batch     0 | loss: 1.4793025Losses:  0.2917362451553345 0.20881792902946472
MemoryTrain:  epoch  2, batch     1 | loss: 0.5005542Losses:  0.3308473825454712 0.6329927444458008
MemoryTrain:  epoch  3, batch     0 | loss: 0.9638401Losses:  0.7226541042327881 0.4303004741668701
MemoryTrain:  epoch  3, batch     1 | loss: 1.1529546Losses:  0.3459808826446533 0.7933636903762817
MemoryTrain:  epoch  4, batch     0 | loss: 1.1393446Losses:  0.2045908272266388 0.22788922488689423
MemoryTrain:  epoch  4, batch     1 | loss: 0.4324800Losses:  0.2614746689796448 0.7956318855285645
MemoryTrain:  epoch  5, batch     0 | loss: 1.0571065Losses:  0.16024915874004364 0.2753109931945801
MemoryTrain:  epoch  5, batch     1 | loss: 0.4355602Losses:  0.2457338124513626 0.7326749563217163
MemoryTrain:  epoch  6, batch     0 | loss: 0.9784088Losses:  0.15088370442390442 0.21859210729599
MemoryTrain:  epoch  6, batch     1 | loss: 0.3694758Losses:  0.17548038065433502 0.6494656801223755
MemoryTrain:  epoch  7, batch     0 | loss: 0.8249460Losses:  0.25647783279418945 0.2830621600151062
MemoryTrain:  epoch  7, batch     1 | loss: 0.5395400Losses:  0.18200430274009705 0.680600643157959
MemoryTrain:  epoch  8, batch     0 | loss: 0.8626050Losses:  0.1790316104888916 0.22053122520446777
MemoryTrain:  epoch  8, batch     1 | loss: 0.3995628Losses:  0.21261459589004517 0.7623218297958374
MemoryTrain:  epoch  9, batch     0 | loss: 0.9749364Losses:  0.12224604189395905 0.12889236211776733
MemoryTrain:  epoch  9, batch     1 | loss: 0.2511384
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 50.96%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 61.31%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.82%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 73.30%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 72.76%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 73.17%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 72.50%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 72.23%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 71.67%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.13%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 92.33%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 92.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.55%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.89%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.55%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.91%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 95.11%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.81%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 94.47%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.68%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 94.30%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 94.07%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 93.85%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 93.25%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 91.89%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 90.67%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 89.49%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 88.62%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 87.68%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 86.78%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 86.80%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 86.46%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 86.39%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 86.32%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 86.33%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 86.27%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 86.28%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 86.14%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 86.08%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 85.90%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 85.69%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 85.42%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 85.15%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 85.03%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 85.57%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 85.72%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 86.26%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 85.91%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 85.74%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 85.52%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 85.60%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.38%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 85.22%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 84.84%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 84.63%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 84.43%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 84.23%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 83.93%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 84.02%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 83.99%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 84.16%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.19%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 84.27%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 84.30%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 83.96%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 83.83%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 83.61%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 83.18%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 82.91%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 82.75%   
cur_acc:  ['0.9484', '0.7113']
his_acc:  ['0.9484', '0.8275']
Clustering into  14  clusters
Clusters:  [ 2 11 13  2  2  2  0  2  9  7  6  2  8  2 10  0  4  2  2  2  0 12  6  2
  2  3  2  1  2  5]
Losses:  7.541919708251953 1.5662540197372437
CurrentTrain: epoch  0, batch     0 | loss: 9.1081734Losses:  8.361660957336426 1.698939323425293
CurrentTrain: epoch  0, batch     1 | loss: 10.0606003Losses:  7.7884840965271 1.645710825920105
CurrentTrain: epoch  0, batch     2 | loss: 9.4341946Losses:  6.018659591674805 0.45204591751098633
CurrentTrain: epoch  0, batch     3 | loss: 6.4707055Losses:  4.140253067016602 1.7468878030776978
CurrentTrain: epoch  1, batch     0 | loss: 5.8871408Losses:  3.789644241333008 1.4904674291610718
CurrentTrain: epoch  1, batch     1 | loss: 5.2801118Losses:  3.7726783752441406 1.3877909183502197
CurrentTrain: epoch  1, batch     2 | loss: 5.1604691Losses:  3.4853644371032715 0.525222897529602
CurrentTrain: epoch  1, batch     3 | loss: 4.0105872Losses:  3.597322940826416 1.5104620456695557
CurrentTrain: epoch  2, batch     0 | loss: 5.1077852Losses:  3.4685325622558594 1.3783278465270996
CurrentTrain: epoch  2, batch     1 | loss: 4.8468604Losses:  3.692852020263672 1.4431719779968262
CurrentTrain: epoch  2, batch     2 | loss: 5.1360240Losses:  2.544498920440674 0.13889533281326294
CurrentTrain: epoch  2, batch     3 | loss: 2.6833942Losses:  2.8277885913848877 1.0206592082977295
CurrentTrain: epoch  3, batch     0 | loss: 3.8484478Losses:  3.5140724182128906 1.3236758708953857
CurrentTrain: epoch  3, batch     1 | loss: 4.8377485Losses:  3.851929187774658 1.508206844329834
CurrentTrain: epoch  3, batch     2 | loss: 5.3601360Losses:  3.2099642753601074 0.23183029890060425
CurrentTrain: epoch  3, batch     3 | loss: 3.4417946Losses:  3.4942259788513184 1.3102864027023315
CurrentTrain: epoch  4, batch     0 | loss: 4.8045125Losses:  3.11521053314209 1.1035202741622925
CurrentTrain: epoch  4, batch     1 | loss: 4.2187309Losses:  2.7702410221099854 0.9961835741996765
CurrentTrain: epoch  4, batch     2 | loss: 3.7664247Losses:  3.949082851409912 0.32994869351387024
CurrentTrain: epoch  4, batch     3 | loss: 4.2790318Losses:  3.0055654048919678 1.0286879539489746
CurrentTrain: epoch  5, batch     0 | loss: 4.0342531Losses:  3.023334264755249 0.904188334941864
CurrentTrain: epoch  5, batch     1 | loss: 3.9275227Losses:  2.8605856895446777 1.1685993671417236
CurrentTrain: epoch  5, batch     2 | loss: 4.0291853Losses:  3.462099552154541 0.34180253744125366
CurrentTrain: epoch  5, batch     3 | loss: 3.8039021Losses:  3.1403703689575195 1.2115325927734375
CurrentTrain: epoch  6, batch     0 | loss: 4.3519030Losses:  2.7606663703918457 1.0476670265197754
CurrentTrain: epoch  6, batch     1 | loss: 3.8083334Losses:  2.602480888366699 1.013191819190979
CurrentTrain: epoch  6, batch     2 | loss: 3.6156726Losses:  3.6034531593322754 0.41024643182754517
CurrentTrain: epoch  6, batch     3 | loss: 4.0136995Losses:  2.5821757316589355 0.9724901914596558
CurrentTrain: epoch  7, batch     0 | loss: 3.5546660Losses:  2.9573264122009277 1.0317120552062988
CurrentTrain: epoch  7, batch     1 | loss: 3.9890385Losses:  2.694986343383789 0.8160289525985718
CurrentTrain: epoch  7, batch     2 | loss: 3.5110154Losses:  2.1956233978271484 0.11540579050779343
CurrentTrain: epoch  7, batch     3 | loss: 2.3110292Losses:  2.5332279205322266 0.8434247970581055
CurrentTrain: epoch  8, batch     0 | loss: 3.3766527Losses:  2.752284049987793 0.9666738510131836
CurrentTrain: epoch  8, batch     1 | loss: 3.7189579Losses:  2.4685614109039307 0.7349055409431458
CurrentTrain: epoch  8, batch     2 | loss: 3.2034669Losses:  3.7541608810424805 0.2823876738548279
CurrentTrain: epoch  8, batch     3 | loss: 4.0365486Losses:  2.844825506210327 0.9773001670837402
CurrentTrain: epoch  9, batch     0 | loss: 3.8221257Losses:  2.3947649002075195 0.7222601175308228
CurrentTrain: epoch  9, batch     1 | loss: 3.1170249Losses:  2.438664197921753 0.8962065577507019
CurrentTrain: epoch  9, batch     2 | loss: 3.3348708Losses:  2.0367989540100098 0.2021605670452118
CurrentTrain: epoch  9, batch     3 | loss: 2.2389596
Losses:  5.643738746643066 1.101622462272644
MemoryTrain:  epoch  0, batch     0 | loss: 6.7453613Losses:  9.95524787902832 0.6126387715339661
MemoryTrain:  epoch  0, batch     1 | loss: 10.5678864Losses:  0.9060660600662231 1.0184292793273926
MemoryTrain:  epoch  1, batch     0 | loss: 1.9244953Losses:  0.6721321940422058 0.6953990459442139
MemoryTrain:  epoch  1, batch     1 | loss: 1.3675313Losses:  0.9042696952819824 0.9005221128463745
MemoryTrain:  epoch  2, batch     0 | loss: 1.8047918Losses:  0.3938646614551544 0.7665303349494934
MemoryTrain:  epoch  2, batch     1 | loss: 1.1603950Losses:  0.39761239290237427 0.7088335752487183
MemoryTrain:  epoch  3, batch     0 | loss: 1.1064460Losses:  0.5213735699653625 0.9895367622375488
MemoryTrain:  epoch  3, batch     1 | loss: 1.5109103Losses:  0.45059508085250854 0.921528160572052
MemoryTrain:  epoch  4, batch     0 | loss: 1.3721232Losses:  0.33151817321777344 0.6802710294723511
MemoryTrain:  epoch  4, batch     1 | loss: 1.0117892Losses:  0.401388555765152 0.8698663115501404
MemoryTrain:  epoch  5, batch     0 | loss: 1.2712549Losses:  0.2896336019039154 0.6597274541854858
MemoryTrain:  epoch  5, batch     1 | loss: 0.9493611Losses:  0.274994432926178 0.766476035118103
MemoryTrain:  epoch  6, batch     0 | loss: 1.0414705Losses:  0.3057475686073303 0.7696705460548401
MemoryTrain:  epoch  6, batch     1 | loss: 1.0754181Losses:  0.27228081226348877 0.7724491953849792
MemoryTrain:  epoch  7, batch     0 | loss: 1.0447299Losses:  0.24228347837924957 0.6390292644500732
MemoryTrain:  epoch  7, batch     1 | loss: 0.8813127Losses:  0.2585139274597168 0.8451889753341675
MemoryTrain:  epoch  8, batch     0 | loss: 1.1037029Losses:  0.2590453624725342 0.6504849195480347
MemoryTrain:  epoch  8, batch     1 | loss: 0.9095303Losses:  0.22376373410224915 0.6748237609863281
MemoryTrain:  epoch  9, batch     0 | loss: 0.8985875Losses:  0.2404787540435791 0.7203683257102966
MemoryTrain:  epoch  9, batch     1 | loss: 0.9608471
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 61.81%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 59.60%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 57.76%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 56.04%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 54.64%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 55.08%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 56.06%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 56.99%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 57.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 61.02%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.76%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 65.43%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 65.32%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 65.26%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 65.09%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.98%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 67.64%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 67.36%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.38%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.04%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 94.43%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.15%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 94.01%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.13%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.12%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.11%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.33%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 93.53%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.21%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 92.32%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.34%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 91.67%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 89.23%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 88.07%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 86.94%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 86.03%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 85.24%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.27%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 85.07%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.97%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 84.87%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 84.74%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 84.62%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 84.49%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 84.22%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 84.26%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 83.77%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 83.06%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 82.22%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 81.69%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 80.24%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.32%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 82.36%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 82.11%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 82.04%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.02%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 81.90%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 81.66%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 80.96%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 80.57%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 80.29%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 79.91%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 79.81%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 79.88%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 80.06%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.13%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 80.24%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 80.00%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 79.76%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 79.42%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 79.18%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 79.05%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 78.87%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 78.64%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 78.29%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 78.17%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.10%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 78.22%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.52%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 78.67%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 78.19%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 77.72%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 77.30%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 76.76%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 76.31%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 76.00%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.16%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.45%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.76%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 76.45%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 75.99%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 75.49%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 75.04%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 74.60%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 74.20%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 74.16%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 75.04%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 75.55%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 75.48%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 75.21%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.07%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 74.83%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 74.90%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 74.86%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 74.90%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 74.97%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.07%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 75.14%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 75.27%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.13%   
cur_acc:  ['0.9484', '0.7113', '0.6736']
his_acc:  ['0.9484', '0.8275', '0.7513']
Clustering into  19  clusters
Clusters:  [ 1 11 13  1  1  1  0  1  9 15  2  1 12  1 10  0  4  1  1  1 17 16  2  1
  1  5  1 18  1  8  1 14  1  1  7  1  1  1  3  6]
Losses:  6.973769664764404 1.4250404834747314
CurrentTrain: epoch  0, batch     0 | loss: 8.3988104Losses:  9.669425964355469 1.2274131774902344
CurrentTrain: epoch  0, batch     1 | loss: 10.8968391Losses:  7.157476902008057 1.4423677921295166
CurrentTrain: epoch  0, batch     2 | loss: 8.5998449Losses:  6.379214286804199 0.22129519283771515
CurrentTrain: epoch  0, batch     3 | loss: 6.6005096Losses:  4.5443949699401855 1.3461860418319702
CurrentTrain: epoch  1, batch     0 | loss: 5.8905811Losses:  3.938718795776367 1.298227310180664
CurrentTrain: epoch  1, batch     1 | loss: 5.2369461Losses:  3.1660499572753906 1.2151356935501099
CurrentTrain: epoch  1, batch     2 | loss: 4.3811855Losses:  4.543144702911377 0.3087523579597473
CurrentTrain: epoch  1, batch     3 | loss: 4.8518972Losses:  3.4243621826171875 1.1743650436401367
CurrentTrain: epoch  2, batch     0 | loss: 4.5987272Losses:  3.996145248413086 1.0444990396499634
CurrentTrain: epoch  2, batch     1 | loss: 5.0406442Losses:  3.7927465438842773 1.3916511535644531
CurrentTrain: epoch  2, batch     2 | loss: 5.1843977Losses:  2.6355905532836914 0.32574543356895447
CurrentTrain: epoch  2, batch     3 | loss: 2.9613359Losses:  3.6816658973693848 1.2469358444213867
CurrentTrain: epoch  3, batch     0 | loss: 4.9286017Losses:  3.377821445465088 0.9974392652511597
CurrentTrain: epoch  3, batch     1 | loss: 4.3752608Losses:  3.382164716720581 1.1189591884613037
CurrentTrain: epoch  3, batch     2 | loss: 4.5011239Losses:  2.6406919956207275 0.27961432933807373
CurrentTrain: epoch  3, batch     3 | loss: 2.9203062Losses:  2.9149911403656006 1.1889967918395996
CurrentTrain: epoch  4, batch     0 | loss: 4.1039877Losses:  3.3157060146331787 1.1139240264892578
CurrentTrain: epoch  4, batch     1 | loss: 4.4296303Losses:  3.2673540115356445 1.1510791778564453
CurrentTrain: epoch  4, batch     2 | loss: 4.4184332Losses:  2.1473851203918457 0.237272709608078
CurrentTrain: epoch  4, batch     3 | loss: 2.3846579Losses:  3.5198938846588135 1.1260895729064941
CurrentTrain: epoch  5, batch     0 | loss: 4.6459837Losses:  2.733837604522705 0.8917557597160339
CurrentTrain: epoch  5, batch     1 | loss: 3.6255934Losses:  2.8083648681640625 0.8346866369247437
CurrentTrain: epoch  5, batch     2 | loss: 3.6430516Losses:  2.6684937477111816 0.24639244377613068
CurrentTrain: epoch  5, batch     3 | loss: 2.9148862Losses:  2.694467306137085 1.0665700435638428
CurrentTrain: epoch  6, batch     0 | loss: 3.7610373Losses:  2.8344974517822266 1.0432066917419434
CurrentTrain: epoch  6, batch     1 | loss: 3.8777041Losses:  3.1878135204315186 1.0220638513565063
CurrentTrain: epoch  6, batch     2 | loss: 4.2098775Losses:  1.7570106983184814 0.03334851190447807
CurrentTrain: epoch  6, batch     3 | loss: 1.7903593Losses:  2.4027910232543945 0.7872047424316406
CurrentTrain: epoch  7, batch     0 | loss: 3.1899958Losses:  2.390768051147461 0.6975325345993042
CurrentTrain: epoch  7, batch     1 | loss: 3.0883007Losses:  3.2732748985290527 1.0311979055404663
CurrentTrain: epoch  7, batch     2 | loss: 4.3044729Losses:  3.5530436038970947 0.2757735550403595
CurrentTrain: epoch  7, batch     3 | loss: 3.8288171Losses:  2.916297674179077 0.8352057933807373
CurrentTrain: epoch  8, batch     0 | loss: 3.7515035Losses:  2.4771409034729004 0.9599841833114624
CurrentTrain: epoch  8, batch     1 | loss: 3.4371252Losses:  2.501427173614502 0.6739134788513184
CurrentTrain: epoch  8, batch     2 | loss: 3.1753407Losses:  1.817051649093628 0.11988206207752228
CurrentTrain: epoch  8, batch     3 | loss: 1.9369338Losses:  2.661954402923584 0.7567805051803589
CurrentTrain: epoch  9, batch     0 | loss: 3.4187350Losses:  2.446072578430176 0.860848605632782
CurrentTrain: epoch  9, batch     1 | loss: 3.3069212Losses:  2.5345354080200195 0.921066164970398
CurrentTrain: epoch  9, batch     2 | loss: 3.4556017Losses:  1.855911135673523 0.04479090869426727
CurrentTrain: epoch  9, batch     3 | loss: 1.9007020
Losses:  5.719644546508789 0.8978852033615112
MemoryTrain:  epoch  0, batch     0 | loss: 6.6175299Losses:  9.844070434570312 0.7699629664421082
MemoryTrain:  epoch  0, batch     1 | loss: 10.6140337Losses:  10.296630859375 0.5222873687744141
MemoryTrain:  epoch  0, batch     2 | loss: 10.8189182Losses:  1.1209907531738281 0.9421514868736267
MemoryTrain:  epoch  1, batch     0 | loss: 2.0631423Losses:  0.5945658087730408 0.7962602972984314
MemoryTrain:  epoch  1, batch     1 | loss: 1.3908261Losses:  1.2546312808990479 0.5395361185073853
MemoryTrain:  epoch  1, batch     2 | loss: 1.7941674Losses:  0.7374026775360107 0.7192376852035522
MemoryTrain:  epoch  2, batch     0 | loss: 1.4566404Losses:  0.7731523513793945 0.9965242743492126
MemoryTrain:  epoch  2, batch     1 | loss: 1.7696767Losses:  0.5241435766220093 0.44661974906921387
MemoryTrain:  epoch  2, batch     2 | loss: 0.9707633Losses:  0.6431215405464172 0.8633673191070557
MemoryTrain:  epoch  3, batch     0 | loss: 1.5064888Losses:  0.6294228434562683 0.8568484783172607
MemoryTrain:  epoch  3, batch     1 | loss: 1.4862714Losses:  0.35916703939437866 0.36360710859298706
MemoryTrain:  epoch  3, batch     2 | loss: 0.7227741Losses:  0.43280279636383057 0.8973398804664612
MemoryTrain:  epoch  4, batch     0 | loss: 1.3301427Losses:  0.46047115325927734 0.7285908460617065
MemoryTrain:  epoch  4, batch     1 | loss: 1.1890620Losses:  0.5181632041931152 0.5586228370666504
MemoryTrain:  epoch  4, batch     2 | loss: 1.0767860Losses:  0.37645846605300903 0.923845112323761
MemoryTrain:  epoch  5, batch     0 | loss: 1.3003036Losses:  0.34597665071487427 0.5449084639549255
MemoryTrain:  epoch  5, batch     1 | loss: 0.8908851Losses:  0.512485146522522 0.7724372148513794
MemoryTrain:  epoch  5, batch     2 | loss: 1.2849224Losses:  0.3248054087162018 0.7531327605247498
MemoryTrain:  epoch  6, batch     0 | loss: 1.0779382Losses:  0.35028862953186035 0.8543273210525513
MemoryTrain:  epoch  6, batch     1 | loss: 1.2046160Losses:  0.5616316199302673 0.3974875211715698
MemoryTrain:  epoch  6, batch     2 | loss: 0.9591191Losses:  0.32863420248031616 0.8179129362106323
MemoryTrain:  epoch  7, batch     0 | loss: 1.1465471Losses:  0.39961257576942444 0.7545561194419861
MemoryTrain:  epoch  7, batch     1 | loss: 1.1541687Losses:  0.3529542088508606 0.328847736120224
MemoryTrain:  epoch  7, batch     2 | loss: 0.6818019Losses:  0.3756476640701294 0.7960400581359863
MemoryTrain:  epoch  8, batch     0 | loss: 1.1716877Losses:  0.2945178151130676 0.7178764343261719
MemoryTrain:  epoch  8, batch     1 | loss: 1.0123942Losses:  0.2847162187099457 0.4365970492362976
MemoryTrain:  epoch  8, batch     2 | loss: 0.7213132Losses:  0.27834808826446533 0.5912138223648071
MemoryTrain:  epoch  9, batch     0 | loss: 0.8695619Losses:  0.35916221141815186 0.7543618083000183
MemoryTrain:  epoch  9, batch     1 | loss: 1.1135240Losses:  0.3515922427177429 0.584585428237915
MemoryTrain:  epoch  9, batch     2 | loss: 0.9361777
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 54.91%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 50.35%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 50.66%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 69.35%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 67.97%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 67.54%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 67.60%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 67.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 67.26%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.43%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 91.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 90.49%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.85%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.28%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.32%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 93.61%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.62%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 92.87%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 92.03%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 90.21%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 89.55%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 89.31%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 88.29%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 86.91%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 85.77%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 84.56%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 83.49%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 82.54%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 81.70%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 81.68%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 81.67%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 81.74%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 81.57%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.49%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 81.49%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 81.17%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 81.17%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 80.72%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 79.82%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 78.87%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 77.94%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 77.03%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 76.29%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.66%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.89%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.00%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 78.86%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 78.55%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 78.66%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 78.33%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 77.66%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 77.12%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 76.53%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 76.24%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 75.61%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 75.50%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 75.86%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 75.62%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 75.10%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 74.74%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 74.24%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 73.84%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 73.50%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 73.36%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 73.13%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 72.95%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 72.77%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.77%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.99%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 73.28%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 72.75%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.85%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 71.35%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 70.89%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 70.57%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.27%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 70.89%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 70.47%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 70.13%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 69.72%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 69.31%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 70.42%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 70.73%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 70.97%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 70.66%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 70.58%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 70.31%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 70.09%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 69.86%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 69.64%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 69.55%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 69.65%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 69.65%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 70.35%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 70.35%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 70.04%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 69.80%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 69.54%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 69.27%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 68.85%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 69.47%   [EVAL] batch:  201 | acc: 18.75%,  total acc: 69.21%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 69.06%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 68.87%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 68.69%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 69.42%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 69.51%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.01%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  225 | acc: 18.75%,  total acc: 70.60%   [EVAL] batch:  226 | acc: 0.00%,  total acc: 70.29%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 70.29%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 69.92%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 69.80%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.03%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 70.33%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 70.24%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 70.16%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 69.99%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 69.91%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 69.83%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 69.77%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 69.77%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 69.77%   
cur_acc:  ['0.9484', '0.7113', '0.6736', '0.6726']
his_acc:  ['0.9484', '0.8275', '0.7513', '0.6977']
Clustering into  24  clusters
Clusters:  [ 0 23 13  0  0  0 21  0 12 15 20  0 14  0 22 17 11  0  0  0 19 18  1  0
  0  6  0  9  0 10  0 16  0  5  8  0  0  0  2  7  3  0  0  1  1  4  0  0
  0  0]
Losses:  7.30704927444458 1.8477404117584229
CurrentTrain: epoch  0, batch     0 | loss: 9.1547899Losses:  8.992815017700195 1.6968469619750977
CurrentTrain: epoch  0, batch     1 | loss: 10.6896620Losses:  7.48975944519043 1.7240461111068726
CurrentTrain: epoch  0, batch     2 | loss: 9.2138052Losses:  6.360017776489258 0.643001139163971
CurrentTrain: epoch  0, batch     3 | loss: 7.0030189Losses:  4.299206256866455 1.8138734102249146
CurrentTrain: epoch  1, batch     0 | loss: 6.1130795Losses:  4.2195210456848145 1.5175652503967285
CurrentTrain: epoch  1, batch     1 | loss: 5.7370863Losses:  3.8086934089660645 1.5060656070709229
CurrentTrain: epoch  1, batch     2 | loss: 5.3147593Losses:  4.010923385620117 0.44121700525283813
CurrentTrain: epoch  1, batch     3 | loss: 4.4521403Losses:  3.519327163696289 1.3106732368469238
CurrentTrain: epoch  2, batch     0 | loss: 4.8300004Losses:  3.710402488708496 1.4976606369018555
CurrentTrain: epoch  2, batch     1 | loss: 5.2080631Losses:  3.654691696166992 1.4551887512207031
CurrentTrain: epoch  2, batch     2 | loss: 5.1098804Losses:  4.994505882263184 0.5236719846725464
CurrentTrain: epoch  2, batch     3 | loss: 5.5181780Losses:  3.471550941467285 1.3320200443267822
CurrentTrain: epoch  3, batch     0 | loss: 4.8035707Losses:  3.5975918769836426 1.2725437879562378
CurrentTrain: epoch  3, batch     1 | loss: 4.8701358Losses:  2.997859239578247 1.2819427251815796
CurrentTrain: epoch  3, batch     2 | loss: 4.2798018Losses:  2.680217981338501 0.22043626010417938
CurrentTrain: epoch  3, batch     3 | loss: 2.9006543Losses:  3.270371437072754 1.5109775066375732
CurrentTrain: epoch  4, batch     0 | loss: 4.7813492Losses:  3.049727439880371 1.3930659294128418
CurrentTrain: epoch  4, batch     1 | loss: 4.4427934Losses:  3.062267541885376 1.401928424835205
CurrentTrain: epoch  4, batch     2 | loss: 4.4641962Losses:  2.3461480140686035 0.0
CurrentTrain: epoch  4, batch     3 | loss: 2.3461480Losses:  2.933793544769287 1.3957823514938354
CurrentTrain: epoch  5, batch     0 | loss: 4.3295760Losses:  2.532911539077759 1.0629243850708008
CurrentTrain: epoch  5, batch     1 | loss: 3.5958359Losses:  3.0811781883239746 1.2001879215240479
CurrentTrain: epoch  5, batch     2 | loss: 4.2813663Losses:  4.305417537689209 0.4277024269104004
CurrentTrain: epoch  5, batch     3 | loss: 4.7331200Losses:  2.7794322967529297 1.164623737335205
CurrentTrain: epoch  6, batch     0 | loss: 3.9440560Losses:  2.9852545261383057 1.305194616317749
CurrentTrain: epoch  6, batch     1 | loss: 4.2904491Losses:  2.561551332473755 1.2169840335845947
CurrentTrain: epoch  6, batch     2 | loss: 3.7785354Losses:  2.757114887237549 0.24971628189086914
CurrentTrain: epoch  6, batch     3 | loss: 3.0068312Losses:  2.9205472469329834 1.1592423915863037
CurrentTrain: epoch  7, batch     0 | loss: 4.0797896Losses:  2.893348455429077 1.128265142440796
CurrentTrain: epoch  7, batch     1 | loss: 4.0216136Losses:  2.275022268295288 0.9452664256095886
CurrentTrain: epoch  7, batch     2 | loss: 3.2202888Losses:  1.830040693283081 0.10623302310705185
CurrentTrain: epoch  7, batch     3 | loss: 1.9362737Losses:  2.6577861309051514 1.0428574085235596
CurrentTrain: epoch  8, batch     0 | loss: 3.7006435Losses:  2.600430965423584 1.1424760818481445
CurrentTrain: epoch  8, batch     1 | loss: 3.7429070Losses:  2.374824047088623 0.8947479128837585
CurrentTrain: epoch  8, batch     2 | loss: 3.2695720Losses:  2.6000330448150635 0.3029732406139374
CurrentTrain: epoch  8, batch     3 | loss: 2.9030063Losses:  2.739077091217041 1.060310959815979
CurrentTrain: epoch  9, batch     0 | loss: 3.7993879Losses:  2.432936906814575 1.0047208070755005
CurrentTrain: epoch  9, batch     1 | loss: 3.4376578Losses:  2.180149793624878 0.8649426698684692
CurrentTrain: epoch  9, batch     2 | loss: 3.0450926Losses:  2.5406999588012695 0.2215646654367447
CurrentTrain: epoch  9, batch     3 | loss: 2.7622647
Losses:  5.901552200317383 0.8270583152770996
MemoryTrain:  epoch  0, batch     0 | loss: 6.7286105Losses:  9.64910888671875 0.9010961055755615
MemoryTrain:  epoch  0, batch     1 | loss: 10.5502052Losses:  10.339362144470215 0.9849947690963745
MemoryTrain:  epoch  0, batch     2 | loss: 11.3243570Losses:  10.191970825195312 0.05319832265377045
MemoryTrain:  epoch  0, batch     3 | loss: 10.2451687Losses:  1.2295303344726562 1.0594699382781982
MemoryTrain:  epoch  1, batch     0 | loss: 2.2890003Losses:  0.5965574383735657 0.9280276298522949
MemoryTrain:  epoch  1, batch     1 | loss: 1.5245850Losses:  0.6612495183944702 0.7482728958129883
MemoryTrain:  epoch  1, batch     2 | loss: 1.4095224Losses:  1.0927629470825195 0.04791968688368797
MemoryTrain:  epoch  1, batch     3 | loss: 1.1406826Losses:  0.6356775164604187 0.9086824655532837
MemoryTrain:  epoch  2, batch     0 | loss: 1.5443599Losses:  0.6191051006317139 0.8426759839057922
MemoryTrain:  epoch  2, batch     1 | loss: 1.4617810Losses:  0.9466605186462402 0.8617854118347168
MemoryTrain:  epoch  2, batch     2 | loss: 1.8084459Losses:  0.4409024119377136 0.13161619007587433
MemoryTrain:  epoch  2, batch     3 | loss: 0.5725186Losses:  0.38966527581214905 0.7255449891090393
MemoryTrain:  epoch  3, batch     0 | loss: 1.1152103Losses:  0.746322512626648 0.8628027439117432
MemoryTrain:  epoch  3, batch     1 | loss: 1.6091253Losses:  0.5782248973846436 0.9588994979858398
MemoryTrain:  epoch  3, batch     2 | loss: 1.5371244Losses:  1.6629304885864258 0.05816076323390007
MemoryTrain:  epoch  3, batch     3 | loss: 1.7210913Losses:  0.5062452554702759 0.850956916809082
MemoryTrain:  epoch  4, batch     0 | loss: 1.3572022Losses:  0.5221987962722778 0.8389036655426025
MemoryTrain:  epoch  4, batch     1 | loss: 1.3611025Losses:  0.5407193899154663 0.7451832294464111
MemoryTrain:  epoch  4, batch     2 | loss: 1.2859026Losses:  0.9486125707626343 0.462685763835907
MemoryTrain:  epoch  4, batch     3 | loss: 1.4112983Losses:  0.5399320125579834 1.0246684551239014
MemoryTrain:  epoch  5, batch     0 | loss: 1.5646005Losses:  0.5429344773292542 0.6721879243850708
MemoryTrain:  epoch  5, batch     1 | loss: 1.2151225Losses:  0.4856504499912262 0.7667714357376099
MemoryTrain:  epoch  5, batch     2 | loss: 1.2524219Losses:  0.3055035173892975 0.14943188428878784
MemoryTrain:  epoch  5, batch     3 | loss: 0.4549354Losses:  0.4409792423248291 0.8335822820663452
MemoryTrain:  epoch  6, batch     0 | loss: 1.2745615Losses:  0.47165507078170776 0.8984604477882385
MemoryTrain:  epoch  6, batch     1 | loss: 1.3701155Losses:  0.4071676731109619 0.7164536118507385
MemoryTrain:  epoch  6, batch     2 | loss: 1.1236212Losses:  0.3657551407814026 0.036621350795030594
MemoryTrain:  epoch  6, batch     3 | loss: 0.4023765Losses:  0.4055851697921753 0.9011406898498535
MemoryTrain:  epoch  7, batch     0 | loss: 1.3067259Losses:  0.29925861954689026 0.6594409346580505
MemoryTrain:  epoch  7, batch     1 | loss: 0.9586996Losses:  0.47663259506225586 0.8218434453010559
MemoryTrain:  epoch  7, batch     2 | loss: 1.2984760Losses:  0.45209944248199463 0.06646411120891571
MemoryTrain:  epoch  7, batch     3 | loss: 0.5185636Losses:  0.40767616033554077 0.8137478232383728
MemoryTrain:  epoch  8, batch     0 | loss: 1.2214240Losses:  0.4270399510860443 0.8969330787658691
MemoryTrain:  epoch  8, batch     1 | loss: 1.3239731Losses:  0.333194762468338 0.6623908281326294
MemoryTrain:  epoch  8, batch     2 | loss: 0.9955856Losses:  0.3165172040462494 0.03856519237160683
MemoryTrain:  epoch  8, batch     3 | loss: 0.3550824Losses:  0.3492538630962372 0.8943489789962769
MemoryTrain:  epoch  9, batch     0 | loss: 1.2436029Losses:  0.35786423087120056 0.6819594502449036
MemoryTrain:  epoch  9, batch     1 | loss: 1.0398237Losses:  0.36068329215049744 0.6897561550140381
MemoryTrain:  epoch  9, batch     2 | loss: 1.0504395Losses:  0.3182472586631775 0.05833830684423447
MemoryTrain:  epoch  9, batch     3 | loss: 0.3765856
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 24.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 52.56%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 52.72%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 53.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 51.44%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 49.54%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 47.77%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 46.12%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 44.58%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 43.15%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 45.08%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 46.14%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 47.50%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 48.44%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 49.49%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 50.49%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 51.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.66%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 53.66%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 54.32%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 55.38%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 55.83%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 55.03%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 54.52%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 54.43%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 54.21%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 54.00%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 53.31%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 53.73%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 54.13%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 54.75%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 54.89%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 55.25%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 54.93%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 54.63%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 54.45%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 54.58%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 54.41%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 54.44%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 53.87%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.98%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.76%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.13%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 91.96%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 91.34%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 90.62%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 89.72%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 89.06%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 88.52%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 88.31%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 87.50%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 86.33%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 85.38%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 84.38%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 83.58%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 82.72%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 81.78%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.68%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 81.59%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 81.74%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 81.57%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 81.41%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.33%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 80.94%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 81.02%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 80.49%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 79.52%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 78.57%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 77.65%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 76.74%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 75.93%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 78.28%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 78.06%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 78.03%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 77.82%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.98%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 77.77%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 76.20%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 75.62%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 75.34%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 74.67%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 75.32%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 75.32%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 74.84%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 74.33%   [EVAL] batch:  121 | acc: 25.00%,  total acc: 73.92%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 73.42%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 72.98%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 72.60%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.52%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.34%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 72.17%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 71.97%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 71.90%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 72.20%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 72.43%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 72.46%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 71.94%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 71.47%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.05%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 70.55%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 70.10%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.79%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 70.21%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 70.49%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 70.07%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 69.65%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 69.40%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 68.95%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 68.55%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 68.51%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 69.17%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 68.90%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 68.71%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 68.45%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 68.27%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 68.12%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 68.16%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 67.88%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 67.78%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 67.51%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 67.42%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 67.42%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 67.92%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 67.70%   [EVAL] batch:  190 | acc: 18.75%,  total acc: 67.44%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 67.19%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 67.00%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 66.78%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 66.92%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 67.46%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.53%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 67.48%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 67.30%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 67.18%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 67.13%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 67.04%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 68.78%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 68.94%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 68.69%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.67%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 68.56%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 68.34%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 68.18%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  238 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  239 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 68.46%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 68.39%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 68.29%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 68.16%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 68.04%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 68.04%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 68.02%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 68.00%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 67.78%   [EVAL] batch:  251 | acc: 25.00%,  total acc: 67.61%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 67.37%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 67.22%   [EVAL] batch:  254 | acc: 6.25%,  total acc: 66.99%   [EVAL] batch:  255 | acc: 18.75%,  total acc: 66.80%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.86%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 67.20%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 67.18%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 67.16%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 67.18%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 67.07%   [EVAL] batch:  268 | acc: 31.25%,  total acc: 66.94%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 66.77%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 66.75%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 66.71%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 66.66%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 66.20%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 65.96%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 65.73%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 65.26%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 65.25%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 66.14%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 65.98%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 65.87%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 65.81%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 65.74%   [EVAL] batch:  299 | acc: 43.75%,  total acc: 65.67%   [EVAL] batch:  300 | acc: 18.75%,  total acc: 65.51%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 65.57%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 65.65%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 65.64%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:  307 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 65.41%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  311 | acc: 56.25%,  total acc: 65.30%   [EVAL] batch:  312 | acc: 18.75%,  total acc: 65.16%   
cur_acc:  ['0.9484', '0.7113', '0.6736', '0.6726', '0.5387']
his_acc:  ['0.9484', '0.8275', '0.7513', '0.6977', '0.6516']
Clustering into  29  clusters
Clusters:  [ 0 19 15  0  0  0 24  0 27 28 20  0 17  0 14  9 18  0  0  0 23 21  2  0
  0 16  0 22  0 25  0 11  0 13 26  0  0  0  6  8 10  0  0  2  2  7  0  0
  0  0  0  0  0  4  0  3  0 12  5  1]
Losses:  6.8594651222229 0.8126606345176697
CurrentTrain: epoch  0, batch     0 | loss: 7.6721258Losses:  8.356141090393066 1.2008728981018066
CurrentTrain: epoch  0, batch     1 | loss: 9.5570145Losses:  8.134992599487305 1.0976591110229492
CurrentTrain: epoch  0, batch     2 | loss: 9.2326517Losses:  5.0234270095825195 0.8106821179389954
CurrentTrain: epoch  0, batch     3 | loss: 5.8341093Losses:  4.097945213317871 0.971264660358429
CurrentTrain: epoch  1, batch     0 | loss: 5.0692101Losses:  3.430265426635742 0.8774070143699646
CurrentTrain: epoch  1, batch     1 | loss: 4.3076725Losses:  3.5439820289611816 0.9777672290802002
CurrentTrain: epoch  1, batch     2 | loss: 4.5217495Losses:  3.341978073120117 0.36084046959877014
CurrentTrain: epoch  1, batch     3 | loss: 3.7028186Losses:  3.279876947402954 0.6521790623664856
CurrentTrain: epoch  2, batch     0 | loss: 3.9320560Losses:  3.9886269569396973 0.7750952243804932
CurrentTrain: epoch  2, batch     1 | loss: 4.7637224Losses:  2.815073013305664 0.7520681619644165
CurrentTrain: epoch  2, batch     2 | loss: 3.5671411Losses:  3.243457794189453 0.249956876039505
CurrentTrain: epoch  2, batch     3 | loss: 3.4934146Losses:  3.1598119735717773 0.8922162055969238
CurrentTrain: epoch  3, batch     0 | loss: 4.0520282Losses:  2.8844242095947266 0.5936909914016724
CurrentTrain: epoch  3, batch     1 | loss: 3.4781151Losses:  2.715332508087158 0.690629243850708
CurrentTrain: epoch  3, batch     2 | loss: 3.4059618Losses:  4.541353225708008 0.06405621767044067
CurrentTrain: epoch  3, batch     3 | loss: 4.6054096Losses:  2.9418301582336426 0.877246081829071
CurrentTrain: epoch  4, batch     0 | loss: 3.8190763Losses:  2.6570770740509033 0.7251169681549072
CurrentTrain: epoch  4, batch     1 | loss: 3.3821940Losses:  2.4077415466308594 0.6750233769416809
CurrentTrain: epoch  4, batch     2 | loss: 3.0827649Losses:  2.4454433917999268 1.1920930376163597e-07
CurrentTrain: epoch  4, batch     3 | loss: 2.4454436Losses:  2.2942306995391846 0.5721403360366821
CurrentTrain: epoch  5, batch     0 | loss: 2.8663712Losses:  2.783877372741699 0.7506383657455444
CurrentTrain: epoch  5, batch     1 | loss: 3.5345159Losses:  2.2991676330566406 0.5172330141067505
CurrentTrain: epoch  5, batch     2 | loss: 2.8164005Losses:  2.0731489658355713 0.0
CurrentTrain: epoch  5, batch     3 | loss: 2.0731490Losses:  2.106654644012451 0.6269116997718811
CurrentTrain: epoch  6, batch     0 | loss: 2.7335663Losses:  2.337869167327881 0.515807569026947
CurrentTrain: epoch  6, batch     1 | loss: 2.8536768Losses:  2.51680326461792 0.6513286828994751
CurrentTrain: epoch  6, batch     2 | loss: 3.1681318Losses:  2.3501510620117188 0.2825467586517334
CurrentTrain: epoch  6, batch     3 | loss: 2.6326978Losses:  2.321559429168701 0.6220957636833191
CurrentTrain: epoch  7, batch     0 | loss: 2.9436553Losses:  2.2760560512542725 0.5262907147407532
CurrentTrain: epoch  7, batch     1 | loss: 2.8023467Losses:  2.2623331546783447 0.42622825503349304
CurrentTrain: epoch  7, batch     2 | loss: 2.6885614Losses:  1.8408622741699219 0.142417773604393
CurrentTrain: epoch  7, batch     3 | loss: 1.9832801Losses:  2.4070563316345215 0.530372142791748
CurrentTrain: epoch  8, batch     0 | loss: 2.9374285Losses:  2.042093515396118 0.3825477361679077
CurrentTrain: epoch  8, batch     1 | loss: 2.4246411Losses:  1.9072259664535522 0.5340414643287659
CurrentTrain: epoch  8, batch     2 | loss: 2.4412675Losses:  2.4084677696228027 0.016766080632805824
CurrentTrain: epoch  8, batch     3 | loss: 2.4252338Losses:  2.016737461090088 0.5653817653656006
CurrentTrain: epoch  9, batch     0 | loss: 2.5821192Losses:  1.892363429069519 0.5222331881523132
CurrentTrain: epoch  9, batch     1 | loss: 2.4145966Losses:  2.0951461791992188 0.4339819550514221
CurrentTrain: epoch  9, batch     2 | loss: 2.5291281Losses:  1.9162347316741943 0.21115195751190186
CurrentTrain: epoch  9, batch     3 | loss: 2.1273866
Losses:  5.921717643737793 0.7355180978775024
MemoryTrain:  epoch  0, batch     0 | loss: 6.6572356Losses:  9.392505645751953 0.8358676433563232
MemoryTrain:  epoch  0, batch     1 | loss: 10.2283735Losses:  10.234781265258789 0.9287546277046204
MemoryTrain:  epoch  0, batch     2 | loss: 11.1635361Losses:  11.580409049987793 0.5540415048599243
MemoryTrain:  epoch  0, batch     3 | loss: 12.1344509Losses:  1.6095550060272217 0.7822331190109253
MemoryTrain:  epoch  1, batch     0 | loss: 2.3917880Losses:  0.8426458835601807 0.8085488677024841
MemoryTrain:  epoch  1, batch     1 | loss: 1.6511948Losses:  1.0518039464950562 0.7188817262649536
MemoryTrain:  epoch  1, batch     2 | loss: 1.7706857Losses:  0.7710399627685547 0.6842128038406372
MemoryTrain:  epoch  1, batch     3 | loss: 1.4552528Losses:  1.05336332321167 0.9234575033187866
MemoryTrain:  epoch  2, batch     0 | loss: 1.9768208Losses:  1.06320059299469 0.8796558380126953
MemoryTrain:  epoch  2, batch     1 | loss: 1.9428564Losses:  0.610682487487793 0.720647931098938
MemoryTrain:  epoch  2, batch     2 | loss: 1.3313304Losses:  0.7193376421928406 0.4550451934337616
MemoryTrain:  epoch  2, batch     3 | loss: 1.1743828Losses:  1.0498645305633545 0.8293359279632568
MemoryTrain:  epoch  3, batch     0 | loss: 1.8792005Losses:  0.6169655323028564 0.783095121383667
MemoryTrain:  epoch  3, batch     1 | loss: 1.4000607Losses:  0.5677564740180969 0.7998702526092529
MemoryTrain:  epoch  3, batch     2 | loss: 1.3676267Losses:  0.6504663825035095 0.44026291370391846
MemoryTrain:  epoch  3, batch     3 | loss: 1.0907292Losses:  0.6645483374595642 0.979145884513855
MemoryTrain:  epoch  4, batch     0 | loss: 1.6436942Losses:  0.6932077407836914 0.8693724274635315
MemoryTrain:  epoch  4, batch     1 | loss: 1.5625801Losses:  0.5763797163963318 0.7014157176017761
MemoryTrain:  epoch  4, batch     2 | loss: 1.2777954Losses:  0.6135729551315308 0.3901950716972351
MemoryTrain:  epoch  4, batch     3 | loss: 1.0037680Losses:  0.6378228664398193 0.8223534226417542
MemoryTrain:  epoch  5, batch     0 | loss: 1.4601762Losses:  0.40725600719451904 0.7729695439338684
MemoryTrain:  epoch  5, batch     1 | loss: 1.1802256Losses:  0.46758145093917847 0.766103982925415
MemoryTrain:  epoch  5, batch     2 | loss: 1.2336855Losses:  0.5828415751457214 0.5466966032981873
MemoryTrain:  epoch  5, batch     3 | loss: 1.1295382Losses:  0.4597446024417877 0.8799945116043091
MemoryTrain:  epoch  6, batch     0 | loss: 1.3397391Losses:  0.5267132520675659 0.7234107255935669
MemoryTrain:  epoch  6, batch     1 | loss: 1.2501240Losses:  0.3628220558166504 0.5497157573699951
MemoryTrain:  epoch  6, batch     2 | loss: 0.9125378Losses:  0.6041208505630493 0.6262179613113403
MemoryTrain:  epoch  6, batch     3 | loss: 1.2303388Losses:  0.4717264771461487 0.6629661917686462
MemoryTrain:  epoch  7, batch     0 | loss: 1.1346927Losses:  0.6467563509941101 0.9142025113105774
MemoryTrain:  epoch  7, batch     1 | loss: 1.5609589Losses:  0.40883153676986694 0.7235724925994873
MemoryTrain:  epoch  7, batch     2 | loss: 1.1324041Losses:  0.3930049538612366 0.5216546058654785
MemoryTrain:  epoch  7, batch     3 | loss: 0.9146596Losses:  0.5343060493469238 0.7596888542175293
MemoryTrain:  epoch  8, batch     0 | loss: 1.2939949Losses:  0.41064462065696716 0.573344886302948
MemoryTrain:  epoch  8, batch     1 | loss: 0.9839895Losses:  0.4121982455253601 0.7086575031280518
MemoryTrain:  epoch  8, batch     2 | loss: 1.1208558Losses:  0.4144873023033142 0.7819737195968628
MemoryTrain:  epoch  8, batch     3 | loss: 1.1964610Losses:  0.4374638497829437 0.7140985727310181
MemoryTrain:  epoch  9, batch     0 | loss: 1.1515625Losses:  0.3873968720436096 0.6652534008026123
MemoryTrain:  epoch  9, batch     1 | loss: 1.0526502Losses:  0.4182242751121521 0.6545618176460266
MemoryTrain:  epoch  9, batch     2 | loss: 1.0727861Losses:  0.5346466302871704 0.6270468235015869
MemoryTrain:  epoch  9, batch     3 | loss: 1.1616935
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 65.26%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 64.02%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 63.78%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 61.17%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 59.44%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 58.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 59.68%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.46%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 61.20%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 64.01%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 64.30%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 65.28%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.41%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.26%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.22%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.47%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 90.68%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 89.66%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 88.67%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 88.02%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 87.40%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 87.20%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 86.41%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 85.35%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 84.52%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 83.62%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 83.02%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 82.26%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 81.79%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 81.61%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.43%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 80.90%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 80.65%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 80.49%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.58%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 80.67%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 80.36%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 80.29%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 80.22%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 79.69%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 79.71%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 79.12%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 78.16%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 77.23%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 76.32%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 75.44%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 74.64%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 74.36%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 77.10%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 76.84%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 76.44%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 76.43%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 76.24%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.82%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 75.23%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 74.66%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 74.09%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 73.82%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 73.38%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 73.65%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.77%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 73.94%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 73.33%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 72.83%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 72.34%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 71.85%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 71.37%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 71.00%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 70.88%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 70.72%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 70.56%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 70.35%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 70.23%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 70.52%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 70.65%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.82%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 70.32%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 69.87%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 69.46%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 68.97%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 68.53%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 68.23%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 69.04%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 68.67%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.22%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 67.50%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 67.16%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.49%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 67.77%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 67.58%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 67.33%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 67.15%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 67.01%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 66.99%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 67.00%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 66.86%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.73%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 66.63%   [EVAL] batch:  174 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 66.40%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 66.34%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.60%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.75%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 67.18%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 66.87%   [EVAL] batch:  189 | acc: 25.00%,  total acc: 66.64%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 66.36%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 66.11%   [EVAL] batch:  192 | acc: 18.75%,  total acc: 65.87%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 65.85%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 65.99%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 66.34%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 66.20%   [EVAL] batch:  201 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 65.73%   [EVAL] batch:  203 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 65.37%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 65.20%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.80%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 66.45%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.26%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  224 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 67.37%   [EVAL] batch:  226 | acc: 12.50%,  total acc: 67.13%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 66.92%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 66.66%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 66.50%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 66.62%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 67.28%   [EVAL] batch:  238 | acc: 31.25%,  total acc: 67.13%   [EVAL] batch:  239 | acc: 31.25%,  total acc: 66.98%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 66.83%   [EVAL] batch:  241 | acc: 25.00%,  total acc: 66.66%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 66.54%   [EVAL] batch:  243 | acc: 25.00%,  total acc: 66.37%   [EVAL] batch:  244 | acc: 56.25%,  total acc: 66.33%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 66.40%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 66.16%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 65.92%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 65.69%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 65.45%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 65.20%   [EVAL] batch:  255 | acc: 12.50%,  total acc: 64.99%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 64.98%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 65.41%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 65.40%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 65.32%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 65.26%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 65.27%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 65.26%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 65.14%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 64.96%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 64.77%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 64.55%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 64.32%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 64.15%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 63.94%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 63.73%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 63.52%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 63.29%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 63.06%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:  279 | acc: 0.00%,  total acc: 62.61%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 62.39%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 62.48%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 62.54%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 62.65%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 62.70%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 62.76%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 62.87%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 62.95%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 63.27%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 63.48%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 63.43%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 63.34%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:  299 | acc: 62.50%,  total acc: 63.23%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 63.16%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 63.32%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 63.32%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 63.36%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 63.27%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 63.15%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:  311 | acc: 62.50%,  total acc: 63.10%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 63.00%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 62.94%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 62.86%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 62.80%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 62.80%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 62.76%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 62.74%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 63.08%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 63.25%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 63.25%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 63.26%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 63.30%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 63.37%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 63.40%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 63.34%   [EVAL] batch:  333 | acc: 81.25%,  total acc: 63.40%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 63.38%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 63.40%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 63.30%   [EVAL] batch:  344 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 63.26%   [EVAL] batch:  347 | acc: 50.00%,  total acc: 63.22%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 63.23%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 63.19%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:  352 | acc: 50.00%,  total acc: 63.16%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 63.15%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 63.17%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 63.17%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 63.13%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 62.99%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 62.87%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 62.73%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 62.64%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 62.55%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 62.53%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 63.21%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 63.22%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 63.37%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 63.45%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 63.52%   
cur_acc:  ['0.9484', '0.7113', '0.6736', '0.6726', '0.5387', '0.6528']
his_acc:  ['0.9484', '0.8275', '0.7513', '0.6977', '0.6516', '0.6352']
Clustering into  34  clusters
Clusters:  [ 0  1 17  0  0  0 29  0 31 32 26  0 20  0 33 23 19  0  0  0 21 24  2  0
  0 18  0 25  0 30  0 12  0 16 14  0  0  0 11 22 27  0  0  2  2 15  0  0
  0  0  0  0  0 10  0  9  0  5 28 13  1  8  0  0  0  4  0  7  6  3]
Losses:  6.6716132164001465 1.4228018522262573
CurrentTrain: epoch  0, batch     0 | loss: 8.0944147Losses:  7.613365173339844 1.2751593589782715
CurrentTrain: epoch  0, batch     1 | loss: 8.8885250Losses:  7.078862190246582 1.4100072383880615
CurrentTrain: epoch  0, batch     2 | loss: 8.4888697Losses:  4.111229419708252 0.1018349751830101
CurrentTrain: epoch  0, batch     3 | loss: 4.2130642Losses:  2.9991369247436523 1.0539392232894897
CurrentTrain: epoch  1, batch     0 | loss: 4.0530763Losses:  3.2488694190979004 1.2710951566696167
CurrentTrain: epoch  1, batch     1 | loss: 4.5199647Losses:  3.453975200653076 0.9513804912567139
CurrentTrain: epoch  1, batch     2 | loss: 4.4053555Losses:  5.060882568359375 0.28286731243133545
CurrentTrain: epoch  1, batch     3 | loss: 5.3437500Losses:  2.9224696159362793 1.038618564605713
CurrentTrain: epoch  2, batch     0 | loss: 3.9610882Losses:  3.025080442428589 0.8525673151016235
CurrentTrain: epoch  2, batch     1 | loss: 3.8776479Losses:  3.189344882965088 1.046154499053955
CurrentTrain: epoch  2, batch     2 | loss: 4.2354994Losses:  1.9908350706100464 0.19714590907096863
CurrentTrain: epoch  2, batch     3 | loss: 2.1879809Losses:  2.9873929023742676 0.847136378288269
CurrentTrain: epoch  3, batch     0 | loss: 3.8345294Losses:  3.061026096343994 0.9696235060691833
CurrentTrain: epoch  3, batch     1 | loss: 4.0306497Losses:  2.2121102809906006 0.7861542105674744
CurrentTrain: epoch  3, batch     2 | loss: 2.9982646Losses:  2.1260173320770264 0.10135893523693085
CurrentTrain: epoch  3, batch     3 | loss: 2.2273762Losses:  2.222703218460083 0.6870176792144775
CurrentTrain: epoch  4, batch     0 | loss: 2.9097209Losses:  2.8137965202331543 0.7670996785163879
CurrentTrain: epoch  4, batch     1 | loss: 3.5808961Losses:  2.449930429458618 0.8200281858444214
CurrentTrain: epoch  4, batch     2 | loss: 3.2699585Losses:  2.482786178588867 0.26252490282058716
CurrentTrain: epoch  4, batch     3 | loss: 2.7453110Losses:  2.3430304527282715 0.6782145500183105
CurrentTrain: epoch  5, batch     0 | loss: 3.0212450Losses:  2.612264394760132 0.8339909315109253
CurrentTrain: epoch  5, batch     1 | loss: 3.4462552Losses:  2.093311309814453 0.5172456502914429
CurrentTrain: epoch  5, batch     2 | loss: 2.6105571Losses:  1.8691110610961914 0.08926299959421158
CurrentTrain: epoch  5, batch     3 | loss: 1.9583740Losses:  2.426774501800537 0.7142994403839111
CurrentTrain: epoch  6, batch     0 | loss: 3.1410739Losses:  2.2364611625671387 0.6796339750289917
CurrentTrain: epoch  6, batch     1 | loss: 2.9160953Losses:  1.9466925859451294 0.5668784379959106
CurrentTrain: epoch  6, batch     2 | loss: 2.5135710Losses:  2.276810646057129 0.04511474072933197
CurrentTrain: epoch  6, batch     3 | loss: 2.3219254Losses:  2.1594512462615967 0.6666549444198608
CurrentTrain: epoch  7, batch     0 | loss: 2.8261061Losses:  2.1146340370178223 0.6524841785430908
CurrentTrain: epoch  7, batch     1 | loss: 2.7671182Losses:  2.0771470069885254 0.6339071989059448
CurrentTrain: epoch  7, batch     2 | loss: 2.7110543Losses:  2.451120138168335 0.08209758251905441
CurrentTrain: epoch  7, batch     3 | loss: 2.5332177Losses:  1.8372708559036255 0.5006568431854248
CurrentTrain: epoch  8, batch     0 | loss: 2.3379278Losses:  2.005053997039795 0.6386280655860901
CurrentTrain: epoch  8, batch     1 | loss: 2.6436820Losses:  2.2485241889953613 0.5906755924224854
CurrentTrain: epoch  8, batch     2 | loss: 2.8391998Losses:  1.692777395248413 0.15624520182609558
CurrentTrain: epoch  8, batch     3 | loss: 1.8490226Losses:  1.8821473121643066 0.4421199858188629
CurrentTrain: epoch  9, batch     0 | loss: 2.3242674Losses:  1.845888614654541 0.40138253569602966
CurrentTrain: epoch  9, batch     1 | loss: 2.2472711Losses:  2.1641018390655518 0.601262092590332
CurrentTrain: epoch  9, batch     2 | loss: 2.7653639Losses:  1.803633451461792 5.960464477539063e-08
CurrentTrain: epoch  9, batch     3 | loss: 1.8036335
Losses:  6.121833324432373 0.5692188739776611
MemoryTrain:  epoch  0, batch     0 | loss: 6.6910524Losses:  9.393094062805176 0.7359387874603271
MemoryTrain:  epoch  0, batch     1 | loss: 10.1290331Losses:  10.42993450164795 0.7640975713729858
MemoryTrain:  epoch  0, batch     2 | loss: 11.1940317Losses:  11.137290954589844 0.7754596471786499
MemoryTrain:  epoch  0, batch     3 | loss: 11.9127502Losses:  11.228167533874512 0.445914089679718
MemoryTrain:  epoch  0, batch     4 | loss: 11.6740818Losses:  1.421766996383667 1.0429468154907227
MemoryTrain:  epoch  1, batch     0 | loss: 2.4647138Losses:  0.9959733486175537 0.789996862411499
MemoryTrain:  epoch  1, batch     1 | loss: 1.7859702Losses:  1.2661372423171997 0.5417791604995728
MemoryTrain:  epoch  1, batch     2 | loss: 1.8079164Losses:  1.3503408432006836 0.6078594923019409
MemoryTrain:  epoch  1, batch     3 | loss: 1.9582003Losses:  0.6499364376068115 0.3609273433685303
MemoryTrain:  epoch  1, batch     4 | loss: 1.0108638Losses:  1.3976986408233643 0.702605664730072
MemoryTrain:  epoch  2, batch     0 | loss: 2.1003044Losses:  1.0477906465530396 0.6192638874053955
MemoryTrain:  epoch  2, batch     1 | loss: 1.6670545Losses:  0.7534849643707275 0.8003885746002197
MemoryTrain:  epoch  2, batch     2 | loss: 1.5538735Losses:  0.9975395202636719 0.8875231146812439
MemoryTrain:  epoch  2, batch     3 | loss: 1.8850627Losses:  0.32778045535087585 0.20112566649913788
MemoryTrain:  epoch  2, batch     4 | loss: 0.5289061Losses:  1.0132122039794922 0.7807067632675171
MemoryTrain:  epoch  3, batch     0 | loss: 1.7939190Losses:  0.7449926137924194 0.6051268577575684
MemoryTrain:  epoch  3, batch     1 | loss: 1.3501195Losses:  0.5188325643539429 0.7571689486503601
MemoryTrain:  epoch  3, batch     2 | loss: 1.2760015Losses:  0.8630133867263794 0.7623623609542847
MemoryTrain:  epoch  3, batch     3 | loss: 1.6253757Losses:  1.1796565055847168 0.2346961945295334
MemoryTrain:  epoch  3, batch     4 | loss: 1.4143527Losses:  0.5629709362983704 0.7832934260368347
MemoryTrain:  epoch  4, batch     0 | loss: 1.3462644Losses:  0.8208866119384766 0.928485631942749
MemoryTrain:  epoch  4, batch     1 | loss: 1.7493722Losses:  0.8543761968612671 0.6126513481140137
MemoryTrain:  epoch  4, batch     2 | loss: 1.4670275Losses:  0.5660406351089478 0.7034778594970703
MemoryTrain:  epoch  4, batch     3 | loss: 1.2695185Losses:  0.5074453353881836 0.14566487073898315
MemoryTrain:  epoch  4, batch     4 | loss: 0.6531102Losses:  0.43722429871559143 0.599060595035553
MemoryTrain:  epoch  5, batch     0 | loss: 1.0362849Losses:  0.6343192458152771 0.7876352071762085
MemoryTrain:  epoch  5, batch     1 | loss: 1.4219544Losses:  0.5661312341690063 0.5929592847824097
MemoryTrain:  epoch  5, batch     2 | loss: 1.1590905Losses:  0.6074721813201904 0.7168077230453491
MemoryTrain:  epoch  5, batch     3 | loss: 1.3242799Losses:  0.7079087495803833 0.3789448142051697
MemoryTrain:  epoch  5, batch     4 | loss: 1.0868535Losses:  0.5327785611152649 0.6974111795425415
MemoryTrain:  epoch  6, batch     0 | loss: 1.2301898Losses:  0.4127960801124573 0.7318847179412842
MemoryTrain:  epoch  6, batch     1 | loss: 1.1446807Losses:  0.5823286175727844 0.6343628168106079
MemoryTrain:  epoch  6, batch     2 | loss: 1.2166915Losses:  0.5546741485595703 0.6997860670089722
MemoryTrain:  epoch  6, batch     3 | loss: 1.2544602Losses:  0.5161968469619751 0.39958178997039795
MemoryTrain:  epoch  6, batch     4 | loss: 0.9157786Losses:  0.5708813071250916 0.932174563407898
MemoryTrain:  epoch  7, batch     0 | loss: 1.5030558Losses:  0.47671353816986084 0.7403997182846069
MemoryTrain:  epoch  7, batch     1 | loss: 1.2171133Losses:  0.34781983494758606 0.44617220759391785
MemoryTrain:  epoch  7, batch     2 | loss: 0.7939920Losses:  0.5016902089118958 0.6882241368293762
MemoryTrain:  epoch  7, batch     3 | loss: 1.1899143Losses:  0.4057294726371765 0.35716569423675537
MemoryTrain:  epoch  7, batch     4 | loss: 0.7628952Losses:  0.46850669384002686 0.6117048263549805
MemoryTrain:  epoch  8, batch     0 | loss: 1.0802115Losses:  0.4135381877422333 0.6253718137741089
MemoryTrain:  epoch  8, batch     1 | loss: 1.0389100Losses:  0.47001615166664124 0.6158546209335327
MemoryTrain:  epoch  8, batch     2 | loss: 1.0858707Losses:  0.47606706619262695 0.6869537830352783
MemoryTrain:  epoch  8, batch     3 | loss: 1.1630208Losses:  0.5593897700309753 0.39163774251937866
MemoryTrain:  epoch  8, batch     4 | loss: 0.9510275Losses:  0.41063880920410156 0.6678774952888489
MemoryTrain:  epoch  9, batch     0 | loss: 1.0785162Losses:  0.4556467831134796 0.6719490885734558
MemoryTrain:  epoch  9, batch     1 | loss: 1.1275959Losses:  0.42265069484710693 0.6086827516555786
MemoryTrain:  epoch  9, batch     2 | loss: 1.0313334Losses:  0.4702894985675812 0.7401137351989746
MemoryTrain:  epoch  9, batch     3 | loss: 1.2104032Losses:  0.37388595938682556 0.35205745697021484
MemoryTrain:  epoch  9, batch     4 | loss: 0.7259434
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 69.44%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 61.28%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 59.97%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 59.87%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 60.10%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 60.82%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.31%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 61.77%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 62.08%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 61.97%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 61.98%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 61.99%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.13%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.65%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 87.91%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 87.77%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 87.76%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.34%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 88.19%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 87.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 87.83%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 87.17%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 86.21%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 85.38%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 84.79%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.97%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 83.13%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 81.93%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 80.96%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 80.02%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 79.20%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 78.31%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 77.63%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.59%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 77.46%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 77.00%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 76.71%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 76.60%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 76.68%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 76.66%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 76.33%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 75.91%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 74.11%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 73.24%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 72.38%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 71.62%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 71.38%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 72.18%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 74.14%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 73.97%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 73.80%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 73.53%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 73.13%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 72.57%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 72.02%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 71.48%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 71.23%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 70.81%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 70.74%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 71.12%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 70.94%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 70.45%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 69.98%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 69.51%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 69.05%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 68.65%   [EVAL] batch:  125 | acc: 43.75%,  total acc: 68.45%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.26%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 68.07%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 67.88%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 67.93%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 67.89%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 68.04%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 68.57%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 68.08%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 67.24%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 66.77%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 66.35%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 66.06%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 66.97%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 66.61%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 66.18%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 65.48%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 65.06%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 65.66%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 66.08%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 65.81%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 65.64%   [EVAL] batch:  167 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 65.38%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 65.37%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 64.98%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 64.81%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 64.69%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 64.54%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 64.39%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 64.41%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 64.33%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.52%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 64.71%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.86%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.05%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 64.95%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 64.70%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 64.10%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 63.89%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 63.72%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 63.78%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 64.09%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 64.43%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 64.17%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 64.04%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 63.91%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 63.72%   [EVAL] batch:  205 | acc: 31.25%,  total acc: 63.56%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 63.85%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 64.83%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 65.01%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  218 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 65.57%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 65.85%   [EVAL] batch:  226 | acc: 18.75%,  total acc: 65.64%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 65.54%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 65.16%   [EVAL] batch:  230 | acc: 25.00%,  total acc: 64.99%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 65.11%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 65.36%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 65.66%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  238 | acc: 31.25%,  total acc: 65.48%   [EVAL] batch:  239 | acc: 31.25%,  total acc: 65.34%   [EVAL] batch:  240 | acc: 25.00%,  total acc: 65.17%   [EVAL] batch:  241 | acc: 18.75%,  total acc: 64.98%   [EVAL] batch:  242 | acc: 37.50%,  total acc: 64.87%   [EVAL] batch:  243 | acc: 18.75%,  total acc: 64.68%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 64.95%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 64.72%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 64.48%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 64.25%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 64.05%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 63.80%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 63.57%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 63.57%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 63.98%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 64.05%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 64.04%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 63.97%   [EVAL] batch:  264 | acc: 43.75%,  total acc: 63.89%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 63.91%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 63.90%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 63.81%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 63.64%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 63.45%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 63.24%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 63.01%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 62.84%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 62.64%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 62.43%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 62.23%   [EVAL] batch:  276 | acc: 0.00%,  total acc: 62.00%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 61.78%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 61.56%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 61.36%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 61.17%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 61.17%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 61.26%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 61.31%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 61.43%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 61.47%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 61.68%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:  290 | acc: 93.75%,  total acc: 62.01%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 62.07%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  294 | acc: 0.00%,  total acc: 62.03%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 61.66%   [EVAL] batch:  297 | acc: 12.50%,  total acc: 61.49%   [EVAL] batch:  298 | acc: 0.00%,  total acc: 61.29%   [EVAL] batch:  299 | acc: 0.00%,  total acc: 61.08%   [EVAL] batch:  300 | acc: 31.25%,  total acc: 60.98%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 61.08%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 61.16%   [EVAL] batch:  304 | acc: 62.50%,  total acc: 61.17%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 61.21%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 61.18%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 61.16%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 61.10%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 61.15%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 61.13%   [EVAL] batch:  311 | acc: 62.50%,  total acc: 61.14%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 61.08%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 61.05%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 60.95%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 60.90%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 60.88%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 60.85%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 60.82%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 61.04%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 61.08%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 61.15%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 61.21%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 61.27%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 61.29%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 61.28%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 61.34%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 61.38%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 61.44%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 61.48%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 61.48%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 61.43%   [EVAL] batch:  333 | acc: 68.75%,  total acc: 61.45%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 61.44%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 61.44%   [EVAL] batch:  336 | acc: 50.00%,  total acc: 61.41%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 61.39%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 61.41%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 61.40%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 61.47%   [EVAL] batch:  341 | acc: 56.25%,  total acc: 61.46%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 61.42%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 61.36%   [EVAL] batch:  344 | acc: 43.75%,  total acc: 61.30%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 61.25%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 61.26%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 61.24%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 61.26%   [EVAL] batch:  349 | acc: 43.75%,  total acc: 61.21%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 61.16%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 61.17%   [EVAL] batch:  352 | acc: 43.75%,  total acc: 61.12%   [EVAL] batch:  353 | acc: 62.50%,  total acc: 61.12%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 61.14%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 61.17%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 61.12%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 60.98%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 60.86%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 60.73%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 60.63%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 60.55%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 60.52%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 60.63%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 60.74%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 60.84%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 60.95%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 61.16%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 61.23%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 61.29%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 61.37%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 61.44%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 61.53%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 61.58%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 61.60%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 61.64%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 61.66%   [EVAL] batch:  378 | acc: 56.25%,  total acc: 61.64%   [EVAL] batch:  379 | acc: 68.75%,  total acc: 61.66%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 61.73%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 61.70%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 61.72%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 61.74%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 61.74%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 61.69%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 61.71%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 61.79%   [EVAL] batch:  388 | acc: 31.25%,  total acc: 61.71%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 61.71%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 61.70%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 61.67%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 61.63%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 61.68%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 61.76%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 62.04%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 62.12%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 62.19%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 62.11%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 62.11%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 62.11%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 62.05%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 62.01%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 61.99%   [EVAL] batch:  407 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 61.80%   [EVAL] batch:  409 | acc: 18.75%,  total acc: 61.69%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 61.56%   [EVAL] batch:  411 | acc: 12.50%,  total acc: 61.44%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 61.43%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 61.49%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 61.51%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 61.56%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 61.60%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 61.63%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 61.64%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 61.65%   [EVAL] batch:  421 | acc: 50.00%,  total acc: 61.63%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 61.63%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 61.63%   [EVAL] batch:  424 | acc: 68.75%,  total acc: 61.65%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 62.18%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 62.33%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 62.49%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 62.54%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 62.60%   
cur_acc:  ['0.9484', '0.7113', '0.6736', '0.6726', '0.5387', '0.6528', '0.6865']
his_acc:  ['0.9484', '0.8275', '0.7513', '0.6977', '0.6516', '0.6352', '0.6260']
Clustering into  39  clusters
Clusters:  [ 0  5 24  0  0  0 33  0 21 38 26  0 22  0 19 25 35  0  0  0 32 27  1  0
  0 37  0 29  0 34  2 31  0  0 36  0  0  0 20 23 17  0  0  1  1  9  0  0
  0  0  0  0  0 12  0 16  0 11 15 30  5 14  0  0  0  2  0 18 28 10  8  4
  0  0 13  7  6  0  0  3]
Losses:  6.462982654571533 1.3671693801879883
CurrentTrain: epoch  0, batch     0 | loss: 7.8301520Losses:  7.904132843017578 1.4113231897354126
CurrentTrain: epoch  0, batch     1 | loss: 9.3154564Losses:  6.889972686767578 1.4875335693359375
CurrentTrain: epoch  0, batch     2 | loss: 8.3775063Losses:  4.879376411437988 0.6003296375274658
CurrentTrain: epoch  0, batch     3 | loss: 5.4797058Losses:  3.353268623352051 1.4352784156799316
CurrentTrain: epoch  1, batch     0 | loss: 4.7885470Losses:  2.6672873497009277 1.4000396728515625
CurrentTrain: epoch  1, batch     1 | loss: 4.0673270Losses:  2.442901611328125 1.1303452253341675
CurrentTrain: epoch  1, batch     2 | loss: 3.5732470Losses:  3.295093536376953 0.14477156102657318
CurrentTrain: epoch  1, batch     3 | loss: 3.4398651Losses:  2.5189878940582275 1.0713658332824707
CurrentTrain: epoch  2, batch     0 | loss: 3.5903537Losses:  2.477830410003662 1.0424318313598633
CurrentTrain: epoch  2, batch     1 | loss: 3.5202622Losses:  2.7339589595794678 1.1707401275634766
CurrentTrain: epoch  2, batch     2 | loss: 3.9046991Losses:  1.883227825164795 0.027419541031122208
CurrentTrain: epoch  2, batch     3 | loss: 1.9106474Losses:  2.447537422180176 0.8617815971374512
CurrentTrain: epoch  3, batch     0 | loss: 3.3093190Losses:  2.477835178375244 1.075842261314392
CurrentTrain: epoch  3, batch     1 | loss: 3.5536776Losses:  2.061579942703247 0.9943678379058838
CurrentTrain: epoch  3, batch     2 | loss: 3.0559478Losses:  1.9686274528503418 2.3841860752327193e-07
CurrentTrain: epoch  3, batch     3 | loss: 1.9686277Losses:  1.9786111116409302 0.7462698221206665
CurrentTrain: epoch  4, batch     0 | loss: 2.7248809Losses:  2.360544204711914 0.8497105836868286
CurrentTrain: epoch  4, batch     1 | loss: 3.2102547Losses:  2.18709135055542 0.7574048042297363
CurrentTrain: epoch  4, batch     2 | loss: 2.9444962Losses:  2.258357524871826 0.1352691799402237
CurrentTrain: epoch  4, batch     3 | loss: 2.3936267Losses:  2.2022976875305176 0.820175290107727
CurrentTrain: epoch  5, batch     0 | loss: 3.0224729Losses:  2.2143137454986572 0.8903325796127319
CurrentTrain: epoch  5, batch     1 | loss: 3.1046462Losses:  2.0053727626800537 0.8653702139854431
CurrentTrain: epoch  5, batch     2 | loss: 2.8707430Losses:  1.7864134311676025 0.03701737895607948
CurrentTrain: epoch  5, batch     3 | loss: 1.8234308Losses:  2.2265851497650146 0.8292180299758911
CurrentTrain: epoch  6, batch     0 | loss: 3.0558033Losses:  1.9880859851837158 0.6578480005264282
CurrentTrain: epoch  6, batch     1 | loss: 2.6459341Losses:  1.9390664100646973 0.741296648979187
CurrentTrain: epoch  6, batch     2 | loss: 2.6803632Losses:  1.958036184310913 0.1110757440328598
CurrentTrain: epoch  6, batch     3 | loss: 2.0691118Losses:  1.9140770435333252 0.5623229146003723
CurrentTrain: epoch  7, batch     0 | loss: 2.4763999Losses:  1.9252126216888428 0.8488178849220276
CurrentTrain: epoch  7, batch     1 | loss: 2.7740304Losses:  2.04792857170105 0.6288861036300659
CurrentTrain: epoch  7, batch     2 | loss: 2.6768146Losses:  2.189832925796509 0.03770499676465988
CurrentTrain: epoch  7, batch     3 | loss: 2.2275379Losses:  1.977706789970398 0.6492663621902466
CurrentTrain: epoch  8, batch     0 | loss: 2.6269732Losses:  1.835998773574829 0.5033984780311584
CurrentTrain: epoch  8, batch     1 | loss: 2.3393972Losses:  1.9441338777542114 0.6401000022888184
CurrentTrain: epoch  8, batch     2 | loss: 2.5842338Losses:  2.1507344245910645 0.10095198452472687
CurrentTrain: epoch  8, batch     3 | loss: 2.2516863Losses:  2.03189754486084 0.6314753890037537
CurrentTrain: epoch  9, batch     0 | loss: 2.6633730Losses:  1.7624152898788452 0.38786017894744873
CurrentTrain: epoch  9, batch     1 | loss: 2.1502755Losses:  1.9623796939849854 0.5199216604232788
CurrentTrain: epoch  9, batch     2 | loss: 2.4823012Losses:  1.8387057781219482 0.03005729615688324
CurrentTrain: epoch  9, batch     3 | loss: 1.8687631
Losses:  5.977352619171143 0.7459352612495422
MemoryTrain:  epoch  0, batch     0 | loss: 6.7232881Losses:  8.857583045959473 0.655741274356842
MemoryTrain:  epoch  0, batch     1 | loss: 9.5133247Losses:  9.347850799560547 0.7234321236610413
MemoryTrain:  epoch  0, batch     2 | loss: 10.0712833Losses:  10.270099639892578 0.8153203725814819
MemoryTrain:  epoch  0, batch     3 | loss: 11.0854197Losses:  10.849441528320312 0.7350339293479919
MemoryTrain:  epoch  0, batch     4 | loss: 11.5844755Losses:  0.7420103549957275 0.6335091590881348
MemoryTrain:  epoch  1, batch     0 | loss: 1.3755195Losses:  0.7381235361099243 0.760520339012146
MemoryTrain:  epoch  1, batch     1 | loss: 1.4986439Losses:  0.9176796674728394 0.8464211225509644
MemoryTrain:  epoch  1, batch     2 | loss: 1.7641008Losses:  0.9734481573104858 0.8785479068756104
MemoryTrain:  epoch  1, batch     3 | loss: 1.8519961Losses:  0.7880144119262695 0.5602097511291504
MemoryTrain:  epoch  1, batch     4 | loss: 1.3482242Losses:  0.7891282439231873 0.77359938621521
MemoryTrain:  epoch  2, batch     0 | loss: 1.5627277Losses:  0.5754995942115784 0.6229261159896851
MemoryTrain:  epoch  2, batch     1 | loss: 1.1984258Losses:  0.5537692308425903 0.6157230734825134
MemoryTrain:  epoch  2, batch     2 | loss: 1.1694922Losses:  0.621099591255188 0.7008296847343445
MemoryTrain:  epoch  2, batch     3 | loss: 1.3219292Losses:  0.8519325256347656 0.7873399257659912
MemoryTrain:  epoch  2, batch     4 | loss: 1.6392725Losses:  0.736388087272644 0.7075281143188477
MemoryTrain:  epoch  3, batch     0 | loss: 1.4439162Losses:  0.4834981858730316 0.7113744616508484
MemoryTrain:  epoch  3, batch     1 | loss: 1.1948726Losses:  0.5914115309715271 0.7469103932380676
MemoryTrain:  epoch  3, batch     2 | loss: 1.3383219Losses:  0.43367767333984375 0.6092811822891235
MemoryTrain:  epoch  3, batch     3 | loss: 1.0429589Losses:  0.5841498374938965 0.751389741897583
MemoryTrain:  epoch  3, batch     4 | loss: 1.3355396Losses:  0.42223429679870605 0.5343553423881531
MemoryTrain:  epoch  4, batch     0 | loss: 0.9565896Losses:  0.48500463366508484 0.6657990217208862
MemoryTrain:  epoch  4, batch     1 | loss: 1.1508037Losses:  0.4974908232688904 0.6453068256378174
MemoryTrain:  epoch  4, batch     2 | loss: 1.1427977Losses:  0.6074809432029724 0.7781844735145569
MemoryTrain:  epoch  4, batch     3 | loss: 1.3856654Losses:  0.5950331091880798 0.6753534078598022
MemoryTrain:  epoch  4, batch     4 | loss: 1.2703865Losses:  0.4466894567012787 0.5632368326187134
MemoryTrain:  epoch  5, batch     0 | loss: 1.0099263Losses:  0.4418826699256897 0.620199978351593
MemoryTrain:  epoch  5, batch     1 | loss: 1.0620826Losses:  0.4196731448173523 0.5797545909881592
MemoryTrain:  epoch  5, batch     2 | loss: 0.9994277Losses:  0.6637526154518127 0.8866761326789856
MemoryTrain:  epoch  5, batch     3 | loss: 1.5504287Losses:  0.5101603269577026 0.6066470146179199
MemoryTrain:  epoch  5, batch     4 | loss: 1.1168073Losses:  0.6103007197380066 0.749925971031189
MemoryTrain:  epoch  6, batch     0 | loss: 1.3602266Losses:  0.39882808923721313 0.48745861649513245
MemoryTrain:  epoch  6, batch     1 | loss: 0.8862867Losses:  0.5236866474151611 0.6873469352722168
MemoryTrain:  epoch  6, batch     2 | loss: 1.2110336Losses:  0.457089900970459 0.6708137392997742
MemoryTrain:  epoch  6, batch     3 | loss: 1.1279037Losses:  0.39757639169692993 0.5929034948348999
MemoryTrain:  epoch  6, batch     4 | loss: 0.9904799Losses:  0.4953157305717468 0.702131986618042
MemoryTrain:  epoch  7, batch     0 | loss: 1.1974478Losses:  0.5121201276779175 0.7795298099517822
MemoryTrain:  epoch  7, batch     1 | loss: 1.2916499Losses:  0.38741981983184814 0.5474753975868225
MemoryTrain:  epoch  7, batch     2 | loss: 0.9348952Losses:  0.43588221073150635 0.550615668296814
MemoryTrain:  epoch  7, batch     3 | loss: 0.9864979Losses:  0.46171584725379944 0.6524816155433655
MemoryTrain:  epoch  7, batch     4 | loss: 1.1141975Losses:  0.4605735242366791 0.5025492906570435
MemoryTrain:  epoch  8, batch     0 | loss: 0.9631228Losses:  0.3709167242050171 0.48801204562187195
MemoryTrain:  epoch  8, batch     1 | loss: 0.8589288Losses:  0.4864812195301056 0.6386356353759766
MemoryTrain:  epoch  8, batch     2 | loss: 1.1251168Losses:  0.5741579532623291 0.8649795651435852
MemoryTrain:  epoch  8, batch     3 | loss: 1.4391375Losses:  0.4151463508605957 0.6292566061019897
MemoryTrain:  epoch  8, batch     4 | loss: 1.0444030Losses:  0.4482376277446747 0.5536034107208252
MemoryTrain:  epoch  9, batch     0 | loss: 1.0018411Losses:  0.41934749484062195 0.5493698716163635
MemoryTrain:  epoch  9, batch     1 | loss: 0.9687173Losses:  0.48910579085350037 0.6390491724014282
MemoryTrain:  epoch  9, batch     2 | loss: 1.1281550Losses:  0.39974677562713623 0.5570899844169617
MemoryTrain:  epoch  9, batch     3 | loss: 0.9568368Losses:  0.4914896786212921 0.6079972982406616
MemoryTrain:  epoch  9, batch     4 | loss: 1.0994869
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.38%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 85.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 84.56%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 84.13%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 84.08%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 83.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 83.18%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 83.37%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 84.07%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 83.53%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.09%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 84.84%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 85.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 85.17%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 84.49%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 82.00%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 80.23%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.04%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 79.27%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 78.12%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 77.21%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 76.33%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 75.65%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 74.82%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 73.70%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 73.54%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 73.62%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 73.72%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 73.40%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 72.52%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 71.65%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 70.81%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 69.99%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 69.25%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 69.11%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 72.40%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 72.18%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 72.03%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 71.81%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 71.90%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 71.76%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 71.44%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 70.89%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 70.41%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 69.89%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 69.59%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 69.67%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 70.01%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 69.06%   [EVAL] batch:  121 | acc: 12.50%,  total acc: 68.60%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 68.14%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 67.69%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 67.30%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 66.98%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 66.85%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 66.73%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 66.65%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 66.81%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 67.01%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 67.07%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.22%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:  138 | acc: 0.00%,  total acc: 66.95%   [EVAL] batch:  139 | acc: 6.25%,  total acc: 66.52%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 66.13%   [EVAL] batch:  141 | acc: 0.00%,  total acc: 65.67%   [EVAL] batch:  142 | acc: 6.25%,  total acc: 65.25%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 64.97%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 65.89%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 65.50%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 64.77%   [EVAL] batch:  154 | acc: 0.00%,  total acc: 64.35%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 63.94%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 64.39%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 64.57%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 64.93%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 64.88%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 64.60%   [EVAL] batch:  164 | acc: 0.00%,  total acc: 64.20%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 63.89%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 63.59%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 63.28%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 63.05%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 63.05%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 63.08%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 62.97%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 62.93%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 62.90%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 62.89%   [EVAL] batch:  175 | acc: 31.25%,  total acc: 62.71%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 62.54%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 62.43%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 62.36%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 62.29%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 62.22%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 62.29%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 62.47%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 62.84%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:  188 | acc: 12.50%,  total acc: 62.90%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 62.66%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 62.37%   [EVAL] batch:  191 | acc: 12.50%,  total acc: 62.11%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 61.92%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 61.79%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 61.86%   [EVAL] batch:  195 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  196 | acc: 93.75%,  total acc: 62.21%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 62.53%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 62.56%   [EVAL] batch:  201 | acc: 12.50%,  total acc: 62.31%   [EVAL] batch:  202 | acc: 37.50%,  total acc: 62.19%   [EVAL] batch:  203 | acc: 25.00%,  total acc: 62.01%   [EVAL] batch:  204 | acc: 25.00%,  total acc: 61.83%   [EVAL] batch:  205 | acc: 25.00%,  total acc: 61.65%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 61.99%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 62.32%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 62.68%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 62.79%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 62.88%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 62.97%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 63.30%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 63.72%   [EVAL] batch:  221 | acc: 81.25%,  total acc: 63.80%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 64.10%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 63.93%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 63.87%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 63.78%   [EVAL] batch:  229 | acc: 12.50%,  total acc: 63.56%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 63.42%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 63.52%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 63.81%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 64.19%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.18%   [EVAL] batch:  238 | acc: 43.75%,  total acc: 64.10%   [EVAL] batch:  239 | acc: 31.25%,  total acc: 63.96%   [EVAL] batch:  240 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:  241 | acc: 18.75%,  total acc: 63.64%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 63.55%   [EVAL] batch:  243 | acc: 31.25%,  total acc: 63.42%   [EVAL] batch:  244 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 63.44%   [EVAL] batch:  246 | acc: 37.50%,  total acc: 63.34%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 63.43%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 63.35%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 62.67%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 62.45%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 62.21%   [EVAL] batch:  255 | acc: 6.25%,  total acc: 61.99%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 61.99%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 62.33%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 62.43%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 62.52%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 62.52%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 62.45%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 62.41%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 62.45%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 62.43%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 62.36%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 62.20%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 62.01%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 61.81%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 61.58%   [EVAL] batch:  272 | acc: 25.00%,  total acc: 61.45%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 61.25%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 61.05%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 60.85%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 60.65%   [EVAL] batch:  277 | acc: 0.00%,  total acc: 60.43%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 60.22%   [EVAL] batch:  279 | acc: 6.25%,  total acc: 60.02%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 59.83%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 59.84%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 59.92%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 59.95%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 60.07%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 60.12%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 60.15%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 60.24%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 60.34%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 60.61%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 60.68%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:  294 | acc: 0.00%,  total acc: 60.66%   [EVAL] batch:  295 | acc: 0.00%,  total acc: 60.45%   [EVAL] batch:  296 | acc: 6.25%,  total acc: 60.27%   [EVAL] batch:  297 | acc: 6.25%,  total acc: 60.09%   [EVAL] batch:  298 | acc: 0.00%,  total acc: 59.89%   [EVAL] batch:  299 | acc: 0.00%,  total acc: 59.69%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 59.63%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 59.71%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 59.76%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 59.81%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 59.80%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 59.84%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 59.79%   [EVAL] batch:  307 | acc: 25.00%,  total acc: 59.68%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 59.63%   [EVAL] batch:  309 | acc: 56.25%,  total acc: 59.62%   [EVAL] batch:  310 | acc: 37.50%,  total acc: 59.55%   [EVAL] batch:  311 | acc: 56.25%,  total acc: 59.54%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 59.44%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 59.43%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 59.40%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 59.41%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 59.40%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 59.53%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 59.64%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 59.69%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 59.77%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 59.84%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 59.92%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 59.95%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 59.92%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 59.95%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 59.99%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 60.04%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 60.10%   [EVAL] batch:  331 | acc: 62.50%,  total acc: 60.11%   [EVAL] batch:  332 | acc: 43.75%,  total acc: 60.06%   [EVAL] batch:  333 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 60.06%   [EVAL] batch:  335 | acc: 62.50%,  total acc: 60.06%   [EVAL] batch:  336 | acc: 43.75%,  total acc: 60.01%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 60.01%   [EVAL] batch:  339 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 60.03%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 60.03%   [EVAL] batch:  342 | acc: 37.50%,  total acc: 59.97%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 59.88%   [EVAL] batch:  344 | acc: 50.00%,  total acc: 59.86%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 59.81%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 59.82%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 59.82%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 59.85%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 59.82%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 59.78%   [EVAL] batch:  351 | acc: 62.50%,  total acc: 59.78%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 59.77%   [EVAL] batch:  353 | acc: 68.75%,  total acc: 59.80%   [EVAL] batch:  354 | acc: 68.75%,  total acc: 59.82%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 59.87%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 59.82%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 59.69%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 59.58%   [EVAL] batch:  359 | acc: 12.50%,  total acc: 59.44%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 59.35%   [EVAL] batch:  361 | acc: 31.25%,  total acc: 59.27%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 59.25%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 59.36%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 59.47%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 59.58%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 59.69%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 59.80%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 59.91%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 59.98%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 60.02%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 60.11%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 60.19%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 60.28%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 60.33%   [EVAL] batch:  375 | acc: 81.25%,  total acc: 60.39%   [EVAL] batch:  376 | acc: 75.00%,  total acc: 60.43%   [EVAL] batch:  377 | acc: 75.00%,  total acc: 60.47%   [EVAL] batch:  378 | acc: 62.50%,  total acc: 60.47%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 60.48%   [EVAL] batch:  380 | acc: 87.50%,  total acc: 60.55%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 60.54%   [EVAL] batch:  382 | acc: 68.75%,  total acc: 60.56%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 60.57%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 60.52%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 60.55%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 60.63%   [EVAL] batch:  388 | acc: 31.25%,  total acc: 60.56%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 60.54%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 60.53%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 60.52%   [EVAL] batch:  392 | acc: 43.75%,  total acc: 60.48%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 60.53%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 60.72%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 60.91%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 61.00%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 61.08%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 61.01%   [EVAL] batch:  402 | acc: 62.50%,  total acc: 61.01%   [EVAL] batch:  403 | acc: 62.50%,  total acc: 61.01%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 60.96%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 60.91%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 60.90%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 60.88%   [EVAL] batch:  408 | acc: 6.25%,  total acc: 60.74%   [EVAL] batch:  409 | acc: 18.75%,  total acc: 60.64%   [EVAL] batch:  410 | acc: 6.25%,  total acc: 60.51%   [EVAL] batch:  411 | acc: 12.50%,  total acc: 60.39%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 60.38%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 60.39%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 60.45%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 60.49%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 60.55%   [EVAL] batch:  417 | acc: 62.50%,  total acc: 60.56%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:  419 | acc: 25.00%,  total acc: 60.49%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 60.45%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 60.41%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 60.39%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 60.38%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 60.48%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 60.57%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 60.66%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 60.84%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 60.93%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 61.01%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 61.13%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 61.21%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 61.27%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 61.43%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 61.50%   [EVAL] batch:  439 | acc: 93.75%,  total acc: 61.58%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 61.66%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 61.79%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 61.88%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 61.94%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 62.01%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 62.07%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 62.15%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 62.25%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  451 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  453 | acc: 43.75%,  total acc: 62.21%   [EVAL] batch:  454 | acc: 37.50%,  total acc: 62.16%   [EVAL] batch:  455 | acc: 43.75%,  total acc: 62.12%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 62.13%   [EVAL] batch:  457 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:  458 | acc: 81.25%,  total acc: 62.19%   [EVAL] batch:  459 | acc: 68.75%,  total acc: 62.20%   [EVAL] batch:  460 | acc: 81.25%,  total acc: 62.24%   [EVAL] batch:  461 | acc: 62.50%,  total acc: 62.24%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 62.30%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 62.46%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 62.54%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  469 | acc: 81.25%,  total acc: 62.82%   [EVAL] batch:  470 | acc: 87.50%,  total acc: 62.87%   [EVAL] batch:  471 | acc: 81.25%,  total acc: 62.91%   [EVAL] batch:  472 | acc: 68.75%,  total acc: 62.92%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 62.95%   [EVAL] batch:  474 | acc: 50.00%,  total acc: 62.92%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 63.37%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  487 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:  488 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 63.83%   [EVAL] batch:  490 | acc: 75.00%,  total acc: 63.85%   [EVAL] batch:  491 | acc: 56.25%,  total acc: 63.83%   [EVAL] batch:  492 | acc: 68.75%,  total acc: 63.84%   [EVAL] batch:  493 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 63.97%   [EVAL] batch:  496 | acc: 100.00%,  total acc: 64.05%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 64.25%   
cur_acc:  ['0.9484', '0.7113', '0.6736', '0.6726', '0.5387', '0.6528', '0.6865', '0.8353']
his_acc:  ['0.9484', '0.8275', '0.7513', '0.6977', '0.6516', '0.6352', '0.6260', '0.6425']
----------END
his_acc mean:  [0.9446 0.8288 0.7646 0.7282 0.7128 0.6852 0.6653 0.652 ]
