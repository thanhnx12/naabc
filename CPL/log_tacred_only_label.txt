#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.211234092712402 1.7084617614746094
CurrentTrain: epoch  0, batch     0 | loss: 13.9196959Losses:  14.5792236328125 1.6901048421859741
CurrentTrain: epoch  0, batch     1 | loss: 16.2693291Losses:  15.08927059173584 1.6444356441497803
CurrentTrain: epoch  0, batch     2 | loss: 16.7337055Losses:  14.998939514160156 1.8157745599746704
CurrentTrain: epoch  0, batch     3 | loss: 16.8147144Losses:  14.828505516052246 1.8496438264846802
CurrentTrain: epoch  0, batch     4 | loss: 16.6781502Losses:  14.450037956237793 1.7259345054626465
CurrentTrain: epoch  0, batch     5 | loss: 16.1759720Losses:  14.117124557495117 1.4077744483947754
CurrentTrain: epoch  0, batch     6 | loss: 15.5248985Losses:  14.20703125 1.5224635601043701
CurrentTrain: epoch  0, batch     7 | loss: 15.7294950Losses:  13.935869216918945 1.552010416984558
CurrentTrain: epoch  0, batch     8 | loss: 15.4878798Losses:  13.722323417663574 1.3304190635681152
CurrentTrain: epoch  0, batch     9 | loss: 15.0527420Losses:  13.285319328308105 1.4591541290283203
CurrentTrain: epoch  0, batch    10 | loss: 14.7444735Losses:  13.020700454711914 1.3309814929962158
CurrentTrain: epoch  0, batch    11 | loss: 14.3516817Losses:  12.596056938171387 1.40211021900177
CurrentTrain: epoch  0, batch    12 | loss: 13.9981670Losses:  12.893190383911133 1.7794709205627441
CurrentTrain: epoch  0, batch    13 | loss: 14.6726608Losses:  12.512130737304688 1.7275407314300537
CurrentTrain: epoch  0, batch    14 | loss: 14.2396717Losses:  12.812847137451172 1.5780695676803589
CurrentTrain: epoch  0, batch    15 | loss: 14.3909168Losses:  12.57288646697998 1.6643247604370117
CurrentTrain: epoch  0, batch    16 | loss: 14.2372112Losses:  12.108726501464844 1.5023317337036133
CurrentTrain: epoch  0, batch    17 | loss: 13.6110582Losses:  12.221277236938477 1.4880919456481934
CurrentTrain: epoch  0, batch    18 | loss: 13.7093697Losses:  11.725299835205078 1.36260986328125
CurrentTrain: epoch  0, batch    19 | loss: 13.0879097Losses:  11.838890075683594 1.5052576065063477
CurrentTrain: epoch  0, batch    20 | loss: 13.3441477Losses:  11.474811553955078 1.4036707878112793
CurrentTrain: epoch  0, batch    21 | loss: 12.8784828Losses:  10.742158889770508 1.2795610427856445
CurrentTrain: epoch  0, batch    22 | loss: 12.0217199Losses:  11.002530097961426 1.4426214694976807
CurrentTrain: epoch  0, batch    23 | loss: 12.4451513Losses:  10.724617004394531 1.3058561086654663
CurrentTrain: epoch  0, batch    24 | loss: 12.0304728Losses:  10.237449645996094 1.317131519317627
CurrentTrain: epoch  0, batch    25 | loss: 11.5545807Losses:  10.040868759155273 1.3431757688522339
CurrentTrain: epoch  0, batch    26 | loss: 11.3840446Losses:  9.878532409667969 1.4160425662994385
CurrentTrain: epoch  0, batch    27 | loss: 11.2945747Losses:  9.681581497192383 1.3569164276123047
CurrentTrain: epoch  0, batch    28 | loss: 11.0384979Losses:  9.386812210083008 1.1338285207748413
CurrentTrain: epoch  0, batch    29 | loss: 10.5206404Losses:  9.53359317779541 1.410855770111084
CurrentTrain: epoch  0, batch    30 | loss: 10.9444485Losses:  9.133207321166992 1.3065769672393799
CurrentTrain: epoch  0, batch    31 | loss: 10.4397840Losses:  8.85720443725586 1.2662198543548584
CurrentTrain: epoch  0, batch    32 | loss: 10.1234245Losses:  9.143937110900879 1.497721791267395
CurrentTrain: epoch  0, batch    33 | loss: 10.6416588Losses:  8.476202964782715 1.055629014968872
CurrentTrain: epoch  0, batch    34 | loss: 9.5318317Losses:  8.083708763122559 1.1793687343597412
CurrentTrain: epoch  0, batch    35 | loss: 9.2630777Losses:  7.9586944580078125 1.1847963333129883
CurrentTrain: epoch  0, batch    36 | loss: 9.1434908Losses:  7.767488479614258 0.7243722677230835
CurrentTrain: epoch  0, batch    37 | loss: 8.4918604Losses:  7.829537391662598 1.410733699798584
CurrentTrain: epoch  1, batch     0 | loss: 9.2402706Losses:  7.670144081115723 1.080012559890747
CurrentTrain: epoch  1, batch     1 | loss: 8.7501564Losses:  7.805513381958008 1.0606021881103516
CurrentTrain: epoch  1, batch     2 | loss: 8.8661156Losses:  7.6887712478637695 1.20601224899292
CurrentTrain: epoch  1, batch     3 | loss: 8.8947830Losses:  7.633707523345947 1.1664268970489502
CurrentTrain: epoch  1, batch     4 | loss: 8.8001347Losses:  7.512103080749512 1.2500213384628296
CurrentTrain: epoch  1, batch     5 | loss: 8.7621241Losses:  7.30303955078125 1.1438157558441162
CurrentTrain: epoch  1, batch     6 | loss: 8.4468555Losses:  7.601259231567383 1.1033666133880615
CurrentTrain: epoch  1, batch     7 | loss: 8.7046261Losses:  7.758469581604004 1.176454782485962
CurrentTrain: epoch  1, batch     8 | loss: 8.9349241Losses:  7.464491844177246 1.1365385055541992
CurrentTrain: epoch  1, batch     9 | loss: 8.6010303Losses:  7.334151268005371 1.0999377965927124
CurrentTrain: epoch  1, batch    10 | loss: 8.4340887Losses:  7.559192180633545 0.9793102741241455
CurrentTrain: epoch  1, batch    11 | loss: 8.5385027Losses:  7.188336372375488 1.0705432891845703
CurrentTrain: epoch  1, batch    12 | loss: 8.2588797Losses:  7.7074785232543945 1.1898207664489746
CurrentTrain: epoch  1, batch    13 | loss: 8.8972988Losses:  7.203503131866455 1.0789190530776978
CurrentTrain: epoch  1, batch    14 | loss: 8.2824221Losses:  7.365644454956055 1.004467487335205
CurrentTrain: epoch  1, batch    15 | loss: 8.3701115Losses:  7.120356559753418 1.0489575862884521
CurrentTrain: epoch  1, batch    16 | loss: 8.1693144Losses:  6.921718597412109 0.9611687064170837
CurrentTrain: epoch  1, batch    17 | loss: 7.8828874Losses:  7.007874488830566 1.0617914199829102
CurrentTrain: epoch  1, batch    18 | loss: 8.0696659Losses:  7.363802909851074 1.2717641592025757
CurrentTrain: epoch  1, batch    19 | loss: 8.6355667Losses:  6.728475570678711 0.8046040534973145
CurrentTrain: epoch  1, batch    20 | loss: 7.5330796Losses:  6.985523223876953 0.7831393480300903
CurrentTrain: epoch  1, batch    21 | loss: 7.7686625Losses:  7.058343887329102 0.8462578058242798
CurrentTrain: epoch  1, batch    22 | loss: 7.9046016Losses:  7.412341117858887 0.9493839740753174
CurrentTrain: epoch  1, batch    23 | loss: 8.3617249Losses:  7.364943504333496 0.8865087032318115
CurrentTrain: epoch  1, batch    24 | loss: 8.2514524Losses:  7.393347263336182 0.9437508583068848
CurrentTrain: epoch  1, batch    25 | loss: 8.3370981Losses:  6.836155891418457 0.8148428201675415
CurrentTrain: epoch  1, batch    26 | loss: 7.6509986Losses:  6.985654354095459 1.143647313117981
CurrentTrain: epoch  1, batch    27 | loss: 8.1293020Losses:  6.842061996459961 0.9046164751052856
CurrentTrain: epoch  1, batch    28 | loss: 7.7466784Losses:  6.686251640319824 0.6849052906036377
CurrentTrain: epoch  1, batch    29 | loss: 7.3711567Losses:  7.040599822998047 0.8197181224822998
CurrentTrain: epoch  1, batch    30 | loss: 7.8603182Losses:  6.913344383239746 0.9781545996665955
CurrentTrain: epoch  1, batch    31 | loss: 7.8914990Losses:  6.590850353240967 0.8220411539077759
CurrentTrain: epoch  1, batch    32 | loss: 7.4128914Losses:  6.544325828552246 0.7862995862960815
CurrentTrain: epoch  1, batch    33 | loss: 7.3306255Losses:  6.867552757263184 0.881051778793335
CurrentTrain: epoch  1, batch    34 | loss: 7.7486048Losses:  6.9573822021484375 0.9340733289718628
CurrentTrain: epoch  1, batch    35 | loss: 7.8914557Losses:  6.946398735046387 0.8817654848098755
CurrentTrain: epoch  1, batch    36 | loss: 7.8281641Losses:  6.711795806884766 0.28816941380500793
CurrentTrain: epoch  1, batch    37 | loss: 6.9999652Losses:  6.660856246948242 0.795781672000885
CurrentTrain: epoch  2, batch     0 | loss: 7.4566379Losses:  6.575999736785889 0.7003750205039978
CurrentTrain: epoch  2, batch     1 | loss: 7.2763748Losses:  5.9904327392578125 0.6568456888198853
CurrentTrain: epoch  2, batch     2 | loss: 6.6472783Losses:  6.073957443237305 0.699337899684906
CurrentTrain: epoch  2, batch     3 | loss: 6.7732954Losses:  7.230225563049316 0.5790846347808838
CurrentTrain: epoch  2, batch     4 | loss: 7.8093100Losses:  6.393065452575684 0.798406720161438
CurrentTrain: epoch  2, batch     5 | loss: 7.1914721Losses:  6.453836917877197 0.7630128860473633
CurrentTrain: epoch  2, batch     6 | loss: 7.2168498Losses:  6.200307846069336 0.7261000871658325
CurrentTrain: epoch  2, batch     7 | loss: 6.9264078Losses:  6.61851167678833 0.8339014649391174
CurrentTrain: epoch  2, batch     8 | loss: 7.4524131Losses:  6.460786819458008 0.551008403301239
CurrentTrain: epoch  2, batch     9 | loss: 7.0117950Losses:  6.337022304534912 0.5244612693786621
CurrentTrain: epoch  2, batch    10 | loss: 6.8614836Losses:  6.425081729888916 0.6552989482879639
CurrentTrain: epoch  2, batch    11 | loss: 7.0803804Losses:  6.00764274597168 0.5133416056632996
CurrentTrain: epoch  2, batch    12 | loss: 6.5209842Losses:  6.39054536819458 0.7164747714996338
CurrentTrain: epoch  2, batch    13 | loss: 7.1070204Losses:  7.019825458526611 0.783290445804596
CurrentTrain: epoch  2, batch    14 | loss: 7.8031158Losses:  6.683718204498291 0.6001595854759216
CurrentTrain: epoch  2, batch    15 | loss: 7.2838778Losses:  6.694159507751465 0.6505954265594482
CurrentTrain: epoch  2, batch    16 | loss: 7.3447552Losses:  6.22608757019043 0.6666820049285889
CurrentTrain: epoch  2, batch    17 | loss: 6.8927698Losses:  6.149224281311035 0.5876628160476685
CurrentTrain: epoch  2, batch    18 | loss: 6.7368870Losses:  6.235168933868408 0.5830065011978149
CurrentTrain: epoch  2, batch    19 | loss: 6.8181753Losses:  5.837520599365234 0.45458656549453735
CurrentTrain: epoch  2, batch    20 | loss: 6.2921071Losses:  6.299874305725098 0.7049486041069031
CurrentTrain: epoch  2, batch    21 | loss: 7.0048227Losses:  6.544215202331543 0.7745466232299805
CurrentTrain: epoch  2, batch    22 | loss: 7.3187618Losses:  6.570363521575928 0.6668256521224976
CurrentTrain: epoch  2, batch    23 | loss: 7.2371893Losses:  6.420406341552734 0.702236533164978
CurrentTrain: epoch  2, batch    24 | loss: 7.1226430Losses:  6.496184349060059 0.6677019596099854
CurrentTrain: epoch  2, batch    25 | loss: 7.1638861Losses:  6.926509380340576 0.7168496251106262
CurrentTrain: epoch  2, batch    26 | loss: 7.6433592Losses:  6.206119060516357 0.5779226422309875
CurrentTrain: epoch  2, batch    27 | loss: 6.7840419Losses:  6.91700553894043 0.7751209735870361
CurrentTrain: epoch  2, batch    28 | loss: 7.6921263Losses:  6.242432594299316 0.6356792449951172
CurrentTrain: epoch  2, batch    29 | loss: 6.8781118Losses:  6.2186994552612305 0.5474273562431335
CurrentTrain: epoch  2, batch    30 | loss: 6.7661266Losses:  6.675220489501953 0.610221266746521
CurrentTrain: epoch  2, batch    31 | loss: 7.2854419Losses:  6.3547258377075195 0.5583531260490417
CurrentTrain: epoch  2, batch    32 | loss: 6.9130788Losses:  6.044363021850586 0.5858771800994873
CurrentTrain: epoch  2, batch    33 | loss: 6.6302404Losses:  6.41864013671875 0.6406068801879883
CurrentTrain: epoch  2, batch    34 | loss: 7.0592470Losses:  6.541497230529785 0.5735889673233032
CurrentTrain: epoch  2, batch    35 | loss: 7.1150861Losses:  6.075345039367676 0.5877593755722046
CurrentTrain: epoch  2, batch    36 | loss: 6.6631045Losses:  6.113597869873047 0.5193241238594055
CurrentTrain: epoch  2, batch    37 | loss: 6.6329222Losses:  5.8961334228515625 0.5393800735473633
CurrentTrain: epoch  3, batch     0 | loss: 6.4355135Losses:  5.928969383239746 0.46621882915496826
CurrentTrain: epoch  3, batch     1 | loss: 6.3951883Losses:  6.228583335876465 0.5882821083068848
CurrentTrain: epoch  3, batch     2 | loss: 6.8168654Losses:  6.144541263580322 0.616010308265686
CurrentTrain: epoch  3, batch     3 | loss: 6.7605515Losses:  5.76629114151001 0.4206744432449341
CurrentTrain: epoch  3, batch     4 | loss: 6.1869655Losses:  6.187243461608887 0.6138985753059387
CurrentTrain: epoch  3, batch     5 | loss: 6.8011422Losses:  5.6440348625183105 0.5425876379013062
CurrentTrain: epoch  3, batch     6 | loss: 6.1866226Losses:  5.899855613708496 0.5130529403686523
CurrentTrain: epoch  3, batch     7 | loss: 6.4129086Losses:  5.825929641723633 0.48977404832839966
CurrentTrain: epoch  3, batch     8 | loss: 6.3157039Losses:  5.815698623657227 0.3871071934700012
CurrentTrain: epoch  3, batch     9 | loss: 6.2028060Losses:  5.766424655914307 0.5314172506332397
CurrentTrain: epoch  3, batch    10 | loss: 6.2978420Losses:  5.866400718688965 0.4411396384239197
CurrentTrain: epoch  3, batch    11 | loss: 6.3075404Losses:  5.893368721008301 0.5527539253234863
CurrentTrain: epoch  3, batch    12 | loss: 6.4461226Losses:  5.839960098266602 0.4731966257095337
CurrentTrain: epoch  3, batch    13 | loss: 6.3131566Losses:  5.6248779296875 0.43994271755218506
CurrentTrain: epoch  3, batch    14 | loss: 6.0648208Losses:  6.067163467407227 0.5121640563011169
CurrentTrain: epoch  3, batch    15 | loss: 6.5793276Losses:  5.5414299964904785 0.3935166001319885
CurrentTrain: epoch  3, batch    16 | loss: 5.9349465Losses:  6.121804237365723 0.5152957439422607
CurrentTrain: epoch  3, batch    17 | loss: 6.6371002Losses:  5.910941123962402 0.419575572013855
CurrentTrain: epoch  3, batch    18 | loss: 6.3305168Losses:  5.870645523071289 0.5861569046974182
CurrentTrain: epoch  3, batch    19 | loss: 6.4568024Losses:  5.4965925216674805 0.3889960050582886
CurrentTrain: epoch  3, batch    20 | loss: 5.8855886Losses:  5.945577621459961 0.3511957824230194
CurrentTrain: epoch  3, batch    21 | loss: 6.2967734Losses:  5.971523284912109 0.6062859296798706
CurrentTrain: epoch  3, batch    22 | loss: 6.5778093Losses:  5.737027168273926 0.39271873235702515
CurrentTrain: epoch  3, batch    23 | loss: 6.1297460Losses:  5.609433174133301 0.3380686044692993
CurrentTrain: epoch  3, batch    24 | loss: 5.9475017Losses:  5.873722076416016 0.38996621966362
CurrentTrain: epoch  3, batch    25 | loss: 6.2636881Losses:  5.961398124694824 0.5474849939346313
CurrentTrain: epoch  3, batch    26 | loss: 6.5088830Losses:  5.566488265991211 0.5238118767738342
CurrentTrain: epoch  3, batch    27 | loss: 6.0903001Losses:  5.97231912612915 0.5253573060035706
CurrentTrain: epoch  3, batch    28 | loss: 6.4976764Losses:  6.113986015319824 0.5737134218215942
CurrentTrain: epoch  3, batch    29 | loss: 6.6876993Losses:  6.283010482788086 0.3666017949581146
CurrentTrain: epoch  3, batch    30 | loss: 6.6496124Losses:  6.709165573120117 0.4754827320575714
CurrentTrain: epoch  3, batch    31 | loss: 7.1846485Losses:  5.9605255126953125 0.45171016454696655
CurrentTrain: epoch  3, batch    32 | loss: 6.4122357Losses:  6.1298346519470215 0.46593305468559265
CurrentTrain: epoch  3, batch    33 | loss: 6.5957675Losses:  6.06788444519043 0.38803914189338684
CurrentTrain: epoch  3, batch    34 | loss: 6.4559236Losses:  5.580272674560547 0.32987719774246216
CurrentTrain: epoch  3, batch    35 | loss: 5.9101501Losses:  5.838329315185547 0.5303447842597961
CurrentTrain: epoch  3, batch    36 | loss: 6.3686743Losses:  4.955124855041504 0.18782901763916016
CurrentTrain: epoch  3, batch    37 | loss: 5.1429539Losses:  5.445795059204102 0.40415576100349426
CurrentTrain: epoch  4, batch     0 | loss: 5.8499508Losses:  6.402994155883789 0.586215615272522
CurrentTrain: epoch  4, batch     1 | loss: 6.9892097Losses:  5.696265697479248 0.37343281507492065
CurrentTrain: epoch  4, batch     2 | loss: 6.0696983Losses:  6.118566513061523 0.5110552310943604
CurrentTrain: epoch  4, batch     3 | loss: 6.6296215Losses:  5.692402362823486 0.39661967754364014
CurrentTrain: epoch  4, batch     4 | loss: 6.0890222Losses:  5.365659713745117 0.3281477093696594
CurrentTrain: epoch  4, batch     5 | loss: 5.6938076Losses:  5.476682662963867 0.44017699360847473
CurrentTrain: epoch  4, batch     6 | loss: 5.9168596Losses:  5.246927261352539 0.33299052715301514
CurrentTrain: epoch  4, batch     7 | loss: 5.5799179Losses:  5.589288711547852 0.328207403421402
CurrentTrain: epoch  4, batch     8 | loss: 5.9174962Losses:  5.3346452713012695 0.3096114695072174
CurrentTrain: epoch  4, batch     9 | loss: 5.6442566Losses:  5.837967872619629 0.396749347448349
CurrentTrain: epoch  4, batch    10 | loss: 6.2347174Losses:  5.277683258056641 0.32496365904808044
CurrentTrain: epoch  4, batch    11 | loss: 5.6026468Losses:  5.431299209594727 0.30721449851989746
CurrentTrain: epoch  4, batch    12 | loss: 5.7385139Losses:  5.814845561981201 0.32138580083847046
CurrentTrain: epoch  4, batch    13 | loss: 6.1362314Losses:  5.449916839599609 0.32159650325775146
CurrentTrain: epoch  4, batch    14 | loss: 5.7715135Losses:  5.8777265548706055 0.499569833278656
CurrentTrain: epoch  4, batch    15 | loss: 6.3772964Losses:  5.632746696472168 0.4164699912071228
CurrentTrain: epoch  4, batch    16 | loss: 6.0492167Losses:  5.422919273376465 0.3075593113899231
CurrentTrain: epoch  4, batch    17 | loss: 5.7304788Losses:  5.365677833557129 0.29895955324172974
CurrentTrain: epoch  4, batch    18 | loss: 5.6646376Losses:  5.316925525665283 0.2890061140060425
CurrentTrain: epoch  4, batch    19 | loss: 5.6059318Losses:  5.627549648284912 0.387641578912735
CurrentTrain: epoch  4, batch    20 | loss: 6.0151911Losses:  5.442474365234375 0.2700526714324951
CurrentTrain: epoch  4, batch    21 | loss: 5.7125273Losses:  6.00586462020874 0.4871850609779358
CurrentTrain: epoch  4, batch    22 | loss: 6.4930496Losses:  5.351221084594727 0.36573609709739685
CurrentTrain: epoch  4, batch    23 | loss: 5.7169571Losses:  5.681283473968506 0.33129268884658813
CurrentTrain: epoch  4, batch    24 | loss: 6.0125761Losses:  5.443327903747559 0.25095924735069275
CurrentTrain: epoch  4, batch    25 | loss: 5.6942873Losses:  6.112590789794922 0.5652074217796326
CurrentTrain: epoch  4, batch    26 | loss: 6.6777983Losses:  5.477855682373047 0.35028332471847534
CurrentTrain: epoch  4, batch    27 | loss: 5.8281388Losses:  5.2319207191467285 0.2992488145828247
CurrentTrain: epoch  4, batch    28 | loss: 5.5311694Losses:  4.963095664978027 0.2516140341758728
CurrentTrain: epoch  4, batch    29 | loss: 5.2147098Losses:  6.115328788757324 0.3272014260292053
CurrentTrain: epoch  4, batch    30 | loss: 6.4425302Losses:  5.267570495605469 0.28369006514549255
CurrentTrain: epoch  4, batch    31 | loss: 5.5512605Losses:  5.613201141357422 0.33424919843673706
CurrentTrain: epoch  4, batch    32 | loss: 5.9474502Losses:  5.257476806640625 0.399002343416214
CurrentTrain: epoch  4, batch    33 | loss: 5.6564794Losses:  5.916821479797363 0.4635256230831146
CurrentTrain: epoch  4, batch    34 | loss: 6.3803473Losses:  6.1323041915893555 0.5144560933113098
CurrentTrain: epoch  4, batch    35 | loss: 6.6467605Losses:  5.127011299133301 0.24681904911994934
CurrentTrain: epoch  4, batch    36 | loss: 5.3738303Losses:  5.187078475952148 0.2941286861896515
CurrentTrain: epoch  4, batch    37 | loss: 5.4812074Losses:  4.987395286560059 0.25505852699279785
CurrentTrain: epoch  5, batch     0 | loss: 5.2424536Losses:  5.2990264892578125 0.18979892134666443
CurrentTrain: epoch  5, batch     1 | loss: 5.4888253Losses:  5.250065803527832 0.25925302505493164
CurrentTrain: epoch  5, batch     2 | loss: 5.5093188Losses:  5.757608413696289 0.34888559579849243
CurrentTrain: epoch  5, batch     3 | loss: 6.1064939Losses:  5.417292594909668 0.2736530900001526
CurrentTrain: epoch  5, batch     4 | loss: 5.6909456Losses:  5.711730003356934 0.35884854197502136
CurrentTrain: epoch  5, batch     5 | loss: 6.0705786Losses:  5.345458030700684 0.32241955399513245
CurrentTrain: epoch  5, batch     6 | loss: 5.6678777Losses:  5.035793304443359 0.24006377160549164
CurrentTrain: epoch  5, batch     7 | loss: 5.2758570Losses:  5.308980941772461 0.31967025995254517
CurrentTrain: epoch  5, batch     8 | loss: 5.6286511Losses:  5.021055221557617 0.12300477921962738
CurrentTrain: epoch  5, batch     9 | loss: 5.1440601Losses:  5.308328628540039 0.3342908024787903
CurrentTrain: epoch  5, batch    10 | loss: 5.6426196Losses:  5.347532272338867 0.28676074743270874
CurrentTrain: epoch  5, batch    11 | loss: 5.6342931Losses:  5.539895057678223 0.38197803497314453
CurrentTrain: epoch  5, batch    12 | loss: 5.9218731Losses:  5.38213586807251 0.3386797308921814
CurrentTrain: epoch  5, batch    13 | loss: 5.7208157Losses:  5.13240385055542 0.2590405344963074
CurrentTrain: epoch  5, batch    14 | loss: 5.3914442Losses:  5.350931167602539 0.24085481464862823
CurrentTrain: epoch  5, batch    15 | loss: 5.5917859Losses:  5.432657718658447 0.23113244771957397
CurrentTrain: epoch  5, batch    16 | loss: 5.6637902Losses:  5.2728776931762695 0.27143195271492004
CurrentTrain: epoch  5, batch    17 | loss: 5.5443096Losses:  5.200649261474609 0.1982881724834442
CurrentTrain: epoch  5, batch    18 | loss: 5.3989372Losses:  4.98899507522583 0.2254304587841034
CurrentTrain: epoch  5, batch    19 | loss: 5.2144256Losses:  5.460363864898682 0.2583281993865967
CurrentTrain: epoch  5, batch    20 | loss: 5.7186918Losses:  5.422558784484863 0.2946596145629883
CurrentTrain: epoch  5, batch    21 | loss: 5.7172184Losses:  4.960963726043701 0.24865278601646423
CurrentTrain: epoch  5, batch    22 | loss: 5.2096167Losses:  5.41184139251709 0.297579288482666
CurrentTrain: epoch  5, batch    23 | loss: 5.7094207Losses:  5.525687217712402 0.3428569436073303
CurrentTrain: epoch  5, batch    24 | loss: 5.8685441Losses:  5.462996959686279 0.37069079279899597
CurrentTrain: epoch  5, batch    25 | loss: 5.8336878Losses:  5.03903865814209 0.20735225081443787
CurrentTrain: epoch  5, batch    26 | loss: 5.2463908Losses:  5.030932903289795 0.21499355137348175
CurrentTrain: epoch  5, batch    27 | loss: 5.2459264Losses:  5.625833511352539 0.3928913474082947
CurrentTrain: epoch  5, batch    28 | loss: 6.0187249Losses:  5.198336124420166 0.28321340680122375
CurrentTrain: epoch  5, batch    29 | loss: 5.4815497Losses:  5.080036640167236 0.2784273624420166
CurrentTrain: epoch  5, batch    30 | loss: 5.3584642Losses:  5.571433067321777 0.4611932039260864
CurrentTrain: epoch  5, batch    31 | loss: 6.0326262Losses:  4.753838539123535 0.2195120006799698
CurrentTrain: epoch  5, batch    32 | loss: 4.9733505Losses:  5.115553855895996 0.21660330891609192
CurrentTrain: epoch  5, batch    33 | loss: 5.3321571Losses:  5.27041482925415 0.26775047183036804
CurrentTrain: epoch  5, batch    34 | loss: 5.5381651Losses:  5.183065891265869 0.2447863519191742
CurrentTrain: epoch  5, batch    35 | loss: 5.4278522Losses:  4.889439582824707 0.1514827013015747
CurrentTrain: epoch  5, batch    36 | loss: 5.0409222Losses:  5.473438262939453 0.1943565458059311
CurrentTrain: epoch  5, batch    37 | loss: 5.6677947Losses:  5.164394855499268 0.2229413092136383
CurrentTrain: epoch  6, batch     0 | loss: 5.3873363Losses:  4.990077018737793 0.24782267212867737
CurrentTrain: epoch  6, batch     1 | loss: 5.2378998Losses:  5.14961051940918 0.29203319549560547
CurrentTrain: epoch  6, batch     2 | loss: 5.4416437Losses:  5.023592948913574 0.2273549884557724
CurrentTrain: epoch  6, batch     3 | loss: 5.2509480Losses:  4.912337779998779 0.20116209983825684
CurrentTrain: epoch  6, batch     4 | loss: 5.1134996Losses:  4.992023944854736 0.24150478839874268
CurrentTrain: epoch  6, batch     5 | loss: 5.2335286Losses:  5.134282112121582 0.2529523968696594
CurrentTrain: epoch  6, batch     6 | loss: 5.3872347Losses:  4.872645378112793 0.19955329596996307
CurrentTrain: epoch  6, batch     7 | loss: 5.0721989Losses:  5.408463478088379 0.28490665555000305
CurrentTrain: epoch  6, batch     8 | loss: 5.6933703Losses:  4.91900634765625 0.2734982371330261
CurrentTrain: epoch  6, batch     9 | loss: 5.1925044Losses:  5.136926651000977 0.26521265506744385
CurrentTrain: epoch  6, batch    10 | loss: 5.4021392Losses:  4.921643257141113 0.20738419890403748
CurrentTrain: epoch  6, batch    11 | loss: 5.1290274Losses:  4.812494277954102 0.18460765480995178
CurrentTrain: epoch  6, batch    12 | loss: 4.9971018Losses:  4.903011322021484 0.17179062962532043
CurrentTrain: epoch  6, batch    13 | loss: 5.0748019Losses:  5.129958152770996 0.21423281729221344
CurrentTrain: epoch  6, batch    14 | loss: 5.3441911Losses:  5.053918838500977 0.1216898113489151
CurrentTrain: epoch  6, batch    15 | loss: 5.1756086Losses:  4.841274738311768 0.1523900181055069
CurrentTrain: epoch  6, batch    16 | loss: 4.9936647Losses:  4.94298791885376 0.15648110210895538
CurrentTrain: epoch  6, batch    17 | loss: 5.0994692Losses:  5.0851545333862305 0.1845804750919342
CurrentTrain: epoch  6, batch    18 | loss: 5.2697349Losses:  5.453447341918945 0.33420389890670776
CurrentTrain: epoch  6, batch    19 | loss: 5.7876511Losses:  4.91008186340332 0.18071478605270386
CurrentTrain: epoch  6, batch    20 | loss: 5.0907965Losses:  5.000861167907715 0.16812542080879211
CurrentTrain: epoch  6, batch    21 | loss: 5.1689868Losses:  5.043094635009766 0.2578423023223877
CurrentTrain: epoch  6, batch    22 | loss: 5.3009367Losses:  4.873022079467773 0.15570349991321564
CurrentTrain: epoch  6, batch    23 | loss: 5.0287256Losses:  5.322277069091797 0.43157947063446045
CurrentTrain: epoch  6, batch    24 | loss: 5.7538567Losses:  4.773895740509033 0.17188143730163574
CurrentTrain: epoch  6, batch    25 | loss: 4.9457769Losses:  5.164210319519043 0.39846399426460266
CurrentTrain: epoch  6, batch    26 | loss: 5.5626745Losses:  4.734740734100342 0.11002311110496521
CurrentTrain: epoch  6, batch    27 | loss: 4.8447638Losses:  5.0240936279296875 0.15036234259605408
CurrentTrain: epoch  6, batch    28 | loss: 5.1744561Losses:  5.00606107711792 0.24869079887866974
CurrentTrain: epoch  6, batch    29 | loss: 5.2547517Losses:  4.781914710998535 0.15433457493782043
CurrentTrain: epoch  6, batch    30 | loss: 4.9362493Losses:  5.201279163360596 0.2613440155982971
CurrentTrain: epoch  6, batch    31 | loss: 5.4626231Losses:  4.9230451583862305 0.2403479367494583
CurrentTrain: epoch  6, batch    32 | loss: 5.1633930Losses:  5.309914588928223 0.3913145065307617
CurrentTrain: epoch  6, batch    33 | loss: 5.7012291Losses:  5.066394329071045 0.32025858759880066
CurrentTrain: epoch  6, batch    34 | loss: 5.3866529Losses:  4.618752479553223 0.1246824711561203
CurrentTrain: epoch  6, batch    35 | loss: 4.7434349Losses:  5.019801139831543 0.1719268411397934
CurrentTrain: epoch  6, batch    36 | loss: 5.1917281Losses:  5.3310227394104 0.3062489926815033
CurrentTrain: epoch  6, batch    37 | loss: 5.6372719Losses:  4.718038558959961 0.14609138667583466
CurrentTrain: epoch  7, batch     0 | loss: 4.8641300Losses:  5.01831579208374 0.17324888706207275
CurrentTrain: epoch  7, batch     1 | loss: 5.1915646Losses:  5.074193954467773 0.2442207634449005
CurrentTrain: epoch  7, batch     2 | loss: 5.3184147Losses:  4.688750743865967 0.15799163281917572
CurrentTrain: epoch  7, batch     3 | loss: 4.8467422Losses:  4.823272705078125 0.20690786838531494
CurrentTrain: epoch  7, batch     4 | loss: 5.0301805Losses:  4.73807430267334 0.14161260426044464
CurrentTrain: epoch  7, batch     5 | loss: 4.8796868Losses:  5.237009048461914 0.31489843130111694
CurrentTrain: epoch  7, batch     6 | loss: 5.5519075Losses:  4.691971778869629 0.1500576138496399
CurrentTrain: epoch  7, batch     7 | loss: 4.8420296Losses:  4.823189735412598 0.1723979264497757
CurrentTrain: epoch  7, batch     8 | loss: 4.9955878Losses:  4.691572189331055 0.15062983334064484
CurrentTrain: epoch  7, batch     9 | loss: 4.8422022Losses:  4.727011203765869 0.18110693991184235
CurrentTrain: epoch  7, batch    10 | loss: 4.9081182Losses:  4.820147514343262 0.20962685346603394
CurrentTrain: epoch  7, batch    11 | loss: 5.0297742Losses:  4.679177284240723 0.16897106170654297
CurrentTrain: epoch  7, batch    12 | loss: 4.8481483Losses:  4.7936506271362305 0.16390913724899292
CurrentTrain: epoch  7, batch    13 | loss: 4.9575596Losses:  4.811428546905518 0.17331257462501526
CurrentTrain: epoch  7, batch    14 | loss: 4.9847412Losses:  4.6713056564331055 0.15433189272880554
CurrentTrain: epoch  7, batch    15 | loss: 4.8256373Losses:  4.710127830505371 0.13837561011314392
CurrentTrain: epoch  7, batch    16 | loss: 4.8485036Losses:  4.642068386077881 0.1465788632631302
CurrentTrain: epoch  7, batch    17 | loss: 4.7886472Losses:  4.719408988952637 0.12321680784225464
CurrentTrain: epoch  7, batch    18 | loss: 4.8426256Losses:  4.819382667541504 0.17534314095973969
CurrentTrain: epoch  7, batch    19 | loss: 4.9947257Losses:  4.671294212341309 0.13415278494358063
CurrentTrain: epoch  7, batch    20 | loss: 4.8054471Losses:  4.646742343902588 0.1372334361076355
CurrentTrain: epoch  7, batch    21 | loss: 4.7839756Losses:  4.709571361541748 0.09937670826911926
CurrentTrain: epoch  7, batch    22 | loss: 4.8089480Losses:  4.952788352966309 0.2579648494720459
CurrentTrain: epoch  7, batch    23 | loss: 5.2107534Losses:  5.002114295959473 0.17610719799995422
CurrentTrain: epoch  7, batch    24 | loss: 5.1782217Losses:  4.733030796051025 0.19008366763591766
CurrentTrain: epoch  7, batch    25 | loss: 4.9231143Losses:  4.59263801574707 0.1323891282081604
CurrentTrain: epoch  7, batch    26 | loss: 4.7250271Losses:  4.943202972412109 0.1529051661491394
CurrentTrain: epoch  7, batch    27 | loss: 5.0961080Losses:  5.084646224975586 0.20098614692687988
CurrentTrain: epoch  7, batch    28 | loss: 5.2856321Losses:  4.636563301086426 0.13379976153373718
CurrentTrain: epoch  7, batch    29 | loss: 4.7703629Losses:  4.8225860595703125 0.20084801316261292
CurrentTrain: epoch  7, batch    30 | loss: 5.0234342Losses:  4.69532585144043 0.15481717884540558
CurrentTrain: epoch  7, batch    31 | loss: 4.8501430Losses:  4.6748456954956055 0.1519199162721634
CurrentTrain: epoch  7, batch    32 | loss: 4.8267655Losses:  4.713836669921875 0.09876365959644318
CurrentTrain: epoch  7, batch    33 | loss: 4.8126001Losses:  4.9501776695251465 0.1963074505329132
CurrentTrain: epoch  7, batch    34 | loss: 5.1464853Losses:  4.632318019866943 0.12299299240112305
CurrentTrain: epoch  7, batch    35 | loss: 4.7553110Losses:  4.633650779724121 0.13897641003131866
CurrentTrain: epoch  7, batch    36 | loss: 4.7726274Losses:  4.8581695556640625 0.14814946055412292
CurrentTrain: epoch  7, batch    37 | loss: 5.0063190Losses:  4.586449146270752 0.09735846519470215
CurrentTrain: epoch  8, batch     0 | loss: 4.6838074Losses:  4.6017374992370605 0.13084135949611664
CurrentTrain: epoch  8, batch     1 | loss: 4.7325788Losses:  4.623926162719727 0.12820905447006226
CurrentTrain: epoch  8, batch     2 | loss: 4.7521353Losses:  4.767068862915039 0.15362301468849182
CurrentTrain: epoch  8, batch     3 | loss: 4.9206920Losses:  4.67639684677124 0.089636892080307
CurrentTrain: epoch  8, batch     4 | loss: 4.7660336Losses:  4.655178070068359 0.0747339054942131
CurrentTrain: epoch  8, batch     5 | loss: 4.7299118Losses:  4.679637908935547 0.12847042083740234
CurrentTrain: epoch  8, batch     6 | loss: 4.8081083Losses:  4.609802722930908 0.14322197437286377
CurrentTrain: epoch  8, batch     7 | loss: 4.7530246Losses:  4.713067054748535 0.15795393288135529
CurrentTrain: epoch  8, batch     8 | loss: 4.8710208Losses:  5.0258331298828125 0.23505958914756775
CurrentTrain: epoch  8, batch     9 | loss: 5.2608929Losses:  4.690088272094727 0.13242638111114502
CurrentTrain: epoch  8, batch    10 | loss: 4.8225145Losses:  4.617883205413818 0.14048941433429718
CurrentTrain: epoch  8, batch    11 | loss: 4.7583728Losses:  5.050669193267822 0.17851699888706207
CurrentTrain: epoch  8, batch    12 | loss: 5.2291861Losses:  4.629554271697998 0.12501491606235504
CurrentTrain: epoch  8, batch    13 | loss: 4.7545691Losses:  4.749965190887451 0.15855097770690918
CurrentTrain: epoch  8, batch    14 | loss: 4.9085159Losses:  4.611532211303711 0.07695136964321136
CurrentTrain: epoch  8, batch    15 | loss: 4.6884837Losses:  4.6259918212890625 0.09114828705787659
CurrentTrain: epoch  8, batch    16 | loss: 4.7171402Losses:  4.723482131958008 0.10706555843353271
CurrentTrain: epoch  8, batch    17 | loss: 4.8305478Losses:  4.666243076324463 0.15178906917572021
CurrentTrain: epoch  8, batch    18 | loss: 4.8180323Losses:  4.641315937042236 0.13589170575141907
CurrentTrain: epoch  8, batch    19 | loss: 4.7772079Losses:  4.588255405426025 0.1267184019088745
CurrentTrain: epoch  8, batch    20 | loss: 4.7149739Losses:  4.623994827270508 0.12688906490802765
CurrentTrain: epoch  8, batch    21 | loss: 4.7508841Losses:  4.666784286499023 0.08678249269723892
CurrentTrain: epoch  8, batch    22 | loss: 4.7535667Losses:  4.614368438720703 0.13147284090518951
CurrentTrain: epoch  8, batch    23 | loss: 4.7458415Losses:  4.627811431884766 0.1107252910733223
CurrentTrain: epoch  8, batch    24 | loss: 4.7385368Losses:  4.58935546875 0.11118993163108826
CurrentTrain: epoch  8, batch    25 | loss: 4.7005453Losses:  4.641190528869629 0.08143244683742523
CurrentTrain: epoch  8, batch    26 | loss: 4.7226229Losses:  4.674603462219238 0.1096123456954956
CurrentTrain: epoch  8, batch    27 | loss: 4.7842159Losses:  4.644966125488281 0.16915418207645416
CurrentTrain: epoch  8, batch    28 | loss: 4.8141203Losses:  4.647848129272461 0.12782326340675354
CurrentTrain: epoch  8, batch    29 | loss: 4.7756715Losses:  4.7127275466918945 0.11365751922130585
CurrentTrain: epoch  8, batch    30 | loss: 4.8263850Losses:  4.578851222991943 0.11654632538557053
CurrentTrain: epoch  8, batch    31 | loss: 4.6953974Losses:  4.708493232727051 0.145926371216774
CurrentTrain: epoch  8, batch    32 | loss: 4.8544197Losses:  4.592425346374512 0.13185259699821472
CurrentTrain: epoch  8, batch    33 | loss: 4.7242780Losses:  4.792752265930176 0.204849511384964
CurrentTrain: epoch  8, batch    34 | loss: 4.9976020Losses:  4.674683094024658 0.10942581295967102
CurrentTrain: epoch  8, batch    35 | loss: 4.7841091Losses:  4.714573860168457 0.16566899418830872
CurrentTrain: epoch  8, batch    36 | loss: 4.8802428Losses:  4.711559295654297 0.15615001320838928
CurrentTrain: epoch  8, batch    37 | loss: 4.8677092Losses:  4.652968883514404 0.08373047411441803
CurrentTrain: epoch  9, batch     0 | loss: 4.7366996Losses:  4.758238792419434 0.16216441988945007
CurrentTrain: epoch  9, batch     1 | loss: 4.9204030Losses:  4.622460842132568 0.1301725208759308
CurrentTrain: epoch  9, batch     2 | loss: 4.7526336Losses:  4.601526737213135 0.12493899464607239
CurrentTrain: epoch  9, batch     3 | loss: 4.7264657Losses:  4.533185958862305 0.08547506481409073
CurrentTrain: epoch  9, batch     4 | loss: 4.6186609Losses:  4.631841659545898 0.11589998006820679
CurrentTrain: epoch  9, batch     5 | loss: 4.7477417Losses:  4.6082444190979 0.08393502980470657
CurrentTrain: epoch  9, batch     6 | loss: 4.6921797Losses:  4.551778316497803 0.10997435450553894
CurrentTrain: epoch  9, batch     7 | loss: 4.6617527Losses:  4.563422203063965 0.13389359414577484
CurrentTrain: epoch  9, batch     8 | loss: 4.6973157Losses:  4.552578926086426 0.09711074829101562
CurrentTrain: epoch  9, batch     9 | loss: 4.6496897Losses:  4.594644546508789 0.11455000936985016
CurrentTrain: epoch  9, batch    10 | loss: 4.7091947Losses:  4.588335037231445 0.10175903141498566
CurrentTrain: epoch  9, batch    11 | loss: 4.6900940Losses:  4.582169055938721 0.08691705018281937
CurrentTrain: epoch  9, batch    12 | loss: 4.6690860Losses:  4.599996566772461 0.12009860575199127
CurrentTrain: epoch  9, batch    13 | loss: 4.7200952Losses:  4.825405120849609 0.17798534035682678
CurrentTrain: epoch  9, batch    14 | loss: 5.0033903Losses:  4.586512565612793 0.0926777571439743
CurrentTrain: epoch  9, batch    15 | loss: 4.6791902Losses:  4.552360534667969 0.09995821863412857
CurrentTrain: epoch  9, batch    16 | loss: 4.6523190Losses:  4.65254020690918 0.14331580698490143
CurrentTrain: epoch  9, batch    17 | loss: 4.7958560Losses:  4.549417972564697 0.11328335106372833
CurrentTrain: epoch  9, batch    18 | loss: 4.6627011Losses:  4.6117753982543945 0.11450904607772827
CurrentTrain: epoch  9, batch    19 | loss: 4.7262845Losses:  4.569647789001465 0.10737967491149902
CurrentTrain: epoch  9, batch    20 | loss: 4.6770277Losses:  4.538755893707275 0.10391299426555634
CurrentTrain: epoch  9, batch    21 | loss: 4.6426687Losses:  4.558935165405273 0.09713514149188995
CurrentTrain: epoch  9, batch    22 | loss: 4.6560702Losses:  4.712092399597168 0.13323700428009033
CurrentTrain: epoch  9, batch    23 | loss: 4.8453293Losses:  4.56348180770874 0.09898489713668823
CurrentTrain: epoch  9, batch    24 | loss: 4.6624665Losses:  4.543874740600586 0.09390649199485779
CurrentTrain: epoch  9, batch    25 | loss: 4.6377811Losses:  4.558060646057129 0.10660134255886078
CurrentTrain: epoch  9, batch    26 | loss: 4.6646619Losses:  4.522183895111084 0.0816144198179245
CurrentTrain: epoch  9, batch    27 | loss: 4.6037984Losses:  4.771533966064453 0.08977708220481873
CurrentTrain: epoch  9, batch    28 | loss: 4.8613110Losses:  4.584692001342773 0.11482058465480804
CurrentTrain: epoch  9, batch    29 | loss: 4.6995125Losses:  4.511891841888428 0.09054113179445267
CurrentTrain: epoch  9, batch    30 | loss: 4.6024332Losses:  4.554139614105225 0.10709145665168762
CurrentTrain: epoch  9, batch    31 | loss: 4.6612310Losses:  4.810476303100586 0.2152155488729477
CurrentTrain: epoch  9, batch    32 | loss: 5.0256920Losses:  4.543734550476074 0.10303197801113129
CurrentTrain: epoch  9, batch    33 | loss: 4.6467667Losses:  4.579029560089111 0.0881757065653801
CurrentTrain: epoch  9, batch    34 | loss: 4.6672053Losses:  4.517849445343018 0.09273296594619751
CurrentTrain: epoch  9, batch    35 | loss: 4.6105824Losses:  4.539884567260742 0.10828767716884613
CurrentTrain: epoch  9, batch    36 | loss: 4.6481724Losses:  4.520918846130371 0.08959294855594635
CurrentTrain: epoch  9, batch    37 | loss: 4.6105118
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 0 1 0 0 1]
Losses:  7.474636077880859 1.2837495803833008
CurrentTrain: epoch  0, batch     0 | loss: 8.7583857Losses:  9.221476554870605 1.4317739009857178
CurrentTrain: epoch  0, batch     1 | loss: 10.6532507Losses:  4.448688507080078 1.3760838508605957
CurrentTrain: epoch  1, batch     0 | loss: 5.8247724Losses:  4.5726423263549805 1.4970299005508423
CurrentTrain: epoch  1, batch     1 | loss: 6.0696721Losses:  4.388538360595703 1.4742107391357422
CurrentTrain: epoch  2, batch     0 | loss: 5.8627491Losses:  3.5975518226623535 1.1693530082702637
CurrentTrain: epoch  2, batch     1 | loss: 4.7669048Losses:  3.529372215270996 1.3294696807861328
CurrentTrain: epoch  3, batch     0 | loss: 4.8588419Losses:  3.936866521835327 1.12954843044281
CurrentTrain: epoch  3, batch     1 | loss: 5.0664148Losses:  3.6879334449768066 1.1275229454040527
CurrentTrain: epoch  4, batch     0 | loss: 4.8154564Losses:  3.605656862258911 1.2230474948883057
CurrentTrain: epoch  4, batch     1 | loss: 4.8287044Losses:  3.3710227012634277 1.1277775764465332
CurrentTrain: epoch  5, batch     0 | loss: 4.4988003Losses:  3.1461501121520996 1.1107383966445923
CurrentTrain: epoch  5, batch     1 | loss: 4.2568884Losses:  3.348605155944824 1.0888745784759521
CurrentTrain: epoch  6, batch     0 | loss: 4.4374800Losses:  2.786386251449585 0.9135223627090454
CurrentTrain: epoch  6, batch     1 | loss: 3.6999087Losses:  2.7222039699554443 0.9926990270614624
CurrentTrain: epoch  7, batch     0 | loss: 3.7149029Losses:  2.738802194595337 0.884619951248169
CurrentTrain: epoch  7, batch     1 | loss: 3.6234221Losses:  3.0708746910095215 1.0064167976379395
CurrentTrain: epoch  8, batch     0 | loss: 4.0772915Losses:  2.2324209213256836 0.8902695178985596
CurrentTrain: epoch  8, batch     1 | loss: 3.1226904Losses:  2.4764156341552734 0.8937317132949829
CurrentTrain: epoch  9, batch     0 | loss: 3.3701472Losses:  2.5420334339141846 0.773075520992279
CurrentTrain: epoch  9, batch     1 | loss: 3.3151090
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 88.85%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 85.77%   
cur_acc:  ['0.8674', '0.8214']
his_acc:  ['0.8674', '0.8577']
Clustering into  3  clusters
Clusters:  [1 0 0 0 0 0 1 2 0 0 2 1 0 0 0 0]
Losses:  7.944095134735107 1.4378385543823242
CurrentTrain: epoch  0, batch     0 | loss: 9.3819332Losses:  10.42221450805664 1.1919052600860596
CurrentTrain: epoch  0, batch     1 | loss: 11.6141195Losses:  9.012036323547363 0.7536007165908813
CurrentTrain: epoch  0, batch     2 | loss: 9.7656374Losses:  5.357692718505859 1.4286134243011475
CurrentTrain: epoch  1, batch     0 | loss: 6.7863064Losses:  4.646108627319336 1.1134352684020996
CurrentTrain: epoch  1, batch     1 | loss: 5.7595439Losses:  3.605713367462158 0.8774582147598267
CurrentTrain: epoch  1, batch     2 | loss: 4.4831715Losses:  4.325839042663574 1.3088351488113403
CurrentTrain: epoch  2, batch     0 | loss: 5.6346741Losses:  4.32798957824707 1.1714625358581543
CurrentTrain: epoch  2, batch     1 | loss: 5.4994521Losses:  5.343474388122559 1.1642391681671143
CurrentTrain: epoch  2, batch     2 | loss: 6.5077133Losses:  4.635103225708008 1.2993135452270508
CurrentTrain: epoch  3, batch     0 | loss: 5.9344168Losses:  3.471407890319824 1.0028541088104248
CurrentTrain: epoch  3, batch     1 | loss: 4.4742622Losses:  4.142666339874268 0.8589872121810913
CurrentTrain: epoch  3, batch     2 | loss: 5.0016537Losses:  3.6749913692474365 1.1859121322631836
CurrentTrain: epoch  4, batch     0 | loss: 4.8609037Losses:  4.668487548828125 0.9663515090942383
CurrentTrain: epoch  4, batch     1 | loss: 5.6348391Losses:  2.5315935611724854 0.5032255053520203
CurrentTrain: epoch  4, batch     2 | loss: 3.0348191Losses:  4.257726192474365 1.0890289545059204
CurrentTrain: epoch  5, batch     0 | loss: 5.3467550Losses:  3.894057035446167 0.8716782927513123
CurrentTrain: epoch  5, batch     1 | loss: 4.7657351Losses:  1.9520480632781982 0.49170923233032227
CurrentTrain: epoch  5, batch     2 | loss: 2.4437573Losses:  3.4765098094940186 1.2325348854064941
CurrentTrain: epoch  6, batch     0 | loss: 4.7090445Losses:  3.433551073074341 1.130310297012329
CurrentTrain: epoch  6, batch     1 | loss: 4.5638614Losses:  4.798179626464844 0.31226786971092224
CurrentTrain: epoch  6, batch     2 | loss: 5.1104474Losses:  3.2341465950012207 1.248267650604248
CurrentTrain: epoch  7, batch     0 | loss: 4.4824142Losses:  3.596734046936035 1.1736493110656738
CurrentTrain: epoch  7, batch     1 | loss: 4.7703834Losses:  3.856468915939331 0.11518121510744095
CurrentTrain: epoch  7, batch     2 | loss: 3.9716501Losses:  2.2839860916137695 1.2857840061187744
CurrentTrain: epoch  8, batch     0 | loss: 3.5697701Losses:  3.5541775226593018 0.9043234586715698
CurrentTrain: epoch  8, batch     1 | loss: 4.4585009Losses:  4.09163761138916 0.43224036693573
CurrentTrain: epoch  8, batch     2 | loss: 4.5238781Losses:  2.720860481262207 1.1625003814697266
CurrentTrain: epoch  9, batch     0 | loss: 3.8833609Losses:  2.8729000091552734 0.9442754983901978
CurrentTrain: epoch  9, batch     1 | loss: 3.8171754Losses:  4.377769470214844 0.6063418388366699
CurrentTrain: epoch  9, batch     2 | loss: 4.9841113
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 37.50%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 55.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 77.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.96%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 78.84%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 77.85%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 75.25%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 74.40%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 73.23%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 71.88%   
cur_acc:  ['0.8674', '0.8214', '0.3750']
his_acc:  ['0.8674', '0.8577', '0.7188']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0]
Losses:  6.837472438812256 1.5892484188079834
CurrentTrain: epoch  0, batch     0 | loss: 8.4267206Losses:  9.102705001831055 1.3435806035995483
CurrentTrain: epoch  0, batch     1 | loss: 10.4462852Losses:  7.067851543426514 1.1889528036117554
CurrentTrain: epoch  0, batch     2 | loss: 8.2568045Losses:  2.593233108520508 1.18048095703125
CurrentTrain: epoch  1, batch     0 | loss: 3.7737141Losses:  2.6456966400146484 1.2575037479400635
CurrentTrain: epoch  1, batch     1 | loss: 3.9032004Losses:  2.9462127685546875 1.2077056169509888
CurrentTrain: epoch  1, batch     2 | loss: 4.1539183Losses:  2.4826860427856445 1.001652717590332
CurrentTrain: epoch  2, batch     0 | loss: 3.4843388Losses:  2.3311541080474854 1.0756187438964844
CurrentTrain: epoch  2, batch     1 | loss: 3.4067729Losses:  2.2853493690490723 0.9564963579177856
CurrentTrain: epoch  2, batch     2 | loss: 3.2418456Losses:  1.9880855083465576 1.2367384433746338
CurrentTrain: epoch  3, batch     0 | loss: 3.2248240Losses:  2.204241991043091 1.0175917148590088
CurrentTrain: epoch  3, batch     1 | loss: 3.2218337Losses:  2.6632907390594482 0.8225972056388855
CurrentTrain: epoch  3, batch     2 | loss: 3.4858880Losses:  2.157745838165283 0.8665032386779785
CurrentTrain: epoch  4, batch     0 | loss: 3.0242491Losses:  2.1083154678344727 1.001624584197998
CurrentTrain: epoch  4, batch     1 | loss: 3.1099401Losses:  1.8975701332092285 0.8897961378097534
CurrentTrain: epoch  4, batch     2 | loss: 2.7873664Losses:  1.8209500312805176 0.9308615922927856
CurrentTrain: epoch  5, batch     0 | loss: 2.7518115Losses:  1.8814719915390015 1.01957106590271
CurrentTrain: epoch  5, batch     1 | loss: 2.9010429Losses:  2.0142128467559814 0.6093079447746277
CurrentTrain: epoch  5, batch     2 | loss: 2.6235209Losses:  1.9375982284545898 0.7380051612854004
CurrentTrain: epoch  6, batch     0 | loss: 2.6756034Losses:  1.4796864986419678 0.865708589553833
CurrentTrain: epoch  6, batch     1 | loss: 2.3453951Losses:  2.090442657470703 0.8556603193283081
CurrentTrain: epoch  6, batch     2 | loss: 2.9461031Losses:  1.9865093231201172 0.9351580739021301
CurrentTrain: epoch  7, batch     0 | loss: 2.9216673Losses:  1.6822248697280884 0.7995126247406006
CurrentTrain: epoch  7, batch     1 | loss: 2.4817376Losses:  1.3137922286987305 0.4830125868320465
CurrentTrain: epoch  7, batch     2 | loss: 1.7968048Losses:  1.4957177639007568 0.887663722038269
CurrentTrain: epoch  8, batch     0 | loss: 2.3833814Losses:  1.7384212017059326 0.9886256456375122
CurrentTrain: epoch  8, batch     1 | loss: 2.7270470Losses:  1.7244236469268799 0.34540119767189026
CurrentTrain: epoch  8, batch     2 | loss: 2.0698249Losses:  1.4723941087722778 0.9163528680801392
CurrentTrain: epoch  9, batch     0 | loss: 2.3887470Losses:  1.6561503410339355 0.7105761170387268
CurrentTrain: epoch  9, batch     1 | loss: 2.3667264Losses:  1.7267524003982544 0.38239216804504395
CurrentTrain: epoch  9, batch     2 | loss: 2.1091447
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 83.65%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 67.75%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 64.50%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 67.24%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 68.27%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 68.28%   
cur_acc:  ['0.8674', '0.8214', '0.3750', '0.8365']
his_acc:  ['0.8674', '0.8577', '0.7188', '0.6828']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0]
Losses:  6.69621467590332 1.46134614944458
CurrentTrain: epoch  0, batch     0 | loss: 8.1575603Losses:  11.17023754119873 1.4961601495742798
CurrentTrain: epoch  0, batch     1 | loss: 12.6663980Losses:  10.225933074951172 1.319955825805664
CurrentTrain: epoch  0, batch     2 | loss: 11.5458889Losses:  3.673861265182495 1.1457573175430298
CurrentTrain: epoch  1, batch     0 | loss: 4.8196187Losses:  3.5586981773376465 1.393059253692627
CurrentTrain: epoch  1, batch     1 | loss: 4.9517574Losses:  4.054159641265869 1.2943252325057983
CurrentTrain: epoch  1, batch     2 | loss: 5.3484850Losses:  2.890047550201416 1.0605814456939697
CurrentTrain: epoch  2, batch     0 | loss: 3.9506290Losses:  4.502251148223877 1.331322431564331
CurrentTrain: epoch  2, batch     1 | loss: 5.8335733Losses:  2.9417567253112793 1.2254527807235718
CurrentTrain: epoch  2, batch     2 | loss: 4.1672096Losses:  2.933678150177002 1.4282739162445068
CurrentTrain: epoch  3, batch     0 | loss: 4.3619518Losses:  3.4756340980529785 1.2408106327056885
CurrentTrain: epoch  3, batch     1 | loss: 4.7164450Losses:  3.5200960636138916 1.3327223062515259
CurrentTrain: epoch  3, batch     2 | loss: 4.8528185Losses:  2.6624302864074707 1.3762800693511963
CurrentTrain: epoch  4, batch     0 | loss: 4.0387106Losses:  3.2513961791992188 1.2960786819458008
CurrentTrain: epoch  4, batch     1 | loss: 4.5474749Losses:  3.4780123233795166 1.2312812805175781
CurrentTrain: epoch  4, batch     2 | loss: 4.7092934Losses:  3.4788739681243896 1.1595189571380615
CurrentTrain: epoch  5, batch     0 | loss: 4.6383929Losses:  2.736363172531128 1.455121397972107
CurrentTrain: epoch  5, batch     1 | loss: 4.1914845Losses:  2.552798271179199 1.0195319652557373
CurrentTrain: epoch  5, batch     2 | loss: 3.5723302Losses:  3.0047826766967773 1.0693690776824951
CurrentTrain: epoch  6, batch     0 | loss: 4.0741520Losses:  2.0998339653015137 1.3241195678710938
CurrentTrain: epoch  6, batch     1 | loss: 3.4239535Losses:  3.294253349304199 1.000740885734558
CurrentTrain: epoch  6, batch     2 | loss: 4.2949944Losses:  2.46923565864563 1.0796834230422974
CurrentTrain: epoch  7, batch     0 | loss: 3.5489192Losses:  3.0307013988494873 1.2074129581451416
CurrentTrain: epoch  7, batch     1 | loss: 4.2381144Losses:  2.441481590270996 1.1183756589889526
CurrentTrain: epoch  7, batch     2 | loss: 3.5598574Losses:  2.4182910919189453 1.1443336009979248
CurrentTrain: epoch  8, batch     0 | loss: 3.5626247Losses:  2.596071720123291 1.052748680114746
CurrentTrain: epoch  8, batch     1 | loss: 3.6488204Losses:  2.401930809020996 1.1171231269836426
CurrentTrain: epoch  8, batch     2 | loss: 3.5190539Losses:  2.0864956378936768 0.9294027090072632
CurrentTrain: epoch  9, batch     0 | loss: 3.0158982Losses:  2.4455666542053223 1.1757845878601074
CurrentTrain: epoch  9, batch     1 | loss: 3.6213512Losses:  2.461704730987549 1.181259036064148
CurrentTrain: epoch  9, batch     2 | loss: 3.6429639
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 57.89%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 57.67%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 69.24%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 67.95%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 67.97%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 68.60%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 66.00%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 64.83%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 63.82%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 62.74%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 64.12%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 63.35%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 61.99%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 60.62%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 60.35%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 60.87%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 61.17%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 61.19%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 61.12%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 60.96%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 60.09%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 59.33%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 58.59%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 58.22%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 58.19%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 58.17%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 58.22%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 58.36%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 58.49%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 58.70%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 59.22%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 59.41%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 59.68%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 60.02%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 60.34%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 60.51%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 60.68%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 60.70%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 60.16%   
cur_acc:  ['0.8674', '0.8214', '0.3750', '0.8365', '0.5767']
his_acc:  ['0.8674', '0.8577', '0.7188', '0.6828', '0.6016']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0]
Losses:  6.3445305824279785 1.2001680135726929
CurrentTrain: epoch  0, batch     0 | loss: 7.5446987Losses:  10.049100875854492 1.4374630451202393
CurrentTrain: epoch  0, batch     1 | loss: 11.4865637Losses:  9.146854400634766 1.282505989074707
CurrentTrain: epoch  0, batch     2 | loss: 10.4293604Losses:  7.101461410522461 0.19885501265525818
CurrentTrain: epoch  0, batch     3 | loss: 7.3003163Losses:  2.363088369369507 1.2323601245880127
CurrentTrain: epoch  1, batch     0 | loss: 3.5954485Losses:  2.5388357639312744 0.9931548833847046
CurrentTrain: epoch  1, batch     1 | loss: 3.5319905Losses:  2.6317601203918457 1.137426495552063
CurrentTrain: epoch  1, batch     2 | loss: 3.7691865Losses:  2.4779467582702637 0.580977201461792
CurrentTrain: epoch  1, batch     3 | loss: 3.0589240Losses:  2.883176803588867 1.1037139892578125
CurrentTrain: epoch  2, batch     0 | loss: 3.9868908Losses:  2.305929183959961 1.0732659101486206
CurrentTrain: epoch  2, batch     1 | loss: 3.3791952Losses:  2.145451068878174 1.2908298969268799
CurrentTrain: epoch  2, batch     2 | loss: 3.4362810Losses:  1.5360236167907715 0.17149870097637177
CurrentTrain: epoch  2, batch     3 | loss: 1.7075223Losses:  2.3117053508758545 1.2361342906951904
CurrentTrain: epoch  3, batch     0 | loss: 3.5478396Losses:  2.4173645973205566 1.022536039352417
CurrentTrain: epoch  3, batch     1 | loss: 3.4399006Losses:  1.763282060623169 1.0767606496810913
CurrentTrain: epoch  3, batch     2 | loss: 2.8400426Losses:  1.6390502452850342 0.2961767911911011
CurrentTrain: epoch  3, batch     3 | loss: 1.9352270Losses:  2.2366371154785156 1.2640799283981323
CurrentTrain: epoch  4, batch     0 | loss: 3.5007172Losses:  1.9480828046798706 1.1717168092727661
CurrentTrain: epoch  4, batch     1 | loss: 3.1197996Losses:  1.8291492462158203 0.9968727231025696
CurrentTrain: epoch  4, batch     2 | loss: 2.8260219Losses:  1.3492021560668945 0.39401766657829285
CurrentTrain: epoch  4, batch     3 | loss: 1.7432199Losses:  2.3089897632598877 0.9818165898323059
CurrentTrain: epoch  5, batch     0 | loss: 3.2908063Losses:  1.9865305423736572 1.080690622329712
CurrentTrain: epoch  5, batch     1 | loss: 3.0672212Losses:  1.3426258563995361 1.162994623184204
CurrentTrain: epoch  5, batch     2 | loss: 2.5056205Losses:  1.4711074829101562 0.32536640763282776
CurrentTrain: epoch  5, batch     3 | loss: 1.7964739Losses:  1.5628881454467773 1.0671539306640625
CurrentTrain: epoch  6, batch     0 | loss: 2.6300421Losses:  2.058467149734497 1.058914065361023
CurrentTrain: epoch  6, batch     1 | loss: 3.1173811Losses:  1.7751916646957397 0.8981086015701294
CurrentTrain: epoch  6, batch     2 | loss: 2.6733003Losses:  1.5048730373382568 0.3397737145423889
CurrentTrain: epoch  6, batch     3 | loss: 1.8446467Losses:  1.624531626701355 0.9626191854476929
CurrentTrain: epoch  7, batch     0 | loss: 2.5871508Losses:  1.5766907930374146 1.0103696584701538
CurrentTrain: epoch  7, batch     1 | loss: 2.5870605Losses:  1.7840385437011719 1.060909628868103
CurrentTrain: epoch  7, batch     2 | loss: 2.8449483Losses:  1.9402549266815186 0.3578821122646332
CurrentTrain: epoch  7, batch     3 | loss: 2.2981369Losses:  1.6506850719451904 1.0081238746643066
CurrentTrain: epoch  8, batch     0 | loss: 2.6588089Losses:  1.5514214038848877 0.8921476602554321
CurrentTrain: epoch  8, batch     1 | loss: 2.4435692Losses:  1.5747017860412598 1.0636603832244873
CurrentTrain: epoch  8, batch     2 | loss: 2.6383622Losses:  1.4344663619995117 0.319121390581131
CurrentTrain: epoch  8, batch     3 | loss: 1.7535877Losses:  1.5764315128326416 1.0572606325149536
CurrentTrain: epoch  9, batch     0 | loss: 2.6336923Losses:  1.6728801727294922 1.0096509456634521
CurrentTrain: epoch  9, batch     1 | loss: 2.6825311Losses:  1.453622817993164 0.7623387575149536
CurrentTrain: epoch  9, batch     2 | loss: 2.2159615Losses:  1.1303939819335938 0.18547718226909637
CurrentTrain: epoch  9, batch     3 | loss: 1.3158711
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 99.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.10%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 73.96%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 67.73%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 65.32%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 63.33%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.66%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 64.76%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 64.30%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 63.75%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 63.42%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 62.10%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 61.91%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 61.83%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 61.74%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 61.47%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 61.12%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 60.60%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 59.73%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 58.89%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 58.07%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 57.71%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 57.60%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 57.58%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 57.24%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 57.14%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 57.13%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 57.20%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 57.73%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 58.10%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 58.38%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 58.73%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 59.08%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 59.04%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 59.16%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 59.27%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 60.56%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 60.99%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 62.89%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 63.32%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 63.99%   
cur_acc:  ['0.8674', '0.8214', '0.3750', '0.8365', '0.5767', '0.9241']
his_acc:  ['0.8674', '0.8577', '0.7188', '0.6828', '0.6016', '0.6399']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0 1 2 0 0 0]
Losses:  6.663100242614746 1.30375337600708
CurrentTrain: epoch  0, batch     0 | loss: 7.9668536Losses:  10.186229705810547 1.2291022539138794
CurrentTrain: epoch  0, batch     1 | loss: 11.4153318Losses:  8.756851196289062 1.1970857381820679
CurrentTrain: epoch  0, batch     2 | loss: 9.9539366Losses:  9.28986644744873 0.8417225480079651
CurrentTrain: epoch  0, batch     3 | loss: 10.1315889Losses:  2.988920211791992 1.2103586196899414
CurrentTrain: epoch  1, batch     0 | loss: 4.1992788Losses:  3.069247245788574 1.2011847496032715
CurrentTrain: epoch  1, batch     1 | loss: 4.2704320Losses:  2.2746987342834473 1.2584140300750732
CurrentTrain: epoch  1, batch     2 | loss: 3.5331128Losses:  2.3201141357421875 0.8613431453704834
CurrentTrain: epoch  1, batch     3 | loss: 3.1814573Losses:  2.5160810947418213 1.0678093433380127
CurrentTrain: epoch  2, batch     0 | loss: 3.5838904Losses:  2.2979283332824707 1.0635135173797607
CurrentTrain: epoch  2, batch     1 | loss: 3.3614419Losses:  2.9195737838745117 1.3085601329803467
CurrentTrain: epoch  2, batch     2 | loss: 4.2281342Losses:  2.0551650524139404 0.8799809217453003
CurrentTrain: epoch  2, batch     3 | loss: 2.9351459Losses:  2.7605724334716797 1.202100396156311
CurrentTrain: epoch  3, batch     0 | loss: 3.9626727Losses:  2.8862435817718506 1.053670883178711
CurrentTrain: epoch  3, batch     1 | loss: 3.9399145Losses:  1.6795408725738525 1.213767409324646
CurrentTrain: epoch  3, batch     2 | loss: 2.8933082Losses:  1.931389331817627 0.5730398893356323
CurrentTrain: epoch  3, batch     3 | loss: 2.5044293Losses:  1.9376522302627563 0.9843235015869141
CurrentTrain: epoch  4, batch     0 | loss: 2.9219756Losses:  2.257575511932373 1.0032929182052612
CurrentTrain: epoch  4, batch     1 | loss: 3.2608685Losses:  2.02128267288208 1.2117267847061157
CurrentTrain: epoch  4, batch     2 | loss: 3.2330093Losses:  2.6110661029815674 0.5823643207550049
CurrentTrain: epoch  4, batch     3 | loss: 3.1934304Losses:  2.776468276977539 0.8939301371574402
CurrentTrain: epoch  5, batch     0 | loss: 3.6703985Losses:  1.5279717445373535 0.8020777702331543
CurrentTrain: epoch  5, batch     1 | loss: 2.3300495Losses:  1.695372223854065 1.0551857948303223
CurrentTrain: epoch  5, batch     2 | loss: 2.7505579Losses:  2.299724578857422 0.7276103496551514
CurrentTrain: epoch  5, batch     3 | loss: 3.0273349Losses:  2.2408881187438965 1.042593002319336
CurrentTrain: epoch  6, batch     0 | loss: 3.2834811Losses:  2.1909427642822266 1.001713752746582
CurrentTrain: epoch  6, batch     1 | loss: 3.1926565Losses:  1.8197729587554932 0.8025550842285156
CurrentTrain: epoch  6, batch     2 | loss: 2.6223280Losses:  0.8982353806495667 0.5178612470626831
CurrentTrain: epoch  6, batch     3 | loss: 1.4160967Losses:  1.9178911447525024 0.9994544386863708
CurrentTrain: epoch  7, batch     0 | loss: 2.9173455Losses:  1.5670220851898193 0.9443141222000122
CurrentTrain: epoch  7, batch     1 | loss: 2.5113363Losses:  1.4609103202819824 1.0655101537704468
CurrentTrain: epoch  7, batch     2 | loss: 2.5264206Losses:  2.3912553787231445 0.7765778303146362
CurrentTrain: epoch  7, batch     3 | loss: 3.1678333Losses:  1.3007524013519287 1.0867526531219482
CurrentTrain: epoch  8, batch     0 | loss: 2.3875051Losses:  1.944406270980835 0.605073869228363
CurrentTrain: epoch  8, batch     1 | loss: 2.5494802Losses:  1.7185492515563965 0.9166868925094604
CurrentTrain: epoch  8, batch     2 | loss: 2.6352363Losses:  1.7134240865707397 0.5108712911605835
CurrentTrain: epoch  8, batch     3 | loss: 2.2242954Losses:  1.6710628271102905 0.6997702121734619
CurrentTrain: epoch  9, batch     0 | loss: 2.3708329Losses:  1.4808545112609863 0.8511284589767456
CurrentTrain: epoch  9, batch     1 | loss: 2.3319831Losses:  1.4969820976257324 0.8624446392059326
CurrentTrain: epoch  9, batch     2 | loss: 2.3594267Losses:  1.9367866516113281 0.529059886932373
CurrentTrain: epoch  9, batch     3 | loss: 2.4658465
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 42.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 67.31%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 64.50%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 62.62%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 61.44%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 61.69%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 62.28%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 61.76%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 61.15%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 60.25%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 59.27%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 58.43%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 57.52%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 57.21%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 56.72%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 56.72%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 56.43%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 56.07%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 55.27%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 54.49%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 53.73%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 53.42%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 53.29%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 53.33%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 53.29%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 53.25%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 53.29%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 53.48%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 54.06%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 54.40%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 54.73%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 55.05%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 55.21%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 54.56%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 53.92%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 53.52%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 53.76%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 54.28%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 54.79%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.29%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.77%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 57.17%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 57.42%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 57.28%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 57.21%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 57.58%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 58.58%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 58.86%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 59.07%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 59.35%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 59.61%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 59.93%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 60.24%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 60.55%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 60.92%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 60.95%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 61.02%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 61.03%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 60.78%   
cur_acc:  ['0.8674', '0.8214', '0.3750', '0.8365', '0.5767', '0.9241', '0.7708']
his_acc:  ['0.8674', '0.8577', '0.7188', '0.6828', '0.6016', '0.6399', '0.6078']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 1 3 0 0 3 1 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0 1 0 0 3 1 0 0 0 1
 2 1 0 0]
Losses:  6.192495346069336 1.0464918613433838
CurrentTrain: epoch  0, batch     0 | loss: 7.2389870Losses:  8.84500789642334 1.11699378490448
CurrentTrain: epoch  0, batch     1 | loss: 9.9620018Losses:  9.176895141601562 1.0565205812454224
CurrentTrain: epoch  0, batch     2 | loss: 10.2334156Losses:  9.936643600463867 1.1500872373580933
CurrentTrain: epoch  0, batch     3 | loss: 11.0867310Losses:  2.1661505699157715 1.0754098892211914
CurrentTrain: epoch  1, batch     0 | loss: 3.2415605Losses:  2.4452505111694336 1.2335748672485352
CurrentTrain: epoch  1, batch     1 | loss: 3.6788254Losses:  2.7446985244750977 0.8679165840148926
CurrentTrain: epoch  1, batch     2 | loss: 3.6126151Losses:  1.737928867340088 0.6777639389038086
CurrentTrain: epoch  1, batch     3 | loss: 2.4156928Losses:  1.980910062789917 1.0134038925170898
CurrentTrain: epoch  2, batch     0 | loss: 2.9943140Losses:  2.326049327850342 1.0789580345153809
CurrentTrain: epoch  2, batch     1 | loss: 3.4050074Losses:  1.827880620956421 0.8385139107704163
CurrentTrain: epoch  2, batch     2 | loss: 2.6663945Losses:  2.0141348838806152 0.8286792039871216
CurrentTrain: epoch  2, batch     3 | loss: 2.8428140Losses:  1.723555326461792 1.0253398418426514
CurrentTrain: epoch  3, batch     0 | loss: 2.7488952Losses:  2.017841100692749 1.0973987579345703
CurrentTrain: epoch  3, batch     1 | loss: 3.1152399Losses:  1.8488616943359375 0.8117058277130127
CurrentTrain: epoch  3, batch     2 | loss: 2.6605675Losses:  1.6856184005737305 0.7906606197357178
CurrentTrain: epoch  3, batch     3 | loss: 2.4762790Losses:  1.508946180343628 0.982538104057312
CurrentTrain: epoch  4, batch     0 | loss: 2.4914842Losses:  1.287238359451294 0.9353638291358948
CurrentTrain: epoch  4, batch     1 | loss: 2.2226021Losses:  1.9417364597320557 0.8330540060997009
CurrentTrain: epoch  4, batch     2 | loss: 2.7747905Losses:  2.254256010055542 0.5406646728515625
CurrentTrain: epoch  4, batch     3 | loss: 2.7949207Losses:  2.0031747817993164 0.8477922677993774
CurrentTrain: epoch  5, batch     0 | loss: 2.8509669Losses:  1.2661738395690918 0.8934392929077148
CurrentTrain: epoch  5, batch     1 | loss: 2.1596131Losses:  1.7892621755599976 0.8758095502853394
CurrentTrain: epoch  5, batch     2 | loss: 2.6650717Losses:  1.4982532262802124 0.8718190789222717
CurrentTrain: epoch  5, batch     3 | loss: 2.3700724Losses:  1.5561434030532837 0.825253963470459
CurrentTrain: epoch  6, batch     0 | loss: 2.3813972Losses:  1.3534749746322632 0.7031748294830322
CurrentTrain: epoch  6, batch     1 | loss: 2.0566497Losses:  1.7636959552764893 0.8167160749435425
CurrentTrain: epoch  6, batch     2 | loss: 2.5804119Losses:  1.3651924133300781 0.79343181848526
CurrentTrain: epoch  6, batch     3 | loss: 2.1586242Losses:  1.537445306777954 0.9439065456390381
CurrentTrain: epoch  7, batch     0 | loss: 2.4813519Losses:  1.3252613544464111 0.6614315509796143
CurrentTrain: epoch  7, batch     1 | loss: 1.9866929Losses:  1.4791195392608643 0.7979837656021118
CurrentTrain: epoch  7, batch     2 | loss: 2.2771034Losses:  1.2247374057769775 0.7643077373504639
CurrentTrain: epoch  7, batch     3 | loss: 1.9890451Losses:  1.4231444597244263 0.7205630540847778
CurrentTrain: epoch  8, batch     0 | loss: 2.1437075Losses:  1.0407578945159912 0.9210394620895386
CurrentTrain: epoch  8, batch     1 | loss: 1.9617974Losses:  1.200416088104248 0.7310516834259033
CurrentTrain: epoch  8, batch     2 | loss: 1.9314678Losses:  1.5578954219818115 0.6993029713630676
CurrentTrain: epoch  8, batch     3 | loss: 2.2571983Losses:  1.3225295543670654 0.6807438731193542
CurrentTrain: epoch  9, batch     0 | loss: 2.0032735Losses:  1.2138071060180664 0.8468989729881287
CurrentTrain: epoch  9, batch     1 | loss: 2.0607061Losses:  1.0950145721435547 0.7903509140014648
CurrentTrain: epoch  9, batch     2 | loss: 1.8853655Losses:  1.4338752031326294 0.6838609576225281
CurrentTrain: epoch  9, batch     3 | loss: 2.1177361
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.91%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 88.19%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 17.86%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 49.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.47%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 64.06%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.33%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 60.69%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 58.91%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 58.54%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 58.48%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 58.28%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 57.67%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 57.36%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.16%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 56.38%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 55.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 54.66%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 54.21%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 53.30%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 53.01%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 53.07%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 53.29%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 52.37%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 51.69%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 51.25%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 50.41%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 49.60%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 48.91%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 48.24%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 47.60%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 47.16%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 47.39%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 47.33%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 47.19%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 46.61%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 45.95%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 45.31%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 45.12%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 45.02%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 45.17%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 45.23%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 45.37%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 45.51%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 45.81%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 46.48%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 46.99%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 47.64%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 48.04%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 48.29%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 47.72%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 47.17%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 46.70%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 47.02%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 47.61%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 48.12%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 48.70%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 49.25%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 49.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 50.33%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 50.86%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 51.17%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 51.03%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 50.83%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 51.26%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 51.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 52.10%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 51.96%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 51.82%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 51.74%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 51.61%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 51.59%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 51.99%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 52.37%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 52.81%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 53.27%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 53.01%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 52.65%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 52.36%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 51.90%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 52.10%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 52.46%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 52.70%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 53.05%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 53.33%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 53.62%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 53.94%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 54.32%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 54.49%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 54.70%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 54.86%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 55.17%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 55.52%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 55.86%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 56.20%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 56.54%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 56.87%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 56.91%   
cur_acc:  ['0.8674', '0.8214', '0.3750', '0.8365', '0.5767', '0.9241', '0.7708', '0.8819']
his_acc:  ['0.8674', '0.8577', '0.7188', '0.6828', '0.6016', '0.6399', '0.6078', '0.5691']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.156584739685059 1.8808813095092773
CurrentTrain: epoch  0, batch     0 | loss: 14.0374660Losses:  14.863170623779297 1.9619382619857788
CurrentTrain: epoch  0, batch     1 | loss: 16.8251095Losses:  15.243658065795898 1.7333335876464844
CurrentTrain: epoch  0, batch     2 | loss: 16.9769917Losses:  15.233745574951172 1.8059395551681519
CurrentTrain: epoch  0, batch     3 | loss: 17.0396843Losses:  14.830804824829102 1.817299485206604
CurrentTrain: epoch  0, batch     4 | loss: 16.6481037Losses:  15.033665657043457 1.5941720008850098
CurrentTrain: epoch  0, batch     5 | loss: 16.6278381Losses:  14.272380828857422 1.7492830753326416
CurrentTrain: epoch  0, batch     6 | loss: 16.0216637Losses:  14.326822280883789 1.5826326608657837
CurrentTrain: epoch  0, batch     7 | loss: 15.9094553Losses:  13.891884803771973 1.5929584503173828
CurrentTrain: epoch  0, batch     8 | loss: 15.4848433Losses:  13.57297134399414 1.2216267585754395
CurrentTrain: epoch  0, batch     9 | loss: 14.7945976Losses:  13.316503524780273 1.2059996128082275
CurrentTrain: epoch  0, batch    10 | loss: 14.5225029Losses:  12.978153228759766 1.5368679761886597
CurrentTrain: epoch  0, batch    11 | loss: 14.5150213Losses:  12.380257606506348 1.1016981601715088
CurrentTrain: epoch  0, batch    12 | loss: 13.4819555Losses:  12.33498764038086 1.4816129207611084
CurrentTrain: epoch  0, batch    13 | loss: 13.8166008Losses:  12.587472915649414 1.528029441833496
CurrentTrain: epoch  0, batch    14 | loss: 14.1155024Losses:  12.431941032409668 1.7079628705978394
CurrentTrain: epoch  0, batch    15 | loss: 14.1399040Losses:  11.871875762939453 1.2651960849761963
CurrentTrain: epoch  0, batch    16 | loss: 13.1370716Losses:  11.618085861206055 1.5879815816879272
CurrentTrain: epoch  0, batch    17 | loss: 13.2060671Losses:  11.740825653076172 1.4079880714416504
CurrentTrain: epoch  0, batch    18 | loss: 13.1488132Losses:  11.335599899291992 1.2215471267700195
CurrentTrain: epoch  0, batch    19 | loss: 12.5571470Losses:  11.625895500183105 1.3583792448043823
CurrentTrain: epoch  0, batch    20 | loss: 12.9842749Losses:  11.60544490814209 1.617377758026123
CurrentTrain: epoch  0, batch    21 | loss: 13.2228222Losses:  11.343027114868164 1.3390600681304932
CurrentTrain: epoch  0, batch    22 | loss: 12.6820869Losses:  11.023173332214355 1.3811746835708618
CurrentTrain: epoch  0, batch    23 | loss: 12.4043484Losses:  10.42839241027832 1.3144571781158447
CurrentTrain: epoch  0, batch    24 | loss: 11.7428493Losses:  10.278974533081055 1.3635902404785156
CurrentTrain: epoch  0, batch    25 | loss: 11.6425648Losses:  10.129927635192871 1.1469402313232422
CurrentTrain: epoch  0, batch    26 | loss: 11.2768679Losses:  10.097322463989258 1.1902830600738525
CurrentTrain: epoch  0, batch    27 | loss: 11.2876053Losses:  9.708620071411133 1.5020562410354614
CurrentTrain: epoch  0, batch    28 | loss: 11.2106762Losses:  9.40124797821045 1.304556131362915
CurrentTrain: epoch  0, batch    29 | loss: 10.7058039Losses:  9.250513076782227 1.4145135879516602
CurrentTrain: epoch  0, batch    30 | loss: 10.6650267Losses:  9.412677764892578 0.9047591686248779
CurrentTrain: epoch  0, batch    31 | loss: 10.3174372Losses:  8.849288940429688 1.1864241361618042
CurrentTrain: epoch  0, batch    32 | loss: 10.0357132Losses:  8.982734680175781 1.0939688682556152
CurrentTrain: epoch  0, batch    33 | loss: 10.0767040Losses:  8.783468246459961 1.0867528915405273
CurrentTrain: epoch  0, batch    34 | loss: 9.8702211Losses:  8.07452392578125 1.3656853437423706
CurrentTrain: epoch  0, batch    35 | loss: 9.4402094Losses:  8.179152488708496 1.147202968597412
CurrentTrain: epoch  0, batch    36 | loss: 9.3263550Losses:  7.847707748413086 0.8218331336975098
CurrentTrain: epoch  0, batch    37 | loss: 8.6695404Losses:  7.529064655303955 1.1107969284057617
CurrentTrain: epoch  1, batch     0 | loss: 8.6398621Losses:  7.447558403015137 1.1777812242507935
CurrentTrain: epoch  1, batch     1 | loss: 8.6253395Losses:  7.599883079528809 1.2179769277572632
CurrentTrain: epoch  1, batch     2 | loss: 8.8178596Losses:  7.440328598022461 1.1488070487976074
CurrentTrain: epoch  1, batch     3 | loss: 8.5891361Losses:  7.526970863342285 1.1414730548858643
CurrentTrain: epoch  1, batch     4 | loss: 8.6684437Losses:  7.78726863861084 1.0806784629821777
CurrentTrain: epoch  1, batch     5 | loss: 8.8679466Losses:  7.59185791015625 1.3019475936889648
CurrentTrain: epoch  1, batch     6 | loss: 8.8938055Losses:  7.485500335693359 1.0475244522094727
CurrentTrain: epoch  1, batch     7 | loss: 8.5330248Losses:  7.566211700439453 1.0571472644805908
CurrentTrain: epoch  1, batch     8 | loss: 8.6233587Losses:  7.140287399291992 1.0739496946334839
CurrentTrain: epoch  1, batch     9 | loss: 8.2142372Losses:  7.836182117462158 0.9660836458206177
CurrentTrain: epoch  1, batch    10 | loss: 8.8022661Losses:  7.023676872253418 0.7456393241882324
CurrentTrain: epoch  1, batch    11 | loss: 7.7693162Losses:  7.644768714904785 1.0110937356948853
CurrentTrain: epoch  1, batch    12 | loss: 8.6558628Losses:  7.298478126525879 0.8406323194503784
CurrentTrain: epoch  1, batch    13 | loss: 8.1391106Losses:  7.265765190124512 0.9217893481254578
CurrentTrain: epoch  1, batch    14 | loss: 8.1875544Losses:  7.175714492797852 0.9802950620651245
CurrentTrain: epoch  1, batch    15 | loss: 8.1560097Losses:  7.444764137268066 1.088698148727417
CurrentTrain: epoch  1, batch    16 | loss: 8.5334625Losses:  7.014725685119629 0.9513944387435913
CurrentTrain: epoch  1, batch    17 | loss: 7.9661202Losses:  7.03622579574585 0.9515320658683777
CurrentTrain: epoch  1, batch    18 | loss: 7.9877577Losses:  7.630486488342285 1.2385025024414062
CurrentTrain: epoch  1, batch    19 | loss: 8.8689890Losses:  7.122487545013428 0.9750290513038635
CurrentTrain: epoch  1, batch    20 | loss: 8.0975170Losses:  6.877511978149414 0.7459314465522766
CurrentTrain: epoch  1, batch    21 | loss: 7.6234436Losses:  6.880899429321289 0.7142354846000671
CurrentTrain: epoch  1, batch    22 | loss: 7.5951347Losses:  6.7318925857543945 0.9050201177597046
CurrentTrain: epoch  1, batch    23 | loss: 7.6369128Losses:  6.77877140045166 0.8997809886932373
CurrentTrain: epoch  1, batch    24 | loss: 7.6785526Losses:  7.184545040130615 0.8952025175094604
CurrentTrain: epoch  1, batch    25 | loss: 8.0797472Losses:  6.7848663330078125 0.8338668346405029
CurrentTrain: epoch  1, batch    26 | loss: 7.6187334Losses:  6.889813423156738 0.8288781642913818
CurrentTrain: epoch  1, batch    27 | loss: 7.7186918Losses:  6.171720504760742 0.6246748566627502
CurrentTrain: epoch  1, batch    28 | loss: 6.7963953Losses:  7.064129829406738 0.7682408094406128
CurrentTrain: epoch  1, batch    29 | loss: 7.8323708Losses:  7.563920974731445 0.9275957942008972
CurrentTrain: epoch  1, batch    30 | loss: 8.4915171Losses:  6.880072116851807 0.904143214225769
CurrentTrain: epoch  1, batch    31 | loss: 7.7842155Losses:  7.027740001678467 0.885844349861145
CurrentTrain: epoch  1, batch    32 | loss: 7.9135842Losses:  6.813505172729492 0.9098273515701294
CurrentTrain: epoch  1, batch    33 | loss: 7.7233324Losses:  6.7559814453125 0.835148811340332
CurrentTrain: epoch  1, batch    34 | loss: 7.5911303Losses:  7.346652984619141 0.7680277824401855
CurrentTrain: epoch  1, batch    35 | loss: 8.1146812Losses:  6.774932861328125 0.8609086275100708
CurrentTrain: epoch  1, batch    36 | loss: 7.6358414Losses:  7.171176910400391 0.426151841878891
CurrentTrain: epoch  1, batch    37 | loss: 7.5973287Losses:  6.384119033813477 0.7999495267868042
CurrentTrain: epoch  2, batch     0 | loss: 7.1840687Losses:  6.591414928436279 0.7592746019363403
CurrentTrain: epoch  2, batch     1 | loss: 7.3506894Losses:  6.212103843688965 0.630181074142456
CurrentTrain: epoch  2, batch     2 | loss: 6.8422852Losses:  6.204378128051758 0.6587924957275391
CurrentTrain: epoch  2, batch     3 | loss: 6.8631706Losses:  6.103933334350586 0.6818370223045349
CurrentTrain: epoch  2, batch     4 | loss: 6.7857704Losses:  6.242592811584473 0.6844738721847534
CurrentTrain: epoch  2, batch     5 | loss: 6.9270668Losses:  6.520562171936035 0.7411078214645386
CurrentTrain: epoch  2, batch     6 | loss: 7.2616701Losses:  7.025519847869873 0.8427867889404297
CurrentTrain: epoch  2, batch     7 | loss: 7.8683066Losses:  6.284463882446289 0.7248585224151611
CurrentTrain: epoch  2, batch     8 | loss: 7.0093222Losses:  6.085931777954102 0.6060543060302734
CurrentTrain: epoch  2, batch     9 | loss: 6.6919861Losses:  6.27650260925293 0.5179851651191711
CurrentTrain: epoch  2, batch    10 | loss: 6.7944880Losses:  6.155036449432373 0.46046096086502075
CurrentTrain: epoch  2, batch    11 | loss: 6.6154976Losses:  6.241659164428711 0.47531479597091675
CurrentTrain: epoch  2, batch    12 | loss: 6.7169738Losses:  7.218276023864746 0.5970686674118042
CurrentTrain: epoch  2, batch    13 | loss: 7.8153448Losses:  6.564052581787109 0.8190324902534485
CurrentTrain: epoch  2, batch    14 | loss: 7.3830853Losses:  6.557994842529297 0.6837921738624573
CurrentTrain: epoch  2, batch    15 | loss: 7.2417870Losses:  6.79456901550293 0.691859245300293
CurrentTrain: epoch  2, batch    16 | loss: 7.4864283Losses:  7.024748802185059 0.7675795555114746
CurrentTrain: epoch  2, batch    17 | loss: 7.7923284Losses:  6.563611030578613 0.6203322410583496
CurrentTrain: epoch  2, batch    18 | loss: 7.1839433Losses:  6.151734352111816 0.5957256555557251
CurrentTrain: epoch  2, batch    19 | loss: 6.7474599Losses:  6.059985160827637 0.5520806312561035
CurrentTrain: epoch  2, batch    20 | loss: 6.6120658Losses:  6.458746433258057 0.6149659156799316
CurrentTrain: epoch  2, batch    21 | loss: 7.0737123Losses:  6.280475616455078 0.6242684721946716
CurrentTrain: epoch  2, batch    22 | loss: 6.9047441Losses:  6.262087345123291 0.5550633072853088
CurrentTrain: epoch  2, batch    23 | loss: 6.8171506Losses:  6.40728759765625 0.605026125907898
CurrentTrain: epoch  2, batch    24 | loss: 7.0123138Losses:  6.33225154876709 0.6673564314842224
CurrentTrain: epoch  2, batch    25 | loss: 6.9996080Losses:  6.566039085388184 0.6379996538162231
CurrentTrain: epoch  2, batch    26 | loss: 7.2040386Losses:  6.0380449295043945 0.43021613359451294
CurrentTrain: epoch  2, batch    27 | loss: 6.4682612Losses:  5.9898271560668945 0.5184773206710815
CurrentTrain: epoch  2, batch    28 | loss: 6.5083046Losses:  6.03941535949707 0.6079707145690918
CurrentTrain: epoch  2, batch    29 | loss: 6.6473861Losses:  6.19270658493042 0.6909843683242798
CurrentTrain: epoch  2, batch    30 | loss: 6.8836908Losses:  5.957242012023926 0.5610659122467041
CurrentTrain: epoch  2, batch    31 | loss: 6.5183077Losses:  6.280616283416748 0.7011711001396179
CurrentTrain: epoch  2, batch    32 | loss: 6.9817872Losses:  6.278207302093506 0.5098675489425659
CurrentTrain: epoch  2, batch    33 | loss: 6.7880750Losses:  5.64951229095459 0.4615525007247925
CurrentTrain: epoch  2, batch    34 | loss: 6.1110649Losses:  6.048150539398193 0.6386982202529907
CurrentTrain: epoch  2, batch    35 | loss: 6.6868486Losses:  6.223868370056152 0.5039927363395691
CurrentTrain: epoch  2, batch    36 | loss: 6.7278609Losses:  5.996419429779053 0.3554825782775879
CurrentTrain: epoch  2, batch    37 | loss: 6.3519020Losses:  6.1805009841918945 0.5205411314964294
CurrentTrain: epoch  3, batch     0 | loss: 6.7010422Losses:  6.122211456298828 0.5792921185493469
CurrentTrain: epoch  3, batch     1 | loss: 6.7015038Losses:  5.81267786026001 0.5666048526763916
CurrentTrain: epoch  3, batch     2 | loss: 6.3792830Losses:  5.692970275878906 0.4738815426826477
CurrentTrain: epoch  3, batch     3 | loss: 6.1668520Losses:  5.584993362426758 0.4043066203594208
CurrentTrain: epoch  3, batch     4 | loss: 5.9892998Losses:  5.745034217834473 0.537716269493103
CurrentTrain: epoch  3, batch     5 | loss: 6.2827506Losses:  5.749469757080078 0.33732718229293823
CurrentTrain: epoch  3, batch     6 | loss: 6.0867968Losses:  5.615246295928955 0.4239159822463989
CurrentTrain: epoch  3, batch     7 | loss: 6.0391622Losses:  5.88918924331665 0.47947072982788086
CurrentTrain: epoch  3, batch     8 | loss: 6.3686600Losses:  6.022717475891113 0.42433956265449524
CurrentTrain: epoch  3, batch     9 | loss: 6.4470572Losses:  6.1957926750183105 0.6708486080169678
CurrentTrain: epoch  3, batch    10 | loss: 6.8666410Losses:  5.657598495483398 0.4289274215698242
CurrentTrain: epoch  3, batch    11 | loss: 6.0865259Losses:  5.9204559326171875 0.46997398138046265
CurrentTrain: epoch  3, batch    12 | loss: 6.3904300Losses:  5.478426456451416 0.3865882158279419
CurrentTrain: epoch  3, batch    13 | loss: 5.8650146Losses:  5.638903617858887 0.33738526701927185
CurrentTrain: epoch  3, batch    14 | loss: 5.9762888Losses:  5.831313133239746 0.5039368271827698
CurrentTrain: epoch  3, batch    15 | loss: 6.3352499Losses:  5.888824462890625 0.4453467130661011
CurrentTrain: epoch  3, batch    16 | loss: 6.3341713Losses:  5.496912002563477 0.2923884689807892
CurrentTrain: epoch  3, batch    17 | loss: 5.7893004Losses:  6.467930793762207 0.582733154296875
CurrentTrain: epoch  3, batch    18 | loss: 7.0506639Losses:  5.639034271240234 0.3848268687725067
CurrentTrain: epoch  3, batch    19 | loss: 6.0238609Losses:  6.213307857513428 0.6327180862426758
CurrentTrain: epoch  3, batch    20 | loss: 6.8460259Losses:  5.641892433166504 0.28088849782943726
CurrentTrain: epoch  3, batch    21 | loss: 5.9227810Losses:  5.698036193847656 0.3876112401485443
CurrentTrain: epoch  3, batch    22 | loss: 6.0856476Losses:  5.720117568969727 0.4360101819038391
CurrentTrain: epoch  3, batch    23 | loss: 6.1561279Losses:  5.43459415435791 0.20834025740623474
CurrentTrain: epoch  3, batch    24 | loss: 5.6429343Losses:  5.43165397644043 0.3631059527397156
CurrentTrain: epoch  3, batch    25 | loss: 5.7947598Losses:  5.517782688140869 0.438755601644516
CurrentTrain: epoch  3, batch    26 | loss: 5.9565382Losses:  6.272451400756836 0.47252732515335083
CurrentTrain: epoch  3, batch    27 | loss: 6.7449789Losses:  5.893139839172363 0.28542202711105347
CurrentTrain: epoch  3, batch    28 | loss: 6.1785617Losses:  5.467901706695557 0.47974440455436707
CurrentTrain: epoch  3, batch    29 | loss: 5.9476461Losses:  5.296913146972656 0.4498053789138794
CurrentTrain: epoch  3, batch    30 | loss: 5.7467184Losses:  5.887750625610352 0.45873260498046875
CurrentTrain: epoch  3, batch    31 | loss: 6.3464832Losses:  5.937338352203369 0.4157114028930664
CurrentTrain: epoch  3, batch    32 | loss: 6.3530498Losses:  5.268195152282715 0.3571234941482544
CurrentTrain: epoch  3, batch    33 | loss: 5.6253185Losses:  5.0446929931640625 0.24842919409275055
CurrentTrain: epoch  3, batch    34 | loss: 5.2931223Losses:  6.942413806915283 0.899744987487793
CurrentTrain: epoch  3, batch    35 | loss: 7.8421588Losses:  5.62971830368042 0.2993190288543701
CurrentTrain: epoch  3, batch    36 | loss: 5.9290371Losses:  5.311999320983887 0.23561213910579681
CurrentTrain: epoch  3, batch    37 | loss: 5.5476112Losses:  5.687359809875488 0.3544498085975647
CurrentTrain: epoch  4, batch     0 | loss: 6.0418096Losses:  5.598879337310791 0.3135637640953064
CurrentTrain: epoch  4, batch     1 | loss: 5.9124432Losses:  5.18721866607666 0.23564457893371582
CurrentTrain: epoch  4, batch     2 | loss: 5.4228630Losses:  5.418231964111328 0.2743350863456726
CurrentTrain: epoch  4, batch     3 | loss: 5.6925669Losses:  5.523894309997559 0.4846753180027008
CurrentTrain: epoch  4, batch     4 | loss: 6.0085697Losses:  5.476469993591309 0.42717286944389343
CurrentTrain: epoch  4, batch     5 | loss: 5.9036427Losses:  5.543476581573486 0.26732364296913147
CurrentTrain: epoch  4, batch     6 | loss: 5.8108001Losses:  5.3550519943237305 0.3048820495605469
CurrentTrain: epoch  4, batch     7 | loss: 5.6599340Losses:  5.696470260620117 0.3300124406814575
CurrentTrain: epoch  4, batch     8 | loss: 6.0264826Losses:  6.039322376251221 0.5728409290313721
CurrentTrain: epoch  4, batch     9 | loss: 6.6121635Losses:  5.638275146484375 0.378526508808136
CurrentTrain: epoch  4, batch    10 | loss: 6.0168018Losses:  5.360922813415527 0.3660822808742523
CurrentTrain: epoch  4, batch    11 | loss: 5.7270050Losses:  6.192033767700195 0.6452595591545105
CurrentTrain: epoch  4, batch    12 | loss: 6.8372931Losses:  5.544219017028809 0.27357593178749084
CurrentTrain: epoch  4, batch    13 | loss: 5.8177948Losses:  5.577396392822266 0.3218744695186615
CurrentTrain: epoch  4, batch    14 | loss: 5.8992710Losses:  5.30312442779541 0.3994283080101013
CurrentTrain: epoch  4, batch    15 | loss: 5.7025528Losses:  5.486631870269775 0.23124998807907104
CurrentTrain: epoch  4, batch    16 | loss: 5.7178817Losses:  5.256369590759277 0.3466842770576477
CurrentTrain: epoch  4, batch    17 | loss: 5.6030540Losses:  5.113698482513428 0.28585416078567505
CurrentTrain: epoch  4, batch    18 | loss: 5.3995528Losses:  5.263010025024414 0.33184748888015747
CurrentTrain: epoch  4, batch    19 | loss: 5.5948577Losses:  5.695586204528809 0.5213420391082764
CurrentTrain: epoch  4, batch    20 | loss: 6.2169285Losses:  5.077721118927002 0.24703362584114075
CurrentTrain: epoch  4, batch    21 | loss: 5.3247547Losses:  5.133803844451904 0.3118601143360138
CurrentTrain: epoch  4, batch    22 | loss: 5.4456639Losses:  5.36146354675293 0.3322141170501709
CurrentTrain: epoch  4, batch    23 | loss: 5.6936779Losses:  5.249333381652832 0.3820175230503082
CurrentTrain: epoch  4, batch    24 | loss: 5.6313510Losses:  5.3617072105407715 0.3425334692001343
CurrentTrain: epoch  4, batch    25 | loss: 5.7042408Losses:  5.022444725036621 0.2590634822845459
CurrentTrain: epoch  4, batch    26 | loss: 5.2815084Losses:  5.755146503448486 0.6137879490852356
CurrentTrain: epoch  4, batch    27 | loss: 6.3689346Losses:  5.294145584106445 0.4069678783416748
CurrentTrain: epoch  4, batch    28 | loss: 5.7011137Losses:  5.138673782348633 0.24664875864982605
CurrentTrain: epoch  4, batch    29 | loss: 5.3853226Losses:  5.46197509765625 0.36676305532455444
CurrentTrain: epoch  4, batch    30 | loss: 5.8287382Losses:  5.540203094482422 0.46635380387306213
CurrentTrain: epoch  4, batch    31 | loss: 6.0065570Losses:  5.541367530822754 0.26454856991767883
CurrentTrain: epoch  4, batch    32 | loss: 5.8059163Losses:  5.508502960205078 0.2277015596628189
CurrentTrain: epoch  4, batch    33 | loss: 5.7362046Losses:  5.7797136306762695 0.34352877736091614
CurrentTrain: epoch  4, batch    34 | loss: 6.1232424Losses:  5.645573616027832 0.30816036462783813
CurrentTrain: epoch  4, batch    35 | loss: 5.9537339Losses:  5.257142066955566 0.29163700342178345
CurrentTrain: epoch  4, batch    36 | loss: 5.5487790Losses:  5.044135570526123 0.2687581479549408
CurrentTrain: epoch  4, batch    37 | loss: 5.3128939Losses:  5.706311225891113 0.4372861981391907
CurrentTrain: epoch  5, batch     0 | loss: 6.1435976Losses:  5.3012189865112305 0.34073230624198914
CurrentTrain: epoch  5, batch     1 | loss: 5.6419511Losses:  5.5740180015563965 0.34205010533332825
CurrentTrain: epoch  5, batch     2 | loss: 5.9160681Losses:  4.920077323913574 0.1848406046628952
CurrentTrain: epoch  5, batch     3 | loss: 5.1049180Losses:  5.226382255554199 0.3068373203277588
CurrentTrain: epoch  5, batch     4 | loss: 5.5332193Losses:  5.057930946350098 0.3182082772254944
CurrentTrain: epoch  5, batch     5 | loss: 5.3761392Losses:  5.340775966644287 0.48876190185546875
CurrentTrain: epoch  5, batch     6 | loss: 5.8295379Losses:  5.417306900024414 0.4255746304988861
CurrentTrain: epoch  5, batch     7 | loss: 5.8428817Losses:  5.045782089233398 0.32890230417251587
CurrentTrain: epoch  5, batch     8 | loss: 5.3746843Losses:  5.0260725021362305 0.22809965908527374
CurrentTrain: epoch  5, batch     9 | loss: 5.2541723Losses:  6.216531753540039 0.435268372297287
CurrentTrain: epoch  5, batch    10 | loss: 6.6518002Losses:  5.610021591186523 0.3172706961631775
CurrentTrain: epoch  5, batch    11 | loss: 5.9272923Losses:  5.127923011779785 0.16382241249084473
CurrentTrain: epoch  5, batch    12 | loss: 5.2917452Losses:  5.057250499725342 0.25521063804626465
CurrentTrain: epoch  5, batch    13 | loss: 5.3124609Losses:  4.882408142089844 0.22371609508991241
CurrentTrain: epoch  5, batch    14 | loss: 5.1061244Losses:  4.989362716674805 0.20394381880760193
CurrentTrain: epoch  5, batch    15 | loss: 5.1933064Losses:  5.205902099609375 0.23165249824523926
CurrentTrain: epoch  5, batch    16 | loss: 5.4375544Losses:  5.1971354484558105 0.29861509799957275
CurrentTrain: epoch  5, batch    17 | loss: 5.4957504Losses:  5.529248237609863 0.5049736499786377
CurrentTrain: epoch  5, batch    18 | loss: 6.0342216Losses:  5.102358341217041 0.21489152312278748
CurrentTrain: epoch  5, batch    19 | loss: 5.3172498Losses:  5.093545913696289 0.24015255272388458
CurrentTrain: epoch  5, batch    20 | loss: 5.3336983Losses:  4.960031509399414 0.24562188982963562
CurrentTrain: epoch  5, batch    21 | loss: 5.2056532Losses:  5.014924049377441 0.19455480575561523
CurrentTrain: epoch  5, batch    22 | loss: 5.2094789Losses:  5.051140785217285 0.24400274455547333
CurrentTrain: epoch  5, batch    23 | loss: 5.2951436Losses:  4.964507102966309 0.2636686861515045
CurrentTrain: epoch  5, batch    24 | loss: 5.2281756Losses:  4.899582862854004 0.1841122806072235
CurrentTrain: epoch  5, batch    25 | loss: 5.0836949Losses:  5.881464958190918 0.35203689336776733
CurrentTrain: epoch  5, batch    26 | loss: 6.2335019Losses:  5.132584571838379 0.19845134019851685
CurrentTrain: epoch  5, batch    27 | loss: 5.3310361Losses:  4.935251235961914 0.20821702480316162
CurrentTrain: epoch  5, batch    28 | loss: 5.1434684Losses:  5.454607963562012 0.4373669922351837
CurrentTrain: epoch  5, batch    29 | loss: 5.8919749Losses:  5.246172904968262 0.245102196931839
CurrentTrain: epoch  5, batch    30 | loss: 5.4912753Losses:  5.058623313903809 0.22429201006889343
CurrentTrain: epoch  5, batch    31 | loss: 5.2829151Losses:  5.169084548950195 0.30316779017448425
CurrentTrain: epoch  5, batch    32 | loss: 5.4722524Losses:  5.084104537963867 0.21346457302570343
CurrentTrain: epoch  5, batch    33 | loss: 5.2975693Losses:  5.3310017585754395 0.299180269241333
CurrentTrain: epoch  5, batch    34 | loss: 5.6301823Losses:  5.080498695373535 0.17669673264026642
CurrentTrain: epoch  5, batch    35 | loss: 5.2571955Losses:  5.1602582931518555 0.234869122505188
CurrentTrain: epoch  5, batch    36 | loss: 5.3951273Losses:  5.338912010192871 0.3618098497390747
CurrentTrain: epoch  5, batch    37 | loss: 5.7007217Losses:  4.9404754638671875 0.2182457149028778
CurrentTrain: epoch  6, batch     0 | loss: 5.1587210Losses:  5.281975746154785 0.23502887785434723
CurrentTrain: epoch  6, batch     1 | loss: 5.5170045Losses:  5.470052719116211 0.41783401370048523
CurrentTrain: epoch  6, batch     2 | loss: 5.8878865Losses:  5.079800128936768 0.19154386222362518
CurrentTrain: epoch  6, batch     3 | loss: 5.2713442Losses:  4.820920944213867 0.20862406492233276
CurrentTrain: epoch  6, batch     4 | loss: 5.0295448Losses:  4.946942329406738 0.19075298309326172
CurrentTrain: epoch  6, batch     5 | loss: 5.1376953Losses:  5.831725120544434 0.5546659827232361
CurrentTrain: epoch  6, batch     6 | loss: 6.3863912Losses:  4.888832092285156 0.1755577027797699
CurrentTrain: epoch  6, batch     7 | loss: 5.0643897Losses:  4.915579319000244 0.21478933095932007
CurrentTrain: epoch  6, batch     8 | loss: 5.1303687Losses:  5.000612258911133 0.17438483238220215
CurrentTrain: epoch  6, batch     9 | loss: 5.1749973Losses:  5.074406623840332 0.2553005516529083
CurrentTrain: epoch  6, batch    10 | loss: 5.3297071Losses:  4.848891258239746 0.17687217891216278
CurrentTrain: epoch  6, batch    11 | loss: 5.0257635Losses:  4.932690143585205 0.19104106724262238
CurrentTrain: epoch  6, batch    12 | loss: 5.1237311Losses:  4.939142227172852 0.17679616808891296
CurrentTrain: epoch  6, batch    13 | loss: 5.1159382Losses:  5.071409702301025 0.24809607863426208
CurrentTrain: epoch  6, batch    14 | loss: 5.3195057Losses:  4.977080345153809 0.20068413019180298
CurrentTrain: epoch  6, batch    15 | loss: 5.1777644Losses:  4.847443103790283 0.16971567273139954
CurrentTrain: epoch  6, batch    16 | loss: 5.0171590Losses:  4.94144344329834 0.18554897606372833
CurrentTrain: epoch  6, batch    17 | loss: 5.1269922Losses:  4.92156982421875 0.19329220056533813
CurrentTrain: epoch  6, batch    18 | loss: 5.1148620Losses:  5.157684326171875 0.2062842845916748
CurrentTrain: epoch  6, batch    19 | loss: 5.3639688Losses:  5.400607109069824 0.29438990354537964
CurrentTrain: epoch  6, batch    20 | loss: 5.6949968Losses:  5.0708818435668945 0.23715302348136902
CurrentTrain: epoch  6, batch    21 | loss: 5.3080349Losses:  4.766942024230957 0.14416643977165222
CurrentTrain: epoch  6, batch    22 | loss: 4.9111085Losses:  4.888967514038086 0.17570137977600098
CurrentTrain: epoch  6, batch    23 | loss: 5.0646687Losses:  5.109728813171387 0.2714874744415283
CurrentTrain: epoch  6, batch    24 | loss: 5.3812160Losses:  4.88430118560791 0.1783927083015442
CurrentTrain: epoch  6, batch    25 | loss: 5.0626941Losses:  5.287035942077637 0.22833722829818726
CurrentTrain: epoch  6, batch    26 | loss: 5.5153732Losses:  4.884413719177246 0.24888628721237183
CurrentTrain: epoch  6, batch    27 | loss: 5.1332998Losses:  4.942889213562012 0.1796998679637909
CurrentTrain: epoch  6, batch    28 | loss: 5.1225891Losses:  4.717658042907715 0.13282835483551025
CurrentTrain: epoch  6, batch    29 | loss: 4.8504863Losses:  5.097890377044678 0.36702919006347656
CurrentTrain: epoch  6, batch    30 | loss: 5.4649196Losses:  4.911644458770752 0.16784560680389404
CurrentTrain: epoch  6, batch    31 | loss: 5.0794902Losses:  4.868742942810059 0.19188371300697327
CurrentTrain: epoch  6, batch    32 | loss: 5.0606265Losses:  5.175775051116943 0.2801443934440613
CurrentTrain: epoch  6, batch    33 | loss: 5.4559193Losses:  4.789552688598633 0.23678646981716156
CurrentTrain: epoch  6, batch    34 | loss: 5.0263391Losses:  4.966719150543213 0.22642332315444946
CurrentTrain: epoch  6, batch    35 | loss: 5.1931424Losses:  4.866559982299805 0.1592714488506317
CurrentTrain: epoch  6, batch    36 | loss: 5.0258312Losses:  4.702269554138184 0.1973097026348114
CurrentTrain: epoch  6, batch    37 | loss: 4.8995790Losses:  5.100637435913086 0.24740397930145264
CurrentTrain: epoch  7, batch     0 | loss: 5.3480415Losses:  4.89503812789917 0.15016047656536102
CurrentTrain: epoch  7, batch     1 | loss: 5.0451984Losses:  4.757522106170654 0.18179422616958618
CurrentTrain: epoch  7, batch     2 | loss: 4.9393163Losses:  4.785292625427246 0.18565261363983154
CurrentTrain: epoch  7, batch     3 | loss: 4.9709454Losses:  5.069247245788574 0.17506930232048035
CurrentTrain: epoch  7, batch     4 | loss: 5.2443166Losses:  4.679305076599121 0.17382201552391052
CurrentTrain: epoch  7, batch     5 | loss: 4.8531270Losses:  4.809455394744873 0.17647786438465118
CurrentTrain: epoch  7, batch     6 | loss: 4.9859333Losses:  4.873655319213867 0.18528030812740326
CurrentTrain: epoch  7, batch     7 | loss: 5.0589356Losses:  4.685461044311523 0.16125023365020752
CurrentTrain: epoch  7, batch     8 | loss: 4.8467112Losses:  4.787826061248779 0.1936478614807129
CurrentTrain: epoch  7, batch     9 | loss: 4.9814739Losses:  4.931880950927734 0.23046201467514038
CurrentTrain: epoch  7, batch    10 | loss: 5.1623430Losses:  4.6678972244262695 0.11668526381254196
CurrentTrain: epoch  7, batch    11 | loss: 4.7845826Losses:  4.918272972106934 0.22786030173301697
CurrentTrain: epoch  7, batch    12 | loss: 5.1461334Losses:  4.873813629150391 0.18130116164684296
CurrentTrain: epoch  7, batch    13 | loss: 5.0551147Losses:  5.02767276763916 0.21720728278160095
CurrentTrain: epoch  7, batch    14 | loss: 5.2448802Losses:  4.694306373596191 0.20663806796073914
CurrentTrain: epoch  7, batch    15 | loss: 4.9009442Losses:  4.662135124206543 0.15122005343437195
CurrentTrain: epoch  7, batch    16 | loss: 4.8133550Losses:  4.791182518005371 0.19570761919021606
CurrentTrain: epoch  7, batch    17 | loss: 4.9868903Losses:  4.721243858337402 0.17788013815879822
CurrentTrain: epoch  7, batch    18 | loss: 4.8991241Losses:  4.859963417053223 0.20740920305252075
CurrentTrain: epoch  7, batch    19 | loss: 5.0673728Losses:  4.7312188148498535 0.12373556196689606
CurrentTrain: epoch  7, batch    20 | loss: 4.8549542Losses:  4.661469459533691 0.1348101645708084
CurrentTrain: epoch  7, batch    21 | loss: 4.7962794Losses:  4.813407897949219 0.12299936264753342
CurrentTrain: epoch  7, batch    22 | loss: 4.9364071Losses:  4.672769546508789 0.16124242544174194
CurrentTrain: epoch  7, batch    23 | loss: 4.8340120Losses:  4.672630310058594 0.16230738162994385
CurrentTrain: epoch  7, batch    24 | loss: 4.8349376Losses:  4.663632869720459 0.1722409874200821
CurrentTrain: epoch  7, batch    25 | loss: 4.8358741Losses:  4.564794540405273 0.1043645441532135
CurrentTrain: epoch  7, batch    26 | loss: 4.6691589Losses:  4.735199451446533 0.14074143767356873
CurrentTrain: epoch  7, batch    27 | loss: 4.8759408Losses:  4.7337517738342285 0.1650587022304535
CurrentTrain: epoch  7, batch    28 | loss: 4.8988104Losses:  4.685691833496094 0.11651021242141724
CurrentTrain: epoch  7, batch    29 | loss: 4.8022022Losses:  4.694479465484619 0.15883170068264008
CurrentTrain: epoch  7, batch    30 | loss: 4.8533111Losses:  4.632018089294434 0.16568876802921295
CurrentTrain: epoch  7, batch    31 | loss: 4.7977071Losses:  4.604506492614746 0.1502331644296646
CurrentTrain: epoch  7, batch    32 | loss: 4.7547398Losses:  4.822633743286133 0.17273256182670593
CurrentTrain: epoch  7, batch    33 | loss: 4.9953661Losses:  4.668171405792236 0.17104783654212952
CurrentTrain: epoch  7, batch    34 | loss: 4.8392191Losses:  4.583446979522705 0.1463821530342102
CurrentTrain: epoch  7, batch    35 | loss: 4.7298293Losses:  4.626378059387207 0.10063998401165009
CurrentTrain: epoch  7, batch    36 | loss: 4.7270179Losses:  4.591731071472168 0.03962921351194382
CurrentTrain: epoch  7, batch    37 | loss: 4.6313601Losses:  4.644018650054932 0.13671225309371948
CurrentTrain: epoch  8, batch     0 | loss: 4.7807307Losses:  4.653414726257324 0.14891965687274933
CurrentTrain: epoch  8, batch     1 | loss: 4.8023343Losses:  4.64797306060791 0.1383177936077118
CurrentTrain: epoch  8, batch     2 | loss: 4.7862906Losses:  4.885472774505615 0.2912801504135132
CurrentTrain: epoch  8, batch     3 | loss: 5.1767530Losses:  4.58325719833374 0.12278270721435547
CurrentTrain: epoch  8, batch     4 | loss: 4.7060399Losses:  4.678343772888184 0.16074706614017487
CurrentTrain: epoch  8, batch     5 | loss: 4.8390908Losses:  4.75603723526001 0.17180532217025757
CurrentTrain: epoch  8, batch     6 | loss: 4.9278426Losses:  4.59805965423584 0.14237090945243835
CurrentTrain: epoch  8, batch     7 | loss: 4.7404304Losses:  4.648471832275391 0.1480703055858612
CurrentTrain: epoch  8, batch     8 | loss: 4.7965422Losses:  5.199001312255859 0.22704783082008362
CurrentTrain: epoch  8, batch     9 | loss: 5.4260492Losses:  4.646655082702637 0.1356428861618042
CurrentTrain: epoch  8, batch    10 | loss: 4.7822981Losses:  4.640498638153076 0.13003136217594147
CurrentTrain: epoch  8, batch    11 | loss: 4.7705302Losses:  4.629117965698242 0.13524113595485687
CurrentTrain: epoch  8, batch    12 | loss: 4.7643590Losses:  4.580846309661865 0.11766906082630157
CurrentTrain: epoch  8, batch    13 | loss: 4.6985154Losses:  4.587255477905273 0.09013181924819946
CurrentTrain: epoch  8, batch    14 | loss: 4.6773872Losses:  4.636811256408691 0.13784044981002808
CurrentTrain: epoch  8, batch    15 | loss: 4.7746515Losses:  4.573266506195068 0.12115804851055145
CurrentTrain: epoch  8, batch    16 | loss: 4.6944246Losses:  4.7624711990356445 0.1585456132888794
CurrentTrain: epoch  8, batch    17 | loss: 4.9210167Losses:  4.665765762329102 0.1425999104976654
CurrentTrain: epoch  8, batch    18 | loss: 4.8083658Losses:  4.611397743225098 0.11848513036966324
CurrentTrain: epoch  8, batch    19 | loss: 4.7298827Losses:  4.581029415130615 0.11931775510311127
CurrentTrain: epoch  8, batch    20 | loss: 4.7003469Losses:  4.9513349533081055 0.18501374125480652
CurrentTrain: epoch  8, batch    21 | loss: 5.1363487Losses:  4.655834674835205 0.11145639419555664
CurrentTrain: epoch  8, batch    22 | loss: 4.7672911Losses:  4.633441925048828 0.09604312479496002
CurrentTrain: epoch  8, batch    23 | loss: 4.7294850Losses:  4.621280670166016 0.1358235776424408
CurrentTrain: epoch  8, batch    24 | loss: 4.7571044Losses:  4.620080471038818 0.130078986287117
CurrentTrain: epoch  8, batch    25 | loss: 4.7501593Losses:  4.600804328918457 0.11738133430480957
CurrentTrain: epoch  8, batch    26 | loss: 4.7181854Losses:  4.554755210876465 0.13890066742897034
CurrentTrain: epoch  8, batch    27 | loss: 4.6936560Losses:  4.573833465576172 0.11986521631479263
CurrentTrain: epoch  8, batch    28 | loss: 4.6936989Losses:  4.52823543548584 0.10248303413391113
CurrentTrain: epoch  8, batch    29 | loss: 4.6307182Losses:  4.618779182434082 0.12857963144779205
CurrentTrain: epoch  8, batch    30 | loss: 4.7473588Losses:  4.602108001708984 0.1294621229171753
CurrentTrain: epoch  8, batch    31 | loss: 4.7315702Losses:  4.571418762207031 0.09260041266679764
CurrentTrain: epoch  8, batch    32 | loss: 4.6640191Losses:  4.589695453643799 0.11497485637664795
CurrentTrain: epoch  8, batch    33 | loss: 4.7046704Losses:  4.713040351867676 0.1467389166355133
CurrentTrain: epoch  8, batch    34 | loss: 4.8597794Losses:  4.554988861083984 0.11168290674686432
CurrentTrain: epoch  8, batch    35 | loss: 4.6666718Losses:  4.654869556427002 0.12013820558786392
CurrentTrain: epoch  8, batch    36 | loss: 4.7750077Losses:  4.559839248657227 0.08192312717437744
CurrentTrain: epoch  8, batch    37 | loss: 4.6417623Losses:  4.686637878417969 0.15202713012695312
CurrentTrain: epoch  9, batch     0 | loss: 4.8386650Losses:  4.558533668518066 0.11444774270057678
CurrentTrain: epoch  9, batch     1 | loss: 4.6729813Losses:  4.561011791229248 0.10497336834669113
CurrentTrain: epoch  9, batch     2 | loss: 4.6659851Losses:  4.587244510650635 0.1061384528875351
CurrentTrain: epoch  9, batch     3 | loss: 4.6933827Losses:  4.566382884979248 0.10644913464784622
CurrentTrain: epoch  9, batch     4 | loss: 4.6728320Losses:  4.5696492195129395 0.1049022376537323
CurrentTrain: epoch  9, batch     5 | loss: 4.6745515Losses:  4.570964813232422 0.10001929104328156
CurrentTrain: epoch  9, batch     6 | loss: 4.6709843Losses:  4.565611362457275 0.10829710960388184
CurrentTrain: epoch  9, batch     7 | loss: 4.6739082Losses:  4.625360488891602 0.12379219383001328
CurrentTrain: epoch  9, batch     8 | loss: 4.7491527Losses:  4.565218925476074 0.10546751320362091
CurrentTrain: epoch  9, batch     9 | loss: 4.6706862Losses:  4.541023254394531 0.08259786665439606
CurrentTrain: epoch  9, batch    10 | loss: 4.6236210Losses:  4.648223876953125 0.09475219994783401
CurrentTrain: epoch  9, batch    11 | loss: 4.7429762Losses:  4.543152809143066 0.1158502995967865
CurrentTrain: epoch  9, batch    12 | loss: 4.6590033Losses:  4.565743923187256 0.11350367963314056
CurrentTrain: epoch  9, batch    13 | loss: 4.6792474Losses:  4.538265228271484 0.08525833487510681
CurrentTrain: epoch  9, batch    14 | loss: 4.6235237Losses:  4.5687255859375 0.10738378763198853
CurrentTrain: epoch  9, batch    15 | loss: 4.6761093Losses:  4.520950794219971 0.10772620141506195
CurrentTrain: epoch  9, batch    16 | loss: 4.6286769Losses:  4.527027606964111 0.07172133028507233
CurrentTrain: epoch  9, batch    17 | loss: 4.5987492Losses:  4.569064617156982 0.09566900134086609
CurrentTrain: epoch  9, batch    18 | loss: 4.6647334Losses:  4.546327590942383 0.10857722163200378
CurrentTrain: epoch  9, batch    19 | loss: 4.6549048Losses:  4.533262729644775 0.06855769455432892
CurrentTrain: epoch  9, batch    20 | loss: 4.6018205Losses:  4.56563663482666 0.11504721641540527
CurrentTrain: epoch  9, batch    21 | loss: 4.6806841Losses:  4.551165580749512 0.11086537688970566
CurrentTrain: epoch  9, batch    22 | loss: 4.6620312Losses:  4.490893840789795 0.09955392777919769
CurrentTrain: epoch  9, batch    23 | loss: 4.5904479Losses:  4.943423748016357 0.10297198593616486
CurrentTrain: epoch  9, batch    24 | loss: 5.0463958Losses:  4.536809921264648 0.0892910435795784
CurrentTrain: epoch  9, batch    25 | loss: 4.6261010Losses:  4.600179672241211 0.10880478471517563
CurrentTrain: epoch  9, batch    26 | loss: 4.7089844Losses:  4.557031631469727 0.08613653481006622
CurrentTrain: epoch  9, batch    27 | loss: 4.6431680Losses:  4.525909423828125 0.07428129017353058
CurrentTrain: epoch  9, batch    28 | loss: 4.6001906Losses:  4.5868682861328125 0.09520592540502548
CurrentTrain: epoch  9, batch    29 | loss: 4.6820741Losses:  4.547079563140869 0.0897158533334732
CurrentTrain: epoch  9, batch    30 | loss: 4.6367955Losses:  4.560816764831543 0.09477768838405609
CurrentTrain: epoch  9, batch    31 | loss: 4.6555943Losses:  4.583568572998047 0.11923325061798096
CurrentTrain: epoch  9, batch    32 | loss: 4.7028017Losses:  4.549416542053223 0.10851946473121643
CurrentTrain: epoch  9, batch    33 | loss: 4.6579361Losses:  4.567553520202637 0.10988495498895645
CurrentTrain: epoch  9, batch    34 | loss: 4.6774383Losses:  4.5225043296813965 0.0953521803021431
CurrentTrain: epoch  9, batch    35 | loss: 4.6178565Losses:  4.515368461608887 0.0995345339179039
CurrentTrain: epoch  9, batch    36 | loss: 4.6149030Losses:  4.476490497589111 0.0616249181330204
CurrentTrain: epoch  9, batch    37 | loss: 4.5381155
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
cur_acc:  ['0.8731']
his_acc:  ['0.8731']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 1 0 0 0 0]
Losses:  7.158572196960449 1.2976725101470947
CurrentTrain: epoch  0, batch     0 | loss: 8.4562445Losses:  9.641956329345703 1.1447789669036865
CurrentTrain: epoch  0, batch     1 | loss: 10.7867355Losses:  3.573819160461426 1.1385838985443115
CurrentTrain: epoch  1, batch     0 | loss: 4.7124033Losses:  3.346378803253174 1.0821480751037598
CurrentTrain: epoch  1, batch     1 | loss: 4.4285269Losses:  3.490084409713745 1.3376293182373047
CurrentTrain: epoch  2, batch     0 | loss: 4.8277140Losses:  3.1902823448181152 0.9950065612792969
CurrentTrain: epoch  2, batch     1 | loss: 4.1852889Losses:  3.0118861198425293 1.205513834953308
CurrentTrain: epoch  3, batch     0 | loss: 4.2174001Losses:  3.6108641624450684 0.9583529829978943
CurrentTrain: epoch  3, batch     1 | loss: 4.5692172Losses:  3.461826801300049 0.9245734214782715
CurrentTrain: epoch  4, batch     0 | loss: 4.3864002Losses:  2.542124032974243 0.9489749670028687
CurrentTrain: epoch  4, batch     1 | loss: 3.4910989Losses:  2.838686943054199 0.8387546539306641
CurrentTrain: epoch  5, batch     0 | loss: 3.6774416Losses:  2.926753282546997 0.8493728041648865
CurrentTrain: epoch  5, batch     1 | loss: 3.7761261Losses:  3.1826210021972656 0.8057263493537903
CurrentTrain: epoch  6, batch     0 | loss: 3.9883473Losses:  2.286349058151245 0.8799309134483337
CurrentTrain: epoch  6, batch     1 | loss: 3.1662800Losses:  2.6093173027038574 0.8057632446289062
CurrentTrain: epoch  7, batch     0 | loss: 3.4150805Losses:  2.502169132232666 0.8048561811447144
CurrentTrain: epoch  7, batch     1 | loss: 3.3070254Losses:  2.271923065185547 0.8331111669540405
CurrentTrain: epoch  8, batch     0 | loss: 3.1050344Losses:  2.702096700668335 0.7153022289276123
CurrentTrain: epoch  8, batch     1 | loss: 3.4173989Losses:  2.399019718170166 0.789251983165741
CurrentTrain: epoch  9, batch     0 | loss: 3.1882718Losses:  2.290384531021118 0.6473485827445984
CurrentTrain: epoch  9, batch     1 | loss: 2.9377332
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 66.67%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 85.85%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 81.76%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 81.96%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 82.18%   
cur_acc:  ['0.8731', '0.6667']
his_acc:  ['0.8731', '0.8218']
Clustering into  3  clusters
Clusters:  [0 2 2 2 2 2 1 0 2 0 2 0 1 2 2 1]
Losses:  6.972354412078857 1.244844913482666
CurrentTrain: epoch  0, batch     0 | loss: 8.2171993Losses:  10.588586807250977 1.3638989925384521
CurrentTrain: epoch  0, batch     1 | loss: 11.9524860Losses:  8.835420608520508 1.1340758800506592
CurrentTrain: epoch  0, batch     2 | loss: 9.9694967Losses:  4.825846195220947 1.171471357345581
CurrentTrain: epoch  1, batch     0 | loss: 5.9973173Losses:  4.763754844665527 1.338858723640442
CurrentTrain: epoch  1, batch     1 | loss: 6.1026134Losses:  3.091733932495117 0.2613334059715271
CurrentTrain: epoch  1, batch     2 | loss: 3.3530674Losses:  4.4768385887146 1.3896055221557617
CurrentTrain: epoch  2, batch     0 | loss: 5.8664441Losses:  4.337657451629639 1.0991262197494507
CurrentTrain: epoch  2, batch     1 | loss: 5.4367838Losses:  3.9684865474700928 0.9244357943534851
CurrentTrain: epoch  2, batch     2 | loss: 4.8929224Losses:  4.070774555206299 1.2650456428527832
CurrentTrain: epoch  3, batch     0 | loss: 5.3358202Losses:  3.713815450668335 1.2496055364608765
CurrentTrain: epoch  3, batch     1 | loss: 4.9634209Losses:  3.7180469036102295 1.0722904205322266
CurrentTrain: epoch  3, batch     2 | loss: 4.7903376Losses:  2.9158527851104736 1.1357581615447998
CurrentTrain: epoch  4, batch     0 | loss: 4.0516109Losses:  4.26555871963501 1.1514627933502197
CurrentTrain: epoch  4, batch     1 | loss: 5.4170218Losses:  2.360898971557617 0.5618083477020264
CurrentTrain: epoch  4, batch     2 | loss: 2.9227073Losses:  3.0959715843200684 1.094165563583374
CurrentTrain: epoch  5, batch     0 | loss: 4.1901369Losses:  3.1337013244628906 1.4050326347351074
CurrentTrain: epoch  5, batch     1 | loss: 4.5387340Losses:  3.1172752380371094 0.5891465544700623
CurrentTrain: epoch  5, batch     2 | loss: 3.7064219Losses:  3.1183876991271973 1.179804801940918
CurrentTrain: epoch  6, batch     0 | loss: 4.2981925Losses:  2.819096088409424 0.9394298791885376
CurrentTrain: epoch  6, batch     1 | loss: 3.7585258Losses:  2.6061172485351562 0.941274881362915
CurrentTrain: epoch  6, batch     2 | loss: 3.5473921Losses:  2.657055377960205 1.1649670600891113
CurrentTrain: epoch  7, batch     0 | loss: 3.8220224Losses:  2.777616024017334 1.171872615814209
CurrentTrain: epoch  7, batch     1 | loss: 3.9494886Losses:  3.192481517791748 0.7410993576049805
CurrentTrain: epoch  7, batch     2 | loss: 3.9335809Losses:  2.690732479095459 1.116401195526123
CurrentTrain: epoch  8, batch     0 | loss: 3.8071337Losses:  2.5904507637023926 1.1060128211975098
CurrentTrain: epoch  8, batch     1 | loss: 3.6964636Losses:  3.011373519897461 0.6178285479545593
CurrentTrain: epoch  8, batch     2 | loss: 3.6292021Losses:  2.4945363998413086 1.1365537643432617
CurrentTrain: epoch  9, batch     0 | loss: 3.6310902Losses:  2.490856170654297 0.94150710105896
CurrentTrain: epoch  9, batch     1 | loss: 3.4323633Losses:  2.490518569946289 0.4742145240306854
CurrentTrain: epoch  9, batch     2 | loss: 2.9647331
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 80.80%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 80.03%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.98%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 76.74%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 71.28%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 71.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 72.99%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 73.62%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 73.26%   
cur_acc:  ['0.8731', '0.6667', '0.8080']
his_acc:  ['0.8731', '0.8218', '0.7326']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0]
Losses:  6.815162658691406 1.1324968338012695
CurrentTrain: epoch  0, batch     0 | loss: 7.9476595Losses:  8.961313247680664 1.4084218740463257
CurrentTrain: epoch  0, batch     1 | loss: 10.3697348Losses:  8.956852912902832 1.2147144079208374
CurrentTrain: epoch  0, batch     2 | loss: 10.1715670Losses:  3.4801793098449707 1.3548600673675537
CurrentTrain: epoch  1, batch     0 | loss: 4.8350391Losses:  3.2169978618621826 1.168946623802185
CurrentTrain: epoch  1, batch     1 | loss: 4.3859444Losses:  2.618351936340332 0.9178450107574463
CurrentTrain: epoch  1, batch     2 | loss: 3.5361969Losses:  2.787522315979004 1.5592741966247559
CurrentTrain: epoch  2, batch     0 | loss: 4.3467965Losses:  3.195695400238037 0.8794219493865967
CurrentTrain: epoch  2, batch     1 | loss: 4.0751171Losses:  2.929856300354004 0.8514613509178162
CurrentTrain: epoch  2, batch     2 | loss: 3.7813177Losses:  3.099778652191162 1.163444995880127
CurrentTrain: epoch  3, batch     0 | loss: 4.2632236Losses:  2.4954569339752197 1.1438651084899902
CurrentTrain: epoch  3, batch     1 | loss: 3.6393220Losses:  2.149127960205078 0.577490508556366
CurrentTrain: epoch  3, batch     2 | loss: 2.7266185Losses:  2.7891082763671875 0.8435494899749756
CurrentTrain: epoch  4, batch     0 | loss: 3.6326578Losses:  2.0127243995666504 1.06758713722229
CurrentTrain: epoch  4, batch     1 | loss: 3.0803115Losses:  2.2243363857269287 0.878250241279602
CurrentTrain: epoch  4, batch     2 | loss: 3.1025867Losses:  2.050114631652832 0.936064600944519
CurrentTrain: epoch  5, batch     0 | loss: 2.9861794Losses:  2.165343999862671 1.2718169689178467
CurrentTrain: epoch  5, batch     1 | loss: 3.4371610Losses:  3.099254608154297 0.7065211534500122
CurrentTrain: epoch  5, batch     2 | loss: 3.8057756Losses:  2.379728317260742 1.1807562112808228
CurrentTrain: epoch  6, batch     0 | loss: 3.5604844Losses:  2.0167360305786133 0.9662536382675171
CurrentTrain: epoch  6, batch     1 | loss: 2.9829898Losses:  2.619734287261963 0.7600497603416443
CurrentTrain: epoch  6, batch     2 | loss: 3.3797841Losses:  2.1887738704681396 1.0014287233352661
CurrentTrain: epoch  7, batch     0 | loss: 3.1902027Losses:  2.1896615028381348 0.7169337272644043
CurrentTrain: epoch  7, batch     1 | loss: 2.9065952Losses:  1.9095412492752075 0.46329110860824585
CurrentTrain: epoch  7, batch     2 | loss: 2.3728323Losses:  1.7892510890960693 1.0076249837875366
CurrentTrain: epoch  8, batch     0 | loss: 2.7968760Losses:  2.142416477203369 0.8432391285896301
CurrentTrain: epoch  8, batch     1 | loss: 2.9856555Losses:  1.9684298038482666 0.81028813123703
CurrentTrain: epoch  8, batch     2 | loss: 2.7787180Losses:  2.0763840675354004 1.0447368621826172
CurrentTrain: epoch  9, batch     0 | loss: 3.1211209Losses:  1.4355754852294922 0.7751563191413879
CurrentTrain: epoch  9, batch     1 | loss: 2.2107317Losses:  2.4180641174316406 0.8046308159828186
CurrentTrain: epoch  9, batch     2 | loss: 3.2226949
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 86.46%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 72.38%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 66.62%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 65.43%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 64.12%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 63.11%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 62.14%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 61.20%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 61.27%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 61.07%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 60.99%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 60.81%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 60.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 61.09%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 61.51%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 61.91%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 62.12%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 62.41%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 63.13%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 63.48%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 66.12%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 66.67%   
cur_acc:  ['0.8731', '0.6667', '0.8080', '0.8646']
his_acc:  ['0.8731', '0.8218', '0.7326', '0.6667']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0]
Losses:  7.074141979217529 1.4581892490386963
CurrentTrain: epoch  0, batch     0 | loss: 8.5323315Losses:  12.076859474182129 1.5888500213623047
CurrentTrain: epoch  0, batch     1 | loss: 13.6657095Losses:  9.624028205871582 1.3790977001190186
CurrentTrain: epoch  0, batch     2 | loss: 11.0031261Losses:  3.8812103271484375 1.2013568878173828
CurrentTrain: epoch  1, batch     0 | loss: 5.0825672Losses:  3.7856879234313965 1.2986394166946411
CurrentTrain: epoch  1, batch     1 | loss: 5.0843272Losses:  4.467401027679443 1.302134394645691
CurrentTrain: epoch  1, batch     2 | loss: 5.7695355Losses:  4.747180938720703 1.3935298919677734
CurrentTrain: epoch  2, batch     0 | loss: 6.1407108Losses:  3.011279582977295 1.1603330373764038
CurrentTrain: epoch  2, batch     1 | loss: 4.1716127Losses:  3.598710060119629 1.2910503149032593
CurrentTrain: epoch  2, batch     2 | loss: 4.8897605Losses:  3.5470077991485596 1.3848662376403809
CurrentTrain: epoch  3, batch     0 | loss: 4.9318743Losses:  3.855165958404541 1.299450159072876
CurrentTrain: epoch  3, batch     1 | loss: 5.1546164Losses:  2.866020441055298 1.1188037395477295
CurrentTrain: epoch  3, batch     2 | loss: 3.9848242Losses:  3.887960433959961 1.2624309062957764
CurrentTrain: epoch  4, batch     0 | loss: 5.1503916Losses:  2.754432439804077 1.3873661756515503
CurrentTrain: epoch  4, batch     1 | loss: 4.1417985Losses:  2.973745822906494 1.0372394323349
CurrentTrain: epoch  4, batch     2 | loss: 4.0109854Losses:  3.3218770027160645 0.9526057839393616
CurrentTrain: epoch  5, batch     0 | loss: 4.2744827Losses:  2.8344264030456543 1.1620371341705322
CurrentTrain: epoch  5, batch     1 | loss: 3.9964635Losses:  2.993013858795166 1.024873971939087
CurrentTrain: epoch  5, batch     2 | loss: 4.0178881Losses:  2.868448257446289 1.2225189208984375
CurrentTrain: epoch  6, batch     0 | loss: 4.0909672Losses:  2.6603710651397705 1.1845779418945312
CurrentTrain: epoch  6, batch     1 | loss: 3.8449490Losses:  3.1557323932647705 1.1216009855270386
CurrentTrain: epoch  6, batch     2 | loss: 4.2773333Losses:  3.222505569458008 1.1740212440490723
CurrentTrain: epoch  7, batch     0 | loss: 4.3965268Losses:  2.165029525756836 1.2363131046295166
CurrentTrain: epoch  7, batch     1 | loss: 3.4013426Losses:  2.812293291091919 0.9874737858772278
CurrentTrain: epoch  7, batch     2 | loss: 3.7997670Losses:  2.6671783924102783 1.1406049728393555
CurrentTrain: epoch  8, batch     0 | loss: 3.8077834Losses:  2.614323377609253 1.259713888168335
CurrentTrain: epoch  8, batch     1 | loss: 3.8740373Losses:  2.8203654289245605 0.9278675317764282
CurrentTrain: epoch  8, batch     2 | loss: 3.7482328Losses:  2.3085999488830566 1.2294663190841675
CurrentTrain: epoch  9, batch     0 | loss: 3.5380664Losses:  2.557622194290161 0.8999111652374268
CurrentTrain: epoch  9, batch     1 | loss: 3.4575334Losses:  2.8649942874908447 1.074769377708435
CurrentTrain: epoch  9, batch     2 | loss: 3.9397635
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 38.54%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 39.42%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 56.85%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 55.97%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.04%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 63.96%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 62.63%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 61.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 60.29%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 59.25%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 58.14%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 57.52%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 57.61%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 57.37%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 57.13%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 57.00%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 56.57%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 56.15%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 56.35%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 56.96%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 57.44%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 57.91%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 58.37%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 58.71%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 58.96%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 59.56%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 59.78%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 59.55%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 59.60%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 59.81%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 60.36%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 60.90%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 62.58%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 62.34%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 61.88%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 61.43%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 60.84%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 60.49%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 60.22%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 60.10%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 59.84%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 59.66%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 59.55%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 59.44%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 59.55%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 59.99%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 60.77%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 60.99%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 61.33%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 61.40%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 61.42%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 61.55%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 61.12%   
cur_acc:  ['0.8731', '0.6667', '0.8080', '0.8646', '0.5597']
his_acc:  ['0.8731', '0.8218', '0.7326', '0.6667', '0.6112']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0]
Losses:  6.799571990966797 1.1737037897109985
CurrentTrain: epoch  0, batch     0 | loss: 7.9732757Losses:  9.687438011169434 1.3358943462371826
CurrentTrain: epoch  0, batch     1 | loss: 11.0233326Losses:  11.033552169799805 1.2086284160614014
CurrentTrain: epoch  0, batch     2 | loss: 12.2421808Losses:  7.677651882171631 0.8263618350028992
CurrentTrain: epoch  0, batch     3 | loss: 8.5040140Losses:  3.115845203399658 1.4860535860061646
CurrentTrain: epoch  1, batch     0 | loss: 4.6018987Losses:  4.087317943572998 1.1263991594314575
CurrentTrain: epoch  1, batch     1 | loss: 5.2137170Losses:  2.8253931999206543 1.1561274528503418
CurrentTrain: epoch  1, batch     2 | loss: 3.9815207Losses:  3.002671003341675 0.4009357690811157
CurrentTrain: epoch  1, batch     3 | loss: 3.4036069Losses:  2.378587007522583 1.1283882856369019
CurrentTrain: epoch  2, batch     0 | loss: 3.5069752Losses:  3.1535871028900146 1.160435438156128
CurrentTrain: epoch  2, batch     1 | loss: 4.3140225Losses:  3.1748032569885254 1.226561188697815
CurrentTrain: epoch  2, batch     2 | loss: 4.4013643Losses:  4.002553939819336 0.617017388343811
CurrentTrain: epoch  2, batch     3 | loss: 4.6195712Losses:  2.9956226348876953 1.144087553024292
CurrentTrain: epoch  3, batch     0 | loss: 4.1397104Losses:  2.1509413719177246 1.3501687049865723
CurrentTrain: epoch  3, batch     1 | loss: 3.5011101Losses:  2.8250815868377686 1.1960076093673706
CurrentTrain: epoch  3, batch     2 | loss: 4.0210891Losses:  2.138112783432007 0.10967403650283813
CurrentTrain: epoch  3, batch     3 | loss: 2.2477868Losses:  1.8235163688659668 1.025148630142212
CurrentTrain: epoch  4, batch     0 | loss: 2.8486650Losses:  2.309966802597046 1.4410970211029053
CurrentTrain: epoch  4, batch     1 | loss: 3.7510638Losses:  3.437202215194702 1.0975151062011719
CurrentTrain: epoch  4, batch     2 | loss: 4.5347176Losses:  0.6410931944847107 0.13060329854488373
CurrentTrain: epoch  4, batch     3 | loss: 0.7716965Losses:  2.237609386444092 1.0297654867172241
CurrentTrain: epoch  5, batch     0 | loss: 3.2673750Losses:  2.6940014362335205 1.2373027801513672
CurrentTrain: epoch  5, batch     1 | loss: 3.9313042Losses:  2.1106202602386475 1.1274703741073608
CurrentTrain: epoch  5, batch     2 | loss: 3.2380905Losses:  0.6252493858337402 0.16894643008708954
CurrentTrain: epoch  5, batch     3 | loss: 0.7941958Losses:  2.291663646697998 1.2520842552185059
CurrentTrain: epoch  6, batch     0 | loss: 3.5437479Losses:  1.8423817157745361 1.0635371208190918
CurrentTrain: epoch  6, batch     1 | loss: 2.9059188Losses:  2.3338143825531006 1.0900838375091553
CurrentTrain: epoch  6, batch     2 | loss: 3.4238982Losses:  2.348573684692383 0.43082305788993835
CurrentTrain: epoch  6, batch     3 | loss: 2.7793968Losses:  2.0658745765686035 0.8977413177490234
CurrentTrain: epoch  7, batch     0 | loss: 2.9636159Losses:  2.26809024810791 1.0526115894317627
CurrentTrain: epoch  7, batch     1 | loss: 3.3207018Losses:  2.0394287109375 1.2654600143432617
CurrentTrain: epoch  7, batch     2 | loss: 3.3048887Losses:  1.6461610794067383 0.24552321434020996
CurrentTrain: epoch  7, batch     3 | loss: 1.8916843Losses:  1.901379108428955 1.1530425548553467
CurrentTrain: epoch  8, batch     0 | loss: 3.0544217Losses:  2.670808792114258 1.1109216213226318
CurrentTrain: epoch  8, batch     1 | loss: 3.7817304Losses:  1.3368321657180786 1.016381859779358
CurrentTrain: epoch  8, batch     2 | loss: 2.3532140Losses:  2.8183178901672363 0.44401025772094727
CurrentTrain: epoch  8, batch     3 | loss: 3.2623281Losses:  2.0656800270080566 1.0707244873046875
CurrentTrain: epoch  9, batch     0 | loss: 3.1364045Losses:  2.1466121673583984 1.1053862571716309
CurrentTrain: epoch  9, batch     1 | loss: 3.2519984Losses:  1.3326747417449951 1.023056983947754
CurrentTrain: epoch  9, batch     2 | loss: 2.3557317Losses:  2.1786625385284424 0.15646573901176453
CurrentTrain: epoch  9, batch     3 | loss: 2.3351283
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 39.06%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 34.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 39.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 43.23%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 45.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.09%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 48.61%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 50.33%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 66.33%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 63.97%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 62.14%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 60.42%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.78%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 58.72%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 59.29%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 60.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 60.17%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 58.81%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 57.50%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 56.25%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 55.45%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 55.34%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 54.34%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 53.25%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 52.21%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 51.20%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 50.24%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 49.65%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 49.55%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 49.00%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 48.36%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 48.17%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 47.88%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 47.19%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 47.13%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 47.48%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 47.52%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 47.66%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 47.88%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 48.30%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 48.41%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 49.17%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 49.18%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 48.93%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 49.12%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 49.48%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 50.17%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 50.84%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 51.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.14%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 52.76%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 53.04%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 52.61%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 52.03%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 51.39%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 50.76%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 50.23%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 49.63%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 49.41%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 49.13%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 48.85%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 48.65%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 48.38%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 48.06%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 48.15%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 48.71%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 49.13%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 49.60%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 50.46%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 50.71%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 50.89%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 51.20%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 52.04%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 52.39%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 52.06%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 51.68%   [EVAL] batch:  104 | acc: 25.00%,  total acc: 51.43%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 50.94%   [EVAL] batch:  106 | acc: 0.00%,  total acc: 50.47%   
cur_acc:  ['0.8731', '0.6667', '0.8080', '0.8646', '0.5597', '0.3906']
his_acc:  ['0.8731', '0.8218', '0.7326', '0.6667', '0.6112', '0.5047']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0]
Losses:  6.544643402099609 1.166373610496521
CurrentTrain: epoch  0, batch     0 | loss: 7.7110171Losses:  9.55256175994873 1.2427071332931519
CurrentTrain: epoch  0, batch     1 | loss: 10.7952690Losses:  8.355210304260254 1.1342864036560059
CurrentTrain: epoch  0, batch     2 | loss: 9.4894962Losses:  8.92514705657959 0.9575929045677185
CurrentTrain: epoch  0, batch     3 | loss: 9.8827400Losses:  2.2611989974975586 1.2416704893112183
CurrentTrain: epoch  1, batch     0 | loss: 3.5028696Losses:  2.5471534729003906 1.1751625537872314
CurrentTrain: epoch  1, batch     1 | loss: 3.7223160Losses:  2.047330379486084 1.101995587348938
CurrentTrain: epoch  1, batch     2 | loss: 3.1493258Losses:  2.008160352706909 0.6316788792610168
CurrentTrain: epoch  1, batch     3 | loss: 2.6398392Losses:  2.390230655670166 1.0354000329971313
CurrentTrain: epoch  2, batch     0 | loss: 3.4256306Losses:  1.5154390335083008 0.9490915536880493
CurrentTrain: epoch  2, batch     1 | loss: 2.4645305Losses:  2.3057844638824463 1.2354846000671387
CurrentTrain: epoch  2, batch     2 | loss: 3.5412691Losses:  1.7832878828048706 0.45885956287384033
CurrentTrain: epoch  2, batch     3 | loss: 2.2421474Losses:  1.8052394390106201 1.198214054107666
CurrentTrain: epoch  3, batch     0 | loss: 3.0034535Losses:  2.0348658561706543 1.0371553897857666
CurrentTrain: epoch  3, batch     1 | loss: 3.0720212Losses:  1.4851362705230713 0.8268768191337585
CurrentTrain: epoch  3, batch     2 | loss: 2.3120131Losses:  2.5685999393463135 0.7554944753646851
CurrentTrain: epoch  3, batch     3 | loss: 3.3240943Losses:  1.8181239366531372 0.9391052722930908
CurrentTrain: epoch  4, batch     0 | loss: 2.7572293Losses:  1.4037760496139526 0.9691312909126282
CurrentTrain: epoch  4, batch     1 | loss: 2.3729074Losses:  2.198735237121582 1.0252749919891357
CurrentTrain: epoch  4, batch     2 | loss: 3.2240102Losses:  1.8315925598144531 0.7126402854919434
CurrentTrain: epoch  4, batch     3 | loss: 2.5442328Losses:  1.5276529788970947 1.0743488073349
CurrentTrain: epoch  5, batch     0 | loss: 2.6020017Losses:  2.157820463180542 0.9366382360458374
CurrentTrain: epoch  5, batch     1 | loss: 3.0944586Losses:  1.5071165561676025 0.8723316788673401
CurrentTrain: epoch  5, batch     2 | loss: 2.3794482Losses:  1.4733418226242065 0.6296055912971497
CurrentTrain: epoch  5, batch     3 | loss: 2.1029475Losses:  1.4692084789276123 0.8803684115409851
CurrentTrain: epoch  6, batch     0 | loss: 2.3495770Losses:  1.7996629476547241 1.0888614654541016
CurrentTrain: epoch  6, batch     1 | loss: 2.8885245Losses:  1.284437894821167 0.946930468082428
CurrentTrain: epoch  6, batch     2 | loss: 2.2313683Losses:  1.816643476486206 0.6130636930465698
CurrentTrain: epoch  6, batch     3 | loss: 2.4297071Losses:  1.9372711181640625 0.8744988441467285
CurrentTrain: epoch  7, batch     0 | loss: 2.8117700Losses:  1.6817610263824463 0.9485844969749451
CurrentTrain: epoch  7, batch     1 | loss: 2.6303456Losses:  1.1922533512115479 0.7214811444282532
CurrentTrain: epoch  7, batch     2 | loss: 1.9137344Losses:  1.4513354301452637 0.49101099371910095
CurrentTrain: epoch  7, batch     3 | loss: 1.9423465Losses:  0.9277400970458984 0.9327068328857422
CurrentTrain: epoch  8, batch     0 | loss: 1.8604469Losses:  1.7160133123397827 0.766157329082489
CurrentTrain: epoch  8, batch     1 | loss: 2.4821706Losses:  1.422913908958435 0.907588005065918
CurrentTrain: epoch  8, batch     2 | loss: 2.3305020Losses:  2.1074204444885254 0.5386062860488892
CurrentTrain: epoch  8, batch     3 | loss: 2.6460266Losses:  1.3954218626022339 0.9337396621704102
CurrentTrain: epoch  9, batch     0 | loss: 2.3291616Losses:  1.296730637550354 0.7851879596710205
CurrentTrain: epoch  9, batch     1 | loss: 2.0819187Losses:  1.2394890785217285 0.7820577621459961
CurrentTrain: epoch  9, batch     2 | loss: 2.0215468Losses:  1.7466849088668823 0.5741910934448242
CurrentTrain: epoch  9, batch     3 | loss: 2.3208761
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 71.63%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 1.04%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 22.66%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 50.66%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.88%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 61.93%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.11%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 58.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 56.77%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 55.41%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 55.26%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 55.45%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 56.55%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 56.10%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.83%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 53.61%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 52.45%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 51.73%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 51.69%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 50.77%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 49.88%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 49.14%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 48.32%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 47.41%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 46.88%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 46.59%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 46.32%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 45.94%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 45.69%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 45.34%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 44.90%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 44.26%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 43.55%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 42.96%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 42.38%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 41.92%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 41.57%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 41.23%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 42.00%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 42.21%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 42.05%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 42.17%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 42.53%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 43.32%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 44.09%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 44.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 45.56%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 46.27%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 46.63%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 46.20%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 45.70%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 45.14%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 44.59%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 44.20%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 43.46%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 43.02%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 42.67%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 42.40%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 42.13%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 41.88%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 41.48%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 41.10%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 40.66%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 40.29%   [EVAL] batch:   94 | acc: 6.25%,  total acc: 39.93%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 39.71%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 39.56%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 39.41%   [EVAL] batch:   98 | acc: 43.75%,  total acc: 39.46%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 39.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 40.47%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 40.93%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 40.66%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 40.32%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 40.06%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 39.68%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 39.66%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 39.87%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 40.08%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 40.34%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 40.88%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 41.41%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 41.92%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 42.38%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 42.39%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 42.35%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 42.79%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 43.06%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 43.17%   
cur_acc:  ['0.8731', '0.6667', '0.8080', '0.8646', '0.5597', '0.3906', '0.7163']
his_acc:  ['0.8731', '0.8218', '0.7326', '0.6667', '0.6112', '0.5047', '0.4317']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0 0
 0 1 0 0]
Losses:  6.420722961425781 0.9827182292938232
CurrentTrain: epoch  0, batch     0 | loss: 7.4034414Losses:  9.424299240112305 1.1699142456054688
CurrentTrain: epoch  0, batch     1 | loss: 10.5942135Losses:  9.46822738647461 1.127484917640686
CurrentTrain: epoch  0, batch     2 | loss: 10.5957127Losses:  8.877215385437012 1.0586189031600952
CurrentTrain: epoch  0, batch     3 | loss: 9.9358339Losses:  2.569373846054077 1.088759183883667
CurrentTrain: epoch  1, batch     0 | loss: 3.6581330Losses:  2.2497076988220215 1.1382317543029785
CurrentTrain: epoch  1, batch     1 | loss: 3.3879395Losses:  2.6678760051727295 1.022028923034668
CurrentTrain: epoch  1, batch     2 | loss: 3.6899049Losses:  1.675062656402588 0.8748953938484192
CurrentTrain: epoch  1, batch     3 | loss: 2.5499580Losses:  2.773876905441284 0.8584096431732178
CurrentTrain: epoch  2, batch     0 | loss: 3.6322865Losses:  2.028766632080078 1.1072947978973389
CurrentTrain: epoch  2, batch     1 | loss: 3.1360614Losses:  1.7138731479644775 1.0628080368041992
CurrentTrain: epoch  2, batch     2 | loss: 2.7766812Losses:  1.9502159357070923 0.9587950110435486
CurrentTrain: epoch  2, batch     3 | loss: 2.9090109Losses:  2.08768892288208 1.07264244556427
CurrentTrain: epoch  3, batch     0 | loss: 3.1603312Losses:  1.8081263303756714 0.8604823350906372
CurrentTrain: epoch  3, batch     1 | loss: 2.6686087Losses:  1.8867788314819336 1.1491667032241821
CurrentTrain: epoch  3, batch     2 | loss: 3.0359454Losses:  1.9681423902511597 0.8845945000648499
CurrentTrain: epoch  3, batch     3 | loss: 2.8527369Losses:  1.8809924125671387 0.9741673469543457
CurrentTrain: epoch  4, batch     0 | loss: 2.8551598Losses:  2.1078009605407715 0.9928964376449585
CurrentTrain: epoch  4, batch     1 | loss: 3.1006975Losses:  1.7804388999938965 1.0033812522888184
CurrentTrain: epoch  4, batch     2 | loss: 2.7838202Losses:  1.3086190223693848 0.9433239102363586
CurrentTrain: epoch  4, batch     3 | loss: 2.2519429Losses:  2.0040149688720703 1.0901837348937988
CurrentTrain: epoch  5, batch     0 | loss: 3.0941987Losses:  1.2598681449890137 1.0529576539993286
CurrentTrain: epoch  5, batch     1 | loss: 2.3128257Losses:  1.0012824535369873 0.9637314081192017
CurrentTrain: epoch  5, batch     2 | loss: 1.9650139Losses:  2.498128652572632 0.8050776720046997
CurrentTrain: epoch  5, batch     3 | loss: 3.3032064Losses:  1.7021291255950928 0.8390445709228516
CurrentTrain: epoch  6, batch     0 | loss: 2.5411737Losses:  1.7359955310821533 0.8723899126052856
CurrentTrain: epoch  6, batch     1 | loss: 2.6083856Losses:  1.1460726261138916 0.8300971388816833
CurrentTrain: epoch  6, batch     2 | loss: 1.9761698Losses:  1.7756319046020508 0.7810414433479309
CurrentTrain: epoch  6, batch     3 | loss: 2.5566733Losses:  1.616194248199463 0.8936952352523804
CurrentTrain: epoch  7, batch     0 | loss: 2.5098896Losses:  1.292684555053711 0.827505350112915
CurrentTrain: epoch  7, batch     1 | loss: 2.1201899Losses:  1.5078994035720825 0.9649195075035095
CurrentTrain: epoch  7, batch     2 | loss: 2.4728189Losses:  1.656463384628296 0.8557438254356384
CurrentTrain: epoch  7, batch     3 | loss: 2.5122073Losses:  1.2361124753952026 1.0943069458007812
CurrentTrain: epoch  8, batch     0 | loss: 2.3304195Losses:  1.514582633972168 0.7811578512191772
CurrentTrain: epoch  8, batch     1 | loss: 2.2957406Losses:  1.737039566040039 0.7584268450737
CurrentTrain: epoch  8, batch     2 | loss: 2.4954665Losses:  1.21182382106781 0.6606947183609009
CurrentTrain: epoch  8, batch     3 | loss: 1.8725185Losses:  1.1082966327667236 0.7933758497238159
CurrentTrain: epoch  9, batch     0 | loss: 1.9016725Losses:  1.6021595001220703 0.6362363696098328
CurrentTrain: epoch  9, batch     1 | loss: 2.2383959Losses:  1.4663586616516113 0.716367244720459
CurrentTrain: epoch  9, batch     2 | loss: 2.1827259Losses:  1.2013161182403564 0.9584713578224182
CurrentTrain: epoch  9, batch     3 | loss: 2.1597874
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.07%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 1.56%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 57.88%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 64.84%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 60.36%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 58.68%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 57.26%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 56.91%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 56.89%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 57.47%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 56.98%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 55.68%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 54.44%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 53.26%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 52.53%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 52.34%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 51.40%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 50.50%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 49.75%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 48.92%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 48.11%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 47.57%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 47.27%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 46.76%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 46.38%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 45.69%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 45.34%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 44.90%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 44.47%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 43.75%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 43.15%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 42.48%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 41.83%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 41.19%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 40.86%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 41.73%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 41.85%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 41.70%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 41.81%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 42.19%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 42.98%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 44.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 45.23%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 45.94%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 46.31%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 45.89%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 45.39%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 44.83%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 44.28%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 43.75%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 43.23%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 42.79%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 42.30%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 41.95%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 41.55%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 41.08%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 40.62%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 40.25%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 39.95%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 39.58%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 39.30%   [EVAL] batch:   94 | acc: 6.25%,  total acc: 38.95%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 38.74%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 38.66%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 38.58%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 38.70%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 39.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 39.73%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 40.01%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 39.75%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 39.42%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 39.11%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 38.74%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 38.73%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 38.95%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 39.16%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 39.32%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 39.81%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 40.35%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 40.82%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 41.28%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 41.30%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 41.38%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 41.72%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 41.63%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 41.70%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 42.14%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 42.61%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 43.08%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 43.55%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 44.00%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 44.45%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 44.89%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 45.32%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 45.65%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 45.74%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 45.96%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 46.33%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 46.69%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 46.80%   
cur_acc:  ['0.8731', '0.6667', '0.8080', '0.8646', '0.5597', '0.3906', '0.7163', '0.9107']
his_acc:  ['0.8731', '0.8218', '0.7326', '0.6667', '0.6112', '0.5047', '0.4317', '0.4680']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.211234092712402 1.7084617614746094
CurrentTrain: epoch  0, batch     0 | loss: 13.9196959Losses:  14.5792236328125 1.6901048421859741
CurrentTrain: epoch  0, batch     1 | loss: 16.2693291Losses:  15.08927059173584 1.6444356441497803
CurrentTrain: epoch  0, batch     2 | loss: 16.7337055Losses:  14.998939514160156 1.8157745599746704
CurrentTrain: epoch  0, batch     3 | loss: 16.8147144Losses:  14.828505516052246 1.8496438264846802
CurrentTrain: epoch  0, batch     4 | loss: 16.6781502Losses:  14.450037956237793 1.7259345054626465
CurrentTrain: epoch  0, batch     5 | loss: 16.1759720Losses:  14.117124557495117 1.4077744483947754
CurrentTrain: epoch  0, batch     6 | loss: 15.5248985Losses:  14.20703125 1.5224635601043701
CurrentTrain: epoch  0, batch     7 | loss: 15.7294950Losses:  13.935869216918945 1.552010416984558
CurrentTrain: epoch  0, batch     8 | loss: 15.4878798Losses:  13.722323417663574 1.3304190635681152
CurrentTrain: epoch  0, batch     9 | loss: 15.0527420Losses:  13.285319328308105 1.4591541290283203
CurrentTrain: epoch  0, batch    10 | loss: 14.7444735Losses:  13.020700454711914 1.3309814929962158
CurrentTrain: epoch  0, batch    11 | loss: 14.3516817Losses:  12.596056938171387 1.40211021900177
CurrentTrain: epoch  0, batch    12 | loss: 13.9981670Losses:  12.893190383911133 1.7794709205627441
CurrentTrain: epoch  0, batch    13 | loss: 14.6726608Losses:  12.512130737304688 1.7275407314300537
CurrentTrain: epoch  0, batch    14 | loss: 14.2396717Losses:  12.812847137451172 1.5780695676803589
CurrentTrain: epoch  0, batch    15 | loss: 14.3909168Losses:  12.57288646697998 1.6643247604370117
CurrentTrain: epoch  0, batch    16 | loss: 14.2372112Losses:  12.108726501464844 1.5023317337036133
CurrentTrain: epoch  0, batch    17 | loss: 13.6110582Losses:  12.221277236938477 1.4880919456481934
CurrentTrain: epoch  0, batch    18 | loss: 13.7093697Losses:  11.725299835205078 1.36260986328125
CurrentTrain: epoch  0, batch    19 | loss: 13.0879097Losses:  11.838890075683594 1.5052576065063477
CurrentTrain: epoch  0, batch    20 | loss: 13.3441477Losses:  11.474811553955078 1.4036707878112793
CurrentTrain: epoch  0, batch    21 | loss: 12.8784828Losses:  10.742158889770508 1.2795610427856445
CurrentTrain: epoch  0, batch    22 | loss: 12.0217199Losses:  11.002530097961426 1.4426214694976807
CurrentTrain: epoch  0, batch    23 | loss: 12.4451513Losses:  10.724617004394531 1.3058561086654663
CurrentTrain: epoch  0, batch    24 | loss: 12.0304728Losses:  10.237449645996094 1.317131519317627
CurrentTrain: epoch  0, batch    25 | loss: 11.5545807Losses:  10.040868759155273 1.3431757688522339
CurrentTrain: epoch  0, batch    26 | loss: 11.3840446Losses:  9.878532409667969 1.4160425662994385
CurrentTrain: epoch  0, batch    27 | loss: 11.2945747Losses:  9.681581497192383 1.3569164276123047
CurrentTrain: epoch  0, batch    28 | loss: 11.0384979Losses:  9.386812210083008 1.1338285207748413
CurrentTrain: epoch  0, batch    29 | loss: 10.5206404Losses:  9.53359317779541 1.410855770111084
CurrentTrain: epoch  0, batch    30 | loss: 10.9444485Losses:  9.133207321166992 1.3065769672393799
CurrentTrain: epoch  0, batch    31 | loss: 10.4397840Losses:  8.85720443725586 1.2662198543548584
CurrentTrain: epoch  0, batch    32 | loss: 10.1234245Losses:  9.143937110900879 1.497721791267395
CurrentTrain: epoch  0, batch    33 | loss: 10.6416588Losses:  8.476202964782715 1.055629014968872
CurrentTrain: epoch  0, batch    34 | loss: 9.5318317Losses:  8.083708763122559 1.1793687343597412
CurrentTrain: epoch  0, batch    35 | loss: 9.2630777Losses:  7.9586944580078125 1.1847963333129883
CurrentTrain: epoch  0, batch    36 | loss: 9.1434908Losses:  7.767488479614258 0.7243722677230835
CurrentTrain: epoch  0, batch    37 | loss: 8.4918604Losses:  7.829537391662598 1.410733699798584
CurrentTrain: epoch  1, batch     0 | loss: 9.2402706Losses:  7.670144081115723 1.080012559890747
CurrentTrain: epoch  1, batch     1 | loss: 8.7501564Losses:  7.805513381958008 1.0606021881103516
CurrentTrain: epoch  1, batch     2 | loss: 8.8661156Losses:  7.6887712478637695 1.20601224899292
CurrentTrain: epoch  1, batch     3 | loss: 8.8947830Losses:  7.633707523345947 1.1664268970489502
CurrentTrain: epoch  1, batch     4 | loss: 8.8001347Losses:  7.512103080749512 1.2500213384628296
CurrentTrain: epoch  1, batch     5 | loss: 8.7621241Losses:  7.30303955078125 1.1438157558441162
CurrentTrain: epoch  1, batch     6 | loss: 8.4468555Losses:  7.601259231567383 1.1033666133880615
CurrentTrain: epoch  1, batch     7 | loss: 8.7046261Losses:  7.758469581604004 1.176454782485962
CurrentTrain: epoch  1, batch     8 | loss: 8.9349241Losses:  7.464491844177246 1.1365385055541992
CurrentTrain: epoch  1, batch     9 | loss: 8.6010303Losses:  7.334151268005371 1.0999377965927124
CurrentTrain: epoch  1, batch    10 | loss: 8.4340887Losses:  7.559192180633545 0.9793102741241455
CurrentTrain: epoch  1, batch    11 | loss: 8.5385027Losses:  7.188336372375488 1.0705432891845703
CurrentTrain: epoch  1, batch    12 | loss: 8.2588797Losses:  7.7074785232543945 1.1898207664489746
CurrentTrain: epoch  1, batch    13 | loss: 8.8972988Losses:  7.203503131866455 1.0789190530776978
CurrentTrain: epoch  1, batch    14 | loss: 8.2824221Losses:  7.365644454956055 1.004467487335205
CurrentTrain: epoch  1, batch    15 | loss: 8.3701115Losses:  7.120356559753418 1.0489575862884521
CurrentTrain: epoch  1, batch    16 | loss: 8.1693144Losses:  6.921718597412109 0.9611687064170837
CurrentTrain: epoch  1, batch    17 | loss: 7.8828874Losses:  7.007874488830566 1.0617914199829102
CurrentTrain: epoch  1, batch    18 | loss: 8.0696659Losses:  7.363802909851074 1.2717641592025757
CurrentTrain: epoch  1, batch    19 | loss: 8.6355667Losses:  6.728475570678711 0.8046040534973145
CurrentTrain: epoch  1, batch    20 | loss: 7.5330796Losses:  6.985523223876953 0.7831393480300903
CurrentTrain: epoch  1, batch    21 | loss: 7.7686625Losses:  7.058343887329102 0.8462578058242798
CurrentTrain: epoch  1, batch    22 | loss: 7.9046016Losses:  7.412341117858887 0.9493839740753174
CurrentTrain: epoch  1, batch    23 | loss: 8.3617249Losses:  7.364943504333496 0.8865087032318115
CurrentTrain: epoch  1, batch    24 | loss: 8.2514524Losses:  7.393347263336182 0.9437508583068848
CurrentTrain: epoch  1, batch    25 | loss: 8.3370981Losses:  6.836155891418457 0.8148428201675415
CurrentTrain: epoch  1, batch    26 | loss: 7.6509986Losses:  6.985654354095459 1.143647313117981
CurrentTrain: epoch  1, batch    27 | loss: 8.1293020Losses:  6.842061996459961 0.9046164751052856
CurrentTrain: epoch  1, batch    28 | loss: 7.7466784Losses:  6.686251640319824 0.6849052906036377
CurrentTrain: epoch  1, batch    29 | loss: 7.3711567Losses:  7.040599822998047 0.8197181224822998
CurrentTrain: epoch  1, batch    30 | loss: 7.8603182Losses:  6.913344383239746 0.9781545996665955
CurrentTrain: epoch  1, batch    31 | loss: 7.8914990Losses:  6.590850353240967 0.8220411539077759
CurrentTrain: epoch  1, batch    32 | loss: 7.4128914Losses:  6.544325828552246 0.7862995862960815
CurrentTrain: epoch  1, batch    33 | loss: 7.3306255Losses:  6.867552757263184 0.881051778793335
CurrentTrain: epoch  1, batch    34 | loss: 7.7486048Losses:  6.9573822021484375 0.9340733289718628
CurrentTrain: epoch  1, batch    35 | loss: 7.8914557Losses:  6.946398735046387 0.8817654848098755
CurrentTrain: epoch  1, batch    36 | loss: 7.8281641Losses:  6.711795806884766 0.28816941380500793
CurrentTrain: epoch  1, batch    37 | loss: 6.9999652Losses:  6.660856246948242 0.795781672000885
CurrentTrain: epoch  2, batch     0 | loss: 7.4566379Losses:  6.575999736785889 0.7003750205039978
CurrentTrain: epoch  2, batch     1 | loss: 7.2763748Losses:  5.9904327392578125 0.6568456888198853
CurrentTrain: epoch  2, batch     2 | loss: 6.6472783Losses:  6.073957443237305 0.699337899684906
CurrentTrain: epoch  2, batch     3 | loss: 6.7732954Losses:  7.230225563049316 0.5790846347808838
CurrentTrain: epoch  2, batch     4 | loss: 7.8093100Losses:  6.393065452575684 0.798406720161438
CurrentTrain: epoch  2, batch     5 | loss: 7.1914721Losses:  6.453836917877197 0.7630128860473633
CurrentTrain: epoch  2, batch     6 | loss: 7.2168498Losses:  6.200307846069336 0.7261000871658325
CurrentTrain: epoch  2, batch     7 | loss: 6.9264078Losses:  6.61851167678833 0.8339014649391174
CurrentTrain: epoch  2, batch     8 | loss: 7.4524131Losses:  6.460786819458008 0.551008403301239
CurrentTrain: epoch  2, batch     9 | loss: 7.0117950Losses:  6.337022304534912 0.5244612693786621
CurrentTrain: epoch  2, batch    10 | loss: 6.8614836Losses:  6.425081729888916 0.6552989482879639
CurrentTrain: epoch  2, batch    11 | loss: 7.0803804Losses:  6.00764274597168 0.5133416056632996
CurrentTrain: epoch  2, batch    12 | loss: 6.5209842Losses:  6.39054536819458 0.7164747714996338
CurrentTrain: epoch  2, batch    13 | loss: 7.1070204Losses:  7.019825458526611 0.783290445804596
CurrentTrain: epoch  2, batch    14 | loss: 7.8031158Losses:  6.683718204498291 0.6001595854759216
CurrentTrain: epoch  2, batch    15 | loss: 7.2838778Losses:  6.694159507751465 0.6505954265594482
CurrentTrain: epoch  2, batch    16 | loss: 7.3447552Losses:  6.22608757019043 0.6666820049285889
CurrentTrain: epoch  2, batch    17 | loss: 6.8927698Losses:  6.149224281311035 0.5876628160476685
CurrentTrain: epoch  2, batch    18 | loss: 6.7368870Losses:  6.235168933868408 0.5830065011978149
CurrentTrain: epoch  2, batch    19 | loss: 6.8181753Losses:  5.837520599365234 0.45458656549453735
CurrentTrain: epoch  2, batch    20 | loss: 6.2921071Losses:  6.299874305725098 0.7049486041069031
CurrentTrain: epoch  2, batch    21 | loss: 7.0048227Losses:  6.544215202331543 0.7745466232299805
CurrentTrain: epoch  2, batch    22 | loss: 7.3187618Losses:  6.570363521575928 0.6668256521224976
CurrentTrain: epoch  2, batch    23 | loss: 7.2371893Losses:  6.420406341552734 0.702236533164978
CurrentTrain: epoch  2, batch    24 | loss: 7.1226430Losses:  6.496184349060059 0.6677019596099854
CurrentTrain: epoch  2, batch    25 | loss: 7.1638861Losses:  6.926509380340576 0.7168496251106262
CurrentTrain: epoch  2, batch    26 | loss: 7.6433592Losses:  6.206119060516357 0.5779226422309875
CurrentTrain: epoch  2, batch    27 | loss: 6.7840419Losses:  6.91700553894043 0.7751209735870361
CurrentTrain: epoch  2, batch    28 | loss: 7.6921263Losses:  6.242432594299316 0.6356792449951172
CurrentTrain: epoch  2, batch    29 | loss: 6.8781118Losses:  6.2186994552612305 0.5474273562431335
CurrentTrain: epoch  2, batch    30 | loss: 6.7661266Losses:  6.675220489501953 0.610221266746521
CurrentTrain: epoch  2, batch    31 | loss: 7.2854419Losses:  6.3547258377075195 0.5583531260490417
CurrentTrain: epoch  2, batch    32 | loss: 6.9130788Losses:  6.044363021850586 0.5858771800994873
CurrentTrain: epoch  2, batch    33 | loss: 6.6302404Losses:  6.41864013671875 0.6406068801879883
CurrentTrain: epoch  2, batch    34 | loss: 7.0592470Losses:  6.541497230529785 0.5735889673233032
CurrentTrain: epoch  2, batch    35 | loss: 7.1150861Losses:  6.075345039367676 0.5877593755722046
CurrentTrain: epoch  2, batch    36 | loss: 6.6631045Losses:  6.113597869873047 0.5193241238594055
CurrentTrain: epoch  2, batch    37 | loss: 6.6329222Losses:  5.8961334228515625 0.5393800735473633
CurrentTrain: epoch  3, batch     0 | loss: 6.4355135Losses:  5.928969383239746 0.46621882915496826
CurrentTrain: epoch  3, batch     1 | loss: 6.3951883Losses:  6.228583335876465 0.5882821083068848
CurrentTrain: epoch  3, batch     2 | loss: 6.8168654Losses:  6.144541263580322 0.616010308265686
CurrentTrain: epoch  3, batch     3 | loss: 6.7605515Losses:  5.76629114151001 0.4206744432449341
CurrentTrain: epoch  3, batch     4 | loss: 6.1869655Losses:  6.187243461608887 0.6138985753059387
CurrentTrain: epoch  3, batch     5 | loss: 6.8011422Losses:  5.6440348625183105 0.5425876379013062
CurrentTrain: epoch  3, batch     6 | loss: 6.1866226Losses:  5.899855613708496 0.5130529403686523
CurrentTrain: epoch  3, batch     7 | loss: 6.4129086Losses:  5.825929641723633 0.48977404832839966
CurrentTrain: epoch  3, batch     8 | loss: 6.3157039Losses:  5.815698623657227 0.3871071934700012
CurrentTrain: epoch  3, batch     9 | loss: 6.2028060Losses:  5.766424655914307 0.5314172506332397
CurrentTrain: epoch  3, batch    10 | loss: 6.2978420Losses:  5.866400718688965 0.4411396384239197
CurrentTrain: epoch  3, batch    11 | loss: 6.3075404Losses:  5.893368721008301 0.5527539253234863
CurrentTrain: epoch  3, batch    12 | loss: 6.4461226Losses:  5.839960098266602 0.4731966257095337
CurrentTrain: epoch  3, batch    13 | loss: 6.3131566Losses:  5.6248779296875 0.43994271755218506
CurrentTrain: epoch  3, batch    14 | loss: 6.0648208Losses:  6.067163467407227 0.5121640563011169
CurrentTrain: epoch  3, batch    15 | loss: 6.5793276Losses:  5.5414299964904785 0.3935166001319885
CurrentTrain: epoch  3, batch    16 | loss: 5.9349465Losses:  6.121804237365723 0.5152957439422607
CurrentTrain: epoch  3, batch    17 | loss: 6.6371002Losses:  5.910941123962402 0.419575572013855
CurrentTrain: epoch  3, batch    18 | loss: 6.3305168Losses:  5.870645523071289 0.5861569046974182
CurrentTrain: epoch  3, batch    19 | loss: 6.4568024Losses:  5.4965925216674805 0.3889960050582886
CurrentTrain: epoch  3, batch    20 | loss: 5.8855886Losses:  5.945577621459961 0.3511957824230194
CurrentTrain: epoch  3, batch    21 | loss: 6.2967734Losses:  5.971523284912109 0.6062859296798706
CurrentTrain: epoch  3, batch    22 | loss: 6.5778093Losses:  5.737027168273926 0.39271873235702515
CurrentTrain: epoch  3, batch    23 | loss: 6.1297460Losses:  5.609433174133301 0.3380686044692993
CurrentTrain: epoch  3, batch    24 | loss: 5.9475017Losses:  5.873722076416016 0.38996621966362
CurrentTrain: epoch  3, batch    25 | loss: 6.2636881Losses:  5.961398124694824 0.5474849939346313
CurrentTrain: epoch  3, batch    26 | loss: 6.5088830Losses:  5.566488265991211 0.5238118767738342
CurrentTrain: epoch  3, batch    27 | loss: 6.0903001Losses:  5.97231912612915 0.5253573060035706
CurrentTrain: epoch  3, batch    28 | loss: 6.4976764Losses:  6.113986015319824 0.5737134218215942
CurrentTrain: epoch  3, batch    29 | loss: 6.6876993Losses:  6.283010482788086 0.3666017949581146
CurrentTrain: epoch  3, batch    30 | loss: 6.6496124Losses:  6.709165573120117 0.4754827320575714
CurrentTrain: epoch  3, batch    31 | loss: 7.1846485Losses:  5.9605255126953125 0.45171016454696655
CurrentTrain: epoch  3, batch    32 | loss: 6.4122357Losses:  6.1298346519470215 0.46593305468559265
CurrentTrain: epoch  3, batch    33 | loss: 6.5957675Losses:  6.06788444519043 0.38803914189338684
CurrentTrain: epoch  3, batch    34 | loss: 6.4559236Losses:  5.580272674560547 0.32987719774246216
CurrentTrain: epoch  3, batch    35 | loss: 5.9101501Losses:  5.838329315185547 0.5303447842597961
CurrentTrain: epoch  3, batch    36 | loss: 6.3686743Losses:  4.955124855041504 0.18782901763916016
CurrentTrain: epoch  3, batch    37 | loss: 5.1429539Losses:  5.445795059204102 0.40415576100349426
CurrentTrain: epoch  4, batch     0 | loss: 5.8499508Losses:  6.402994155883789 0.586215615272522
CurrentTrain: epoch  4, batch     1 | loss: 6.9892097Losses:  5.696265697479248 0.37343281507492065
CurrentTrain: epoch  4, batch     2 | loss: 6.0696983Losses:  6.118566513061523 0.5110552310943604
CurrentTrain: epoch  4, batch     3 | loss: 6.6296215Losses:  5.692402362823486 0.39661967754364014
CurrentTrain: epoch  4, batch     4 | loss: 6.0890222Losses:  5.365659713745117 0.3281477093696594
CurrentTrain: epoch  4, batch     5 | loss: 5.6938076Losses:  5.476682662963867 0.44017699360847473
CurrentTrain: epoch  4, batch     6 | loss: 5.9168596Losses:  5.246927261352539 0.33299052715301514
CurrentTrain: epoch  4, batch     7 | loss: 5.5799179Losses:  5.589288711547852 0.328207403421402
CurrentTrain: epoch  4, batch     8 | loss: 5.9174962Losses:  5.3346452713012695 0.3096114695072174
CurrentTrain: epoch  4, batch     9 | loss: 5.6442566Losses:  5.837967872619629 0.396749347448349
CurrentTrain: epoch  4, batch    10 | loss: 6.2347174Losses:  5.277683258056641 0.32496365904808044
CurrentTrain: epoch  4, batch    11 | loss: 5.6026468Losses:  5.431299209594727 0.30721449851989746
CurrentTrain: epoch  4, batch    12 | loss: 5.7385139Losses:  5.814845561981201 0.32138580083847046
CurrentTrain: epoch  4, batch    13 | loss: 6.1362314Losses:  5.449916839599609 0.32159650325775146
CurrentTrain: epoch  4, batch    14 | loss: 5.7715135Losses:  5.8777265548706055 0.499569833278656
CurrentTrain: epoch  4, batch    15 | loss: 6.3772964Losses:  5.632746696472168 0.4164699912071228
CurrentTrain: epoch  4, batch    16 | loss: 6.0492167Losses:  5.422919273376465 0.3075593113899231
CurrentTrain: epoch  4, batch    17 | loss: 5.7304788Losses:  5.365677833557129 0.29895955324172974
CurrentTrain: epoch  4, batch    18 | loss: 5.6646376Losses:  5.316925525665283 0.2890061140060425
CurrentTrain: epoch  4, batch    19 | loss: 5.6059318Losses:  5.627549648284912 0.387641578912735
CurrentTrain: epoch  4, batch    20 | loss: 6.0151911Losses:  5.442474365234375 0.2700526714324951
CurrentTrain: epoch  4, batch    21 | loss: 5.7125273Losses:  6.00586462020874 0.4871850609779358
CurrentTrain: epoch  4, batch    22 | loss: 6.4930496Losses:  5.351221084594727 0.36573609709739685
CurrentTrain: epoch  4, batch    23 | loss: 5.7169571Losses:  5.681283473968506 0.33129268884658813
CurrentTrain: epoch  4, batch    24 | loss: 6.0125761Losses:  5.443327903747559 0.25095924735069275
CurrentTrain: epoch  4, batch    25 | loss: 5.6942873Losses:  6.112590789794922 0.5652074217796326
CurrentTrain: epoch  4, batch    26 | loss: 6.6777983Losses:  5.477855682373047 0.35028332471847534
CurrentTrain: epoch  4, batch    27 | loss: 5.8281388Losses:  5.2319207191467285 0.2992488145828247
CurrentTrain: epoch  4, batch    28 | loss: 5.5311694Losses:  4.963095664978027 0.2516140341758728
CurrentTrain: epoch  4, batch    29 | loss: 5.2147098Losses:  6.115328788757324 0.3272014260292053
CurrentTrain: epoch  4, batch    30 | loss: 6.4425302Losses:  5.267570495605469 0.28369006514549255
CurrentTrain: epoch  4, batch    31 | loss: 5.5512605Losses:  5.613201141357422 0.33424919843673706
CurrentTrain: epoch  4, batch    32 | loss: 5.9474502Losses:  5.257476806640625 0.399002343416214
CurrentTrain: epoch  4, batch    33 | loss: 5.6564794Losses:  5.916821479797363 0.4635256230831146
CurrentTrain: epoch  4, batch    34 | loss: 6.3803473Losses:  6.1323041915893555 0.5144560933113098
CurrentTrain: epoch  4, batch    35 | loss: 6.6467605Losses:  5.127011299133301 0.24681904911994934
CurrentTrain: epoch  4, batch    36 | loss: 5.3738303Losses:  5.187078475952148 0.2941286861896515
CurrentTrain: epoch  4, batch    37 | loss: 5.4812074Losses:  4.987395286560059 0.25505852699279785
CurrentTrain: epoch  5, batch     0 | loss: 5.2424536Losses:  5.2990264892578125 0.18979892134666443
CurrentTrain: epoch  5, batch     1 | loss: 5.4888253Losses:  5.250065803527832 0.25925302505493164
CurrentTrain: epoch  5, batch     2 | loss: 5.5093188Losses:  5.757608413696289 0.34888559579849243
CurrentTrain: epoch  5, batch     3 | loss: 6.1064939Losses:  5.417292594909668 0.2736530900001526
CurrentTrain: epoch  5, batch     4 | loss: 5.6909456Losses:  5.711730003356934 0.35884854197502136
CurrentTrain: epoch  5, batch     5 | loss: 6.0705786Losses:  5.345458030700684 0.32241955399513245
CurrentTrain: epoch  5, batch     6 | loss: 5.6678777Losses:  5.035793304443359 0.24006377160549164
CurrentTrain: epoch  5, batch     7 | loss: 5.2758570Losses:  5.308980941772461 0.31967025995254517
CurrentTrain: epoch  5, batch     8 | loss: 5.6286511Losses:  5.021055221557617 0.12300477921962738
CurrentTrain: epoch  5, batch     9 | loss: 5.1440601Losses:  5.308328628540039 0.3342908024787903
CurrentTrain: epoch  5, batch    10 | loss: 5.6426196Losses:  5.347532272338867 0.28676074743270874
CurrentTrain: epoch  5, batch    11 | loss: 5.6342931Losses:  5.539895057678223 0.38197803497314453
CurrentTrain: epoch  5, batch    12 | loss: 5.9218731Losses:  5.38213586807251 0.3386797308921814
CurrentTrain: epoch  5, batch    13 | loss: 5.7208157Losses:  5.13240385055542 0.2590405344963074
CurrentTrain: epoch  5, batch    14 | loss: 5.3914442Losses:  5.350931167602539 0.24085481464862823
CurrentTrain: epoch  5, batch    15 | loss: 5.5917859Losses:  5.432657718658447 0.23113244771957397
CurrentTrain: epoch  5, batch    16 | loss: 5.6637902Losses:  5.2728776931762695 0.27143195271492004
CurrentTrain: epoch  5, batch    17 | loss: 5.5443096Losses:  5.200649261474609 0.1982881724834442
CurrentTrain: epoch  5, batch    18 | loss: 5.3989372Losses:  4.98899507522583 0.2254304587841034
CurrentTrain: epoch  5, batch    19 | loss: 5.2144256Losses:  5.460363864898682 0.2583281993865967
CurrentTrain: epoch  5, batch    20 | loss: 5.7186918Losses:  5.422558784484863 0.2946596145629883
CurrentTrain: epoch  5, batch    21 | loss: 5.7172184Losses:  4.960963726043701 0.24865278601646423
CurrentTrain: epoch  5, batch    22 | loss: 5.2096167Losses:  5.41184139251709 0.297579288482666
CurrentTrain: epoch  5, batch    23 | loss: 5.7094207Losses:  5.525687217712402 0.3428569436073303
CurrentTrain: epoch  5, batch    24 | loss: 5.8685441Losses:  5.462996959686279 0.37069079279899597
CurrentTrain: epoch  5, batch    25 | loss: 5.8336878Losses:  5.03903865814209 0.20735225081443787
CurrentTrain: epoch  5, batch    26 | loss: 5.2463908Losses:  5.030932903289795 0.21499355137348175
CurrentTrain: epoch  5, batch    27 | loss: 5.2459264Losses:  5.625833511352539 0.3928913474082947
CurrentTrain: epoch  5, batch    28 | loss: 6.0187249Losses:  5.198336124420166 0.28321340680122375
CurrentTrain: epoch  5, batch    29 | loss: 5.4815497Losses:  5.080036640167236 0.2784273624420166
CurrentTrain: epoch  5, batch    30 | loss: 5.3584642Losses:  5.571433067321777 0.4611932039260864
CurrentTrain: epoch  5, batch    31 | loss: 6.0326262Losses:  4.753838539123535 0.2195120006799698
CurrentTrain: epoch  5, batch    32 | loss: 4.9733505Losses:  5.115553855895996 0.21660330891609192
CurrentTrain: epoch  5, batch    33 | loss: 5.3321571Losses:  5.27041482925415 0.26775047183036804
CurrentTrain: epoch  5, batch    34 | loss: 5.5381651Losses:  5.183065891265869 0.2447863519191742
CurrentTrain: epoch  5, batch    35 | loss: 5.4278522Losses:  4.889439582824707 0.1514827013015747
CurrentTrain: epoch  5, batch    36 | loss: 5.0409222Losses:  5.473438262939453 0.1943565458059311
CurrentTrain: epoch  5, batch    37 | loss: 5.6677947Losses:  5.164394855499268 0.2229413092136383
CurrentTrain: epoch  6, batch     0 | loss: 5.3873363Losses:  4.990077018737793 0.24782267212867737
CurrentTrain: epoch  6, batch     1 | loss: 5.2378998Losses:  5.14961051940918 0.29203319549560547
CurrentTrain: epoch  6, batch     2 | loss: 5.4416437Losses:  5.023592948913574 0.2273549884557724
CurrentTrain: epoch  6, batch     3 | loss: 5.2509480Losses:  4.912337779998779 0.20116209983825684
CurrentTrain: epoch  6, batch     4 | loss: 5.1134996Losses:  4.992023944854736 0.24150478839874268
CurrentTrain: epoch  6, batch     5 | loss: 5.2335286Losses:  5.134282112121582 0.2529523968696594
CurrentTrain: epoch  6, batch     6 | loss: 5.3872347Losses:  4.872645378112793 0.19955329596996307
CurrentTrain: epoch  6, batch     7 | loss: 5.0721989Losses:  5.408463478088379 0.28490665555000305
CurrentTrain: epoch  6, batch     8 | loss: 5.6933703Losses:  4.91900634765625 0.2734982371330261
CurrentTrain: epoch  6, batch     9 | loss: 5.1925044Losses:  5.136926651000977 0.26521265506744385
CurrentTrain: epoch  6, batch    10 | loss: 5.4021392Losses:  4.921643257141113 0.20738419890403748
CurrentTrain: epoch  6, batch    11 | loss: 5.1290274Losses:  4.812494277954102 0.18460765480995178
CurrentTrain: epoch  6, batch    12 | loss: 4.9971018Losses:  4.903011322021484 0.17179062962532043
CurrentTrain: epoch  6, batch    13 | loss: 5.0748019Losses:  5.129958152770996 0.21423281729221344
CurrentTrain: epoch  6, batch    14 | loss: 5.3441911Losses:  5.053918838500977 0.1216898113489151
CurrentTrain: epoch  6, batch    15 | loss: 5.1756086Losses:  4.841274738311768 0.1523900181055069
CurrentTrain: epoch  6, batch    16 | loss: 4.9936647Losses:  4.94298791885376 0.15648110210895538
CurrentTrain: epoch  6, batch    17 | loss: 5.0994692Losses:  5.0851545333862305 0.1845804750919342
CurrentTrain: epoch  6, batch    18 | loss: 5.2697349Losses:  5.453447341918945 0.33420389890670776
CurrentTrain: epoch  6, batch    19 | loss: 5.7876511Losses:  4.91008186340332 0.18071478605270386
CurrentTrain: epoch  6, batch    20 | loss: 5.0907965Losses:  5.000861167907715 0.16812542080879211
CurrentTrain: epoch  6, batch    21 | loss: 5.1689868Losses:  5.043094635009766 0.2578423023223877
CurrentTrain: epoch  6, batch    22 | loss: 5.3009367Losses:  4.873022079467773 0.15570349991321564
CurrentTrain: epoch  6, batch    23 | loss: 5.0287256Losses:  5.322277069091797 0.43157947063446045
CurrentTrain: epoch  6, batch    24 | loss: 5.7538567Losses:  4.773895740509033 0.17188143730163574
CurrentTrain: epoch  6, batch    25 | loss: 4.9457769Losses:  5.164210319519043 0.39846399426460266
CurrentTrain: epoch  6, batch    26 | loss: 5.5626745Losses:  4.734740734100342 0.11002311110496521
CurrentTrain: epoch  6, batch    27 | loss: 4.8447638Losses:  5.0240936279296875 0.15036234259605408
CurrentTrain: epoch  6, batch    28 | loss: 5.1744561Losses:  5.00606107711792 0.24869079887866974
CurrentTrain: epoch  6, batch    29 | loss: 5.2547517Losses:  4.781914710998535 0.15433457493782043
CurrentTrain: epoch  6, batch    30 | loss: 4.9362493Losses:  5.201279163360596 0.2613440155982971
CurrentTrain: epoch  6, batch    31 | loss: 5.4626231Losses:  4.9230451583862305 0.2403479367494583
CurrentTrain: epoch  6, batch    32 | loss: 5.1633930Losses:  5.309914588928223 0.3913145065307617
CurrentTrain: epoch  6, batch    33 | loss: 5.7012291Losses:  5.066394329071045 0.32025858759880066
CurrentTrain: epoch  6, batch    34 | loss: 5.3866529Losses:  4.618752479553223 0.1246824711561203
CurrentTrain: epoch  6, batch    35 | loss: 4.7434349Losses:  5.019801139831543 0.1719268411397934
CurrentTrain: epoch  6, batch    36 | loss: 5.1917281Losses:  5.3310227394104 0.3062489926815033
CurrentTrain: epoch  6, batch    37 | loss: 5.6372719Losses:  4.718038558959961 0.14609138667583466
CurrentTrain: epoch  7, batch     0 | loss: 4.8641300Losses:  5.01831579208374 0.17324888706207275
CurrentTrain: epoch  7, batch     1 | loss: 5.1915646Losses:  5.074193954467773 0.2442207634449005
CurrentTrain: epoch  7, batch     2 | loss: 5.3184147Losses:  4.688750743865967 0.15799163281917572
CurrentTrain: epoch  7, batch     3 | loss: 4.8467422Losses:  4.823272705078125 0.20690786838531494
CurrentTrain: epoch  7, batch     4 | loss: 5.0301805Losses:  4.73807430267334 0.14161260426044464
CurrentTrain: epoch  7, batch     5 | loss: 4.8796868Losses:  5.237009048461914 0.31489843130111694
CurrentTrain: epoch  7, batch     6 | loss: 5.5519075Losses:  4.691971778869629 0.1500576138496399
CurrentTrain: epoch  7, batch     7 | loss: 4.8420296Losses:  4.823189735412598 0.1723979264497757
CurrentTrain: epoch  7, batch     8 | loss: 4.9955878Losses:  4.691572189331055 0.15062983334064484
CurrentTrain: epoch  7, batch     9 | loss: 4.8422022Losses:  4.727011203765869 0.18110693991184235
CurrentTrain: epoch  7, batch    10 | loss: 4.9081182Losses:  4.820147514343262 0.20962685346603394
CurrentTrain: epoch  7, batch    11 | loss: 5.0297742Losses:  4.679177284240723 0.16897106170654297
CurrentTrain: epoch  7, batch    12 | loss: 4.8481483Losses:  4.7936506271362305 0.16390913724899292
CurrentTrain: epoch  7, batch    13 | loss: 4.9575596Losses:  4.811428546905518 0.17331257462501526
CurrentTrain: epoch  7, batch    14 | loss: 4.9847412Losses:  4.6713056564331055 0.15433189272880554
CurrentTrain: epoch  7, batch    15 | loss: 4.8256373Losses:  4.710127830505371 0.13837561011314392
CurrentTrain: epoch  7, batch    16 | loss: 4.8485036Losses:  4.642068386077881 0.1465788632631302
CurrentTrain: epoch  7, batch    17 | loss: 4.7886472Losses:  4.719408988952637 0.12321680784225464
CurrentTrain: epoch  7, batch    18 | loss: 4.8426256Losses:  4.819382667541504 0.17534314095973969
CurrentTrain: epoch  7, batch    19 | loss: 4.9947257Losses:  4.671294212341309 0.13415278494358063
CurrentTrain: epoch  7, batch    20 | loss: 4.8054471Losses:  4.646742343902588 0.1372334361076355
CurrentTrain: epoch  7, batch    21 | loss: 4.7839756Losses:  4.709571361541748 0.09937670826911926
CurrentTrain: epoch  7, batch    22 | loss: 4.8089480Losses:  4.952788352966309 0.2579648494720459
CurrentTrain: epoch  7, batch    23 | loss: 5.2107534Losses:  5.002114295959473 0.17610719799995422
CurrentTrain: epoch  7, batch    24 | loss: 5.1782217Losses:  4.733030796051025 0.19008366763591766
CurrentTrain: epoch  7, batch    25 | loss: 4.9231143Losses:  4.59263801574707 0.1323891282081604
CurrentTrain: epoch  7, batch    26 | loss: 4.7250271Losses:  4.943202972412109 0.1529051661491394
CurrentTrain: epoch  7, batch    27 | loss: 5.0961080Losses:  5.084646224975586 0.20098614692687988
CurrentTrain: epoch  7, batch    28 | loss: 5.2856321Losses:  4.636563301086426 0.13379976153373718
CurrentTrain: epoch  7, batch    29 | loss: 4.7703629Losses:  4.8225860595703125 0.20084801316261292
CurrentTrain: epoch  7, batch    30 | loss: 5.0234342Losses:  4.69532585144043 0.15481717884540558
CurrentTrain: epoch  7, batch    31 | loss: 4.8501430Losses:  4.6748456954956055 0.1519199162721634
CurrentTrain: epoch  7, batch    32 | loss: 4.8267655Losses:  4.713836669921875 0.09876365959644318
CurrentTrain: epoch  7, batch    33 | loss: 4.8126001Losses:  4.9501776695251465 0.1963074505329132
CurrentTrain: epoch  7, batch    34 | loss: 5.1464853Losses:  4.632318019866943 0.12299299240112305
CurrentTrain: epoch  7, batch    35 | loss: 4.7553110Losses:  4.633650779724121 0.13897641003131866
CurrentTrain: epoch  7, batch    36 | loss: 4.7726274Losses:  4.8581695556640625 0.14814946055412292
CurrentTrain: epoch  7, batch    37 | loss: 5.0063190Losses:  4.586449146270752 0.09735846519470215
CurrentTrain: epoch  8, batch     0 | loss: 4.6838074Losses:  4.6017374992370605 0.13084135949611664
CurrentTrain: epoch  8, batch     1 | loss: 4.7325788Losses:  4.623926162719727 0.12820905447006226
CurrentTrain: epoch  8, batch     2 | loss: 4.7521353Losses:  4.767068862915039 0.15362301468849182
CurrentTrain: epoch  8, batch     3 | loss: 4.9206920Losses:  4.67639684677124 0.089636892080307
CurrentTrain: epoch  8, batch     4 | loss: 4.7660336Losses:  4.655178070068359 0.0747339054942131
CurrentTrain: epoch  8, batch     5 | loss: 4.7299118Losses:  4.679637908935547 0.12847042083740234
CurrentTrain: epoch  8, batch     6 | loss: 4.8081083Losses:  4.609802722930908 0.14322197437286377
CurrentTrain: epoch  8, batch     7 | loss: 4.7530246Losses:  4.713067054748535 0.15795393288135529
CurrentTrain: epoch  8, batch     8 | loss: 4.8710208Losses:  5.0258331298828125 0.23505958914756775
CurrentTrain: epoch  8, batch     9 | loss: 5.2608929Losses:  4.690088272094727 0.13242638111114502
CurrentTrain: epoch  8, batch    10 | loss: 4.8225145Losses:  4.617883205413818 0.14048941433429718
CurrentTrain: epoch  8, batch    11 | loss: 4.7583728Losses:  5.050669193267822 0.17851699888706207
CurrentTrain: epoch  8, batch    12 | loss: 5.2291861Losses:  4.629554271697998 0.12501491606235504
CurrentTrain: epoch  8, batch    13 | loss: 4.7545691Losses:  4.749965190887451 0.15855097770690918
CurrentTrain: epoch  8, batch    14 | loss: 4.9085159Losses:  4.611532211303711 0.07695136964321136
CurrentTrain: epoch  8, batch    15 | loss: 4.6884837Losses:  4.6259918212890625 0.09114828705787659
CurrentTrain: epoch  8, batch    16 | loss: 4.7171402Losses:  4.723482131958008 0.10706555843353271
CurrentTrain: epoch  8, batch    17 | loss: 4.8305478Losses:  4.666243076324463 0.15178906917572021
CurrentTrain: epoch  8, batch    18 | loss: 4.8180323Losses:  4.641315937042236 0.13589170575141907
CurrentTrain: epoch  8, batch    19 | loss: 4.7772079Losses:  4.588255405426025 0.1267184019088745
CurrentTrain: epoch  8, batch    20 | loss: 4.7149739Losses:  4.623994827270508 0.12688906490802765
CurrentTrain: epoch  8, batch    21 | loss: 4.7508841Losses:  4.666784286499023 0.08678249269723892
CurrentTrain: epoch  8, batch    22 | loss: 4.7535667Losses:  4.614368438720703 0.13147284090518951
CurrentTrain: epoch  8, batch    23 | loss: 4.7458415Losses:  4.627811431884766 0.1107252910733223
CurrentTrain: epoch  8, batch    24 | loss: 4.7385368Losses:  4.58935546875 0.11118993163108826
CurrentTrain: epoch  8, batch    25 | loss: 4.7005453Losses:  4.641190528869629 0.08143244683742523
CurrentTrain: epoch  8, batch    26 | loss: 4.7226229Losses:  4.674603462219238 0.1096123456954956
CurrentTrain: epoch  8, batch    27 | loss: 4.7842159Losses:  4.644966125488281 0.16915418207645416
CurrentTrain: epoch  8, batch    28 | loss: 4.8141203Losses:  4.647848129272461 0.12782326340675354
CurrentTrain: epoch  8, batch    29 | loss: 4.7756715Losses:  4.7127275466918945 0.11365751922130585
CurrentTrain: epoch  8, batch    30 | loss: 4.8263850Losses:  4.578851222991943 0.11654632538557053
CurrentTrain: epoch  8, batch    31 | loss: 4.6953974Losses:  4.708493232727051 0.145926371216774
CurrentTrain: epoch  8, batch    32 | loss: 4.8544197Losses:  4.592425346374512 0.13185259699821472
CurrentTrain: epoch  8, batch    33 | loss: 4.7242780Losses:  4.792752265930176 0.204849511384964
CurrentTrain: epoch  8, batch    34 | loss: 4.9976020Losses:  4.674683094024658 0.10942581295967102
CurrentTrain: epoch  8, batch    35 | loss: 4.7841091Losses:  4.714573860168457 0.16566899418830872
CurrentTrain: epoch  8, batch    36 | loss: 4.8802428Losses:  4.711559295654297 0.15615001320838928
CurrentTrain: epoch  8, batch    37 | loss: 4.8677092Losses:  4.652968883514404 0.08373047411441803
CurrentTrain: epoch  9, batch     0 | loss: 4.7366996Losses:  4.758238792419434 0.16216441988945007
CurrentTrain: epoch  9, batch     1 | loss: 4.9204030Losses:  4.622460842132568 0.1301725208759308
CurrentTrain: epoch  9, batch     2 | loss: 4.7526336Losses:  4.601526737213135 0.12493899464607239
CurrentTrain: epoch  9, batch     3 | loss: 4.7264657Losses:  4.533185958862305 0.08547506481409073
CurrentTrain: epoch  9, batch     4 | loss: 4.6186609Losses:  4.631841659545898 0.11589998006820679
CurrentTrain: epoch  9, batch     5 | loss: 4.7477417Losses:  4.6082444190979 0.08393502980470657
CurrentTrain: epoch  9, batch     6 | loss: 4.6921797Losses:  4.551778316497803 0.10997435450553894
CurrentTrain: epoch  9, batch     7 | loss: 4.6617527Losses:  4.563422203063965 0.13389359414577484
CurrentTrain: epoch  9, batch     8 | loss: 4.6973157Losses:  4.552578926086426 0.09711074829101562
CurrentTrain: epoch  9, batch     9 | loss: 4.6496897Losses:  4.594644546508789 0.11455000936985016
CurrentTrain: epoch  9, batch    10 | loss: 4.7091947Losses:  4.588335037231445 0.10175903141498566
CurrentTrain: epoch  9, batch    11 | loss: 4.6900940Losses:  4.582169055938721 0.08691705018281937
CurrentTrain: epoch  9, batch    12 | loss: 4.6690860Losses:  4.599996566772461 0.12009860575199127
CurrentTrain: epoch  9, batch    13 | loss: 4.7200952Losses:  4.825405120849609 0.17798534035682678
CurrentTrain: epoch  9, batch    14 | loss: 5.0033903Losses:  4.586512565612793 0.0926777571439743
CurrentTrain: epoch  9, batch    15 | loss: 4.6791902Losses:  4.552360534667969 0.09995821863412857
CurrentTrain: epoch  9, batch    16 | loss: 4.6523190Losses:  4.65254020690918 0.14331580698490143
CurrentTrain: epoch  9, batch    17 | loss: 4.7958560Losses:  4.549417972564697 0.11328335106372833
CurrentTrain: epoch  9, batch    18 | loss: 4.6627011Losses:  4.6117753982543945 0.11450904607772827
CurrentTrain: epoch  9, batch    19 | loss: 4.7262845Losses:  4.569647789001465 0.10737967491149902
CurrentTrain: epoch  9, batch    20 | loss: 4.6770277Losses:  4.538755893707275 0.10391299426555634
CurrentTrain: epoch  9, batch    21 | loss: 4.6426687Losses:  4.558935165405273 0.09713514149188995
CurrentTrain: epoch  9, batch    22 | loss: 4.6560702Losses:  4.712092399597168 0.13323700428009033
CurrentTrain: epoch  9, batch    23 | loss: 4.8453293Losses:  4.56348180770874 0.09898489713668823
CurrentTrain: epoch  9, batch    24 | loss: 4.6624665Losses:  4.543874740600586 0.09390649199485779
CurrentTrain: epoch  9, batch    25 | loss: 4.6377811Losses:  4.558060646057129 0.10660134255886078
CurrentTrain: epoch  9, batch    26 | loss: 4.6646619Losses:  4.522183895111084 0.0816144198179245
CurrentTrain: epoch  9, batch    27 | loss: 4.6037984Losses:  4.771533966064453 0.08977708220481873
CurrentTrain: epoch  9, batch    28 | loss: 4.8613110Losses:  4.584692001342773 0.11482058465480804
CurrentTrain: epoch  9, batch    29 | loss: 4.6995125Losses:  4.511891841888428 0.09054113179445267
CurrentTrain: epoch  9, batch    30 | loss: 4.6024332Losses:  4.554139614105225 0.10709145665168762
CurrentTrain: epoch  9, batch    31 | loss: 4.6612310Losses:  4.810476303100586 0.2152155488729477
CurrentTrain: epoch  9, batch    32 | loss: 5.0256920Losses:  4.543734550476074 0.10303197801113129
CurrentTrain: epoch  9, batch    33 | loss: 4.6467667Losses:  4.579029560089111 0.0881757065653801
CurrentTrain: epoch  9, batch    34 | loss: 4.6672053Losses:  4.517849445343018 0.09273296594619751
CurrentTrain: epoch  9, batch    35 | loss: 4.6105824Losses:  4.539884567260742 0.10828767716884613
CurrentTrain: epoch  9, batch    36 | loss: 4.6481724Losses:  4.520918846130371 0.08959294855594635
CurrentTrain: epoch  9, batch    37 | loss: 4.6105118
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 0 1 0 0 1]
Losses:  7.790854454040527 1.6319148540496826
CurrentTrain: epoch  0, batch     0 | loss: 9.4227695Losses:  7.398643493652344 1.3467092514038086
CurrentTrain: epoch  0, batch     1 | loss: 8.7453527Losses:  4.895320892333984 1.393587350845337
CurrentTrain: epoch  1, batch     0 | loss: 6.2889080Losses:  4.761144161224365 1.342865228652954
CurrentTrain: epoch  1, batch     1 | loss: 6.1040096Losses:  4.529007911682129 1.2646918296813965
CurrentTrain: epoch  2, batch     0 | loss: 5.7936997Losses:  3.783229351043701 1.1369879245758057
CurrentTrain: epoch  2, batch     1 | loss: 4.9202175Losses:  3.7031538486480713 1.092299222946167
CurrentTrain: epoch  3, batch     0 | loss: 4.7954531Losses:  3.545628547668457 0.9130025506019592
CurrentTrain: epoch  3, batch     1 | loss: 4.4586310Losses:  3.454220771789551 1.066779613494873
CurrentTrain: epoch  4, batch     0 | loss: 4.5210004Losses:  3.6437175273895264 1.020902395248413
CurrentTrain: epoch  4, batch     1 | loss: 4.6646199Losses:  3.3735456466674805 0.956762433052063
CurrentTrain: epoch  5, batch     0 | loss: 4.3303080Losses:  3.3852598667144775 0.7002761363983154
CurrentTrain: epoch  5, batch     1 | loss: 4.0855360Losses:  2.861661672592163 0.9545747637748718
CurrentTrain: epoch  6, batch     0 | loss: 3.8162365Losses:  2.7171359062194824 0.7909213304519653
CurrentTrain: epoch  6, batch     1 | loss: 3.5080571Losses:  2.496830701828003 0.7951055765151978
CurrentTrain: epoch  7, batch     0 | loss: 3.2919364Losses:  2.2008471488952637 0.6473283767700195
CurrentTrain: epoch  7, batch     1 | loss: 2.8481755Losses:  2.3214797973632812 0.7408943176269531
CurrentTrain: epoch  8, batch     0 | loss: 3.0623741Losses:  2.16282057762146 0.6851469874382019
CurrentTrain: epoch  8, batch     1 | loss: 2.8479676Losses:  2.2761754989624023 0.7948642373085022
CurrentTrain: epoch  9, batch     0 | loss: 3.0710397Losses:  2.153986692428589 0.6361196637153625
CurrentTrain: epoch  9, batch     1 | loss: 2.7901063
Losses:  5.038311004638672 0.851634681224823
MemoryTrain:  epoch  0, batch     0 | loss: 5.8899455Losses:  0.7774031758308411 0.7932271361351013
MemoryTrain:  epoch  1, batch     0 | loss: 1.5706303Losses:  0.718764066696167 0.8293982744216919
MemoryTrain:  epoch  2, batch     0 | loss: 1.5481623Losses:  0.9188827276229858 0.7932413220405579
MemoryTrain:  epoch  3, batch     0 | loss: 1.7121241Losses:  0.773993194103241 0.8247017860412598
MemoryTrain:  epoch  4, batch     0 | loss: 1.5986950Losses:  0.6687049865722656 0.7783332467079163
MemoryTrain:  epoch  5, batch     0 | loss: 1.4470382Losses:  0.8878458738327026 0.8185244798660278
MemoryTrain:  epoch  6, batch     0 | loss: 1.7063704Losses:  0.8892773985862732 0.8432716131210327
MemoryTrain:  epoch  7, batch     0 | loss: 1.7325490Losses:  0.830790102481842 0.7841319441795349
MemoryTrain:  epoch  8, batch     0 | loss: 1.6149220Losses:  1.0132827758789062 0.7724666595458984
MemoryTrain:  epoch  9, batch     0 | loss: 1.7857494
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 63.39%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 87.82%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 86.43%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 85.42%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 83.38%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 81.25%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 79.52%   
cur_acc:  ['0.8674', '0.6339']
his_acc:  ['0.8674', '0.7952']
Clustering into  3  clusters
Clusters:  [1 0 0 0 0 0 1 2 0 0 2 1 0 0 0 0]
Losses:  7.2150163650512695 1.162186861038208
CurrentTrain: epoch  0, batch     0 | loss: 8.3772030Losses:  8.26640796661377 0.8103353977203369
CurrentTrain: epoch  0, batch     1 | loss: 9.0767431Losses:  4.808936595916748 1.0621072053909302
CurrentTrain: epoch  1, batch     0 | loss: 5.8710437Losses:  4.069642066955566 1.0515836477279663
CurrentTrain: epoch  1, batch     1 | loss: 5.1212258Losses:  3.6980791091918945 0.9316072463989258
CurrentTrain: epoch  2, batch     0 | loss: 4.6296864Losses:  4.586738109588623 1.1158636808395386
CurrentTrain: epoch  2, batch     1 | loss: 5.7026019Losses:  3.5056543350219727 1.0787540674209595
CurrentTrain: epoch  3, batch     0 | loss: 4.5844083Losses:  3.957378387451172 0.7054116129875183
CurrentTrain: epoch  3, batch     1 | loss: 4.6627898Losses:  3.847675323486328 1.1177446842193604
CurrentTrain: epoch  4, batch     0 | loss: 4.9654198Losses:  2.9044885635375977 0.7110514640808105
CurrentTrain: epoch  4, batch     1 | loss: 3.6155400Losses:  3.3823859691619873 0.9454671144485474
CurrentTrain: epoch  5, batch     0 | loss: 4.3278532Losses:  2.7691502571105957 0.6540196537971497
CurrentTrain: epoch  5, batch     1 | loss: 3.4231699Losses:  2.7268974781036377 0.924561619758606
CurrentTrain: epoch  6, batch     0 | loss: 3.6514592Losses:  3.439096212387085 0.9279759526252747
CurrentTrain: epoch  6, batch     1 | loss: 4.3670721Losses:  2.9381866455078125 1.004188895225525
CurrentTrain: epoch  7, batch     0 | loss: 3.9423757Losses:  2.702909231185913 0.4958614706993103
CurrentTrain: epoch  7, batch     1 | loss: 3.1987708Losses:  2.8794240951538086 0.8826209902763367
CurrentTrain: epoch  8, batch     0 | loss: 3.7620451Losses:  2.444251537322998 0.780677080154419
CurrentTrain: epoch  8, batch     1 | loss: 3.2249286Losses:  2.6114861965179443 0.774346113204956
CurrentTrain: epoch  9, batch     0 | loss: 3.3858323Losses:  2.6204915046691895 0.856254518032074
CurrentTrain: epoch  9, batch     1 | loss: 3.4767461
Losses:  5.925792694091797 1.2733019590377808
MemoryTrain:  epoch  0, batch     0 | loss: 7.1990948Losses:  2.0453062057495117 1.2998601198196411
MemoryTrain:  epoch  1, batch     0 | loss: 3.3451662Losses:  2.2262229919433594 1.292526125907898
MemoryTrain:  epoch  2, batch     0 | loss: 3.5187492Losses:  2.179319381713867 1.297677993774414
MemoryTrain:  epoch  3, batch     0 | loss: 3.4769974Losses:  2.049175262451172 1.310596227645874
MemoryTrain:  epoch  4, batch     0 | loss: 3.3597715Losses:  1.9361376762390137 1.280932068824768
MemoryTrain:  epoch  5, batch     0 | loss: 3.2170696Losses:  2.021216869354248 1.3084640502929688
MemoryTrain:  epoch  6, batch     0 | loss: 3.3296809Losses:  2.0213234424591064 1.3199818134307861
MemoryTrain:  epoch  7, batch     0 | loss: 3.3413053Losses:  2.016411304473877 1.2974371910095215
MemoryTrain:  epoch  8, batch     0 | loss: 3.3138485Losses:  1.841460943222046 1.3045167922973633
MemoryTrain:  epoch  9, batch     0 | loss: 3.1459777
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.16%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 81.10%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 78.69%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 76.73%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 76.40%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 73.20%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 72.05%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 70.72%   
cur_acc:  ['0.8674', '0.6339', '0.3516']
his_acc:  ['0.8674', '0.7952', '0.7072']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0]
Losses:  5.820435523986816 1.0566129684448242
CurrentTrain: epoch  0, batch     0 | loss: 6.8770485Losses:  8.08812427520752 0.7973424196243286
CurrentTrain: epoch  0, batch     1 | loss: 8.8854666Losses:  2.5647435188293457 0.910485565662384
CurrentTrain: epoch  1, batch     0 | loss: 3.4752290Losses:  3.1087870597839355 0.737683117389679
CurrentTrain: epoch  1, batch     1 | loss: 3.8464701Losses:  2.2910709381103516 0.8222280740737915
CurrentTrain: epoch  2, batch     0 | loss: 3.1132989Losses:  2.185309648513794 0.9457564949989319
CurrentTrain: epoch  2, batch     1 | loss: 3.1310661Losses:  2.1584055423736572 0.7574024796485901
CurrentTrain: epoch  3, batch     0 | loss: 2.9158080Losses:  2.1954100131988525 0.7123197317123413
CurrentTrain: epoch  3, batch     1 | loss: 2.9077296Losses:  2.062775135040283 0.7222967147827148
CurrentTrain: epoch  4, batch     0 | loss: 2.7850718Losses:  2.070903778076172 0.517273485660553
CurrentTrain: epoch  4, batch     1 | loss: 2.5881772Losses:  1.9021430015563965 0.5771613717079163
CurrentTrain: epoch  5, batch     0 | loss: 2.4793043Losses:  1.9927406311035156 0.4855945408344269
CurrentTrain: epoch  5, batch     1 | loss: 2.4783351Losses:  1.9238886833190918 0.5204233527183533
CurrentTrain: epoch  6, batch     0 | loss: 2.4443121Losses:  1.8205691576004028 0.44741037487983704
CurrentTrain: epoch  6, batch     1 | loss: 2.2679796Losses:  1.810537338256836 0.4428185820579529
CurrentTrain: epoch  7, batch     0 | loss: 2.2533560Losses:  1.9160455465316772 0.3113745152950287
CurrentTrain: epoch  7, batch     1 | loss: 2.2274201Losses:  1.8305574655532837 0.3853216767311096
CurrentTrain: epoch  8, batch     0 | loss: 2.2158792Losses:  1.7694095373153687 0.34868738055229187
CurrentTrain: epoch  8, batch     1 | loss: 2.1180968Losses:  1.7771497964859009 0.3435652554035187
CurrentTrain: epoch  9, batch     0 | loss: 2.1207151Losses:  1.736524224281311 0.2825284004211426
CurrentTrain: epoch  9, batch     1 | loss: 2.0190525
Losses:  6.139668941497803 1.3746178150177002
MemoryTrain:  epoch  0, batch     0 | loss: 7.5142870Losses:  11.185800552368164 0.5336308479309082
MemoryTrain:  epoch  0, batch     1 | loss: 11.7194309Losses:  2.2508177757263184 1.067711353302002
MemoryTrain:  epoch  1, batch     0 | loss: 3.3185291Losses:  2.8242745399475098 0.6780664324760437
MemoryTrain:  epoch  1, batch     1 | loss: 3.5023410Losses:  2.511855125427246 1.3671977519989014
MemoryTrain:  epoch  2, batch     0 | loss: 3.8790529Losses:  1.2840708494186401 0.4191651940345764
MemoryTrain:  epoch  2, batch     1 | loss: 1.7032361Losses:  2.1291825771331787 1.2697436809539795
MemoryTrain:  epoch  3, batch     0 | loss: 3.3989263Losses:  2.114746332168579 0.7205514907836914
MemoryTrain:  epoch  3, batch     1 | loss: 2.8352978Losses:  1.885973334312439 1.1505829095840454
MemoryTrain:  epoch  4, batch     0 | loss: 3.0365562Losses:  2.8233659267425537 0.6774500012397766
MemoryTrain:  epoch  4, batch     1 | loss: 3.5008159Losses:  2.3429994583129883 1.2692660093307495
MemoryTrain:  epoch  5, batch     0 | loss: 3.6122656Losses:  2.307486057281494 0.40296173095703125
MemoryTrain:  epoch  5, batch     1 | loss: 2.7104478Losses:  2.346031904220581 1.259360909461975
MemoryTrain:  epoch  6, batch     0 | loss: 3.6053929Losses:  1.6565830707550049 0.7315273284912109
MemoryTrain:  epoch  6, batch     1 | loss: 2.3881104Losses:  2.0425422191619873 1.2393763065338135
MemoryTrain:  epoch  7, batch     0 | loss: 3.2819185Losses:  2.1798901557922363 0.6475198268890381
MemoryTrain:  epoch  7, batch     1 | loss: 2.8274100Losses:  2.1825308799743652 1.3416939973831177
MemoryTrain:  epoch  8, batch     0 | loss: 3.5242248Losses:  1.9385842084884644 0.39675623178482056
MemoryTrain:  epoch  8, batch     1 | loss: 2.3353405Losses:  1.9871069192886353 1.2610626220703125
MemoryTrain:  epoch  9, batch     0 | loss: 3.2481694Losses:  2.3034565448760986 0.5100168585777283
MemoryTrain:  epoch  9, batch     1 | loss: 2.8134735
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 77.40%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 77.43%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.34%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 72.60%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 72.03%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 71.13%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 69.03%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 67.73%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.78%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 64.03%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 66.67%   
cur_acc:  ['0.8674', '0.6339', '0.3516', '0.7740']
his_acc:  ['0.8674', '0.7952', '0.7072', '0.6667']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0]
Losses:  8.478776931762695 1.3816192150115967
CurrentTrain: epoch  0, batch     0 | loss: 9.8603964Losses:  9.195147514343262 0.8277565836906433
CurrentTrain: epoch  0, batch     1 | loss: 10.0229044Losses:  4.941303253173828 1.2177541255950928
CurrentTrain: epoch  1, batch     0 | loss: 6.1590576Losses:  5.716263294219971 1.2133898735046387
CurrentTrain: epoch  1, batch     1 | loss: 6.9296532Losses:  5.297664642333984 1.257598876953125
CurrentTrain: epoch  2, batch     0 | loss: 6.5552635Losses:  4.309886932373047 1.0207364559173584
CurrentTrain: epoch  2, batch     1 | loss: 5.3306236Losses:  5.414943695068359 1.27711820602417
CurrentTrain: epoch  3, batch     0 | loss: 6.6920619Losses:  3.6421279907226562 0.717673122882843
CurrentTrain: epoch  3, batch     1 | loss: 4.3598013Losses:  4.790858745574951 1.1730016469955444
CurrentTrain: epoch  4, batch     0 | loss: 5.9638605Losses:  4.548445224761963 1.0871950387954712
CurrentTrain: epoch  4, batch     1 | loss: 5.6356401Losses:  4.870148658752441 1.1409754753112793
CurrentTrain: epoch  5, batch     0 | loss: 6.0111241Losses:  3.448805809020996 0.6985230445861816
CurrentTrain: epoch  5, batch     1 | loss: 4.1473289Losses:  3.8566927909851074 1.0189820528030396
CurrentTrain: epoch  6, batch     0 | loss: 4.8756747Losses:  4.344625949859619 0.8283583521842957
CurrentTrain: epoch  6, batch     1 | loss: 5.1729841Losses:  4.010056018829346 1.043601632118225
CurrentTrain: epoch  7, batch     0 | loss: 5.0536575Losses:  3.362675189971924 0.9113667607307434
CurrentTrain: epoch  7, batch     1 | loss: 4.2740421Losses:  3.4569873809814453 0.9737730026245117
CurrentTrain: epoch  8, batch     0 | loss: 4.4307604Losses:  3.8028132915496826 0.8720483779907227
CurrentTrain: epoch  8, batch     1 | loss: 4.6748619Losses:  3.247899293899536 0.8904216885566711
CurrentTrain: epoch  9, batch     0 | loss: 4.1383209Losses:  3.7823054790496826 0.865974485874176
CurrentTrain: epoch  9, batch     1 | loss: 4.6482801
Losses:  6.580289840698242 1.4246180057525635
MemoryTrain:  epoch  0, batch     0 | loss: 8.0049076Losses:  11.006924629211426 1.2271318435668945
MemoryTrain:  epoch  0, batch     1 | loss: 12.2340565Losses:  2.896068572998047 1.3260979652404785
MemoryTrain:  epoch  1, batch     0 | loss: 4.2221665Losses:  1.8455952405929565 1.1175609827041626
MemoryTrain:  epoch  1, batch     1 | loss: 2.9631562Losses:  2.5431063175201416 1.315365195274353
MemoryTrain:  epoch  2, batch     0 | loss: 3.8584714Losses:  2.2101504802703857 1.0540339946746826
MemoryTrain:  epoch  2, batch     1 | loss: 3.2641845Losses:  2.004399538040161 1.3427457809448242
MemoryTrain:  epoch  3, batch     0 | loss: 3.3471453Losses:  2.703526020050049 1.1571425199508667
MemoryTrain:  epoch  3, batch     1 | loss: 3.8606687Losses:  2.461880683898926 1.3033015727996826
MemoryTrain:  epoch  4, batch     0 | loss: 3.7651823Losses:  1.9858330488204956 1.0436843633651733
MemoryTrain:  epoch  4, batch     1 | loss: 3.0295174Losses:  2.4018561840057373 1.4894150495529175
MemoryTrain:  epoch  5, batch     0 | loss: 3.8912711Losses:  1.8578033447265625 0.8082418441772461
MemoryTrain:  epoch  5, batch     1 | loss: 2.6660452Losses:  1.9758141040802002 1.4542235136032104
MemoryTrain:  epoch  6, batch     0 | loss: 3.4300375Losses:  2.077784776687622 0.9250453114509583
MemoryTrain:  epoch  6, batch     1 | loss: 3.0028300Losses:  2.35150146484375 1.294771671295166
MemoryTrain:  epoch  7, batch     0 | loss: 3.6462731Losses:  1.6107813119888306 1.2023639678955078
MemoryTrain:  epoch  7, batch     1 | loss: 2.8131452Losses:  1.8934530019760132 1.5457534790039062
MemoryTrain:  epoch  8, batch     0 | loss: 3.4392066Losses:  2.4797236919403076 0.9605796933174133
MemoryTrain:  epoch  8, batch     1 | loss: 3.4403033Losses:  2.2556214332580566 1.3829057216644287
MemoryTrain:  epoch  9, batch     0 | loss: 3.6385272Losses:  2.37912917137146 0.9193178415298462
MemoryTrain:  epoch  9, batch     1 | loss: 3.2984471
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 9.82%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 14.06%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 19.44%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 21.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 25.00%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 27.08%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 29.33%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 32.59%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 34.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 35.55%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 35.66%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 35.76%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 34.87%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 34.69%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 34.52%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 33.81%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 76.43%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 70.16%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 69.19%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 67.12%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 67.02%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 67.09%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 66.00%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 66.50%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 65.93%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 66.11%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 65.17%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 64.40%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 63.48%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 62.59%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 61.81%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 61.39%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 61.23%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 61.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 60.86%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 60.88%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 60.82%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 60.60%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 60.96%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 60.59%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 60.39%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 60.04%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 59.56%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 59.30%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 58.98%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 58.45%   
cur_acc:  ['0.8674', '0.6339', '0.3516', '0.7740', '0.3381']
his_acc:  ['0.8674', '0.7952', '0.7072', '0.6667', '0.5845']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0]
Losses:  6.024168491363525 0.8799886703491211
CurrentTrain: epoch  0, batch     0 | loss: 6.9041572Losses:  5.916749954223633 1.0978041887283325
CurrentTrain: epoch  0, batch     1 | loss: 7.0145540Losses:  2.7921395301818848 0.8693281412124634
CurrentTrain: epoch  1, batch     0 | loss: 3.6614676Losses:  2.431342601776123 0.7149508595466614
CurrentTrain: epoch  1, batch     1 | loss: 3.1462934Losses:  2.391174077987671 0.7398122549057007
CurrentTrain: epoch  2, batch     0 | loss: 3.1309862Losses:  2.3217766284942627 0.5713794827461243
CurrentTrain: epoch  2, batch     1 | loss: 2.8931561Losses:  2.224961280822754 0.7150748372077942
CurrentTrain: epoch  3, batch     0 | loss: 2.9400361Losses:  2.0372047424316406 0.417551189661026
CurrentTrain: epoch  3, batch     1 | loss: 2.4547560Losses:  2.066826343536377 0.5626996755599976
CurrentTrain: epoch  4, batch     0 | loss: 2.6295261Losses:  2.078653573989868 0.7133530378341675
CurrentTrain: epoch  4, batch     1 | loss: 2.7920065Losses:  1.9905295372009277 0.5385112762451172
CurrentTrain: epoch  5, batch     0 | loss: 2.5290408Losses:  1.9229768514633179 0.38716432452201843
CurrentTrain: epoch  5, batch     1 | loss: 2.3101411Losses:  1.8545300960540771 0.505648136138916
CurrentTrain: epoch  6, batch     0 | loss: 2.3601782Losses:  1.8491662740707397 0.47816193103790283
CurrentTrain: epoch  6, batch     1 | loss: 2.3273282Losses:  1.8094207048416138 0.4781063199043274
CurrentTrain: epoch  7, batch     0 | loss: 2.2875271Losses:  1.8430290222167969 0.39110031723976135
CurrentTrain: epoch  7, batch     1 | loss: 2.2341294Losses:  1.7758889198303223 0.41302287578582764
CurrentTrain: epoch  8, batch     0 | loss: 2.1889119Losses:  1.721980094909668 0.27072563767433167
CurrentTrain: epoch  8, batch     1 | loss: 1.9927057Losses:  1.761455774307251 0.4118974208831787
CurrentTrain: epoch  9, batch     0 | loss: 2.1733532Losses:  1.7157938480377197 0.32026126980781555
CurrentTrain: epoch  9, batch     1 | loss: 2.0360551
Losses:  6.798223495483398 1.2811822891235352
MemoryTrain:  epoch  0, batch     0 | loss: 8.0794058Losses:  11.636395454406738 1.4819937944412231
MemoryTrain:  epoch  0, batch     1 | loss: 13.1183891Losses:  3.489745616912842 1.226418375968933
MemoryTrain:  epoch  1, batch     0 | loss: 4.7161641Losses:  2.93579363822937 1.4438225030899048
MemoryTrain:  epoch  1, batch     1 | loss: 4.3796163Losses:  3.162069320678711 1.3634049892425537
MemoryTrain:  epoch  2, batch     0 | loss: 4.5254745Losses:  3.0708937644958496 1.4237018823623657
MemoryTrain:  epoch  2, batch     1 | loss: 4.4945955Losses:  3.0016868114471436 1.381546974182129
MemoryTrain:  epoch  3, batch     0 | loss: 4.3832340Losses:  3.170497179031372 1.399409294128418
MemoryTrain:  epoch  3, batch     1 | loss: 4.5699062Losses:  2.9671499729156494 1.1409993171691895
MemoryTrain:  epoch  4, batch     0 | loss: 4.1081495Losses:  2.804013252258301 1.5034019947052002
MemoryTrain:  epoch  4, batch     1 | loss: 4.3074150Losses:  3.3950400352478027 1.3840935230255127
MemoryTrain:  epoch  5, batch     0 | loss: 4.7791338Losses:  2.4145331382751465 1.278384804725647
MemoryTrain:  epoch  5, batch     1 | loss: 3.6929178Losses:  3.1658639907836914 1.4711027145385742
MemoryTrain:  epoch  6, batch     0 | loss: 4.6369667Losses:  2.81089448928833 1.146048665046692
MemoryTrain:  epoch  6, batch     1 | loss: 3.9569430Losses:  2.698143482208252 1.3949806690216064
MemoryTrain:  epoch  7, batch     0 | loss: 4.0931244Losses:  3.2650041580200195 1.2662312984466553
MemoryTrain:  epoch  7, batch     1 | loss: 4.5312357Losses:  3.213693380355835 1.5025607347488403
MemoryTrain:  epoch  8, batch     0 | loss: 4.7162542Losses:  2.803525924682617 1.167982578277588
MemoryTrain:  epoch  8, batch     1 | loss: 3.9715085Losses:  2.8470826148986816 1.3505208492279053
MemoryTrain:  epoch  9, batch     0 | loss: 4.1976032Losses:  2.9494659900665283 1.312318205833435
MemoryTrain:  epoch  9, batch     1 | loss: 4.2617841
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 94.44%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 80.36%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 65.56%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 64.75%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 63.98%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 64.36%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 64.76%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 65.37%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 64.72%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 65.04%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 64.90%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 62.78%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 61.96%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 61.07%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 60.21%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 59.46%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 58.99%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 58.61%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 58.50%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 58.14%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 57.87%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 57.77%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 57.52%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 57.66%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 57.72%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 57.39%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 57.23%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 56.85%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 56.32%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 56.03%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 55.46%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 55.75%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 56.18%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 56.60%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.54%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.44%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.88%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.11%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 58.83%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 58.61%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 58.90%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 58.94%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 58.97%   
cur_acc:  ['0.8674', '0.6339', '0.3516', '0.7740', '0.3381', '0.8036']
his_acc:  ['0.8674', '0.7952', '0.7072', '0.6667', '0.5845', '0.5897']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0 1 2 0 0 0]
Losses:  6.537957191467285 1.0231839418411255
CurrentTrain: epoch  0, batch     0 | loss: 7.5611410Losses:  6.199331760406494 0.8661232590675354
CurrentTrain: epoch  0, batch     1 | loss: 7.0654550Losses:  3.314667224884033 0.7433710098266602
CurrentTrain: epoch  1, batch     0 | loss: 4.0580382Losses:  4.103122711181641 0.7277711033821106
CurrentTrain: epoch  1, batch     1 | loss: 4.8308940Losses:  3.648441791534424 0.7828770875930786
CurrentTrain: epoch  2, batch     0 | loss: 4.4313188Losses:  3.129795551300049 0.5937861204147339
CurrentTrain: epoch  2, batch     1 | loss: 3.7235818Losses:  2.9334588050842285 0.6168912649154663
CurrentTrain: epoch  3, batch     0 | loss: 3.5503502Losses:  3.8081963062286377 0.721047580242157
CurrentTrain: epoch  3, batch     1 | loss: 4.5292439Losses:  3.277097463607788 0.5975527167320251
CurrentTrain: epoch  4, batch     0 | loss: 3.8746502Losses:  2.9159693717956543 0.6416239738464355
CurrentTrain: epoch  4, batch     1 | loss: 3.5575933Losses:  3.150886058807373 0.5989798903465271
CurrentTrain: epoch  5, batch     0 | loss: 3.7498660Losses:  2.7276132106781006 0.5769771933555603
CurrentTrain: epoch  5, batch     1 | loss: 3.3045905Losses:  2.409857749938965 0.48632270097732544
CurrentTrain: epoch  6, batch     0 | loss: 2.8961804Losses:  3.395021438598633 0.5884221792221069
CurrentTrain: epoch  6, batch     1 | loss: 3.9834437Losses:  2.5580835342407227 0.40960946679115295
CurrentTrain: epoch  7, batch     0 | loss: 2.9676931Losses:  2.583104133605957 0.40361133217811584
CurrentTrain: epoch  7, batch     1 | loss: 2.9867156Losses:  2.309798002243042 0.4148412346839905
CurrentTrain: epoch  8, batch     0 | loss: 2.7246392Losses:  2.727328062057495 0.41975608468055725
CurrentTrain: epoch  8, batch     1 | loss: 3.1470842Losses:  2.2390570640563965 0.35224539041519165
CurrentTrain: epoch  9, batch     0 | loss: 2.5913024Losses:  2.4189350605010986 0.3256798982620239
CurrentTrain: epoch  9, batch     1 | loss: 2.7446151
Losses:  6.935982704162598 1.3075863122940063
MemoryTrain:  epoch  0, batch     0 | loss: 8.2435694Losses:  12.908472061157227 1.2108081579208374
MemoryTrain:  epoch  0, batch     1 | loss: 14.1192799Losses:  13.31697940826416 0.49222350120544434
MemoryTrain:  epoch  0, batch     2 | loss: 13.8092031Losses:  3.1194918155670166 1.3395644426345825
MemoryTrain:  epoch  1, batch     0 | loss: 4.4590564Losses:  3.513251781463623 1.187173843383789
MemoryTrain:  epoch  1, batch     1 | loss: 4.7004256Losses:  4.528460502624512 0.3579457402229309
MemoryTrain:  epoch  1, batch     2 | loss: 4.8864064Losses:  3.085353374481201 1.2805603742599487
MemoryTrain:  epoch  2, batch     0 | loss: 4.3659139Losses:  3.5804412364959717 1.3011367321014404
MemoryTrain:  epoch  2, batch     1 | loss: 4.8815780Losses:  3.977917194366455 0.5360103845596313
MemoryTrain:  epoch  2, batch     2 | loss: 4.5139275Losses:  3.453141689300537 1.3081581592559814
MemoryTrain:  epoch  3, batch     0 | loss: 4.7613001Losses:  3.117443084716797 1.3593826293945312
MemoryTrain:  epoch  3, batch     1 | loss: 4.4768257Losses:  4.3816423416137695 0.4335433840751648
MemoryTrain:  epoch  3, batch     2 | loss: 4.8151855Losses:  2.8747482299804688 1.3308606147766113
MemoryTrain:  epoch  4, batch     0 | loss: 4.2056088Losses:  3.6688051223754883 1.4020730257034302
MemoryTrain:  epoch  4, batch     1 | loss: 5.0708780Losses:  2.72379469871521 0.4003359377384186
MemoryTrain:  epoch  4, batch     2 | loss: 3.1241307Losses:  3.3642587661743164 1.2231311798095703
MemoryTrain:  epoch  5, batch     0 | loss: 4.5873899Losses:  2.74802565574646 1.3838400840759277
MemoryTrain:  epoch  5, batch     1 | loss: 4.1318655Losses:  3.1862194538116455 0.5010318756103516
MemoryTrain:  epoch  5, batch     2 | loss: 3.6872513Losses:  3.1777467727661133 1.2860229015350342
MemoryTrain:  epoch  6, batch     0 | loss: 4.4637699Losses:  2.9896645545959473 1.2096096277236938
MemoryTrain:  epoch  6, batch     1 | loss: 4.1992741Losses:  3.0363821983337402 0.18709290027618408
MemoryTrain:  epoch  6, batch     2 | loss: 3.2234750Losses:  3.1314687728881836 1.245123267173767
MemoryTrain:  epoch  7, batch     0 | loss: 4.3765922Losses:  3.1181833744049072 1.3359370231628418
MemoryTrain:  epoch  7, batch     1 | loss: 4.4541206Losses:  2.830902576446533 0.314767986536026
MemoryTrain:  epoch  7, batch     2 | loss: 3.1456707Losses:  3.103734254837036 1.3029108047485352
MemoryTrain:  epoch  8, batch     0 | loss: 4.4066448Losses:  2.7642955780029297 1.2443318367004395
MemoryTrain:  epoch  8, batch     1 | loss: 4.0086274Losses:  3.2406294345855713 0.42849481105804443
MemoryTrain:  epoch  8, batch     2 | loss: 3.6691241Losses:  2.854694366455078 1.3643001317977905
MemoryTrain:  epoch  9, batch     0 | loss: 4.2189946Losses:  3.0778470039367676 1.282002568244934
MemoryTrain:  epoch  9, batch     1 | loss: 4.3598495Losses:  2.766984462738037 0.46303877234458923
MemoryTrain:  epoch  9, batch     2 | loss: 3.2300231
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 41.25%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 79.02%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 77.21%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 72.30%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 71.22%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 67.56%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 66.72%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 64.31%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 63.32%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 62.88%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 61.75%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 60.78%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.49%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 61.21%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 62.19%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 62.20%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 62.40%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 62.03%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 61.10%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 60.29%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 59.42%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 58.57%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 57.75%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 56.94%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 56.42%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 56.08%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 56.00%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 55.67%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 55.44%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 55.45%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 55.22%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 55.47%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 55.40%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 55.03%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 54.89%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 54.54%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 54.04%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 53.71%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 53.16%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 53.55%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 53.93%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 54.37%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 54.88%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.37%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 55.85%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.32%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 56.78%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 57.03%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 56.77%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 56.57%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 56.82%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 56.94%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 57.05%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 56.99%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 56.98%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 56.85%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 56.85%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 56.66%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 56.89%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 57.18%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 57.34%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 57.44%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 57.43%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 56.92%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 56.47%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 55.98%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 55.49%   [EVAL] batch:  115 | acc: 0.00%,  total acc: 55.01%   
cur_acc:  ['0.8674', '0.6339', '0.3516', '0.7740', '0.3381', '0.8036', '0.4125']
his_acc:  ['0.8674', '0.7952', '0.7072', '0.6667', '0.5845', '0.5897', '0.5501']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 1 3 0 0 3 1 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0 1 0 0 3 1 0 0 0 1
 2 1 0 0]
Losses:  6.786345481872559 0.9758442044258118
CurrentTrain: epoch  0, batch     0 | loss: 7.7621899Losses:  7.082573890686035 0.7790349125862122
CurrentTrain: epoch  0, batch     1 | loss: 7.8616090Losses:  3.319612503051758 0.6566745042800903
CurrentTrain: epoch  1, batch     0 | loss: 3.9762869Losses:  3.47896146774292 0.5978682041168213
CurrentTrain: epoch  1, batch     1 | loss: 4.0768299Losses:  2.7942357063293457 0.728649377822876
CurrentTrain: epoch  2, batch     0 | loss: 3.5228851Losses:  3.181901693344116 0.9596249461174011
CurrentTrain: epoch  2, batch     1 | loss: 4.1415267Losses:  2.7289113998413086 0.6249080896377563
CurrentTrain: epoch  3, batch     0 | loss: 3.3538194Losses:  2.6752421855926514 0.6823052167892456
CurrentTrain: epoch  3, batch     1 | loss: 3.3575473Losses:  2.5756278038024902 0.6661557555198669
CurrentTrain: epoch  4, batch     0 | loss: 3.2417836Losses:  2.179003953933716 0.5440744757652283
CurrentTrain: epoch  4, batch     1 | loss: 2.7230785Losses:  2.2073416709899902 0.49180126190185547
CurrentTrain: epoch  5, batch     0 | loss: 2.6991429Losses:  2.135066509246826 0.4922191798686981
CurrentTrain: epoch  5, batch     1 | loss: 2.6272857Losses:  2.1426596641540527 0.5515027046203613
CurrentTrain: epoch  6, batch     0 | loss: 2.6941624Losses:  1.9628701210021973 0.32655349373817444
CurrentTrain: epoch  6, batch     1 | loss: 2.2894237Losses:  2.0206289291381836 0.41488343477249146
CurrentTrain: epoch  7, batch     0 | loss: 2.4355123Losses:  1.8685226440429688 0.3497432768344879
CurrentTrain: epoch  7, batch     1 | loss: 2.2182660Losses:  1.8870410919189453 0.4242146611213684
CurrentTrain: epoch  8, batch     0 | loss: 2.3112557Losses:  1.9320536851882935 0.3496982753276825
CurrentTrain: epoch  8, batch     1 | loss: 2.2817519Losses:  1.8518229722976685 0.3879657983779907
CurrentTrain: epoch  9, batch     0 | loss: 2.2397888Losses:  1.8527251482009888 0.38658633828163147
CurrentTrain: epoch  9, batch     1 | loss: 2.2393115
Losses:  6.612381935119629 1.0352845191955566
MemoryTrain:  epoch  0, batch     0 | loss: 7.6476665Losses:  11.843184471130371 1.4092092514038086
MemoryTrain:  epoch  0, batch     1 | loss: 13.2523937Losses:  13.614726066589355 1.0812287330627441
MemoryTrain:  epoch  0, batch     2 | loss: 14.6959553Losses:  3.20076847076416 1.426112413406372
MemoryTrain:  epoch  1, batch     0 | loss: 4.6268806Losses:  3.0851123332977295 1.144637107849121
MemoryTrain:  epoch  1, batch     1 | loss: 4.2297497Losses:  2.7812211513519287 0.885039210319519
MemoryTrain:  epoch  1, batch     2 | loss: 3.6662602Losses:  3.253636360168457 1.1535875797271729
MemoryTrain:  epoch  2, batch     0 | loss: 4.4072237Losses:  3.634148120880127 1.375121831893921
MemoryTrain:  epoch  2, batch     1 | loss: 5.0092697Losses:  2.0214877128601074 0.8132147789001465
MemoryTrain:  epoch  2, batch     2 | loss: 2.8347025Losses:  2.8461930751800537 1.331078290939331
MemoryTrain:  epoch  3, batch     0 | loss: 4.1772714Losses:  3.431570053100586 1.3599166870117188
MemoryTrain:  epoch  3, batch     1 | loss: 4.7914867Losses:  3.133066415786743 0.9095175266265869
MemoryTrain:  epoch  3, batch     2 | loss: 4.0425839Losses:  2.952594518661499 1.1883344650268555
MemoryTrain:  epoch  4, batch     0 | loss: 4.1409292Losses:  3.270413875579834 1.3444132804870605
MemoryTrain:  epoch  4, batch     1 | loss: 4.6148272Losses:  2.7046585083007812 0.6950635313987732
MemoryTrain:  epoch  4, batch     2 | loss: 3.3997221Losses:  2.9642677307128906 1.3336845636367798
MemoryTrain:  epoch  5, batch     0 | loss: 4.2979522Losses:  3.1483001708984375 1.3184086084365845
MemoryTrain:  epoch  5, batch     1 | loss: 4.4667087Losses:  2.5232086181640625 0.8382065296173096
MemoryTrain:  epoch  5, batch     2 | loss: 3.3614151Losses:  3.3398900032043457 1.2506203651428223
MemoryTrain:  epoch  6, batch     0 | loss: 4.5905104Losses:  2.6487231254577637 1.238358497619629
MemoryTrain:  epoch  6, batch     1 | loss: 3.8870816Losses:  3.23940372467041 0.7877452969551086
MemoryTrain:  epoch  6, batch     2 | loss: 4.0271492Losses:  3.1310925483703613 1.280869483947754
MemoryTrain:  epoch  7, batch     0 | loss: 4.4119620Losses:  2.8275227546691895 1.176560401916504
MemoryTrain:  epoch  7, batch     1 | loss: 4.0040832Losses:  3.083439826965332 0.9267531633377075
MemoryTrain:  epoch  7, batch     2 | loss: 4.0101929Losses:  2.919973134994507 1.2096619606018066
MemoryTrain:  epoch  8, batch     0 | loss: 4.1296349Losses:  2.9645516872406006 1.2717543840408325
MemoryTrain:  epoch  8, batch     1 | loss: 4.2363062Losses:  2.570463180541992 0.8361176252365112
MemoryTrain:  epoch  8, batch     2 | loss: 3.4065809Losses:  2.7670693397521973 1.2703182697296143
MemoryTrain:  epoch  9, batch     0 | loss: 4.0373878Losses:  2.855492353439331 1.361512541770935
MemoryTrain:  epoch  9, batch     1 | loss: 4.2170048Losses:  2.7132365703582764 0.6308353543281555
MemoryTrain:  epoch  9, batch     2 | loss: 3.3440719
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 38.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 58.33%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 72.74%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 69.08%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 67.31%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 18.75%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 60.24%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.07%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 60.08%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 59.00%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 57.97%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 57.09%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 56.01%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 55.79%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 56.02%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 56.14%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 56.36%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 56.90%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 57.63%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 58.02%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 57.89%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 57.76%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 58.30%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 57.98%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 57.58%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 56.72%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 55.97%   [EVAL] batch:   68 | acc: 0.00%,  total acc: 55.16%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 54.37%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 53.61%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 52.86%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 52.48%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 52.20%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 51.81%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 51.62%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 51.68%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 51.66%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 51.95%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 52.24%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 52.13%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 52.26%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 52.16%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 51.62%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 51.02%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 50.43%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 50.71%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 51.19%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 51.67%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 52.20%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 52.72%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.23%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 53.72%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.21%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 54.49%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 54.25%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 53.95%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 54.23%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 54.37%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 54.35%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 54.31%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 54.39%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 54.35%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 54.36%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 54.61%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 54.92%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 55.22%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 55.40%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 55.41%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 54.91%   [EVAL] batch:  112 | acc: 0.00%,  total acc: 54.42%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 53.95%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 53.48%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 53.12%   [EVAL] batch:  116 | acc: 6.25%,  total acc: 52.72%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 52.44%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 52.15%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 51.82%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 51.65%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 51.59%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 51.93%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 51.81%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 51.90%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 52.03%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 52.26%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 52.64%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 53.00%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 53.37%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 53.72%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 54.07%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 54.14%   
cur_acc:  ['0.8674', '0.6339', '0.3516', '0.7740', '0.3381', '0.8036', '0.4125', '0.5833']
his_acc:  ['0.8674', '0.7952', '0.7072', '0.6667', '0.5845', '0.5897', '0.5501', '0.5414']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.131872177124023 1.8913828134536743
CurrentTrain: epoch  0, batch     0 | loss: 14.0232553Losses:  14.742366790771484 1.7792258262634277
CurrentTrain: epoch  0, batch     1 | loss: 16.5215931Losses:  15.195734024047852 1.79475998878479
CurrentTrain: epoch  0, batch     2 | loss: 16.9904938Losses:  14.806375503540039 1.870894193649292
CurrentTrain: epoch  0, batch     3 | loss: 16.6772690Losses:  15.013158798217773 1.6119316816329956
CurrentTrain: epoch  0, batch     4 | loss: 16.6250896Losses:  14.548206329345703 1.6070127487182617
CurrentTrain: epoch  0, batch     5 | loss: 16.1552200Losses:  14.351371765136719 1.6405653953552246
CurrentTrain: epoch  0, batch     6 | loss: 15.9919376Losses:  14.166609764099121 1.671035885810852
CurrentTrain: epoch  0, batch     7 | loss: 15.8376455Losses:  13.633050918579102 1.3996299505233765
CurrentTrain: epoch  0, batch     8 | loss: 15.0326805Losses:  13.77049446105957 1.4046003818511963
CurrentTrain: epoch  0, batch     9 | loss: 15.1750946Losses:  13.429084777832031 1.3679578304290771
CurrentTrain: epoch  0, batch    10 | loss: 14.7970428Losses:  13.075887680053711 1.6486767530441284
CurrentTrain: epoch  0, batch    11 | loss: 14.7245646Losses:  12.71322250366211 1.6642835140228271
CurrentTrain: epoch  0, batch    12 | loss: 14.3775063Losses:  12.919533729553223 1.3308831453323364
CurrentTrain: epoch  0, batch    13 | loss: 14.2504168Losses:  12.653867721557617 1.5441102981567383
CurrentTrain: epoch  0, batch    14 | loss: 14.1979780Losses:  12.528643608093262 1.7600226402282715
CurrentTrain: epoch  0, batch    15 | loss: 14.2886658Losses:  12.003894805908203 1.4014227390289307
CurrentTrain: epoch  0, batch    16 | loss: 13.4053173Losses:  12.211471557617188 1.1105012893676758
CurrentTrain: epoch  0, batch    17 | loss: 13.3219728Losses:  12.052783966064453 1.6824100017547607
CurrentTrain: epoch  0, batch    18 | loss: 13.7351942Losses:  11.960264205932617 1.4021153450012207
CurrentTrain: epoch  0, batch    19 | loss: 13.3623791Losses:  11.38636589050293 1.5042564868927002
CurrentTrain: epoch  0, batch    20 | loss: 12.8906221Losses:  11.480481147766113 1.3717682361602783
CurrentTrain: epoch  0, batch    21 | loss: 12.8522491Losses:  10.715855598449707 1.1307241916656494
CurrentTrain: epoch  0, batch    22 | loss: 11.8465796Losses:  11.078897476196289 1.4328789710998535
CurrentTrain: epoch  0, batch    23 | loss: 12.5117760Losses:  10.432657241821289 1.284116506576538
CurrentTrain: epoch  0, batch    24 | loss: 11.7167740Losses:  10.32376480102539 1.3076541423797607
CurrentTrain: epoch  0, batch    25 | loss: 11.6314192Losses:  9.866674423217773 1.4380332231521606
CurrentTrain: epoch  0, batch    26 | loss: 11.3047075Losses:  9.94776439666748 1.3588452339172363
CurrentTrain: epoch  0, batch    27 | loss: 11.3066101Losses:  9.866791725158691 1.560653567314148
CurrentTrain: epoch  0, batch    28 | loss: 11.4274454Losses:  9.543638229370117 1.3728872537612915
CurrentTrain: epoch  0, batch    29 | loss: 10.9165258Losses:  9.318142890930176 1.405696153640747
CurrentTrain: epoch  0, batch    30 | loss: 10.7238388Losses:  8.931352615356445 1.3045539855957031
CurrentTrain: epoch  0, batch    31 | loss: 10.2359066Losses:  8.857967376708984 1.3791420459747314
CurrentTrain: epoch  0, batch    32 | loss: 10.2371092Losses:  9.086360931396484 1.134852409362793
CurrentTrain: epoch  0, batch    33 | loss: 10.2212133Losses:  8.226900100708008 1.1974554061889648
CurrentTrain: epoch  0, batch    34 | loss: 9.4243555Losses:  8.06071949005127 1.072685718536377
CurrentTrain: epoch  0, batch    35 | loss: 9.1334057Losses:  8.204072952270508 1.4462844133377075
CurrentTrain: epoch  0, batch    36 | loss: 9.6503572Losses:  8.017904281616211 1.186192274093628
CurrentTrain: epoch  0, batch    37 | loss: 9.2040968Losses:  7.633821964263916 0.9956024885177612
CurrentTrain: epoch  1, batch     0 | loss: 8.6294241Losses:  7.95204496383667 1.1659990549087524
CurrentTrain: epoch  1, batch     1 | loss: 9.1180439Losses:  7.697319030761719 1.2594302892684937
CurrentTrain: epoch  1, batch     2 | loss: 8.9567490Losses:  7.8062849044799805 1.178335428237915
CurrentTrain: epoch  1, batch     3 | loss: 8.9846201Losses:  7.7593584060668945 1.2586894035339355
CurrentTrain: epoch  1, batch     4 | loss: 9.0180473Losses:  7.607862949371338 0.8853325247764587
CurrentTrain: epoch  1, batch     5 | loss: 8.4931955Losses:  7.168952941894531 0.9938403964042664
CurrentTrain: epoch  1, batch     6 | loss: 8.1627932Losses:  7.131348133087158 0.9837249517440796
CurrentTrain: epoch  1, batch     7 | loss: 8.1150732Losses:  7.4873762130737305 1.0955662727355957
CurrentTrain: epoch  1, batch     8 | loss: 8.5829430Losses:  7.402875900268555 1.093144178390503
CurrentTrain: epoch  1, batch     9 | loss: 8.4960203Losses:  7.894754409790039 1.2387419939041138
CurrentTrain: epoch  1, batch    10 | loss: 9.1334963Losses:  7.8850016593933105 1.2471108436584473
CurrentTrain: epoch  1, batch    11 | loss: 9.1321125Losses:  7.281269073486328 1.1163675785064697
CurrentTrain: epoch  1, batch    12 | loss: 8.3976364Losses:  7.515227794647217 1.0634729862213135
CurrentTrain: epoch  1, batch    13 | loss: 8.5787010Losses:  7.233384132385254 1.1188982725143433
CurrentTrain: epoch  1, batch    14 | loss: 8.3522825Losses:  7.298734664916992 0.901027500629425
CurrentTrain: epoch  1, batch    15 | loss: 8.1997623Losses:  7.366793632507324 1.1829979419708252
CurrentTrain: epoch  1, batch    16 | loss: 8.5497913Losses:  7.384695053100586 1.0951290130615234
CurrentTrain: epoch  1, batch    17 | loss: 8.4798241Losses:  7.3599853515625 1.038201093673706
CurrentTrain: epoch  1, batch    18 | loss: 8.3981867Losses:  7.440211296081543 1.0082430839538574
CurrentTrain: epoch  1, batch    19 | loss: 8.4484539Losses:  7.106571197509766 0.8618899583816528
CurrentTrain: epoch  1, batch    20 | loss: 7.9684610Losses:  7.235069274902344 1.1180360317230225
CurrentTrain: epoch  1, batch    21 | loss: 8.3531055Losses:  7.135340690612793 0.8979493379592896
CurrentTrain: epoch  1, batch    22 | loss: 8.0332899Losses:  7.2086052894592285 1.056962013244629
CurrentTrain: epoch  1, batch    23 | loss: 8.2655678Losses:  7.165635585784912 0.9434769153594971
CurrentTrain: epoch  1, batch    24 | loss: 8.1091127Losses:  7.246881008148193 0.8683952689170837
CurrentTrain: epoch  1, batch    25 | loss: 8.1152763Losses:  6.885375022888184 0.9240517020225525
CurrentTrain: epoch  1, batch    26 | loss: 7.8094268Losses:  7.161286354064941 0.8378618359565735
CurrentTrain: epoch  1, batch    27 | loss: 7.9991484Losses:  6.792082786560059 0.8057968616485596
CurrentTrain: epoch  1, batch    28 | loss: 7.5978794Losses:  6.470256805419922 0.790645956993103
CurrentTrain: epoch  1, batch    29 | loss: 7.2609029Losses:  6.952241897583008 0.9083437919616699
CurrentTrain: epoch  1, batch    30 | loss: 7.8605857Losses:  6.471577167510986 0.8887773752212524
CurrentTrain: epoch  1, batch    31 | loss: 7.3603544Losses:  6.737320899963379 0.8059943914413452
CurrentTrain: epoch  1, batch    32 | loss: 7.5433154Losses:  6.741690635681152 0.8534759283065796
CurrentTrain: epoch  1, batch    33 | loss: 7.5951667Losses:  7.006421089172363 0.705332338809967
CurrentTrain: epoch  1, batch    34 | loss: 7.7117534Losses:  7.007246971130371 0.7807964086532593
CurrentTrain: epoch  1, batch    35 | loss: 7.7880435Losses:  6.680917739868164 0.7988356947898865
CurrentTrain: epoch  1, batch    36 | loss: 7.4797535Losses:  7.4951348304748535 0.5994182825088501
CurrentTrain: epoch  1, batch    37 | loss: 8.0945530Losses:  6.335797309875488 0.5816715359687805
CurrentTrain: epoch  2, batch     0 | loss: 6.9174690Losses:  7.1393842697143555 0.6914769411087036
CurrentTrain: epoch  2, batch     1 | loss: 7.8308611Losses:  6.2190046310424805 0.5926204919815063
CurrentTrain: epoch  2, batch     2 | loss: 6.8116250Losses:  6.590860366821289 0.6379553079605103
CurrentTrain: epoch  2, batch     3 | loss: 7.2288156Losses:  6.93571138381958 0.8164882063865662
CurrentTrain: epoch  2, batch     4 | loss: 7.7521996Losses:  6.7108564376831055 0.6083735227584839
CurrentTrain: epoch  2, batch     5 | loss: 7.3192301Losses:  6.484554767608643 0.4723801910877228
CurrentTrain: epoch  2, batch     6 | loss: 6.9569349Losses:  6.441708564758301 0.6815884113311768
CurrentTrain: epoch  2, batch     7 | loss: 7.1232967Losses:  6.285828590393066 0.6356979608535767
CurrentTrain: epoch  2, batch     8 | loss: 6.9215264Losses:  6.371412754058838 0.6122816801071167
CurrentTrain: epoch  2, batch     9 | loss: 6.9836946Losses:  6.691205978393555 0.6038104295730591
CurrentTrain: epoch  2, batch    10 | loss: 7.2950163Losses:  6.418874740600586 0.6856513023376465
CurrentTrain: epoch  2, batch    11 | loss: 7.1045260Losses:  6.5039873123168945 0.8439463376998901
CurrentTrain: epoch  2, batch    12 | loss: 7.3479338Losses:  6.308584213256836 0.6016364693641663
CurrentTrain: epoch  2, batch    13 | loss: 6.9102206Losses:  6.565544605255127 0.760028600692749
CurrentTrain: epoch  2, batch    14 | loss: 7.3255730Losses:  6.323193550109863 0.5310932397842407
CurrentTrain: epoch  2, batch    15 | loss: 6.8542867Losses:  6.850303649902344 0.6942614316940308
CurrentTrain: epoch  2, batch    16 | loss: 7.5445652Losses:  6.450420379638672 0.4259710907936096
CurrentTrain: epoch  2, batch    17 | loss: 6.8763914Losses:  6.420260429382324 0.6233296394348145
CurrentTrain: epoch  2, batch    18 | loss: 7.0435901Losses:  6.3466081619262695 0.5237577557563782
CurrentTrain: epoch  2, batch    19 | loss: 6.8703661Losses:  6.618942737579346 0.6845846176147461
CurrentTrain: epoch  2, batch    20 | loss: 7.3035274Losses:  6.016637802124023 0.6307469606399536
CurrentTrain: epoch  2, batch    21 | loss: 6.6473846Losses:  6.250328063964844 0.787568986415863
CurrentTrain: epoch  2, batch    22 | loss: 7.0378971Losses:  6.107128620147705 0.5601887702941895
CurrentTrain: epoch  2, batch    23 | loss: 6.6673174Losses:  6.069659233093262 0.5649892091751099
CurrentTrain: epoch  2, batch    24 | loss: 6.6346483Losses:  5.890676498413086 0.5851696729660034
CurrentTrain: epoch  2, batch    25 | loss: 6.4758463Losses:  6.792631149291992 0.623396635055542
CurrentTrain: epoch  2, batch    26 | loss: 7.4160280Losses:  6.4437971115112305 0.4744415879249573
CurrentTrain: epoch  2, batch    27 | loss: 6.9182386Losses:  6.065735816955566 0.6057295203208923
CurrentTrain: epoch  2, batch    28 | loss: 6.6714654Losses:  6.33125114440918 0.6377531290054321
CurrentTrain: epoch  2, batch    29 | loss: 6.9690042Losses:  6.6306562423706055 0.6981173753738403
CurrentTrain: epoch  2, batch    30 | loss: 7.3287735Losses:  6.077884197235107 0.5201065540313721
CurrentTrain: epoch  2, batch    31 | loss: 6.5979910Losses:  6.616953372955322 0.7868461012840271
CurrentTrain: epoch  2, batch    32 | loss: 7.4037995Losses:  6.728896617889404 0.7881909608840942
CurrentTrain: epoch  2, batch    33 | loss: 7.5170875Losses:  6.026033401489258 0.4694824814796448
CurrentTrain: epoch  2, batch    34 | loss: 6.4955158Losses:  5.9355645179748535 0.5947840213775635
CurrentTrain: epoch  2, batch    35 | loss: 6.5303488Losses:  5.918248176574707 0.5755195617675781
CurrentTrain: epoch  2, batch    36 | loss: 6.4937677Losses:  6.802042007446289 0.4911148250102997
CurrentTrain: epoch  2, batch    37 | loss: 7.2931566Losses:  5.504096984863281 0.30245277285575867
CurrentTrain: epoch  3, batch     0 | loss: 5.8065495Losses:  5.967280387878418 0.55182945728302
CurrentTrain: epoch  3, batch     1 | loss: 6.5191097Losses:  5.544126033782959 0.30640289187431335
CurrentTrain: epoch  3, batch     2 | loss: 5.8505287Losses:  6.996488094329834 0.6260716915130615
CurrentTrain: epoch  3, batch     3 | loss: 7.6225595Losses:  5.696898460388184 0.4843943119049072
CurrentTrain: epoch  3, batch     4 | loss: 6.1812925Losses:  5.931342124938965 0.4173124134540558
CurrentTrain: epoch  3, batch     5 | loss: 6.3486547Losses:  6.128661155700684 0.4595826268196106
CurrentTrain: epoch  3, batch     6 | loss: 6.5882440Losses:  6.446081161499023 0.43629100918769836
CurrentTrain: epoch  3, batch     7 | loss: 6.8823724Losses:  6.227062702178955 0.5007185935974121
CurrentTrain: epoch  3, batch     8 | loss: 6.7277813Losses:  6.311295509338379 0.5873380899429321
CurrentTrain: epoch  3, batch     9 | loss: 6.8986335Losses:  5.673903942108154 0.39509719610214233
CurrentTrain: epoch  3, batch    10 | loss: 6.0690012Losses:  5.735538959503174 0.3692871928215027
CurrentTrain: epoch  3, batch    11 | loss: 6.1048260Losses:  6.229720115661621 0.6027112007141113
CurrentTrain: epoch  3, batch    12 | loss: 6.8324313Losses:  6.105222225189209 0.5568240284919739
CurrentTrain: epoch  3, batch    13 | loss: 6.6620464Losses:  5.996874809265137 0.6846801042556763
CurrentTrain: epoch  3, batch    14 | loss: 6.6815548Losses:  6.562288761138916 0.5349235534667969
CurrentTrain: epoch  3, batch    15 | loss: 7.0972123Losses:  6.316837787628174 0.6276577115058899
CurrentTrain: epoch  3, batch    16 | loss: 6.9444957Losses:  5.777778148651123 0.4050306975841522
CurrentTrain: epoch  3, batch    17 | loss: 6.1828089Losses:  5.906065940856934 0.41202259063720703
CurrentTrain: epoch  3, batch    18 | loss: 6.3180885Losses:  6.3184051513671875 0.4711361527442932
CurrentTrain: epoch  3, batch    19 | loss: 6.7895412Losses:  6.043811798095703 0.4478594660758972
CurrentTrain: epoch  3, batch    20 | loss: 6.4916711Losses:  5.763928413391113 0.3745361566543579
CurrentTrain: epoch  3, batch    21 | loss: 6.1384645Losses:  5.830460548400879 0.32887834310531616
CurrentTrain: epoch  3, batch    22 | loss: 6.1593390Losses:  5.662655353546143 0.44528186321258545
CurrentTrain: epoch  3, batch    23 | loss: 6.1079373Losses:  5.817844390869141 0.47349482774734497
CurrentTrain: epoch  3, batch    24 | loss: 6.2913394Losses:  5.3677778244018555 0.3435799777507782
CurrentTrain: epoch  3, batch    25 | loss: 5.7113576Losses:  5.252986431121826 0.3924108147621155
CurrentTrain: epoch  3, batch    26 | loss: 5.6453972Losses:  5.590521335601807 0.3884570300579071
CurrentTrain: epoch  3, batch    27 | loss: 5.9789782Losses:  5.701912879943848 0.3397939205169678
CurrentTrain: epoch  3, batch    28 | loss: 6.0417070Losses:  5.97305965423584 0.3891638219356537
CurrentTrain: epoch  3, batch    29 | loss: 6.3622236Losses:  5.680330276489258 0.43945109844207764
CurrentTrain: epoch  3, batch    30 | loss: 6.1197815Losses:  5.794552326202393 0.403851181268692
CurrentTrain: epoch  3, batch    31 | loss: 6.1984034Losses:  5.901678085327148 0.34972289204597473
CurrentTrain: epoch  3, batch    32 | loss: 6.2514009Losses:  5.5381903648376465 0.39126428961753845
CurrentTrain: epoch  3, batch    33 | loss: 5.9294548Losses:  5.1936235427856445 0.23779088258743286
CurrentTrain: epoch  3, batch    34 | loss: 5.4314146Losses:  6.043900966644287 0.43222564458847046
CurrentTrain: epoch  3, batch    35 | loss: 6.4761267Losses:  5.3840532302856445 0.32588833570480347
CurrentTrain: epoch  3, batch    36 | loss: 5.7099414Losses:  5.243546485900879 0.26365238428115845
CurrentTrain: epoch  3, batch    37 | loss: 5.5071988Losses:  5.441046237945557 0.3296186923980713
CurrentTrain: epoch  4, batch     0 | loss: 5.7706652Losses:  5.50412654876709 0.4031425714492798
CurrentTrain: epoch  4, batch     1 | loss: 5.9072690Losses:  5.441652297973633 0.3151814937591553
CurrentTrain: epoch  4, batch     2 | loss: 5.7568340Losses:  5.423480033874512 0.2676018178462982
CurrentTrain: epoch  4, batch     3 | loss: 5.6910820Losses:  5.417142868041992 0.3595222234725952
CurrentTrain: epoch  4, batch     4 | loss: 5.7766652Losses:  5.642971992492676 0.24499711394309998
CurrentTrain: epoch  4, batch     5 | loss: 5.8879690Losses:  5.172462463378906 0.27856457233428955
CurrentTrain: epoch  4, batch     6 | loss: 5.4510269Losses:  5.70212459564209 0.5571433305740356
CurrentTrain: epoch  4, batch     7 | loss: 6.2592678Losses:  5.242288112640381 0.29933178424835205
CurrentTrain: epoch  4, batch     8 | loss: 5.5416198Losses:  6.035554885864258 0.4012661576271057
CurrentTrain: epoch  4, batch     9 | loss: 6.4368210Losses:  5.685696601867676 0.36847174167633057
CurrentTrain: epoch  4, batch    10 | loss: 6.0541682Losses:  5.279044151306152 0.3434831500053406
CurrentTrain: epoch  4, batch    11 | loss: 5.6225271Losses:  5.672462463378906 0.4099743664264679
CurrentTrain: epoch  4, batch    12 | loss: 6.0824370Losses:  5.282699108123779 0.3873406648635864
CurrentTrain: epoch  4, batch    13 | loss: 5.6700397Losses:  5.486347675323486 0.41750335693359375
CurrentTrain: epoch  4, batch    14 | loss: 5.9038510Losses:  5.89295768737793 0.5956649780273438
CurrentTrain: epoch  4, batch    15 | loss: 6.4886227Losses:  5.530651092529297 0.4357060194015503
CurrentTrain: epoch  4, batch    16 | loss: 5.9663572Losses:  5.4916839599609375 0.33129650354385376
CurrentTrain: epoch  4, batch    17 | loss: 5.8229804Losses:  6.306333541870117 0.751688539981842
CurrentTrain: epoch  4, batch    18 | loss: 7.0580220Losses:  5.702437400817871 0.44159314036369324
CurrentTrain: epoch  4, batch    19 | loss: 6.1440306Losses:  5.282149791717529 0.3704756498336792
CurrentTrain: epoch  4, batch    20 | loss: 5.6526256Losses:  5.1485795974731445 0.282017320394516
CurrentTrain: epoch  4, batch    21 | loss: 5.4305968Losses:  5.236246109008789 0.3399489223957062
CurrentTrain: epoch  4, batch    22 | loss: 5.5761952Losses:  5.875368595123291 0.31749221682548523
CurrentTrain: epoch  4, batch    23 | loss: 6.1928606Losses:  5.606135845184326 0.4270537793636322
CurrentTrain: epoch  4, batch    24 | loss: 6.0331898Losses:  5.899321556091309 0.41215160489082336
CurrentTrain: epoch  4, batch    25 | loss: 6.3114734Losses:  5.83632755279541 0.3186512887477875
CurrentTrain: epoch  4, batch    26 | loss: 6.1549788Losses:  5.611135482788086 0.3469875454902649
CurrentTrain: epoch  4, batch    27 | loss: 5.9581232Losses:  5.528954029083252 0.30458712577819824
CurrentTrain: epoch  4, batch    28 | loss: 5.8335409Losses:  5.1020050048828125 0.2431013435125351
CurrentTrain: epoch  4, batch    29 | loss: 5.3451061Losses:  5.382184028625488 0.30013740062713623
CurrentTrain: epoch  4, batch    30 | loss: 5.6823215Losses:  5.70959997177124 0.22179704904556274
CurrentTrain: epoch  4, batch    31 | loss: 5.9313970Losses:  5.84615421295166 0.43017253279685974
CurrentTrain: epoch  4, batch    32 | loss: 6.2763267Losses:  5.217511177062988 0.312351256608963
CurrentTrain: epoch  4, batch    33 | loss: 5.5298624Losses:  5.438059329986572 0.34946954250335693
CurrentTrain: epoch  4, batch    34 | loss: 5.7875290Losses:  5.3068671226501465 0.30294597148895264
CurrentTrain: epoch  4, batch    35 | loss: 5.6098132Losses:  5.604380130767822 0.34069931507110596
CurrentTrain: epoch  4, batch    36 | loss: 5.9450793Losses:  5.040426254272461 0.26850441098213196
CurrentTrain: epoch  4, batch    37 | loss: 5.3089309Losses:  5.22642707824707 0.3284386396408081
CurrentTrain: epoch  5, batch     0 | loss: 5.5548658Losses:  5.166947364807129 0.2529851496219635
CurrentTrain: epoch  5, batch     1 | loss: 5.4199324Losses:  5.239251136779785 0.24430310726165771
CurrentTrain: epoch  5, batch     2 | loss: 5.4835544Losses:  5.086338996887207 0.19139881432056427
CurrentTrain: epoch  5, batch     3 | loss: 5.2777376Losses:  5.271745681762695 0.2971158027648926
CurrentTrain: epoch  5, batch     4 | loss: 5.5688615Losses:  5.601274013519287 0.35557112097740173
CurrentTrain: epoch  5, batch     5 | loss: 5.9568453Losses:  5.205297470092773 0.31839990615844727
CurrentTrain: epoch  5, batch     6 | loss: 5.5236974Losses:  5.117185592651367 0.2055089771747589
CurrentTrain: epoch  5, batch     7 | loss: 5.3226948Losses:  5.4006757736206055 0.22789280116558075
CurrentTrain: epoch  5, batch     8 | loss: 5.6285686Losses:  5.7089362144470215 0.40933454036712646
CurrentTrain: epoch  5, batch     9 | loss: 6.1182709Losses:  5.659022331237793 0.4627421200275421
CurrentTrain: epoch  5, batch    10 | loss: 6.1217647Losses:  4.836730480194092 0.20456771552562714
CurrentTrain: epoch  5, batch    11 | loss: 5.0412984Losses:  5.248863220214844 0.28335344791412354
CurrentTrain: epoch  5, batch    12 | loss: 5.5322165Losses:  5.252843379974365 0.22721436619758606
CurrentTrain: epoch  5, batch    13 | loss: 5.4800577Losses:  5.25521183013916 0.3016665577888489
CurrentTrain: epoch  5, batch    14 | loss: 5.5568786Losses:  5.3203535079956055 0.31500160694122314
CurrentTrain: epoch  5, batch    15 | loss: 5.6353550Losses:  5.345602989196777 0.30879056453704834
CurrentTrain: epoch  5, batch    16 | loss: 5.6543937Losses:  5.235600471496582 0.2860841155052185
CurrentTrain: epoch  5, batch    17 | loss: 5.5216846Losses:  5.165661811828613 0.24408993124961853
CurrentTrain: epoch  5, batch    18 | loss: 5.4097519Losses:  5.286245346069336 0.2025526762008667
CurrentTrain: epoch  5, batch    19 | loss: 5.4887981Losses:  5.2131476402282715 0.2853180468082428
CurrentTrain: epoch  5, batch    20 | loss: 5.4984655Losses:  5.140235424041748 0.18971386551856995
CurrentTrain: epoch  5, batch    21 | loss: 5.3299494Losses:  5.016757965087891 0.10622875392436981
CurrentTrain: epoch  5, batch    22 | loss: 5.1229868Losses:  5.50262975692749 0.4671502709388733
CurrentTrain: epoch  5, batch    23 | loss: 5.9697800Losses:  4.958531379699707 0.2455478012561798
CurrentTrain: epoch  5, batch    24 | loss: 5.2040792Losses:  5.159701824188232 0.18123170733451843
CurrentTrain: epoch  5, batch    25 | loss: 5.3409333Losses:  5.012916088104248 0.25603601336479187
CurrentTrain: epoch  5, batch    26 | loss: 5.2689519Losses:  5.205779552459717 0.24371790885925293
CurrentTrain: epoch  5, batch    27 | loss: 5.4494972Losses:  5.41989803314209 0.2808351218700409
CurrentTrain: epoch  5, batch    28 | loss: 5.7007332Losses:  4.9638752937316895 0.21524140238761902
CurrentTrain: epoch  5, batch    29 | loss: 5.1791167Losses:  5.112395286560059 0.28666388988494873
CurrentTrain: epoch  5, batch    30 | loss: 5.3990593Losses:  5.126798629760742 0.23483744263648987
CurrentTrain: epoch  5, batch    31 | loss: 5.3616362Losses:  6.111995697021484 0.3756362497806549
CurrentTrain: epoch  5, batch    32 | loss: 6.4876318Losses:  4.855337142944336 0.19811245799064636
CurrentTrain: epoch  5, batch    33 | loss: 5.0534496Losses:  5.057602882385254 0.231865793466568
CurrentTrain: epoch  5, batch    34 | loss: 5.2894688Losses:  4.8754425048828125 0.23888368904590607
CurrentTrain: epoch  5, batch    35 | loss: 5.1143260Losses:  4.8861494064331055 0.20657896995544434
CurrentTrain: epoch  5, batch    36 | loss: 5.0927286Losses:  5.4680914878845215 0.2159721404314041
CurrentTrain: epoch  5, batch    37 | loss: 5.6840634Losses:  5.001487731933594 0.23923569917678833
CurrentTrain: epoch  6, batch     0 | loss: 5.2407236Losses:  4.857438564300537 0.2259250283241272
CurrentTrain: epoch  6, batch     1 | loss: 5.0833635Losses:  5.112678527832031 0.31094712018966675
CurrentTrain: epoch  6, batch     2 | loss: 5.4236255Losses:  5.119739532470703 0.11742424219846725
CurrentTrain: epoch  6, batch     3 | loss: 5.2371635Losses:  5.0863118171691895 0.3194130063056946
CurrentTrain: epoch  6, batch     4 | loss: 5.4057250Losses:  4.989975929260254 0.1961343139410019
CurrentTrain: epoch  6, batch     5 | loss: 5.1861100Losses:  4.994986534118652 0.19220693409442902
CurrentTrain: epoch  6, batch     6 | loss: 5.1871934Losses:  5.098122596740723 0.22602349519729614
CurrentTrain: epoch  6, batch     7 | loss: 5.3241463Losses:  4.986328601837158 0.18425048887729645
CurrentTrain: epoch  6, batch     8 | loss: 5.1705790Losses:  5.1656575202941895 0.2807772755622864
CurrentTrain: epoch  6, batch     9 | loss: 5.4464350Losses:  5.04841423034668 0.18980884552001953
CurrentTrain: epoch  6, batch    10 | loss: 5.2382231Losses:  5.36226224899292 0.3657004237174988
CurrentTrain: epoch  6, batch    11 | loss: 5.7279625Losses:  5.396832466125488 0.29670271277427673
CurrentTrain: epoch  6, batch    12 | loss: 5.6935353Losses:  5.034006118774414 0.2973976135253906
CurrentTrain: epoch  6, batch    13 | loss: 5.3314037Losses:  4.744895935058594 0.18753790855407715
CurrentTrain: epoch  6, batch    14 | loss: 4.9324341Losses:  4.970000267028809 0.1951700896024704
CurrentTrain: epoch  6, batch    15 | loss: 5.1651702Losses:  4.786277770996094 0.1711060106754303
CurrentTrain: epoch  6, batch    16 | loss: 4.9573836Losses:  4.950163841247559 0.21973267197608948
CurrentTrain: epoch  6, batch    17 | loss: 5.1698966Losses:  4.906927108764648 0.1974586546421051
CurrentTrain: epoch  6, batch    18 | loss: 5.1043859Losses:  4.905161380767822 0.2401582896709442
CurrentTrain: epoch  6, batch    19 | loss: 5.1453195Losses:  4.769696235656738 0.13599058985710144
CurrentTrain: epoch  6, batch    20 | loss: 4.9056869Losses:  5.600511074066162 0.25257471203804016
CurrentTrain: epoch  6, batch    21 | loss: 5.8530860Losses:  4.734078407287598 0.20159444212913513
CurrentTrain: epoch  6, batch    22 | loss: 4.9356728Losses:  4.862672328948975 0.20350411534309387
CurrentTrain: epoch  6, batch    23 | loss: 5.0661764Losses:  5.266082286834717 0.331123411655426
CurrentTrain: epoch  6, batch    24 | loss: 5.5972056Losses:  4.904384613037109 0.2043527364730835
CurrentTrain: epoch  6, batch    25 | loss: 5.1087375Losses:  4.821752548217773 0.17560890316963196
CurrentTrain: epoch  6, batch    26 | loss: 4.9973617Losses:  5.080209732055664 0.26338329911231995
CurrentTrain: epoch  6, batch    27 | loss: 5.3435931Losses:  4.824373245239258 0.17760516703128815
CurrentTrain: epoch  6, batch    28 | loss: 5.0019784Losses:  4.84719181060791 0.1301295906305313
CurrentTrain: epoch  6, batch    29 | loss: 4.9773216Losses:  5.078673362731934 0.21385937929153442
CurrentTrain: epoch  6, batch    30 | loss: 5.2925329Losses:  4.727890968322754 0.1655476689338684
CurrentTrain: epoch  6, batch    31 | loss: 4.8934388Losses:  5.241893768310547 0.27977633476257324
CurrentTrain: epoch  6, batch    32 | loss: 5.5216703Losses:  5.015544891357422 0.22624659538269043
CurrentTrain: epoch  6, batch    33 | loss: 5.2417917Losses:  4.83466911315918 0.16796454787254333
CurrentTrain: epoch  6, batch    34 | loss: 5.0026336Losses:  4.782824516296387 0.17335176467895508
CurrentTrain: epoch  6, batch    35 | loss: 4.9561763Losses:  5.009589672088623 0.2222229391336441
CurrentTrain: epoch  6, batch    36 | loss: 5.2318125Losses:  4.713607311248779 0.12957313656806946
CurrentTrain: epoch  6, batch    37 | loss: 4.8431807Losses:  4.880245208740234 0.15437787771224976
CurrentTrain: epoch  7, batch     0 | loss: 5.0346231Losses:  4.846250534057617 0.1856290102005005
CurrentTrain: epoch  7, batch     1 | loss: 5.0318794Losses:  4.706624984741211 0.18385830521583557
CurrentTrain: epoch  7, batch     2 | loss: 4.8904834Losses:  5.082763195037842 0.1711365431547165
CurrentTrain: epoch  7, batch     3 | loss: 5.2538996Losses:  4.755609512329102 0.12896382808685303
CurrentTrain: epoch  7, batch     4 | loss: 4.8845735Losses:  4.900679588317871 0.16873322427272797
CurrentTrain: epoch  7, batch     5 | loss: 5.0694127Losses:  4.655578136444092 0.11705377697944641
CurrentTrain: epoch  7, batch     6 | loss: 4.7726321Losses:  4.673169136047363 0.13583499193191528
CurrentTrain: epoch  7, batch     7 | loss: 4.8090043Losses:  4.665526866912842 0.16674542427062988
CurrentTrain: epoch  7, batch     8 | loss: 4.8322725Losses:  4.75300407409668 0.1653386652469635
CurrentTrain: epoch  7, batch     9 | loss: 4.9183426Losses:  4.816838264465332 0.1982497125864029
CurrentTrain: epoch  7, batch    10 | loss: 5.0150881Losses:  5.259225845336914 0.2523866295814514
CurrentTrain: epoch  7, batch    11 | loss: 5.5116124Losses:  4.708680629730225 0.14969846606254578
CurrentTrain: epoch  7, batch    12 | loss: 4.8583789Losses:  4.756758689880371 0.1694025993347168
CurrentTrain: epoch  7, batch    13 | loss: 4.9261613Losses:  5.067805290222168 0.24051640927791595
CurrentTrain: epoch  7, batch    14 | loss: 5.3083215Losses:  4.688051700592041 0.15141049027442932
CurrentTrain: epoch  7, batch    15 | loss: 4.8394623Losses:  4.719403266906738 0.15966796875
CurrentTrain: epoch  7, batch    16 | loss: 4.8790712Losses:  4.863918304443359 0.15789654850959778
CurrentTrain: epoch  7, batch    17 | loss: 5.0218148Losses:  5.252961158752441 0.2652759253978729
CurrentTrain: epoch  7, batch    18 | loss: 5.5182371Losses:  5.486324310302734 0.3390798270702362
CurrentTrain: epoch  7, batch    19 | loss: 5.8254042Losses:  5.132359027862549 0.17839168012142181
CurrentTrain: epoch  7, batch    20 | loss: 5.3107505Losses:  4.691532135009766 0.10725811123847961
CurrentTrain: epoch  7, batch    21 | loss: 4.7987905Losses:  4.699295997619629 0.10160156339406967
CurrentTrain: epoch  7, batch    22 | loss: 4.8008976Losses:  4.849099159240723 0.19306229054927826
CurrentTrain: epoch  7, batch    23 | loss: 5.0421615Losses:  4.8912482261657715 0.15273788571357727
CurrentTrain: epoch  7, batch    24 | loss: 5.0439863Losses:  4.875229835510254 0.08853752166032791
CurrentTrain: epoch  7, batch    25 | loss: 4.9637675Losses:  4.7721638679504395 0.16506385803222656
CurrentTrain: epoch  7, batch    26 | loss: 4.9372277Losses:  4.730812072753906 0.11316163092851639
CurrentTrain: epoch  7, batch    27 | loss: 4.8439736Losses:  4.814098358154297 0.07051174342632294
CurrentTrain: epoch  7, batch    28 | loss: 4.8846102Losses:  4.7735395431518555 0.2141500860452652
CurrentTrain: epoch  7, batch    29 | loss: 4.9876895Losses:  5.001461982727051 0.3242300748825073
CurrentTrain: epoch  7, batch    30 | loss: 5.3256922Losses:  5.195416450500488 0.17924878001213074
CurrentTrain: epoch  7, batch    31 | loss: 5.3746653Losses:  4.827599048614502 0.14050275087356567
CurrentTrain: epoch  7, batch    32 | loss: 4.9681020Losses:  4.709792613983154 0.13049204647541046
CurrentTrain: epoch  7, batch    33 | loss: 4.8402848Losses:  4.769150733947754 0.15426120162010193
CurrentTrain: epoch  7, batch    34 | loss: 4.9234118Losses:  4.892578125 0.2500647306442261
CurrentTrain: epoch  7, batch    35 | loss: 5.1426430Losses:  4.830653190612793 0.1395680010318756
CurrentTrain: epoch  7, batch    36 | loss: 4.9702210Losses:  4.609773635864258 0.07691014558076859
CurrentTrain: epoch  7, batch    37 | loss: 4.6866837Losses:  4.7258405685424805 0.14606595039367676
CurrentTrain: epoch  8, batch     0 | loss: 4.8719063Losses:  4.709875106811523 0.1510976105928421
CurrentTrain: epoch  8, batch     1 | loss: 4.8609729Losses:  4.730315685272217 0.10753173381090164
CurrentTrain: epoch  8, batch     2 | loss: 4.8378472Losses:  5.015499114990234 0.14762207865715027
CurrentTrain: epoch  8, batch     3 | loss: 5.1631212Losses:  4.69359016418457 0.12491483241319656
CurrentTrain: epoch  8, batch     4 | loss: 4.8185048Losses:  4.656846046447754 0.12225589156150818
CurrentTrain: epoch  8, batch     5 | loss: 4.7791018Losses:  5.158176898956299 0.2484894096851349
CurrentTrain: epoch  8, batch     6 | loss: 5.4066663Losses:  4.86287784576416 0.18984338641166687
CurrentTrain: epoch  8, batch     7 | loss: 5.0527210Losses:  4.674880027770996 0.1405084729194641
CurrentTrain: epoch  8, batch     8 | loss: 4.8153887Losses:  4.713905334472656 0.14002788066864014
CurrentTrain: epoch  8, batch     9 | loss: 4.8539333Losses:  4.998226642608643 0.20047470927238464
CurrentTrain: epoch  8, batch    10 | loss: 5.1987014Losses:  4.702513694763184 0.1622459590435028
CurrentTrain: epoch  8, batch    11 | loss: 4.8647594Losses:  4.666974067687988 0.1451396644115448
CurrentTrain: epoch  8, batch    12 | loss: 4.8121138Losses:  4.670301914215088 0.13360773026943207
CurrentTrain: epoch  8, batch    13 | loss: 4.8039098Losses:  4.676127910614014 0.14007164537906647
CurrentTrain: epoch  8, batch    14 | loss: 4.8161998Losses:  5.437973499298096 0.4336780607700348
CurrentTrain: epoch  8, batch    15 | loss: 5.8716516Losses:  4.941018104553223 0.12037155032157898
CurrentTrain: epoch  8, batch    16 | loss: 5.0613894Losses:  4.784509658813477 0.1328747272491455
CurrentTrain: epoch  8, batch    17 | loss: 4.9173841Losses:  4.674820899963379 0.14142394065856934
CurrentTrain: epoch  8, batch    18 | loss: 4.8162451Losses:  4.615286827087402 0.10822856426239014
CurrentTrain: epoch  8, batch    19 | loss: 4.7235155Losses:  4.66175651550293 0.13895787298679352
CurrentTrain: epoch  8, batch    20 | loss: 4.8007145Losses:  4.600739002227783 0.09905712306499481
CurrentTrain: epoch  8, batch    21 | loss: 4.6997962Losses:  4.819517135620117 0.14389696717262268
CurrentTrain: epoch  8, batch    22 | loss: 4.9634142Losses:  4.714940071105957 0.09799295663833618
CurrentTrain: epoch  8, batch    23 | loss: 4.8129330Losses:  4.780519485473633 0.09300943464040756
CurrentTrain: epoch  8, batch    24 | loss: 4.8735290Losses:  4.76313591003418 0.09673109650611877
CurrentTrain: epoch  8, batch    25 | loss: 4.8598671Losses:  4.757037162780762 0.17976132035255432
CurrentTrain: epoch  8, batch    26 | loss: 4.9367986Losses:  4.828152179718018 0.10498657822608948
CurrentTrain: epoch  8, batch    27 | loss: 4.9331388Losses:  4.625732421875 0.14558756351470947
CurrentTrain: epoch  8, batch    28 | loss: 4.7713199Losses:  4.581686019897461 0.07384222745895386
CurrentTrain: epoch  8, batch    29 | loss: 4.6555281Losses:  4.6059980392456055 0.09295584261417389
CurrentTrain: epoch  8, batch    30 | loss: 4.6989541Losses:  4.6553239822387695 0.14614321291446686
CurrentTrain: epoch  8, batch    31 | loss: 4.8014674Losses:  4.696491241455078 0.09312546998262405
CurrentTrain: epoch  8, batch    32 | loss: 4.7896166Losses:  4.678497791290283 0.12334106862545013
CurrentTrain: epoch  8, batch    33 | loss: 4.8018389Losses:  4.691520690917969 0.0951494500041008
CurrentTrain: epoch  8, batch    34 | loss: 4.7866702Losses:  4.660270690917969 0.13738875091075897
CurrentTrain: epoch  8, batch    35 | loss: 4.7976594Losses:  4.6740617752075195 0.13192826509475708
CurrentTrain: epoch  8, batch    36 | loss: 4.8059902Losses:  4.548984050750732 0.05750028043985367
CurrentTrain: epoch  8, batch    37 | loss: 4.6064844Losses:  4.601681709289551 0.1315852403640747
CurrentTrain: epoch  9, batch     0 | loss: 4.7332668Losses:  4.572053909301758 0.15109074115753174
CurrentTrain: epoch  9, batch     1 | loss: 4.7231445Losses:  4.61922550201416 0.1325276792049408
CurrentTrain: epoch  9, batch     2 | loss: 4.7517533Losses:  4.593287467956543 0.12771129608154297
CurrentTrain: epoch  9, batch     3 | loss: 4.7209988Losses:  4.587482452392578 0.14432667195796967
CurrentTrain: epoch  9, batch     4 | loss: 4.7318091Losses:  4.612771034240723 0.132055401802063
CurrentTrain: epoch  9, batch     5 | loss: 4.7448263Losses:  4.643608093261719 0.13733629882335663
CurrentTrain: epoch  9, batch     6 | loss: 4.7809443Losses:  4.617374420166016 0.15566657483577728
CurrentTrain: epoch  9, batch     7 | loss: 4.7730408Losses:  4.556917190551758 0.1321139633655548
CurrentTrain: epoch  9, batch     8 | loss: 4.6890311Losses:  4.635043144226074 0.07761700451374054
CurrentTrain: epoch  9, batch     9 | loss: 4.7126603Losses:  4.584194660186768 0.0902063399553299
CurrentTrain: epoch  9, batch    10 | loss: 4.6744008Losses:  4.865937232971191 0.17381060123443604
CurrentTrain: epoch  9, batch    11 | loss: 5.0397477Losses:  4.604090690612793 0.13014090061187744
CurrentTrain: epoch  9, batch    12 | loss: 4.7342315Losses:  4.566287994384766 0.13047721982002258
CurrentTrain: epoch  9, batch    13 | loss: 4.6967654Losses:  4.6178879737854 0.12460073083639145
CurrentTrain: epoch  9, batch    14 | loss: 4.7424889Losses:  4.673988342285156 0.13347256183624268
CurrentTrain: epoch  9, batch    15 | loss: 4.8074608Losses:  4.55668830871582 0.1357482522726059
CurrentTrain: epoch  9, batch    16 | loss: 4.6924367Losses:  4.598718643188477 0.11518387496471405
CurrentTrain: epoch  9, batch    17 | loss: 4.7139025Losses:  4.679953098297119 0.10467444360256195
CurrentTrain: epoch  9, batch    18 | loss: 4.7846274Losses:  5.028704643249512 0.29980626702308655
CurrentTrain: epoch  9, batch    19 | loss: 5.3285108Losses:  4.599173545837402 0.09345723688602448
CurrentTrain: epoch  9, batch    20 | loss: 4.6926308Losses:  4.60040283203125 0.130233034491539
CurrentTrain: epoch  9, batch    21 | loss: 4.7306356Losses:  4.641989231109619 0.1353706270456314
CurrentTrain: epoch  9, batch    22 | loss: 4.7773600Losses:  4.604678153991699 0.14155040681362152
CurrentTrain: epoch  9, batch    23 | loss: 4.7462287Losses:  4.615451812744141 0.11999072134494781
CurrentTrain: epoch  9, batch    24 | loss: 4.7354426Losses:  4.580000877380371 0.09765532612800598
CurrentTrain: epoch  9, batch    25 | loss: 4.6776562Losses:  4.677433967590332 0.10176071524620056
CurrentTrain: epoch  9, batch    26 | loss: 4.7791948Losses:  4.672625541687012 0.10910823941230774
CurrentTrain: epoch  9, batch    27 | loss: 4.7817340Losses:  4.8280487060546875 0.24735304713249207
CurrentTrain: epoch  9, batch    28 | loss: 5.0754018Losses:  4.641942024230957 0.12193815410137177
CurrentTrain: epoch  9, batch    29 | loss: 4.7638803Losses:  4.8269944190979 0.19673755764961243
CurrentTrain: epoch  9, batch    30 | loss: 5.0237322Losses:  4.611164093017578 0.11524690687656403
CurrentTrain: epoch  9, batch    31 | loss: 4.7264109Losses:  4.538359642028809 0.11182952672243118
CurrentTrain: epoch  9, batch    32 | loss: 4.6501894Losses:  4.52467155456543 0.10927615314722061
CurrentTrain: epoch  9, batch    33 | loss: 4.6339478Losses:  4.5623064041137695 0.12491218745708466
CurrentTrain: epoch  9, batch    34 | loss: 4.6872187Losses:  4.578734397888184 0.11948367208242416
CurrentTrain: epoch  9, batch    35 | loss: 4.6982179Losses:  4.621431350708008 0.10401815176010132
CurrentTrain: epoch  9, batch    36 | loss: 4.7254496Losses:  4.543859004974365 0.12265492975711823
CurrentTrain: epoch  9, batch    37 | loss: 4.6665139
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
cur_acc:  ['0.8693']
his_acc:  ['0.8693']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 1 0 0 0 0]
Losses:  6.886012554168701 1.4007089138031006
CurrentTrain: epoch  0, batch     0 | loss: 8.2867212Losses:  7.246652603149414 1.0691486597061157
CurrentTrain: epoch  0, batch     1 | loss: 8.3158016Losses:  3.522472381591797 1.2014915943145752
CurrentTrain: epoch  1, batch     0 | loss: 4.7239637Losses:  3.8546252250671387 0.8098151087760925
CurrentTrain: epoch  1, batch     1 | loss: 4.6644402Losses:  3.3154187202453613 0.9858371019363403
CurrentTrain: epoch  2, batch     0 | loss: 4.3012557Losses:  3.5597801208496094 1.001753807067871
CurrentTrain: epoch  2, batch     1 | loss: 4.5615339Losses:  3.2964794635772705 1.0258030891418457
CurrentTrain: epoch  3, batch     0 | loss: 4.3222828Losses:  2.818152904510498 0.780076801776886
CurrentTrain: epoch  3, batch     1 | loss: 3.5982296Losses:  3.1339948177337646 0.7441483736038208
CurrentTrain: epoch  4, batch     0 | loss: 3.8781433Losses:  2.619819164276123 0.9303907752037048
CurrentTrain: epoch  4, batch     1 | loss: 3.5502100Losses:  2.7853379249572754 0.7924792766571045
CurrentTrain: epoch  5, batch     0 | loss: 3.5778172Losses:  2.524545907974243 0.6542131900787354
CurrentTrain: epoch  5, batch     1 | loss: 3.1787591Losses:  2.5226471424102783 0.7369774580001831
CurrentTrain: epoch  6, batch     0 | loss: 3.2596245Losses:  2.534987211227417 0.6332555413246155
CurrentTrain: epoch  6, batch     1 | loss: 3.1682427Losses:  2.436063528060913 0.5722924470901489
CurrentTrain: epoch  7, batch     0 | loss: 3.0083561Losses:  2.352391481399536 0.4803042411804199
CurrentTrain: epoch  7, batch     1 | loss: 2.8326957Losses:  2.2468080520629883 0.5929080247879028
CurrentTrain: epoch  8, batch     0 | loss: 2.8397160Losses:  2.0891871452331543 0.5488835573196411
CurrentTrain: epoch  8, batch     1 | loss: 2.6380706Losses:  2.0552635192871094 0.47122517228126526
CurrentTrain: epoch  9, batch     0 | loss: 2.5264888Losses:  2.0758373737335205 0.5565540194511414
CurrentTrain: epoch  9, batch     1 | loss: 2.6323915
Losses:  4.97885274887085 0.6938912272453308
MemoryTrain:  epoch  0, batch     0 | loss: 5.6727438Losses:  1.2321054935455322 0.7042328715324402
MemoryTrain:  epoch  1, batch     0 | loss: 1.9363384Losses:  1.0915313959121704 0.7474089860916138
MemoryTrain:  epoch  2, batch     0 | loss: 1.8389404Losses:  0.7508587837219238 0.7026863098144531
MemoryTrain:  epoch  3, batch     0 | loss: 1.4535451Losses:  0.7721412181854248 0.6628642082214355
MemoryTrain:  epoch  4, batch     0 | loss: 1.4350054Losses:  0.9459773302078247 0.7219333648681641
MemoryTrain:  epoch  5, batch     0 | loss: 1.6679107Losses:  1.0607510805130005 0.7217242121696472
MemoryTrain:  epoch  6, batch     0 | loss: 1.7824752Losses:  0.975342869758606 0.6449112892150879
MemoryTrain:  epoch  7, batch     0 | loss: 1.6202542Losses:  0.7888121604919434 0.7082449793815613
MemoryTrain:  epoch  8, batch     0 | loss: 1.4970572Losses:  0.7400839924812317 0.6755757331848145
MemoryTrain:  epoch  9, batch     0 | loss: 1.4156597
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 65.00%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 83.82%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 79.22%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 79.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 79.51%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 80.32%   
cur_acc:  ['0.8693', '0.6500']
his_acc:  ['0.8693', '0.8032']
Clustering into  3  clusters
Clusters:  [0 2 2 2 2 2 1 0 2 0 2 0 1 2 2 1]
Losses:  7.752936840057373 1.2286357879638672
CurrentTrain: epoch  0, batch     0 | loss: 8.9815731Losses:  8.315110206604004 0.7670750021934509
CurrentTrain: epoch  0, batch     1 | loss: 9.0821848Losses:  5.323808670043945 1.0330121517181396
CurrentTrain: epoch  1, batch     0 | loss: 6.3568211Losses:  3.993846893310547 0.8396955728530884
CurrentTrain: epoch  1, batch     1 | loss: 4.8335423Losses:  4.663983345031738 0.9541311264038086
CurrentTrain: epoch  2, batch     0 | loss: 5.6181145Losses:  4.318434238433838 0.9274993538856506
CurrentTrain: epoch  2, batch     1 | loss: 5.2459335Losses:  4.47804069519043 0.8063158988952637
CurrentTrain: epoch  3, batch     0 | loss: 5.2843566Losses:  3.6469624042510986 0.841924786567688
CurrentTrain: epoch  3, batch     1 | loss: 4.4888873Losses:  3.36234712600708 0.7392904758453369
CurrentTrain: epoch  4, batch     0 | loss: 4.1016378Losses:  4.437968730926514 0.5616782307624817
CurrentTrain: epoch  4, batch     1 | loss: 4.9996471Losses:  3.964416027069092 0.8329099416732788
CurrentTrain: epoch  5, batch     0 | loss: 4.7973261Losses:  3.0118374824523926 0.6313038468360901
CurrentTrain: epoch  5, batch     1 | loss: 3.6431413Losses:  3.556689977645874 0.7509733438491821
CurrentTrain: epoch  6, batch     0 | loss: 4.3076634Losses:  2.9655086994171143 0.6144431233406067
CurrentTrain: epoch  6, batch     1 | loss: 3.5799518Losses:  3.2402472496032715 0.8157669305801392
CurrentTrain: epoch  7, batch     0 | loss: 4.0560141Losses:  2.4389827251434326 0.4503747224807739
CurrentTrain: epoch  7, batch     1 | loss: 2.8893576Losses:  2.521137237548828 0.6725690960884094
CurrentTrain: epoch  8, batch     0 | loss: 3.1937063Losses:  3.397519826889038 0.7192625403404236
CurrentTrain: epoch  8, batch     1 | loss: 4.1167822Losses:  2.321059465408325 0.6407347917556763
CurrentTrain: epoch  9, batch     0 | loss: 2.9617944Losses:  2.71744966506958 0.7056249976158142
CurrentTrain: epoch  9, batch     1 | loss: 3.4230747
Losses:  5.993809223175049 1.4118008613586426
MemoryTrain:  epoch  0, batch     0 | loss: 7.4056101Losses:  2.4267966747283936 1.3682806491851807
MemoryTrain:  epoch  1, batch     0 | loss: 3.7950773Losses:  2.4844698905944824 1.3576748371124268
MemoryTrain:  epoch  2, batch     0 | loss: 3.8421447Losses:  2.4728360176086426 1.3070967197418213
MemoryTrain:  epoch  3, batch     0 | loss: 3.7799327Losses:  2.286313056945801 1.323479413986206
MemoryTrain:  epoch  4, batch     0 | loss: 3.6097925Losses:  2.195375919342041 1.2526037693023682
MemoryTrain:  epoch  5, batch     0 | loss: 3.4479797Losses:  2.028229236602783 1.2744145393371582
MemoryTrain:  epoch  6, batch     0 | loss: 3.3026438Losses:  1.9263232946395874 1.3310356140136719
MemoryTrain:  epoch  7, batch     0 | loss: 3.2573590Losses:  2.082843542098999 1.2689603567123413
MemoryTrain:  epoch  8, batch     0 | loss: 3.3518038Losses:  2.2400498390197754 1.3703300952911377
MemoryTrain:  epoch  9, batch     0 | loss: 3.6103799
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 50.45%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 82.46%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.92%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 76.98%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 77.53%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 75.43%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 75.56%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 74.59%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 73.94%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 72.25%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 71.39%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 70.64%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 70.37%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 70.76%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 70.26%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 70.13%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 68.44%   
cur_acc:  ['0.8693', '0.6500', '0.5045']
his_acc:  ['0.8693', '0.8032', '0.6844']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0]
Losses:  6.444236755371094 1.2279868125915527
CurrentTrain: epoch  0, batch     0 | loss: 7.6722236Losses:  6.453657150268555 0.8452544808387756
CurrentTrain: epoch  0, batch     1 | loss: 7.2989116Losses:  3.532097339630127 0.9852917790412903
CurrentTrain: epoch  1, batch     0 | loss: 4.5173893Losses:  2.9060473442077637 0.7999964952468872
CurrentTrain: epoch  1, batch     1 | loss: 3.7060437Losses:  2.838362455368042 0.8515250086784363
CurrentTrain: epoch  2, batch     0 | loss: 3.6898875Losses:  3.183659076690674 0.7751825451850891
CurrentTrain: epoch  2, batch     1 | loss: 3.9588416Losses:  2.6933979988098145 0.7614334225654602
CurrentTrain: epoch  3, batch     0 | loss: 3.4548314Losses:  2.4832751750946045 0.6265599727630615
CurrentTrain: epoch  3, batch     1 | loss: 3.1098351Losses:  2.5853726863861084 0.6417655944824219
CurrentTrain: epoch  4, batch     0 | loss: 3.2271383Losses:  2.389219045639038 0.4966658651828766
CurrentTrain: epoch  4, batch     1 | loss: 2.8858850Losses:  2.4617133140563965 0.5357651710510254
CurrentTrain: epoch  5, batch     0 | loss: 2.9974785Losses:  2.0329487323760986 0.3662501573562622
CurrentTrain: epoch  5, batch     1 | loss: 2.3991990Losses:  2.1582155227661133 0.5666784048080444
CurrentTrain: epoch  6, batch     0 | loss: 2.7248940Losses:  2.0507712364196777 0.33943256735801697
CurrentTrain: epoch  6, batch     1 | loss: 2.3902037Losses:  2.0644023418426514 0.5161684155464172
CurrentTrain: epoch  7, batch     0 | loss: 2.5805707Losses:  2.0194852352142334 0.45025476813316345
CurrentTrain: epoch  7, batch     1 | loss: 2.4697399Losses:  2.0041348934173584 0.44214239716529846
CurrentTrain: epoch  8, batch     0 | loss: 2.4462774Losses:  1.9404244422912598 0.4180331826210022
CurrentTrain: epoch  8, batch     1 | loss: 2.3584576Losses:  1.9725937843322754 0.4930189251899719
CurrentTrain: epoch  9, batch     0 | loss: 2.4656126Losses:  1.8696839809417725 0.3160482943058014
CurrentTrain: epoch  9, batch     1 | loss: 2.1857324
Losses:  6.3528313636779785 1.3227651119232178
MemoryTrain:  epoch  0, batch     0 | loss: 7.6755962Losses:  12.63296890258789 0.36660000681877136
MemoryTrain:  epoch  0, batch     1 | loss: 12.9995689Losses:  2.3919873237609863 1.2889423370361328
MemoryTrain:  epoch  1, batch     0 | loss: 3.6809297Losses:  3.654042959213257 0.5062693953514099
MemoryTrain:  epoch  1, batch     1 | loss: 4.1603122Losses:  2.5259451866149902 1.2894001007080078
MemoryTrain:  epoch  2, batch     0 | loss: 3.8153453Losses:  2.668689250946045 0.9218922853469849
MemoryTrain:  epoch  2, batch     1 | loss: 3.5905814Losses:  2.728325843811035 1.2337878942489624
MemoryTrain:  epoch  3, batch     0 | loss: 3.9621139Losses:  2.684692859649658 0.5706855058670044
MemoryTrain:  epoch  3, batch     1 | loss: 3.2553782Losses:  2.972623586654663 1.3808064460754395
MemoryTrain:  epoch  4, batch     0 | loss: 4.3534298Losses:  1.9452511072158813 0.4046189486980438
MemoryTrain:  epoch  4, batch     1 | loss: 2.3498700Losses:  2.5570077896118164 1.3653302192687988
MemoryTrain:  epoch  5, batch     0 | loss: 3.9223380Losses:  1.7698942422866821 0.572619616985321
MemoryTrain:  epoch  5, batch     1 | loss: 2.3425138Losses:  2.5859744548797607 1.2861597537994385
MemoryTrain:  epoch  6, batch     0 | loss: 3.8721342Losses:  1.896722435951233 0.4551876485347748
MemoryTrain:  epoch  6, batch     1 | loss: 2.3519101Losses:  2.3615071773529053 1.1566073894500732
MemoryTrain:  epoch  7, batch     0 | loss: 3.5181146Losses:  2.6777749061584473 0.8627787828445435
MemoryTrain:  epoch  7, batch     1 | loss: 3.5405536Losses:  2.3618226051330566 1.1569466590881348
MemoryTrain:  epoch  8, batch     0 | loss: 3.5187693Losses:  2.297820806503296 0.4970654547214508
MemoryTrain:  epoch  8, batch     1 | loss: 2.7948864Losses:  2.5202040672302246 1.231403112411499
MemoryTrain:  epoch  9, batch     0 | loss: 3.7516072Losses:  1.266002893447876 0.7711994051933289
MemoryTrain:  epoch  9, batch     1 | loss: 2.0372024
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 18.75%,  total acc: 77.78%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 77.46%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 72.05%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 66.85%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 64.67%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 63.11%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 62.98%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 61.91%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 61.34%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 61.14%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 60.71%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 60.42%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 59.64%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 59.06%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 58.81%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 59.17%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 59.52%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 59.77%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 59.94%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 60.26%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 60.66%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 60.69%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 60.62%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 60.65%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 60.85%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 61.39%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 61.91%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 63.46%   
cur_acc:  ['0.8693', '0.6500', '0.5045', '0.7778']
his_acc:  ['0.8693', '0.8032', '0.6844', '0.6346']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0]
Losses:  8.059057235717773 1.487323522567749
CurrentTrain: epoch  0, batch     0 | loss: 9.5463810Losses:  9.618494987487793 1.2146742343902588
CurrentTrain: epoch  0, batch     1 | loss: 10.8331690Losses:  5.572136878967285 1.225230097770691
CurrentTrain: epoch  1, batch     0 | loss: 6.7973671Losses:  5.869513034820557 1.2519887685775757
CurrentTrain: epoch  1, batch     1 | loss: 7.1215019Losses:  5.956364631652832 1.3405694961547852
CurrentTrain: epoch  2, batch     0 | loss: 7.2969341Losses:  4.2176737785339355 0.9719499945640564
CurrentTrain: epoch  2, batch     1 | loss: 5.1896238Losses:  4.84797477722168 1.2075023651123047
CurrentTrain: epoch  3, batch     0 | loss: 6.0554771Losses:  4.478540897369385 1.1193944215774536
CurrentTrain: epoch  3, batch     1 | loss: 5.5979352Losses:  4.932878494262695 1.1589627265930176
CurrentTrain: epoch  4, batch     0 | loss: 6.0918412Losses:  3.5856831073760986 0.682728111743927
CurrentTrain: epoch  4, batch     1 | loss: 4.2684112Losses:  4.393145561218262 1.0333478450775146
CurrentTrain: epoch  5, batch     0 | loss: 5.4264936Losses:  3.969965696334839 1.021241307258606
CurrentTrain: epoch  5, batch     1 | loss: 4.9912071Losses:  4.275130748748779 0.9930959939956665
CurrentTrain: epoch  6, batch     0 | loss: 5.2682266Losses:  3.511153221130371 0.6381391882896423
CurrentTrain: epoch  6, batch     1 | loss: 4.1492925Losses:  3.422640800476074 0.8458732962608337
CurrentTrain: epoch  7, batch     0 | loss: 4.2685142Losses:  4.384954452514648 0.8830966353416443
CurrentTrain: epoch  7, batch     1 | loss: 5.2680511Losses:  3.44557523727417 0.8804259300231934
CurrentTrain: epoch  8, batch     0 | loss: 4.3260012Losses:  3.9462482929229736 0.8936798572540283
CurrentTrain: epoch  8, batch     1 | loss: 4.8399282Losses:  3.683690309524536 0.9218265414237976
CurrentTrain: epoch  9, batch     0 | loss: 4.6055169Losses:  2.8567497730255127 0.7433169484138489
CurrentTrain: epoch  9, batch     1 | loss: 3.6000667
Losses:  6.75141716003418 1.5404314994812012
MemoryTrain:  epoch  0, batch     0 | loss: 8.2918491Losses:  12.140389442443848 0.9009130597114563
MemoryTrain:  epoch  0, batch     1 | loss: 13.0413027Losses:  4.01155424118042 1.5365052223205566
MemoryTrain:  epoch  1, batch     0 | loss: 5.5480595Losses:  2.1226508617401123 0.9047182202339172
MemoryTrain:  epoch  1, batch     1 | loss: 3.0273690Losses:  2.6754794120788574 1.323798656463623
MemoryTrain:  epoch  2, batch     0 | loss: 3.9992781Losses:  3.585834503173828 1.2072014808654785
MemoryTrain:  epoch  2, batch     1 | loss: 4.7930360Losses:  3.12156343460083 1.300113558769226
MemoryTrain:  epoch  3, batch     0 | loss: 4.4216771Losses:  2.764434814453125 1.1007901430130005
MemoryTrain:  epoch  3, batch     1 | loss: 3.8652248Losses:  3.531736373901367 1.5101044178009033
MemoryTrain:  epoch  4, batch     0 | loss: 5.0418406Losses:  2.391853094100952 0.8374495506286621
MemoryTrain:  epoch  4, batch     1 | loss: 3.2293026Losses:  3.220478057861328 1.2851049900054932
MemoryTrain:  epoch  5, batch     0 | loss: 4.5055828Losses:  2.7197022438049316 1.1339462995529175
MemoryTrain:  epoch  5, batch     1 | loss: 3.8536487Losses:  3.339411497116089 1.5330069065093994
MemoryTrain:  epoch  6, batch     0 | loss: 4.8724184Losses:  2.0000007152557373 0.7672237157821655
MemoryTrain:  epoch  6, batch     1 | loss: 2.7672243Losses:  2.6326687335968018 1.3800816535949707
MemoryTrain:  epoch  7, batch     0 | loss: 4.0127506Losses:  3.200875997543335 0.9447739720344543
MemoryTrain:  epoch  7, batch     1 | loss: 4.1456499Losses:  2.952423095703125 1.370978832244873
MemoryTrain:  epoch  8, batch     0 | loss: 4.3234019Losses:  2.6580443382263184 0.7539181709289551
MemoryTrain:  epoch  8, batch     1 | loss: 3.4119625Losses:  2.184244394302368 1.36328125
MemoryTrain:  epoch  9, batch     0 | loss: 3.5475256Losses:  3.8211731910705566 0.9241521954536438
MemoryTrain:  epoch  9, batch     1 | loss: 4.7453256
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 22.92%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 25.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 29.55%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 30.73%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 32.21%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 37.05%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 41.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 44.92%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 47.43%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 49.31%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 48.36%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 48.44%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 48.51%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 48.01%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 76.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 74.26%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 69.57%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 69.48%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 68.04%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 66.94%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 64.23%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 63.14%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 62.25%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 61.40%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 60.70%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 59.55%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 59.09%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 59.26%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 59.21%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.16%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 58.90%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 58.65%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 57.99%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 58.27%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 58.83%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 59.08%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 59.04%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 59.00%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 58.96%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 59.56%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 59.69%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 59.64%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 59.60%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 59.81%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 60.36%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 60.90%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 61.92%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 62.42%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 62.66%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 62.26%   [EVAL] batch:   79 | acc: 37.50%,  total acc: 61.95%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 61.34%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 60.67%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 60.02%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 59.45%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 59.12%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 58.79%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 58.62%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 58.52%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 58.57%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 58.54%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 58.52%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 59.41%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 60.07%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 60.22%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 59.92%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 59.76%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 59.79%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 59.38%   
cur_acc:  ['0.8693', '0.6500', '0.5045', '0.7778', '0.4801']
his_acc:  ['0.8693', '0.8032', '0.6844', '0.6346', '0.5938']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0]
Losses:  6.88720703125 1.159042477607727
CurrentTrain: epoch  0, batch     0 | loss: 8.0462494Losses:  7.903567790985107 1.085438847541809
CurrentTrain: epoch  0, batch     1 | loss: 8.9890070Losses:  4.809412956237793 1.107739806175232
CurrentTrain: epoch  1, batch     0 | loss: 5.9171529Losses:  3.696467876434326 0.7122898101806641
CurrentTrain: epoch  1, batch     1 | loss: 4.4087577Losses:  4.437349319458008 1.0384941101074219
CurrentTrain: epoch  2, batch     0 | loss: 5.4758434Losses:  3.3413445949554443 0.791450023651123
CurrentTrain: epoch  2, batch     1 | loss: 4.1327944Losses:  3.0564279556274414 0.8355200886726379
CurrentTrain: epoch  3, batch     0 | loss: 3.8919480Losses:  4.761100769042969 1.1246566772460938
CurrentTrain: epoch  3, batch     1 | loss: 5.8857574Losses:  3.169940948486328 0.8075457811355591
CurrentTrain: epoch  4, batch     0 | loss: 3.9774866Losses:  3.8906941413879395 0.8618583083152771
CurrentTrain: epoch  4, batch     1 | loss: 4.7525525Losses:  2.933347225189209 0.8825117349624634
CurrentTrain: epoch  5, batch     0 | loss: 3.8158588Losses:  3.5812716484069824 0.8034970164299011
CurrentTrain: epoch  5, batch     1 | loss: 4.3847685Losses:  2.956282138824463 0.734003484249115
CurrentTrain: epoch  6, batch     0 | loss: 3.6902857Losses:  2.5693397521972656 0.8117207288742065
CurrentTrain: epoch  6, batch     1 | loss: 3.3810606Losses:  2.371461868286133 0.7962846159934998
CurrentTrain: epoch  7, batch     0 | loss: 3.1677465Losses:  3.010723352432251 0.556198239326477
CurrentTrain: epoch  7, batch     1 | loss: 3.5669217Losses:  2.4556283950805664 0.7081187963485718
CurrentTrain: epoch  8, batch     0 | loss: 3.1637473Losses:  2.564934015274048 0.779241681098938
CurrentTrain: epoch  8, batch     1 | loss: 3.3441758Losses:  2.393176317214966 0.6831077337265015
CurrentTrain: epoch  9, batch     0 | loss: 3.0762839Losses:  2.4636611938476562 0.7667940258979797
CurrentTrain: epoch  9, batch     1 | loss: 3.2304552
Losses:  6.7664594650268555 1.5421679019927979
MemoryTrain:  epoch  0, batch     0 | loss: 8.3086271Losses:  11.728389739990234 1.3059629201889038
MemoryTrain:  epoch  0, batch     1 | loss: 13.0343523Losses:  2.8819503784179688 1.4836063385009766
MemoryTrain:  epoch  1, batch     0 | loss: 4.3655567Losses:  3.057342290878296 1.375685214996338
MemoryTrain:  epoch  1, batch     1 | loss: 4.4330273Losses:  3.059504508972168 1.4723509550094604
MemoryTrain:  epoch  2, batch     0 | loss: 4.5318556Losses:  3.019542694091797 1.2252177000045776
MemoryTrain:  epoch  2, batch     1 | loss: 4.2447605Losses:  2.6108829975128174 1.2925890684127808
MemoryTrain:  epoch  3, batch     0 | loss: 3.9034719Losses:  3.3947997093200684 1.376367211341858
MemoryTrain:  epoch  3, batch     1 | loss: 4.7711668Losses:  3.198502540588379 1.3620935678482056
MemoryTrain:  epoch  4, batch     0 | loss: 4.5605960Losses:  3.0287842750549316 1.4092812538146973
MemoryTrain:  epoch  4, batch     1 | loss: 4.4380655Losses:  2.6287713050842285 1.492424726486206
MemoryTrain:  epoch  5, batch     0 | loss: 4.1211958Losses:  3.1221258640289307 1.2224282026290894
MemoryTrain:  epoch  5, batch     1 | loss: 4.3445539Losses:  2.504425048828125 1.2786266803741455
MemoryTrain:  epoch  6, batch     0 | loss: 3.7830517Losses:  2.911558151245117 1.378493309020996
MemoryTrain:  epoch  6, batch     1 | loss: 4.2900515Losses:  3.039787530899048 1.370831847190857
MemoryTrain:  epoch  7, batch     0 | loss: 4.4106193Losses:  2.414763927459717 1.2982991933822632
MemoryTrain:  epoch  7, batch     1 | loss: 3.7130632Losses:  2.970433473587036 1.4652059078216553
MemoryTrain:  epoch  8, batch     0 | loss: 4.4356394Losses:  2.6333723068237305 1.079493761062622
MemoryTrain:  epoch  8, batch     1 | loss: 3.7128661Losses:  2.980881690979004 1.405318021774292
MemoryTrain:  epoch  9, batch     0 | loss: 4.3862000Losses:  2.69868803024292 1.182661533355713
MemoryTrain:  epoch  9, batch     1 | loss: 3.8813496
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.94%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 10.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 53.95%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 68.35%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 64.46%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 61.66%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 61.51%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 62.94%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 61.25%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 60.19%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 59.18%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 58.72%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 58.29%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 57.38%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 56.62%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 55.89%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 54.83%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 54.51%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 54.66%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 54.91%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 54.82%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 54.85%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 54.66%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 54.37%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 53.79%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 53.93%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 54.07%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 54.10%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 53.94%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 53.88%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 53.92%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 54.60%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 54.53%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 54.46%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 54.49%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 54.69%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 55.31%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 55.91%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 57.63%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 57.85%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 57.12%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 56.48%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 55.79%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 55.11%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 54.52%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 53.94%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 53.46%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 53.12%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 53.02%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 52.70%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 52.60%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 52.43%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 52.61%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 53.06%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 53.43%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 53.86%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 54.14%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 54.43%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 54.38%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 54.40%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 54.61%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 54.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 55.26%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 55.45%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 55.04%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 54.63%   [EVAL] batch:  104 | acc: 18.75%,  total acc: 54.29%   [EVAL] batch:  105 | acc: 12.50%,  total acc: 53.89%   [EVAL] batch:  106 | acc: 0.00%,  total acc: 53.39%   
cur_acc:  ['0.8693', '0.6500', '0.5045', '0.7778', '0.4801', '0.3594']
his_acc:  ['0.8693', '0.8032', '0.6844', '0.6346', '0.5938', '0.5339']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0]
Losses:  5.942652225494385 0.8643062114715576
CurrentTrain: epoch  0, batch     0 | loss: 6.8069582Losses:  6.327579021453857 0.7646070122718811
CurrentTrain: epoch  0, batch     1 | loss: 7.0921860Losses:  2.397088050842285 0.7859108448028564
CurrentTrain: epoch  1, batch     0 | loss: 3.1829989Losses:  2.515841007232666 0.7504453659057617
CurrentTrain: epoch  1, batch     1 | loss: 3.2662864Losses:  2.3544626235961914 0.7679734230041504
CurrentTrain: epoch  2, batch     0 | loss: 3.1224360Losses:  2.0668532848358154 0.4504852294921875
CurrentTrain: epoch  2, batch     1 | loss: 2.5173385Losses:  2.087470769882202 0.6097901463508606
CurrentTrain: epoch  3, batch     0 | loss: 2.6972609Losses:  1.9799681901931763 0.4975433349609375
CurrentTrain: epoch  3, batch     1 | loss: 2.4775114Losses:  2.0305004119873047 0.5731883645057678
CurrentTrain: epoch  4, batch     0 | loss: 2.6036887Losses:  1.91681969165802 0.4220855236053467
CurrentTrain: epoch  4, batch     1 | loss: 2.3389053Losses:  1.95166015625 0.47818732261657715
CurrentTrain: epoch  5, batch     0 | loss: 2.4298475Losses:  2.6954896450042725 0.5348207950592041
CurrentTrain: epoch  5, batch     1 | loss: 3.2303104Losses:  2.1647725105285645 0.41317179799079895
CurrentTrain: epoch  6, batch     0 | loss: 2.5779443Losses:  2.4211273193359375 0.394854873418808
CurrentTrain: epoch  6, batch     1 | loss: 2.8159821Losses:  2.1928231716156006 0.3843173384666443
CurrentTrain: epoch  7, batch     0 | loss: 2.5771406Losses:  2.047773599624634 0.22869722545146942
CurrentTrain: epoch  7, batch     1 | loss: 2.2764709Losses:  2.3676276206970215 0.3944540023803711
CurrentTrain: epoch  8, batch     0 | loss: 2.7620816Losses:  1.9586743116378784 0.3233133852481842
CurrentTrain: epoch  8, batch     1 | loss: 2.2819877Losses:  1.9339213371276855 0.32967740297317505
CurrentTrain: epoch  9, batch     0 | loss: 2.2635987Losses:  1.777773380279541 0.2431316077709198
CurrentTrain: epoch  9, batch     1 | loss: 2.0209050
Losses:  6.362805366516113 1.2411620616912842
MemoryTrain:  epoch  0, batch     0 | loss: 7.6039677Losses:  11.055878639221191 1.491809606552124
MemoryTrain:  epoch  0, batch     1 | loss: 12.5476885Losses:  12.802985191345215 0.2207028865814209
MemoryTrain:  epoch  0, batch     2 | loss: 13.0236883Losses:  2.585523843765259 1.1230723857879639
MemoryTrain:  epoch  1, batch     0 | loss: 3.7085962Losses:  3.754948139190674 1.4581406116485596
MemoryTrain:  epoch  1, batch     1 | loss: 5.2130890Losses:  2.988454818725586 0.394225537776947
MemoryTrain:  epoch  1, batch     2 | loss: 3.3826804Losses:  3.4165306091308594 1.2907240390777588
MemoryTrain:  epoch  2, batch     0 | loss: 4.7072544Losses:  3.008181571960449 1.315567135810852
MemoryTrain:  epoch  2, batch     1 | loss: 4.3237486Losses:  1.6319568157196045 0.3603178858757019
MemoryTrain:  epoch  2, batch     2 | loss: 1.9922748Losses:  2.8840670585632324 1.129024863243103
MemoryTrain:  epoch  3, batch     0 | loss: 4.0130920Losses:  3.399500608444214 1.4980388879776
MemoryTrain:  epoch  3, batch     1 | loss: 4.8975396Losses:  2.512674331665039 0.38631105422973633
MemoryTrain:  epoch  3, batch     2 | loss: 2.8989854Losses:  2.7632598876953125 1.2179762125015259
MemoryTrain:  epoch  4, batch     0 | loss: 3.9812360Losses:  3.290276050567627 1.4116573333740234
MemoryTrain:  epoch  4, batch     1 | loss: 4.7019334Losses:  2.404568672180176 0.27822554111480713
MemoryTrain:  epoch  4, batch     2 | loss: 2.6827941Losses:  3.200570583343506 1.3870830535888672
MemoryTrain:  epoch  5, batch     0 | loss: 4.5876536Losses:  3.058821201324463 1.390960454940796
MemoryTrain:  epoch  5, batch     1 | loss: 4.4497814Losses:  1.5966625213623047 0.2541502118110657
MemoryTrain:  epoch  5, batch     2 | loss: 1.8508127Losses:  2.941293716430664 1.3463841676712036
MemoryTrain:  epoch  6, batch     0 | loss: 4.2876778Losses:  2.921553134918213 1.1716126203536987
MemoryTrain:  epoch  6, batch     1 | loss: 4.0931659Losses:  2.049391031265259 0.5992027521133423
MemoryTrain:  epoch  6, batch     2 | loss: 2.6485939Losses:  3.4584479331970215 1.359931230545044
MemoryTrain:  epoch  7, batch     0 | loss: 4.8183794Losses:  2.8142027854919434 1.2685894966125488
MemoryTrain:  epoch  7, batch     1 | loss: 4.0827923Losses:  1.6049787998199463 0.39810803532600403
MemoryTrain:  epoch  7, batch     2 | loss: 2.0030868Losses:  2.6420822143554688 1.2377076148986816
MemoryTrain:  epoch  8, batch     0 | loss: 3.8797898Losses:  2.7355422973632812 1.2647076845169067
MemoryTrain:  epoch  8, batch     1 | loss: 4.0002499Losses:  4.773957252502441 0.33193570375442505
MemoryTrain:  epoch  8, batch     2 | loss: 5.1058931Losses:  2.796485424041748 1.3471055030822754
MemoryTrain:  epoch  9, batch     0 | loss: 4.1435909Losses:  2.853494644165039 1.230761170387268
MemoryTrain:  epoch  9, batch     1 | loss: 4.0842557Losses:  3.4759631156921387 0.2783365547657013
MemoryTrain:  epoch  9, batch     2 | loss: 3.7542996
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 57.21%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 52.40%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 52.23%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 52.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.86%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 6.25%,  total acc: 60.04%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 58.46%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 57.14%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 55.73%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 54.39%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 54.28%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 54.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 56.09%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 56.86%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 57.89%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 56.69%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 55.68%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 54.86%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 53.67%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 52.66%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 52.08%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 51.53%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 50.62%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 49.88%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 49.04%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 48.11%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 47.69%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 48.07%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 48.33%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 48.25%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 48.17%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 47.99%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 47.81%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 47.23%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 46.47%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 46.13%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 45.70%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 45.48%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 44.98%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 44.59%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 45.40%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 45.56%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 45.45%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 45.33%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 45.66%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 46.40%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 47.13%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 47.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 48.52%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 49.19%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 49.52%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 48.89%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 48.36%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 47.76%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 47.18%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 46.61%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 46.13%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 45.88%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 45.57%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 45.47%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 45.17%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 45.08%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 45.00%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 45.05%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 45.45%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 45.70%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 45.94%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 46.18%   [EVAL] batch:   95 | acc: 56.25%,  total acc: 46.29%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 46.07%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 45.92%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 45.83%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 46.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 46.72%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 47.00%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 46.66%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 46.33%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 45.95%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 45.52%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 45.27%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 45.49%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 45.58%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 45.68%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 46.11%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 46.54%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 46.79%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 46.77%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 46.52%   [EVAL] batch:  115 | acc: 12.50%,  total acc: 46.23%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 46.21%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 46.50%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 46.80%   
cur_acc:  ['0.8693', '0.6500', '0.5045', '0.7778', '0.4801', '0.3594', '0.5721']
his_acc:  ['0.8693', '0.8032', '0.6844', '0.6346', '0.5938', '0.5339', '0.4680']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0 0
 0 1 0 0]
Losses:  5.628806114196777 0.7954704761505127
CurrentTrain: epoch  0, batch     0 | loss: 6.4242764Losses:  7.040207862854004 0.5300238728523254
CurrentTrain: epoch  0, batch     1 | loss: 7.5702319Losses:  2.8669981956481934 0.6933567523956299
CurrentTrain: epoch  1, batch     0 | loss: 3.5603549Losses:  2.6183865070343018 0.6740623712539673
CurrentTrain: epoch  1, batch     1 | loss: 3.2924490Losses:  2.434952735900879 0.6851330995559692
CurrentTrain: epoch  2, batch     0 | loss: 3.1200857Losses:  2.3158771991729736 0.6342834830284119
CurrentTrain: epoch  2, batch     1 | loss: 2.9501607Losses:  2.1877408027648926 0.643049955368042
CurrentTrain: epoch  3, batch     0 | loss: 2.8307908Losses:  2.0905771255493164 0.5355054140090942
CurrentTrain: epoch  3, batch     1 | loss: 2.6260824Losses:  2.0329182147979736 0.5067868828773499
CurrentTrain: epoch  4, batch     0 | loss: 2.5397050Losses:  1.9905763864517212 0.5985267758369446
CurrentTrain: epoch  4, batch     1 | loss: 2.5891032Losses:  1.9546408653259277 0.5374202728271484
CurrentTrain: epoch  5, batch     0 | loss: 2.4920611Losses:  1.8661577701568604 0.43951717019081116
CurrentTrain: epoch  5, batch     1 | loss: 2.3056750Losses:  1.8439199924468994 0.42219558358192444
CurrentTrain: epoch  6, batch     0 | loss: 2.2661157Losses:  1.8476600646972656 0.42803511023521423
CurrentTrain: epoch  6, batch     1 | loss: 2.2756951Losses:  1.8050742149353027 0.3974730968475342
CurrentTrain: epoch  7, batch     0 | loss: 2.2025473Losses:  1.7860512733459473 0.24502110481262207
CurrentTrain: epoch  7, batch     1 | loss: 2.0310724Losses:  1.7622642517089844 0.3890952467918396
CurrentTrain: epoch  8, batch     0 | loss: 2.1513596Losses:  1.7383352518081665 0.4215260148048401
CurrentTrain: epoch  8, batch     1 | loss: 2.1598613Losses:  1.716750144958496 0.3612302541732788
CurrentTrain: epoch  9, batch     0 | loss: 2.0779805Losses:  1.7122397422790527 0.2980751097202301
CurrentTrain: epoch  9, batch     1 | loss: 2.0103149
Losses:  7.0704522132873535 1.381829023361206
MemoryTrain:  epoch  0, batch     0 | loss: 8.4522810Losses:  11.810529708862305 1.356762409210205
MemoryTrain:  epoch  0, batch     1 | loss: 13.1672916Losses:  13.018741607666016 0.7448962330818176
MemoryTrain:  epoch  0, batch     2 | loss: 13.7636375Losses:  3.497974157333374 1.2885465621948242
MemoryTrain:  epoch  1, batch     0 | loss: 4.7865210Losses:  3.784696578979492 1.2458508014678955
MemoryTrain:  epoch  1, batch     1 | loss: 5.0305471Losses:  2.8782200813293457 0.9665277600288391
MemoryTrain:  epoch  1, batch     2 | loss: 3.8447478Losses:  3.281400680541992 1.3130618333816528
MemoryTrain:  epoch  2, batch     0 | loss: 4.5944624Losses:  3.85530161857605 1.2722320556640625
MemoryTrain:  epoch  2, batch     1 | loss: 5.1275339Losses:  3.233813524246216 0.9661005139350891
MemoryTrain:  epoch  2, batch     2 | loss: 4.1999140Losses:  3.047534704208374 1.501827359199524
MemoryTrain:  epoch  3, batch     0 | loss: 4.5493622Losses:  3.60709285736084 1.2464932203292847
MemoryTrain:  epoch  3, batch     1 | loss: 4.8535862Losses:  3.348191261291504 0.8440536856651306
MemoryTrain:  epoch  3, batch     2 | loss: 4.1922450Losses:  3.5145606994628906 1.1854068040847778
MemoryTrain:  epoch  4, batch     0 | loss: 4.6999674Losses:  3.0739173889160156 1.1585922241210938
MemoryTrain:  epoch  4, batch     1 | loss: 4.2325096Losses:  4.062699317932129 0.9471157789230347
MemoryTrain:  epoch  4, batch     2 | loss: 5.0098152Losses:  3.5808162689208984 1.314344882965088
MemoryTrain:  epoch  5, batch     0 | loss: 4.8951612Losses:  3.052945852279663 1.420522928237915
MemoryTrain:  epoch  5, batch     1 | loss: 4.4734688Losses:  3.6445999145507812 0.907200813293457
MemoryTrain:  epoch  5, batch     2 | loss: 4.5518007Losses:  3.436150312423706 1.3000545501708984
MemoryTrain:  epoch  6, batch     0 | loss: 4.7362051Losses:  3.160834550857544 1.2434053421020508
MemoryTrain:  epoch  6, batch     1 | loss: 4.4042397Losses:  3.7112114429473877 0.9655545353889465
MemoryTrain:  epoch  6, batch     2 | loss: 4.6767659Losses:  3.5435574054718018 1.214646339416504
MemoryTrain:  epoch  7, batch     0 | loss: 4.7582035Losses:  2.726611614227295 1.310951590538025
MemoryTrain:  epoch  7, batch     1 | loss: 4.0375633Losses:  3.653961658477783 0.9825984239578247
MemoryTrain:  epoch  7, batch     2 | loss: 4.6365600Losses:  2.750199556350708 1.1959505081176758
MemoryTrain:  epoch  8, batch     0 | loss: 3.9461501Losses:  3.6397223472595215 1.188994288444519
MemoryTrain:  epoch  8, batch     1 | loss: 4.8287168Losses:  3.2706480026245117 0.8466725945472717
MemoryTrain:  epoch  8, batch     2 | loss: 4.1173205Losses:  2.8279433250427246 1.3117053508758545
MemoryTrain:  epoch  9, batch     0 | loss: 4.1396484Losses:  3.25942325592041 1.2782645225524902
MemoryTrain:  epoch  9, batch     1 | loss: 4.5376878Losses:  3.509615898132324 0.7289825081825256
MemoryTrain:  epoch  9, batch     2 | loss: 4.2385983
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 80.80%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 7.81%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 26.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 34.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 52.63%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 62.72%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:   31 | acc: 43.75%,  total acc: 61.52%   [EVAL] batch:   32 | acc: 18.75%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 58.46%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 55.56%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 54.22%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 54.28%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 54.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.94%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 56.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 56.40%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 55.26%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 54.31%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 53.12%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 52.13%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 51.56%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 51.02%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 50.12%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 49.39%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 48.56%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 47.64%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 47.22%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 47.27%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 47.54%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 47.37%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 46.98%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 46.72%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 46.35%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 45.70%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 44.96%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 44.35%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 43.65%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 43.17%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 42.52%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 42.16%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 43.01%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 43.21%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 43.12%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 43.31%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 43.66%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 44.43%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 45.19%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 45.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 46.63%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 47.32%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 47.68%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 47.15%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 46.64%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 46.06%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 45.50%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 44.95%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 44.42%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 43.97%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 43.53%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 43.03%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 42.54%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 42.21%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 42.01%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 41.83%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 42.26%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 42.47%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 42.69%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 42.96%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 43.03%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 42.85%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 42.73%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 42.68%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 43.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 43.63%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 43.69%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 43.39%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 43.09%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 42.80%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 42.39%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 42.17%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 42.30%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 42.49%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 42.50%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 42.96%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 43.42%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 43.59%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 43.53%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 43.64%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 44.01%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 44.38%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 44.64%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 44.99%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 45.44%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 45.88%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 46.32%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 46.75%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 47.17%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 47.59%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 47.75%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 47.72%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 47.69%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 47.90%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 48.15%   [EVAL] batch:  132 | acc: 43.75%,  total acc: 48.12%   
cur_acc:  ['0.8693', '0.6500', '0.5045', '0.7778', '0.4801', '0.3594', '0.5721', '0.8080']
his_acc:  ['0.8693', '0.8032', '0.6844', '0.6346', '0.5938', '0.5339', '0.4680', '0.4812']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.211234092712402 1.7084617614746094
CurrentTrain: epoch  0, batch     0 | loss: 13.9196959Losses:  14.5792236328125 1.6901048421859741
CurrentTrain: epoch  0, batch     1 | loss: 16.2693291Losses:  15.08927059173584 1.6444356441497803
CurrentTrain: epoch  0, batch     2 | loss: 16.7337055Losses:  14.998939514160156 1.8157745599746704
CurrentTrain: epoch  0, batch     3 | loss: 16.8147144Losses:  14.828505516052246 1.8496438264846802
CurrentTrain: epoch  0, batch     4 | loss: 16.6781502Losses:  14.450037956237793 1.7259345054626465
CurrentTrain: epoch  0, batch     5 | loss: 16.1759720Losses:  14.117124557495117 1.4077744483947754
CurrentTrain: epoch  0, batch     6 | loss: 15.5248985Losses:  14.20703125 1.5224635601043701
CurrentTrain: epoch  0, batch     7 | loss: 15.7294950Losses:  13.935869216918945 1.552010416984558
CurrentTrain: epoch  0, batch     8 | loss: 15.4878798Losses:  13.722323417663574 1.3304190635681152
CurrentTrain: epoch  0, batch     9 | loss: 15.0527420Losses:  13.285319328308105 1.4591541290283203
CurrentTrain: epoch  0, batch    10 | loss: 14.7444735Losses:  13.020700454711914 1.3309814929962158
CurrentTrain: epoch  0, batch    11 | loss: 14.3516817Losses:  12.596056938171387 1.40211021900177
CurrentTrain: epoch  0, batch    12 | loss: 13.9981670Losses:  12.893190383911133 1.7794709205627441
CurrentTrain: epoch  0, batch    13 | loss: 14.6726608Losses:  12.512130737304688 1.7275407314300537
CurrentTrain: epoch  0, batch    14 | loss: 14.2396717Losses:  12.812847137451172 1.5780695676803589
CurrentTrain: epoch  0, batch    15 | loss: 14.3909168Losses:  12.57288646697998 1.6643247604370117
CurrentTrain: epoch  0, batch    16 | loss: 14.2372112Losses:  12.108726501464844 1.5023317337036133
CurrentTrain: epoch  0, batch    17 | loss: 13.6110582Losses:  12.221277236938477 1.4880919456481934
CurrentTrain: epoch  0, batch    18 | loss: 13.7093697Losses:  11.725299835205078 1.36260986328125
CurrentTrain: epoch  0, batch    19 | loss: 13.0879097Losses:  11.838890075683594 1.5052576065063477
CurrentTrain: epoch  0, batch    20 | loss: 13.3441477Losses:  11.474811553955078 1.4036707878112793
CurrentTrain: epoch  0, batch    21 | loss: 12.8784828Losses:  10.742158889770508 1.2795610427856445
CurrentTrain: epoch  0, batch    22 | loss: 12.0217199Losses:  11.002530097961426 1.4426214694976807
CurrentTrain: epoch  0, batch    23 | loss: 12.4451513Losses:  10.724617004394531 1.3058561086654663
CurrentTrain: epoch  0, batch    24 | loss: 12.0304728Losses:  10.237449645996094 1.317131519317627
CurrentTrain: epoch  0, batch    25 | loss: 11.5545807Losses:  10.040868759155273 1.3431757688522339
CurrentTrain: epoch  0, batch    26 | loss: 11.3840446Losses:  9.878532409667969 1.4160425662994385
CurrentTrain: epoch  0, batch    27 | loss: 11.2945747Losses:  9.681581497192383 1.3569164276123047
CurrentTrain: epoch  0, batch    28 | loss: 11.0384979Losses:  9.386812210083008 1.1338285207748413
CurrentTrain: epoch  0, batch    29 | loss: 10.5206404Losses:  9.53359317779541 1.410855770111084
CurrentTrain: epoch  0, batch    30 | loss: 10.9444485Losses:  9.133207321166992 1.3065769672393799
CurrentTrain: epoch  0, batch    31 | loss: 10.4397840Losses:  8.85720443725586 1.2662198543548584
CurrentTrain: epoch  0, batch    32 | loss: 10.1234245Losses:  9.143937110900879 1.497721791267395
CurrentTrain: epoch  0, batch    33 | loss: 10.6416588Losses:  8.476202964782715 1.055629014968872
CurrentTrain: epoch  0, batch    34 | loss: 9.5318317Losses:  8.083708763122559 1.1793687343597412
CurrentTrain: epoch  0, batch    35 | loss: 9.2630777Losses:  7.9586944580078125 1.1847963333129883
CurrentTrain: epoch  0, batch    36 | loss: 9.1434908Losses:  7.767488479614258 0.7243722677230835
CurrentTrain: epoch  0, batch    37 | loss: 8.4918604Losses:  7.829537391662598 1.410733699798584
CurrentTrain: epoch  1, batch     0 | loss: 9.2402706Losses:  7.670144081115723 1.080012559890747
CurrentTrain: epoch  1, batch     1 | loss: 8.7501564Losses:  7.805513381958008 1.0606021881103516
CurrentTrain: epoch  1, batch     2 | loss: 8.8661156Losses:  7.6887712478637695 1.20601224899292
CurrentTrain: epoch  1, batch     3 | loss: 8.8947830Losses:  7.633707523345947 1.1664268970489502
CurrentTrain: epoch  1, batch     4 | loss: 8.8001347Losses:  7.512103080749512 1.2500213384628296
CurrentTrain: epoch  1, batch     5 | loss: 8.7621241Losses:  7.30303955078125 1.1438157558441162
CurrentTrain: epoch  1, batch     6 | loss: 8.4468555Losses:  7.601259231567383 1.1033666133880615
CurrentTrain: epoch  1, batch     7 | loss: 8.7046261Losses:  7.758469581604004 1.176454782485962
CurrentTrain: epoch  1, batch     8 | loss: 8.9349241Losses:  7.464491844177246 1.1365385055541992
CurrentTrain: epoch  1, batch     9 | loss: 8.6010303Losses:  7.334151268005371 1.0999377965927124
CurrentTrain: epoch  1, batch    10 | loss: 8.4340887Losses:  7.559192180633545 0.9793102741241455
CurrentTrain: epoch  1, batch    11 | loss: 8.5385027Losses:  7.188336372375488 1.0705432891845703
CurrentTrain: epoch  1, batch    12 | loss: 8.2588797Losses:  7.7074785232543945 1.1898207664489746
CurrentTrain: epoch  1, batch    13 | loss: 8.8972988Losses:  7.203503131866455 1.0789190530776978
CurrentTrain: epoch  1, batch    14 | loss: 8.2824221Losses:  7.365644454956055 1.004467487335205
CurrentTrain: epoch  1, batch    15 | loss: 8.3701115Losses:  7.120356559753418 1.0489575862884521
CurrentTrain: epoch  1, batch    16 | loss: 8.1693144Losses:  6.921718597412109 0.9611687064170837
CurrentTrain: epoch  1, batch    17 | loss: 7.8828874Losses:  7.007874488830566 1.0617914199829102
CurrentTrain: epoch  1, batch    18 | loss: 8.0696659Losses:  7.363802909851074 1.2717641592025757
CurrentTrain: epoch  1, batch    19 | loss: 8.6355667Losses:  6.728475570678711 0.8046040534973145
CurrentTrain: epoch  1, batch    20 | loss: 7.5330796Losses:  6.985523223876953 0.7831393480300903
CurrentTrain: epoch  1, batch    21 | loss: 7.7686625Losses:  7.058343887329102 0.8462578058242798
CurrentTrain: epoch  1, batch    22 | loss: 7.9046016Losses:  7.412341117858887 0.9493839740753174
CurrentTrain: epoch  1, batch    23 | loss: 8.3617249Losses:  7.364943504333496 0.8865087032318115
CurrentTrain: epoch  1, batch    24 | loss: 8.2514524Losses:  7.393347263336182 0.9437508583068848
CurrentTrain: epoch  1, batch    25 | loss: 8.3370981Losses:  6.836155891418457 0.8148428201675415
CurrentTrain: epoch  1, batch    26 | loss: 7.6509986Losses:  6.985654354095459 1.143647313117981
CurrentTrain: epoch  1, batch    27 | loss: 8.1293020Losses:  6.842061996459961 0.9046164751052856
CurrentTrain: epoch  1, batch    28 | loss: 7.7466784Losses:  6.686251640319824 0.6849052906036377
CurrentTrain: epoch  1, batch    29 | loss: 7.3711567Losses:  7.040599822998047 0.8197181224822998
CurrentTrain: epoch  1, batch    30 | loss: 7.8603182Losses:  6.913344383239746 0.9781545996665955
CurrentTrain: epoch  1, batch    31 | loss: 7.8914990Losses:  6.590850353240967 0.8220411539077759
CurrentTrain: epoch  1, batch    32 | loss: 7.4128914Losses:  6.544325828552246 0.7862995862960815
CurrentTrain: epoch  1, batch    33 | loss: 7.3306255Losses:  6.867552757263184 0.881051778793335
CurrentTrain: epoch  1, batch    34 | loss: 7.7486048Losses:  6.9573822021484375 0.9340733289718628
CurrentTrain: epoch  1, batch    35 | loss: 7.8914557Losses:  6.946398735046387 0.8817654848098755
CurrentTrain: epoch  1, batch    36 | loss: 7.8281641Losses:  6.711795806884766 0.28816941380500793
CurrentTrain: epoch  1, batch    37 | loss: 6.9999652Losses:  6.660856246948242 0.795781672000885
CurrentTrain: epoch  2, batch     0 | loss: 7.4566379Losses:  6.575999736785889 0.7003750205039978
CurrentTrain: epoch  2, batch     1 | loss: 7.2763748Losses:  5.9904327392578125 0.6568456888198853
CurrentTrain: epoch  2, batch     2 | loss: 6.6472783Losses:  6.073957443237305 0.699337899684906
CurrentTrain: epoch  2, batch     3 | loss: 6.7732954Losses:  7.230225563049316 0.5790846347808838
CurrentTrain: epoch  2, batch     4 | loss: 7.8093100Losses:  6.393065452575684 0.798406720161438
CurrentTrain: epoch  2, batch     5 | loss: 7.1914721Losses:  6.453836917877197 0.7630128860473633
CurrentTrain: epoch  2, batch     6 | loss: 7.2168498Losses:  6.200307846069336 0.7261000871658325
CurrentTrain: epoch  2, batch     7 | loss: 6.9264078Losses:  6.61851167678833 0.8339014649391174
CurrentTrain: epoch  2, batch     8 | loss: 7.4524131Losses:  6.460786819458008 0.551008403301239
CurrentTrain: epoch  2, batch     9 | loss: 7.0117950Losses:  6.337022304534912 0.5244612693786621
CurrentTrain: epoch  2, batch    10 | loss: 6.8614836Losses:  6.425081729888916 0.6552989482879639
CurrentTrain: epoch  2, batch    11 | loss: 7.0803804Losses:  6.00764274597168 0.5133416056632996
CurrentTrain: epoch  2, batch    12 | loss: 6.5209842Losses:  6.39054536819458 0.7164747714996338
CurrentTrain: epoch  2, batch    13 | loss: 7.1070204Losses:  7.019825458526611 0.783290445804596
CurrentTrain: epoch  2, batch    14 | loss: 7.8031158Losses:  6.683718204498291 0.6001595854759216
CurrentTrain: epoch  2, batch    15 | loss: 7.2838778Losses:  6.694159507751465 0.6505954265594482
CurrentTrain: epoch  2, batch    16 | loss: 7.3447552Losses:  6.22608757019043 0.6666820049285889
CurrentTrain: epoch  2, batch    17 | loss: 6.8927698Losses:  6.149224281311035 0.5876628160476685
CurrentTrain: epoch  2, batch    18 | loss: 6.7368870Losses:  6.235168933868408 0.5830065011978149
CurrentTrain: epoch  2, batch    19 | loss: 6.8181753Losses:  5.837520599365234 0.45458656549453735
CurrentTrain: epoch  2, batch    20 | loss: 6.2921071Losses:  6.299874305725098 0.7049486041069031
CurrentTrain: epoch  2, batch    21 | loss: 7.0048227Losses:  6.544215202331543 0.7745466232299805
CurrentTrain: epoch  2, batch    22 | loss: 7.3187618Losses:  6.570363521575928 0.6668256521224976
CurrentTrain: epoch  2, batch    23 | loss: 7.2371893Losses:  6.420406341552734 0.702236533164978
CurrentTrain: epoch  2, batch    24 | loss: 7.1226430Losses:  6.496184349060059 0.6677019596099854
CurrentTrain: epoch  2, batch    25 | loss: 7.1638861Losses:  6.926509380340576 0.7168496251106262
CurrentTrain: epoch  2, batch    26 | loss: 7.6433592Losses:  6.206119060516357 0.5779226422309875
CurrentTrain: epoch  2, batch    27 | loss: 6.7840419Losses:  6.91700553894043 0.7751209735870361
CurrentTrain: epoch  2, batch    28 | loss: 7.6921263Losses:  6.242432594299316 0.6356792449951172
CurrentTrain: epoch  2, batch    29 | loss: 6.8781118Losses:  6.2186994552612305 0.5474273562431335
CurrentTrain: epoch  2, batch    30 | loss: 6.7661266Losses:  6.675220489501953 0.610221266746521
CurrentTrain: epoch  2, batch    31 | loss: 7.2854419Losses:  6.3547258377075195 0.5583531260490417
CurrentTrain: epoch  2, batch    32 | loss: 6.9130788Losses:  6.044363021850586 0.5858771800994873
CurrentTrain: epoch  2, batch    33 | loss: 6.6302404Losses:  6.41864013671875 0.6406068801879883
CurrentTrain: epoch  2, batch    34 | loss: 7.0592470Losses:  6.541497230529785 0.5735889673233032
CurrentTrain: epoch  2, batch    35 | loss: 7.1150861Losses:  6.075345039367676 0.5877593755722046
CurrentTrain: epoch  2, batch    36 | loss: 6.6631045Losses:  6.113597869873047 0.5193241238594055
CurrentTrain: epoch  2, batch    37 | loss: 6.6329222Losses:  5.8961334228515625 0.5393800735473633
CurrentTrain: epoch  3, batch     0 | loss: 6.4355135Losses:  5.928969383239746 0.46621882915496826
CurrentTrain: epoch  3, batch     1 | loss: 6.3951883Losses:  6.228583335876465 0.5882821083068848
CurrentTrain: epoch  3, batch     2 | loss: 6.8168654Losses:  6.144541263580322 0.616010308265686
CurrentTrain: epoch  3, batch     3 | loss: 6.7605515Losses:  5.76629114151001 0.4206744432449341
CurrentTrain: epoch  3, batch     4 | loss: 6.1869655Losses:  6.187243461608887 0.6138985753059387
CurrentTrain: epoch  3, batch     5 | loss: 6.8011422Losses:  5.6440348625183105 0.5425876379013062
CurrentTrain: epoch  3, batch     6 | loss: 6.1866226Losses:  5.899855613708496 0.5130529403686523
CurrentTrain: epoch  3, batch     7 | loss: 6.4129086Losses:  5.825929641723633 0.48977404832839966
CurrentTrain: epoch  3, batch     8 | loss: 6.3157039Losses:  5.815698623657227 0.3871071934700012
CurrentTrain: epoch  3, batch     9 | loss: 6.2028060Losses:  5.766424655914307 0.5314172506332397
CurrentTrain: epoch  3, batch    10 | loss: 6.2978420Losses:  5.866400718688965 0.4411396384239197
CurrentTrain: epoch  3, batch    11 | loss: 6.3075404Losses:  5.893368721008301 0.5527539253234863
CurrentTrain: epoch  3, batch    12 | loss: 6.4461226Losses:  5.839960098266602 0.4731966257095337
CurrentTrain: epoch  3, batch    13 | loss: 6.3131566Losses:  5.6248779296875 0.43994271755218506
CurrentTrain: epoch  3, batch    14 | loss: 6.0648208Losses:  6.067163467407227 0.5121640563011169
CurrentTrain: epoch  3, batch    15 | loss: 6.5793276Losses:  5.5414299964904785 0.3935166001319885
CurrentTrain: epoch  3, batch    16 | loss: 5.9349465Losses:  6.121804237365723 0.5152957439422607
CurrentTrain: epoch  3, batch    17 | loss: 6.6371002Losses:  5.910941123962402 0.419575572013855
CurrentTrain: epoch  3, batch    18 | loss: 6.3305168Losses:  5.870645523071289 0.5861569046974182
CurrentTrain: epoch  3, batch    19 | loss: 6.4568024Losses:  5.4965925216674805 0.3889960050582886
CurrentTrain: epoch  3, batch    20 | loss: 5.8855886Losses:  5.945577621459961 0.3511957824230194
CurrentTrain: epoch  3, batch    21 | loss: 6.2967734Losses:  5.971523284912109 0.6062859296798706
CurrentTrain: epoch  3, batch    22 | loss: 6.5778093Losses:  5.737027168273926 0.39271873235702515
CurrentTrain: epoch  3, batch    23 | loss: 6.1297460Losses:  5.609433174133301 0.3380686044692993
CurrentTrain: epoch  3, batch    24 | loss: 5.9475017Losses:  5.873722076416016 0.38996621966362
CurrentTrain: epoch  3, batch    25 | loss: 6.2636881Losses:  5.961398124694824 0.5474849939346313
CurrentTrain: epoch  3, batch    26 | loss: 6.5088830Losses:  5.566488265991211 0.5238118767738342
CurrentTrain: epoch  3, batch    27 | loss: 6.0903001Losses:  5.97231912612915 0.5253573060035706
CurrentTrain: epoch  3, batch    28 | loss: 6.4976764Losses:  6.113986015319824 0.5737134218215942
CurrentTrain: epoch  3, batch    29 | loss: 6.6876993Losses:  6.283010482788086 0.3666017949581146
CurrentTrain: epoch  3, batch    30 | loss: 6.6496124Losses:  6.709165573120117 0.4754827320575714
CurrentTrain: epoch  3, batch    31 | loss: 7.1846485Losses:  5.9605255126953125 0.45171016454696655
CurrentTrain: epoch  3, batch    32 | loss: 6.4122357Losses:  6.1298346519470215 0.46593305468559265
CurrentTrain: epoch  3, batch    33 | loss: 6.5957675Losses:  6.06788444519043 0.38803914189338684
CurrentTrain: epoch  3, batch    34 | loss: 6.4559236Losses:  5.580272674560547 0.32987719774246216
CurrentTrain: epoch  3, batch    35 | loss: 5.9101501Losses:  5.838329315185547 0.5303447842597961
CurrentTrain: epoch  3, batch    36 | loss: 6.3686743Losses:  4.955124855041504 0.18782901763916016
CurrentTrain: epoch  3, batch    37 | loss: 5.1429539Losses:  5.445795059204102 0.40415576100349426
CurrentTrain: epoch  4, batch     0 | loss: 5.8499508Losses:  6.402994155883789 0.586215615272522
CurrentTrain: epoch  4, batch     1 | loss: 6.9892097Losses:  5.696265697479248 0.37343281507492065
CurrentTrain: epoch  4, batch     2 | loss: 6.0696983Losses:  6.118566513061523 0.5110552310943604
CurrentTrain: epoch  4, batch     3 | loss: 6.6296215Losses:  5.692402362823486 0.39661967754364014
CurrentTrain: epoch  4, batch     4 | loss: 6.0890222Losses:  5.365659713745117 0.3281477093696594
CurrentTrain: epoch  4, batch     5 | loss: 5.6938076Losses:  5.476682662963867 0.44017699360847473
CurrentTrain: epoch  4, batch     6 | loss: 5.9168596Losses:  5.246927261352539 0.33299052715301514
CurrentTrain: epoch  4, batch     7 | loss: 5.5799179Losses:  5.589288711547852 0.328207403421402
CurrentTrain: epoch  4, batch     8 | loss: 5.9174962Losses:  5.3346452713012695 0.3096114695072174
CurrentTrain: epoch  4, batch     9 | loss: 5.6442566Losses:  5.837967872619629 0.396749347448349
CurrentTrain: epoch  4, batch    10 | loss: 6.2347174Losses:  5.277683258056641 0.32496365904808044
CurrentTrain: epoch  4, batch    11 | loss: 5.6026468Losses:  5.431299209594727 0.30721449851989746
CurrentTrain: epoch  4, batch    12 | loss: 5.7385139Losses:  5.814845561981201 0.32138580083847046
CurrentTrain: epoch  4, batch    13 | loss: 6.1362314Losses:  5.449916839599609 0.32159650325775146
CurrentTrain: epoch  4, batch    14 | loss: 5.7715135Losses:  5.8777265548706055 0.499569833278656
CurrentTrain: epoch  4, batch    15 | loss: 6.3772964Losses:  5.632746696472168 0.4164699912071228
CurrentTrain: epoch  4, batch    16 | loss: 6.0492167Losses:  5.422919273376465 0.3075593113899231
CurrentTrain: epoch  4, batch    17 | loss: 5.7304788Losses:  5.365677833557129 0.29895955324172974
CurrentTrain: epoch  4, batch    18 | loss: 5.6646376Losses:  5.316925525665283 0.2890061140060425
CurrentTrain: epoch  4, batch    19 | loss: 5.6059318Losses:  5.627549648284912 0.387641578912735
CurrentTrain: epoch  4, batch    20 | loss: 6.0151911Losses:  5.442474365234375 0.2700526714324951
CurrentTrain: epoch  4, batch    21 | loss: 5.7125273Losses:  6.00586462020874 0.4871850609779358
CurrentTrain: epoch  4, batch    22 | loss: 6.4930496Losses:  5.351221084594727 0.36573609709739685
CurrentTrain: epoch  4, batch    23 | loss: 5.7169571Losses:  5.681283473968506 0.33129268884658813
CurrentTrain: epoch  4, batch    24 | loss: 6.0125761Losses:  5.443327903747559 0.25095924735069275
CurrentTrain: epoch  4, batch    25 | loss: 5.6942873Losses:  6.112590789794922 0.5652074217796326
CurrentTrain: epoch  4, batch    26 | loss: 6.6777983Losses:  5.477855682373047 0.35028332471847534
CurrentTrain: epoch  4, batch    27 | loss: 5.8281388Losses:  5.2319207191467285 0.2992488145828247
CurrentTrain: epoch  4, batch    28 | loss: 5.5311694Losses:  4.963095664978027 0.2516140341758728
CurrentTrain: epoch  4, batch    29 | loss: 5.2147098Losses:  6.115328788757324 0.3272014260292053
CurrentTrain: epoch  4, batch    30 | loss: 6.4425302Losses:  5.267570495605469 0.28369006514549255
CurrentTrain: epoch  4, batch    31 | loss: 5.5512605Losses:  5.613201141357422 0.33424919843673706
CurrentTrain: epoch  4, batch    32 | loss: 5.9474502Losses:  5.257476806640625 0.399002343416214
CurrentTrain: epoch  4, batch    33 | loss: 5.6564794Losses:  5.916821479797363 0.4635256230831146
CurrentTrain: epoch  4, batch    34 | loss: 6.3803473Losses:  6.1323041915893555 0.5144560933113098
CurrentTrain: epoch  4, batch    35 | loss: 6.6467605Losses:  5.127011299133301 0.24681904911994934
CurrentTrain: epoch  4, batch    36 | loss: 5.3738303Losses:  5.187078475952148 0.2941286861896515
CurrentTrain: epoch  4, batch    37 | loss: 5.4812074Losses:  4.987395286560059 0.25505852699279785
CurrentTrain: epoch  5, batch     0 | loss: 5.2424536Losses:  5.2990264892578125 0.18979892134666443
CurrentTrain: epoch  5, batch     1 | loss: 5.4888253Losses:  5.250065803527832 0.25925302505493164
CurrentTrain: epoch  5, batch     2 | loss: 5.5093188Losses:  5.757608413696289 0.34888559579849243
CurrentTrain: epoch  5, batch     3 | loss: 6.1064939Losses:  5.417292594909668 0.2736530900001526
CurrentTrain: epoch  5, batch     4 | loss: 5.6909456Losses:  5.711730003356934 0.35884854197502136
CurrentTrain: epoch  5, batch     5 | loss: 6.0705786Losses:  5.345458030700684 0.32241955399513245
CurrentTrain: epoch  5, batch     6 | loss: 5.6678777Losses:  5.035793304443359 0.24006377160549164
CurrentTrain: epoch  5, batch     7 | loss: 5.2758570Losses:  5.308980941772461 0.31967025995254517
CurrentTrain: epoch  5, batch     8 | loss: 5.6286511Losses:  5.021055221557617 0.12300477921962738
CurrentTrain: epoch  5, batch     9 | loss: 5.1440601Losses:  5.308328628540039 0.3342908024787903
CurrentTrain: epoch  5, batch    10 | loss: 5.6426196Losses:  5.347532272338867 0.28676074743270874
CurrentTrain: epoch  5, batch    11 | loss: 5.6342931Losses:  5.539895057678223 0.38197803497314453
CurrentTrain: epoch  5, batch    12 | loss: 5.9218731Losses:  5.38213586807251 0.3386797308921814
CurrentTrain: epoch  5, batch    13 | loss: 5.7208157Losses:  5.13240385055542 0.2590405344963074
CurrentTrain: epoch  5, batch    14 | loss: 5.3914442Losses:  5.350931167602539 0.24085481464862823
CurrentTrain: epoch  5, batch    15 | loss: 5.5917859Losses:  5.432657718658447 0.23113244771957397
CurrentTrain: epoch  5, batch    16 | loss: 5.6637902Losses:  5.2728776931762695 0.27143195271492004
CurrentTrain: epoch  5, batch    17 | loss: 5.5443096Losses:  5.200649261474609 0.1982881724834442
CurrentTrain: epoch  5, batch    18 | loss: 5.3989372Losses:  4.98899507522583 0.2254304587841034
CurrentTrain: epoch  5, batch    19 | loss: 5.2144256Losses:  5.460363864898682 0.2583281993865967
CurrentTrain: epoch  5, batch    20 | loss: 5.7186918Losses:  5.422558784484863 0.2946596145629883
CurrentTrain: epoch  5, batch    21 | loss: 5.7172184Losses:  4.960963726043701 0.24865278601646423
CurrentTrain: epoch  5, batch    22 | loss: 5.2096167Losses:  5.41184139251709 0.297579288482666
CurrentTrain: epoch  5, batch    23 | loss: 5.7094207Losses:  5.525687217712402 0.3428569436073303
CurrentTrain: epoch  5, batch    24 | loss: 5.8685441Losses:  5.462996959686279 0.37069079279899597
CurrentTrain: epoch  5, batch    25 | loss: 5.8336878Losses:  5.03903865814209 0.20735225081443787
CurrentTrain: epoch  5, batch    26 | loss: 5.2463908Losses:  5.030932903289795 0.21499355137348175
CurrentTrain: epoch  5, batch    27 | loss: 5.2459264Losses:  5.625833511352539 0.3928913474082947
CurrentTrain: epoch  5, batch    28 | loss: 6.0187249Losses:  5.198336124420166 0.28321340680122375
CurrentTrain: epoch  5, batch    29 | loss: 5.4815497Losses:  5.080036640167236 0.2784273624420166
CurrentTrain: epoch  5, batch    30 | loss: 5.3584642Losses:  5.571433067321777 0.4611932039260864
CurrentTrain: epoch  5, batch    31 | loss: 6.0326262Losses:  4.753838539123535 0.2195120006799698
CurrentTrain: epoch  5, batch    32 | loss: 4.9733505Losses:  5.115553855895996 0.21660330891609192
CurrentTrain: epoch  5, batch    33 | loss: 5.3321571Losses:  5.27041482925415 0.26775047183036804
CurrentTrain: epoch  5, batch    34 | loss: 5.5381651Losses:  5.183065891265869 0.2447863519191742
CurrentTrain: epoch  5, batch    35 | loss: 5.4278522Losses:  4.889439582824707 0.1514827013015747
CurrentTrain: epoch  5, batch    36 | loss: 5.0409222Losses:  5.473438262939453 0.1943565458059311
CurrentTrain: epoch  5, batch    37 | loss: 5.6677947Losses:  5.164394855499268 0.2229413092136383
CurrentTrain: epoch  6, batch     0 | loss: 5.3873363Losses:  4.990077018737793 0.24782267212867737
CurrentTrain: epoch  6, batch     1 | loss: 5.2378998Losses:  5.14961051940918 0.29203319549560547
CurrentTrain: epoch  6, batch     2 | loss: 5.4416437Losses:  5.023592948913574 0.2273549884557724
CurrentTrain: epoch  6, batch     3 | loss: 5.2509480Losses:  4.912337779998779 0.20116209983825684
CurrentTrain: epoch  6, batch     4 | loss: 5.1134996Losses:  4.992023944854736 0.24150478839874268
CurrentTrain: epoch  6, batch     5 | loss: 5.2335286Losses:  5.134282112121582 0.2529523968696594
CurrentTrain: epoch  6, batch     6 | loss: 5.3872347Losses:  4.872645378112793 0.19955329596996307
CurrentTrain: epoch  6, batch     7 | loss: 5.0721989Losses:  5.408463478088379 0.28490665555000305
CurrentTrain: epoch  6, batch     8 | loss: 5.6933703Losses:  4.91900634765625 0.2734982371330261
CurrentTrain: epoch  6, batch     9 | loss: 5.1925044Losses:  5.136926651000977 0.26521265506744385
CurrentTrain: epoch  6, batch    10 | loss: 5.4021392Losses:  4.921643257141113 0.20738419890403748
CurrentTrain: epoch  6, batch    11 | loss: 5.1290274Losses:  4.812494277954102 0.18460765480995178
CurrentTrain: epoch  6, batch    12 | loss: 4.9971018Losses:  4.903011322021484 0.17179062962532043
CurrentTrain: epoch  6, batch    13 | loss: 5.0748019Losses:  5.129958152770996 0.21423281729221344
CurrentTrain: epoch  6, batch    14 | loss: 5.3441911Losses:  5.053918838500977 0.1216898113489151
CurrentTrain: epoch  6, batch    15 | loss: 5.1756086Losses:  4.841274738311768 0.1523900181055069
CurrentTrain: epoch  6, batch    16 | loss: 4.9936647Losses:  4.94298791885376 0.15648110210895538
CurrentTrain: epoch  6, batch    17 | loss: 5.0994692Losses:  5.0851545333862305 0.1845804750919342
CurrentTrain: epoch  6, batch    18 | loss: 5.2697349Losses:  5.453447341918945 0.33420389890670776
CurrentTrain: epoch  6, batch    19 | loss: 5.7876511Losses:  4.91008186340332 0.18071478605270386
CurrentTrain: epoch  6, batch    20 | loss: 5.0907965Losses:  5.000861167907715 0.16812542080879211
CurrentTrain: epoch  6, batch    21 | loss: 5.1689868Losses:  5.043094635009766 0.2578423023223877
CurrentTrain: epoch  6, batch    22 | loss: 5.3009367Losses:  4.873022079467773 0.15570349991321564
CurrentTrain: epoch  6, batch    23 | loss: 5.0287256Losses:  5.322277069091797 0.43157947063446045
CurrentTrain: epoch  6, batch    24 | loss: 5.7538567Losses:  4.773895740509033 0.17188143730163574
CurrentTrain: epoch  6, batch    25 | loss: 4.9457769Losses:  5.164210319519043 0.39846399426460266
CurrentTrain: epoch  6, batch    26 | loss: 5.5626745Losses:  4.734740734100342 0.11002311110496521
CurrentTrain: epoch  6, batch    27 | loss: 4.8447638Losses:  5.0240936279296875 0.15036234259605408
CurrentTrain: epoch  6, batch    28 | loss: 5.1744561Losses:  5.00606107711792 0.24869079887866974
CurrentTrain: epoch  6, batch    29 | loss: 5.2547517Losses:  4.781914710998535 0.15433457493782043
CurrentTrain: epoch  6, batch    30 | loss: 4.9362493Losses:  5.201279163360596 0.2613440155982971
CurrentTrain: epoch  6, batch    31 | loss: 5.4626231Losses:  4.9230451583862305 0.2403479367494583
CurrentTrain: epoch  6, batch    32 | loss: 5.1633930Losses:  5.309914588928223 0.3913145065307617
CurrentTrain: epoch  6, batch    33 | loss: 5.7012291Losses:  5.066394329071045 0.32025858759880066
CurrentTrain: epoch  6, batch    34 | loss: 5.3866529Losses:  4.618752479553223 0.1246824711561203
CurrentTrain: epoch  6, batch    35 | loss: 4.7434349Losses:  5.019801139831543 0.1719268411397934
CurrentTrain: epoch  6, batch    36 | loss: 5.1917281Losses:  5.3310227394104 0.3062489926815033
CurrentTrain: epoch  6, batch    37 | loss: 5.6372719Losses:  4.718038558959961 0.14609138667583466
CurrentTrain: epoch  7, batch     0 | loss: 4.8641300Losses:  5.01831579208374 0.17324888706207275
CurrentTrain: epoch  7, batch     1 | loss: 5.1915646Losses:  5.074193954467773 0.2442207634449005
CurrentTrain: epoch  7, batch     2 | loss: 5.3184147Losses:  4.688750743865967 0.15799163281917572
CurrentTrain: epoch  7, batch     3 | loss: 4.8467422Losses:  4.823272705078125 0.20690786838531494
CurrentTrain: epoch  7, batch     4 | loss: 5.0301805Losses:  4.73807430267334 0.14161260426044464
CurrentTrain: epoch  7, batch     5 | loss: 4.8796868Losses:  5.237009048461914 0.31489843130111694
CurrentTrain: epoch  7, batch     6 | loss: 5.5519075Losses:  4.691971778869629 0.1500576138496399
CurrentTrain: epoch  7, batch     7 | loss: 4.8420296Losses:  4.823189735412598 0.1723979264497757
CurrentTrain: epoch  7, batch     8 | loss: 4.9955878Losses:  4.691572189331055 0.15062983334064484
CurrentTrain: epoch  7, batch     9 | loss: 4.8422022Losses:  4.727011203765869 0.18110693991184235
CurrentTrain: epoch  7, batch    10 | loss: 4.9081182Losses:  4.820147514343262 0.20962685346603394
CurrentTrain: epoch  7, batch    11 | loss: 5.0297742Losses:  4.679177284240723 0.16897106170654297
CurrentTrain: epoch  7, batch    12 | loss: 4.8481483Losses:  4.7936506271362305 0.16390913724899292
CurrentTrain: epoch  7, batch    13 | loss: 4.9575596Losses:  4.811428546905518 0.17331257462501526
CurrentTrain: epoch  7, batch    14 | loss: 4.9847412Losses:  4.6713056564331055 0.15433189272880554
CurrentTrain: epoch  7, batch    15 | loss: 4.8256373Losses:  4.710127830505371 0.13837561011314392
CurrentTrain: epoch  7, batch    16 | loss: 4.8485036Losses:  4.642068386077881 0.1465788632631302
CurrentTrain: epoch  7, batch    17 | loss: 4.7886472Losses:  4.719408988952637 0.12321680784225464
CurrentTrain: epoch  7, batch    18 | loss: 4.8426256Losses:  4.819382667541504 0.17534314095973969
CurrentTrain: epoch  7, batch    19 | loss: 4.9947257Losses:  4.671294212341309 0.13415278494358063
CurrentTrain: epoch  7, batch    20 | loss: 4.8054471Losses:  4.646742343902588 0.1372334361076355
CurrentTrain: epoch  7, batch    21 | loss: 4.7839756Losses:  4.709571361541748 0.09937670826911926
CurrentTrain: epoch  7, batch    22 | loss: 4.8089480Losses:  4.952788352966309 0.2579648494720459
CurrentTrain: epoch  7, batch    23 | loss: 5.2107534Losses:  5.002114295959473 0.17610719799995422
CurrentTrain: epoch  7, batch    24 | loss: 5.1782217Losses:  4.733030796051025 0.19008366763591766
CurrentTrain: epoch  7, batch    25 | loss: 4.9231143Losses:  4.59263801574707 0.1323891282081604
CurrentTrain: epoch  7, batch    26 | loss: 4.7250271Losses:  4.943202972412109 0.1529051661491394
CurrentTrain: epoch  7, batch    27 | loss: 5.0961080Losses:  5.084646224975586 0.20098614692687988
CurrentTrain: epoch  7, batch    28 | loss: 5.2856321Losses:  4.636563301086426 0.13379976153373718
CurrentTrain: epoch  7, batch    29 | loss: 4.7703629Losses:  4.8225860595703125 0.20084801316261292
CurrentTrain: epoch  7, batch    30 | loss: 5.0234342Losses:  4.69532585144043 0.15481717884540558
CurrentTrain: epoch  7, batch    31 | loss: 4.8501430Losses:  4.6748456954956055 0.1519199162721634
CurrentTrain: epoch  7, batch    32 | loss: 4.8267655Losses:  4.713836669921875 0.09876365959644318
CurrentTrain: epoch  7, batch    33 | loss: 4.8126001Losses:  4.9501776695251465 0.1963074505329132
CurrentTrain: epoch  7, batch    34 | loss: 5.1464853Losses:  4.632318019866943 0.12299299240112305
CurrentTrain: epoch  7, batch    35 | loss: 4.7553110Losses:  4.633650779724121 0.13897641003131866
CurrentTrain: epoch  7, batch    36 | loss: 4.7726274Losses:  4.8581695556640625 0.14814946055412292
CurrentTrain: epoch  7, batch    37 | loss: 5.0063190Losses:  4.586449146270752 0.09735846519470215
CurrentTrain: epoch  8, batch     0 | loss: 4.6838074Losses:  4.6017374992370605 0.13084135949611664
CurrentTrain: epoch  8, batch     1 | loss: 4.7325788Losses:  4.623926162719727 0.12820905447006226
CurrentTrain: epoch  8, batch     2 | loss: 4.7521353Losses:  4.767068862915039 0.15362301468849182
CurrentTrain: epoch  8, batch     3 | loss: 4.9206920Losses:  4.67639684677124 0.089636892080307
CurrentTrain: epoch  8, batch     4 | loss: 4.7660336Losses:  4.655178070068359 0.0747339054942131
CurrentTrain: epoch  8, batch     5 | loss: 4.7299118Losses:  4.679637908935547 0.12847042083740234
CurrentTrain: epoch  8, batch     6 | loss: 4.8081083Losses:  4.609802722930908 0.14322197437286377
CurrentTrain: epoch  8, batch     7 | loss: 4.7530246Losses:  4.713067054748535 0.15795393288135529
CurrentTrain: epoch  8, batch     8 | loss: 4.8710208Losses:  5.0258331298828125 0.23505958914756775
CurrentTrain: epoch  8, batch     9 | loss: 5.2608929Losses:  4.690088272094727 0.13242638111114502
CurrentTrain: epoch  8, batch    10 | loss: 4.8225145Losses:  4.617883205413818 0.14048941433429718
CurrentTrain: epoch  8, batch    11 | loss: 4.7583728Losses:  5.050669193267822 0.17851699888706207
CurrentTrain: epoch  8, batch    12 | loss: 5.2291861Losses:  4.629554271697998 0.12501491606235504
CurrentTrain: epoch  8, batch    13 | loss: 4.7545691Losses:  4.749965190887451 0.15855097770690918
CurrentTrain: epoch  8, batch    14 | loss: 4.9085159Losses:  4.611532211303711 0.07695136964321136
CurrentTrain: epoch  8, batch    15 | loss: 4.6884837Losses:  4.6259918212890625 0.09114828705787659
CurrentTrain: epoch  8, batch    16 | loss: 4.7171402Losses:  4.723482131958008 0.10706555843353271
CurrentTrain: epoch  8, batch    17 | loss: 4.8305478Losses:  4.666243076324463 0.15178906917572021
CurrentTrain: epoch  8, batch    18 | loss: 4.8180323Losses:  4.641315937042236 0.13589170575141907
CurrentTrain: epoch  8, batch    19 | loss: 4.7772079Losses:  4.588255405426025 0.1267184019088745
CurrentTrain: epoch  8, batch    20 | loss: 4.7149739Losses:  4.623994827270508 0.12688906490802765
CurrentTrain: epoch  8, batch    21 | loss: 4.7508841Losses:  4.666784286499023 0.08678249269723892
CurrentTrain: epoch  8, batch    22 | loss: 4.7535667Losses:  4.614368438720703 0.13147284090518951
CurrentTrain: epoch  8, batch    23 | loss: 4.7458415Losses:  4.627811431884766 0.1107252910733223
CurrentTrain: epoch  8, batch    24 | loss: 4.7385368Losses:  4.58935546875 0.11118993163108826
CurrentTrain: epoch  8, batch    25 | loss: 4.7005453Losses:  4.641190528869629 0.08143244683742523
CurrentTrain: epoch  8, batch    26 | loss: 4.7226229Losses:  4.674603462219238 0.1096123456954956
CurrentTrain: epoch  8, batch    27 | loss: 4.7842159Losses:  4.644966125488281 0.16915418207645416
CurrentTrain: epoch  8, batch    28 | loss: 4.8141203Losses:  4.647848129272461 0.12782326340675354
CurrentTrain: epoch  8, batch    29 | loss: 4.7756715Losses:  4.7127275466918945 0.11365751922130585
CurrentTrain: epoch  8, batch    30 | loss: 4.8263850Losses:  4.578851222991943 0.11654632538557053
CurrentTrain: epoch  8, batch    31 | loss: 4.6953974Losses:  4.708493232727051 0.145926371216774
CurrentTrain: epoch  8, batch    32 | loss: 4.8544197Losses:  4.592425346374512 0.13185259699821472
CurrentTrain: epoch  8, batch    33 | loss: 4.7242780Losses:  4.792752265930176 0.204849511384964
CurrentTrain: epoch  8, batch    34 | loss: 4.9976020Losses:  4.674683094024658 0.10942581295967102
CurrentTrain: epoch  8, batch    35 | loss: 4.7841091Losses:  4.714573860168457 0.16566899418830872
CurrentTrain: epoch  8, batch    36 | loss: 4.8802428Losses:  4.711559295654297 0.15615001320838928
CurrentTrain: epoch  8, batch    37 | loss: 4.8677092Losses:  4.652968883514404 0.08373047411441803
CurrentTrain: epoch  9, batch     0 | loss: 4.7366996Losses:  4.758238792419434 0.16216441988945007
CurrentTrain: epoch  9, batch     1 | loss: 4.9204030Losses:  4.622460842132568 0.1301725208759308
CurrentTrain: epoch  9, batch     2 | loss: 4.7526336Losses:  4.601526737213135 0.12493899464607239
CurrentTrain: epoch  9, batch     3 | loss: 4.7264657Losses:  4.533185958862305 0.08547506481409073
CurrentTrain: epoch  9, batch     4 | loss: 4.6186609Losses:  4.631841659545898 0.11589998006820679
CurrentTrain: epoch  9, batch     5 | loss: 4.7477417Losses:  4.6082444190979 0.08393502980470657
CurrentTrain: epoch  9, batch     6 | loss: 4.6921797Losses:  4.551778316497803 0.10997435450553894
CurrentTrain: epoch  9, batch     7 | loss: 4.6617527Losses:  4.563422203063965 0.13389359414577484
CurrentTrain: epoch  9, batch     8 | loss: 4.6973157Losses:  4.552578926086426 0.09711074829101562
CurrentTrain: epoch  9, batch     9 | loss: 4.6496897Losses:  4.594644546508789 0.11455000936985016
CurrentTrain: epoch  9, batch    10 | loss: 4.7091947Losses:  4.588335037231445 0.10175903141498566
CurrentTrain: epoch  9, batch    11 | loss: 4.6900940Losses:  4.582169055938721 0.08691705018281937
CurrentTrain: epoch  9, batch    12 | loss: 4.6690860Losses:  4.599996566772461 0.12009860575199127
CurrentTrain: epoch  9, batch    13 | loss: 4.7200952Losses:  4.825405120849609 0.17798534035682678
CurrentTrain: epoch  9, batch    14 | loss: 5.0033903Losses:  4.586512565612793 0.0926777571439743
CurrentTrain: epoch  9, batch    15 | loss: 4.6791902Losses:  4.552360534667969 0.09995821863412857
CurrentTrain: epoch  9, batch    16 | loss: 4.6523190Losses:  4.65254020690918 0.14331580698490143
CurrentTrain: epoch  9, batch    17 | loss: 4.7958560Losses:  4.549417972564697 0.11328335106372833
CurrentTrain: epoch  9, batch    18 | loss: 4.6627011Losses:  4.6117753982543945 0.11450904607772827
CurrentTrain: epoch  9, batch    19 | loss: 4.7262845Losses:  4.569647789001465 0.10737967491149902
CurrentTrain: epoch  9, batch    20 | loss: 4.6770277Losses:  4.538755893707275 0.10391299426555634
CurrentTrain: epoch  9, batch    21 | loss: 4.6426687Losses:  4.558935165405273 0.09713514149188995
CurrentTrain: epoch  9, batch    22 | loss: 4.6560702Losses:  4.712092399597168 0.13323700428009033
CurrentTrain: epoch  9, batch    23 | loss: 4.8453293Losses:  4.56348180770874 0.09898489713668823
CurrentTrain: epoch  9, batch    24 | loss: 4.6624665Losses:  4.543874740600586 0.09390649199485779
CurrentTrain: epoch  9, batch    25 | loss: 4.6377811Losses:  4.558060646057129 0.10660134255886078
CurrentTrain: epoch  9, batch    26 | loss: 4.6646619Losses:  4.522183895111084 0.0816144198179245
CurrentTrain: epoch  9, batch    27 | loss: 4.6037984Losses:  4.771533966064453 0.08977708220481873
CurrentTrain: epoch  9, batch    28 | loss: 4.8613110Losses:  4.584692001342773 0.11482058465480804
CurrentTrain: epoch  9, batch    29 | loss: 4.6995125Losses:  4.511891841888428 0.09054113179445267
CurrentTrain: epoch  9, batch    30 | loss: 4.6024332Losses:  4.554139614105225 0.10709145665168762
CurrentTrain: epoch  9, batch    31 | loss: 4.6612310Losses:  4.810476303100586 0.2152155488729477
CurrentTrain: epoch  9, batch    32 | loss: 5.0256920Losses:  4.543734550476074 0.10303197801113129
CurrentTrain: epoch  9, batch    33 | loss: 4.6467667Losses:  4.579029560089111 0.0881757065653801
CurrentTrain: epoch  9, batch    34 | loss: 4.6672053Losses:  4.517849445343018 0.09273296594619751
CurrentTrain: epoch  9, batch    35 | loss: 4.6105824Losses:  4.539884567260742 0.10828767716884613
CurrentTrain: epoch  9, batch    36 | loss: 4.6481724Losses:  4.520918846130371 0.08959294855594635
CurrentTrain: epoch  9, batch    37 | loss: 4.6105118
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.74%   
cur_acc:  ['0.8674']
his_acc:  ['0.8674']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 0 1 0 0 1]
Losses:  7.4644341468811035 1.322157382965088
CurrentTrain: epoch  0, batch     0 | loss: 8.7865915Losses:  9.64135456085205 1.4347134828567505
CurrentTrain: epoch  0, batch     1 | loss: 11.0760679Losses:  4.404918670654297 1.4532580375671387
CurrentTrain: epoch  1, batch     0 | loss: 5.8581767Losses:  4.695321559906006 1.4454370737075806
CurrentTrain: epoch  1, batch     1 | loss: 6.1407585Losses:  4.468328475952148 1.3824307918548584
CurrentTrain: epoch  2, batch     0 | loss: 5.8507595Losses:  3.7364394664764404 1.1929751634597778
CurrentTrain: epoch  2, batch     1 | loss: 4.9294147Losses:  3.7920098304748535 1.3184927701950073
CurrentTrain: epoch  3, batch     0 | loss: 5.1105027Losses:  3.922412395477295 1.1175669431686401
CurrentTrain: epoch  3, batch     1 | loss: 5.0399795Losses:  3.805845260620117 1.1545422077178955
CurrentTrain: epoch  4, batch     0 | loss: 4.9603872Losses:  3.602489709854126 1.1584446430206299
CurrentTrain: epoch  4, batch     1 | loss: 4.7609344Losses:  3.4345171451568604 1.1921086311340332
CurrentTrain: epoch  5, batch     0 | loss: 4.6266260Losses:  3.2373149394989014 1.091141939163208
CurrentTrain: epoch  5, batch     1 | loss: 4.3284569Losses:  3.466993808746338 1.1104588508605957
CurrentTrain: epoch  6, batch     0 | loss: 4.5774527Losses:  2.753488302230835 0.8335896730422974
CurrentTrain: epoch  6, batch     1 | loss: 3.5870781Losses:  2.746143341064453 0.9809648394584656
CurrentTrain: epoch  7, batch     0 | loss: 3.7271082Losses:  2.8064968585968018 0.8960766196250916
CurrentTrain: epoch  7, batch     1 | loss: 3.7025735Losses:  3.1268131732940674 1.018378734588623
CurrentTrain: epoch  8, batch     0 | loss: 4.1451921Losses:  2.2559452056884766 0.897993266582489
CurrentTrain: epoch  8, batch     1 | loss: 3.1539385Losses:  2.5758399963378906 0.9374783635139465
CurrentTrain: epoch  9, batch     0 | loss: 3.5133183Losses:  2.5564000606536865 0.767926037311554
CurrentTrain: epoch  9, batch     1 | loss: 3.3243260
Losses:  4.453679084777832 0.7753415703773499
MemoryTrain:  epoch  0, batch     0 | loss: 5.2290206Losses:  0.4623233377933502 0.7950745820999146
MemoryTrain:  epoch  1, batch     0 | loss: 1.2573979Losses:  0.40988144278526306 0.7334957122802734
MemoryTrain:  epoch  2, batch     0 | loss: 1.1433772Losses:  0.42639264464378357 0.7223159670829773
MemoryTrain:  epoch  3, batch     0 | loss: 1.1487086Losses:  0.45656391978263855 0.7572392821311951
MemoryTrain:  epoch  4, batch     0 | loss: 1.2138032Losses:  0.36214005947113037 0.7795049548149109
MemoryTrain:  epoch  5, batch     0 | loss: 1.1416450Losses:  0.4002361297607422 0.7584607005119324
MemoryTrain:  epoch  6, batch     0 | loss: 1.1586969Losses:  0.4258665442466736 0.7148572206497192
MemoryTrain:  epoch  7, batch     0 | loss: 1.1407237Losses:  0.3271433413028717 0.7390400767326355
MemoryTrain:  epoch  8, batch     0 | loss: 1.0661834Losses:  0.36375054717063904 0.7167997360229492
MemoryTrain:  epoch  9, batch     0 | loss: 1.0805503
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 80.36%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 88.21%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 86.82%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 84.97%   
cur_acc:  ['0.8674', '0.8036']
his_acc:  ['0.8674', '0.8497']
Clustering into  3  clusters
Clusters:  [1 0 0 0 0 0 1 2 0 0 2 1 0 0 0 0]
Losses:  7.473459243774414 1.1875345706939697
CurrentTrain: epoch  0, batch     0 | loss: 8.6609936Losses:  11.038259506225586 1.4362761974334717
CurrentTrain: epoch  0, batch     1 | loss: 12.4745359Losses:  10.301067352294922 0.2996116876602173
CurrentTrain: epoch  0, batch     2 | loss: 10.6006794Losses:  4.379742622375488 1.2909667491912842
CurrentTrain: epoch  1, batch     0 | loss: 5.6707096Losses:  4.705720901489258 1.19589102268219
CurrentTrain: epoch  1, batch     1 | loss: 5.9016118Losses:  5.199344158172607 0.41947755217552185
CurrentTrain: epoch  1, batch     2 | loss: 5.6188216Losses:  5.359482765197754 1.1642110347747803
CurrentTrain: epoch  2, batch     0 | loss: 6.5236940Losses:  3.956148862838745 1.0758124589920044
CurrentTrain: epoch  2, batch     1 | loss: 5.0319614Losses:  2.761075496673584 0.7000877261161804
CurrentTrain: epoch  2, batch     2 | loss: 3.4611633Losses:  3.6734118461608887 1.4684367179870605
CurrentTrain: epoch  3, batch     0 | loss: 5.1418486Losses:  4.273281097412109 1.2400459051132202
CurrentTrain: epoch  3, batch     1 | loss: 5.5133271Losses:  4.4232635498046875 0.41308334469795227
CurrentTrain: epoch  3, batch     2 | loss: 4.8363471Losses:  4.21546745300293 1.1557505130767822
CurrentTrain: epoch  4, batch     0 | loss: 5.3712177Losses:  3.3531088829040527 1.2589516639709473
CurrentTrain: epoch  4, batch     1 | loss: 4.6120605Losses:  2.598860263824463 0.22184622287750244
CurrentTrain: epoch  4, batch     2 | loss: 2.8207064Losses:  3.561051845550537 1.2254408597946167
CurrentTrain: epoch  5, batch     0 | loss: 4.7864928Losses:  3.6952097415924072 1.0158816576004028
CurrentTrain: epoch  5, batch     1 | loss: 4.7110915Losses:  2.916496753692627 0.39716559648513794
CurrentTrain: epoch  5, batch     2 | loss: 3.3136623Losses:  3.5108814239501953 1.0238511562347412
CurrentTrain: epoch  6, batch     0 | loss: 4.5347328Losses:  3.0932416915893555 1.1163363456726074
CurrentTrain: epoch  6, batch     1 | loss: 4.2095780Losses:  2.9018731117248535 0.29373887181282043
CurrentTrain: epoch  6, batch     2 | loss: 3.1956120Losses:  2.804349184036255 1.1288042068481445
CurrentTrain: epoch  7, batch     0 | loss: 3.9331534Losses:  3.260732650756836 1.0406091213226318
CurrentTrain: epoch  7, batch     1 | loss: 4.3013420Losses:  1.997166633605957 0.6036990284919739
CurrentTrain: epoch  7, batch     2 | loss: 2.6008656Losses:  2.785529375076294 1.0283725261688232
CurrentTrain: epoch  8, batch     0 | loss: 3.8139019Losses:  2.6856350898742676 0.9801930785179138
CurrentTrain: epoch  8, batch     1 | loss: 3.6658282Losses:  4.069082260131836 0.8696795105934143
CurrentTrain: epoch  8, batch     2 | loss: 4.9387617Losses:  2.4981143474578857 0.8123561143875122
CurrentTrain: epoch  9, batch     0 | loss: 3.3104706Losses:  2.775320529937744 0.9645962715148926
CurrentTrain: epoch  9, batch     1 | loss: 3.7399168Losses:  4.0254082679748535 0.4087581932544708
CurrentTrain: epoch  9, batch     2 | loss: 4.4341664
Losses:  5.012589454650879 1.1567440032958984
MemoryTrain:  epoch  0, batch     0 | loss: 6.1693335Losses:  0.819276750087738 1.2532094717025757
MemoryTrain:  epoch  1, batch     0 | loss: 2.0724862Losses:  0.7356670498847961 1.1470144987106323
MemoryTrain:  epoch  2, batch     0 | loss: 1.8826816Losses:  0.6479336023330688 1.2478854656219482
MemoryTrain:  epoch  3, batch     0 | loss: 1.8958191Losses:  0.720816433429718 1.1670277118682861
MemoryTrain:  epoch  4, batch     0 | loss: 1.8878441Losses:  0.7978945970535278 1.2155935764312744
MemoryTrain:  epoch  5, batch     0 | loss: 2.0134883Losses:  0.8154780864715576 1.183092713356018
MemoryTrain:  epoch  6, batch     0 | loss: 1.9985708Losses:  0.7599653005599976 1.1340456008911133
MemoryTrain:  epoch  7, batch     0 | loss: 1.8940109Losses:  0.7203071117401123 1.119351863861084
MemoryTrain:  epoch  8, batch     0 | loss: 1.8396590Losses:  0.7555058002471924 1.1640002727508545
MemoryTrain:  epoch  9, batch     0 | loss: 1.9195061
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 46.09%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.61%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 79.01%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 78.91%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 79.32%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 77.93%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 77.93%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 76.75%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 75.98%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 75.60%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 74.65%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 73.26%   
cur_acc:  ['0.8674', '0.8036', '0.4609']
his_acc:  ['0.8674', '0.8497', '0.7326']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0]
Losses:  6.627874374389648 1.48884117603302
CurrentTrain: epoch  0, batch     0 | loss: 8.1167154Losses:  9.032846450805664 1.466507077217102
CurrentTrain: epoch  0, batch     1 | loss: 10.4993534Losses:  7.537303447723389 1.1490305662155151
CurrentTrain: epoch  0, batch     2 | loss: 8.6863337Losses:  2.9661130905151367 1.3731111288070679
CurrentTrain: epoch  1, batch     0 | loss: 4.3392243Losses:  2.577198028564453 1.3090314865112305
CurrentTrain: epoch  1, batch     1 | loss: 3.8862295Losses:  2.94584321975708 0.7699912190437317
CurrentTrain: epoch  1, batch     2 | loss: 3.7158344Losses:  2.602222442626953 1.0811302661895752
CurrentTrain: epoch  2, batch     0 | loss: 3.6833527Losses:  2.5373330116271973 0.9240328073501587
CurrentTrain: epoch  2, batch     1 | loss: 3.4613657Losses:  2.6528618335723877 0.9973301291465759
CurrentTrain: epoch  2, batch     2 | loss: 3.6501920Losses:  2.3616244792938232 1.0562171936035156
CurrentTrain: epoch  3, batch     0 | loss: 3.4178417Losses:  2.1376101970672607 1.1451871395111084
CurrentTrain: epoch  3, batch     1 | loss: 3.2827973Losses:  2.628842353820801 0.7977703213691711
CurrentTrain: epoch  3, batch     2 | loss: 3.4266126Losses:  2.0501232147216797 0.9544392824172974
CurrentTrain: epoch  4, batch     0 | loss: 3.0045624Losses:  2.2303452491760254 1.1190834045410156
CurrentTrain: epoch  4, batch     1 | loss: 3.3494287Losses:  1.95912766456604 0.7953592538833618
CurrentTrain: epoch  4, batch     2 | loss: 2.7544870Losses:  1.7096965312957764 1.0302276611328125
CurrentTrain: epoch  5, batch     0 | loss: 2.7399242Losses:  2.3354320526123047 0.8577161431312561
CurrentTrain: epoch  5, batch     1 | loss: 3.1931481Losses:  1.6843327283859253 0.5935779213905334
CurrentTrain: epoch  5, batch     2 | loss: 2.2779107Losses:  1.7399102449417114 0.8844584226608276
CurrentTrain: epoch  6, batch     0 | loss: 2.6243687Losses:  1.692537784576416 0.9241381883621216
CurrentTrain: epoch  6, batch     1 | loss: 2.6166759Losses:  2.232231616973877 0.49604231119155884
CurrentTrain: epoch  6, batch     2 | loss: 2.7282739Losses:  1.6838927268981934 0.8309858441352844
CurrentTrain: epoch  7, batch     0 | loss: 2.5148785Losses:  1.9359720945358276 0.6757872700691223
CurrentTrain: epoch  7, batch     1 | loss: 2.6117594Losses:  1.5584790706634521 0.7352195382118225
CurrentTrain: epoch  7, batch     2 | loss: 2.2936985Losses:  1.6199387311935425 1.063040018081665
CurrentTrain: epoch  8, batch     0 | loss: 2.6829786Losses:  1.6160986423492432 0.696858286857605
CurrentTrain: epoch  8, batch     1 | loss: 2.3129568Losses:  1.8350435495376587 0.5233826041221619
CurrentTrain: epoch  8, batch     2 | loss: 2.3584261Losses:  1.696634292602539 0.5828059911727905
CurrentTrain: epoch  9, batch     0 | loss: 2.2794404Losses:  1.5581094026565552 0.6822901964187622
CurrentTrain: epoch  9, batch     1 | loss: 2.2403996Losses:  1.4489538669586182 0.6954273581504822
CurrentTrain: epoch  9, batch     2 | loss: 2.1443813
Losses:  4.812698841094971 1.0573451519012451
MemoryTrain:  epoch  0, batch     0 | loss: 5.8700438Losses:  8.922233581542969 0.470745325088501
MemoryTrain:  epoch  0, batch     1 | loss: 9.3929787Losses:  0.34197619557380676 0.8651068806648254
MemoryTrain:  epoch  1, batch     0 | loss: 1.2070831Losses:  0.4501360356807709 0.4506954252719879
MemoryTrain:  epoch  1, batch     1 | loss: 0.9008315Losses:  0.3997185230255127 1.0940665006637573
MemoryTrain:  epoch  2, batch     0 | loss: 1.4937850Losses:  0.18302540481090546 0.29937201738357544
MemoryTrain:  epoch  2, batch     1 | loss: 0.4823974Losses:  0.404824435710907 0.9848423600196838
MemoryTrain:  epoch  3, batch     0 | loss: 1.3896668Losses:  0.3797861635684967 0.4217894673347473
MemoryTrain:  epoch  3, batch     1 | loss: 0.8015757Losses:  0.36119580268859863 0.9312481880187988
MemoryTrain:  epoch  4, batch     0 | loss: 1.2924440Losses:  0.4286623001098633 0.47749438881874084
MemoryTrain:  epoch  4, batch     1 | loss: 0.9061567Losses:  0.3967795968055725 1.0243513584136963
MemoryTrain:  epoch  5, batch     0 | loss: 1.4211309Losses:  0.3346564769744873 0.31672540307044983
MemoryTrain:  epoch  5, batch     1 | loss: 0.6513819Losses:  0.379011332988739 0.9876120090484619
MemoryTrain:  epoch  6, batch     0 | loss: 1.3666234Losses:  0.4049927294254303 0.46770739555358887
MemoryTrain:  epoch  6, batch     1 | loss: 0.8727001Losses:  0.37567079067230225 0.9979017376899719
MemoryTrain:  epoch  7, batch     0 | loss: 1.3735726Losses:  0.4139917492866516 0.49137791991233826
MemoryTrain:  epoch  7, batch     1 | loss: 0.9053696Losses:  0.3715246915817261 1.0536682605743408
MemoryTrain:  epoch  8, batch     0 | loss: 1.4251930Losses:  0.2486686259508133 0.28210946917533875
MemoryTrain:  epoch  8, batch     1 | loss: 0.5307781Losses:  0.35567018389701843 1.0829542875289917
MemoryTrain:  epoch  9, batch     0 | loss: 1.4386245Losses:  0.3040180802345276 0.310207337141037
MemoryTrain:  epoch  9, batch     1 | loss: 0.6142254
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 77.40%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.77%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 77.22%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 75.54%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 67.26%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 66.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 65.20%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 64.42%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 63.33%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.66%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 65.13%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 64.96%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 64.92%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 66.10%   
cur_acc:  ['0.8674', '0.8036', '0.4609', '0.7740']
his_acc:  ['0.8674', '0.8497', '0.7326', '0.6610']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0]
Losses:  6.87677001953125 1.1900030374526978
CurrentTrain: epoch  0, batch     0 | loss: 8.0667734Losses:  11.638575553894043 1.594815969467163
CurrentTrain: epoch  0, batch     1 | loss: 13.2333918Losses:  9.65673828125 1.0722635984420776
CurrentTrain: epoch  0, batch     2 | loss: 10.7290020Losses:  3.850295305252075 1.4133002758026123
CurrentTrain: epoch  1, batch     0 | loss: 5.2635956Losses:  3.3415660858154297 1.3046009540557861
CurrentTrain: epoch  1, batch     1 | loss: 4.6461668Losses:  4.0757246017456055 1.1111249923706055
CurrentTrain: epoch  1, batch     2 | loss: 5.1868496Losses:  4.229231834411621 1.4037699699401855
CurrentTrain: epoch  2, batch     0 | loss: 5.6330018Losses:  3.7602992057800293 1.3564430475234985
CurrentTrain: epoch  2, batch     1 | loss: 5.1167421Losses:  2.439330577850342 1.1070241928100586
CurrentTrain: epoch  2, batch     2 | loss: 3.5463548Losses:  3.557804584503174 1.3623532056808472
CurrentTrain: epoch  3, batch     0 | loss: 4.9201579Losses:  3.6934738159179688 1.320819616317749
CurrentTrain: epoch  3, batch     1 | loss: 5.0142937Losses:  2.4145960807800293 1.2167280912399292
CurrentTrain: epoch  3, batch     2 | loss: 3.6313243Losses:  2.547400951385498 1.2979487180709839
CurrentTrain: epoch  4, batch     0 | loss: 3.8453498Losses:  3.577376365661621 1.2205665111541748
CurrentTrain: epoch  4, batch     1 | loss: 4.7979431Losses:  3.0185275077819824 0.9464827179908752
CurrentTrain: epoch  4, batch     2 | loss: 3.9650102Losses:  2.825608491897583 1.0728719234466553
CurrentTrain: epoch  5, batch     0 | loss: 3.8984804Losses:  2.8983993530273438 1.119170069694519
CurrentTrain: epoch  5, batch     1 | loss: 4.0175695Losses:  2.9660804271698 1.2361879348754883
CurrentTrain: epoch  5, batch     2 | loss: 4.2022686Losses:  2.9910311698913574 1.1840670108795166
CurrentTrain: epoch  6, batch     0 | loss: 4.1750984Losses:  2.8222455978393555 1.2289798259735107
CurrentTrain: epoch  6, batch     1 | loss: 4.0512257Losses:  2.577190399169922 1.0413264036178589
CurrentTrain: epoch  6, batch     2 | loss: 3.6185169Losses:  2.4737110137939453 1.21711003780365
CurrentTrain: epoch  7, batch     0 | loss: 3.6908212Losses:  3.054813861846924 1.0512722730636597
CurrentTrain: epoch  7, batch     1 | loss: 4.1060863Losses:  2.389479160308838 1.1401047706604004
CurrentTrain: epoch  7, batch     2 | loss: 3.5295839Losses:  2.8793845176696777 0.9774543046951294
CurrentTrain: epoch  8, batch     0 | loss: 3.8568387Losses:  2.0229361057281494 1.069192886352539
CurrentTrain: epoch  8, batch     1 | loss: 3.0921290Losses:  2.7147748470306396 1.2997461557388306
CurrentTrain: epoch  8, batch     2 | loss: 4.0145211Losses:  2.2818174362182617 1.072758436203003
CurrentTrain: epoch  9, batch     0 | loss: 3.3545759Losses:  2.6936869621276855 1.0068143606185913
CurrentTrain: epoch  9, batch     1 | loss: 3.7005014Losses:  2.2436108589172363 0.9246488809585571
CurrentTrain: epoch  9, batch     2 | loss: 3.1682596
Losses:  4.907865047454834 1.027209758758545
MemoryTrain:  epoch  0, batch     0 | loss: 5.9350748Losses:  9.387303352355957 0.8266809582710266
MemoryTrain:  epoch  0, batch     1 | loss: 10.2139845Losses:  0.48371219635009766 1.0022497177124023
MemoryTrain:  epoch  1, batch     0 | loss: 1.4859619Losses:  0.4650540053844452 0.91445392370224
MemoryTrain:  epoch  1, batch     1 | loss: 1.3795079Losses:  0.5290194749832153 1.0310816764831543
MemoryTrain:  epoch  2, batch     0 | loss: 1.5601012Losses:  0.482211172580719 0.7679085731506348
MemoryTrain:  epoch  2, batch     1 | loss: 1.2501197Losses:  0.4700624644756317 1.0743556022644043
MemoryTrain:  epoch  3, batch     0 | loss: 1.5444181Losses:  0.5028244256973267 0.7976356148719788
MemoryTrain:  epoch  3, batch     1 | loss: 1.3004601Losses:  0.54482501745224 1.0619966983795166
MemoryTrain:  epoch  4, batch     0 | loss: 1.6068218Losses:  0.40466737747192383 0.8148213624954224
MemoryTrain:  epoch  4, batch     1 | loss: 1.2194887Losses:  0.48739591240882874 1.0920002460479736
MemoryTrain:  epoch  5, batch     0 | loss: 1.5793961Losses:  0.4202732741832733 0.6477638483047485
MemoryTrain:  epoch  5, batch     1 | loss: 1.0680372Losses:  0.44516128301620483 1.1426169872283936
MemoryTrain:  epoch  6, batch     0 | loss: 1.5877783Losses:  0.38544389605522156 0.7098320722579956
MemoryTrain:  epoch  6, batch     1 | loss: 1.0952760Losses:  0.4413165748119354 0.9791460037231445
MemoryTrain:  epoch  7, batch     0 | loss: 1.4204626Losses:  0.4667087495326996 0.8525756001472473
MemoryTrain:  epoch  7, batch     1 | loss: 1.3192843Losses:  0.6018860340118408 1.2309045791625977
MemoryTrain:  epoch  8, batch     0 | loss: 1.8327906Losses:  0.45551690459251404 0.708110511302948
MemoryTrain:  epoch  8, batch     1 | loss: 1.1636274Losses:  0.44437599182128906 1.1100660562515259
MemoryTrain:  epoch  9, batch     0 | loss: 1.5544420Losses:  0.5482658743858337 0.7680328488349915
MemoryTrain:  epoch  9, batch     1 | loss: 1.3162987
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 30.36%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 42.79%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 53.69%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 75.74%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 74.29%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 68.91%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 67.47%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 66.92%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 66.72%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 65.83%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 64.67%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 64.41%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 62.13%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 61.42%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 60.38%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 60.76%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 62.61%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 60.70%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 59.79%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 59.02%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 58.17%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 57.34%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 56.93%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 57.86%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 57.93%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 58.00%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 57.79%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 56.96%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 56.16%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 55.56%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 55.22%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 55.25%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 55.18%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 55.36%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 55.29%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 55.54%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 55.78%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 55.94%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 56.17%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 56.40%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 56.77%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 56.91%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 56.98%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 57.18%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 56.68%   
cur_acc:  ['0.8674', '0.8036', '0.4609', '0.7740', '0.5369']
his_acc:  ['0.8674', '0.8497', '0.7326', '0.6610', '0.5668']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0]
Losses:  6.177613258361816 1.086733341217041
CurrentTrain: epoch  0, batch     0 | loss: 7.2643466Losses:  8.92314338684082 1.3024814128875732
CurrentTrain: epoch  0, batch     1 | loss: 10.2256250Losses:  9.58242130279541 1.136565923690796
CurrentTrain: epoch  0, batch     2 | loss: 10.7189875Losses:  10.23973274230957 0.4765492379665375
CurrentTrain: epoch  0, batch     3 | loss: 10.7162819Losses:  3.368656635284424 1.242354393005371
CurrentTrain: epoch  1, batch     0 | loss: 4.6110110Losses:  2.082061767578125 1.1253857612609863
CurrentTrain: epoch  1, batch     1 | loss: 3.2074475Losses:  2.419814109802246 1.0993404388427734
CurrentTrain: epoch  1, batch     2 | loss: 3.5191545Losses:  1.5101406574249268 0.20407770574092865
CurrentTrain: epoch  1, batch     3 | loss: 1.7142184Losses:  2.3403029441833496 1.178110122680664
CurrentTrain: epoch  2, batch     0 | loss: 3.5184131Losses:  1.846697211265564 1.2031852006912231
CurrentTrain: epoch  2, batch     1 | loss: 3.0498824Losses:  2.4938201904296875 0.9036095142364502
CurrentTrain: epoch  2, batch     2 | loss: 3.3974297Losses:  2.628047466278076 0.11120371520519257
CurrentTrain: epoch  2, batch     3 | loss: 2.7392511Losses:  1.8579890727996826 0.9743657112121582
CurrentTrain: epoch  3, batch     0 | loss: 2.8323548Losses:  1.959600806236267 1.2185314893722534
CurrentTrain: epoch  3, batch     1 | loss: 3.1781323Losses:  2.447390079498291 1.0953611135482788
CurrentTrain: epoch  3, batch     2 | loss: 3.5427513Losses:  2.9783949851989746 0.3374902009963989
CurrentTrain: epoch  3, batch     3 | loss: 3.3158851Losses:  2.0346860885620117 1.190171241760254
CurrentTrain: epoch  4, batch     0 | loss: 3.2248573Losses:  1.960159420967102 1.0225247144699097
CurrentTrain: epoch  4, batch     1 | loss: 2.9826841Losses:  1.9390933513641357 0.9316343069076538
CurrentTrain: epoch  4, batch     2 | loss: 2.8707275Losses:  3.0359950065612793 0.14513461291790009
CurrentTrain: epoch  4, batch     3 | loss: 3.1811297Losses:  1.8111519813537598 0.9932806491851807
CurrentTrain: epoch  5, batch     0 | loss: 2.8044326Losses:  1.6789690256118774 0.9685136079788208
CurrentTrain: epoch  5, batch     1 | loss: 2.6474826Losses:  2.198913097381592 1.00002121925354
CurrentTrain: epoch  5, batch     2 | loss: 3.1989343Losses:  1.366396427154541 0.3091576099395752
CurrentTrain: epoch  5, batch     3 | loss: 1.6755540Losses:  1.9523537158966064 1.0971534252166748
CurrentTrain: epoch  6, batch     0 | loss: 3.0495071Losses:  1.6369327306747437 0.9212062954902649
CurrentTrain: epoch  6, batch     1 | loss: 2.5581391Losses:  2.1043856143951416 0.8812214136123657
CurrentTrain: epoch  6, batch     2 | loss: 2.9856071Losses:  1.275264024734497 0.16164420545101166
CurrentTrain: epoch  6, batch     3 | loss: 1.4369082Losses:  2.0306520462036133 0.8786725401878357
CurrentTrain: epoch  7, batch     0 | loss: 2.9093246Losses:  1.5468040704727173 0.8913432359695435
CurrentTrain: epoch  7, batch     1 | loss: 2.4381473Losses:  1.629921793937683 1.0341241359710693
CurrentTrain: epoch  7, batch     2 | loss: 2.6640458Losses:  1.4278767108917236 0.2514666020870209
CurrentTrain: epoch  7, batch     3 | loss: 1.6793433Losses:  1.7386572360992432 1.0282642841339111
CurrentTrain: epoch  8, batch     0 | loss: 2.7669215Losses:  1.6192891597747803 0.7610073089599609
CurrentTrain: epoch  8, batch     1 | loss: 2.3802965Losses:  1.2065513134002686 0.9900771379470825
CurrentTrain: epoch  8, batch     2 | loss: 2.1966286Losses:  2.640475034713745 0.05637785419821739
CurrentTrain: epoch  8, batch     3 | loss: 2.6968529Losses:  1.2940115928649902 0.9232130646705627
CurrentTrain: epoch  9, batch     0 | loss: 2.2172246Losses:  1.6406769752502441 0.8289150595664978
CurrentTrain: epoch  9, batch     1 | loss: 2.4695921Losses:  1.6437604427337646 1.0410994291305542
CurrentTrain: epoch  9, batch     2 | loss: 2.6848598Losses:  1.337572455406189 0.4066837430000305
CurrentTrain: epoch  9, batch     3 | loss: 1.7442563
Losses:  4.928101062774658 0.9275226593017578
MemoryTrain:  epoch  0, batch     0 | loss: 5.8556237Losses:  8.908438682556152 1.1135207414627075
MemoryTrain:  epoch  0, batch     1 | loss: 10.0219593Losses:  0.43000468611717224 0.9324598908424377
MemoryTrain:  epoch  1, batch     0 | loss: 1.3624645Losses:  0.38127389550209045 1.0458842515945435
MemoryTrain:  epoch  1, batch     1 | loss: 1.4271581Losses:  0.4928801655769348 1.0149855613708496
MemoryTrain:  epoch  2, batch     0 | loss: 1.5078657Losses:  0.46656671166419983 0.9595630168914795
MemoryTrain:  epoch  2, batch     1 | loss: 1.4261297Losses:  0.4266235828399658 0.9408696889877319
MemoryTrain:  epoch  3, batch     0 | loss: 1.3674933Losses:  0.44831618666648865 1.0496028661727905
MemoryTrain:  epoch  3, batch     1 | loss: 1.4979191Losses:  0.413918137550354 0.9648337960243225
MemoryTrain:  epoch  4, batch     0 | loss: 1.3787520Losses:  0.4145458936691284 0.9444660544395447
MemoryTrain:  epoch  4, batch     1 | loss: 1.3590119Losses:  0.36327996850013733 0.9021766781806946
MemoryTrain:  epoch  5, batch     0 | loss: 1.2654567Losses:  0.4200788140296936 1.03752601146698
MemoryTrain:  epoch  5, batch     1 | loss: 1.4576049Losses:  0.4934168756008148 1.0831068754196167
MemoryTrain:  epoch  6, batch     0 | loss: 1.5765238Losses:  0.32815247774124146 0.829261302947998
MemoryTrain:  epoch  6, batch     1 | loss: 1.1574137Losses:  0.4198414981365204 1.0626262426376343
MemoryTrain:  epoch  7, batch     0 | loss: 1.4824678Losses:  0.3859139680862427 0.8615971803665161
MemoryTrain:  epoch  7, batch     1 | loss: 1.2475111Losses:  0.4433302879333496 1.1307034492492676
MemoryTrain:  epoch  8, batch     0 | loss: 1.5740337Losses:  0.37692791223526 0.8231399059295654
MemoryTrain:  epoch  8, batch     1 | loss: 1.2000678Losses:  0.40773341059684753 1.006026029586792
MemoryTrain:  epoch  9, batch     0 | loss: 1.4137595Losses:  0.411079466342926 0.9245157837867737
MemoryTrain:  epoch  9, batch     1 | loss: 1.3355953
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 76.47%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 70.07%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 65.18%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 62.87%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 62.14%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 61.08%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 61.55%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 60.62%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 59.03%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 58.89%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 58.94%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 59.00%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 58.86%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 58.55%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 58.06%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 57.23%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 56.43%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 55.64%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 55.31%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 54.98%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 55.00%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 54.77%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 54.71%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 54.73%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 54.91%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 55.16%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 55.32%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 55.80%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 56.10%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 56.18%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 56.10%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 56.03%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 56.46%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 56.95%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 57.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.90%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.67%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 59.73%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 59.44%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.79%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 60.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 60.52%   
cur_acc:  ['0.8674', '0.8036', '0.4609', '0.7740', '0.5369', '0.8750']
his_acc:  ['0.8674', '0.8497', '0.7326', '0.6610', '0.5668', '0.6052']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0 1 2 0 0 0]
Losses:  6.233112335205078 1.2313424348831177
CurrentTrain: epoch  0, batch     0 | loss: 7.4644547Losses:  10.08334732055664 1.0955113172531128
CurrentTrain: epoch  0, batch     1 | loss: 11.1788588Losses:  9.737839698791504 1.184765338897705
CurrentTrain: epoch  0, batch     2 | loss: 10.9226055Losses:  8.964962005615234 0.7016806602478027
CurrentTrain: epoch  0, batch     3 | loss: 9.6666431Losses:  2.6749448776245117 0.9273999333381653
CurrentTrain: epoch  1, batch     0 | loss: 3.6023448Losses:  2.279726505279541 1.2671321630477905
CurrentTrain: epoch  1, batch     1 | loss: 3.5468588Losses:  3.15138578414917 1.1202677488327026
CurrentTrain: epoch  1, batch     2 | loss: 4.2716537Losses:  1.9976592063903809 0.9086745381355286
CurrentTrain: epoch  1, batch     3 | loss: 2.9063337Losses:  2.029360055923462 1.0100302696228027
CurrentTrain: epoch  2, batch     0 | loss: 3.0393903Losses:  3.110407829284668 1.2272825241088867
CurrentTrain: epoch  2, batch     1 | loss: 4.3376904Losses:  2.7056989669799805 1.2028321027755737
CurrentTrain: epoch  2, batch     2 | loss: 3.9085312Losses:  1.760733723640442 0.7458035945892334
CurrentTrain: epoch  2, batch     3 | loss: 2.5065374Losses:  2.430532455444336 1.0132687091827393
CurrentTrain: epoch  3, batch     0 | loss: 3.4438012Losses:  2.58381724357605 1.0897746086120605
CurrentTrain: epoch  3, batch     1 | loss: 3.6735919Losses:  1.78622567653656 1.299574375152588
CurrentTrain: epoch  3, batch     2 | loss: 3.0858002Losses:  2.6321983337402344 0.7358794212341309
CurrentTrain: epoch  3, batch     3 | loss: 3.3680778Losses:  2.0840792655944824 1.2996493577957153
CurrentTrain: epoch  4, batch     0 | loss: 3.3837285Losses:  1.8026365041732788 1.2071247100830078
CurrentTrain: epoch  4, batch     1 | loss: 3.0097613Losses:  2.615196704864502 0.89152991771698
CurrentTrain: epoch  4, batch     2 | loss: 3.5067267Losses:  1.7596569061279297 0.6892286539077759
CurrentTrain: epoch  4, batch     3 | loss: 2.4488854Losses:  2.348865270614624 1.0420960187911987
CurrentTrain: epoch  5, batch     0 | loss: 3.3909612Losses:  1.693988561630249 1.0687024593353271
CurrentTrain: epoch  5, batch     1 | loss: 2.7626910Losses:  2.298795700073242 0.9544718265533447
CurrentTrain: epoch  5, batch     2 | loss: 3.2532675Losses:  1.5223228931427002 0.8126109838485718
CurrentTrain: epoch  5, batch     3 | loss: 2.3349338Losses:  1.7782249450683594 0.9961086511611938
CurrentTrain: epoch  6, batch     0 | loss: 2.7743335Losses:  1.9636592864990234 1.0977436304092407
CurrentTrain: epoch  6, batch     1 | loss: 3.0614028Losses:  1.7727975845336914 0.9914602041244507
CurrentTrain: epoch  6, batch     2 | loss: 2.7642579Losses:  2.7419497966766357 0.6584649085998535
CurrentTrain: epoch  6, batch     3 | loss: 3.4004147Losses:  2.5851237773895264 0.8407207131385803
CurrentTrain: epoch  7, batch     0 | loss: 3.4258444Losses:  1.5526412725448608 1.010869026184082
CurrentTrain: epoch  7, batch     1 | loss: 2.5635104Losses:  1.6390631198883057 0.9860021471977234
CurrentTrain: epoch  7, batch     2 | loss: 2.6250653Losses:  1.366547703742981 0.5287389159202576
CurrentTrain: epoch  7, batch     3 | loss: 1.8952866Losses:  1.6720125675201416 1.0494219064712524
CurrentTrain: epoch  8, batch     0 | loss: 2.7214346Losses:  1.8323040008544922 1.0272482633590698
CurrentTrain: epoch  8, batch     1 | loss: 2.8595524Losses:  1.5497443675994873 0.8827059864997864
CurrentTrain: epoch  8, batch     2 | loss: 2.4324503Losses:  1.914480447769165 0.6500517129898071
CurrentTrain: epoch  8, batch     3 | loss: 2.5645323Losses:  1.756300687789917 0.8391709327697754
CurrentTrain: epoch  9, batch     0 | loss: 2.5954716Losses:  1.629978895187378 0.8805460929870605
CurrentTrain: epoch  9, batch     1 | loss: 2.5105250Losses:  1.7523428201675415 0.9365337491035461
CurrentTrain: epoch  9, batch     2 | loss: 2.6888766Losses:  1.5328459739685059 0.628278374671936
CurrentTrain: epoch  9, batch     3 | loss: 2.1611242
Losses:  5.124765396118164 1.117663025856018
MemoryTrain:  epoch  0, batch     0 | loss: 6.2424283Losses:  9.156169891357422 0.7614130973815918
MemoryTrain:  epoch  0, batch     1 | loss: 9.9175835Losses:  9.814950942993164 0.3326759338378906
MemoryTrain:  epoch  0, batch     2 | loss: 10.1476269Losses:  0.42248496413230896 1.0267360210418701
MemoryTrain:  epoch  1, batch     0 | loss: 1.4492210Losses:  0.4433457553386688 0.8851080536842346
MemoryTrain:  epoch  1, batch     1 | loss: 1.3284538Losses:  0.44993218779563904 0.18862572312355042
MemoryTrain:  epoch  1, batch     2 | loss: 0.6385579Losses:  0.47217193245887756 0.9414405822753906
MemoryTrain:  epoch  2, batch     0 | loss: 1.4136125Losses:  0.4579429030418396 0.9188183546066284
MemoryTrain:  epoch  2, batch     1 | loss: 1.3767612Losses:  0.37475326657295227 0.4247598648071289
MemoryTrain:  epoch  2, batch     2 | loss: 0.7995131Losses:  0.4414432644844055 0.9925336837768555
MemoryTrain:  epoch  3, batch     0 | loss: 1.4339769Losses:  0.4977879226207733 0.8599562048912048
MemoryTrain:  epoch  3, batch     1 | loss: 1.3577441Losses:  0.3428787291049957 0.3609902262687683
MemoryTrain:  epoch  3, batch     2 | loss: 0.7038690Losses:  0.4784938097000122 0.9704161286354065
MemoryTrain:  epoch  4, batch     0 | loss: 1.4489100Losses:  0.4449009895324707 0.9944563508033752
MemoryTrain:  epoch  4, batch     1 | loss: 1.4393573Losses:  0.2900960445404053 0.23193040490150452
MemoryTrain:  epoch  4, batch     2 | loss: 0.5220264Losses:  0.38357627391815186 0.9073629379272461
MemoryTrain:  epoch  5, batch     0 | loss: 1.2909392Losses:  0.4079796075820923 0.8531057834625244
MemoryTrain:  epoch  5, batch     1 | loss: 1.2610854Losses:  0.5829230546951294 0.28944265842437744
MemoryTrain:  epoch  5, batch     2 | loss: 0.8723657Losses:  0.4524104595184326 0.942889928817749
MemoryTrain:  epoch  6, batch     0 | loss: 1.3953004Losses:  0.38383859395980835 0.8537724018096924
MemoryTrain:  epoch  6, batch     1 | loss: 1.2376111Losses:  0.3781331777572632 0.16278818249702454
MemoryTrain:  epoch  6, batch     2 | loss: 0.5409213Losses:  0.3941328227519989 0.8687864542007446
MemoryTrain:  epoch  7, batch     0 | loss: 1.2629193Losses:  0.5009102821350098 1.0121498107910156
MemoryTrain:  epoch  7, batch     1 | loss: 1.5130601Losses:  0.502782940864563 0.18478882312774658
MemoryTrain:  epoch  7, batch     2 | loss: 0.6875718Losses:  0.5229181051254272 0.9957965016365051
MemoryTrain:  epoch  8, batch     0 | loss: 1.5187147Losses:  0.36173710227012634 0.8687025308609009
MemoryTrain:  epoch  8, batch     1 | loss: 1.2304397Losses:  0.32569149136543274 0.23059312999248505
MemoryTrain:  epoch  8, batch     2 | loss: 0.5562846Losses:  0.38024085760116577 0.9172080755233765
MemoryTrain:  epoch  9, batch     0 | loss: 1.2974489Losses:  0.44177359342575073 0.9567248821258545
MemoryTrain:  epoch  9, batch     1 | loss: 1.3984985Losses:  0.34957271814346313 0.31311899423599243
MemoryTrain:  epoch  9, batch     2 | loss: 0.6626917
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 77.92%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.24%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 69.26%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 67.60%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 66.19%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 64.77%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 64.31%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 63.18%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 62.76%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 61.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 60.54%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 59.86%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 59.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 60.64%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 59.91%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 59.11%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 58.54%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 57.68%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.75%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 55.85%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 55.08%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 54.90%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 54.83%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 54.76%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 54.50%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 54.08%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 53.30%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 52.55%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 51.91%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 51.63%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 51.35%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 51.42%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 51.32%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 51.30%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 51.36%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 51.42%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 51.72%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 51.93%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 52.06%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 52.26%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 52.23%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 51.62%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 51.09%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 50.50%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 50.78%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 51.33%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 51.88%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 52.40%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 52.92%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.43%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 53.92%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.41%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 54.69%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 54.38%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 54.08%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 54.48%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 54.94%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 55.38%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 55.58%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 55.70%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 55.95%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 56.54%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 56.83%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 57.18%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 57.45%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 57.84%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 57.94%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 58.04%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 58.08%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 58.28%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 58.42%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 58.24%   
cur_acc:  ['0.8674', '0.8036', '0.4609', '0.7740', '0.5369', '0.8750', '0.7792']
his_acc:  ['0.8674', '0.8497', '0.7326', '0.6610', '0.5668', '0.6052', '0.5824']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 1 3 0 0 3 1 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0 1 0 0 3 1 0 0 0 1
 2 1 0 0]
Losses:  6.407138824462891 1.214455008506775
CurrentTrain: epoch  0, batch     0 | loss: 7.6215940Losses:  9.227523803710938 1.134194016456604
CurrentTrain: epoch  0, batch     1 | loss: 10.3617182Losses:  9.226289749145508 0.9179800748825073
CurrentTrain: epoch  0, batch     2 | loss: 10.1442699Losses:  8.882344245910645 1.005052924156189
CurrentTrain: epoch  0, batch     3 | loss: 9.8873968Losses:  2.4184374809265137 1.1922687292099
CurrentTrain: epoch  1, batch     0 | loss: 3.6107063Losses:  1.7393884658813477 0.9742242097854614
CurrentTrain: epoch  1, batch     1 | loss: 2.7136126Losses:  2.628645420074463 0.8608562350273132
CurrentTrain: epoch  1, batch     2 | loss: 3.4895017Losses:  2.318636417388916 0.9932019710540771
CurrentTrain: epoch  1, batch     3 | loss: 3.3118384Losses:  2.5604872703552246 1.0006285905838013
CurrentTrain: epoch  2, batch     0 | loss: 3.5611157Losses:  2.4933366775512695 0.8528662323951721
CurrentTrain: epoch  2, batch     1 | loss: 3.3462029Losses:  1.4904814958572388 0.9421433806419373
CurrentTrain: epoch  2, batch     2 | loss: 2.4326248Losses:  1.7891582250595093 0.9251081347465515
CurrentTrain: epoch  2, batch     3 | loss: 2.7142663Losses:  2.511355400085449 0.957607626914978
CurrentTrain: epoch  3, batch     0 | loss: 3.4689631Losses:  1.4695082902908325 1.0202875137329102
CurrentTrain: epoch  3, batch     1 | loss: 2.4897957Losses:  2.083832263946533 0.8721559047698975
CurrentTrain: epoch  3, batch     2 | loss: 2.9559882Losses:  1.3972889184951782 0.9198946356773376
CurrentTrain: epoch  3, batch     3 | loss: 2.3171835Losses:  2.086921453475952 0.9194931983947754
CurrentTrain: epoch  4, batch     0 | loss: 3.0064147Losses:  1.7550413608551025 1.0098985433578491
CurrentTrain: epoch  4, batch     1 | loss: 2.7649398Losses:  1.6618870496749878 0.8842850923538208
CurrentTrain: epoch  4, batch     2 | loss: 2.5461721Losses:  1.3677035570144653 0.8390475511550903
CurrentTrain: epoch  4, batch     3 | loss: 2.2067511Losses:  1.2959949970245361 0.9625504016876221
CurrentTrain: epoch  5, batch     0 | loss: 2.2585454Losses:  1.654263973236084 0.8117381930351257
CurrentTrain: epoch  5, batch     1 | loss: 2.4660022Losses:  1.814759373664856 1.0488746166229248
CurrentTrain: epoch  5, batch     2 | loss: 2.8636341Losses:  1.7317564487457275 0.716853678226471
CurrentTrain: epoch  5, batch     3 | loss: 2.4486101Losses:  1.4577884674072266 0.9021011590957642
CurrentTrain: epoch  6, batch     0 | loss: 2.3598895Losses:  1.6110196113586426 0.6776434779167175
CurrentTrain: epoch  6, batch     1 | loss: 2.2886631Losses:  1.7617594003677368 0.9214586615562439
CurrentTrain: epoch  6, batch     2 | loss: 2.6832180Losses:  1.1444863080978394 0.5482577085494995
CurrentTrain: epoch  6, batch     3 | loss: 1.6927440Losses:  1.8909059762954712 0.7024252414703369
CurrentTrain: epoch  7, batch     0 | loss: 2.5933313Losses:  1.4454081058502197 0.8126164674758911
CurrentTrain: epoch  7, batch     1 | loss: 2.2580247Losses:  0.944035530090332 0.7879974246025085
CurrentTrain: epoch  7, batch     2 | loss: 1.7320330Losses:  1.589617371559143 0.7521449327468872
CurrentTrain: epoch  7, batch     3 | loss: 2.3417623Losses:  1.2689465284347534 0.7859641909599304
CurrentTrain: epoch  8, batch     0 | loss: 2.0549107Losses:  1.2811517715454102 0.6831527948379517
CurrentTrain: epoch  8, batch     1 | loss: 1.9643046Losses:  1.2368193864822388 0.7050032019615173
CurrentTrain: epoch  8, batch     2 | loss: 1.9418225Losses:  1.655314326286316 0.7195436954498291
CurrentTrain: epoch  8, batch     3 | loss: 2.3748579Losses:  1.4829490184783936 0.6606104373931885
CurrentTrain: epoch  9, batch     0 | loss: 2.1435595Losses:  0.9928805232048035 0.8390249013900757
CurrentTrain: epoch  9, batch     1 | loss: 1.8319054Losses:  1.578287124633789 0.5875701904296875
CurrentTrain: epoch  9, batch     2 | loss: 2.1658573Losses:  1.0040156841278076 0.7782738208770752
CurrentTrain: epoch  9, batch     3 | loss: 1.7822895
Losses:  5.131788730621338 0.739769458770752
MemoryTrain:  epoch  0, batch     0 | loss: 5.8715582Losses:  8.82313060760498 0.9561260938644409
MemoryTrain:  epoch  0, batch     1 | loss: 9.7792568Losses:  9.739175796508789 0.5832611322402954
MemoryTrain:  epoch  0, batch     2 | loss: 10.3224373Losses:  0.3830919563770294 0.9183276295661926
MemoryTrain:  epoch  1, batch     0 | loss: 1.3014196Losses:  0.4064808189868927 0.7513524293899536
MemoryTrain:  epoch  1, batch     1 | loss: 1.1578332Losses:  0.5894966125488281 0.5974397659301758
MemoryTrain:  epoch  1, batch     2 | loss: 1.1869364Losses:  0.4264144003391266 0.8108866810798645
MemoryTrain:  epoch  2, batch     0 | loss: 1.2373011Losses:  0.45279639959335327 0.9396752119064331
MemoryTrain:  epoch  2, batch     1 | loss: 1.3924716Losses:  0.3339042067527771 0.42910805344581604
MemoryTrain:  epoch  2, batch     2 | loss: 0.7630123Losses:  0.37059614062309265 0.9780021905899048
MemoryTrain:  epoch  3, batch     0 | loss: 1.3485984Losses:  0.5010781288146973 0.8801435232162476
MemoryTrain:  epoch  3, batch     1 | loss: 1.3812217Losses:  0.40688633918762207 0.5851496458053589
MemoryTrain:  epoch  3, batch     2 | loss: 0.9920360Losses:  0.4055022597312927 0.7830793857574463
MemoryTrain:  epoch  4, batch     0 | loss: 1.1885817Losses:  0.40270480513572693 0.8825346231460571
MemoryTrain:  epoch  4, batch     1 | loss: 1.2852395Losses:  0.41383859515190125 0.44211292266845703
MemoryTrain:  epoch  4, batch     2 | loss: 0.8559515Losses:  0.4129287600517273 0.8151489496231079
MemoryTrain:  epoch  5, batch     0 | loss: 1.2280777Losses:  0.4290275573730469 0.96410071849823
MemoryTrain:  epoch  5, batch     1 | loss: 1.3931283Losses:  0.37761762738227844 0.4607054889202118
MemoryTrain:  epoch  5, batch     2 | loss: 0.8383231Losses:  0.4618210196495056 0.8440715074539185
MemoryTrain:  epoch  6, batch     0 | loss: 1.3058925Losses:  0.3838399648666382 0.8147976398468018
MemoryTrain:  epoch  6, batch     1 | loss: 1.1986376Losses:  0.4484480321407318 0.49280208349227905
MemoryTrain:  epoch  6, batch     2 | loss: 0.9412501Losses:  0.3932047188282013 0.8085039854049683
MemoryTrain:  epoch  7, batch     0 | loss: 1.2017087Losses:  0.4079129099845886 0.8182187080383301
MemoryTrain:  epoch  7, batch     1 | loss: 1.2261317Losses:  0.4211074411869049 0.5644472241401672
MemoryTrain:  epoch  7, batch     2 | loss: 0.9855547Losses:  0.4192325472831726 0.8453904390335083
MemoryTrain:  epoch  8, batch     0 | loss: 1.2646229Losses:  0.4025925397872925 0.7803337574005127
MemoryTrain:  epoch  8, batch     1 | loss: 1.1829263Losses:  0.3742397129535675 0.5324689149856567
MemoryTrain:  epoch  8, batch     2 | loss: 0.9067086Losses:  0.3344380259513855 0.7096973657608032
MemoryTrain:  epoch  9, batch     0 | loss: 1.0441353Losses:  0.3702573776245117 0.886775016784668
MemoryTrain:  epoch  9, batch     1 | loss: 1.2570324Losses:  0.44029682874679565 0.4722278416156769
MemoryTrain:  epoch  9, batch     2 | loss: 0.9125247
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 86.11%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 21.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 31.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 51.92%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.51%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 71.21%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 69.67%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 65.80%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 62.34%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 60.90%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 60.31%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 59.45%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 59.23%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 58.72%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 57.53%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 56.81%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 55.57%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 55.59%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 56.51%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 55.61%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 54.75%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 53.80%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 53.25%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 52.36%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 52.16%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 52.23%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 52.41%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 51.83%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 51.17%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 50.83%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 50.10%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 49.29%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 48.61%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 48.05%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 47.31%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 46.88%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 46.92%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 46.78%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 46.47%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 45.80%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 45.16%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 44.62%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 44.35%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 44.17%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 44.25%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 44.08%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 44.16%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 44.31%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 44.46%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 44.77%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 45.06%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 45.27%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 45.56%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 45.61%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 45.07%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 44.55%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 44.04%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 44.39%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 45.01%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 45.62%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 46.22%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 46.81%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 47.38%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 47.94%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 48.49%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 48.83%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 48.39%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 48.15%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 48.61%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 49.12%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 49.57%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 49.57%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 49.45%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 49.34%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 49.40%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 49.53%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 49.88%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 50.29%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 50.75%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 51.19%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 51.35%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 51.00%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 50.77%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 50.38%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 49.95%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 50.05%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 50.43%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 50.64%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 51.00%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 51.20%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 51.45%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 51.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 52.13%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 52.27%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 52.45%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 52.68%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 53.00%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 53.37%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 53.73%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 54.09%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 54.44%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 54.78%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 54.84%   
cur_acc:  ['0.8674', '0.8036', '0.4609', '0.7740', '0.5369', '0.8750', '0.7792', '0.8611']
his_acc:  ['0.8674', '0.8497', '0.7326', '0.6610', '0.5668', '0.6052', '0.5824', '0.5484']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.187501907348633 1.601911187171936
CurrentTrain: epoch  0, batch     0 | loss: 13.7894135Losses:  13.572486877441406 1.7025686502456665
CurrentTrain: epoch  0, batch     1 | loss: 15.2750559Losses:  14.686980247497559 1.9468410015106201
CurrentTrain: epoch  0, batch     2 | loss: 16.6338215Losses:  14.50394058227539 0.926012396812439
CurrentTrain: epoch  0, batch     3 | loss: 15.4299526Losses:  14.852020263671875 1.7320154905319214
CurrentTrain: epoch  0, batch     4 | loss: 16.5840359Losses:  14.584939956665039 1.3219749927520752
CurrentTrain: epoch  0, batch     5 | loss: 15.9069147Losses:  14.430516242980957 1.9251072406768799
CurrentTrain: epoch  0, batch     6 | loss: 16.3556232Losses:  14.711616516113281 1.1677179336547852
CurrentTrain: epoch  0, batch     7 | loss: 15.8793344Losses:  14.205677032470703 1.420594334602356
CurrentTrain: epoch  0, batch     8 | loss: 15.6262712Losses:  14.566125869750977 1.5492202043533325
CurrentTrain: epoch  0, batch     9 | loss: 16.1153469Losses:  13.953519821166992 1.5038014650344849
CurrentTrain: epoch  0, batch    10 | loss: 15.4573212Losses:  13.622058868408203 1.3236253261566162
CurrentTrain: epoch  0, batch    11 | loss: 14.9456844Losses:  13.583511352539062 1.1626845598220825
CurrentTrain: epoch  0, batch    12 | loss: 14.7461958Losses:  13.11435317993164 0.5029946565628052
CurrentTrain: epoch  0, batch    13 | loss: 13.6173477Losses:  13.519577980041504 1.3571770191192627
CurrentTrain: epoch  0, batch    14 | loss: 14.8767548Losses:  13.740646362304688 1.615996241569519
CurrentTrain: epoch  0, batch    15 | loss: 15.3566427Losses:  13.394720077514648 1.4339666366577148
CurrentTrain: epoch  0, batch    16 | loss: 14.8286867Losses:  13.331049919128418 1.0934935808181763
CurrentTrain: epoch  0, batch    17 | loss: 14.4245434Losses:  13.66632080078125 1.4305601119995117
CurrentTrain: epoch  0, batch    18 | loss: 15.0968809Losses:  12.722012519836426 1.3265043497085571
CurrentTrain: epoch  0, batch    19 | loss: 14.0485172Losses:  12.973885536193848 1.455970048904419
CurrentTrain: epoch  0, batch    20 | loss: 14.4298553Losses:  13.044240951538086 1.2139241695404053
CurrentTrain: epoch  0, batch    21 | loss: 14.2581654Losses:  12.704200744628906 1.1134308576583862
CurrentTrain: epoch  0, batch    22 | loss: 13.8176317Losses:  13.03849983215332 1.386960744857788
CurrentTrain: epoch  0, batch    23 | loss: 14.4254608Losses:  12.502408027648926 1.2451703548431396
CurrentTrain: epoch  0, batch    24 | loss: 13.7475786Losses:  12.017715454101562 1.1108980178833008
CurrentTrain: epoch  0, batch    25 | loss: 13.1286135Losses:  12.571279525756836 1.2084803581237793
CurrentTrain: epoch  0, batch    26 | loss: 13.7797604Losses:  12.42242431640625 1.4845740795135498
CurrentTrain: epoch  0, batch    27 | loss: 13.9069986Losses:  11.68784236907959 1.0136611461639404
CurrentTrain: epoch  0, batch    28 | loss: 12.7015038Losses:  11.719474792480469 1.005924940109253
CurrentTrain: epoch  0, batch    29 | loss: 12.7254000Losses:  11.885603904724121 1.129868984222412
CurrentTrain: epoch  0, batch    30 | loss: 13.0154724Losses:  11.808131217956543 1.220468521118164
CurrentTrain: epoch  0, batch    31 | loss: 13.0285997Losses:  11.84216594696045 1.0128997564315796
CurrentTrain: epoch  0, batch    32 | loss: 12.8550653Losses:  11.535568237304688 1.2083462476730347
CurrentTrain: epoch  0, batch    33 | loss: 12.7439146Losses:  11.009588241577148 1.5193407535552979
CurrentTrain: epoch  0, batch    34 | loss: 12.5289288Losses:  11.413716316223145 1.1528481245040894
CurrentTrain: epoch  0, batch    35 | loss: 12.5665646Losses:  11.447757720947266 1.1488410234451294
CurrentTrain: epoch  0, batch    36 | loss: 12.5965986Losses:  11.121220588684082 1.28225839138031
CurrentTrain: epoch  0, batch    37 | loss: 12.4034786Losses:  10.957980155944824 1.292968988418579
CurrentTrain: epoch  0, batch    38 | loss: 12.2509489Losses:  10.764118194580078 0.8083463311195374
CurrentTrain: epoch  0, batch    39 | loss: 11.5724649Losses:  11.068303108215332 1.1799662113189697
CurrentTrain: epoch  0, batch    40 | loss: 12.2482691Losses:  10.889246940612793 1.3197097778320312
CurrentTrain: epoch  0, batch    41 | loss: 12.2089567Losses:  10.754032135009766 1.1841697692871094
CurrentTrain: epoch  0, batch    42 | loss: 11.9382019Losses:  10.607024192810059 0.7415692210197449
CurrentTrain: epoch  0, batch    43 | loss: 11.3485937Losses:  10.244211196899414 0.8392834663391113
CurrentTrain: epoch  0, batch    44 | loss: 11.0834942Losses:  10.073652267456055 1.1066594123840332
CurrentTrain: epoch  0, batch    45 | loss: 11.1803112Losses:  9.970499038696289 1.2762253284454346
CurrentTrain: epoch  0, batch    46 | loss: 11.2467241Losses:  10.674013137817383 0.7367323637008667
CurrentTrain: epoch  0, batch    47 | loss: 11.4107456Losses:  9.737404823303223 0.9537410140037537
CurrentTrain: epoch  0, batch    48 | loss: 10.6911459Losses:  10.306746482849121 1.1024119853973389
CurrentTrain: epoch  0, batch    49 | loss: 11.4091587Losses:  9.865797996520996 1.210819125175476
CurrentTrain: epoch  0, batch    50 | loss: 11.0766172Losses:  9.385814666748047 1.2042312622070312
CurrentTrain: epoch  0, batch    51 | loss: 10.5900459Losses:  9.646474838256836 1.1531498432159424
CurrentTrain: epoch  0, batch    52 | loss: 10.7996244Losses:  9.514360427856445 0.9722390174865723
CurrentTrain: epoch  0, batch    53 | loss: 10.4865990Losses:  9.239487648010254 1.1717698574066162
CurrentTrain: epoch  0, batch    54 | loss: 10.4112577Losses:  9.60950756072998 1.3075404167175293
CurrentTrain: epoch  0, batch    55 | loss: 10.9170475Losses:  9.262455940246582 0.9610280394554138
CurrentTrain: epoch  0, batch    56 | loss: 10.2234840Losses:  8.866462707519531 1.1750032901763916
CurrentTrain: epoch  0, batch    57 | loss: 10.0414658Losses:  9.214910507202148 0.645134687423706
CurrentTrain: epoch  0, batch    58 | loss: 9.8600454Losses:  8.805296897888184 0.40550345182418823
CurrentTrain: epoch  0, batch    59 | loss: 9.2108002Losses:  8.845993041992188 0.9803789258003235
CurrentTrain: epoch  0, batch    60 | loss: 9.8263721Losses:  8.672300338745117 1.0910804271697998
CurrentTrain: epoch  0, batch    61 | loss: 9.7633810Losses:  8.487933158874512 0.9562691450119019
CurrentTrain: epoch  0, batch    62 | loss: 9.4442024Losses:  8.240002632141113 1.0031378269195557
CurrentTrain: epoch  0, batch    63 | loss: 9.2431402Losses:  7.964794158935547 0.5446385145187378
CurrentTrain: epoch  0, batch    64 | loss: 8.5094328Losses:  8.279226303100586 0.9685318470001221
CurrentTrain: epoch  0, batch    65 | loss: 9.2477579Losses:  8.80112075805664 1.2302660942077637
CurrentTrain: epoch  0, batch    66 | loss: 10.0313873Losses:  8.356411933898926 0.9055763483047485
CurrentTrain: epoch  0, batch    67 | loss: 9.2619886Losses:  7.922969341278076 0.6797787547111511
CurrentTrain: epoch  0, batch    68 | loss: 8.6027479Losses:  7.728596210479736 0.9152683019638062
CurrentTrain: epoch  0, batch    69 | loss: 8.6438646Losses:  7.575203895568848 0.7108507752418518
CurrentTrain: epoch  0, batch    70 | loss: 8.2860546Losses:  7.49957275390625 0.8811309337615967
CurrentTrain: epoch  0, batch    71 | loss: 8.3807039Losses:  7.115305423736572 0.7010053396224976
CurrentTrain: epoch  0, batch    72 | loss: 7.8163109Losses:  7.572809219360352 1.0045933723449707
CurrentTrain: epoch  0, batch    73 | loss: 8.5774021Losses:  7.2599968910217285 0.6075420379638672
CurrentTrain: epoch  0, batch    74 | loss: 7.8675389Losses:  7.103831768035889 0.6707437038421631
CurrentTrain: epoch  1, batch     0 | loss: 7.7745752Losses:  7.497029781341553 0.7421324253082275
CurrentTrain: epoch  1, batch     1 | loss: 8.2391624Losses:  6.857823371887207 0.5790457725524902
CurrentTrain: epoch  1, batch     2 | loss: 7.4368691Losses:  7.6908369064331055 0.44587641954421997
CurrentTrain: epoch  1, batch     3 | loss: 8.1367130Losses:  7.360891342163086 0.34995391964912415
CurrentTrain: epoch  1, batch     4 | loss: 7.7108455Losses:  7.1750030517578125 0.9322924613952637
CurrentTrain: epoch  1, batch     5 | loss: 8.1072960Losses:  7.318305015563965 0.6273302435874939
CurrentTrain: epoch  1, batch     6 | loss: 7.9456353Losses:  7.026594161987305 0.9394534826278687
CurrentTrain: epoch  1, batch     7 | loss: 7.9660478Losses:  7.41688346862793 0.601621150970459
CurrentTrain: epoch  1, batch     8 | loss: 8.0185051Losses:  6.881918907165527 0.7433387041091919
CurrentTrain: epoch  1, batch     9 | loss: 7.6252575Losses:  7.01113224029541 0.7677222490310669
CurrentTrain: epoch  1, batch    10 | loss: 7.7788544Losses:  7.0321502685546875 0.4517468810081482
CurrentTrain: epoch  1, batch    11 | loss: 7.4838972Losses:  7.061477184295654 0.5154765844345093
CurrentTrain: epoch  1, batch    12 | loss: 7.5769539Losses:  6.8801984786987305 0.7839630842208862
CurrentTrain: epoch  1, batch    13 | loss: 7.6641617Losses:  6.736593246459961 0.7307747006416321
CurrentTrain: epoch  1, batch    14 | loss: 7.4673681Losses:  7.4250640869140625 0.6777174472808838
CurrentTrain: epoch  1, batch    15 | loss: 8.1027813Losses:  7.65637731552124 1.1369140148162842
CurrentTrain: epoch  1, batch    16 | loss: 8.7932911Losses:  6.958765983581543 0.6341803073883057
CurrentTrain: epoch  1, batch    17 | loss: 7.5929461Losses:  6.825491905212402 0.6964669227600098
CurrentTrain: epoch  1, batch    18 | loss: 7.5219588Losses:  6.985177993774414 0.8724195957183838
CurrentTrain: epoch  1, batch    19 | loss: 7.8575974Losses:  6.582218647003174 0.450974702835083
CurrentTrain: epoch  1, batch    20 | loss: 7.0331936Losses:  7.351344585418701 0.918129563331604
CurrentTrain: epoch  1, batch    21 | loss: 8.2694740Losses:  7.1285200119018555 0.731120228767395
CurrentTrain: epoch  1, batch    22 | loss: 7.8596401Losses:  6.967259883880615 0.6277869343757629
CurrentTrain: epoch  1, batch    23 | loss: 7.5950470Losses:  6.320888042449951 0.6190566420555115
CurrentTrain: epoch  1, batch    24 | loss: 6.9399447Losses:  6.605963230133057 0.7042598724365234
CurrentTrain: epoch  1, batch    25 | loss: 7.3102231Losses:  6.986543655395508 0.6991480588912964
CurrentTrain: epoch  1, batch    26 | loss: 7.6856918Losses:  7.44795036315918 0.8712990283966064
CurrentTrain: epoch  1, batch    27 | loss: 8.3192492Losses:  6.458518981933594 0.6365307569503784
CurrentTrain: epoch  1, batch    28 | loss: 7.0950499Losses:  7.180100917816162 0.7486176490783691
CurrentTrain: epoch  1, batch    29 | loss: 7.9287186Losses:  6.994465351104736 0.49901214241981506
CurrentTrain: epoch  1, batch    30 | loss: 7.4934773Losses:  7.333635330200195 0.6049773097038269
CurrentTrain: epoch  1, batch    31 | loss: 7.9386125Losses:  6.659127712249756 0.8675447702407837
CurrentTrain: epoch  1, batch    32 | loss: 7.5266724Losses:  6.237551689147949 0.5879420042037964
CurrentTrain: epoch  1, batch    33 | loss: 6.8254938Losses:  6.487132549285889 0.44022661447525024
CurrentTrain: epoch  1, batch    34 | loss: 6.9273591Losses:  6.358077049255371 0.6243372559547424
CurrentTrain: epoch  1, batch    35 | loss: 6.9824142Losses:  5.817318916320801 0.3838415741920471
CurrentTrain: epoch  1, batch    36 | loss: 6.2011604Losses:  7.124288558959961 0.6626805067062378
CurrentTrain: epoch  1, batch    37 | loss: 7.7869692Losses:  6.710258483886719 0.7884519100189209
CurrentTrain: epoch  1, batch    38 | loss: 7.4987106Losses:  6.843700408935547 0.9832556843757629
CurrentTrain: epoch  1, batch    39 | loss: 7.8269563Losses:  5.667191505432129 0.2975025773048401
CurrentTrain: epoch  1, batch    40 | loss: 5.9646940Losses:  6.667753219604492 0.6419028043746948
CurrentTrain: epoch  1, batch    41 | loss: 7.3096561Losses:  6.554637908935547 0.47719714045524597
CurrentTrain: epoch  1, batch    42 | loss: 7.0318351Losses:  6.197808265686035 0.5005677938461304
CurrentTrain: epoch  1, batch    43 | loss: 6.6983762Losses:  7.381643295288086 0.6661145687103271
CurrentTrain: epoch  1, batch    44 | loss: 8.0477581Losses:  6.324609756469727 0.30106258392333984
CurrentTrain: epoch  1, batch    45 | loss: 6.6256723Losses:  7.185860633850098 0.5429765582084656
CurrentTrain: epoch  1, batch    46 | loss: 7.7288370Losses:  6.787034034729004 0.3894594609737396
CurrentTrain: epoch  1, batch    47 | loss: 7.1764936Losses:  6.570846080780029 0.5926767587661743
CurrentTrain: epoch  1, batch    48 | loss: 7.1635227Losses:  6.584059715270996 0.5011811852455139
CurrentTrain: epoch  1, batch    49 | loss: 7.0852408Losses:  6.7672505378723145 0.5247783660888672
CurrentTrain: epoch  1, batch    50 | loss: 7.2920289Losses:  7.020975112915039 0.6645700931549072
CurrentTrain: epoch  1, batch    51 | loss: 7.6855450Losses:  6.178273677825928 0.6520689725875854
CurrentTrain: epoch  1, batch    52 | loss: 6.8303428Losses:  6.31154727935791 0.2721557915210724
CurrentTrain: epoch  1, batch    53 | loss: 6.5837030Losses:  6.424463272094727 0.5498474836349487
CurrentTrain: epoch  1, batch    54 | loss: 6.9743109Losses:  6.7036004066467285 0.5607393980026245
CurrentTrain: epoch  1, batch    55 | loss: 7.2643399Losses:  6.629533767700195 0.44973599910736084
CurrentTrain: epoch  1, batch    56 | loss: 7.0792699Losses:  6.233902454376221 0.31363388895988464
CurrentTrain: epoch  1, batch    57 | loss: 6.5475364Losses:  6.151227951049805 0.2932555675506592
CurrentTrain: epoch  1, batch    58 | loss: 6.4444838Losses:  6.623345375061035 0.4320599436759949
CurrentTrain: epoch  1, batch    59 | loss: 7.0554051Losses:  6.021434783935547 0.5352576971054077
CurrentTrain: epoch  1, batch    60 | loss: 6.5566926Losses:  6.835048675537109 0.4537830352783203
CurrentTrain: epoch  1, batch    61 | loss: 7.2888317Losses:  6.013144493103027 0.6770673990249634
CurrentTrain: epoch  1, batch    62 | loss: 6.6902118Losses:  6.427619457244873 0.5800604820251465
CurrentTrain: epoch  1, batch    63 | loss: 7.0076799Losses:  5.753012657165527 0.4661690294742584
CurrentTrain: epoch  1, batch    64 | loss: 6.2191815Losses:  6.518829345703125 0.6322072744369507
CurrentTrain: epoch  1, batch    65 | loss: 7.1510367Losses:  5.966783046722412 0.42522355914115906
CurrentTrain: epoch  1, batch    66 | loss: 6.3920064Losses:  6.092422008514404 0.3900323212146759
CurrentTrain: epoch  1, batch    67 | loss: 6.4824543Losses:  6.448138236999512 0.7293523550033569
CurrentTrain: epoch  1, batch    68 | loss: 7.1774907Losses:  6.588374137878418 0.5423926115036011
CurrentTrain: epoch  1, batch    69 | loss: 7.1307669Losses:  6.9764580726623535 0.5907173156738281
CurrentTrain: epoch  1, batch    70 | loss: 7.5671754Losses:  5.965442657470703 0.4634506106376648
CurrentTrain: epoch  1, batch    71 | loss: 6.4288931Losses:  5.876951694488525 0.5157632231712341
CurrentTrain: epoch  1, batch    72 | loss: 6.3927150Losses:  6.5941033363342285 0.4792199432849884
CurrentTrain: epoch  1, batch    73 | loss: 7.0733232Losses:  6.29977560043335 0.21419550478458405
CurrentTrain: epoch  1, batch    74 | loss: 6.5139713Losses:  5.503991603851318 0.35543137788772583
CurrentTrain: epoch  2, batch     0 | loss: 5.8594232Losses:  6.455741882324219 0.34079670906066895
CurrentTrain: epoch  2, batch     1 | loss: 6.7965384Losses:  6.585504531860352 0.4876231551170349
CurrentTrain: epoch  2, batch     2 | loss: 7.0731277Losses:  6.00737190246582 0.319629043340683
CurrentTrain: epoch  2, batch     3 | loss: 6.3270011Losses:  5.687148571014404 0.48347124457359314
CurrentTrain: epoch  2, batch     4 | loss: 6.1706200Losses:  5.355778694152832 0.2013872265815735
CurrentTrain: epoch  2, batch     5 | loss: 5.5571661Losses:  5.548107147216797 0.24585804343223572
CurrentTrain: epoch  2, batch     6 | loss: 5.7939653Losses:  5.792132377624512 0.3509679138660431
CurrentTrain: epoch  2, batch     7 | loss: 6.1431003Losses:  6.345276832580566 0.23498180508613586
CurrentTrain: epoch  2, batch     8 | loss: 6.5802588Losses:  6.315652370452881 0.4165118932723999
CurrentTrain: epoch  2, batch     9 | loss: 6.7321644Losses:  5.4168291091918945 0.21806256473064423
CurrentTrain: epoch  2, batch    10 | loss: 5.6348915Losses:  6.155717849731445 0.19075706601142883
CurrentTrain: epoch  2, batch    11 | loss: 6.3464751Losses:  6.15114688873291 0.35789161920547485
CurrentTrain: epoch  2, batch    12 | loss: 6.5090384Losses:  5.532461166381836 0.4119373857975006
CurrentTrain: epoch  2, batch    13 | loss: 5.9443984Losses:  5.678161144256592 0.4577482342720032
CurrentTrain: epoch  2, batch    14 | loss: 6.1359096Losses:  5.541003227233887 0.3805387616157532
CurrentTrain: epoch  2, batch    15 | loss: 5.9215422Losses:  6.4239397048950195 0.6649743914604187
CurrentTrain: epoch  2, batch    16 | loss: 7.0889139Losses:  5.9760918617248535 0.38417211174964905
CurrentTrain: epoch  2, batch    17 | loss: 6.3602638Losses:  5.9946136474609375 0.2906903326511383
CurrentTrain: epoch  2, batch    18 | loss: 6.2853041Losses:  6.052334785461426 0.1691243052482605
CurrentTrain: epoch  2, batch    19 | loss: 6.2214589Losses:  6.5553789138793945 0.2776581943035126
CurrentTrain: epoch  2, batch    20 | loss: 6.8330369Losses:  5.360840320587158 0.4035113453865051
CurrentTrain: epoch  2, batch    21 | loss: 5.7643518Losses:  5.852659225463867 0.33568865060806274
CurrentTrain: epoch  2, batch    22 | loss: 6.1883478Losses:  5.543899059295654 0.3343416750431061
CurrentTrain: epoch  2, batch    23 | loss: 5.8782406Losses:  5.531101226806641 0.38429492712020874
CurrentTrain: epoch  2, batch    24 | loss: 5.9153962Losses:  5.451230525970459 0.17573432624340057
CurrentTrain: epoch  2, batch    25 | loss: 5.6269650Losses:  5.899256229400635 0.4191855192184448
CurrentTrain: epoch  2, batch    26 | loss: 6.3184419Losses:  5.883405685424805 0.32741665840148926
CurrentTrain: epoch  2, batch    27 | loss: 6.2108221Losses:  6.340539932250977 0.4557386040687561
CurrentTrain: epoch  2, batch    28 | loss: 6.7962785Losses:  6.319678783416748 0.3909401297569275
CurrentTrain: epoch  2, batch    29 | loss: 6.7106190Losses:  6.336952209472656 0.6321156024932861
CurrentTrain: epoch  2, batch    30 | loss: 6.9690676Losses:  6.0483880043029785 0.3661136031150818
CurrentTrain: epoch  2, batch    31 | loss: 6.4145017Losses:  6.418412208557129 0.4584416151046753
CurrentTrain: epoch  2, batch    32 | loss: 6.8768539Losses:  5.872514247894287 0.3436530828475952
CurrentTrain: epoch  2, batch    33 | loss: 6.2161674Losses:  5.2185845375061035 0.3535180985927582
CurrentTrain: epoch  2, batch    34 | loss: 5.5721025Losses:  6.659220218658447 0.5897823572158813
CurrentTrain: epoch  2, batch    35 | loss: 7.2490025Losses:  5.766684055328369 0.32860180735588074
CurrentTrain: epoch  2, batch    36 | loss: 6.0952859Losses:  6.108462333679199 0.45352745056152344
CurrentTrain: epoch  2, batch    37 | loss: 6.5619898Losses:  5.20936393737793 0.24522212147712708
CurrentTrain: epoch  2, batch    38 | loss: 5.4545860Losses:  5.947113990783691 0.31583189964294434
CurrentTrain: epoch  2, batch    39 | loss: 6.2629461Losses:  5.661828994750977 0.29369696974754333
CurrentTrain: epoch  2, batch    40 | loss: 5.9555259Losses:  5.199032783508301 0.273221880197525
CurrentTrain: epoch  2, batch    41 | loss: 5.4722548Losses:  6.210984230041504 0.524568498134613
CurrentTrain: epoch  2, batch    42 | loss: 6.7355528Losses:  5.421299934387207 0.3325420618057251
CurrentTrain: epoch  2, batch    43 | loss: 5.7538419Losses:  5.869841575622559 0.5098130702972412
CurrentTrain: epoch  2, batch    44 | loss: 6.3796549Losses:  5.738576889038086 0.2688989043235779
CurrentTrain: epoch  2, batch    45 | loss: 6.0074759Losses:  6.572734832763672 0.3749086856842041
CurrentTrain: epoch  2, batch    46 | loss: 6.9476433Losses:  5.604750156402588 0.12780867516994476
CurrentTrain: epoch  2, batch    47 | loss: 5.7325587Losses:  6.045217514038086 0.5691713094711304
CurrentTrain: epoch  2, batch    48 | loss: 6.6143889Losses:  5.846277236938477 0.2621343731880188
CurrentTrain: epoch  2, batch    49 | loss: 6.1084118Losses:  6.4866485595703125 0.5464800000190735
CurrentTrain: epoch  2, batch    50 | loss: 7.0331287Losses:  5.866216659545898 0.5195730328559875
CurrentTrain: epoch  2, batch    51 | loss: 6.3857899Losses:  5.769859313964844 0.3860633373260498
CurrentTrain: epoch  2, batch    52 | loss: 6.1559229Losses:  7.244248867034912 0.6043531894683838
CurrentTrain: epoch  2, batch    53 | loss: 7.8486023Losses:  6.346037864685059 0.5701605677604675
CurrentTrain: epoch  2, batch    54 | loss: 6.9161983Losses:  5.147303581237793 0.16898025572299957
CurrentTrain: epoch  2, batch    55 | loss: 5.3162837Losses:  6.382320404052734 0.5618318319320679
CurrentTrain: epoch  2, batch    56 | loss: 6.9441524Losses:  6.512124061584473 0.6183812618255615
CurrentTrain: epoch  2, batch    57 | loss: 7.1305056Losses:  5.946192741394043 0.3704203963279724
CurrentTrain: epoch  2, batch    58 | loss: 6.3166132Losses:  5.665887832641602 0.4996092915534973
CurrentTrain: epoch  2, batch    59 | loss: 6.1654973Losses:  6.113109588623047 0.15545955300331116
CurrentTrain: epoch  2, batch    60 | loss: 6.2685690Losses:  5.91031551361084 0.6334129571914673
CurrentTrain: epoch  2, batch    61 | loss: 6.5437284Losses:  5.826249599456787 0.35146868228912354
CurrentTrain: epoch  2, batch    62 | loss: 6.1777182Losses:  6.090350151062012 0.28850096464157104
CurrentTrain: epoch  2, batch    63 | loss: 6.3788509Losses:  5.750408172607422 0.3707588315010071
CurrentTrain: epoch  2, batch    64 | loss: 6.1211672Losses:  5.5586700439453125 0.2992385923862457
CurrentTrain: epoch  2, batch    65 | loss: 5.8579087Losses:  5.222987651824951 0.27087146043777466
CurrentTrain: epoch  2, batch    66 | loss: 5.4938593Losses:  6.145428657531738 0.32075104117393494
CurrentTrain: epoch  2, batch    67 | loss: 6.4661798Losses:  6.170222759246826 0.47513115406036377
CurrentTrain: epoch  2, batch    68 | loss: 6.6453538Losses:  5.581539154052734 0.31305694580078125
CurrentTrain: epoch  2, batch    69 | loss: 5.8945961Losses:  6.189585208892822 0.24458925426006317
CurrentTrain: epoch  2, batch    70 | loss: 6.4341745Losses:  6.220952033996582 0.45741114020347595
CurrentTrain: epoch  2, batch    71 | loss: 6.6783633Losses:  5.475464820861816 0.4641772210597992
CurrentTrain: epoch  2, batch    72 | loss: 5.9396420Losses:  5.649020195007324 0.3056963086128235
CurrentTrain: epoch  2, batch    73 | loss: 5.9547167Losses:  5.636430263519287 0.4726768434047699
CurrentTrain: epoch  2, batch    74 | loss: 6.1091070Losses:  5.415579795837402 0.3307546377182007
CurrentTrain: epoch  3, batch     0 | loss: 5.7463346Losses:  5.3241496086120605 0.2123107612133026
CurrentTrain: epoch  3, batch     1 | loss: 5.5364604Losses:  5.2394585609436035 0.15955322980880737
CurrentTrain: epoch  3, batch     2 | loss: 5.3990116Losses:  5.999407768249512 0.3854166865348816
CurrentTrain: epoch  3, batch     3 | loss: 6.3848243Losses:  5.359551429748535 0.3553658127784729
CurrentTrain: epoch  3, batch     4 | loss: 5.7149172Losses:  5.969210147857666 0.3253695070743561
CurrentTrain: epoch  3, batch     5 | loss: 6.2945795Losses:  5.285070419311523 0.27755969762802124
CurrentTrain: epoch  3, batch     6 | loss: 5.5626302Losses:  6.227899074554443 0.4920857846736908
CurrentTrain: epoch  3, batch     7 | loss: 6.7199850Losses:  5.451488971710205 0.11982912570238113
CurrentTrain: epoch  3, batch     8 | loss: 5.5713181Losses:  5.35933780670166 0.3424694836139679
CurrentTrain: epoch  3, batch     9 | loss: 5.7018075Losses:  5.89320182800293 0.6085046529769897
CurrentTrain: epoch  3, batch    10 | loss: 6.5017066Losses:  5.325876235961914 0.3309480547904968
CurrentTrain: epoch  3, batch    11 | loss: 5.6568241Losses:  5.211101531982422 0.34456169605255127
CurrentTrain: epoch  3, batch    12 | loss: 5.5556631Losses:  5.079963684082031 0.22140644490718842
CurrentTrain: epoch  3, batch    13 | loss: 5.3013701Losses:  5.433437347412109 0.31990140676498413
CurrentTrain: epoch  3, batch    14 | loss: 5.7533388Losses:  5.209076881408691 0.2489473819732666
CurrentTrain: epoch  3, batch    15 | loss: 5.4580240Losses:  5.1672163009643555 0.21304455399513245
CurrentTrain: epoch  3, batch    16 | loss: 5.3802609Losses:  5.970198154449463 0.4247967600822449
CurrentTrain: epoch  3, batch    17 | loss: 6.3949947Losses:  5.548684120178223 0.3888322412967682
CurrentTrain: epoch  3, batch    18 | loss: 5.9375162Losses:  5.2272186279296875 0.17203423380851746
CurrentTrain: epoch  3, batch    19 | loss: 5.3992529Losses:  5.3059892654418945 0.20383253693580627
CurrentTrain: epoch  3, batch    20 | loss: 5.5098219Losses:  5.470561504364014 0.4015004336833954
CurrentTrain: epoch  3, batch    21 | loss: 5.8720617Losses:  5.319659233093262 0.18089689314365387
CurrentTrain: epoch  3, batch    22 | loss: 5.5005560Losses:  5.3998894691467285 0.26753219962120056
CurrentTrain: epoch  3, batch    23 | loss: 5.6674218Losses:  6.1571364402771 0.5782731175422668
CurrentTrain: epoch  3, batch    24 | loss: 6.7354097Losses:  5.025487899780273 0.197584867477417
CurrentTrain: epoch  3, batch    25 | loss: 5.2230730Losses:  5.760421276092529 0.3594573140144348
CurrentTrain: epoch  3, batch    26 | loss: 6.1198788Losses:  5.240118980407715 0.2792939245700836
CurrentTrain: epoch  3, batch    27 | loss: 5.5194130Losses:  5.096997261047363 0.1870008111000061
CurrentTrain: epoch  3, batch    28 | loss: 5.2839980Losses:  5.2060546875 0.2347601056098938
CurrentTrain: epoch  3, batch    29 | loss: 5.4408150Losses:  5.108761787414551 0.2545592486858368
CurrentTrain: epoch  3, batch    30 | loss: 5.3633208Losses:  5.765114784240723 0.21684035658836365
CurrentTrain: epoch  3, batch    31 | loss: 5.9819551Losses:  5.014358043670654 0.20510998368263245
CurrentTrain: epoch  3, batch    32 | loss: 5.2194681Losses:  5.068614482879639 0.22847485542297363
CurrentTrain: epoch  3, batch    33 | loss: 5.2970896Losses:  5.402742862701416 0.12427753955125809
CurrentTrain: epoch  3, batch    34 | loss: 5.5270205Losses:  5.551617622375488 0.4183295667171478
CurrentTrain: epoch  3, batch    35 | loss: 5.9699473Losses:  5.640995502471924 0.29532575607299805
CurrentTrain: epoch  3, batch    36 | loss: 5.9363213Losses:  4.89870548248291 0.11536994576454163
CurrentTrain: epoch  3, batch    37 | loss: 5.0140753Losses:  5.394044876098633 0.2169877588748932
CurrentTrain: epoch  3, batch    38 | loss: 5.6110325Losses:  5.343621730804443 0.25859445333480835
CurrentTrain: epoch  3, batch    39 | loss: 5.6022162Losses:  5.0740251541137695 0.04394131153821945
CurrentTrain: epoch  3, batch    40 | loss: 5.1179667Losses:  5.21427059173584 0.256231427192688
CurrentTrain: epoch  3, batch    41 | loss: 5.4705019Losses:  5.400094032287598 0.20014847815036774
CurrentTrain: epoch  3, batch    42 | loss: 5.6002426Losses:  5.0364789962768555 0.174546480178833
CurrentTrain: epoch  3, batch    43 | loss: 5.2110252Losses:  5.623690605163574 0.561499834060669
CurrentTrain: epoch  3, batch    44 | loss: 6.1851902Losses:  5.207673072814941 0.18959417939186096
CurrentTrain: epoch  3, batch    45 | loss: 5.3972673Losses:  5.265397071838379 0.23762264847755432
CurrentTrain: epoch  3, batch    46 | loss: 5.5030198Losses:  5.021239280700684 0.1156211569905281
CurrentTrain: epoch  3, batch    47 | loss: 5.1368604Losses:  5.183035850524902 0.2131197154521942
CurrentTrain: epoch  3, batch    48 | loss: 5.3961554Losses:  5.1556477546691895 0.22082234919071198
CurrentTrain: epoch  3, batch    49 | loss: 5.3764701Losses:  5.576245307922363 0.41268813610076904
CurrentTrain: epoch  3, batch    50 | loss: 5.9889336Losses:  5.217756748199463 0.09529808163642883
CurrentTrain: epoch  3, batch    51 | loss: 5.3130550Losses:  5.825250625610352 0.5621733665466309
CurrentTrain: epoch  3, batch    52 | loss: 6.3874240Losses:  5.1579132080078125 0.1278093159198761
CurrentTrain: epoch  3, batch    53 | loss: 5.2857227Losses:  5.100111961364746 0.2520431876182556
CurrentTrain: epoch  3, batch    54 | loss: 5.3521552Losses:  5.159134864807129 0.2058432251214981
CurrentTrain: epoch  3, batch    55 | loss: 5.3649783Losses:  5.345069885253906 0.3003602623939514
CurrentTrain: epoch  3, batch    56 | loss: 5.6454301Losses:  5.2964067459106445 0.21081534028053284
CurrentTrain: epoch  3, batch    57 | loss: 5.5072222Losses:  5.4465012550354 0.3202541172504425
CurrentTrain: epoch  3, batch    58 | loss: 5.7667556Losses:  5.953958511352539 0.16392621397972107
CurrentTrain: epoch  3, batch    59 | loss: 6.1178846Losses:  5.313549041748047 0.09980481117963791
CurrentTrain: epoch  3, batch    60 | loss: 5.4133539Losses:  6.427063941955566 0.39714840054512024
CurrentTrain: epoch  3, batch    61 | loss: 6.8242126Losses:  5.863842010498047 0.20377510786056519
CurrentTrain: epoch  3, batch    62 | loss: 6.0676169Losses:  6.39519739151001 0.28021475672721863
CurrentTrain: epoch  3, batch    63 | loss: 6.6754122Losses:  5.473596096038818 0.24515733122825623
CurrentTrain: epoch  3, batch    64 | loss: 5.7187533Losses:  5.227202415466309 0.15917617082595825
CurrentTrain: epoch  3, batch    65 | loss: 5.3863788Losses:  5.357507228851318 0.16520512104034424
CurrentTrain: epoch  3, batch    66 | loss: 5.5227122Losses:  6.027533531188965 0.4206543564796448
CurrentTrain: epoch  3, batch    67 | loss: 6.4481878Losses:  5.3510422706604 0.22730356454849243
CurrentTrain: epoch  3, batch    68 | loss: 5.5783458Losses:  6.045724868774414 0.07293674349784851
CurrentTrain: epoch  3, batch    69 | loss: 6.1186614Losses:  5.201525688171387 0.16479617357254028
CurrentTrain: epoch  3, batch    70 | loss: 5.3663220Losses:  5.691147804260254 0.32543009519577026
CurrentTrain: epoch  3, batch    71 | loss: 6.0165777Losses:  6.415390968322754 0.5177668929100037
CurrentTrain: epoch  3, batch    72 | loss: 6.9331579Losses:  5.048894882202148 0.16920283436775208
CurrentTrain: epoch  3, batch    73 | loss: 5.2180977Losses:  4.778623104095459 0.09846606105566025
CurrentTrain: epoch  3, batch    74 | loss: 4.8770890Losses:  4.890598773956299 0.16815589368343353
CurrentTrain: epoch  4, batch     0 | loss: 5.0587544Losses:  5.344836711883545 0.2850249409675598
CurrentTrain: epoch  4, batch     1 | loss: 5.6298618Losses:  5.116080284118652 0.16352882981300354
CurrentTrain: epoch  4, batch     2 | loss: 5.2796092Losses:  6.410921573638916 0.5342880487442017
CurrentTrain: epoch  4, batch     3 | loss: 6.9452095Losses:  5.269958972930908 0.2166290134191513
CurrentTrain: epoch  4, batch     4 | loss: 5.4865880Losses:  5.142118453979492 0.221027210354805
CurrentTrain: epoch  4, batch     5 | loss: 5.3631458Losses:  5.847874641418457 0.382875919342041
CurrentTrain: epoch  4, batch     6 | loss: 6.2307506Losses:  5.188182830810547 0.2068103402853012
CurrentTrain: epoch  4, batch     7 | loss: 5.3949933Losses:  6.013309955596924 0.4532369077205658
CurrentTrain: epoch  4, batch     8 | loss: 6.4665470Losses:  5.023364067077637 0.14330294728279114
CurrentTrain: epoch  4, batch     9 | loss: 5.1666670Losses:  5.133146286010742 0.22891387343406677
CurrentTrain: epoch  4, batch    10 | loss: 5.3620601Losses:  5.112473011016846 0.17945736646652222
CurrentTrain: epoch  4, batch    11 | loss: 5.2919302Losses:  4.960380554199219 0.2268901765346527
CurrentTrain: epoch  4, batch    12 | loss: 5.1872706Losses:  5.574714660644531 0.34505128860473633
CurrentTrain: epoch  4, batch    13 | loss: 5.9197659Losses:  4.808448791503906 0.18384632468223572
CurrentTrain: epoch  4, batch    14 | loss: 4.9922953Losses:  5.013591289520264 0.14142188429832458
CurrentTrain: epoch  4, batch    15 | loss: 5.1550131Losses:  4.930697441101074 0.20354869961738586
CurrentTrain: epoch  4, batch    16 | loss: 5.1342463Losses:  5.1119914054870605 0.18564093112945557
CurrentTrain: epoch  4, batch    17 | loss: 5.2976322Losses:  4.994259834289551 0.173641175031662
CurrentTrain: epoch  4, batch    18 | loss: 5.1679010Losses:  5.1046037673950195 0.18617810308933258
CurrentTrain: epoch  4, batch    19 | loss: 5.2907820Losses:  6.137886047363281 0.31860876083374023
CurrentTrain: epoch  4, batch    20 | loss: 6.4564948Losses:  5.27729606628418 0.21873462200164795
CurrentTrain: epoch  4, batch    21 | loss: 5.4960308Losses:  5.092881202697754 0.22210726141929626
CurrentTrain: epoch  4, batch    22 | loss: 5.3149886Losses:  5.1904826164245605 0.25145086646080017
CurrentTrain: epoch  4, batch    23 | loss: 5.4419336Losses:  5.127774715423584 0.25426626205444336
CurrentTrain: epoch  4, batch    24 | loss: 5.3820410Losses:  5.0484209060668945 0.16914069652557373
CurrentTrain: epoch  4, batch    25 | loss: 5.2175617Losses:  5.297813892364502 0.12020222842693329
CurrentTrain: epoch  4, batch    26 | loss: 5.4180160Losses:  5.410702705383301 0.2886497676372528
CurrentTrain: epoch  4, batch    27 | loss: 5.6993523Losses:  5.041248321533203 0.15532037615776062
CurrentTrain: epoch  4, batch    28 | loss: 5.1965685Losses:  4.971612930297852 0.17224901914596558
CurrentTrain: epoch  4, batch    29 | loss: 5.1438618Losses:  5.7592010498046875 0.5299065113067627
CurrentTrain: epoch  4, batch    30 | loss: 6.2891073Losses:  5.488117218017578 0.3923119306564331
CurrentTrain: epoch  4, batch    31 | loss: 5.8804293Losses:  4.845981121063232 0.19951605796813965
CurrentTrain: epoch  4, batch    32 | loss: 5.0454969Losses:  5.562399864196777 0.2751857042312622
CurrentTrain: epoch  4, batch    33 | loss: 5.8375854Losses:  4.819907188415527 0.19482317566871643
CurrentTrain: epoch  4, batch    34 | loss: 5.0147305Losses:  5.176950454711914 0.19690383970737457
CurrentTrain: epoch  4, batch    35 | loss: 5.3738542Losses:  5.073038101196289 0.09720200300216675
CurrentTrain: epoch  4, batch    36 | loss: 5.1702399Losses:  5.266757488250732 0.3814767003059387
CurrentTrain: epoch  4, batch    37 | loss: 5.6482344Losses:  4.984835624694824 0.1288795918226242
CurrentTrain: epoch  4, batch    38 | loss: 5.1137152Losses:  4.989279270172119 0.2477932870388031
CurrentTrain: epoch  4, batch    39 | loss: 5.2370725Losses:  5.119182586669922 0.19355988502502441
CurrentTrain: epoch  4, batch    40 | loss: 5.3127422Losses:  5.177667140960693 0.19376936554908752
CurrentTrain: epoch  4, batch    41 | loss: 5.3714366Losses:  5.12532901763916 0.20311607420444489
CurrentTrain: epoch  4, batch    42 | loss: 5.3284450Losses:  5.055721282958984 0.14480388164520264
CurrentTrain: epoch  4, batch    43 | loss: 5.2005253Losses:  5.650934219360352 0.3799683749675751
CurrentTrain: epoch  4, batch    44 | loss: 6.0309024Losses:  5.0058183670043945 0.2493373155593872
CurrentTrain: epoch  4, batch    45 | loss: 5.2551556Losses:  4.7128095626831055 0.1159420982003212
CurrentTrain: epoch  4, batch    46 | loss: 4.8287516Losses:  5.3277764320373535 0.3329100012779236
CurrentTrain: epoch  4, batch    47 | loss: 5.6606865Losses:  4.875849723815918 0.15337955951690674
CurrentTrain: epoch  4, batch    48 | loss: 5.0292292Losses:  5.060789108276367 0.2014979124069214
CurrentTrain: epoch  4, batch    49 | loss: 5.2622871Losses:  5.25753116607666 0.08125241100788116
CurrentTrain: epoch  4, batch    50 | loss: 5.3387837Losses:  4.670862674713135 0.11659116297960281
CurrentTrain: epoch  4, batch    51 | loss: 4.7874537Losses:  5.699322700500488 0.20587867498397827
CurrentTrain: epoch  4, batch    52 | loss: 5.9052014Losses:  5.0829668045043945 0.1923261284828186
CurrentTrain: epoch  4, batch    53 | loss: 5.2752929Losses:  4.852110385894775 0.17444424331188202
CurrentTrain: epoch  4, batch    54 | loss: 5.0265546Losses:  4.941005706787109 0.20396208763122559
CurrentTrain: epoch  4, batch    55 | loss: 5.1449680Losses:  5.086319923400879 0.20178532600402832
CurrentTrain: epoch  4, batch    56 | loss: 5.2881050Losses:  4.671302795410156 0.12957516312599182
CurrentTrain: epoch  4, batch    57 | loss: 4.8008780Losses:  4.764430522918701 0.12362555414438248
CurrentTrain: epoch  4, batch    58 | loss: 4.8880563Losses:  4.69871711730957 0.12086229771375656
CurrentTrain: epoch  4, batch    59 | loss: 4.8195796Losses:  5.013823986053467 0.17220865190029144
CurrentTrain: epoch  4, batch    60 | loss: 5.1860328Losses:  5.9643754959106445 0.3291304111480713
CurrentTrain: epoch  4, batch    61 | loss: 6.2935057Losses:  4.955075263977051 0.17955490946769714
CurrentTrain: epoch  4, batch    62 | loss: 5.1346302Losses:  4.878488540649414 0.165274977684021
CurrentTrain: epoch  4, batch    63 | loss: 5.0437636Losses:  5.683995723724365 0.16449235379695892
CurrentTrain: epoch  4, batch    64 | loss: 5.8484879Losses:  5.741997241973877 0.27643752098083496
CurrentTrain: epoch  4, batch    65 | loss: 6.0184345Losses:  4.839641571044922 0.10363349318504333
CurrentTrain: epoch  4, batch    66 | loss: 4.9432750Losses:  4.987701416015625 0.19490593671798706
CurrentTrain: epoch  4, batch    67 | loss: 5.1826072Losses:  4.923238277435303 0.15946167707443237
CurrentTrain: epoch  4, batch    68 | loss: 5.0826998Losses:  5.797924518585205 0.28515565395355225
CurrentTrain: epoch  4, batch    69 | loss: 6.0830803Losses:  4.7018561363220215 0.14038249850273132
CurrentTrain: epoch  4, batch    70 | loss: 4.8422384Losses:  5.164645195007324 0.22380700707435608
CurrentTrain: epoch  4, batch    71 | loss: 5.3884521Losses:  4.854944229125977 0.17486271262168884
CurrentTrain: epoch  4, batch    72 | loss: 5.0298071Losses:  4.702434539794922 0.13416483998298645
CurrentTrain: epoch  4, batch    73 | loss: 4.8365993Losses:  4.900515556335449 0.17333990335464478
CurrentTrain: epoch  4, batch    74 | loss: 5.0738554Losses:  4.771671295166016 0.16356483101844788
CurrentTrain: epoch  5, batch     0 | loss: 4.9352360Losses:  4.732385635375977 0.12322938442230225
CurrentTrain: epoch  5, batch     1 | loss: 4.8556151Losses:  4.924919128417969 0.14847785234451294
CurrentTrain: epoch  5, batch     2 | loss: 5.0733972Losses:  4.878063678741455 0.0957973375916481
CurrentTrain: epoch  5, batch     3 | loss: 4.9738612Losses:  4.850679397583008 0.1363581418991089
CurrentTrain: epoch  5, batch     4 | loss: 4.9870377Losses:  4.798634052276611 0.0872374176979065
CurrentTrain: epoch  5, batch     5 | loss: 4.8858714Losses:  4.820625305175781 0.22178545594215393
CurrentTrain: epoch  5, batch     6 | loss: 5.0424109Losses:  5.31742000579834 0.16240012645721436
CurrentTrain: epoch  5, batch     7 | loss: 5.4798203Losses:  4.999131679534912 0.16910311579704285
CurrentTrain: epoch  5, batch     8 | loss: 5.1682348Losses:  4.8811540603637695 0.05971862003207207
CurrentTrain: epoch  5, batch     9 | loss: 4.9408727Losses:  4.71579122543335 0.15742158889770508
CurrentTrain: epoch  5, batch    10 | loss: 4.8732128Losses:  4.900031566619873 0.06731151789426804
CurrentTrain: epoch  5, batch    11 | loss: 4.9673429Losses:  4.834936618804932 0.10493329167366028
CurrentTrain: epoch  5, batch    12 | loss: 4.9398699Losses:  4.786004066467285 0.14463874697685242
CurrentTrain: epoch  5, batch    13 | loss: 4.9306426Losses:  4.756921291351318 0.07536860555410385
CurrentTrain: epoch  5, batch    14 | loss: 4.8322897Losses:  4.64231014251709 0.15871071815490723
CurrentTrain: epoch  5, batch    15 | loss: 4.8010206Losses:  4.927618026733398 0.18890559673309326
CurrentTrain: epoch  5, batch    16 | loss: 5.1165237Losses:  4.76289176940918 0.16562247276306152
CurrentTrain: epoch  5, batch    17 | loss: 4.9285145Losses:  4.703535556793213 0.0929512307047844
CurrentTrain: epoch  5, batch    18 | loss: 4.7964869Losses:  4.664438247680664 0.06443943083286285
CurrentTrain: epoch  5, batch    19 | loss: 4.7288775Losses:  4.916438102722168 0.1161547303199768
CurrentTrain: epoch  5, batch    20 | loss: 5.0325928Losses:  4.914777755737305 0.15708115696907043
CurrentTrain: epoch  5, batch    21 | loss: 5.0718589Losses:  4.848802089691162 0.17990899085998535
CurrentTrain: epoch  5, batch    22 | loss: 5.0287113Losses:  4.8442864418029785 0.10464886575937271
CurrentTrain: epoch  5, batch    23 | loss: 4.9489355Losses:  6.015661716461182 0.40272998809814453
CurrentTrain: epoch  5, batch    24 | loss: 6.4183917Losses:  4.894814968109131 0.10589770972728729
CurrentTrain: epoch  5, batch    25 | loss: 5.0007129Losses:  5.311134338378906 0.3957364559173584
CurrentTrain: epoch  5, batch    26 | loss: 5.7068710Losses:  4.879210948944092 0.15033367276191711
CurrentTrain: epoch  5, batch    27 | loss: 5.0295448Losses:  4.813806533813477 0.1436120867729187
CurrentTrain: epoch  5, batch    28 | loss: 4.9574184Losses:  4.727198600769043 0.09133124351501465
CurrentTrain: epoch  5, batch    29 | loss: 4.8185301Losses:  4.918521881103516 0.11738801747560501
CurrentTrain: epoch  5, batch    30 | loss: 5.0359101Losses:  5.410458564758301 0.33324897289276123
CurrentTrain: epoch  5, batch    31 | loss: 5.7437077Losses:  4.903332233428955 0.13931141793727875
CurrentTrain: epoch  5, batch    32 | loss: 5.0426435Losses:  4.800914764404297 0.07891853153705597
CurrentTrain: epoch  5, batch    33 | loss: 4.8798332Losses:  5.1286940574646 0.19709458947181702
CurrentTrain: epoch  5, batch    34 | loss: 5.3257885Losses:  4.7851643562316895 0.11652408540248871
CurrentTrain: epoch  5, batch    35 | loss: 4.9016886Losses:  5.003478527069092 0.12027031183242798
CurrentTrain: epoch  5, batch    36 | loss: 5.1237488Losses:  4.87727165222168 0.11100971698760986
CurrentTrain: epoch  5, batch    37 | loss: 4.9882812Losses:  4.620738983154297 0.08754702657461166
CurrentTrain: epoch  5, batch    38 | loss: 4.7082858Losses:  5.082088470458984 0.2588183879852295
CurrentTrain: epoch  5, batch    39 | loss: 5.3409071Losses:  4.809429168701172 0.13231688737869263
CurrentTrain: epoch  5, batch    40 | loss: 4.9417462Losses:  5.180894374847412 0.11737281084060669
CurrentTrain: epoch  5, batch    41 | loss: 5.2982674Losses:  5.754474639892578 0.26646891236305237
CurrentTrain: epoch  5, batch    42 | loss: 6.0209436Losses:  4.680642127990723 0.13820606470108032
CurrentTrain: epoch  5, batch    43 | loss: 4.8188481Losses:  4.768657684326172 0.1661902368068695
CurrentTrain: epoch  5, batch    44 | loss: 4.9348478Losses:  4.768523216247559 0.14652566611766815
CurrentTrain: epoch  5, batch    45 | loss: 4.9150491Losses:  4.933627128601074 0.13067516684532166
CurrentTrain: epoch  5, batch    46 | loss: 5.0643024Losses:  4.826700210571289 0.07121150195598602
CurrentTrain: epoch  5, batch    47 | loss: 4.8979115Losses:  4.816154479980469 0.08025240153074265
CurrentTrain: epoch  5, batch    48 | loss: 4.8964067Losses:  5.042271614074707 0.12623220682144165
CurrentTrain: epoch  5, batch    49 | loss: 5.1685038Losses:  4.709139823913574 0.1066250205039978
CurrentTrain: epoch  5, batch    50 | loss: 4.8157649Losses:  5.543445587158203 0.3406623601913452
CurrentTrain: epoch  5, batch    51 | loss: 5.8841081Losses:  4.657011032104492 0.10860828310251236
CurrentTrain: epoch  5, batch    52 | loss: 4.7656193Losses:  4.83698034286499 0.13219541311264038
CurrentTrain: epoch  5, batch    53 | loss: 4.9691758Losses:  4.7860236167907715 0.15147653222084045
CurrentTrain: epoch  5, batch    54 | loss: 4.9375000Losses:  4.80935525894165 0.1300034523010254
CurrentTrain: epoch  5, batch    55 | loss: 4.9393587Losses:  4.844045639038086 0.11239949613809586
CurrentTrain: epoch  5, batch    56 | loss: 4.9564452Losses:  4.786993026733398 0.10016903281211853
CurrentTrain: epoch  5, batch    57 | loss: 4.8871622Losses:  4.68890905380249 0.10727471113204956
CurrentTrain: epoch  5, batch    58 | loss: 4.7961836Losses:  5.565706253051758 0.49177759885787964
CurrentTrain: epoch  5, batch    59 | loss: 6.0574837Losses:  4.77971076965332 0.10116887837648392
CurrentTrain: epoch  5, batch    60 | loss: 4.8808799Losses:  5.072788238525391 0.12102886289358139
CurrentTrain: epoch  5, batch    61 | loss: 5.1938171Losses:  4.838412761688232 0.21785245835781097
CurrentTrain: epoch  5, batch    62 | loss: 5.0562654Losses:  4.887177467346191 0.07595038414001465
CurrentTrain: epoch  5, batch    63 | loss: 4.9631281Losses:  4.619219779968262 0.12601226568222046
CurrentTrain: epoch  5, batch    64 | loss: 4.7452321Losses:  4.5803914070129395 0.11802826821804047
CurrentTrain: epoch  5, batch    65 | loss: 4.6984196Losses:  4.693971633911133 0.08793268352746964
CurrentTrain: epoch  5, batch    66 | loss: 4.7819042Losses:  4.842128753662109 0.10629573464393616
CurrentTrain: epoch  5, batch    67 | loss: 4.9484243Losses:  4.783015251159668 0.09302007406949997
CurrentTrain: epoch  5, batch    68 | loss: 4.8760352Losses:  5.060002326965332 0.15529868006706238
CurrentTrain: epoch  5, batch    69 | loss: 5.2153010Losses:  5.011833667755127 0.18640047311782837
CurrentTrain: epoch  5, batch    70 | loss: 5.1982341Losses:  4.834527492523193 0.12622861564159393
CurrentTrain: epoch  5, batch    71 | loss: 4.9607563Losses:  4.699299335479736 0.10302376747131348
CurrentTrain: epoch  5, batch    72 | loss: 4.8023233Losses:  4.839692115783691 0.15113487839698792
CurrentTrain: epoch  5, batch    73 | loss: 4.9908271Losses:  5.160486698150635 0.131134033203125
CurrentTrain: epoch  5, batch    74 | loss: 5.2916207Losses:  4.695135116577148 0.08276477456092834
CurrentTrain: epoch  6, batch     0 | loss: 4.7778997Losses:  4.750658988952637 0.11731220781803131
CurrentTrain: epoch  6, batch     1 | loss: 4.8679714Losses:  4.672205924987793 0.07890720665454865
CurrentTrain: epoch  6, batch     2 | loss: 4.7511129Losses:  4.883346080780029 0.1167660802602768
CurrentTrain: epoch  6, batch     3 | loss: 5.0001121Losses:  4.685998916625977 0.11776779592037201
CurrentTrain: epoch  6, batch     4 | loss: 4.8037667Losses:  4.719083786010742 0.12009894102811813
CurrentTrain: epoch  6, batch     5 | loss: 4.8391829Losses:  4.671981334686279 0.12370304763317108
CurrentTrain: epoch  6, batch     6 | loss: 4.7956843Losses:  4.650397777557373 0.05783692002296448
CurrentTrain: epoch  6, batch     7 | loss: 4.7082348Losses:  4.670287132263184 0.10321474075317383
CurrentTrain: epoch  6, batch     8 | loss: 4.7735019Losses:  4.6465229988098145 0.124352365732193
CurrentTrain: epoch  6, batch     9 | loss: 4.7708755Losses:  4.803147315979004 0.12485606968402863
CurrentTrain: epoch  6, batch    10 | loss: 4.9280033Losses:  4.5820465087890625 0.14053533971309662
CurrentTrain: epoch  6, batch    11 | loss: 4.7225819Losses:  4.648688316345215 0.10022009909152985
CurrentTrain: epoch  6, batch    12 | loss: 4.7489085Losses:  4.8473591804504395 0.19855710864067078
CurrentTrain: epoch  6, batch    13 | loss: 5.0459161Losses:  4.5533905029296875 0.11046315729618073
CurrentTrain: epoch  6, batch    14 | loss: 4.6638536Losses:  4.6323981285095215 0.1265074610710144
CurrentTrain: epoch  6, batch    15 | loss: 4.7589054Losses:  4.651928901672363 0.14157721400260925
CurrentTrain: epoch  6, batch    16 | loss: 4.7935061Losses:  4.772651672363281 0.05988939851522446
CurrentTrain: epoch  6, batch    17 | loss: 4.8325410Losses:  5.190812110900879 0.13848233222961426
CurrentTrain: epoch  6, batch    18 | loss: 5.3292942Losses:  4.616464614868164 0.0713501125574112
CurrentTrain: epoch  6, batch    19 | loss: 4.6878147Losses:  5.234124183654785 0.2933821380138397
CurrentTrain: epoch  6, batch    20 | loss: 5.5275064Losses:  4.622564315795898 0.13555707037448883
CurrentTrain: epoch  6, batch    21 | loss: 4.7581215Losses:  4.719276428222656 0.12180551886558533
CurrentTrain: epoch  6, batch    22 | loss: 4.8410821Losses:  4.5944671630859375 0.10971517860889435
CurrentTrain: epoch  6, batch    23 | loss: 4.7041821Losses:  4.576147079467773 0.13778889179229736
CurrentTrain: epoch  6, batch    24 | loss: 4.7139359Losses:  4.660645961761475 0.09180294722318649
CurrentTrain: epoch  6, batch    25 | loss: 4.7524490Losses:  4.6788434982299805 0.06788654625415802
CurrentTrain: epoch  6, batch    26 | loss: 4.7467299Losses:  4.697031497955322 0.08523204922676086
CurrentTrain: epoch  6, batch    27 | loss: 4.7822638Losses:  4.868481636047363 0.20551319420337677
CurrentTrain: epoch  6, batch    28 | loss: 5.0739946Losses:  4.726095676422119 0.1155284121632576
CurrentTrain: epoch  6, batch    29 | loss: 4.8416243Losses:  4.821805953979492 0.08508596569299698
CurrentTrain: epoch  6, batch    30 | loss: 4.9068918Losses:  4.764218330383301 0.06254489719867706
CurrentTrain: epoch  6, batch    31 | loss: 4.8267632Losses:  4.653360843658447 0.0928855761885643
CurrentTrain: epoch  6, batch    32 | loss: 4.7462463Losses:  5.023439407348633 0.16055577993392944
CurrentTrain: epoch  6, batch    33 | loss: 5.1839952Losses:  4.703592300415039 0.07069697976112366
CurrentTrain: epoch  6, batch    34 | loss: 4.7742891Losses:  4.680546760559082 0.1011156439781189
CurrentTrain: epoch  6, batch    35 | loss: 4.7816625Losses:  4.754197120666504 0.05494217202067375
CurrentTrain: epoch  6, batch    36 | loss: 4.8091393Losses:  5.468396186828613 0.4955540895462036
CurrentTrain: epoch  6, batch    37 | loss: 5.9639502Losses:  4.835625648498535 0.12387635558843613
CurrentTrain: epoch  6, batch    38 | loss: 4.9595022Losses:  5.0951385498046875 0.11233310401439667
CurrentTrain: epoch  6, batch    39 | loss: 5.2074718Losses:  4.723076820373535 0.13034749031066895
CurrentTrain: epoch  6, batch    40 | loss: 4.8534241Losses:  4.666993141174316 0.12547436356544495
CurrentTrain: epoch  6, batch    41 | loss: 4.7924676Losses:  4.613621711730957 0.08077362179756165
CurrentTrain: epoch  6, batch    42 | loss: 4.6943955Losses:  4.830358028411865 0.07314066588878632
CurrentTrain: epoch  6, batch    43 | loss: 4.9034986Losses:  4.662907600402832 0.12388083338737488
CurrentTrain: epoch  6, batch    44 | loss: 4.7867885Losses:  4.645258903503418 0.12571212649345398
CurrentTrain: epoch  6, batch    45 | loss: 4.7709708Losses:  4.6132893562316895 0.09400083124637604
CurrentTrain: epoch  6, batch    46 | loss: 4.7072902Losses:  4.629798412322998 0.10074560344219208
CurrentTrain: epoch  6, batch    47 | loss: 4.7305441Losses:  5.013829231262207 0.17521879076957703
CurrentTrain: epoch  6, batch    48 | loss: 5.1890478Losses:  4.6710662841796875 0.14859363436698914
CurrentTrain: epoch  6, batch    49 | loss: 4.8196597Losses:  4.62630558013916 0.08713945746421814
CurrentTrain: epoch  6, batch    50 | loss: 4.7134452Losses:  4.602611541748047 0.08889605104923248
CurrentTrain: epoch  6, batch    51 | loss: 4.6915078Losses:  4.655751705169678 0.10827133804559708
CurrentTrain: epoch  6, batch    52 | loss: 4.7640228Losses:  4.653655052185059 0.19686678051948547
CurrentTrain: epoch  6, batch    53 | loss: 4.8505220Losses:  4.601873397827148 0.05902780592441559
CurrentTrain: epoch  6, batch    54 | loss: 4.6609011Losses:  4.645150184631348 0.06086255982518196
CurrentTrain: epoch  6, batch    55 | loss: 4.7060127Losses:  4.700716018676758 0.09636862576007843
CurrentTrain: epoch  6, batch    56 | loss: 4.7970848Losses:  4.760190963745117 0.11496811360120773
CurrentTrain: epoch  6, batch    57 | loss: 4.8751593Losses:  4.735549449920654 0.1898186206817627
CurrentTrain: epoch  6, batch    58 | loss: 4.9253683Losses:  4.54192590713501 0.0749569833278656
CurrentTrain: epoch  6, batch    59 | loss: 4.6168828Losses:  4.695470809936523 0.056740909814834595
CurrentTrain: epoch  6, batch    60 | loss: 4.7522116Losses:  5.727996349334717 0.3520912230014801
CurrentTrain: epoch  6, batch    61 | loss: 6.0800877Losses:  4.646723747253418 0.10407097637653351
CurrentTrain: epoch  6, batch    62 | loss: 4.7507949Losses:  4.934918403625488 0.1767359972000122
CurrentTrain: epoch  6, batch    63 | loss: 5.1116543Losses:  4.595634937286377 0.11762209981679916
CurrentTrain: epoch  6, batch    64 | loss: 4.7132568Losses:  4.8605499267578125 0.12092888355255127
CurrentTrain: epoch  6, batch    65 | loss: 4.9814787Losses:  4.668653964996338 0.12835688889026642
CurrentTrain: epoch  6, batch    66 | loss: 4.7970109Losses:  4.646820068359375 0.10083027184009552
CurrentTrain: epoch  6, batch    67 | loss: 4.7476501Losses:  4.59233283996582 0.06771788001060486
CurrentTrain: epoch  6, batch    68 | loss: 4.6600509Losses:  4.679887771606445 0.13833080232143402
CurrentTrain: epoch  6, batch    69 | loss: 4.8182187Losses:  4.550471305847168 0.09180384874343872
CurrentTrain: epoch  6, batch    70 | loss: 4.6422753Losses:  4.523613929748535 0.08202652633190155
CurrentTrain: epoch  6, batch    71 | loss: 4.6056404Losses:  4.779932022094727 0.08192270994186401
CurrentTrain: epoch  6, batch    72 | loss: 4.8618546Losses:  4.642120361328125 0.07387283444404602
CurrentTrain: epoch  6, batch    73 | loss: 4.7159934Losses:  4.883016586303711 0.1516779661178589
CurrentTrain: epoch  6, batch    74 | loss: 5.0346947Losses:  4.611327171325684 0.09828847646713257
CurrentTrain: epoch  7, batch     0 | loss: 4.7096157Losses:  4.7065911293029785 0.052986063063144684
CurrentTrain: epoch  7, batch     1 | loss: 4.7595773Losses:  4.728939056396484 0.05840950086712837
CurrentTrain: epoch  7, batch     2 | loss: 4.7873487Losses:  4.59245491027832 0.08433768153190613
CurrentTrain: epoch  7, batch     3 | loss: 4.6767926Losses:  5.124905586242676 0.30726301670074463
CurrentTrain: epoch  7, batch     4 | loss: 5.4321685Losses:  4.601654052734375 0.08001711964607239
CurrentTrain: epoch  7, batch     5 | loss: 4.6816711Losses:  4.6225690841674805 0.08466386795043945
CurrentTrain: epoch  7, batch     6 | loss: 4.7072330Losses:  4.515325546264648 0.044140756130218506
CurrentTrain: epoch  7, batch     7 | loss: 4.5594664Losses:  4.521705627441406 0.07915271818637848
CurrentTrain: epoch  7, batch     8 | loss: 4.6008582Losses:  4.596226692199707 0.09615564346313477
CurrentTrain: epoch  7, batch     9 | loss: 4.6923823Losses:  4.566378593444824 0.08869549632072449
CurrentTrain: epoch  7, batch    10 | loss: 4.6550741Losses:  4.5716142654418945 0.09746477007865906
CurrentTrain: epoch  7, batch    11 | loss: 4.6690788Losses:  4.68264102935791 0.1029830127954483
CurrentTrain: epoch  7, batch    12 | loss: 4.7856240Losses:  4.745147705078125 0.11860276758670807
CurrentTrain: epoch  7, batch    13 | loss: 4.8637505Losses:  4.508399963378906 0.07946868985891342
CurrentTrain: epoch  7, batch    14 | loss: 4.5878687Losses:  4.577447891235352 0.10838061571121216
CurrentTrain: epoch  7, batch    15 | loss: 4.6858287Losses:  4.583059787750244 0.0822935625910759
CurrentTrain: epoch  7, batch    16 | loss: 4.6653533Losses:  4.5741682052612305 0.09506500512361526
CurrentTrain: epoch  7, batch    17 | loss: 4.6692333Losses:  4.475349426269531 0.03866514936089516
CurrentTrain: epoch  7, batch    18 | loss: 4.5140147Losses:  4.733590126037598 0.08315315842628479
CurrentTrain: epoch  7, batch    19 | loss: 4.8167434Losses:  4.58620548248291 0.1044975072145462
CurrentTrain: epoch  7, batch    20 | loss: 4.6907029Losses:  4.511002063751221 0.10532127320766449
CurrentTrain: epoch  7, batch    21 | loss: 4.6163235Losses:  4.631497383117676 0.094705730676651
CurrentTrain: epoch  7, batch    22 | loss: 4.7262030Losses:  4.573066234588623 0.11609897017478943
CurrentTrain: epoch  7, batch    23 | loss: 4.6891651Losses:  4.5265960693359375 0.07774534821510315
CurrentTrain: epoch  7, batch    24 | loss: 4.6043415Losses:  4.565584182739258 0.08863461017608643
CurrentTrain: epoch  7, batch    25 | loss: 4.6542187Losses:  4.496881484985352 0.09396035969257355
CurrentTrain: epoch  7, batch    26 | loss: 4.5908418Losses:  4.5161919593811035 0.08169689029455185
CurrentTrain: epoch  7, batch    27 | loss: 4.5978889Losses:  4.5444135665893555 0.07115642726421356
CurrentTrain: epoch  7, batch    28 | loss: 4.6155701Losses:  4.645890235900879 0.0595383457839489
CurrentTrain: epoch  7, batch    29 | loss: 4.7054286Losses:  4.532743453979492 0.08664873987436295
CurrentTrain: epoch  7, batch    30 | loss: 4.6193924Losses:  4.5670166015625 0.10629118978977203
CurrentTrain: epoch  7, batch    31 | loss: 4.6733079Losses:  4.515801429748535 0.0969693511724472
CurrentTrain: epoch  7, batch    32 | loss: 4.6127706Losses:  4.539724349975586 0.09771516174077988
CurrentTrain: epoch  7, batch    33 | loss: 4.6374397Losses:  4.6588335037231445 0.09177510440349579
CurrentTrain: epoch  7, batch    34 | loss: 4.7506084Losses:  4.503075122833252 0.09401939064264297
CurrentTrain: epoch  7, batch    35 | loss: 4.5970945Losses:  4.589672088623047 0.0330720916390419
CurrentTrain: epoch  7, batch    36 | loss: 4.6227441Losses:  4.607925891876221 0.05622095614671707
CurrentTrain: epoch  7, batch    37 | loss: 4.6641469Losses:  4.62211799621582 0.08599862456321716
CurrentTrain: epoch  7, batch    38 | loss: 4.7081165Losses:  4.642115592956543 0.08901287615299225
CurrentTrain: epoch  7, batch    39 | loss: 4.7311287Losses:  4.568877220153809 0.09112462401390076
CurrentTrain: epoch  7, batch    40 | loss: 4.6600018Losses:  4.591277122497559 0.07124534249305725
CurrentTrain: epoch  7, batch    41 | loss: 4.6625223Losses:  4.555250644683838 0.0610639862716198
CurrentTrain: epoch  7, batch    42 | loss: 4.6163144Losses:  4.513420104980469 0.07286150753498077
CurrentTrain: epoch  7, batch    43 | loss: 4.5862818Losses:  4.610936164855957 0.03685970604419708
CurrentTrain: epoch  7, batch    44 | loss: 4.6477957Losses:  4.5192341804504395 0.04066430404782295
CurrentTrain: epoch  7, batch    45 | loss: 4.5598984Losses:  4.596708297729492 0.1553233563899994
CurrentTrain: epoch  7, batch    46 | loss: 4.7520318Losses:  4.516926288604736 0.09126584976911545
CurrentTrain: epoch  7, batch    47 | loss: 4.6081920Losses:  4.516537666320801 0.07168598473072052
CurrentTrain: epoch  7, batch    48 | loss: 4.5882235Losses:  4.569244384765625 0.10769529640674591
CurrentTrain: epoch  7, batch    49 | loss: 4.6769395Losses:  4.546139240264893 0.07896215468645096
CurrentTrain: epoch  7, batch    50 | loss: 4.6251016Losses:  4.574774742126465 0.11912605911493301
CurrentTrain: epoch  7, batch    51 | loss: 4.6939006Losses:  4.510462760925293 0.09244649112224579
CurrentTrain: epoch  7, batch    52 | loss: 4.6029091Losses:  4.49582052230835 0.09774141013622284
CurrentTrain: epoch  7, batch    53 | loss: 4.5935621Losses:  4.609226226806641 0.08808154612779617
CurrentTrain: epoch  7, batch    54 | loss: 4.6973076Losses:  4.560698509216309 0.069746233522892
CurrentTrain: epoch  7, batch    55 | loss: 4.6304445Losses:  4.5295491218566895 0.060132041573524475
CurrentTrain: epoch  7, batch    56 | loss: 4.5896811Losses:  4.556135654449463 0.11212371289730072
CurrentTrain: epoch  7, batch    57 | loss: 4.6682591Losses:  4.531174659729004 0.12141988426446915
CurrentTrain: epoch  7, batch    58 | loss: 4.6525946Losses:  4.500802516937256 0.07121841609477997
CurrentTrain: epoch  7, batch    59 | loss: 4.5720210Losses:  4.633513927459717 0.16029974818229675
CurrentTrain: epoch  7, batch    60 | loss: 4.7938137Losses:  4.54493522644043 0.13007675111293793
CurrentTrain: epoch  7, batch    61 | loss: 4.6750121Losses:  4.491275310516357 0.07792380452156067
CurrentTrain: epoch  7, batch    62 | loss: 4.5691991Losses:  4.52683687210083 0.0901462584733963
CurrentTrain: epoch  7, batch    63 | loss: 4.6169829Losses:  4.544558048248291 0.0674939900636673
CurrentTrain: epoch  7, batch    64 | loss: 4.6120520Losses:  4.497143268585205 0.07603272050619125
CurrentTrain: epoch  7, batch    65 | loss: 4.5731759Losses:  4.980926036834717 0.09624514728784561
CurrentTrain: epoch  7, batch    66 | loss: 5.0771713Losses:  4.536807060241699 0.023719513788819313
CurrentTrain: epoch  7, batch    67 | loss: 4.5605264Losses:  4.5276079177856445 0.0737861916422844
CurrentTrain: epoch  7, batch    68 | loss: 4.6013942Losses:  4.584611415863037 0.08458924293518066
CurrentTrain: epoch  7, batch    69 | loss: 4.6692009Losses:  4.547492504119873 0.05739539861679077
CurrentTrain: epoch  7, batch    70 | loss: 4.6048880Losses:  4.496605396270752 0.07124508172273636
CurrentTrain: epoch  7, batch    71 | loss: 4.5678506Losses:  4.556667327880859 0.06552368402481079
CurrentTrain: epoch  7, batch    72 | loss: 4.6221910Losses:  4.586139678955078 0.07560554891824722
CurrentTrain: epoch  7, batch    73 | loss: 4.6617451Losses:  5.31821346282959 0.3156672418117523
CurrentTrain: epoch  7, batch    74 | loss: 5.6338806Losses:  4.5327558517456055 0.07804945856332779
CurrentTrain: epoch  8, batch     0 | loss: 4.6108055Losses:  4.489990711212158 0.0574628971517086
CurrentTrain: epoch  8, batch     1 | loss: 4.5474534Losses:  4.449873924255371 0.0386839359998703
CurrentTrain: epoch  8, batch     2 | loss: 4.4885578Losses:  4.599100112915039 0.11292868107557297
CurrentTrain: epoch  8, batch     3 | loss: 4.7120290Losses:  4.506176948547363 0.05517331510782242
CurrentTrain: epoch  8, batch     4 | loss: 4.5613503Losses:  4.516104698181152 0.05822727456688881
CurrentTrain: epoch  8, batch     5 | loss: 4.5743318Losses:  4.594983100891113 0.04597312584519386
CurrentTrain: epoch  8, batch     6 | loss: 4.6409564Losses:  4.513461112976074 0.08158786594867706
CurrentTrain: epoch  8, batch     7 | loss: 4.5950489Losses:  4.541764736175537 0.060352202504873276
CurrentTrain: epoch  8, batch     8 | loss: 4.6021171Losses:  4.572534084320068 0.054185181856155396
CurrentTrain: epoch  8, batch     9 | loss: 4.6267195Losses:  4.456173896789551 0.025613155215978622
CurrentTrain: epoch  8, batch    10 | loss: 4.4817872Losses:  4.648613929748535 0.031539320945739746
CurrentTrain: epoch  8, batch    11 | loss: 4.6801534Losses:  4.583331108093262 0.08975419402122498
CurrentTrain: epoch  8, batch    12 | loss: 4.6730852Losses:  4.525029182434082 0.07342644780874252
CurrentTrain: epoch  8, batch    13 | loss: 4.5984554Losses:  4.55048942565918 0.07397700101137161
CurrentTrain: epoch  8, batch    14 | loss: 4.6244664Losses:  4.532792091369629 0.10449911653995514
CurrentTrain: epoch  8, batch    15 | loss: 4.6372914Losses:  4.527729034423828 0.0752495527267456
CurrentTrain: epoch  8, batch    16 | loss: 4.6029787Losses:  4.4960126876831055 0.06774081289768219
CurrentTrain: epoch  8, batch    17 | loss: 4.5637536Losses:  4.602789878845215 0.0957241952419281
CurrentTrain: epoch  8, batch    18 | loss: 4.6985140Losses:  5.0895233154296875 0.45959675312042236
CurrentTrain: epoch  8, batch    19 | loss: 5.5491199Losses:  4.514138221740723 0.08458393067121506
CurrentTrain: epoch  8, batch    20 | loss: 4.5987220Losses:  4.547111988067627 0.09098942577838898
CurrentTrain: epoch  8, batch    21 | loss: 4.6381016Losses:  4.551800727844238 0.08696684241294861
CurrentTrain: epoch  8, batch    22 | loss: 4.6387677Losses:  4.5351176261901855 0.06735151261091232
CurrentTrain: epoch  8, batch    23 | loss: 4.6024690Losses:  4.497309684753418 0.06910350918769836
CurrentTrain: epoch  8, batch    24 | loss: 4.5664134Losses:  4.594498634338379 0.05592647194862366
CurrentTrain: epoch  8, batch    25 | loss: 4.6504250Losses:  4.520443916320801 0.042118463665246964
CurrentTrain: epoch  8, batch    26 | loss: 4.5625625Losses:  4.645422458648682 0.11006427556276321
CurrentTrain: epoch  8, batch    27 | loss: 4.7554870Losses:  4.5243377685546875 0.06337078660726547
CurrentTrain: epoch  8, batch    28 | loss: 4.5877085Losses:  4.698158264160156 0.09079079329967499
CurrentTrain: epoch  8, batch    29 | loss: 4.7889490Losses:  4.581884384155273 0.026812365278601646
CurrentTrain: epoch  8, batch    30 | loss: 4.6086969Losses:  4.582244873046875 0.05688469111919403
CurrentTrain: epoch  8, batch    31 | loss: 4.6391296Losses:  4.59619140625 0.06553791463375092
CurrentTrain: epoch  8, batch    32 | loss: 4.6617293Losses:  4.546113014221191 0.03371778130531311
CurrentTrain: epoch  8, batch    33 | loss: 4.5798306Losses:  4.533566474914551 0.061812564730644226
CurrentTrain: epoch  8, batch    34 | loss: 4.5953789Losses:  4.504304885864258 0.07175450026988983
CurrentTrain: epoch  8, batch    35 | loss: 4.5760593Losses:  4.600836753845215 0.07790783047676086
CurrentTrain: epoch  8, batch    36 | loss: 4.6787448Losses:  4.516134262084961 0.09183627367019653
CurrentTrain: epoch  8, batch    37 | loss: 4.6079707Losses:  4.501976013183594 0.08615982532501221
CurrentTrain: epoch  8, batch    38 | loss: 4.5881357Losses:  4.580981254577637 0.06800658255815506
CurrentTrain: epoch  8, batch    39 | loss: 4.6489878Losses:  4.440707206726074 0.05298752710223198
CurrentTrain: epoch  8, batch    40 | loss: 4.4936948Losses:  4.5287556648254395 0.0772060677409172
CurrentTrain: epoch  8, batch    41 | loss: 4.6059618Losses:  4.567673206329346 0.0631444901227951
CurrentTrain: epoch  8, batch    42 | loss: 4.6308179Losses:  4.52811336517334 0.07293550670146942
CurrentTrain: epoch  8, batch    43 | loss: 4.6010489Losses:  4.480854034423828 0.03809921443462372
CurrentTrain: epoch  8, batch    44 | loss: 4.5189533Losses:  4.580521583557129 0.05468760430812836
CurrentTrain: epoch  8, batch    45 | loss: 4.6352091Losses:  4.529726982116699 0.10699468851089478
CurrentTrain: epoch  8, batch    46 | loss: 4.6367216Losses:  4.523687362670898 0.09817206859588623
CurrentTrain: epoch  8, batch    47 | loss: 4.6218596Losses:  4.513204574584961 0.040413595736026764
CurrentTrain: epoch  8, batch    48 | loss: 4.5536180Losses:  4.5336456298828125 0.056164152920246124
CurrentTrain: epoch  8, batch    49 | loss: 4.5898099Losses:  4.451794147491455 0.07422320544719696
CurrentTrain: epoch  8, batch    50 | loss: 4.5260172Losses:  4.49293851852417 0.06130675598978996
CurrentTrain: epoch  8, batch    51 | loss: 4.5542455Losses:  4.551484107971191 0.030115483328700066
CurrentTrain: epoch  8, batch    52 | loss: 4.5815997Losses:  4.516767978668213 0.06818778812885284
CurrentTrain: epoch  8, batch    53 | loss: 4.5849557Losses:  4.648303985595703 0.11372682452201843
CurrentTrain: epoch  8, batch    54 | loss: 4.7620306Losses:  4.516895771026611 0.03967641294002533
CurrentTrain: epoch  8, batch    55 | loss: 4.5565720Losses:  4.517687797546387 0.06702771037817001
CurrentTrain: epoch  8, batch    56 | loss: 4.5847154Losses:  4.500334739685059 0.08907493948936462
CurrentTrain: epoch  8, batch    57 | loss: 4.5894098Losses:  4.52535343170166 0.054309599101543427
CurrentTrain: epoch  8, batch    58 | loss: 4.5796628Losses:  4.547524452209473 0.08376578986644745
CurrentTrain: epoch  8, batch    59 | loss: 4.6312904Losses:  5.049847602844238 0.09083914756774902
CurrentTrain: epoch  8, batch    60 | loss: 5.1406870Losses:  4.583527088165283 0.030757378786802292
CurrentTrain: epoch  8, batch    61 | loss: 4.6142845Losses:  4.504681587219238 0.03885626047849655
CurrentTrain: epoch  8, batch    62 | loss: 4.5435376Losses:  4.553309440612793 0.1040688082575798
CurrentTrain: epoch  8, batch    63 | loss: 4.6573782Losses:  4.565603256225586 0.05618813633918762
CurrentTrain: epoch  8, batch    64 | loss: 4.6217914Losses:  4.517534255981445 0.08887729793787003
CurrentTrain: epoch  8, batch    65 | loss: 4.6064115Losses:  4.551204681396484 0.053064316511154175
CurrentTrain: epoch  8, batch    66 | loss: 4.6042690Losses:  4.493104934692383 0.08627796173095703
CurrentTrain: epoch  8, batch    67 | loss: 4.5793829Losses:  4.532435417175293 0.08076916635036469
CurrentTrain: epoch  8, batch    68 | loss: 4.6132045Losses:  4.97232723236084 0.2869625687599182
CurrentTrain: epoch  8, batch    69 | loss: 5.2592897Losses:  4.5756096839904785 0.07567961513996124
CurrentTrain: epoch  8, batch    70 | loss: 4.6512895Losses:  4.569346904754639 0.06414186209440231
CurrentTrain: epoch  8, batch    71 | loss: 4.6334887Losses:  4.511134624481201 0.061806224286556244
CurrentTrain: epoch  8, batch    72 | loss: 4.5729408Losses:  4.550180435180664 0.06969091296195984
CurrentTrain: epoch  8, batch    73 | loss: 4.6198711Losses:  5.420524597167969 0.233424574136734
CurrentTrain: epoch  8, batch    74 | loss: 5.6539493Losses:  4.59987735748291 0.06531696766614914
CurrentTrain: epoch  9, batch     0 | loss: 4.6651945Losses:  4.903059005737305 0.10408763587474823
CurrentTrain: epoch  9, batch     1 | loss: 5.0071468Losses:  4.716314315795898 0.16571049392223358
CurrentTrain: epoch  9, batch     2 | loss: 4.8820248Losses:  4.611388683319092 0.07604452222585678
CurrentTrain: epoch  9, batch     3 | loss: 4.6874332Losses:  4.6928935050964355 0.06384710222482681
CurrentTrain: epoch  9, batch     4 | loss: 4.7567406Losses:  4.567419052124023 0.06331800669431686
CurrentTrain: epoch  9, batch     5 | loss: 4.6307368Losses:  4.539703845977783 0.08926670253276825
CurrentTrain: epoch  9, batch     6 | loss: 4.6289706Losses:  4.528441429138184 0.07406233251094818
CurrentTrain: epoch  9, batch     7 | loss: 4.6025038Losses:  4.496512413024902 0.03733539953827858
CurrentTrain: epoch  9, batch     8 | loss: 4.5338478Losses:  4.512861728668213 0.03793831169605255
CurrentTrain: epoch  9, batch     9 | loss: 4.5507998Losses:  4.548564910888672 0.06646318733692169
CurrentTrain: epoch  9, batch    10 | loss: 4.6150279Losses:  4.627087116241455 0.0730208158493042
CurrentTrain: epoch  9, batch    11 | loss: 4.7001081Losses:  4.567356586456299 0.0682244524359703
CurrentTrain: epoch  9, batch    12 | loss: 4.6355810Losses:  4.523566246032715 0.037791624665260315
CurrentTrain: epoch  9, batch    13 | loss: 4.5613580Losses:  4.543243408203125 0.08285502344369888
CurrentTrain: epoch  9, batch    14 | loss: 4.6260986Losses:  4.636751174926758 0.09574626386165619
CurrentTrain: epoch  9, batch    15 | loss: 4.7324972Losses:  4.5825042724609375 0.14814718067646027
CurrentTrain: epoch  9, batch    16 | loss: 4.7306514Losses:  4.893616199493408 0.17810624837875366
CurrentTrain: epoch  9, batch    17 | loss: 5.0717225Losses:  4.488350868225098 0.06662862747907639
CurrentTrain: epoch  9, batch    18 | loss: 4.5549793Losses:  4.549413204193115 0.0722958892583847
CurrentTrain: epoch  9, batch    19 | loss: 4.6217089Losses:  4.524160861968994 0.06521609425544739
CurrentTrain: epoch  9, batch    20 | loss: 4.5893769Losses:  4.553351879119873 0.04726316034793854
CurrentTrain: epoch  9, batch    21 | loss: 4.6006150Losses:  4.590233325958252 0.051375217735767365
CurrentTrain: epoch  9, batch    22 | loss: 4.6416087Losses:  4.7015275955200195 0.13389547169208527
CurrentTrain: epoch  9, batch    23 | loss: 4.8354230Losses:  4.546168804168701 0.0654674768447876
CurrentTrain: epoch  9, batch    24 | loss: 4.6116362Losses:  4.529048919677734 0.04940175265073776
CurrentTrain: epoch  9, batch    25 | loss: 4.5784507Losses:  4.620573997497559 0.09942813217639923
CurrentTrain: epoch  9, batch    26 | loss: 4.7200022Losses:  4.525765895843506 0.06513690948486328
CurrentTrain: epoch  9, batch    27 | loss: 4.5909028Losses:  4.5185089111328125 0.056393250823020935
CurrentTrain: epoch  9, batch    28 | loss: 4.5749021Losses:  4.529455661773682 0.052934348583221436
CurrentTrain: epoch  9, batch    29 | loss: 4.5823898Losses:  4.511821746826172 0.06919033825397491
CurrentTrain: epoch  9, batch    30 | loss: 4.5810122Losses:  4.561600208282471 0.04543086141347885
CurrentTrain: epoch  9, batch    31 | loss: 4.6070309Losses:  4.476729869842529 0.052948564291000366
CurrentTrain: epoch  9, batch    32 | loss: 4.5296783Losses:  4.545251846313477 0.07510021328926086
CurrentTrain: epoch  9, batch    33 | loss: 4.6203523Losses:  4.602826118469238 0.06749414652585983
CurrentTrain: epoch  9, batch    34 | loss: 4.6703200Losses:  4.495119571685791 0.05370554327964783
CurrentTrain: epoch  9, batch    35 | loss: 4.5488253Losses:  4.569202899932861 0.048048410564661026
CurrentTrain: epoch  9, batch    36 | loss: 4.6172514Losses:  4.579039096832275 0.04521333426237106
CurrentTrain: epoch  9, batch    37 | loss: 4.6242523Losses:  4.597782611846924 0.07426436990499496
CurrentTrain: epoch  9, batch    38 | loss: 4.6720471Losses:  4.529995918273926 0.08093824982643127
CurrentTrain: epoch  9, batch    39 | loss: 4.6109343Losses:  4.523282051086426 0.07780957221984863
CurrentTrain: epoch  9, batch    40 | loss: 4.6010914Losses:  4.513131141662598 0.05979976803064346
CurrentTrain: epoch  9, batch    41 | loss: 4.5729308Losses:  4.497878074645996 0.07822093367576599
CurrentTrain: epoch  9, batch    42 | loss: 4.5760989Losses:  4.495316505432129 0.06570135802030563
CurrentTrain: epoch  9, batch    43 | loss: 4.5610180Losses:  4.52352237701416 0.04701772332191467
CurrentTrain: epoch  9, batch    44 | loss: 4.5705400Losses:  4.463983058929443 0.061421528458595276
CurrentTrain: epoch  9, batch    45 | loss: 4.5254045Losses:  4.515195846557617 0.10415393114089966
CurrentTrain: epoch  9, batch    46 | loss: 4.6193500Losses:  4.514247894287109 0.06105098873376846
CurrentTrain: epoch  9, batch    47 | loss: 4.5752988Losses:  4.527287006378174 0.06197914481163025
CurrentTrain: epoch  9, batch    48 | loss: 4.5892663Losses:  4.548452377319336 0.0493263378739357
CurrentTrain: epoch  9, batch    49 | loss: 4.5977788Losses:  4.938636779785156 0.09776565432548523
CurrentTrain: epoch  9, batch    50 | loss: 5.0364022Losses:  4.533734321594238 0.07545043528079987
CurrentTrain: epoch  9, batch    51 | loss: 4.6091847Losses:  4.531574249267578 0.0787879154086113
CurrentTrain: epoch  9, batch    52 | loss: 4.6103621Losses:  4.515721797943115 0.06895063817501068
CurrentTrain: epoch  9, batch    53 | loss: 4.5846725Losses:  4.500991344451904 0.038259074091911316
CurrentTrain: epoch  9, batch    54 | loss: 4.5392504Losses:  4.527682304382324 0.063718281686306
CurrentTrain: epoch  9, batch    55 | loss: 4.5914006Losses:  4.537988662719727 0.0744069516658783
CurrentTrain: epoch  9, batch    56 | loss: 4.6123958Losses:  4.517866134643555 0.06203678995370865
CurrentTrain: epoch  9, batch    57 | loss: 4.5799031Losses:  4.537137508392334 0.07186605036258698
CurrentTrain: epoch  9, batch    58 | loss: 4.6090035Losses:  4.707013130187988 0.09002071619033813
CurrentTrain: epoch  9, batch    59 | loss: 4.7970338Losses:  4.564838409423828 0.04468649998307228
CurrentTrain: epoch  9, batch    60 | loss: 4.6095247Losses:  4.557803153991699 0.07679636031389236
CurrentTrain: epoch  9, batch    61 | loss: 4.6345997Losses:  4.768152236938477 0.06399755924940109
CurrentTrain: epoch  9, batch    62 | loss: 4.8321500Losses:  4.527477264404297 0.06499034911394119
CurrentTrain: epoch  9, batch    63 | loss: 4.5924678Losses:  4.504607200622559 0.08844595402479172
CurrentTrain: epoch  9, batch    64 | loss: 4.5930533Losses:  4.534836769104004 0.06649500131607056
CurrentTrain: epoch  9, batch    65 | loss: 4.6013317Losses:  5.735356330871582 0.49348512291908264
CurrentTrain: epoch  9, batch    66 | loss: 6.2288413Losses:  5.126319408416748 0.09807693958282471
CurrentTrain: epoch  9, batch    67 | loss: 5.2243962Losses:  4.557684421539307 0.08373008668422699
CurrentTrain: epoch  9, batch    68 | loss: 4.6414146Losses:  4.6632795333862305 0.07173050940036774
CurrentTrain: epoch  9, batch    69 | loss: 4.7350101Losses:  4.579066276550293 0.07670217752456665
CurrentTrain: epoch  9, batch    70 | loss: 4.6557684Losses:  4.470283508300781 0.052016861736774445
CurrentTrain: epoch  9, batch    71 | loss: 4.5223002Losses:  4.529762268066406 0.07282747328281403
CurrentTrain: epoch  9, batch    72 | loss: 4.6025896Losses:  4.6121931076049805 0.10075339674949646
CurrentTrain: epoch  9, batch    73 | loss: 4.7129464Losses:  4.487404823303223 0.07380777597427368
CurrentTrain: epoch  9, batch    74 | loss: 4.5612125
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 82.77%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.87%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.18%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 82.77%   
cur_acc:  ['0.8277']
his_acc:  ['0.8277']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 0 1 0 0 1]
Losses:  7.369796276092529 0.7619168162345886
CurrentTrain: epoch  0, batch     0 | loss: 8.1317129Losses:  10.873184204101562 1.4360530376434326
CurrentTrain: epoch  0, batch     1 | loss: 12.3092375Losses:  10.56157112121582 0.8497229814529419
CurrentTrain: epoch  0, batch     2 | loss: 11.4112940Losses:  8.280620574951172 0.9622424244880676
CurrentTrain: epoch  0, batch     3 | loss: 9.2428627Losses:  5.142066955566406 1.2666499614715576
CurrentTrain: epoch  1, batch     0 | loss: 6.4087172Losses:  3.926981210708618 1.3388592004776
CurrentTrain: epoch  1, batch     1 | loss: 5.2658405Losses:  4.694521903991699 1.5557146072387695
CurrentTrain: epoch  1, batch     2 | loss: 6.2502365Losses:  5.664285182952881 1.2021164894104004
CurrentTrain: epoch  1, batch     3 | loss: 6.8664017Losses:  4.510834693908691 1.0257291793823242
CurrentTrain: epoch  2, batch     0 | loss: 5.5365639Losses:  5.074047565460205 1.3656694889068604
CurrentTrain: epoch  2, batch     1 | loss: 6.4397173Losses:  3.317002773284912 0.7446649670600891
CurrentTrain: epoch  2, batch     2 | loss: 4.0616679Losses:  4.38144588470459 1.282463788986206
CurrentTrain: epoch  2, batch     3 | loss: 5.6639099Losses:  3.877678871154785 1.0046247243881226
CurrentTrain: epoch  3, batch     0 | loss: 4.8823037Losses:  4.12069845199585 0.9594812393188477
CurrentTrain: epoch  3, batch     1 | loss: 5.0801797Losses:  4.339437961578369 0.7730125188827515
CurrentTrain: epoch  3, batch     2 | loss: 5.1124506Losses:  3.080390453338623 0.8352869749069214
CurrentTrain: epoch  3, batch     3 | loss: 3.9156775Losses:  3.5323030948638916 1.1054751873016357
CurrentTrain: epoch  4, batch     0 | loss: 4.6377783Losses:  4.293288707733154 0.9327631592750549
CurrentTrain: epoch  4, batch     1 | loss: 5.2260518Losses:  4.219939708709717 1.1386165618896484
CurrentTrain: epoch  4, batch     2 | loss: 5.3585563Losses:  3.035417318344116 0.7194458842277527
CurrentTrain: epoch  4, batch     3 | loss: 3.7548633Losses:  3.9145891666412354 0.6665648221969604
CurrentTrain: epoch  5, batch     0 | loss: 4.5811539Losses:  3.0145263671875 0.7075496912002563
CurrentTrain: epoch  5, batch     1 | loss: 3.7220759Losses:  2.749748706817627 0.714806318283081
CurrentTrain: epoch  5, batch     2 | loss: 3.4645550Losses:  4.553682804107666 0.8623225688934326
CurrentTrain: epoch  5, batch     3 | loss: 5.4160051Losses:  2.4802825450897217 0.8829907178878784
CurrentTrain: epoch  6, batch     0 | loss: 3.3632731Losses:  4.402862071990967 0.9992458820343018
CurrentTrain: epoch  6, batch     1 | loss: 5.4021082Losses:  2.2580838203430176 0.3636876344680786
CurrentTrain: epoch  6, batch     2 | loss: 2.6217713Losses:  3.733121156692505 0.7233141660690308
CurrentTrain: epoch  6, batch     3 | loss: 4.4564352Losses:  2.5155632495880127 0.6521466970443726
CurrentTrain: epoch  7, batch     0 | loss: 3.1677098Losses:  3.6825356483459473 0.8323684930801392
CurrentTrain: epoch  7, batch     1 | loss: 4.5149040Losses:  2.606823444366455 0.5197217464447021
CurrentTrain: epoch  7, batch     2 | loss: 3.1265452Losses:  3.577401638031006 0.6052743196487427
CurrentTrain: epoch  7, batch     3 | loss: 4.1826758Losses:  3.875459671020508 0.8989362716674805
CurrentTrain: epoch  8, batch     0 | loss: 4.7743959Losses:  3.1482741832733154 0.8107002973556519
CurrentTrain: epoch  8, batch     1 | loss: 3.9589744Losses:  1.7723636627197266 0.6388338804244995
CurrentTrain: epoch  8, batch     2 | loss: 2.4111977Losses:  3.155055046081543 0.6337490081787109
CurrentTrain: epoch  8, batch     3 | loss: 3.7888041Losses:  3.1597602367401123 0.700603723526001
CurrentTrain: epoch  9, batch     0 | loss: 3.8603640Losses:  2.685851573944092 0.6589672565460205
CurrentTrain: epoch  9, batch     1 | loss: 3.3448188Losses:  3.0007379055023193 0.5548834800720215
CurrentTrain: epoch  9, batch     2 | loss: 3.5556214Losses:  1.9245914220809937 0.5116742253303528
CurrentTrain: epoch  9, batch     3 | loss: 2.4362657
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.35%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 87.06%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.94%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 85.46%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 83.64%   
cur_acc:  ['0.8277', '0.8214']
his_acc:  ['0.8277', '0.8364']
Clustering into  3  clusters
Clusters:  [1 0 0 0 0 0 1 2 0 0 2 1 0 0 0 0]
Losses:  7.061181545257568 1.0114266872406006
CurrentTrain: epoch  0, batch     0 | loss: 8.0726080Losses:  11.188602447509766 1.347534418106079
CurrentTrain: epoch  0, batch     1 | loss: 12.5361366Losses:  9.733779907226562 1.1867092847824097
CurrentTrain: epoch  0, batch     2 | loss: 10.9204893Losses:  10.929863929748535 1.0427709817886353
CurrentTrain: epoch  0, batch     3 | loss: 11.9726353Losses:  9.87226390838623 0.4907422363758087
CurrentTrain: epoch  0, batch     4 | loss: 10.3630066Losses:  3.624732494354248 0.7828577756881714
CurrentTrain: epoch  1, batch     0 | loss: 4.4075904Losses:  6.163477897644043 1.0723985433578491
CurrentTrain: epoch  1, batch     1 | loss: 7.2358766Losses:  3.9415369033813477 1.0636532306671143
CurrentTrain: epoch  1, batch     2 | loss: 5.0051899Losses:  6.76904296875 0.6115084886550903
CurrentTrain: epoch  1, batch     3 | loss: 7.3805513Losses:  3.869826078414917 0.8886095285415649
CurrentTrain: epoch  1, batch     4 | loss: 4.7584357Losses:  5.076230049133301 1.1610355377197266
CurrentTrain: epoch  2, batch     0 | loss: 6.2372656Losses:  4.035381317138672 0.8226194381713867
CurrentTrain: epoch  2, batch     1 | loss: 4.8580008Losses:  5.09378719329834 1.377530574798584
CurrentTrain: epoch  2, batch     2 | loss: 6.4713178Losses:  4.1305975914001465 0.7807229161262512
CurrentTrain: epoch  2, batch     3 | loss: 4.9113207Losses:  4.251060485839844 0.933195948600769
CurrentTrain: epoch  2, batch     4 | loss: 5.1842566Losses:  5.075736045837402 1.1130119562149048
CurrentTrain: epoch  3, batch     0 | loss: 6.1887479Losses:  4.487576961517334 0.7353253364562988
CurrentTrain: epoch  3, batch     1 | loss: 5.2229023Losses:  3.0315582752227783 0.7737905979156494
CurrentTrain: epoch  3, batch     2 | loss: 3.8053489Losses:  4.195587158203125 0.6049530506134033
CurrentTrain: epoch  3, batch     3 | loss: 4.8005400Losses:  3.8095481395721436 0.5874185562133789
CurrentTrain: epoch  3, batch     4 | loss: 4.3969669Losses:  3.328857660293579 0.8370590209960938
CurrentTrain: epoch  4, batch     0 | loss: 4.1659164Losses:  3.420783758163452 1.0374966859817505
CurrentTrain: epoch  4, batch     1 | loss: 4.4582806Losses:  4.284972190856934 0.5271422863006592
CurrentTrain: epoch  4, batch     2 | loss: 4.8121147Losses:  5.546367168426514 0.9211997389793396
CurrentTrain: epoch  4, batch     3 | loss: 6.4675670Losses:  1.7862496376037598 0.4934067726135254
CurrentTrain: epoch  4, batch     4 | loss: 2.2796564Losses:  3.8873305320739746 0.7230385541915894
CurrentTrain: epoch  5, batch     0 | loss: 4.6103692Losses:  3.3990836143493652 0.8307468295097351
CurrentTrain: epoch  5, batch     1 | loss: 4.2298303Losses:  3.986220121383667 0.5299373865127563
CurrentTrain: epoch  5, batch     2 | loss: 4.5161576Losses:  3.61203670501709 0.8599798083305359
CurrentTrain: epoch  5, batch     3 | loss: 4.4720163Losses:  1.7829828262329102 0.45424938201904297
CurrentTrain: epoch  5, batch     4 | loss: 2.2372322Losses:  3.238508939743042 0.44093579053878784
CurrentTrain: epoch  6, batch     0 | loss: 3.6794448Losses:  3.787334442138672 0.8274897336959839
CurrentTrain: epoch  6, batch     1 | loss: 4.6148243Losses:  3.0999886989593506 1.021647572517395
CurrentTrain: epoch  6, batch     2 | loss: 4.1216364Losses:  2.592775821685791 1.0279161930084229
CurrentTrain: epoch  6, batch     3 | loss: 3.6206920Losses:  4.150681018829346 0.3193091154098511
CurrentTrain: epoch  6, batch     4 | loss: 4.4699903Losses:  3.4494946002960205 1.1573998928070068
CurrentTrain: epoch  7, batch     0 | loss: 4.6068945Losses:  2.6932592391967773 0.9145152568817139
CurrentTrain: epoch  7, batch     1 | loss: 3.6077745Losses:  2.7650842666625977 0.8391695618629456
CurrentTrain: epoch  7, batch     2 | loss: 3.6042538Losses:  2.8629722595214844 0.7049438953399658
CurrentTrain: epoch  7, batch     3 | loss: 3.5679162Losses:  3.811954975128174 0.09274789690971375
CurrentTrain: epoch  7, batch     4 | loss: 3.9047029Losses:  1.846850872039795 0.7207953333854675
CurrentTrain: epoch  8, batch     0 | loss: 2.5676463Losses:  3.063778877258301 0.8433551788330078
CurrentTrain: epoch  8, batch     1 | loss: 3.9071341Losses:  3.1588621139526367 0.9102643728256226
CurrentTrain: epoch  8, batch     2 | loss: 4.0691266Losses:  2.5994181632995605 0.7508591413497925
CurrentTrain: epoch  8, batch     3 | loss: 3.3502774Losses:  3.7762835025787354 0.31845584511756897
CurrentTrain: epoch  8, batch     4 | loss: 4.0947394Losses:  2.668748378753662 0.6025228500366211
CurrentTrain: epoch  9, batch     0 | loss: 3.2712712Losses:  1.9987049102783203 0.5697118043899536
CurrentTrain: epoch  9, batch     1 | loss: 2.5684166Losses:  3.0771210193634033 0.7459970712661743
CurrentTrain: epoch  9, batch     2 | loss: 3.8231182Losses:  1.928157925605774 0.5321575403213501
CurrentTrain: epoch  9, batch     3 | loss: 2.4603155Losses:  2.9677844047546387 0.7206019163131714
CurrentTrain: epoch  9, batch     4 | loss: 3.6883864
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 36.72%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.13%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 75.68%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 76.72%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 76.67%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 75.41%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.91%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 74.75%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 73.53%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 69.68%   
cur_acc:  ['0.8277', '0.8214', '0.3672']
his_acc:  ['0.8277', '0.8364', '0.6968']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0]
Losses:  6.636040687561035 1.1721851825714111
CurrentTrain: epoch  0, batch     0 | loss: 7.8082256Losses:  10.125385284423828 0.9683704376220703
CurrentTrain: epoch  0, batch     1 | loss: 11.0937557Losses:  9.12923526763916 0.8701990842819214
CurrentTrain: epoch  0, batch     2 | loss: 9.9994345Losses:  8.845051765441895 1.1629905700683594
CurrentTrain: epoch  0, batch     3 | loss: 10.0080423Losses:  7.798561096191406 0.9464890956878662
CurrentTrain: epoch  0, batch     4 | loss: 8.7450504Losses:  4.274681568145752 2.3841860752327193e-07
CurrentTrain: epoch  0, batch     5 | loss: 4.2746820Losses:  2.7536253929138184 0.8579208850860596
CurrentTrain: epoch  1, batch     0 | loss: 3.6115463Losses:  2.4806222915649414 1.1282894611358643
CurrentTrain: epoch  1, batch     1 | loss: 3.6089118Losses:  2.14836049079895 0.6784594058990479
CurrentTrain: epoch  1, batch     2 | loss: 2.8268199Losses:  3.138422727584839 1.0151219367980957
CurrentTrain: epoch  1, batch     3 | loss: 4.1535444Losses:  2.6568715572357178 1.0635039806365967
CurrentTrain: epoch  1, batch     4 | loss: 3.7203755Losses:  3.1214449405670166 1.1920930376163597e-07
CurrentTrain: epoch  1, batch     5 | loss: 3.1214452Losses:  2.431213855743408 0.672906219959259
CurrentTrain: epoch  2, batch     0 | loss: 3.1041200Losses:  2.569646120071411 0.8688920140266418
CurrentTrain: epoch  2, batch     1 | loss: 3.4385381Losses:  2.225476026535034 0.7134228348731995
CurrentTrain: epoch  2, batch     2 | loss: 2.9388988Losses:  2.0065698623657227 0.5526981353759766
CurrentTrain: epoch  2, batch     3 | loss: 2.5592680Losses:  1.7671658992767334 0.9557144641876221
CurrentTrain: epoch  2, batch     4 | loss: 2.7228804Losses:  2.9168176651000977 2.98023280720372e-07
CurrentTrain: epoch  2, batch     5 | loss: 2.9168179Losses:  1.8721222877502441 0.8358063101768494
CurrentTrain: epoch  3, batch     0 | loss: 2.7079287Losses:  1.4209561347961426 0.6556321382522583
CurrentTrain: epoch  3, batch     1 | loss: 2.0765882Losses:  2.2911455631256104 0.7695742845535278
CurrentTrain: epoch  3, batch     2 | loss: 3.0607200Losses:  1.5699195861816406 0.8788632154464722
CurrentTrain: epoch  3, batch     3 | loss: 2.4487829Losses:  2.3823046684265137 0.7041733264923096
CurrentTrain: epoch  3, batch     4 | loss: 3.0864780Losses:  3.0767662525177 1.1920930376163597e-07
CurrentTrain: epoch  3, batch     5 | loss: 3.0767665Losses:  2.0324087142944336 0.7998721599578857
CurrentTrain: epoch  4, batch     0 | loss: 2.8322809Losses:  2.064340591430664 0.7553166151046753
CurrentTrain: epoch  4, batch     1 | loss: 2.8196573Losses:  1.828143835067749 0.6236796379089355
CurrentTrain: epoch  4, batch     2 | loss: 2.4518235Losses:  1.529496192932129 0.6235985159873962
CurrentTrain: epoch  4, batch     3 | loss: 2.1530948Losses:  1.8144781589508057 0.850825309753418
CurrentTrain: epoch  4, batch     4 | loss: 2.6653035Losses:  0.24243605136871338 0.0
CurrentTrain: epoch  4, batch     5 | loss: 0.2424361Losses:  1.9106261730194092 0.5975666046142578
CurrentTrain: epoch  5, batch     0 | loss: 2.5081928Losses:  1.3254797458648682 0.8585087656974792
CurrentTrain: epoch  5, batch     1 | loss: 2.1839886Losses:  2.0161049365997314 0.8157630562782288
CurrentTrain: epoch  5, batch     2 | loss: 2.8318679Losses:  1.350601077079773 0.7316904067993164
CurrentTrain: epoch  5, batch     3 | loss: 2.0822916Losses:  1.9483224153518677 0.5479671955108643
CurrentTrain: epoch  5, batch     4 | loss: 2.4962897Losses:  0.6699767708778381 1.1920930376163597e-07
CurrentTrain: epoch  5, batch     5 | loss: 0.6699769Losses:  1.7915396690368652 0.6312310695648193
CurrentTrain: epoch  6, batch     0 | loss: 2.4227707Losses:  1.9048205614089966 0.42314404249191284
CurrentTrain: epoch  6, batch     1 | loss: 2.3279645Losses:  1.4465781450271606 0.7679322957992554
CurrentTrain: epoch  6, batch     2 | loss: 2.2145104Losses:  1.1940853595733643 0.5503883957862854
CurrentTrain: epoch  6, batch     3 | loss: 1.7444737Losses:  1.636967420578003 0.6037690043449402
CurrentTrain: epoch  6, batch     4 | loss: 2.2407365Losses:  2.366952419281006 1.1920930376163597e-07
CurrentTrain: epoch  6, batch     5 | loss: 2.3669527Losses:  1.3387891054153442 0.7776962518692017
CurrentTrain: epoch  7, batch     0 | loss: 2.1164854Losses:  1.9413673877716064 0.2782772183418274
CurrentTrain: epoch  7, batch     1 | loss: 2.2196445Losses:  1.6016432046890259 0.6329352855682373
CurrentTrain: epoch  7, batch     2 | loss: 2.2345786Losses:  1.2722280025482178 0.5494009852409363
CurrentTrain: epoch  7, batch     3 | loss: 1.8216290Losses:  1.3060967922210693 0.5117546319961548
CurrentTrain: epoch  7, batch     4 | loss: 1.8178514Losses:  2.1086463928222656 1.1920930376163597e-07
CurrentTrain: epoch  7, batch     5 | loss: 2.1086466Losses:  1.2205307483673096 0.744826078414917
CurrentTrain: epoch  8, batch     0 | loss: 1.9653568Losses:  1.5623306035995483 0.5502049922943115
CurrentTrain: epoch  8, batch     1 | loss: 2.1125355Losses:  1.6454188823699951 0.45137256383895874
CurrentTrain: epoch  8, batch     2 | loss: 2.0967915Losses:  1.228240966796875 0.504365086555481
CurrentTrain: epoch  8, batch     3 | loss: 1.7326061Losses:  1.9497616291046143 0.43020743131637573
CurrentTrain: epoch  8, batch     4 | loss: 2.3799691Losses:  1.8046973943710327 1.1920930376163597e-07
CurrentTrain: epoch  8, batch     5 | loss: 1.8046975Losses:  1.033825159072876 0.537245512008667
CurrentTrain: epoch  9, batch     0 | loss: 1.5710707Losses:  1.3360722064971924 0.4793958067893982
CurrentTrain: epoch  9, batch     1 | loss: 1.8154681Losses:  1.6486594676971436 0.5420433282852173
CurrentTrain: epoch  9, batch     2 | loss: 2.1907029Losses:  1.523752212524414 0.5341922044754028
CurrentTrain: epoch  9, batch     3 | loss: 2.0579443Losses:  1.7616043090820312 0.3041013479232788
CurrentTrain: epoch  9, batch     4 | loss: 2.0657058Losses:  1.2881261110305786 3.576279254957626e-07
CurrentTrain: epoch  9, batch     5 | loss: 1.2881265
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 81.73%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 74.60%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 74.26%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 72.86%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 71.18%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 66.51%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 68.18%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 68.19%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 67.12%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 65.32%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 64.06%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 62.97%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 63.31%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 64.91%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 65.19%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 65.88%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 65.87%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 66.67%   
cur_acc:  ['0.8277', '0.8214', '0.3672', '0.8173']
his_acc:  ['0.8277', '0.8364', '0.6968', '0.6667']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0]
Losses:  6.4916462898254395 1.2769004106521606
CurrentTrain: epoch  0, batch     0 | loss: 7.7685466Losses:  9.583597183227539 1.0690451860427856
CurrentTrain: epoch  0, batch     1 | loss: 10.6526423Losses:  10.478697776794434 0.9120966196060181
CurrentTrain: epoch  0, batch     2 | loss: 11.3907948Losses:  10.44492244720459 0.8611444234848022
CurrentTrain: epoch  0, batch     3 | loss: 11.3060665Losses:  10.758401870727539 1.376051902770996
CurrentTrain: epoch  0, batch     4 | loss: 12.1344538Losses:  8.62564754486084 0.9440962672233582
CurrentTrain: epoch  0, batch     5 | loss: 9.5697441Losses:  4.564512729644775 0.9113523960113525
CurrentTrain: epoch  1, batch     0 | loss: 5.4758654Losses:  1.917627215385437 0.7000898122787476
CurrentTrain: epoch  1, batch     1 | loss: 2.6177170Losses:  2.932800769805908 0.7196754217147827
CurrentTrain: epoch  1, batch     2 | loss: 3.6524763Losses:  3.7870852947235107 1.2821284532546997
CurrentTrain: epoch  1, batch     3 | loss: 5.0692139Losses:  3.702747344970703 0.938753604888916
CurrentTrain: epoch  1, batch     4 | loss: 4.6415009Losses:  5.089061737060547 0.8212686777114868
CurrentTrain: epoch  1, batch     5 | loss: 5.9103303Losses:  2.2117669582366943 0.5938316583633423
CurrentTrain: epoch  2, batch     0 | loss: 2.8055987Losses:  3.825620412826538 0.6947284936904907
CurrentTrain: epoch  2, batch     1 | loss: 4.5203490Losses:  3.478994607925415 0.7682193517684937
CurrentTrain: epoch  2, batch     2 | loss: 4.2472138Losses:  4.333680152893066 0.9930634498596191
CurrentTrain: epoch  2, batch     3 | loss: 5.3267436Losses:  2.385957717895508 0.9372255802154541
CurrentTrain: epoch  2, batch     4 | loss: 3.3231833Losses:  3.4910640716552734 1.0091580152511597
CurrentTrain: epoch  2, batch     5 | loss: 4.5002222Losses:  2.1100003719329834 0.669076681137085
CurrentTrain: epoch  3, batch     0 | loss: 2.7790771Losses:  3.183978319168091 0.9668862819671631
CurrentTrain: epoch  3, batch     1 | loss: 4.1508646Losses:  2.753382682800293 0.891287088394165
CurrentTrain: epoch  3, batch     2 | loss: 3.6446698Losses:  3.4552435874938965 0.8805131912231445
CurrentTrain: epoch  3, batch     3 | loss: 4.3357568Losses:  3.5549473762512207 0.9706618189811707
CurrentTrain: epoch  3, batch     4 | loss: 4.5256090Losses:  2.769925832748413 0.8978887796401978
CurrentTrain: epoch  3, batch     5 | loss: 3.6678147Losses:  2.262941598892212 0.9043470621109009
CurrentTrain: epoch  4, batch     0 | loss: 3.1672888Losses:  2.282449245452881 0.6207981109619141
CurrentTrain: epoch  4, batch     1 | loss: 2.9032474Losses:  2.131448745727539 0.8050466775894165
CurrentTrain: epoch  4, batch     2 | loss: 2.9364953Losses:  3.663851499557495 1.0028302669525146
CurrentTrain: epoch  4, batch     3 | loss: 4.6666818Losses:  2.9445085525512695 1.1478184461593628
CurrentTrain: epoch  4, batch     4 | loss: 4.0923271Losses:  3.5224356651306152 0.5896070003509521
CurrentTrain: epoch  4, batch     5 | loss: 4.1120424Losses:  3.949580669403076 0.9283188581466675
CurrentTrain: epoch  5, batch     0 | loss: 4.8778996Losses:  2.201143980026245 0.5074124932289124
CurrentTrain: epoch  5, batch     1 | loss: 2.7085564Losses:  2.931509256362915 0.8840547204017639
CurrentTrain: epoch  5, batch     2 | loss: 3.8155639Losses:  1.4906179904937744 0.7939484119415283
CurrentTrain: epoch  5, batch     3 | loss: 2.2845664Losses:  3.364025592803955 0.9825013875961304
CurrentTrain: epoch  5, batch     4 | loss: 4.3465271Losses:  1.3792728185653687 0.7047869563102722
CurrentTrain: epoch  5, batch     5 | loss: 2.0840597Losses:  2.78115177154541 0.9492975473403931
CurrentTrain: epoch  6, batch     0 | loss: 3.7304492Losses:  2.205237865447998 0.6227889060974121
CurrentTrain: epoch  6, batch     1 | loss: 2.8280268Losses:  2.5204591751098633 1.150668740272522
CurrentTrain: epoch  6, batch     2 | loss: 3.6711278Losses:  1.325531244277954 0.6983816623687744
CurrentTrain: epoch  6, batch     3 | loss: 2.0239129Losses:  2.563570499420166 0.9275996685028076
CurrentTrain: epoch  6, batch     4 | loss: 3.4911702Losses:  3.496560573577881 0.7207157015800476
CurrentTrain: epoch  6, batch     5 | loss: 4.2172761Losses:  1.6645516157150269 0.686321496963501
CurrentTrain: epoch  7, batch     0 | loss: 2.3508730Losses:  2.3165740966796875 0.7536017894744873
CurrentTrain: epoch  7, batch     1 | loss: 3.0701759Losses:  2.9525961875915527 0.7910025715827942
CurrentTrain: epoch  7, batch     2 | loss: 3.7435987Losses:  2.531357526779175 0.9399685859680176
CurrentTrain: epoch  7, batch     3 | loss: 3.4713261Losses:  1.988142967224121 0.7385438680648804
CurrentTrain: epoch  7, batch     4 | loss: 2.7266870Losses:  2.1126198768615723 0.6899865865707397
CurrentTrain: epoch  7, batch     5 | loss: 2.8026066Losses:  2.3769326210021973 0.8528422713279724
CurrentTrain: epoch  8, batch     0 | loss: 3.2297750Losses:  1.5229095220565796 0.738304615020752
CurrentTrain: epoch  8, batch     1 | loss: 2.2612143Losses:  2.187453508377075 0.5782455205917358
CurrentTrain: epoch  8, batch     2 | loss: 2.7656989Losses:  2.155954360961914 0.5929006338119507
CurrentTrain: epoch  8, batch     3 | loss: 2.7488551Losses:  1.7943487167358398 0.9385890960693359
CurrentTrain: epoch  8, batch     4 | loss: 2.7329378Losses:  1.9342641830444336 0.4523799419403076
CurrentTrain: epoch  8, batch     5 | loss: 2.3866441Losses:  1.0840904712677002 0.6093488931655884
CurrentTrain: epoch  9, batch     0 | loss: 1.6934394Losses:  2.467911720275879 1.0566577911376953
CurrentTrain: epoch  9, batch     1 | loss: 3.5245695Losses:  2.4873106479644775 0.9284703731536865
CurrentTrain: epoch  9, batch     2 | loss: 3.4157810Losses:  1.4697141647338867 0.5621284246444702
CurrentTrain: epoch  9, batch     3 | loss: 2.0318427Losses:  1.8919904232025146 0.41711512207984924
CurrentTrain: epoch  9, batch     4 | loss: 2.3091056Losses:  1.6445683240890503 0.5378991365432739
CurrentTrain: epoch  9, batch     5 | loss: 2.1824675
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 56.82%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 69.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 66.05%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 64.47%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 62.98%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 63.72%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 64.44%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 63.45%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 63.39%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 62.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 61.15%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 59.98%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 58.84%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 59.26%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 61.29%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 61.42%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 61.97%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.08%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 61.58%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 60.69%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 59.82%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 60.23%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 60.45%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 60.48%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 60.33%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 59.91%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 59.51%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 58.99%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 58.70%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 58.67%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 58.47%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 58.52%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 58.49%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 58.54%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 58.91%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 58.95%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 58.92%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 58.73%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 58.85%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 59.12%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 59.45%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 59.70%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 59.23%   
cur_acc:  ['0.8277', '0.8214', '0.3672', '0.8173', '0.5682']
his_acc:  ['0.8277', '0.8364', '0.6968', '0.6667', '0.5923']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0]
Losses:  6.501572608947754 0.8540774583816528
CurrentTrain: epoch  0, batch     0 | loss: 7.3556499Losses:  8.068499565124512 0.8766540288925171
CurrentTrain: epoch  0, batch     1 | loss: 8.9451532Losses:  9.509906768798828 1.1802997589111328
CurrentTrain: epoch  0, batch     2 | loss: 10.6902065Losses:  9.618772506713867 0.9570544362068176
CurrentTrain: epoch  0, batch     3 | loss: 10.5758266Losses:  9.30561351776123 0.8910538554191589
CurrentTrain: epoch  0, batch     4 | loss: 10.1966677Losses:  8.758548736572266 0.617485523223877
CurrentTrain: epoch  0, batch     5 | loss: 9.3760338Losses:  7.3831915855407715 0.18944987654685974
CurrentTrain: epoch  0, batch     6 | loss: 7.5726414Losses:  2.1324028968811035 0.89834064245224
CurrentTrain: epoch  1, batch     0 | loss: 3.0307436Losses:  2.331707715988159 0.8440905213356018
CurrentTrain: epoch  1, batch     1 | loss: 3.1757982Losses:  2.1979706287384033 0.8876326680183411
CurrentTrain: epoch  1, batch     2 | loss: 3.0856032Losses:  2.886289119720459 0.5766826272010803
CurrentTrain: epoch  1, batch     3 | loss: 3.4629717Losses:  2.801589250564575 0.9483857154846191
CurrentTrain: epoch  1, batch     4 | loss: 3.7499750Losses:  2.475015163421631 0.9821589589118958
CurrentTrain: epoch  1, batch     5 | loss: 3.4571741Losses:  1.8500604629516602 0.11632724106311798
CurrentTrain: epoch  1, batch     6 | loss: 1.9663877Losses:  3.6201865673065186 0.6933519244194031
CurrentTrain: epoch  2, batch     0 | loss: 4.3135386Losses:  2.1876749992370605 0.6807589530944824
CurrentTrain: epoch  2, batch     1 | loss: 2.8684340Losses:  2.320735454559326 0.6939970254898071
CurrentTrain: epoch  2, batch     2 | loss: 3.0147324Losses:  2.1520471572875977 0.7232733964920044
CurrentTrain: epoch  2, batch     3 | loss: 2.8753204Losses:  1.5446054935455322 0.5795700550079346
CurrentTrain: epoch  2, batch     4 | loss: 2.1241755Losses:  1.9587006568908691 0.8029806613922119
CurrentTrain: epoch  2, batch     5 | loss: 2.7616813Losses:  1.8036513328552246 0.1904025375843048
CurrentTrain: epoch  2, batch     6 | loss: 1.9940538Losses:  3.0631141662597656 0.846081018447876
CurrentTrain: epoch  3, batch     0 | loss: 3.9091952Losses:  1.355146884918213 0.6296032071113586
CurrentTrain: epoch  3, batch     1 | loss: 1.9847500Losses:  1.8415141105651855 0.7908558249473572
CurrentTrain: epoch  3, batch     2 | loss: 2.6323700Losses:  2.6032729148864746 0.6701661348342896
CurrentTrain: epoch  3, batch     3 | loss: 3.2734389Losses:  1.9893921613693237 0.578869640827179
CurrentTrain: epoch  3, batch     4 | loss: 2.5682619Losses:  1.3049917221069336 0.5973719954490662
CurrentTrain: epoch  3, batch     5 | loss: 1.9023638Losses:  1.4821152687072754 0.3647277057170868
CurrentTrain: epoch  3, batch     6 | loss: 1.8468430Losses:  2.2878661155700684 0.7872929573059082
CurrentTrain: epoch  4, batch     0 | loss: 3.0751591Losses:  1.5861399173736572 0.6068759560585022
CurrentTrain: epoch  4, batch     1 | loss: 2.1930158Losses:  1.6202239990234375 0.7781018018722534
CurrentTrain: epoch  4, batch     2 | loss: 2.3983259Losses:  2.195491313934326 0.8040900230407715
CurrentTrain: epoch  4, batch     3 | loss: 2.9995813Losses:  1.1332380771636963 0.6828626394271851
CurrentTrain: epoch  4, batch     4 | loss: 1.8161007Losses:  1.9451069831848145 0.6054842472076416
CurrentTrain: epoch  4, batch     5 | loss: 2.5505912Losses:  0.9777650833129883 0.19684594869613647
CurrentTrain: epoch  4, batch     6 | loss: 1.1746111Losses:  2.320232391357422 0.6264517903327942
CurrentTrain: epoch  5, batch     0 | loss: 2.9466841Losses:  1.895447015762329 0.651336133480072
CurrentTrain: epoch  5, batch     1 | loss: 2.5467832Losses:  1.7781376838684082 0.4570487141609192
CurrentTrain: epoch  5, batch     2 | loss: 2.2351863Losses:  1.79426109790802 0.6726853251457214
CurrentTrain: epoch  5, batch     3 | loss: 2.4669464Losses:  1.0472866296768188 0.7570893168449402
CurrentTrain: epoch  5, batch     4 | loss: 1.8043759Losses:  1.1121726036071777 0.7438358068466187
CurrentTrain: epoch  5, batch     5 | loss: 1.8560084Losses:  1.2991610765457153 0.19011887907981873
CurrentTrain: epoch  5, batch     6 | loss: 1.4892800Losses:  1.6483230590820312 0.6437188386917114
CurrentTrain: epoch  6, batch     0 | loss: 2.2920418Losses:  1.2174334526062012 0.5037750005722046
CurrentTrain: epoch  6, batch     1 | loss: 1.7212085Losses:  2.309342861175537 0.46390509605407715
CurrentTrain: epoch  6, batch     2 | loss: 2.7732480Losses:  1.2120962142944336 0.5125519037246704
CurrentTrain: epoch  6, batch     3 | loss: 1.7246481Losses:  1.5712045431137085 0.7432045936584473
CurrentTrain: epoch  6, batch     4 | loss: 2.3144093Losses:  1.3240602016448975 0.2975967228412628
CurrentTrain: epoch  6, batch     5 | loss: 1.6216569Losses:  1.3573683500289917 0.29868650436401367
CurrentTrain: epoch  6, batch     6 | loss: 1.6560549Losses:  1.469907283782959 0.6430420875549316
CurrentTrain: epoch  7, batch     0 | loss: 2.1129494Losses:  1.579287052154541 0.6293622255325317
CurrentTrain: epoch  7, batch     1 | loss: 2.2086492Losses:  1.440060019493103 0.7272775769233704
CurrentTrain: epoch  7, batch     2 | loss: 2.1673377Losses:  1.055914044380188 0.5939405560493469
CurrentTrain: epoch  7, batch     3 | loss: 1.6498547Losses:  1.5114272832870483 0.6490024328231812
CurrentTrain: epoch  7, batch     4 | loss: 2.1604297Losses:  1.5205386877059937 0.7978710532188416
CurrentTrain: epoch  7, batch     5 | loss: 2.3184097Losses:  2.0466718673706055 0.2462950348854065
CurrentTrain: epoch  7, batch     6 | loss: 2.2929668Losses:  1.0679858922958374 0.5434719324111938
CurrentTrain: epoch  8, batch     0 | loss: 1.6114578Losses:  1.4514228105545044 0.5068037509918213
CurrentTrain: epoch  8, batch     1 | loss: 1.9582266Losses:  1.3358922004699707 0.6307986378669739
CurrentTrain: epoch  8, batch     2 | loss: 1.9666908Losses:  1.5591788291931152 0.4556539058685303
CurrentTrain: epoch  8, batch     3 | loss: 2.0148327Losses:  1.55217707157135 0.8784303665161133
CurrentTrain: epoch  8, batch     4 | loss: 2.4306073Losses:  1.2605453729629517 0.6628186702728271
CurrentTrain: epoch  8, batch     5 | loss: 1.9233640Losses:  0.8453794717788696 0.07805272936820984
CurrentTrain: epoch  8, batch     6 | loss: 0.9234322Losses:  1.259954810142517 0.605004072189331
CurrentTrain: epoch  9, batch     0 | loss: 1.8649589Losses:  1.2675611972808838 0.5903944373130798
CurrentTrain: epoch  9, batch     1 | loss: 1.8579557Losses:  1.212548851966858 0.6505560874938965
CurrentTrain: epoch  9, batch     2 | loss: 1.8631049Losses:  1.3911255598068237 0.6515612602233887
CurrentTrain: epoch  9, batch     3 | loss: 2.0426869Losses:  1.0668578147888184 0.5573117136955261
CurrentTrain: epoch  9, batch     4 | loss: 1.6241696Losses:  1.5676518678665161 0.4549606442451477
CurrentTrain: epoch  9, batch     5 | loss: 2.0226126Losses:  0.8936585783958435 0.09741635620594025
CurrentTrain: epoch  9, batch     6 | loss: 0.9910749
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 99.31%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 91.96%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 55.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 56.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.80%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 71.29%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 65.54%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 63.98%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 62.96%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 63.37%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 63.04%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 63.16%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 63.14%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 61.88%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 60.78%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 59.62%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 58.49%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 61.56%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 61.37%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 60.69%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 60.22%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 59.71%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 59.28%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 58.77%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 58.00%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 57.52%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 56.70%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 55.90%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 55.12%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 54.62%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 54.39%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 54.42%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 54.19%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 54.14%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 54.09%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 54.03%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 54.30%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 54.40%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 54.50%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 54.52%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 54.61%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 54.71%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 54.94%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 55.24%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 55.68%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 56.18%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 56.67%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.61%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.06%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.51%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.95%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 59.28%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 59.57%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.91%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 60.46%   
cur_acc:  ['0.8277', '0.8214', '0.3672', '0.8173', '0.5682', '0.9196']
his_acc:  ['0.8277', '0.8364', '0.6968', '0.6667', '0.5923', '0.6046']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0 1 2 0 0 0]
Losses:  7.217158317565918 1.1945223808288574
CurrentTrain: epoch  0, batch     0 | loss: 8.4116802Losses:  8.229291915893555 0.89566969871521
CurrentTrain: epoch  0, batch     1 | loss: 9.1249619Losses:  9.885137557983398 0.8072916865348816
CurrentTrain: epoch  0, batch     2 | loss: 10.6924295Losses:  9.832191467285156 0.6352941393852234
CurrentTrain: epoch  0, batch     3 | loss: 10.4674854Losses:  9.680177688598633 0.600766658782959
CurrentTrain: epoch  0, batch     4 | loss: 10.2809448Losses:  8.237508773803711 0.9555464386940002
CurrentTrain: epoch  0, batch     5 | loss: 9.1930552Losses:  9.28741455078125 0.8350434303283691
CurrentTrain: epoch  0, batch     6 | loss: 10.1224575Losses:  2.4261603355407715 0.9518556594848633
CurrentTrain: epoch  1, batch     0 | loss: 3.3780160Losses:  3.109382152557373 0.916095495223999
CurrentTrain: epoch  1, batch     1 | loss: 4.0254774Losses:  2.385770082473755 0.5952757000923157
CurrentTrain: epoch  1, batch     2 | loss: 2.9810457Losses:  3.203868865966797 0.6163873672485352
CurrentTrain: epoch  1, batch     3 | loss: 3.8202562Losses:  2.304293155670166 0.798352062702179
CurrentTrain: epoch  1, batch     4 | loss: 3.1026452Losses:  2.36810564994812 0.7131336331367493
CurrentTrain: epoch  1, batch     5 | loss: 3.0812392Losses:  2.7089977264404297 0.8586390018463135
CurrentTrain: epoch  1, batch     6 | loss: 3.5676367Losses:  2.2965147495269775 0.8657859563827515
CurrentTrain: epoch  2, batch     0 | loss: 3.1623006Losses:  2.071943759918213 0.5983492136001587
CurrentTrain: epoch  2, batch     1 | loss: 2.6702929Losses:  2.2662711143493652 0.8086988925933838
CurrentTrain: epoch  2, batch     2 | loss: 3.0749700Losses:  2.0739831924438477 0.7017512321472168
CurrentTrain: epoch  2, batch     3 | loss: 2.7757344Losses:  3.0724565982818604 0.7527483701705933
CurrentTrain: epoch  2, batch     4 | loss: 3.8252048Losses:  1.680793285369873 0.7784467935562134
CurrentTrain: epoch  2, batch     5 | loss: 2.4592400Losses:  2.110454797744751 0.534839391708374
CurrentTrain: epoch  2, batch     6 | loss: 2.6452942Losses:  2.679879665374756 0.7235270738601685
CurrentTrain: epoch  3, batch     0 | loss: 3.4034066Losses:  2.152357816696167 0.664299488067627
CurrentTrain: epoch  3, batch     1 | loss: 2.8166573Losses:  2.18917179107666 0.6883176565170288
CurrentTrain: epoch  3, batch     2 | loss: 2.8774896Losses:  2.508437156677246 0.8280084133148193
CurrentTrain: epoch  3, batch     3 | loss: 3.3364456Losses:  1.1883834600448608 0.5598679780960083
CurrentTrain: epoch  3, batch     4 | loss: 1.7482514Losses:  1.9646776914596558 0.7687053680419922
CurrentTrain: epoch  3, batch     5 | loss: 2.7333832Losses:  1.736378788948059 0.7258272767066956
CurrentTrain: epoch  3, batch     6 | loss: 2.4622061Losses:  1.5611015558242798 0.5458792448043823
CurrentTrain: epoch  4, batch     0 | loss: 2.1069808Losses:  1.817389965057373 0.45455220341682434
CurrentTrain: epoch  4, batch     1 | loss: 2.2719421Losses:  2.0019826889038086 0.804929792881012
CurrentTrain: epoch  4, batch     2 | loss: 2.8069124Losses:  2.1857504844665527 0.6596909165382385
CurrentTrain: epoch  4, batch     3 | loss: 2.8454413Losses:  2.5378451347351074 0.8744039535522461
CurrentTrain: epoch  4, batch     4 | loss: 3.4122491Losses:  1.0947916507720947 0.36958491802215576
CurrentTrain: epoch  4, batch     5 | loss: 1.4643766Losses:  2.354271411895752 0.5675482749938965
CurrentTrain: epoch  4, batch     6 | loss: 2.9218197Losses:  2.679108142852783 0.6903607845306396
CurrentTrain: epoch  5, batch     0 | loss: 3.3694689Losses:  2.3491835594177246 0.6961743831634521
CurrentTrain: epoch  5, batch     1 | loss: 3.0453579Losses:  1.4535307884216309 0.4277832508087158
CurrentTrain: epoch  5, batch     2 | loss: 1.8813140Losses:  1.1629970073699951 0.4867156147956848
CurrentTrain: epoch  5, batch     3 | loss: 1.6497126Losses:  1.2770411968231201 0.6061941385269165
CurrentTrain: epoch  5, batch     4 | loss: 1.8832353Losses:  1.5622215270996094 0.585869550704956
CurrentTrain: epoch  5, batch     5 | loss: 2.1480911Losses:  2.184002637863159 0.9260390996932983
CurrentTrain: epoch  5, batch     6 | loss: 3.1100416Losses:  2.1085219383239746 0.7444384098052979
CurrentTrain: epoch  6, batch     0 | loss: 2.8529603Losses:  1.9012624025344849 0.6318694353103638
CurrentTrain: epoch  6, batch     1 | loss: 2.5331318Losses:  2.2553021907806396 0.6051517724990845
CurrentTrain: epoch  6, batch     2 | loss: 2.8604541Losses:  1.4448390007019043 0.7001349925994873
CurrentTrain: epoch  6, batch     3 | loss: 2.1449740Losses:  1.7758781909942627 0.5234444737434387
CurrentTrain: epoch  6, batch     4 | loss: 2.2993226Losses:  1.5020215511322021 0.4260683059692383
CurrentTrain: epoch  6, batch     5 | loss: 1.9280899Losses:  0.8133977651596069 0.5639675855636597
CurrentTrain: epoch  6, batch     6 | loss: 1.3773654Losses:  1.346230149269104 0.49523869156837463
CurrentTrain: epoch  7, batch     0 | loss: 1.8414688Losses:  2.115570545196533 0.4882248640060425
CurrentTrain: epoch  7, batch     1 | loss: 2.6037955Losses:  1.6388297080993652 0.4391314685344696
CurrentTrain: epoch  7, batch     2 | loss: 2.0779612Losses:  1.2919646501541138 0.680702269077301
CurrentTrain: epoch  7, batch     3 | loss: 1.9726670Losses:  0.8697707056999207 0.4639470875263214
CurrentTrain: epoch  7, batch     4 | loss: 1.3337178Losses:  1.5194571018218994 0.58221036195755
CurrentTrain: epoch  7, batch     5 | loss: 2.1016674Losses:  1.975293517112732 0.599088191986084
CurrentTrain: epoch  7, batch     6 | loss: 2.5743818Losses:  1.4257862567901611 0.5072591304779053
CurrentTrain: epoch  8, batch     0 | loss: 1.9330454Losses:  0.8061187863349915 0.5530393719673157
CurrentTrain: epoch  8, batch     1 | loss: 1.3591582Losses:  2.141387939453125 0.3163139522075653
CurrentTrain: epoch  8, batch     2 | loss: 2.4577019Losses:  1.3883235454559326 0.4812593162059784
CurrentTrain: epoch  8, batch     3 | loss: 1.8695829Losses:  1.6639297008514404 0.6412447690963745
CurrentTrain: epoch  8, batch     4 | loss: 2.3051744Losses:  1.245025396347046 0.5791682004928589
CurrentTrain: epoch  8, batch     5 | loss: 1.8241936Losses:  1.3884881734848022 0.6242032647132874
CurrentTrain: epoch  8, batch     6 | loss: 2.0126915Losses:  1.6327353715896606 0.41212260723114014
CurrentTrain: epoch  9, batch     0 | loss: 2.0448580Losses:  1.2945294380187988 0.6047085523605347
CurrentTrain: epoch  9, batch     1 | loss: 1.8992380Losses:  1.2142688035964966 0.7310152053833008
CurrentTrain: epoch  9, batch     2 | loss: 1.9452840Losses:  1.1707732677459717 0.6553162336349487
CurrentTrain: epoch  9, batch     3 | loss: 1.8260895Losses:  1.3845309019088745 0.6070009469985962
CurrentTrain: epoch  9, batch     4 | loss: 1.9915318Losses:  1.1187942028045654 0.5192883014678955
CurrentTrain: epoch  9, batch     5 | loss: 1.6380825Losses:  1.6064233779907227 0.49718907475471497
CurrentTrain: epoch  9, batch     6 | loss: 2.1036124
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 80.83%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 54.02%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 70.26%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 64.70%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 61.70%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.05%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 62.21%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 61.28%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 62.11%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 61.22%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 60.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 59.31%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 58.29%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 57.19%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 57.52%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 58.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 59.32%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.27%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 59.69%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 59.02%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 58.06%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 57.24%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 56.45%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 56.15%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 55.59%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 55.32%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 54.69%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 54.26%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 53.48%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 52.82%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 52.08%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 51.71%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 51.44%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 51.42%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 51.23%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 51.30%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 51.28%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 51.34%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 51.95%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 52.24%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 52.44%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 52.48%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 52.53%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 51.91%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 51.38%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 50.93%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 51.35%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 51.90%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 52.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 52.95%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.46%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.97%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.45%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.93%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 55.21%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 55.15%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 55.48%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.87%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 56.31%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 56.62%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 56.74%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 57.16%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 57.45%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 57.80%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 58.02%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 58.29%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 58.62%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 59.00%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 59.15%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 59.29%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 59.32%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 59.40%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 59.70%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 59.89%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 59.70%   
cur_acc:  ['0.8277', '0.8214', '0.3672', '0.8173', '0.5682', '0.9196', '0.8083']
his_acc:  ['0.8277', '0.8364', '0.6968', '0.6667', '0.5923', '0.6046', '0.5970']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 1 3 0 0 3 1 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0 1 0 0 3 1 0 0 0 1
 2 1 0 0]
Losses:  6.678242206573486 1.1571091413497925
CurrentTrain: epoch  0, batch     0 | loss: 7.8353515Losses:  7.721399784088135 0.44909054040908813
CurrentTrain: epoch  0, batch     1 | loss: 8.1704903Losses:  9.6085786819458 0.7283042073249817
CurrentTrain: epoch  0, batch     2 | loss: 10.3368826Losses:  8.393025398254395 0.9121364951133728
CurrentTrain: epoch  0, batch     3 | loss: 9.3051615Losses:  10.448867797851562 0.49570396542549133
CurrentTrain: epoch  0, batch     4 | loss: 10.9445715Losses:  7.961109638214111 0.6292135715484619
CurrentTrain: epoch  0, batch     5 | loss: 8.5903234Losses:  9.842784881591797 0.6265883445739746
CurrentTrain: epoch  0, batch     6 | loss: 10.4693737Losses:  9.84521198272705 0.4674587845802307
CurrentTrain: epoch  0, batch     7 | loss: 10.3126707Losses:  1.5841212272644043 0.7317019701004028
CurrentTrain: epoch  1, batch     0 | loss: 2.3158231Losses:  2.9159250259399414 0.6484075784683228
CurrentTrain: epoch  1, batch     1 | loss: 3.5643325Losses:  1.5785095691680908 0.6332554817199707
CurrentTrain: epoch  1, batch     2 | loss: 2.2117651Losses:  3.124669075012207 0.7874575257301331
CurrentTrain: epoch  1, batch     3 | loss: 3.9121265Losses:  3.0984816551208496 0.8057982921600342
CurrentTrain: epoch  1, batch     4 | loss: 3.9042799Losses:  2.6485517024993896 0.6211888790130615
CurrentTrain: epoch  1, batch     5 | loss: 3.2697406Losses:  1.2998435497283936 0.3865423798561096
CurrentTrain: epoch  1, batch     6 | loss: 1.6863859Losses:  2.115358352661133 0.483217716217041
CurrentTrain: epoch  1, batch     7 | loss: 2.5985761Losses:  2.522097587585449 0.6876479983329773
CurrentTrain: epoch  2, batch     0 | loss: 3.2097456Losses:  1.4176685810089111 0.46723031997680664
CurrentTrain: epoch  2, batch     1 | loss: 1.8848989Losses:  1.9317073822021484 0.8381103277206421
CurrentTrain: epoch  2, batch     2 | loss: 2.7698178Losses:  2.0085299015045166 0.8607923984527588
CurrentTrain: epoch  2, batch     3 | loss: 2.8693223Losses:  2.0294322967529297 0.40538474917411804
CurrentTrain: epoch  2, batch     4 | loss: 2.4348171Losses:  2.136819362640381 0.5806810855865479
CurrentTrain: epoch  2, batch     5 | loss: 2.7175004Losses:  1.7086734771728516 0.6754311323165894
CurrentTrain: epoch  2, batch     6 | loss: 2.3841047Losses:  2.522735357284546 0.3140992224216461
CurrentTrain: epoch  2, batch     7 | loss: 2.8368347Losses:  2.5062665939331055 0.7018700838088989
CurrentTrain: epoch  3, batch     0 | loss: 3.2081366Losses:  0.8109067678451538 0.6191636323928833
CurrentTrain: epoch  3, batch     1 | loss: 1.4300704Losses:  2.371840000152588 0.7730376720428467
CurrentTrain: epoch  3, batch     2 | loss: 3.1448777Losses:  1.1031885147094727 0.5878406167030334
CurrentTrain: epoch  3, batch     3 | loss: 1.6910291Losses:  1.5377464294433594 0.6453333497047424
CurrentTrain: epoch  3, batch     4 | loss: 2.1830797Losses:  2.1650989055633545 0.7467166185379028
CurrentTrain: epoch  3, batch     5 | loss: 2.9118156Losses:  1.2510530948638916 0.4333975911140442
CurrentTrain: epoch  3, batch     6 | loss: 1.6844506Losses:  2.674201250076294 0.5406676530838013
CurrentTrain: epoch  3, batch     7 | loss: 3.2148690Losses:  1.3623499870300293 0.553525447845459
CurrentTrain: epoch  4, batch     0 | loss: 1.9158754Losses:  1.4307861328125 0.5315361618995667
CurrentTrain: epoch  4, batch     1 | loss: 1.9623222Losses:  0.9966404438018799 0.5224554538726807
CurrentTrain: epoch  4, batch     2 | loss: 1.5190959Losses:  1.3179235458374023 0.4191739559173584
CurrentTrain: epoch  4, batch     3 | loss: 1.7370975Losses:  1.5869500637054443 0.520129919052124
CurrentTrain: epoch  4, batch     4 | loss: 2.1070800Losses:  2.1400258541107178 0.6925026774406433
CurrentTrain: epoch  4, batch     5 | loss: 2.8325286Losses:  2.4144039154052734 0.36270225048065186
CurrentTrain: epoch  4, batch     6 | loss: 2.7771063Losses:  1.6749635934829712 0.25506535172462463
CurrentTrain: epoch  4, batch     7 | loss: 1.9300289Losses:  2.5611095428466797 0.7310617566108704
CurrentTrain: epoch  5, batch     0 | loss: 3.2921712Losses:  1.0504369735717773 0.4884214997291565
CurrentTrain: epoch  5, batch     1 | loss: 1.5388584Losses:  0.7668009996414185 0.5603052377700806
CurrentTrain: epoch  5, batch     2 | loss: 1.3271062Losses:  1.2484045028686523 0.5606696605682373
CurrentTrain: epoch  5, batch     3 | loss: 1.8090742Losses:  1.5646144151687622 0.3864361047744751
CurrentTrain: epoch  5, batch     4 | loss: 1.9510505Losses:  1.3269977569580078 0.4567893147468567
CurrentTrain: epoch  5, batch     5 | loss: 1.7837870Losses:  1.3469500541687012 0.5947903394699097
CurrentTrain: epoch  5, batch     6 | loss: 1.9417404Losses:  1.3510974645614624 0.5550845265388489
CurrentTrain: epoch  5, batch     7 | loss: 1.9061821Losses:  1.6795920133590698 0.48314225673675537
CurrentTrain: epoch  6, batch     0 | loss: 2.1627343Losses:  1.234388828277588 0.5251059532165527
CurrentTrain: epoch  6, batch     1 | loss: 1.7594948Losses:  1.19672691822052 0.6110361814498901
CurrentTrain: epoch  6, batch     2 | loss: 1.8077631Losses:  1.0146584510803223 0.5339928865432739
CurrentTrain: epoch  6, batch     3 | loss: 1.5486513Losses:  1.2729548215866089 0.6021432280540466
CurrentTrain: epoch  6, batch     4 | loss: 1.8750980Losses:  1.688111424446106 0.43942874670028687
CurrentTrain: epoch  6, batch     5 | loss: 2.1275401Losses:  1.5702135562896729 0.46391358971595764
CurrentTrain: epoch  6, batch     6 | loss: 2.0341272Losses:  0.7152950167655945 0.28352561593055725
CurrentTrain: epoch  6, batch     7 | loss: 0.9988207Losses:  0.9913510680198669 0.4227425456047058
CurrentTrain: epoch  7, batch     0 | loss: 1.4140936Losses:  1.5804551839828491 0.43205034732818604
CurrentTrain: epoch  7, batch     1 | loss: 2.0125055Losses:  1.0642011165618896 0.4205496907234192
CurrentTrain: epoch  7, batch     2 | loss: 1.4847507Losses:  1.4652814865112305 0.36780107021331787
CurrentTrain: epoch  7, batch     3 | loss: 1.8330826Losses:  1.148916244506836 0.5367333889007568
CurrentTrain: epoch  7, batch     4 | loss: 1.6856496Losses:  1.4126865863800049 0.45720037817955017
CurrentTrain: epoch  7, batch     5 | loss: 1.8698870Losses:  1.1851942539215088 0.7620182037353516
CurrentTrain: epoch  7, batch     6 | loss: 1.9472125Losses:  1.2651081085205078 0.2891244888305664
CurrentTrain: epoch  7, batch     7 | loss: 1.5542326Losses:  1.219097375869751 0.4109193682670593
CurrentTrain: epoch  8, batch     0 | loss: 1.6300168Losses:  1.5967860221862793 0.42511969804763794
CurrentTrain: epoch  8, batch     1 | loss: 2.0219057Losses:  0.8165643811225891 0.597082257270813
CurrentTrain: epoch  8, batch     2 | loss: 1.4136467Losses:  0.9411282539367676 0.54057776927948
CurrentTrain: epoch  8, batch     3 | loss: 1.4817060Losses:  1.5229300260543823 0.49917516112327576
CurrentTrain: epoch  8, batch     4 | loss: 2.0221052Losses:  0.922365665435791 0.59853196144104
CurrentTrain: epoch  8, batch     5 | loss: 1.5208976Losses:  1.161508321762085 0.5197717547416687
CurrentTrain: epoch  8, batch     6 | loss: 1.6812801Losses:  1.7139545679092407 0.2393016368150711
CurrentTrain: epoch  8, batch     7 | loss: 1.9532562Losses:  1.592294692993164 0.3949032425880432
CurrentTrain: epoch  9, batch     0 | loss: 1.9871979Losses:  1.045836091041565 0.4306643009185791
CurrentTrain: epoch  9, batch     1 | loss: 1.4765004Losses:  1.0049792528152466 0.413653701543808
CurrentTrain: epoch  9, batch     2 | loss: 1.4186330Losses:  1.1716545820236206 0.4644092917442322
CurrentTrain: epoch  9, batch     3 | loss: 1.6360638Losses:  1.0942986011505127 0.5610578656196594
CurrentTrain: epoch  9, batch     4 | loss: 1.6553564Losses:  0.9273966550827026 0.5534095168113708
CurrentTrain: epoch  9, batch     5 | loss: 1.4808061Losses:  1.4895739555358887 0.39687180519104004
CurrentTrain: epoch  9, batch     6 | loss: 1.8864458Losses:  0.951518177986145 0.35685333609580994
CurrentTrain: epoch  9, batch     7 | loss: 1.3083715
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 89.58%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.61%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.66%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 68.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 67.65%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 65.71%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 63.89%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 60.53%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 58.97%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 58.91%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 58.69%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 58.63%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 58.58%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 57.95%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 57.64%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 56.66%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 56.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.68%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 57.02%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 56.12%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 55.15%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 54.21%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 53.18%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 53.01%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 53.18%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 53.46%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 53.62%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 53.77%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 54.03%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 54.06%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 53.48%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 52.72%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 51.98%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 51.46%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 50.67%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 50.19%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 49.82%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 49.82%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 49.20%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 48.68%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 48.00%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 47.69%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 47.47%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 47.50%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 47.45%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 47.56%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 47.60%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 47.71%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 48.20%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 48.53%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 48.70%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 48.80%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 48.81%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 48.24%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 47.75%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 47.20%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 47.59%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 48.17%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 48.75%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 49.31%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 49.86%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 50.40%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 50.93%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 51.45%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 51.76%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 51.74%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 52.04%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 52.46%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 52.88%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 53.16%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 52.76%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 52.49%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 52.16%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 52.02%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 51.95%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 52.28%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 52.66%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 53.10%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 53.41%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 53.60%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 53.18%   [EVAL] batch:  112 | acc: 0.00%,  total acc: 52.71%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 52.30%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 51.85%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 51.99%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 52.35%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 52.54%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 52.94%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 53.28%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 53.56%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 53.84%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 54.22%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 54.44%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 54.70%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 54.91%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 55.27%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 55.62%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 55.96%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 56.30%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 56.63%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 56.96%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 57.00%   
cur_acc:  ['0.8277', '0.8214', '0.3672', '0.8173', '0.5682', '0.9196', '0.8083', '0.8958']
his_acc:  ['0.8277', '0.8364', '0.6968', '0.6667', '0.5923', '0.6046', '0.5970', '0.5700']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.121541976928711 1.5976037979125977
CurrentTrain: epoch  0, batch     0 | loss: 13.7191458Losses:  14.25031852722168 1.9907472133636475
CurrentTrain: epoch  0, batch     1 | loss: 16.2410660Losses:  14.983429908752441 1.631555438041687
CurrentTrain: epoch  0, batch     2 | loss: 16.6149845Losses:  14.855550765991211 1.7731378078460693
CurrentTrain: epoch  0, batch     3 | loss: 16.6286888Losses:  15.049274444580078 1.2781182527542114
CurrentTrain: epoch  0, batch     4 | loss: 16.3273926Losses:  14.938459396362305 1.6879124641418457
CurrentTrain: epoch  0, batch     5 | loss: 16.6263714Losses:  14.97679615020752 1.4971377849578857
CurrentTrain: epoch  0, batch     6 | loss: 16.4739342Losses:  14.569348335266113 1.3755395412445068
CurrentTrain: epoch  0, batch     7 | loss: 15.9448881Losses:  14.697254180908203 0.8155482411384583
CurrentTrain: epoch  0, batch     8 | loss: 15.5128021Losses:  14.009381294250488 1.5714142322540283
CurrentTrain: epoch  0, batch     9 | loss: 15.5807953Losses:  14.177678108215332 0.8589882254600525
CurrentTrain: epoch  0, batch    10 | loss: 15.0366659Losses:  14.19879150390625 1.1756978034973145
CurrentTrain: epoch  0, batch    11 | loss: 15.3744888Losses:  13.621471405029297 1.5872371196746826
CurrentTrain: epoch  0, batch    12 | loss: 15.2087088Losses:  13.108308792114258 1.4721872806549072
CurrentTrain: epoch  0, batch    13 | loss: 14.5804958Losses:  13.631195068359375 1.5142405033111572
CurrentTrain: epoch  0, batch    14 | loss: 15.1454353Losses:  13.192155838012695 1.112528920173645
CurrentTrain: epoch  0, batch    15 | loss: 14.3046846Losses:  13.386098861694336 1.6347763538360596
CurrentTrain: epoch  0, batch    16 | loss: 15.0208750Losses:  12.975961685180664 1.1959043741226196
CurrentTrain: epoch  0, batch    17 | loss: 14.1718664Losses:  13.381465911865234 1.2491848468780518
CurrentTrain: epoch  0, batch    18 | loss: 14.6306505Losses:  13.263728141784668 1.1343202590942383
CurrentTrain: epoch  0, batch    19 | loss: 14.3980484Losses:  13.121238708496094 1.1418523788452148
CurrentTrain: epoch  0, batch    20 | loss: 14.2630911Losses:  12.875863075256348 1.2607616186141968
CurrentTrain: epoch  0, batch    21 | loss: 14.1366243Losses:  12.723573684692383 1.3568308353424072
CurrentTrain: epoch  0, batch    22 | loss: 14.0804043Losses:  12.74021053314209 1.7417125701904297
CurrentTrain: epoch  0, batch    23 | loss: 14.4819231Losses:  12.051517486572266 0.9476669430732727
CurrentTrain: epoch  0, batch    24 | loss: 12.9991846Losses:  12.548173904418945 1.1929796934127808
CurrentTrain: epoch  0, batch    25 | loss: 13.7411537Losses:  12.413225173950195 1.567718505859375
CurrentTrain: epoch  0, batch    26 | loss: 13.9809437Losses:  12.018451690673828 1.4045703411102295
CurrentTrain: epoch  0, batch    27 | loss: 13.4230223Losses:  12.19393539428711 1.4952971935272217
CurrentTrain: epoch  0, batch    28 | loss: 13.6892328Losses:  12.2758207321167 0.9571472406387329
CurrentTrain: epoch  0, batch    29 | loss: 13.2329683Losses:  11.637328147888184 1.4065313339233398
CurrentTrain: epoch  0, batch    30 | loss: 13.0438595Losses:  12.049568176269531 1.0696254968643188
CurrentTrain: epoch  0, batch    31 | loss: 13.1191940Losses:  11.512859344482422 0.9496199488639832
CurrentTrain: epoch  0, batch    32 | loss: 12.4624796Losses:  10.988859176635742 1.0066457986831665
CurrentTrain: epoch  0, batch    33 | loss: 11.9955053Losses:  11.171156883239746 1.092383861541748
CurrentTrain: epoch  0, batch    34 | loss: 12.2635403Losses:  11.291297912597656 1.2466322183609009
CurrentTrain: epoch  0, batch    35 | loss: 12.5379305Losses:  11.903963088989258 0.9714468717575073
CurrentTrain: epoch  0, batch    36 | loss: 12.8754101Losses:  10.59321117401123 1.0907349586486816
CurrentTrain: epoch  0, batch    37 | loss: 11.6839466Losses:  10.058836936950684 0.5324522256851196
CurrentTrain: epoch  0, batch    38 | loss: 10.5912895Losses:  11.174384117126465 1.3228758573532104
CurrentTrain: epoch  0, batch    39 | loss: 12.4972601Losses:  11.034526824951172 0.9325944781303406
CurrentTrain: epoch  0, batch    40 | loss: 11.9671211Losses:  10.499479293823242 1.2329490184783936
CurrentTrain: epoch  0, batch    41 | loss: 11.7324286Losses:  10.600370407104492 1.3739985227584839
CurrentTrain: epoch  0, batch    42 | loss: 11.9743690Losses:  10.995790481567383 1.0463075637817383
CurrentTrain: epoch  0, batch    43 | loss: 12.0420980Losses:  10.320028305053711 1.011617660522461
CurrentTrain: epoch  0, batch    44 | loss: 11.3316460Losses:  10.842962265014648 0.9082585573196411
CurrentTrain: epoch  0, batch    45 | loss: 11.7512207Losses:  10.451863288879395 0.758496105670929
CurrentTrain: epoch  0, batch    46 | loss: 11.2103596Losses:  10.276187896728516 1.397550106048584
CurrentTrain: epoch  0, batch    47 | loss: 11.6737385Losses:  9.624316215515137 0.8557438850402832
CurrentTrain: epoch  0, batch    48 | loss: 10.4800606Losses:  9.772432327270508 1.2031410932540894
CurrentTrain: epoch  0, batch    49 | loss: 10.9755735Losses:  9.713249206542969 0.8803976774215698
CurrentTrain: epoch  0, batch    50 | loss: 10.5936470Losses:  9.734784126281738 1.2776156663894653
CurrentTrain: epoch  0, batch    51 | loss: 11.0123997Losses:  9.838202476501465 0.8938969969749451
CurrentTrain: epoch  0, batch    52 | loss: 10.7320995Losses:  8.8289794921875 0.7701795101165771
CurrentTrain: epoch  0, batch    53 | loss: 9.5991592Losses:  9.43586540222168 0.958101749420166
CurrentTrain: epoch  0, batch    54 | loss: 10.3939667Losses:  9.775595664978027 1.0260097980499268
CurrentTrain: epoch  0, batch    55 | loss: 10.8016052Losses:  9.296119689941406 0.9036308526992798
CurrentTrain: epoch  0, batch    56 | loss: 10.1997509Losses:  9.185796737670898 1.1990681886672974
CurrentTrain: epoch  0, batch    57 | loss: 10.3848648Losses:  8.969172477722168 1.2377177476882935
CurrentTrain: epoch  0, batch    58 | loss: 10.2068901Losses:  9.265047073364258 0.9719303846359253
CurrentTrain: epoch  0, batch    59 | loss: 10.2369776Losses:  9.11650562286377 0.7271235585212708
CurrentTrain: epoch  0, batch    60 | loss: 9.8436289Losses:  8.967605590820312 1.377366542816162
CurrentTrain: epoch  0, batch    61 | loss: 10.3449726Losses:  8.791138648986816 0.9259659647941589
CurrentTrain: epoch  0, batch    62 | loss: 9.7171049Losses:  8.888571739196777 0.5712909698486328
CurrentTrain: epoch  0, batch    63 | loss: 9.4598627Losses:  8.799827575683594 1.287129282951355
CurrentTrain: epoch  0, batch    64 | loss: 10.0869570Losses:  7.914494514465332 0.6308119893074036
CurrentTrain: epoch  0, batch    65 | loss: 8.5453062Losses:  8.50808048248291 0.7695218324661255
CurrentTrain: epoch  0, batch    66 | loss: 9.2776022Losses:  8.196794509887695 0.8104592561721802
CurrentTrain: epoch  0, batch    67 | loss: 9.0072536Losses:  8.543909072875977 0.8348357677459717
CurrentTrain: epoch  0, batch    68 | loss: 9.3787451Losses:  8.130385398864746 0.8385194540023804
CurrentTrain: epoch  0, batch    69 | loss: 8.9689045Losses:  7.445448398590088 1.1333553791046143
CurrentTrain: epoch  0, batch    70 | loss: 8.5788040Losses:  7.597661018371582 1.099969506263733
CurrentTrain: epoch  0, batch    71 | loss: 8.6976309Losses:  7.909970283508301 0.6631656885147095
CurrentTrain: epoch  0, batch    72 | loss: 8.5731363Losses:  7.194993495941162 0.826669454574585
CurrentTrain: epoch  0, batch    73 | loss: 8.0216627Losses:  7.5319623947143555 0.8416250348091125
CurrentTrain: epoch  0, batch    74 | loss: 8.3735876Losses:  7.0624237060546875 0.7695639133453369
CurrentTrain: epoch  1, batch     0 | loss: 7.8319874Losses:  7.359341144561768 0.9467779397964478
CurrentTrain: epoch  1, batch     1 | loss: 8.3061190Losses:  6.790874004364014 0.7072696685791016
CurrentTrain: epoch  1, batch     2 | loss: 7.4981437Losses:  7.325838088989258 0.9201198816299438
CurrentTrain: epoch  1, batch     3 | loss: 8.2459583Losses:  7.274842262268066 0.6871323585510254
CurrentTrain: epoch  1, batch     4 | loss: 7.9619746Losses:  6.6999921798706055 0.7025538682937622
CurrentTrain: epoch  1, batch     5 | loss: 7.4025459Losses:  7.229715347290039 0.7551370859146118
CurrentTrain: epoch  1, batch     6 | loss: 7.9848523Losses:  6.818057060241699 0.6452431082725525
CurrentTrain: epoch  1, batch     7 | loss: 7.4633002Losses:  7.394681930541992 0.8545599579811096
CurrentTrain: epoch  1, batch     8 | loss: 8.2492418Losses:  6.777596473693848 0.5559166669845581
CurrentTrain: epoch  1, batch     9 | loss: 7.3335133Losses:  7.406157493591309 0.7222822904586792
CurrentTrain: epoch  1, batch    10 | loss: 8.1284399Losses:  7.246309280395508 0.8050788640975952
CurrentTrain: epoch  1, batch    11 | loss: 8.0513878Losses:  7.5846405029296875 1.1749444007873535
CurrentTrain: epoch  1, batch    12 | loss: 8.7595844Losses:  6.69110107421875 0.813196063041687
CurrentTrain: epoch  1, batch    13 | loss: 7.5042973Losses:  7.08601188659668 0.7607555985450745
CurrentTrain: epoch  1, batch    14 | loss: 7.8467674Losses:  6.848464012145996 0.6027501821517944
CurrentTrain: epoch  1, batch    15 | loss: 7.4512143Losses:  6.688002109527588 0.6814088821411133
CurrentTrain: epoch  1, batch    16 | loss: 7.3694110Losses:  7.355891227722168 0.7699596881866455
CurrentTrain: epoch  1, batch    17 | loss: 8.1258507Losses:  6.71830940246582 0.6532363891601562
CurrentTrain: epoch  1, batch    18 | loss: 7.3715458Losses:  6.704474449157715 0.780575156211853
CurrentTrain: epoch  1, batch    19 | loss: 7.4850497Losses:  7.527527809143066 0.868517279624939
CurrentTrain: epoch  1, batch    20 | loss: 8.3960447Losses:  7.199103355407715 0.6225298643112183
CurrentTrain: epoch  1, batch    21 | loss: 7.8216333Losses:  6.161952018737793 0.44409483671188354
CurrentTrain: epoch  1, batch    22 | loss: 6.6060467Losses:  6.719091415405273 0.38531896471977234
CurrentTrain: epoch  1, batch    23 | loss: 7.1044102Losses:  6.885330677032471 0.7156429290771484
CurrentTrain: epoch  1, batch    24 | loss: 7.6009736Losses:  7.673777103424072 0.4496620297431946
CurrentTrain: epoch  1, batch    25 | loss: 8.1234388Losses:  6.915767669677734 0.6968849897384644
CurrentTrain: epoch  1, batch    26 | loss: 7.6126528Losses:  6.809977054595947 0.46021467447280884
CurrentTrain: epoch  1, batch    27 | loss: 7.2701917Losses:  7.266482830047607 0.5859566926956177
CurrentTrain: epoch  1, batch    28 | loss: 7.8524394Losses:  6.444890022277832 0.6643489599227905
CurrentTrain: epoch  1, batch    29 | loss: 7.1092391Losses:  7.066165924072266 0.7001509070396423
CurrentTrain: epoch  1, batch    30 | loss: 7.7663169Losses:  6.379341125488281 0.533986508846283
CurrentTrain: epoch  1, batch    31 | loss: 6.9133277Losses:  7.264060020446777 0.5248944759368896
CurrentTrain: epoch  1, batch    32 | loss: 7.7889547Losses:  6.4687652587890625 0.5894172191619873
CurrentTrain: epoch  1, batch    33 | loss: 7.0581827Losses:  6.611087799072266 0.5864881873130798
CurrentTrain: epoch  1, batch    34 | loss: 7.1975760Losses:  6.277189254760742 0.5333138704299927
CurrentTrain: epoch  1, batch    35 | loss: 6.8105030Losses:  6.461929798126221 0.7069187760353088
CurrentTrain: epoch  1, batch    36 | loss: 7.1688485Losses:  6.610100746154785 0.651166558265686
CurrentTrain: epoch  1, batch    37 | loss: 7.2612672Losses:  7.124235153198242 0.8797429203987122
CurrentTrain: epoch  1, batch    38 | loss: 8.0039778Losses:  7.613260269165039 1.0892634391784668
CurrentTrain: epoch  1, batch    39 | loss: 8.7025242Losses:  6.951593399047852 0.6353092193603516
CurrentTrain: epoch  1, batch    40 | loss: 7.5869026Losses:  6.422555446624756 0.6094295382499695
CurrentTrain: epoch  1, batch    41 | loss: 7.0319848Losses:  6.403261184692383 0.4651569724082947
CurrentTrain: epoch  1, batch    42 | loss: 6.8684182Losses:  6.411979675292969 0.533406138420105
CurrentTrain: epoch  1, batch    43 | loss: 6.9453859Losses:  6.163989067077637 0.3702729344367981
CurrentTrain: epoch  1, batch    44 | loss: 6.5342622Losses:  6.242099761962891 0.41168537735939026
CurrentTrain: epoch  1, batch    45 | loss: 6.6537852Losses:  6.414161682128906 0.49831727147102356
CurrentTrain: epoch  1, batch    46 | loss: 6.9124789Losses:  6.339019775390625 0.6133939623832703
CurrentTrain: epoch  1, batch    47 | loss: 6.9524136Losses:  6.349348068237305 0.4722130596637726
CurrentTrain: epoch  1, batch    48 | loss: 6.8215613Losses:  6.476221084594727 0.6804928183555603
CurrentTrain: epoch  1, batch    49 | loss: 7.1567140Losses:  6.220065116882324 0.47368210554122925
CurrentTrain: epoch  1, batch    50 | loss: 6.6937470Losses:  7.137824535369873 0.6552310585975647
CurrentTrain: epoch  1, batch    51 | loss: 7.7930555Losses:  6.191483497619629 0.5494924783706665
CurrentTrain: epoch  1, batch    52 | loss: 6.7409759Losses:  6.436260223388672 0.40440136194229126
CurrentTrain: epoch  1, batch    53 | loss: 6.8406615Losses:  6.613225936889648 0.6587990522384644
CurrentTrain: epoch  1, batch    54 | loss: 7.2720251Losses:  6.899934768676758 0.4776700735092163
CurrentTrain: epoch  1, batch    55 | loss: 7.3776050Losses:  5.484859943389893 0.3830151855945587
CurrentTrain: epoch  1, batch    56 | loss: 5.8678751Losses:  5.850852966308594 0.2549729347229004
CurrentTrain: epoch  1, batch    57 | loss: 6.1058259Losses:  6.237588882446289 0.3345322012901306
CurrentTrain: epoch  1, batch    58 | loss: 6.5721211Losses:  7.169619560241699 0.6956945061683655
CurrentTrain: epoch  1, batch    59 | loss: 7.8653140Losses:  7.423120021820068 0.8657270073890686
CurrentTrain: epoch  1, batch    60 | loss: 8.2888470Losses:  6.937913417816162 0.5818304419517517
CurrentTrain: epoch  1, batch    61 | loss: 7.5197439Losses:  6.147492408752441 0.5558022260665894
CurrentTrain: epoch  1, batch    62 | loss: 6.7032948Losses:  6.951845169067383 0.8731568455696106
CurrentTrain: epoch  1, batch    63 | loss: 7.8250022Losses:  6.728673458099365 0.6804267168045044
CurrentTrain: epoch  1, batch    64 | loss: 7.4091001Losses:  6.321267604827881 0.5649343729019165
CurrentTrain: epoch  1, batch    65 | loss: 6.8862019Losses:  6.494629859924316 0.6197064518928528
CurrentTrain: epoch  1, batch    66 | loss: 7.1143365Losses:  6.359680652618408 0.8448153138160706
CurrentTrain: epoch  1, batch    67 | loss: 7.2044959Losses:  6.18096399307251 0.614079475402832
CurrentTrain: epoch  1, batch    68 | loss: 6.7950435Losses:  6.394459247589111 0.5389800071716309
CurrentTrain: epoch  1, batch    69 | loss: 6.9334393Losses:  7.0753374099731445 0.3228614926338196
CurrentTrain: epoch  1, batch    70 | loss: 7.3981991Losses:  6.5959696769714355 0.4627508521080017
CurrentTrain: epoch  1, batch    71 | loss: 7.0587206Losses:  6.440517902374268 0.7931380271911621
CurrentTrain: epoch  1, batch    72 | loss: 7.2336559Losses:  6.240665435791016 0.4876585006713867
CurrentTrain: epoch  1, batch    73 | loss: 6.7283239Losses:  6.445864677429199 0.2511535584926605
CurrentTrain: epoch  1, batch    74 | loss: 6.6970181Losses:  5.50168514251709 0.2983761429786682
CurrentTrain: epoch  2, batch     0 | loss: 5.8000612Losses:  6.4245219230651855 0.4600170850753784
CurrentTrain: epoch  2, batch     1 | loss: 6.8845391Losses:  5.634507179260254 0.31892454624176025
CurrentTrain: epoch  2, batch     2 | loss: 5.9534316Losses:  6.710184097290039 0.6040787696838379
CurrentTrain: epoch  2, batch     3 | loss: 7.3142629Losses:  5.618223190307617 0.4949893355369568
CurrentTrain: epoch  2, batch     4 | loss: 6.1132126Losses:  6.1181182861328125 0.33090195059776306
CurrentTrain: epoch  2, batch     5 | loss: 6.4490204Losses:  5.3799591064453125 0.3256024122238159
CurrentTrain: epoch  2, batch     6 | loss: 5.7055616Losses:  6.471249580383301 0.6680459976196289
CurrentTrain: epoch  2, batch     7 | loss: 7.1392956Losses:  5.622020721435547 0.4533049762248993
CurrentTrain: epoch  2, batch     8 | loss: 6.0753255Losses:  5.508167266845703 0.22638443112373352
CurrentTrain: epoch  2, batch     9 | loss: 5.7345519Losses:  5.745866775512695 0.486680269241333
CurrentTrain: epoch  2, batch    10 | loss: 6.2325468Losses:  5.83745813369751 0.3414207696914673
CurrentTrain: epoch  2, batch    11 | loss: 6.1788788Losses:  5.941001892089844 0.49838584661483765
CurrentTrain: epoch  2, batch    12 | loss: 6.4393878Losses:  5.902778148651123 0.46612483263015747
CurrentTrain: epoch  2, batch    13 | loss: 6.3689032Losses:  6.794566631317139 0.7389112114906311
CurrentTrain: epoch  2, batch    14 | loss: 7.5334778Losses:  6.048031806945801 0.46201980113983154
CurrentTrain: epoch  2, batch    15 | loss: 6.5100517Losses:  6.142698764801025 0.6325700283050537
CurrentTrain: epoch  2, batch    16 | loss: 6.7752686Losses:  5.682975769042969 0.2653956711292267
CurrentTrain: epoch  2, batch    17 | loss: 5.9483714Losses:  5.353349685668945 0.34257030487060547
CurrentTrain: epoch  2, batch    18 | loss: 5.6959200Losses:  5.890240669250488 0.4154409170150757
CurrentTrain: epoch  2, batch    19 | loss: 6.3056817Losses:  5.87639045715332 0.28957027196884155
CurrentTrain: epoch  2, batch    20 | loss: 6.1659608Losses:  5.880473613739014 0.42930442094802856
CurrentTrain: epoch  2, batch    21 | loss: 6.3097782Losses:  5.910798072814941 0.22699889540672302
CurrentTrain: epoch  2, batch    22 | loss: 6.1377969Losses:  5.706479072570801 0.34881988167762756
CurrentTrain: epoch  2, batch    23 | loss: 6.0552988Losses:  5.659852027893066 0.2520575523376465
CurrentTrain: epoch  2, batch    24 | loss: 5.9119096Losses:  5.780205726623535 0.20671534538269043
CurrentTrain: epoch  2, batch    25 | loss: 5.9869213Losses:  6.559303283691406 0.3564455509185791
CurrentTrain: epoch  2, batch    26 | loss: 6.9157486Losses:  6.475090503692627 0.45903685688972473
CurrentTrain: epoch  2, batch    27 | loss: 6.9341273Losses:  6.476362705230713 0.3897373080253601
CurrentTrain: epoch  2, batch    28 | loss: 6.8660998Losses:  6.171593189239502 0.4672449231147766
CurrentTrain: epoch  2, batch    29 | loss: 6.6388383Losses:  6.008237361907959 0.5944514870643616
CurrentTrain: epoch  2, batch    30 | loss: 6.6026888Losses:  6.152837753295898 0.3854185938835144
CurrentTrain: epoch  2, batch    31 | loss: 6.5382562Losses:  6.398187637329102 0.507883608341217
CurrentTrain: epoch  2, batch    32 | loss: 6.9060712Losses:  6.724123001098633 0.699302613735199
CurrentTrain: epoch  2, batch    33 | loss: 7.4234257Losses:  7.184034824371338 0.8312848210334778
CurrentTrain: epoch  2, batch    34 | loss: 8.0153198Losses:  6.499726295471191 0.5128647685050964
CurrentTrain: epoch  2, batch    35 | loss: 7.0125909Losses:  5.959538459777832 0.33992820978164673
CurrentTrain: epoch  2, batch    36 | loss: 6.2994666Losses:  6.288195610046387 0.41120320558547974
CurrentTrain: epoch  2, batch    37 | loss: 6.6993990Losses:  5.3194780349731445 0.29062342643737793
CurrentTrain: epoch  2, batch    38 | loss: 5.6101017Losses:  6.061275482177734 0.38255900144577026
CurrentTrain: epoch  2, batch    39 | loss: 6.4438343Losses:  6.107444763183594 0.32971805334091187
CurrentTrain: epoch  2, batch    40 | loss: 6.4371629Losses:  5.498554229736328 0.28401291370391846
CurrentTrain: epoch  2, batch    41 | loss: 5.7825670Losses:  6.48343563079834 0.5202481746673584
CurrentTrain: epoch  2, batch    42 | loss: 7.0036840Losses:  5.93471097946167 0.34726008772850037
CurrentTrain: epoch  2, batch    43 | loss: 6.2819710Losses:  5.783865928649902 0.356411874294281
CurrentTrain: epoch  2, batch    44 | loss: 6.1402779Losses:  5.624539375305176 0.363776832818985
CurrentTrain: epoch  2, batch    45 | loss: 5.9883161Losses:  5.6357269287109375 0.2424035370349884
CurrentTrain: epoch  2, batch    46 | loss: 5.8781304Losses:  5.8506927490234375 0.41156482696533203
CurrentTrain: epoch  2, batch    47 | loss: 6.2622576Losses:  5.879124164581299 0.3840382993221283
CurrentTrain: epoch  2, batch    48 | loss: 6.2631626Losses:  6.2814412117004395 0.3873908817768097
CurrentTrain: epoch  2, batch    49 | loss: 6.6688323Losses:  5.705028533935547 0.3017476797103882
CurrentTrain: epoch  2, batch    50 | loss: 6.0067763Losses:  5.9541239738464355 0.615972638130188
CurrentTrain: epoch  2, batch    51 | loss: 6.5700965Losses:  6.630553722381592 0.4429316222667694
CurrentTrain: epoch  2, batch    52 | loss: 7.0734854Losses:  5.928854465484619 0.2196158766746521
CurrentTrain: epoch  2, batch    53 | loss: 6.1484704Losses:  5.564192771911621 0.2795036733150482
CurrentTrain: epoch  2, batch    54 | loss: 5.8436966Losses:  5.847776412963867 0.20800380408763885
CurrentTrain: epoch  2, batch    55 | loss: 6.0557804Losses:  5.486783981323242 0.30966252088546753
CurrentTrain: epoch  2, batch    56 | loss: 5.7964463Losses:  5.566162586212158 0.267811119556427
CurrentTrain: epoch  2, batch    57 | loss: 5.8339739Losses:  5.542503356933594 0.2670714259147644
CurrentTrain: epoch  2, batch    58 | loss: 5.8095746Losses:  5.971707344055176 0.5359864234924316
CurrentTrain: epoch  2, batch    59 | loss: 6.5076938Losses:  5.968288421630859 0.4763249158859253
CurrentTrain: epoch  2, batch    60 | loss: 6.4446135Losses:  5.492334365844727 0.45219045877456665
CurrentTrain: epoch  2, batch    61 | loss: 5.9445248Losses:  5.743198871612549 0.4775151014328003
CurrentTrain: epoch  2, batch    62 | loss: 6.2207141Losses:  5.111991882324219 0.2159392237663269
CurrentTrain: epoch  2, batch    63 | loss: 5.3279309Losses:  5.867580413818359 0.3681643009185791
CurrentTrain: epoch  2, batch    64 | loss: 6.2357445Losses:  6.21023416519165 0.5644590854644775
CurrentTrain: epoch  2, batch    65 | loss: 6.7746935Losses:  6.246366500854492 0.4724673926830292
CurrentTrain: epoch  2, batch    66 | loss: 6.7188339Losses:  5.7802839279174805 0.18623924255371094
CurrentTrain: epoch  2, batch    67 | loss: 5.9665232Losses:  5.340793609619141 0.22071561217308044
CurrentTrain: epoch  2, batch    68 | loss: 5.5615091Losses:  5.374451637268066 0.30513206124305725
CurrentTrain: epoch  2, batch    69 | loss: 5.6795835Losses:  5.7139458656311035 0.4031466245651245
CurrentTrain: epoch  2, batch    70 | loss: 6.1170926Losses:  5.564254283905029 0.26518115401268005
CurrentTrain: epoch  2, batch    71 | loss: 5.8294353Losses:  5.911482334136963 0.20257747173309326
CurrentTrain: epoch  2, batch    72 | loss: 6.1140599Losses:  5.7049360275268555 0.5498341917991638
CurrentTrain: epoch  2, batch    73 | loss: 6.2547703Losses:  5.471826553344727 0.22817076742649078
CurrentTrain: epoch  2, batch    74 | loss: 5.6999974Losses:  5.684459686279297 0.19758428633213043
CurrentTrain: epoch  3, batch     0 | loss: 5.8820438Losses:  5.879689693450928 0.5615144371986389
CurrentTrain: epoch  3, batch     1 | loss: 6.4412041Losses:  5.351907253265381 0.14683839678764343
CurrentTrain: epoch  3, batch     2 | loss: 5.4987454Losses:  5.970385551452637 0.49057236313819885
CurrentTrain: epoch  3, batch     3 | loss: 6.4609580Losses:  5.716750144958496 0.43245628476142883
CurrentTrain: epoch  3, batch     4 | loss: 6.1492066Losses:  5.1445817947387695 0.22279661893844604
CurrentTrain: epoch  3, batch     5 | loss: 5.3673782Losses:  5.084954261779785 0.3076649308204651
CurrentTrain: epoch  3, batch     6 | loss: 5.3926191Losses:  5.585577011108398 0.26076745986938477
CurrentTrain: epoch  3, batch     7 | loss: 5.8463445Losses:  5.450992584228516 0.19589927792549133
CurrentTrain: epoch  3, batch     8 | loss: 5.6468921Losses:  5.254961013793945 0.30539581179618835
CurrentTrain: epoch  3, batch     9 | loss: 5.5603566Losses:  6.156975746154785 0.4269985556602478
CurrentTrain: epoch  3, batch    10 | loss: 6.5839744Losses:  5.202477931976318 0.21351756155490875
CurrentTrain: epoch  3, batch    11 | loss: 5.4159956Losses:  5.419122695922852 0.15705357491970062
CurrentTrain: epoch  3, batch    12 | loss: 5.5761762Losses:  5.165109634399414 0.16254428029060364
CurrentTrain: epoch  3, batch    13 | loss: 5.3276539Losses:  5.337442398071289 0.2041037529706955
CurrentTrain: epoch  3, batch    14 | loss: 5.5415463Losses:  5.24979829788208 0.32022082805633545
CurrentTrain: epoch  3, batch    15 | loss: 5.5700192Losses:  5.489456653594971 0.23460614681243896
CurrentTrain: epoch  3, batch    16 | loss: 5.7240629Losses:  5.262576103210449 0.24024824798107147
CurrentTrain: epoch  3, batch    17 | loss: 5.5028243Losses:  5.742680549621582 0.3189268112182617
CurrentTrain: epoch  3, batch    18 | loss: 6.0616074Losses:  5.462880611419678 0.2647031545639038
CurrentTrain: epoch  3, batch    19 | loss: 5.7275839Losses:  5.40641975402832 0.2546824812889099
CurrentTrain: epoch  3, batch    20 | loss: 5.6611023Losses:  6.80881404876709 0.2997596263885498
CurrentTrain: epoch  3, batch    21 | loss: 7.1085739Losses:  5.059744834899902 0.20270049571990967
CurrentTrain: epoch  3, batch    22 | loss: 5.2624454Losses:  5.827456474304199 0.43885621428489685
CurrentTrain: epoch  3, batch    23 | loss: 6.2663126Losses:  5.1279802322387695 0.29277122020721436
CurrentTrain: epoch  3, batch    24 | loss: 5.4207516Losses:  5.978702068328857 0.2860007882118225
CurrentTrain: epoch  3, batch    25 | loss: 6.2647028Losses:  5.464556694030762 0.23354630172252655
CurrentTrain: epoch  3, batch    26 | loss: 5.6981030Losses:  5.442957401275635 0.2334619164466858
CurrentTrain: epoch  3, batch    27 | loss: 5.6764193Losses:  5.1617655754089355 0.21419334411621094
CurrentTrain: epoch  3, batch    28 | loss: 5.3759589Losses:  5.638222694396973 0.22906282544136047
CurrentTrain: epoch  3, batch    29 | loss: 5.8672857Losses:  5.65443754196167 0.23396897315979004
CurrentTrain: epoch  3, batch    30 | loss: 5.8884068Losses:  5.6358160972595215 0.18415366113185883
CurrentTrain: epoch  3, batch    31 | loss: 5.8199697Losses:  5.5802130699157715 0.3854810893535614
CurrentTrain: epoch  3, batch    32 | loss: 5.9656940Losses:  5.517601013183594 0.12293997406959534
CurrentTrain: epoch  3, batch    33 | loss: 5.6405411Losses:  5.355422496795654 0.1744416505098343
CurrentTrain: epoch  3, batch    34 | loss: 5.5298643Losses:  5.4281206130981445 0.16688409447669983
CurrentTrain: epoch  3, batch    35 | loss: 5.5950046Losses:  5.412347793579102 0.20813283324241638
CurrentTrain: epoch  3, batch    36 | loss: 5.6204805Losses:  6.566967010498047 0.6714339256286621
CurrentTrain: epoch  3, batch    37 | loss: 7.2384009Losses:  5.733700275421143 0.29438072443008423
CurrentTrain: epoch  3, batch    38 | loss: 6.0280809Losses:  5.233365058898926 0.21698853373527527
CurrentTrain: epoch  3, batch    39 | loss: 5.4503536Losses:  6.152488708496094 0.6338393092155457
CurrentTrain: epoch  3, batch    40 | loss: 6.7863278Losses:  6.3024444580078125 0.5625589489936829
CurrentTrain: epoch  3, batch    41 | loss: 6.8650036Losses:  5.230611801147461 0.2552018463611603
CurrentTrain: epoch  3, batch    42 | loss: 5.4858136Losses:  5.674560546875 0.07821675390005112
CurrentTrain: epoch  3, batch    43 | loss: 5.7527771Losses:  5.59802770614624 0.20207709074020386
CurrentTrain: epoch  3, batch    44 | loss: 5.8001046Losses:  5.261230945587158 0.16949592530727386
CurrentTrain: epoch  3, batch    45 | loss: 5.4307270Losses:  5.344307899475098 0.23916971683502197
CurrentTrain: epoch  3, batch    46 | loss: 5.5834775Losses:  5.644831657409668 0.3291410803794861
CurrentTrain: epoch  3, batch    47 | loss: 5.9739728Losses:  5.230897426605225 0.2091520130634308
CurrentTrain: epoch  3, batch    48 | loss: 5.4400496Losses:  5.146371364593506 0.04185953736305237
CurrentTrain: epoch  3, batch    49 | loss: 5.1882310Losses:  5.218257904052734 0.263811320066452
CurrentTrain: epoch  3, batch    50 | loss: 5.4820690Losses:  5.388707160949707 0.24133655428886414
CurrentTrain: epoch  3, batch    51 | loss: 5.6300435Losses:  5.14459753036499 0.2404341995716095
CurrentTrain: epoch  3, batch    52 | loss: 5.3850317Losses:  5.485213279724121 0.2200462818145752
CurrentTrain: epoch  3, batch    53 | loss: 5.7052593Losses:  5.608395576477051 0.17144326865673065
CurrentTrain: epoch  3, batch    54 | loss: 5.7798390Losses:  6.016407489776611 0.32662761211395264
CurrentTrain: epoch  3, batch    55 | loss: 6.3430352Losses:  5.995185375213623 0.18352025747299194
CurrentTrain: epoch  3, batch    56 | loss: 6.1787057Losses:  5.154143333435059 0.16951292753219604
CurrentTrain: epoch  3, batch    57 | loss: 5.3236561Losses:  5.058995246887207 0.18231847882270813
CurrentTrain: epoch  3, batch    58 | loss: 5.2413139Losses:  5.670612335205078 0.26450085639953613
CurrentTrain: epoch  3, batch    59 | loss: 5.9351130Losses:  5.136447429656982 0.24423670768737793
CurrentTrain: epoch  3, batch    60 | loss: 5.3806839Losses:  5.053583145141602 0.17606228590011597
CurrentTrain: epoch  3, batch    61 | loss: 5.2296453Losses:  5.574723243713379 0.3053385019302368
CurrentTrain: epoch  3, batch    62 | loss: 5.8800616Losses:  5.4588775634765625 0.3259788751602173
CurrentTrain: epoch  3, batch    63 | loss: 5.7848563Losses:  5.613957405090332 0.20068565011024475
CurrentTrain: epoch  3, batch    64 | loss: 5.8146429Losses:  5.381512641906738 0.2310878038406372
CurrentTrain: epoch  3, batch    65 | loss: 5.6126003Losses:  5.117245197296143 0.2704945504665375
CurrentTrain: epoch  3, batch    66 | loss: 5.3877397Losses:  5.161820411682129 0.18029357492923737
CurrentTrain: epoch  3, batch    67 | loss: 5.3421140Losses:  4.951237678527832 0.11545141041278839
CurrentTrain: epoch  3, batch    68 | loss: 5.0666890Losses:  4.862251281738281 0.18570953607559204
CurrentTrain: epoch  3, batch    69 | loss: 5.0479608Losses:  5.71368408203125 0.436841756105423
CurrentTrain: epoch  3, batch    70 | loss: 6.1505260Losses:  7.102810382843018 0.752455472946167
CurrentTrain: epoch  3, batch    71 | loss: 7.8552656Losses:  5.285759449005127 0.19835776090621948
CurrentTrain: epoch  3, batch    72 | loss: 5.4841170Losses:  5.462265968322754 0.20726892352104187
CurrentTrain: epoch  3, batch    73 | loss: 5.6695347Losses:  5.072268009185791 0.14325855672359467
CurrentTrain: epoch  3, batch    74 | loss: 5.2155266Losses:  5.147762298583984 0.16058644652366638
CurrentTrain: epoch  4, batch     0 | loss: 5.3083487Losses:  5.274700164794922 0.1486157774925232
CurrentTrain: epoch  4, batch     1 | loss: 5.4233160Losses:  5.259984016418457 0.16073988378047943
CurrentTrain: epoch  4, batch     2 | loss: 5.4207239Losses:  5.204771995544434 0.1424558162689209
CurrentTrain: epoch  4, batch     3 | loss: 5.3472281Losses:  4.9172258377075195 0.15747305750846863
CurrentTrain: epoch  4, batch     4 | loss: 5.0746989Losses:  5.196298599243164 0.20332542061805725
CurrentTrain: epoch  4, batch     5 | loss: 5.3996239Losses:  5.136652946472168 0.14553377032279968
CurrentTrain: epoch  4, batch     6 | loss: 5.2821865Losses:  4.962764739990234 0.14161989092826843
CurrentTrain: epoch  4, batch     7 | loss: 5.1043844Losses:  5.476553916931152 0.517119288444519
CurrentTrain: epoch  4, batch     8 | loss: 5.9936733Losses:  4.949946880340576 0.13735352456569672
CurrentTrain: epoch  4, batch     9 | loss: 5.0873003Losses:  4.942561149597168 0.2001805603504181
CurrentTrain: epoch  4, batch    10 | loss: 5.1427417Losses:  5.009397506713867 0.21016868948936462
CurrentTrain: epoch  4, batch    11 | loss: 5.2195663Losses:  4.9873270988464355 0.2200140357017517
CurrentTrain: epoch  4, batch    12 | loss: 5.2073412Losses:  5.163586139678955 0.06612294167280197
CurrentTrain: epoch  4, batch    13 | loss: 5.2297091Losses:  5.127298831939697 0.19278627634048462
CurrentTrain: epoch  4, batch    14 | loss: 5.3200850Losses:  4.965450286865234 0.16575869917869568
CurrentTrain: epoch  4, batch    15 | loss: 5.1312089Losses:  5.021775722503662 0.15980038046836853
CurrentTrain: epoch  4, batch    16 | loss: 5.1815763Losses:  6.130948066711426 0.24873870611190796
CurrentTrain: epoch  4, batch    17 | loss: 6.3796868Losses:  5.014218330383301 0.16764003038406372
CurrentTrain: epoch  4, batch    18 | loss: 5.1818585Losses:  6.302130699157715 0.647142231464386
CurrentTrain: epoch  4, batch    19 | loss: 6.9492731Losses:  6.093267440795898 0.3508971333503723
CurrentTrain: epoch  4, batch    20 | loss: 6.4441648Losses:  5.205820083618164 0.25497329235076904
CurrentTrain: epoch  4, batch    21 | loss: 5.4607935Losses:  5.176529884338379 0.16133630275726318
CurrentTrain: epoch  4, batch    22 | loss: 5.3378663Losses:  4.924901485443115 0.1989491879940033
CurrentTrain: epoch  4, batch    23 | loss: 5.1238508Losses:  5.60481071472168 0.31089019775390625
CurrentTrain: epoch  4, batch    24 | loss: 5.9157009Losses:  5.809807300567627 0.46170181035995483
CurrentTrain: epoch  4, batch    25 | loss: 6.2715092Losses:  4.960656642913818 0.24062104523181915
CurrentTrain: epoch  4, batch    26 | loss: 5.2012777Losses:  5.247809410095215 0.12760010361671448
CurrentTrain: epoch  4, batch    27 | loss: 5.3754096Losses:  5.17343807220459 0.1760798990726471
CurrentTrain: epoch  4, batch    28 | loss: 5.3495178Losses:  5.142672538757324 0.11188581585884094
CurrentTrain: epoch  4, batch    29 | loss: 5.2545586Losses:  4.826897621154785 0.15294760465621948
CurrentTrain: epoch  4, batch    30 | loss: 4.9798450Losses:  4.98875617980957 0.24046707153320312
CurrentTrain: epoch  4, batch    31 | loss: 5.2292233Losses:  5.1439080238342285 0.2015988528728485
CurrentTrain: epoch  4, batch    32 | loss: 5.3455067Losses:  5.023336410522461 0.1268804371356964
CurrentTrain: epoch  4, batch    33 | loss: 5.1502171Losses:  5.011892318725586 0.24103489518165588
CurrentTrain: epoch  4, batch    34 | loss: 5.2529273Losses:  5.013272285461426 0.39861416816711426
CurrentTrain: epoch  4, batch    35 | loss: 5.4118862Losses:  4.966728210449219 0.19761602580547333
CurrentTrain: epoch  4, batch    36 | loss: 5.1643443Losses:  4.792227745056152 0.1768280267715454
CurrentTrain: epoch  4, batch    37 | loss: 4.9690557Losses:  4.846968650817871 0.1286712884902954
CurrentTrain: epoch  4, batch    38 | loss: 4.9756398Losses:  4.890281677246094 0.13359397649765015
CurrentTrain: epoch  4, batch    39 | loss: 5.0238757Losses:  5.664951324462891 0.49332988262176514
CurrentTrain: epoch  4, batch    40 | loss: 6.1582813Losses:  4.926501274108887 0.17592164874076843
CurrentTrain: epoch  4, batch    41 | loss: 5.1024227Losses:  4.934327125549316 0.13906487822532654
CurrentTrain: epoch  4, batch    42 | loss: 5.0733919Losses:  4.865665435791016 0.2054019570350647
CurrentTrain: epoch  4, batch    43 | loss: 5.0710673Losses:  4.960395812988281 0.14360034465789795
CurrentTrain: epoch  4, batch    44 | loss: 5.1039963Losses:  4.698305606842041 0.16309002041816711
CurrentTrain: epoch  4, batch    45 | loss: 4.8613958Losses:  5.1556806564331055 0.2547149658203125
CurrentTrain: epoch  4, batch    46 | loss: 5.4103956Losses:  4.770601272583008 0.14026549458503723
CurrentTrain: epoch  4, batch    47 | loss: 4.9108667Losses:  5.133595943450928 0.1315908133983612
CurrentTrain: epoch  4, batch    48 | loss: 5.2651868Losses:  4.751869201660156 0.16632267832756042
CurrentTrain: epoch  4, batch    49 | loss: 4.9181919Losses:  5.581876754760742 0.3513023853302002
CurrentTrain: epoch  4, batch    50 | loss: 5.9331789Losses:  4.93658971786499 0.1588982343673706
CurrentTrain: epoch  4, batch    51 | loss: 5.0954881Losses:  4.73569393157959 0.14730003476142883
CurrentTrain: epoch  4, batch    52 | loss: 4.8829942Losses:  4.972282409667969 0.19298988580703735
CurrentTrain: epoch  4, batch    53 | loss: 5.1652722Losses:  6.179872512817383 0.18960022926330566
CurrentTrain: epoch  4, batch    54 | loss: 6.3694725Losses:  5.730404853820801 0.19602102041244507
CurrentTrain: epoch  4, batch    55 | loss: 5.9264259Losses:  5.46724271774292 0.17856919765472412
CurrentTrain: epoch  4, batch    56 | loss: 5.6458120Losses:  5.039554119110107 0.05893206596374512
CurrentTrain: epoch  4, batch    57 | loss: 5.0984859Losses:  4.961312294006348 0.16356690227985382
CurrentTrain: epoch  4, batch    58 | loss: 5.1248794Losses:  5.120975971221924 0.16084866225719452
CurrentTrain: epoch  4, batch    59 | loss: 5.2818246Losses:  4.822314262390137 0.12148281931877136
CurrentTrain: epoch  4, batch    60 | loss: 4.9437971Losses:  5.345308303833008 0.25100135803222656
CurrentTrain: epoch  4, batch    61 | loss: 5.5963097Losses:  5.088402271270752 0.21650050580501556
CurrentTrain: epoch  4, batch    62 | loss: 5.3049026Losses:  5.023603916168213 0.21422313153743744
CurrentTrain: epoch  4, batch    63 | loss: 5.2378268Losses:  4.850000381469727 0.13907021284103394
CurrentTrain: epoch  4, batch    64 | loss: 4.9890704Losses:  5.649348735809326 0.21371181309223175
CurrentTrain: epoch  4, batch    65 | loss: 5.8630605Losses:  5.339005470275879 0.24653682112693787
CurrentTrain: epoch  4, batch    66 | loss: 5.5855422Losses:  4.828188896179199 0.15086834132671356
CurrentTrain: epoch  4, batch    67 | loss: 4.9790573Losses:  5.285749435424805 0.2033599317073822
CurrentTrain: epoch  4, batch    68 | loss: 5.4891095Losses:  5.174018859863281 0.1427658200263977
CurrentTrain: epoch  4, batch    69 | loss: 5.3167849Losses:  4.901003837585449 0.1385752558708191
CurrentTrain: epoch  4, batch    70 | loss: 5.0395789Losses:  5.803272247314453 0.24832704663276672
CurrentTrain: epoch  4, batch    71 | loss: 6.0515995Losses:  5.120180130004883 0.2110675573348999
CurrentTrain: epoch  4, batch    72 | loss: 5.3312478Losses:  4.990983486175537 0.1937362551689148
CurrentTrain: epoch  4, batch    73 | loss: 5.1847196Losses:  5.050497055053711 0.17577281594276428
CurrentTrain: epoch  4, batch    74 | loss: 5.2262697Losses:  5.548921585083008 0.4810989797115326
CurrentTrain: epoch  5, batch     0 | loss: 6.0300207Losses:  4.991293907165527 0.1853858232498169
CurrentTrain: epoch  5, batch     1 | loss: 5.1766796Losses:  4.850191116333008 0.17288513481616974
CurrentTrain: epoch  5, batch     2 | loss: 5.0230761Losses:  5.417630195617676 0.24249428510665894
CurrentTrain: epoch  5, batch     3 | loss: 5.6601243Losses:  5.331984043121338 0.4321375787258148
CurrentTrain: epoch  5, batch     4 | loss: 5.7641215Losses:  5.150109767913818 0.13990238308906555
CurrentTrain: epoch  5, batch     5 | loss: 5.2900124Losses:  4.870790958404541 0.13063158094882965
CurrentTrain: epoch  5, batch     6 | loss: 5.0014224Losses:  4.699992656707764 0.0794859305024147
CurrentTrain: epoch  5, batch     7 | loss: 4.7794785Losses:  4.921994686126709 0.1694214642047882
CurrentTrain: epoch  5, batch     8 | loss: 5.0914164Losses:  4.890932083129883 0.21386078000068665
CurrentTrain: epoch  5, batch     9 | loss: 5.1047931Losses:  4.918161392211914 0.31127309799194336
CurrentTrain: epoch  5, batch    10 | loss: 5.2294345Losses:  5.33534049987793 0.22314304113388062
CurrentTrain: epoch  5, batch    11 | loss: 5.5584836Losses:  4.852744102478027 0.19353275001049042
CurrentTrain: epoch  5, batch    12 | loss: 5.0462770Losses:  4.886795997619629 0.22754919528961182
CurrentTrain: epoch  5, batch    13 | loss: 5.1143451Losses:  5.6678643226623535 0.3895231783390045
CurrentTrain: epoch  5, batch    14 | loss: 6.0573874Losses:  4.9845991134643555 0.18927904963493347
CurrentTrain: epoch  5, batch    15 | loss: 5.1738782Losses:  4.786625862121582 0.08927980065345764
CurrentTrain: epoch  5, batch    16 | loss: 4.8759055Losses:  5.268886566162109 0.4334790110588074
CurrentTrain: epoch  5, batch    17 | loss: 5.7023654Losses:  4.970855236053467 0.1541280448436737
CurrentTrain: epoch  5, batch    18 | loss: 5.1249833Losses:  4.786201477050781 0.14185236394405365
CurrentTrain: epoch  5, batch    19 | loss: 4.9280539Losses:  5.017996788024902 0.15412846207618713
CurrentTrain: epoch  5, batch    20 | loss: 5.1721253Losses:  5.300670623779297 0.21837520599365234
CurrentTrain: epoch  5, batch    21 | loss: 5.5190458Losses:  5.512300968170166 0.18697360157966614
CurrentTrain: epoch  5, batch    22 | loss: 5.6992745Losses:  5.100790500640869 0.19750504195690155
CurrentTrain: epoch  5, batch    23 | loss: 5.2982955Losses:  4.812408447265625 0.086853988468647
CurrentTrain: epoch  5, batch    24 | loss: 4.8992624Losses:  4.687010765075684 0.09085474908351898
CurrentTrain: epoch  5, batch    25 | loss: 4.7778654Losses:  4.679619789123535 0.15505848824977875
CurrentTrain: epoch  5, batch    26 | loss: 4.8346782Losses:  5.096202850341797 0.1479020118713379
CurrentTrain: epoch  5, batch    27 | loss: 5.2441049Losses:  4.8272199630737305 0.14435335993766785
CurrentTrain: epoch  5, batch    28 | loss: 4.9715734Losses:  4.699125289916992 0.07167203724384308
CurrentTrain: epoch  5, batch    29 | loss: 4.7707973Losses:  4.8653764724731445 0.13605168461799622
CurrentTrain: epoch  5, batch    30 | loss: 5.0014281Losses:  4.832077503204346 0.12855984270572662
CurrentTrain: epoch  5, batch    31 | loss: 4.9606376Losses:  5.230803489685059 0.22819679975509644
CurrentTrain: epoch  5, batch    32 | loss: 5.4590001Losses:  4.818499565124512 0.130052387714386
CurrentTrain: epoch  5, batch    33 | loss: 4.9485521Losses:  4.646572113037109 0.14278042316436768
CurrentTrain: epoch  5, batch    34 | loss: 4.7893524Losses:  4.8575334548950195 0.07085315883159637
CurrentTrain: epoch  5, batch    35 | loss: 4.9283867Losses:  4.763131141662598 0.1715092957019806
CurrentTrain: epoch  5, batch    36 | loss: 4.9346404Losses:  5.268646240234375 0.42804446816444397
CurrentTrain: epoch  5, batch    37 | loss: 5.6966906Losses:  5.092921257019043 0.22959241271018982
CurrentTrain: epoch  5, batch    38 | loss: 5.3225136Losses:  4.827713966369629 0.13769686222076416
CurrentTrain: epoch  5, batch    39 | loss: 4.9654107Losses:  4.841055870056152 0.1349651962518692
CurrentTrain: epoch  5, batch    40 | loss: 4.9760213Losses:  4.82308292388916 0.14590953290462494
CurrentTrain: epoch  5, batch    41 | loss: 4.9689922Losses:  4.878958702087402 0.17125964164733887
CurrentTrain: epoch  5, batch    42 | loss: 5.0502186Losses:  4.770845413208008 0.07957884669303894
CurrentTrain: epoch  5, batch    43 | loss: 4.8504243Losses:  4.686002731323242 0.09768262505531311
CurrentTrain: epoch  5, batch    44 | loss: 4.7836852Losses:  5.0501556396484375 0.16563880443572998
CurrentTrain: epoch  5, batch    45 | loss: 5.2157946Losses:  4.948554992675781 0.08427243679761887
CurrentTrain: epoch  5, batch    46 | loss: 5.0328274Losses:  4.792031288146973 0.11262664198875427
CurrentTrain: epoch  5, batch    47 | loss: 4.9046578Losses:  4.708658218383789 0.12125710397958755
CurrentTrain: epoch  5, batch    48 | loss: 4.8299155Losses:  4.760323524475098 0.14561782777309418
CurrentTrain: epoch  5, batch    49 | loss: 4.9059415Losses:  4.796043872833252 0.1502700001001358
CurrentTrain: epoch  5, batch    50 | loss: 4.9463139Losses:  4.7154154777526855 0.15020713210105896
CurrentTrain: epoch  5, batch    51 | loss: 4.8656225Losses:  5.012124538421631 0.1766635775566101
CurrentTrain: epoch  5, batch    52 | loss: 5.1887879Losses:  5.207304000854492 0.20413362979888916
CurrentTrain: epoch  5, batch    53 | loss: 5.4114375Losses:  4.691123008728027 0.11276113986968994
CurrentTrain: epoch  5, batch    54 | loss: 4.8038840Losses:  4.8010125160217285 0.09099242836236954
CurrentTrain: epoch  5, batch    55 | loss: 4.8920050Losses:  4.779375076293945 0.09276752918958664
CurrentTrain: epoch  5, batch    56 | loss: 4.8721428Losses:  4.731459617614746 0.1389268934726715
CurrentTrain: epoch  5, batch    57 | loss: 4.8703866Losses:  4.813779830932617 0.11167523264884949
CurrentTrain: epoch  5, batch    58 | loss: 4.9254551Losses:  4.880099296569824 0.11838605254888535
CurrentTrain: epoch  5, batch    59 | loss: 4.9984856Losses:  5.2508320808410645 0.4450196921825409
CurrentTrain: epoch  5, batch    60 | loss: 5.6958518Losses:  4.909131050109863 0.10465693473815918
CurrentTrain: epoch  5, batch    61 | loss: 5.0137882Losses:  4.806328773498535 0.15291695296764374
CurrentTrain: epoch  5, batch    62 | loss: 4.9592457Losses:  4.85942268371582 0.09621842205524445
CurrentTrain: epoch  5, batch    63 | loss: 4.9556413Losses:  4.692519187927246 0.1232270747423172
CurrentTrain: epoch  5, batch    64 | loss: 4.8157463Losses:  5.220417022705078 0.27132588624954224
CurrentTrain: epoch  5, batch    65 | loss: 5.4917431Losses:  4.747596740722656 0.09416523575782776
CurrentTrain: epoch  5, batch    66 | loss: 4.8417621Losses:  4.824278831481934 0.12775936722755432
CurrentTrain: epoch  5, batch    67 | loss: 4.9520383Losses:  4.891131401062012 0.15406303107738495
CurrentTrain: epoch  5, batch    68 | loss: 5.0451946Losses:  4.922248840332031 0.06269743293523788
CurrentTrain: epoch  5, batch    69 | loss: 4.9849463Losses:  4.7195353507995605 0.08442211896181107
CurrentTrain: epoch  5, batch    70 | loss: 4.8039575Losses:  4.8340044021606445 0.16032466292381287
CurrentTrain: epoch  5, batch    71 | loss: 4.9943290Losses:  4.821996688842773 0.13001024723052979
CurrentTrain: epoch  5, batch    72 | loss: 4.9520068Losses:  4.624650955200195 0.09255829453468323
CurrentTrain: epoch  5, batch    73 | loss: 4.7172093Losses:  4.903807640075684 0.23964737355709076
CurrentTrain: epoch  5, batch    74 | loss: 5.1434550Losses:  4.7569122314453125 0.12293359637260437
CurrentTrain: epoch  6, batch     0 | loss: 4.8798456Losses:  4.59670352935791 0.12169861793518066
CurrentTrain: epoch  6, batch     1 | loss: 4.7184019Losses:  4.927494049072266 0.13207373023033142
CurrentTrain: epoch  6, batch     2 | loss: 5.0595679Losses:  4.6157684326171875 0.12756070494651794
CurrentTrain: epoch  6, batch     3 | loss: 4.7433290Losses:  4.883796215057373 0.15336087346076965
CurrentTrain: epoch  6, batch     4 | loss: 5.0371571Losses:  4.691099166870117 0.16128361225128174
CurrentTrain: epoch  6, batch     5 | loss: 4.8523827Losses:  4.776392936706543 0.08920897543430328
CurrentTrain: epoch  6, batch     6 | loss: 4.8656020Losses:  4.667697906494141 0.12378816306591034
CurrentTrain: epoch  6, batch     7 | loss: 4.7914863Losses:  4.649598121643066 0.09796614944934845
CurrentTrain: epoch  6, batch     8 | loss: 4.7475643Losses:  4.577124118804932 0.08661621809005737
CurrentTrain: epoch  6, batch     9 | loss: 4.6637402Losses:  4.6237053871154785 0.11553262174129486
CurrentTrain: epoch  6, batch    10 | loss: 4.7392378Losses:  4.681457042694092 0.11695735901594162
CurrentTrain: epoch  6, batch    11 | loss: 4.7984142Losses:  4.7239990234375 0.13075020909309387
CurrentTrain: epoch  6, batch    12 | loss: 4.8547492Losses:  4.894097328186035 0.11007474362850189
CurrentTrain: epoch  6, batch    13 | loss: 5.0041718Losses:  4.937305927276611 0.14954032003879547
CurrentTrain: epoch  6, batch    14 | loss: 5.0868464Losses:  4.636653900146484 0.11359821259975433
CurrentTrain: epoch  6, batch    15 | loss: 4.7502522Losses:  5.581409454345703 0.564182698726654
CurrentTrain: epoch  6, batch    16 | loss: 6.1455922Losses:  4.746382236480713 0.14803193509578705
CurrentTrain: epoch  6, batch    17 | loss: 4.8944139Losses:  5.152454376220703 0.057953547686338425
CurrentTrain: epoch  6, batch    18 | loss: 5.2104077Losses:  4.748414516448975 0.07942479848861694
CurrentTrain: epoch  6, batch    19 | loss: 4.8278394Losses:  4.650744438171387 0.09398356080055237
CurrentTrain: epoch  6, batch    20 | loss: 4.7447281Losses:  4.996382236480713 0.2671442925930023
CurrentTrain: epoch  6, batch    21 | loss: 5.2635264Losses:  4.632012367248535 0.11164616048336029
CurrentTrain: epoch  6, batch    22 | loss: 4.7436585Losses:  4.681224346160889 0.0953051894903183
CurrentTrain: epoch  6, batch    23 | loss: 4.7765293Losses:  4.7511701583862305 0.12651272118091583
CurrentTrain: epoch  6, batch    24 | loss: 4.8776827Losses:  4.652469635009766 0.11188676953315735
CurrentTrain: epoch  6, batch    25 | loss: 4.7643566Losses:  4.7393083572387695 0.1260155737400055
CurrentTrain: epoch  6, batch    26 | loss: 4.8653240Losses:  4.700296401977539 0.1087150052189827
CurrentTrain: epoch  6, batch    27 | loss: 4.8090115Losses:  4.738509654998779 0.04721789062023163
CurrentTrain: epoch  6, batch    28 | loss: 4.7857275Losses:  4.987696647644043 0.25824683904647827
CurrentTrain: epoch  6, batch    29 | loss: 5.2459435Losses:  4.642688751220703 0.10575047880411148
CurrentTrain: epoch  6, batch    30 | loss: 4.7484393Losses:  4.705527305603027 0.10459057241678238
CurrentTrain: epoch  6, batch    31 | loss: 4.8101177Losses:  4.680876731872559 0.11697380244731903
CurrentTrain: epoch  6, batch    32 | loss: 4.7978506Losses:  4.669492721557617 0.08589445054531097
CurrentTrain: epoch  6, batch    33 | loss: 4.7553873Losses:  4.70438289642334 0.10299919545650482
CurrentTrain: epoch  6, batch    34 | loss: 4.8073821Losses:  4.616578102111816 0.11174909770488739
CurrentTrain: epoch  6, batch    35 | loss: 4.7283273Losses:  4.67186164855957 0.1258355677127838
CurrentTrain: epoch  6, batch    36 | loss: 4.7976971Losses:  4.711014747619629 0.11214689165353775
CurrentTrain: epoch  6, batch    37 | loss: 4.8231616Losses:  4.740422248840332 0.07744401693344116
CurrentTrain: epoch  6, batch    38 | loss: 4.8178663Losses:  4.659615516662598 0.0625925064086914
CurrentTrain: epoch  6, batch    39 | loss: 4.7222080Losses:  4.694100379943848 0.11565057933330536
CurrentTrain: epoch  6, batch    40 | loss: 4.8097510Losses:  4.66054105758667 0.09863991290330887
CurrentTrain: epoch  6, batch    41 | loss: 4.7591810Losses:  4.637243270874023 0.10583797097206116
CurrentTrain: epoch  6, batch    42 | loss: 4.7430811Losses:  5.1262054443359375 0.2356041967868805
CurrentTrain: epoch  6, batch    43 | loss: 5.3618097Losses:  4.667274475097656 0.0869172215461731
CurrentTrain: epoch  6, batch    44 | loss: 4.7541919Losses:  4.666995525360107 0.056640107184648514
CurrentTrain: epoch  6, batch    45 | loss: 4.7236357Losses:  4.667123794555664 0.09356656670570374
CurrentTrain: epoch  6, batch    46 | loss: 4.7606902Losses:  4.6397294998168945 0.08828569948673248
CurrentTrain: epoch  6, batch    47 | loss: 4.7280154Losses:  4.7822370529174805 0.10359631478786469
CurrentTrain: epoch  6, batch    48 | loss: 4.8858333Losses:  4.500380516052246 0.04561206325888634
CurrentTrain: epoch  6, batch    49 | loss: 4.5459924Losses:  4.677657127380371 0.14145085215568542
CurrentTrain: epoch  6, batch    50 | loss: 4.8191080Losses:  4.7143659591674805 0.13459134101867676
CurrentTrain: epoch  6, batch    51 | loss: 4.8489571Losses:  4.993594169616699 0.17954318225383759
CurrentTrain: epoch  6, batch    52 | loss: 5.1731372Losses:  4.499805927276611 0.03636884689331055
CurrentTrain: epoch  6, batch    53 | loss: 4.5361748Losses:  4.626989364624023 0.20358295738697052
CurrentTrain: epoch  6, batch    54 | loss: 4.8305721Losses:  4.639866352081299 0.13719214498996735
CurrentTrain: epoch  6, batch    55 | loss: 4.7770586Losses:  4.5938401222229 0.07496868818998337
CurrentTrain: epoch  6, batch    56 | loss: 4.6688089Losses:  4.547137260437012 0.05049401894211769
CurrentTrain: epoch  6, batch    57 | loss: 4.5976315Losses:  4.589230537414551 0.10563864558935165
CurrentTrain: epoch  6, batch    58 | loss: 4.6948690Losses:  4.598809242248535 0.0889236330986023
CurrentTrain: epoch  6, batch    59 | loss: 4.6877327Losses:  5.075467586517334 0.22867462038993835
CurrentTrain: epoch  6, batch    60 | loss: 5.3041420Losses:  4.740717887878418 0.10717851668596268
CurrentTrain: epoch  6, batch    61 | loss: 4.8478966Losses:  4.6348419189453125 0.12480378895998001
CurrentTrain: epoch  6, batch    62 | loss: 4.7596459Losses:  4.682821273803711 0.09718045592308044
CurrentTrain: epoch  6, batch    63 | loss: 4.7800016Losses:  4.5784993171691895 0.1275356113910675
CurrentTrain: epoch  6, batch    64 | loss: 4.7060351Losses:  4.6536078453063965 0.10303635895252228
CurrentTrain: epoch  6, batch    65 | loss: 4.7566442Losses:  4.61883544921875 0.08219771832227707
CurrentTrain: epoch  6, batch    66 | loss: 4.7010331Losses:  4.6902923583984375 0.08370691537857056
CurrentTrain: epoch  6, batch    67 | loss: 4.7739992Losses:  4.508878707885742 0.10103912651538849
CurrentTrain: epoch  6, batch    68 | loss: 4.6099176Losses:  4.593624114990234 0.12211471796035767
CurrentTrain: epoch  6, batch    69 | loss: 4.7157388Losses:  5.509702682495117 0.4661959111690521
CurrentTrain: epoch  6, batch    70 | loss: 5.9758987Losses:  4.566343307495117 0.09138286113739014
CurrentTrain: epoch  6, batch    71 | loss: 4.6577263Losses:  4.828361988067627 0.08174543082714081
CurrentTrain: epoch  6, batch    72 | loss: 4.9101076Losses:  5.279958724975586 0.1341225951910019
CurrentTrain: epoch  6, batch    73 | loss: 5.4140811Losses:  4.669267654418945 0.10684923827648163
CurrentTrain: epoch  6, batch    74 | loss: 4.7761168Losses:  4.785067558288574 0.06987161189317703
CurrentTrain: epoch  7, batch     0 | loss: 4.8549390Losses:  4.588437080383301 0.09127692133188248
CurrentTrain: epoch  7, batch     1 | loss: 4.6797142Losses:  4.875861167907715 0.05492120236158371
CurrentTrain: epoch  7, batch     2 | loss: 4.9307823Losses:  4.631338119506836 0.0423668771982193
CurrentTrain: epoch  7, batch     3 | loss: 4.6737051Losses:  4.610315322875977 0.07013222575187683
CurrentTrain: epoch  7, batch     4 | loss: 4.6804476Losses:  4.602504730224609 0.09063045680522919
CurrentTrain: epoch  7, batch     5 | loss: 4.6931353Losses:  4.732364654541016 0.16833612322807312
CurrentTrain: epoch  7, batch     6 | loss: 4.9007006Losses:  4.695888996124268 0.08968620002269745
CurrentTrain: epoch  7, batch     7 | loss: 4.7855754Losses:  5.020140171051025 0.1479700207710266
CurrentTrain: epoch  7, batch     8 | loss: 5.1681104Losses:  4.61623477935791 0.057582560926675797
CurrentTrain: epoch  7, batch     9 | loss: 4.6738172Losses:  4.634297847747803 0.12091276049613953
CurrentTrain: epoch  7, batch    10 | loss: 4.7552104Losses:  4.537747859954834 0.08207473903894424
CurrentTrain: epoch  7, batch    11 | loss: 4.6198225Losses:  4.569351673126221 0.08292054384946823
CurrentTrain: epoch  7, batch    12 | loss: 4.6522722Losses:  4.634549140930176 0.09928430616855621
CurrentTrain: epoch  7, batch    13 | loss: 4.7338333Losses:  4.543924331665039 0.054494116455316544
CurrentTrain: epoch  7, batch    14 | loss: 4.5984182Losses:  4.673736572265625 0.10683798789978027
CurrentTrain: epoch  7, batch    15 | loss: 4.7805748Losses:  4.5551323890686035 0.07845234125852585
CurrentTrain: epoch  7, batch    16 | loss: 4.6335845Losses:  4.60202693939209 0.0958993136882782
CurrentTrain: epoch  7, batch    17 | loss: 4.6979260Losses:  4.616410732269287 0.08311912417411804
CurrentTrain: epoch  7, batch    18 | loss: 4.6995296Losses:  5.086743354797363 0.2731319069862366
CurrentTrain: epoch  7, batch    19 | loss: 5.3598752Losses:  4.593471527099609 0.09141740202903748
CurrentTrain: epoch  7, batch    20 | loss: 4.6848888Losses:  4.884746074676514 0.14692258834838867
CurrentTrain: epoch  7, batch    21 | loss: 5.0316687Losses:  4.4936323165893555 0.08031372725963593
CurrentTrain: epoch  7, batch    22 | loss: 4.5739460Losses:  4.510835647583008 0.06979800760746002
CurrentTrain: epoch  7, batch    23 | loss: 4.5806336Losses:  4.974727630615234 0.13166096806526184
CurrentTrain: epoch  7, batch    24 | loss: 5.1063886Losses:  4.68304443359375 0.08767509460449219
CurrentTrain: epoch  7, batch    25 | loss: 4.7707195Losses:  5.236954212188721 0.2748975455760956
CurrentTrain: epoch  7, batch    26 | loss: 5.5118518Losses:  4.691137313842773 0.09975528717041016
CurrentTrain: epoch  7, batch    27 | loss: 4.7908926Losses:  4.56777811050415 0.07288771867752075
CurrentTrain: epoch  7, batch    28 | loss: 4.6406660Losses:  4.606115818023682 0.08538048714399338
CurrentTrain: epoch  7, batch    29 | loss: 4.6914964Losses:  4.51271915435791 0.1406884342432022
CurrentTrain: epoch  7, batch    30 | loss: 4.6534076Losses:  4.539096355438232 0.09350372850894928
CurrentTrain: epoch  7, batch    31 | loss: 4.6326003Losses:  4.595122337341309 0.04114781320095062
CurrentTrain: epoch  7, batch    32 | loss: 4.6362700Losses:  4.703395843505859 0.10962837934494019
CurrentTrain: epoch  7, batch    33 | loss: 4.8130240Losses:  4.550114631652832 0.09958569705486298
CurrentTrain: epoch  7, batch    34 | loss: 4.6497002Losses:  4.559919834136963 0.12111243605613708
CurrentTrain: epoch  7, batch    35 | loss: 4.6810322Losses:  4.550937652587891 0.10924957692623138
CurrentTrain: epoch  7, batch    36 | loss: 4.6601872Losses:  4.625600814819336 0.10784612596035004
CurrentTrain: epoch  7, batch    37 | loss: 4.7334471Losses:  4.523694038391113 0.07426358759403229
CurrentTrain: epoch  7, batch    38 | loss: 4.5979576Losses:  4.796999931335449 0.13966825604438782
CurrentTrain: epoch  7, batch    39 | loss: 4.9366684Losses:  4.581913948059082 0.026569997891783714
CurrentTrain: epoch  7, batch    40 | loss: 4.6084838Losses:  4.614856243133545 0.09664192795753479
CurrentTrain: epoch  7, batch    41 | loss: 4.7114983Losses:  4.572820663452148 0.0701860561966896
CurrentTrain: epoch  7, batch    42 | loss: 4.6430068Losses:  4.553644180297852 0.0792827308177948
CurrentTrain: epoch  7, batch    43 | loss: 4.6329269Losses:  4.569841384887695 0.09248097985982895
CurrentTrain: epoch  7, batch    44 | loss: 4.6623225Losses:  4.65333366394043 0.043210286647081375
CurrentTrain: epoch  7, batch    45 | loss: 4.6965442Losses:  4.6669020652771 0.14780211448669434
CurrentTrain: epoch  7, batch    46 | loss: 4.8147039Losses:  4.543566703796387 0.10395660996437073
CurrentTrain: epoch  7, batch    47 | loss: 4.6475234Losses:  4.495974063873291 0.07769860327243805
CurrentTrain: epoch  7, batch    48 | loss: 4.5736728Losses:  4.53404426574707 0.09619104862213135
CurrentTrain: epoch  7, batch    49 | loss: 4.6302352Losses:  4.582982540130615 0.12325189262628555
CurrentTrain: epoch  7, batch    50 | loss: 4.7062345Losses:  4.540193557739258 0.09615999460220337
CurrentTrain: epoch  7, batch    51 | loss: 4.6363535Losses:  5.419069290161133 0.2079043835401535
CurrentTrain: epoch  7, batch    52 | loss: 5.6269736Losses:  4.505419731140137 0.07143047451972961
CurrentTrain: epoch  7, batch    53 | loss: 4.5768504Losses:  4.77025032043457 0.08818969130516052
CurrentTrain: epoch  7, batch    54 | loss: 4.8584399Losses:  4.579108238220215 0.10938405990600586
CurrentTrain: epoch  7, batch    55 | loss: 4.6884923Losses:  4.5880608558654785 0.09442213177680969
CurrentTrain: epoch  7, batch    56 | loss: 4.6824832Losses:  4.647841453552246 0.06505770981311798
CurrentTrain: epoch  7, batch    57 | loss: 4.7128992Losses:  4.576983451843262 0.060626134276390076
CurrentTrain: epoch  7, batch    58 | loss: 4.6376095Losses:  4.550821304321289 0.04731482267379761
CurrentTrain: epoch  7, batch    59 | loss: 4.5981359Losses:  4.496626377105713 0.06993667781352997
CurrentTrain: epoch  7, batch    60 | loss: 4.5665631Losses:  4.676187515258789 0.0989057719707489
CurrentTrain: epoch  7, batch    61 | loss: 4.7750931Losses:  4.5328145027160645 0.08714096993207932
CurrentTrain: epoch  7, batch    62 | loss: 4.6199555Losses:  4.585728168487549 0.10915672779083252
CurrentTrain: epoch  7, batch    63 | loss: 4.6948848Losses:  4.541460037231445 0.12004219740629196
CurrentTrain: epoch  7, batch    64 | loss: 4.6615024Losses:  4.615527153015137 0.07500036060810089
CurrentTrain: epoch  7, batch    65 | loss: 4.6905274Losses:  4.627623081207275 0.10266301035881042
CurrentTrain: epoch  7, batch    66 | loss: 4.7302861Losses:  4.616344451904297 0.08174466341733932
CurrentTrain: epoch  7, batch    67 | loss: 4.6980891Losses:  4.646515846252441 0.06620589643716812
CurrentTrain: epoch  7, batch    68 | loss: 4.7127218Losses:  4.542696475982666 0.0727730542421341
CurrentTrain: epoch  7, batch    69 | loss: 4.6154695Losses:  4.5346269607543945 0.09098364412784576
CurrentTrain: epoch  7, batch    70 | loss: 4.6256108Losses:  4.483490943908691 0.09582524001598358
CurrentTrain: epoch  7, batch    71 | loss: 4.5793161Losses:  4.531671524047852 0.05266854166984558
CurrentTrain: epoch  7, batch    72 | loss: 4.5843401Losses:  4.511347770690918 0.047937050461769104
CurrentTrain: epoch  7, batch    73 | loss: 4.5592847Losses:  4.5563507080078125 0.02711459994316101
CurrentTrain: epoch  7, batch    74 | loss: 4.5834651Losses:  4.649525165557861 0.054698798805475235
CurrentTrain: epoch  8, batch     0 | loss: 4.7042241Losses:  4.57559061050415 0.09645208716392517
CurrentTrain: epoch  8, batch     1 | loss: 4.6720428Losses:  4.681317329406738 0.06025184318423271
CurrentTrain: epoch  8, batch     2 | loss: 4.7415690Losses:  4.555639266967773 0.07917668670415878
CurrentTrain: epoch  8, batch     3 | loss: 4.6348162Losses:  4.785425186157227 0.13227437436580658
CurrentTrain: epoch  8, batch     4 | loss: 4.9176993Losses:  4.551583290100098 0.08625435829162598
CurrentTrain: epoch  8, batch     5 | loss: 4.6378374Losses:  4.637701034545898 0.07856535911560059
CurrentTrain: epoch  8, batch     6 | loss: 4.7162666Losses:  4.529428482055664 0.11383886635303497
CurrentTrain: epoch  8, batch     7 | loss: 4.6432672Losses:  4.580302715301514 0.10653166472911835
CurrentTrain: epoch  8, batch     8 | loss: 4.6868343Losses:  4.526630401611328 0.026273062452673912
CurrentTrain: epoch  8, batch     9 | loss: 4.5529037Losses:  4.567431449890137 0.09183330833911896
CurrentTrain: epoch  8, batch    10 | loss: 4.6592646Losses:  4.568255424499512 0.10408736765384674
CurrentTrain: epoch  8, batch    11 | loss: 4.6723428Losses:  4.542226314544678 0.08283163607120514
CurrentTrain: epoch  8, batch    12 | loss: 4.6250582Losses:  4.589105606079102 0.0979229286313057
CurrentTrain: epoch  8, batch    13 | loss: 4.6870284Losses:  4.533580303192139 0.06663995236158371
CurrentTrain: epoch  8, batch    14 | loss: 4.6002202Losses:  4.554086208343506 0.10405098646879196
CurrentTrain: epoch  8, batch    15 | loss: 4.6581373Losses:  4.588556289672852 0.06199958547949791
CurrentTrain: epoch  8, batch    16 | loss: 4.6505561Losses:  4.553901672363281 0.0740339457988739
CurrentTrain: epoch  8, batch    17 | loss: 4.6279354Losses:  4.588406085968018 0.07019186019897461
CurrentTrain: epoch  8, batch    18 | loss: 4.6585979Losses:  4.67613410949707 0.08068832010030746
CurrentTrain: epoch  8, batch    19 | loss: 4.7568226Losses:  4.553092956542969 0.09467228502035141
CurrentTrain: epoch  8, batch    20 | loss: 4.6477652Losses:  4.517460823059082 0.09501273930072784
CurrentTrain: epoch  8, batch    21 | loss: 4.6124735Losses:  4.486996650695801 0.08539475500583649
CurrentTrain: epoch  8, batch    22 | loss: 4.5723915Losses:  4.525787830352783 0.07540302723646164
CurrentTrain: epoch  8, batch    23 | loss: 4.6011910Losses:  4.493459701538086 0.07007801532745361
CurrentTrain: epoch  8, batch    24 | loss: 4.5635376Losses:  4.513226509094238 0.08710770308971405
CurrentTrain: epoch  8, batch    25 | loss: 4.6003342Losses:  4.506715774536133 0.07013989984989166
CurrentTrain: epoch  8, batch    26 | loss: 4.5768557Losses:  4.473574638366699 0.06832313537597656
CurrentTrain: epoch  8, batch    27 | loss: 4.5418978Losses:  4.558640956878662 0.07020755112171173
CurrentTrain: epoch  8, batch    28 | loss: 4.6288486Losses:  4.509823799133301 0.08143317699432373
CurrentTrain: epoch  8, batch    29 | loss: 4.5912571Losses:  4.503993988037109 0.0855044275522232
CurrentTrain: epoch  8, batch    30 | loss: 4.5894985Losses:  4.568169593811035 0.08625555038452148
CurrentTrain: epoch  8, batch    31 | loss: 4.6544251Losses:  4.518648147583008 0.05742567032575607
CurrentTrain: epoch  8, batch    32 | loss: 4.5760736Losses:  4.5394182205200195 0.09609077870845795
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.211234092712402 1.7084617614746094
CurrentTrain: epoch  0, batch     0 | loss: 15.6281576Losses:  14.590405464172363 1.6869049072265625
CurrentTrain: epoch  0, batch     1 | loss: 17.9642143Losses:  15.123201370239258 1.635493278503418
CurrentTrain: epoch  0, batch     2 | loss: 18.3941879Losses:  15.048903465270996 1.8035235404968262
CurrentTrain: epoch  0, batch     3 | loss: 18.6559505Losses:  14.904525756835938 1.829665184020996
CurrentTrain: epoch  0, batch     4 | loss: 18.5638561Losses:  14.553403854370117 1.6987481117248535
CurrentTrain: epoch  0, batch     5 | loss: 17.9509010Losses:  14.246960639953613 1.3758904933929443
CurrentTrain: epoch  0, batch     6 | loss: 16.9987411Losses:  14.348455429077148 1.4979770183563232
CurrentTrain: epoch  0, batch     7 | loss: 17.3444099Losses:  14.091314315795898 1.5140293836593628
CurrentTrain: epoch  0, batch     8 | loss: 17.1193733Losses:  13.885848045349121 1.3077623844146729
CurrentTrain: epoch  0, batch     9 | loss: 16.5013733Losses:  13.478333473205566 1.4326756000518799
CurrentTrain: epoch  0, batch    10 | loss: 16.3436852Losses:  13.201543807983398 1.3012471199035645
CurrentTrain: epoch  0, batch    11 | loss: 15.8040380Losses:  12.787351608276367 1.3456456661224365
CurrentTrain: epoch  0, batch    12 | loss: 15.4786434Losses:  12.978690147399902 1.7332110404968262
CurrentTrain: epoch  0, batch    13 | loss: 16.4451122Losses:  12.495721817016602 1.6404314041137695
CurrentTrain: epoch  0, batch    14 | loss: 15.7765846Losses:  12.8441162109375 1.497147798538208
CurrentTrain: epoch  0, batch    15 | loss: 15.8384113Losses:  12.533636093139648 1.555801272392273
CurrentTrain: epoch  0, batch    16 | loss: 15.6452389Losses:  12.14575481414795 1.4309892654418945
CurrentTrain: epoch  0, batch    17 | loss: 15.0077333Losses:  12.256380081176758 1.426823377609253
CurrentTrain: epoch  0, batch    18 | loss: 15.1100273Losses:  11.762889862060547 1.2611186504364014
CurrentTrain: epoch  0, batch    19 | loss: 14.2851276Losses:  11.832528114318848 1.450270175933838
CurrentTrain: epoch  0, batch    20 | loss: 14.7330685Losses:  11.533447265625 1.314969778060913
CurrentTrain: epoch  0, batch    21 | loss: 14.1633873Losses:  10.868295669555664 1.164790391921997
CurrentTrain: epoch  0, batch    22 | loss: 13.1978760Losses:  11.094350814819336 1.3798463344573975
CurrentTrain: epoch  0, batch    23 | loss: 13.8540440Losses:  10.837430953979492 1.216853141784668
CurrentTrain: epoch  0, batch    24 | loss: 13.2711372Losses:  10.408609390258789 1.239661693572998
CurrentTrain: epoch  0, batch    25 | loss: 12.8879328Losses:  10.164285659790039 1.248490333557129
CurrentTrain: epoch  0, batch    26 | loss: 12.6612663Losses:  10.015030860900879 1.36817467212677
CurrentTrain: epoch  0, batch    27 | loss: 12.7513800Losses:  9.797012329101562 1.2468359470367432
CurrentTrain: epoch  0, batch    28 | loss: 12.2906837Losses:  9.51292610168457 1.041691541671753
CurrentTrain: epoch  0, batch    29 | loss: 11.5963097Losses:  9.611381530761719 1.304675817489624
CurrentTrain: epoch  0, batch    30 | loss: 12.2207336Losses:  9.240623474121094 1.1933726072311401
CurrentTrain: epoch  0, batch    31 | loss: 11.6273689Losses:  8.91335678100586 1.154226303100586
CurrentTrain: epoch  0, batch    32 | loss: 11.2218094Losses:  9.194211959838867 1.3870117664337158
CurrentTrain: epoch  0, batch    33 | loss: 11.9682350Losses:  8.548370361328125 0.9578831195831299
CurrentTrain: epoch  0, batch    34 | loss: 10.4641361Losses:  8.12706184387207 1.0678746700286865
CurrentTrain: epoch  0, batch    35 | loss: 10.2628117Losses:  8.025906562805176 1.074544906616211
CurrentTrain: epoch  0, batch    36 | loss: 10.1749964Losses:  7.863584041595459 0.6487324237823486
CurrentTrain: epoch  0, batch    37 | loss: 9.1610489Losses:  7.860480308532715 1.275437355041504
CurrentTrain: epoch  1, batch     0 | loss: 10.4113550Losses:  7.7391252517700195 0.9362732172012329
CurrentTrain: epoch  1, batch     1 | loss: 9.6116714Losses:  7.827878952026367 0.9642068147659302
CurrentTrain: epoch  1, batch     2 | loss: 9.7562923Losses:  7.695582389831543 1.0787944793701172
CurrentTrain: epoch  1, batch     3 | loss: 9.8531713Losses:  7.672398567199707 1.0661085844039917
CurrentTrain: epoch  1, batch     4 | loss: 9.8046160Losses:  7.570900917053223 1.0977898836135864
CurrentTrain: epoch  1, batch     5 | loss: 9.7664804Losses:  7.375711441040039 0.9896603226661682
CurrentTrain: epoch  1, batch     6 | loss: 9.3550320Losses:  7.658570289611816 0.9679425358772278
CurrentTrain: epoch  1, batch     7 | loss: 9.5944557Losses:  7.791335105895996 1.0567231178283691
CurrentTrain: epoch  1, batch     8 | loss: 9.9047813Losses:  7.537327766418457 0.9896286725997925
CurrentTrain: epoch  1, batch     9 | loss: 9.5165854Losses:  7.417506217956543 0.9730116128921509
CurrentTrain: epoch  1, batch    10 | loss: 9.3635292Losses:  7.616722106933594 0.8320401906967163
CurrentTrain: epoch  1, batch    11 | loss: 9.2808027Losses:  7.237014293670654 0.9040889739990234
CurrentTrain: epoch  1, batch    12 | loss: 9.0451927Losses:  7.750865936279297 1.0535005331039429
CurrentTrain: epoch  1, batch    13 | loss: 9.8578672Losses:  7.240640640258789 0.913689911365509
CurrentTrain: epoch  1, batch    14 | loss: 9.0680208Losses:  7.396017074584961 0.8527387976646423
CurrentTrain: epoch  1, batch    15 | loss: 9.1014948Losses:  7.1962432861328125 0.898192286491394
CurrentTrain: epoch  1, batch    16 | loss: 8.9926281Losses:  6.999520301818848 0.784110963344574
CurrentTrain: epoch  1, batch    17 | loss: 8.5677423Losses:  7.10562801361084 0.9066076278686523
CurrentTrain: epoch  1, batch    18 | loss: 8.9188433Losses:  7.417612075805664 1.151785135269165
CurrentTrain: epoch  1, batch    19 | loss: 9.7211819Losses:  6.774362564086914 0.6301441192626953
CurrentTrain: epoch  1, batch    20 | loss: 8.0346508Losses:  7.069716453552246 0.630650520324707
CurrentTrain: epoch  1, batch    21 | loss: 8.3310175Losses:  7.133741855621338 0.7202831506729126
CurrentTrain: epoch  1, batch    22 | loss: 8.5743084Losses:  7.532308578491211 0.8336362242698669
CurrentTrain: epoch  1, batch    23 | loss: 9.1995811Losses:  7.4228901863098145 0.7362117767333984
CurrentTrain: epoch  1, batch    24 | loss: 8.8953133Losses:  7.473757743835449 0.8253737092018127
CurrentTrain: epoch  1, batch    25 | loss: 9.1245050Losses:  6.926028251647949 0.6368599534034729
CurrentTrain: epoch  1, batch    26 | loss: 8.1997480Losses:  7.086969375610352 1.034348964691162
CurrentTrain: epoch  1, batch    27 | loss: 9.1556673Losses:  6.894691467285156 0.7371262907981873
CurrentTrain: epoch  1, batch    28 | loss: 8.3689442Losses:  6.725159645080566 0.5355240106582642
CurrentTrain: epoch  1, batch    29 | loss: 7.7962074Losses:  7.132471084594727 0.663038969039917
CurrentTrain: epoch  1, batch    30 | loss: 8.4585495Losses:  6.966468811035156 0.7957364916801453
CurrentTrain: epoch  1, batch    31 | loss: 8.5579414Losses:  6.611806392669678 0.6670607328414917
CurrentTrain: epoch  1, batch    32 | loss: 7.9459276Losses:  6.612133979797363 0.6223118305206299
CurrentTrain: epoch  1, batch    33 | loss: 7.8567576Losses:  6.916256904602051 0.756959080696106
CurrentTrain: epoch  1, batch    34 | loss: 8.4301748Losses:  7.078645706176758 0.7817932367324829
CurrentTrain: epoch  1, batch    35 | loss: 8.6422319Losses:  7.026623725891113 0.7365887761116028
CurrentTrain: epoch  1, batch    36 | loss: 8.4998016Losses:  6.739083766937256 0.20618607103824615
CurrentTrain: epoch  1, batch    37 | loss: 7.1514559Losses:  6.763641834259033 0.6536815166473389
CurrentTrain: epoch  2, batch     0 | loss: 8.0710049Losses:  6.655898571014404 0.5685274600982666
CurrentTrain: epoch  2, batch     1 | loss: 7.7929535Losses:  6.10695219039917 0.5148762464523315
CurrentTrain: epoch  2, batch     2 | loss: 7.1367044Losses:  6.186583518981934 0.5369405150413513
CurrentTrain: epoch  2, batch     3 | loss: 7.2604647Losses:  7.257628440856934 0.4726610779762268
CurrentTrain: epoch  2, batch     4 | loss: 8.2029505Losses:  6.427973747253418 0.6622793674468994
CurrentTrain: epoch  2, batch     5 | loss: 7.7525325Losses:  6.511045932769775 0.6131995320320129
CurrentTrain: epoch  2, batch     6 | loss: 7.7374449Losses:  6.270449161529541 0.5729343891143799
CurrentTrain: epoch  2, batch     7 | loss: 7.4163179Losses:  6.706655979156494 0.7673287391662598
CurrentTrain: epoch  2, batch     8 | loss: 8.2413139Losses:  6.546754360198975 0.4262179434299469
CurrentTrain: epoch  2, batch     9 | loss: 7.3991904Losses:  6.475062370300293 0.41139286756515503
CurrentTrain: epoch  2, batch    10 | loss: 7.2978482Losses:  6.509321212768555 0.4953157901763916
CurrentTrain: epoch  2, batch    11 | loss: 7.4999528Losses:  6.053991317749023 0.38283973932266235
CurrentTrain: epoch  2, batch    12 | loss: 6.8196707Losses:  6.468142509460449 0.5806758999824524
CurrentTrain: epoch  2, batch    13 | loss: 7.6294942Losses:  7.107734680175781 0.6875520944595337
CurrentTrain: epoch  2, batch    14 | loss: 8.4828386Losses:  6.693836212158203 0.5394572615623474
CurrentTrain: epoch  2, batch    15 | loss: 7.7727509Losses:  6.698583126068115 0.502028226852417
CurrentTrain: epoch  2, batch    16 | loss: 7.7026396Losses:  6.264935493469238 0.568628191947937
CurrentTrain: epoch  2, batch    17 | loss: 7.4021921Losses:  6.199659824371338 0.4451242983341217
CurrentTrain: epoch  2, batch    18 | loss: 7.0899086Losses:  6.344558238983154 0.4194144606590271
CurrentTrain: epoch  2, batch    19 | loss: 7.1833873Losses:  5.849307060241699 0.33733752369880676
CurrentTrain: epoch  2, batch    20 | loss: 6.5239820Losses:  6.354839324951172 0.5898507237434387
CurrentTrain: epoch  2, batch    21 | loss: 7.5345407Losses:  6.575338363647461 0.6813335418701172
CurrentTrain: epoch  2, batch    22 | loss: 7.9380054Losses:  6.633923530578613 0.5431911945343018
CurrentTrain: epoch  2, batch    23 | loss: 7.7203059Losses:  6.451902389526367 0.5519793033599854
CurrentTrain: epoch  2, batch    24 | loss: 7.5558610Losses:  6.496133804321289 0.5426738262176514
CurrentTrain: epoch  2, batch    25 | loss: 7.5814815Losses:  7.0403361320495605 0.6269203424453735
CurrentTrain: epoch  2, batch    26 | loss: 8.2941771Losses:  6.26837158203125 0.48016074299812317
CurrentTrain: epoch  2, batch    27 | loss: 7.2286930Losses:  7.002192974090576 0.6896915435791016
CurrentTrain: epoch  2, batch    28 | loss: 8.3815765Losses:  6.33122444152832 0.5182701349258423
CurrentTrain: epoch  2, batch    29 | loss: 7.3677645Losses:  6.2204413414001465 0.4309384822845459
CurrentTrain: epoch  2, batch    30 | loss: 7.0823183Losses:  6.737101078033447 0.4991578459739685
CurrentTrain: epoch  2, batch    31 | loss: 7.7354169Losses:  6.469945907592773 0.43641501665115356
CurrentTrain: epoch  2, batch    32 | loss: 7.3427758Losses:  6.121305465698242 0.4653934836387634
CurrentTrain: epoch  2, batch    33 | loss: 7.0520926Losses:  6.495879173278809 0.49426233768463135
CurrentTrain: epoch  2, batch    34 | loss: 7.4844036Losses:  6.6590728759765625 0.4882943630218506
CurrentTrain: epoch  2, batch    35 | loss: 7.6356616Losses:  6.133545875549316 0.4667251706123352
CurrentTrain: epoch  2, batch    36 | loss: 7.0669961Losses:  6.134523391723633 0.46464216709136963
CurrentTrain: epoch  2, batch    37 | loss: 7.0638075Losses:  5.94710636138916 0.4107997417449951
CurrentTrain: epoch  3, batch     0 | loss: 6.7687058Losses:  5.980441093444824 0.3677799105644226
CurrentTrain: epoch  3, batch     1 | loss: 6.7160010Losses:  6.240583419799805 0.4476049244403839
CurrentTrain: epoch  3, batch     2 | loss: 7.1357932Losses:  6.188763618469238 0.4886259436607361
CurrentTrain: epoch  3, batch     3 | loss: 7.1660156Losses:  5.855801582336426 0.3143216371536255
CurrentTrain: epoch  3, batch     4 | loss: 6.4844446Losses:  6.294183731079102 0.5008139610290527
CurrentTrain: epoch  3, batch     5 | loss: 7.2958117Losses:  5.719735622406006 0.4386998116970062
CurrentTrain: epoch  3, batch     6 | loss: 6.5971351Losses:  5.989468574523926 0.40918320417404175
CurrentTrain: epoch  3, batch     7 | loss: 6.8078351Losses:  5.866325855255127 0.3931289315223694
CurrentTrain: epoch  3, batch     8 | loss: 6.6525836Losses:  5.84158992767334 0.3203276991844177
CurrentTrain: epoch  3, batch     9 | loss: 6.4822454Losses:  5.7714524269104 0.4197421669960022
CurrentTrain: epoch  3, batch    10 | loss: 6.6109366Losses:  5.990849494934082 0.33243218064308167
CurrentTrain: epoch  3, batch    11 | loss: 6.6557140Losses:  5.939760684967041 0.45936334133148193
CurrentTrain: epoch  3, batch    12 | loss: 6.8584871Losses:  5.917273044586182 0.38395556807518005
CurrentTrain: epoch  3, batch    13 | loss: 6.6851840Losses:  5.668092727661133 0.3375175893306732
CurrentTrain: epoch  3, batch    14 | loss: 6.3431277Losses:  6.221132755279541 0.4214423894882202
CurrentTrain: epoch  3, batch    15 | loss: 7.0640173Losses:  5.634846210479736 0.3021407723426819
CurrentTrain: epoch  3, batch    16 | loss: 6.2391276Losses:  6.2557597160339355 0.45121949911117554
CurrentTrain: epoch  3, batch    17 | loss: 7.1581988Losses:  6.045289993286133 0.3374926447868347
CurrentTrain: epoch  3, batch    18 | loss: 6.7202754Losses:  5.893269062042236 0.46941328048706055
CurrentTrain: epoch  3, batch    19 | loss: 6.8320956Losses:  5.588645935058594 0.33450448513031006
CurrentTrain: epoch  3, batch    20 | loss: 6.2576551Losses:  6.107129096984863 0.2881022095680237
CurrentTrain: epoch  3, batch    21 | loss: 6.6833334Losses:  6.055597305297852 0.4955148994922638
CurrentTrain: epoch  3, batch    22 | loss: 7.0466270Losses:  5.796962261199951 0.2884926199913025
CurrentTrain: epoch  3, batch    23 | loss: 6.3739476Losses:  5.705976963043213 0.24832221865653992
CurrentTrain: epoch  3, batch    24 | loss: 6.2026215Losses:  5.950539588928223 0.32927531003952026
CurrentTrain: epoch  3, batch    25 | loss: 6.6090903Losses:  6.047939300537109 0.4651111364364624
CurrentTrain: epoch  3, batch    26 | loss: 6.9781618Losses:  5.623054504394531 0.4075550436973572
CurrentTrain: epoch  3, batch    27 | loss: 6.4381647Losses:  6.074487686157227 0.41905781626701355
CurrentTrain: epoch  3, batch    28 | loss: 6.9126034Losses:  6.201038360595703 0.5024826526641846
CurrentTrain: epoch  3, batch    29 | loss: 7.2060037Losses:  6.304722785949707 0.31225866079330444
CurrentTrain: epoch  3, batch    30 | loss: 6.9292402Losses:  6.777945518493652 0.4023401141166687
CurrentTrain: epoch  3, batch    31 | loss: 7.5826259Losses:  6.065536975860596 0.37936538457870483
CurrentTrain: epoch  3, batch    32 | loss: 6.8242679Losses:  6.259455680847168 0.38893449306488037
CurrentTrain: epoch  3, batch    33 | loss: 7.0373249Losses:  6.18377161026001 0.29902976751327515
CurrentTrain: epoch  3, batch    34 | loss: 6.7818313Losses:  5.6285295486450195 0.259419709444046
CurrentTrain: epoch  3, batch    35 | loss: 6.1473689Losses:  5.956878185272217 0.46375519037246704
CurrentTrain: epoch  3, batch    36 | loss: 6.8843884Losses:  5.007541656494141 0.1386195719242096
CurrentTrain: epoch  3, batch    37 | loss: 5.2847810Losses:  5.541889667510986 0.31303492188453674
CurrentTrain: epoch  4, batch     0 | loss: 6.1679597Losses:  6.493824005126953 0.5357988476753235
CurrentTrain: epoch  4, batch     1 | loss: 7.5654216Losses:  5.805650234222412 0.2843843698501587
CurrentTrain: epoch  4, batch     2 | loss: 6.3744192Losses:  6.143041133880615 0.41463959217071533
CurrentTrain: epoch  4, batch     3 | loss: 6.9723206Losses:  5.8046183586120605 0.2477291077375412
CurrentTrain: epoch  4, batch     4 | loss: 6.3000765Losses:  5.416284561157227 0.23990486562252045
CurrentTrain: epoch  4, batch     5 | loss: 5.8960943Losses:  5.509523391723633 0.3700420558452606
CurrentTrain: epoch  4, batch     6 | loss: 6.2496076Losses:  5.277015686035156 0.24934843182563782
CurrentTrain: epoch  4, batch     7 | loss: 5.7757125Losses:  5.609899520874023 0.2622224688529968
CurrentTrain: epoch  4, batch     8 | loss: 6.1343446Losses:  5.381107330322266 0.24747061729431152
CurrentTrain: epoch  4, batch     9 | loss: 5.8760486Losses:  5.886064052581787 0.3332230746746063
CurrentTrain: epoch  4, batch    10 | loss: 6.5525103Losses:  5.278877258300781 0.2455858737230301
CurrentTrain: epoch  4, batch    11 | loss: 5.7700491Losses:  5.511357307434082 0.22740046679973602
CurrentTrain: epoch  4, batch    12 | loss: 5.9661584Losses:  5.902894973754883 0.2629430294036865
CurrentTrain: epoch  4, batch    13 | loss: 6.4287810Losses:  5.494095325469971 0.2467615306377411
CurrentTrain: epoch  4, batch    14 | loss: 5.9876184Losses:  5.940912246704102 0.44875335693359375
CurrentTrain: epoch  4, batch    15 | loss: 6.8384190Losses:  5.666195392608643 0.33891409635543823
CurrentTrain: epoch  4, batch    16 | loss: 6.3440237Losses:  5.511198043823242 0.2221474051475525
CurrentTrain: epoch  4, batch    17 | loss: 5.9554930Losses:  5.438390731811523 0.21881666779518127
CurrentTrain: epoch  4, batch    18 | loss: 5.8760242Losses:  5.299452304840088 0.20615515112876892
CurrentTrain: epoch  4, batch    19 | loss: 5.7117624Losses:  5.649513244628906 0.3051588535308838
CurrentTrain: epoch  4, batch    20 | loss: 6.2598310Losses:  5.498830795288086 0.19488169252872467
CurrentTrain: epoch  4, batch    21 | loss: 5.8885942Losses:  6.065166473388672 0.44293832778930664
CurrentTrain: epoch  4, batch    22 | loss: 6.9510431Losses:  5.408687591552734 0.28384485840797424
CurrentTrain: epoch  4, batch    23 | loss: 5.9763775Losses:  5.766580581665039 0.246464803814888
CurrentTrain: epoch  4, batch    24 | loss: 6.2595100Losses:  5.499266624450684 0.18522018194198608
CurrentTrain: epoch  4, batch    25 | loss: 5.8697071Losses:  6.215850830078125 0.4915997385978699
CurrentTrain: epoch  4, batch    26 | loss: 7.1990504Losses:  5.535138130187988 0.3129134774208069
CurrentTrain: epoch  4, batch    27 | loss: 6.1609650Losses:  5.277280807495117 0.2287198156118393
CurrentTrain: epoch  4, batch    28 | loss: 5.7347202Losses:  5.001803398132324 0.19539275765419006
CurrentTrain: epoch  4, batch    29 | loss: 5.3925891Losses:  6.241280555725098 0.26724007725715637
CurrentTrain: epoch  4, batch    30 | loss: 6.7757607Losses:  5.359238147735596 0.22682824730873108
CurrentTrain: epoch  4, batch    31 | loss: 5.8128948Losses:  5.668661117553711 0.3072611093521118
CurrentTrain: epoch  4, batch    32 | loss: 6.2831831Losses:  5.261165142059326 0.3195146322250366
CurrentTrain: epoch  4, batch    33 | loss: 5.9001942Losses:  5.936758041381836 0.38969534635543823
CurrentTrain: epoch  4, batch    34 | loss: 6.7161489Losses:  6.231619834899902 0.47084400057792664
CurrentTrain: epoch  4, batch    35 | loss: 7.1733079Losses:  5.176602840423584 0.1848955750465393
CurrentTrain: epoch  4, batch    36 | loss: 5.5463939Losses:  5.164793968200684 0.22177593410015106
CurrentTrain: epoch  4, batch    37 | loss: 5.6083460Losses:  5.038305759429932 0.1931079924106598
CurrentTrain: epoch  5, batch     0 | loss: 5.4245219Losses:  5.345816135406494 0.13864721357822418
CurrentTrain: epoch  5, batch     1 | loss: 5.6231108Losses:  5.253810882568359 0.19757704436779022
CurrentTrain: epoch  5, batch     2 | loss: 5.6489649Losses:  5.748076438903809 0.28440991044044495
CurrentTrain: epoch  5, batch     3 | loss: 6.3168964Losses:  5.522172927856445 0.20479938387870789
CurrentTrain: epoch  5, batch     4 | loss: 5.9317718Losses:  5.819791316986084 0.3094731867313385
CurrentTrain: epoch  5, batch     5 | loss: 6.4387379Losses:  5.324010848999023 0.23115228116512299
CurrentTrain: epoch  5, batch     6 | loss: 5.7863154Losses:  5.078095436096191 0.18494035303592682
CurrentTrain: epoch  5, batch     7 | loss: 5.4479761Losses:  5.380487442016602 0.261182576417923
CurrentTrain: epoch  5, batch     8 | loss: 5.9028525Losses:  5.102996826171875 0.09382469952106476
CurrentTrain: epoch  5, batch     9 | loss: 5.2906461Losses:  5.275636672973633 0.2439056932926178
CurrentTrain: epoch  5, batch    10 | loss: 5.7634482Losses:  5.388718605041504 0.23225052654743195
CurrentTrain: epoch  5, batch    11 | loss: 5.8532195Losses:  5.5238800048828125 0.31932252645492554
CurrentTrain: epoch  5, batch    12 | loss: 6.1625252Losses:  5.424495697021484 0.29002508521080017
CurrentTrain: epoch  5, batch    13 | loss: 6.0045457Losses:  5.122930526733398 0.17768794298171997
CurrentTrain: epoch  5, batch    14 | loss: 5.4783063Losses:  5.367440223693848 0.17227891087532043
CurrentTrain: epoch  5, batch    15 | loss: 5.7119980Losses:  5.429513454437256 0.15775291621685028
CurrentTrain: epoch  5, batch    16 | loss: 5.7450194Losses:  5.216026306152344 0.20363029837608337
CurrentTrain: epoch  5, batch    17 | loss: 5.6232867Losses:  5.143821716308594 0.14441044628620148
CurrentTrain: epoch  5, batch    18 | loss: 5.4326425Losses:  5.019381523132324 0.19600893557071686
CurrentTrain: epoch  5, batch    19 | loss: 5.4113994Losses:  5.493271827697754 0.20851978659629822
CurrentTrain: epoch  5, batch    20 | loss: 5.9103112Losses:  5.441887378692627 0.2456894814968109
CurrentTrain: epoch  5, batch    21 | loss: 5.9332662Losses:  4.988223075866699 0.1902930736541748
CurrentTrain: epoch  5, batch    22 | loss: 5.3688092Losses:  5.337042808532715 0.22310885787010193
CurrentTrain: epoch  5, batch    23 | loss: 5.7832603Losses:  5.561905860900879 0.2827168107032776
CurrentTrain: epoch  5, batch    24 | loss: 6.1273394Losses:  5.720736503601074 0.3661620020866394
CurrentTrain: epoch  5, batch    25 | loss: 6.4530606Losses:  5.024571895599365 0.15565451979637146
CurrentTrain: epoch  5, batch    26 | loss: 5.3358808Losses:  5.135227203369141 0.17319390177726746
CurrentTrain: epoch  5, batch    27 | loss: 5.4816151Losses:  5.953544616699219 0.4000140428543091
CurrentTrain: epoch  5, batch    28 | loss: 6.7535725Losses:  5.140157222747803 0.217241108417511
CurrentTrain: epoch  5, batch    29 | loss: 5.5746393Losses:  5.336212158203125 0.21556702256202698
CurrentTrain: epoch  5, batch    30 | loss: 5.7673464Losses:  5.6200714111328125 0.36705324053764343
CurrentTrain: epoch  5, batch    31 | loss: 6.3541780Losses:  4.78834342956543 0.16284486651420593
CurrentTrain: epoch  5, batch    32 | loss: 5.1140332Losses:  5.14389705657959 0.16526854038238525
CurrentTrain: epoch  5, batch    33 | loss: 5.4744339Losses:  5.228889465332031 0.19425195455551147
CurrentTrain: epoch  5, batch    34 | loss: 5.6173935Losses:  5.132022857666016 0.17988066375255585
CurrentTrain: epoch  5, batch    35 | loss: 5.4917841Losses:  4.901582717895508 0.11722145229578018
CurrentTrain: epoch  5, batch    36 | loss: 5.1360254Losses:  5.566827774047852 0.1619407683610916
CurrentTrain: epoch  5, batch    37 | loss: 5.8907094Losses:  5.307095050811768 0.18085849285125732
CurrentTrain: epoch  6, batch     0 | loss: 5.6688118Losses:  5.015290260314941 0.20598386228084564
CurrentTrain: epoch  6, batch     1 | loss: 5.4272580Losses:  5.248697280883789 0.2428751140832901
CurrentTrain: epoch  6, batch     2 | loss: 5.7344475Losses:  5.047758102416992 0.17436030507087708
CurrentTrain: epoch  6, batch     3 | loss: 5.3964787Losses:  5.089552402496338 0.1673356592655182
CurrentTrain: epoch  6, batch     4 | loss: 5.4242239Losses:  5.075647830963135 0.1765117198228836
CurrentTrain: epoch  6, batch     5 | loss: 5.4286714Losses:  5.176657676696777 0.21348100900650024
CurrentTrain: epoch  6, batch     6 | loss: 5.6036196Losses:  4.996614933013916 0.15339258313179016
CurrentTrain: epoch  6, batch     7 | loss: 5.3034000Losses:  5.433671951293945 0.22555020451545715
CurrentTrain: epoch  6, batch     8 | loss: 5.8847723Losses:  4.9922261238098145 0.2376420944929123
CurrentTrain: epoch  6, batch     9 | loss: 5.4675102Losses:  5.205998420715332 0.20728284120559692
CurrentTrain: epoch  6, batch    10 | loss: 5.6205640Losses:  4.909764766693115 0.14097225666046143
CurrentTrain: epoch  6, batch    11 | loss: 5.1917095Losses:  4.900737762451172 0.14618942141532898
CurrentTrain: epoch  6, batch    12 | loss: 5.1931167Losses:  4.913783073425293 0.13466782867908478
CurrentTrain: epoch  6, batch    13 | loss: 5.1831188Losses:  5.186213970184326 0.18068033456802368
CurrentTrain: epoch  6, batch    14 | loss: 5.5475745Losses:  5.023565292358398 0.09318824112415314
CurrentTrain: epoch  6, batch    15 | loss: 5.2099419Losses:  4.865015029907227 0.11121340095996857
CurrentTrain: epoch  6, batch    16 | loss: 5.0874419Losses:  4.938492774963379 0.1315590888261795
CurrentTrain: epoch  6, batch    17 | loss: 5.2016110Losses:  5.097394943237305 0.12117773294448853
CurrentTrain: epoch  6, batch    18 | loss: 5.3397503Losses:  5.382213115692139 0.25122299790382385
CurrentTrain: epoch  6, batch    19 | loss: 5.8846593Losses:  5.114072322845459 0.18980325758457184
CurrentTrain: epoch  6, batch    20 | loss: 5.4936790Losses:  5.022770881652832 0.12784527242183685
CurrentTrain: epoch  6, batch    21 | loss: 5.2784615Losses:  5.098496913909912 0.2215152084827423
CurrentTrain: epoch  6, batch    22 | loss: 5.5415273Losses:  4.970132827758789 0.11840380728244781
CurrentTrain: epoch  6, batch    23 | loss: 5.2069407Losses:  5.496746063232422 0.3722255825996399
CurrentTrain: epoch  6, batch    24 | loss: 6.2411971Losses:  4.8444600105285645 0.13379749655723572
CurrentTrain: epoch  6, batch    25 | loss: 5.1120548Losses:  5.113825798034668 0.3092028796672821
CurrentTrain: epoch  6, batch    26 | loss: 5.7322316Losses:  4.7854156494140625 0.08695103228092194
CurrentTrain: epoch  6, batch    27 | loss: 4.9593177Losses:  5.109531402587891 0.13548643887043
CurrentTrain: epoch  6, batch    28 | loss: 5.3805041Losses:  4.959368705749512 0.17512547969818115
CurrentTrain: epoch  6, batch    29 | loss: 5.3096199Losses:  4.786030292510986 0.12123335152864456
CurrentTrain: epoch  6, batch    30 | loss: 5.0284972Losses:  5.249611854553223 0.225681334733963
CurrentTrain: epoch  6, batch    31 | loss: 5.7009745Losses:  4.930911064147949 0.19893378019332886
CurrentTrain: epoch  6, batch    32 | loss: 5.3287787Losses:  5.314529895782471 0.3294101357460022
CurrentTrain: epoch  6, batch    33 | loss: 5.9733500Losses:  5.188536643981934 0.29260963201522827
CurrentTrain: epoch  6, batch    34 | loss: 5.7737560Losses:  4.64132022857666 0.09243413805961609
CurrentTrain: epoch  6, batch    35 | loss: 4.8261886Losses:  5.163200378417969 0.15686580538749695
CurrentTrain: epoch  6, batch    36 | loss: 5.4769320Losses:  5.291919708251953 0.24064671993255615
CurrentTrain: epoch  6, batch    37 | loss: 5.7732134Losses:  4.735610485076904 0.1164923682808876
CurrentTrain: epoch  7, batch     0 | loss: 4.9685950Losses:  5.070425033569336 0.14602622389793396
CurrentTrain: epoch  7, batch     1 | loss: 5.3624773Losses:  5.031014442443848 0.1952795535326004
CurrentTrain: epoch  7, batch     2 | loss: 5.4215736Losses:  4.690609455108643 0.12917840480804443
CurrentTrain: epoch  7, batch     3 | loss: 4.9489660Losses:  4.947051525115967 0.24128520488739014
CurrentTrain: epoch  7, batch     4 | loss: 5.4296217Losses:  4.752127170562744 0.10741202533245087
CurrentTrain: epoch  7, batch     5 | loss: 4.9669514Losses:  5.0446319580078125 0.1926945149898529
CurrentTrain: epoch  7, batch     6 | loss: 5.4300208Losses:  4.700638294219971 0.11357634514570236
CurrentTrain: epoch  7, batch     7 | loss: 4.9277911Losses:  4.826596260070801 0.12531733512878418
CurrentTrain: epoch  7, batch     8 | loss: 5.0772309Losses:  4.726183891296387 0.11879511922597885
CurrentTrain: epoch  7, batch     9 | loss: 4.9637742Losses:  4.828437328338623 0.18427477777004242
CurrentTrain: epoch  7, batch    10 | loss: 5.1969867Losses:  4.827886581420898 0.16692140698432922
CurrentTrain: epoch  7, batch    11 | loss: 5.1617293Losses:  4.669797897338867 0.12959793210029602
CurrentTrain: epoch  7, batch    12 | loss: 4.9289937Losses:  4.687162399291992 0.1147957593202591
CurrentTrain: epoch  7, batch    13 | loss: 4.9167538Losses:  4.852884292602539 0.15261830389499664
CurrentTrain: epoch  7, batch    14 | loss: 5.1581211Losses:  4.675670623779297 0.1185176819562912
CurrentTrain: epoch  7, batch    15 | loss: 4.9127059Losses:  4.676146507263184 0.10714893043041229
CurrentTrain: epoch  7, batch    16 | loss: 4.8904443Losses:  4.637945175170898 0.11629626899957657
CurrentTrain: epoch  7, batch    17 | loss: 4.8705378Losses:  4.706540107727051 0.08898085355758667
CurrentTrain: epoch  7, batch    18 | loss: 4.8845019Losses:  4.768070220947266 0.13232311606407166
CurrentTrain: epoch  7, batch    19 | loss: 5.0327163Losses:  4.68963623046875 0.10201697051525116
CurrentTrain: epoch  7, batch    20 | loss: 4.8936701Losses:  4.678074836730957 0.1142607182264328
CurrentTrain: epoch  7, batch    21 | loss: 4.9065962Losses:  4.749301910400391 0.0954631119966507
CurrentTrain: epoch  7, batch    22 | loss: 4.9402280Losses:  4.911006927490234 0.2073434293270111
CurrentTrain: epoch  7, batch    23 | loss: 5.3256936Losses:  4.788798809051514 0.12319955974817276
CurrentTrain: epoch  7, batch    24 | loss: 5.0351977Losses:  4.9218058586120605 0.231875479221344
CurrentTrain: epoch  7, batch    25 | loss: 5.3855567Losses:  4.589069366455078 0.10655984282493591
CurrentTrain: epoch  7, batch    26 | loss: 4.8021889Losses:  4.880650520324707 0.1033073216676712
CurrentTrain: epoch  7, batch    27 | loss: 5.0872650Losses:  4.83544921875 0.15859544277191162
CurrentTrain: epoch  7, batch    28 | loss: 5.1526403Losses:  4.6033101081848145 0.10284346342086792
CurrentTrain: epoch  7, batch    29 | loss: 4.8089972Losses:  4.8560614585876465 0.17147418856620789
CurrentTrain: epoch  7, batch    30 | loss: 5.1990099Losses:  4.626438140869141 0.12128379940986633
CurrentTrain: epoch  7, batch    31 | loss: 4.8690057Losses:  4.6025919914245605 0.11593823879957199
CurrentTrain: epoch  7, batch    32 | loss: 4.8344684Losses:  4.796895503997803 0.07858668267726898
CurrentTrain: epoch  7, batch    33 | loss: 4.9540687Losses:  4.992885589599609 0.2034229040145874
CurrentTrain: epoch  7, batch    34 | loss: 5.3997316Losses:  4.658398628234863 0.1061893180012703
CurrentTrain: epoch  7, batch    35 | loss: 4.8707771Losses:  4.679664134979248 0.11573824286460876
CurrentTrain: epoch  7, batch    36 | loss: 4.9111404Losses:  4.933838844299316 0.12893661856651306
CurrentTrain: epoch  7, batch    37 | loss: 5.1917119Losses:  4.592324256896973 0.07558485865592957
CurrentTrain: epoch  8, batch     0 | loss: 4.7434940Losses:  4.585598945617676 0.10556749254465103
CurrentTrain: epoch  8, batch     1 | loss: 4.7967339Losses:  4.623937606811523 0.10085959732532501
CurrentTrain: epoch  8, batch     2 | loss: 4.8256569Losses:  4.830472469329834 0.16576746106147766
CurrentTrain: epoch  8, batch     3 | loss: 5.1620073Losses:  4.710431098937988 0.09961531311273575
CurrentTrain: epoch  8, batch     4 | loss: 4.9096618Losses:  4.866367340087891 0.11787272989749908
CurrentTrain: epoch  8, batch     5 | loss: 5.1021128Losses:  4.692866325378418 0.09812489151954651
CurrentTrain: epoch  8, batch     6 | loss: 4.8891163Losses:  4.607682704925537 0.10621385276317596
CurrentTrain: epoch  8, batch     7 | loss: 4.8201103Losses:  4.724250793457031 0.12334743142127991
CurrentTrain: epoch  8, batch     8 | loss: 4.9709458Losses:  4.88837194442749 0.20881637930870056
CurrentTrain: epoch  8, batch     9 | loss: 5.3060045Losses:  5.01070499420166 0.23314779996871948
CurrentTrain: epoch  8, batch    10 | loss: 5.4770007Losses:  4.623096466064453 0.11341917514801025
CurrentTrain: epoch  8, batch    11 | loss: 4.8499346Losses:  4.626824378967285 0.10697270929813385
CurrentTrain: epoch  8, batch    12 | loss: 4.8407698Losses:  4.635677814483643 0.0940968468785286
CurrentTrain: epoch  8, batch    13 | loss: 4.8238716Losses:  4.754462242126465 0.12826499342918396
CurrentTrain: epoch  8, batch    14 | loss: 5.0109921Losses:  4.602116584777832 0.05634539574384689
CurrentTrain: epoch  8, batch    15 | loss: 4.7148075Losses:  4.613651275634766 0.0677083283662796
CurrentTrain: epoch  8, batch    16 | loss: 4.7490678Losses:  4.602923393249512 0.08318514376878738
CurrentTrain: epoch  8, batch    17 | loss: 4.7692938Losses:  4.644190788269043 0.10947908461093903
CurrentTrain: epoch  8, batch    18 | loss: 4.8631492Losses:  4.6436872482299805 0.11197395622730255
CurrentTrain: epoch  8, batch    19 | loss: 4.8676353Losses:  4.574230194091797 0.10024495422840118
CurrentTrain: epoch  8, batch    20 | loss: 4.7747202Losses:  4.699300289154053 0.11564073711633682
CurrentTrain: epoch  8, batch    21 | loss: 4.9305816Losses:  4.679986000061035 0.06820431351661682
CurrentTrain: epoch  8, batch    22 | loss: 4.8163948Losses:  4.616733551025391 0.10285807400941849
CurrentTrain: epoch  8, batch    23 | loss: 4.8224497Losses:  4.6555047035217285 0.08800829946994781
CurrentTrain: epoch  8, batch    24 | loss: 4.8315215Losses:  4.582525253295898 0.08918490260839462
CurrentTrain: epoch  8, batch    25 | loss: 4.7608953Losses:  4.6547160148620605 0.06647282838821411
CurrentTrain: epoch  8, batch    26 | loss: 4.7876616Losses:  4.651735782623291 0.0791623592376709
CurrentTrain: epoch  8, batch    27 | loss: 4.8100605Losses:  4.59730339050293 0.10723137855529785
CurrentTrain: epoch  8, batch    28 | loss: 4.8117661Losses:  4.627559185028076 0.09563082456588745
CurrentTrain: epoch  8, batch    29 | loss: 4.8188210Losses:  4.792679786682129 0.13245850801467896
CurrentTrain: epoch  8, batch    30 | loss: 5.0575967Losses:  4.599025726318359 0.0879237949848175
CurrentTrain: epoch  8, batch    31 | loss: 4.7748733Losses:  4.761045932769775 0.12297715991735458
CurrentTrain: epoch  8, batch    32 | loss: 5.0070004Losses:  4.596039295196533 0.1011645495891571
CurrentTrain: epoch  8, batch    33 | loss: 4.7983685Losses:  4.7606120109558105 0.14019563794136047
CurrentTrain: epoch  8, batch    34 | loss: 5.0410032Losses:  4.674145698547363 0.07738595455884933
CurrentTrain: epoch  8, batch    35 | loss: 4.8289175Losses:  4.747678756713867 0.12286517769098282
CurrentTrain: epoch  8, batch    36 | loss: 4.9934092Losses:  4.787506103515625 0.1570856273174286
CurrentTrain: epoch  8, batch    37 | loss: 5.1016774Losses:  4.692536354064941 0.0646703913807869
CurrentTrain: epoch  9, batch     0 | loss: 4.8218770Losses:  4.7520904541015625 0.13911351561546326
CurrentTrain: epoch  9, batch     1 | loss: 5.0303173Losses:  4.597923755645752 0.08162149786949158
CurrentTrain: epoch  9, batch     2 | loss: 4.7611666Losses:  4.748055934906006 0.15448082983493805
CurrentTrain: epoch  9, batch     3 | loss: 5.0570178Losses:  4.531586647033691 0.06925084441900253
CurrentTrain: epoch  9, batch     4 | loss: 4.6700883Losses:  4.653660774230957 0.09668918699026108
CurrentTrain: epoch  9, batch     5 | loss: 4.8470392Losses:  5.24945068359375 0.22179125249385834
CurrentTrain: epoch  9, batch     6 | loss: 5.6930332Losses:  4.558757781982422 0.08805271238088608
CurrentTrain: epoch  9, batch     7 | loss: 4.7348633Losses:  4.598905086517334 0.09887051582336426
CurrentTrain: epoch  9, batch     8 | loss: 4.7966461Losses:  4.598254203796387 0.07402874529361725
CurrentTrain: epoch  9, batch     9 | loss: 4.7463117Losses:  4.622541904449463 0.0945766270160675
CurrentTrain: epoch  9, batch    10 | loss: 4.8116951Losses:  4.631080627441406 0.07634937763214111
CurrentTrain: epoch  9, batch    11 | loss: 4.7837791Losses:  4.646275043487549 0.0693836361169815
CurrentTrain: epoch  9, batch    12 | loss: 4.7850423Losses:  4.7096710205078125 0.1244698241353035
CurrentTrain: epoch  9, batch    13 | loss: 4.9586105Losses:  4.542909622192383 0.08889329433441162
CurrentTrain: epoch  9, batch    14 | loss: 4.7206964Losses:  4.6061296463012695 0.07061739265918732
CurrentTrain: epoch  9, batch    15 | loss: 4.7473645Losses:  4.596953392028809 0.07647038996219635
CurrentTrain: epoch  9, batch    16 | loss: 4.7498941Losses:  4.612414360046387 0.10097643733024597
CurrentTrain: epoch  9, batch    17 | loss: 4.8143673Losses:  4.606897830963135 0.08742497861385345
CurrentTrain: epoch  9, batch    18 | loss: 4.7817478Losses:  4.6371660232543945 0.08899722248315811
CurrentTrain: epoch  9, batch    19 | loss: 4.8151603Losses:  4.592708587646484 0.08403630554676056
CurrentTrain: epoch  9, batch    20 | loss: 4.7607813Losses:  4.549735069274902 0.0816577821969986
CurrentTrain: epoch  9, batch    21 | loss: 4.7130508Losses:  4.613500118255615 0.12946707010269165
CurrentTrain: epoch  9, batch    22 | loss: 4.8724341Losses:  4.549343585968018 0.08176417648792267
CurrentTrain: epoch  9, batch    23 | loss: 4.7128720Losses:  4.625749111175537 0.07834713160991669
CurrentTrain: epoch  9, batch    24 | loss: 4.7824435Losses:  4.573719024658203 0.0787496566772461
CurrentTrain: epoch  9, batch    25 | loss: 4.7312183Losses:  4.57234001159668 0.08429179340600967
CurrentTrain: epoch  9, batch    26 | loss: 4.7409234Losses:  4.513423919677734 0.06360048800706863
CurrentTrain: epoch  9, batch    27 | loss: 4.6406250Losses:  4.568272590637207 0.06643722951412201
CurrentTrain: epoch  9, batch    28 | loss: 4.7011471Losses:  4.6073198318481445 0.09238211810588837
CurrentTrain: epoch  9, batch    29 | loss: 4.7920842Losses:  4.510190963745117 0.07195749133825302
CurrentTrain: epoch  9, batch    30 | loss: 4.6541061Losses:  4.70693302154541 0.09189611673355103
CurrentTrain: epoch  9, batch    31 | loss: 4.8907251Losses:  4.557101249694824 0.08793795108795166
CurrentTrain: epoch  9, batch    32 | loss: 4.7329769Losses:  4.566583633422852 0.07962387055158615
CurrentTrain: epoch  9, batch    33 | loss: 4.7258315Losses:  4.595127582550049 0.07032568752765656
CurrentTrain: epoch  9, batch    34 | loss: 4.7357788Losses:  4.529941558837891 0.07340887188911438
CurrentTrain: epoch  9, batch    35 | loss: 4.6767592Losses:  4.559533596038818 0.08751943707466125
CurrentTrain: epoch  9, batch    36 | loss: 4.7345724Losses:  4.504293918609619 0.0687771588563919
CurrentTrain: epoch  9, batch    37 | loss: 4.6418481
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
cur_acc:  ['0.8750']
his_acc:  ['0.8750']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 0 1 0 0 1]
Losses:  7.426634311676025 1.234986424446106
CurrentTrain: epoch  0, batch     0 | loss: 9.8966074Losses:  10.133487701416016 1.4074280261993408
CurrentTrain: epoch  0, batch     1 | loss: 12.9483433Losses:  4.501175403594971 1.433794617652893
CurrentTrain: epoch  1, batch     0 | loss: 7.3687649Losses:  4.870473861694336 1.3771377801895142
CurrentTrain: epoch  1, batch     1 | loss: 7.6247492Losses:  4.539382457733154 1.2463703155517578
CurrentTrain: epoch  2, batch     0 | loss: 7.0321231Losses:  3.9794371128082275 1.072987675666809
CurrentTrain: epoch  2, batch     1 | loss: 6.1254125Losses:  4.207435607910156 1.0965960025787354
CurrentTrain: epoch  3, batch     0 | loss: 6.4006276Losses:  4.350541591644287 0.977936863899231
CurrentTrain: epoch  3, batch     1 | loss: 6.3064156Losses:  3.9733970165252686 0.9916286468505859
CurrentTrain: epoch  4, batch     0 | loss: 5.9566545Losses:  3.8665010929107666 1.1328915357589722
CurrentTrain: epoch  4, batch     1 | loss: 6.1322842Losses:  3.7099239826202393 1.0667508840560913
CurrentTrain: epoch  5, batch     0 | loss: 5.8434258Losses:  3.5081722736358643 1.0079957246780396
CurrentTrain: epoch  5, batch     1 | loss: 5.5241637Losses:  3.839345932006836 1.008108377456665
CurrentTrain: epoch  6, batch     0 | loss: 5.8555627Losses:  3.0650949478149414 0.6772547364234924
CurrentTrain: epoch  6, batch     1 | loss: 4.4196043Losses:  3.2132585048675537 0.8929203152656555
CurrentTrain: epoch  7, batch     0 | loss: 4.9990993Losses:  2.8741273880004883 0.7617543935775757
CurrentTrain: epoch  7, batch     1 | loss: 4.3976364Losses:  3.371405839920044 0.8136601448059082
CurrentTrain: epoch  8, batch     0 | loss: 4.9987259Losses:  2.450601100921631 0.7643930315971375
CurrentTrain: epoch  8, batch     1 | loss: 3.9793873Losses:  2.8943395614624023 0.7971463203430176
CurrentTrain: epoch  9, batch     0 | loss: 4.4886322Losses:  2.7095255851745605 0.6534265279769897
CurrentTrain: epoch  9, batch     1 | loss: 4.0163784
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.68%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 87.09%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 85.24%   
cur_acc:  ['0.8750', '0.8214']
his_acc:  ['0.8750', '0.8524']
Clustering into  3  clusters
Clusters:  [1 0 0 0 0 0 1 2 0 0 2 1 0 0 0 0]
Losses:  8.048151016235352 1.4122319221496582
CurrentTrain: epoch  0, batch     0 | loss: 10.8726149Losses:  10.174707412719727 1.2103341817855835
CurrentTrain: epoch  0, batch     1 | loss: 12.5953760Losses:  10.509725570678711 0.35469990968704224
CurrentTrain: epoch  0, batch     2 | loss: 11.2191257Losses:  5.245273590087891 1.4063355922698975
CurrentTrain: epoch  1, batch     0 | loss: 8.0579453Losses:  4.715803623199463 1.01285719871521
CurrentTrain: epoch  1, batch     1 | loss: 6.7415180Losses:  4.479440689086914 0.9009431600570679
CurrentTrain: epoch  1, batch     2 | loss: 6.2813272Losses:  4.734824180603027 1.137190341949463
CurrentTrain: epoch  2, batch     0 | loss: 7.0092049Losses:  3.9654669761657715 1.0608927011489868
CurrentTrain: epoch  2, batch     1 | loss: 6.0872526Losses:  5.590384483337402 1.4165074825286865
CurrentTrain: epoch  2, batch     2 | loss: 8.4233990Losses:  4.90014123916626 1.1773676872253418
CurrentTrain: epoch  3, batch     0 | loss: 7.2548766Losses:  3.6222777366638184 0.94952392578125
CurrentTrain: epoch  3, batch     1 | loss: 5.5213256Losses:  3.452094793319702 0.613284170627594
CurrentTrain: epoch  3, batch     2 | loss: 4.6786633Losses:  3.4696292877197266 1.0993061065673828
CurrentTrain: epoch  4, batch     0 | loss: 5.6682415Losses:  5.2286577224731445 0.9464998245239258
CurrentTrain: epoch  4, batch     1 | loss: 7.1216574Losses:  1.9570586681365967 0.45689642429351807
CurrentTrain: epoch  4, batch     2 | loss: 2.8708515Losses:  4.0728607177734375 1.0442286729812622
CurrentTrain: epoch  5, batch     0 | loss: 6.1613178Losses:  4.2995123863220215 0.7288268804550171
CurrentTrain: epoch  5, batch     1 | loss: 5.7571659Losses:  1.8171257972717285 0.2696201801300049
CurrentTrain: epoch  5, batch     2 | loss: 2.3563662Losses:  3.738440990447998 0.9984254837036133
CurrentTrain: epoch  6, batch     0 | loss: 5.7352920Losses:  3.4611754417419434 1.1109609603881836
CurrentTrain: epoch  6, batch     1 | loss: 5.6830974Losses:  4.504755973815918 0.13173282146453857
CurrentTrain: epoch  6, batch     2 | loss: 4.7682219Losses:  3.475431442260742 0.9886487126350403
CurrentTrain: epoch  7, batch     0 | loss: 5.4527287Losses:  3.656893730163574 1.0612050294876099
CurrentTrain: epoch  7, batch     1 | loss: 5.7793036Losses:  4.095493316650391 0.005989184603095055
CurrentTrain: epoch  7, batch     2 | loss: 4.1074715Losses:  3.0307867527008057 0.9638668298721313
CurrentTrain: epoch  8, batch     0 | loss: 4.9585204Losses:  3.4341931343078613 0.8619152903556824
CurrentTrain: epoch  8, batch     1 | loss: 5.1580238Losses:  4.728553295135498 0.3027961254119873
CurrentTrain: epoch  8, batch     2 | loss: 5.3341455Losses:  3.10660982131958 0.8629644513130188
CurrentTrain: epoch  9, batch     0 | loss: 4.8325386Losses:  3.500365734100342 0.636508584022522
CurrentTrain: epoch  9, batch     1 | loss: 4.7733831Losses:  3.360415458679199 0.6338431239128113
CurrentTrain: epoch  9, batch     2 | loss: 4.6281018
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.16%   
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 12.50%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 10.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 11.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 75.33%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 75.48%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 75.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 76.31%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 75.14%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 75.64%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 74.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 72.92%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 69.21%   
cur_acc:  ['0.8750', '0.8214', '0.3516']
his_acc:  ['0.8750', '0.8524', '0.6921']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0]
Losses:  7.022182941436768 1.5099024772644043
CurrentTrain: epoch  0, batch     0 | loss: 10.0419884Losses:  9.54814338684082 1.2652816772460938
CurrentTrain: epoch  0, batch     1 | loss: 12.0787067Losses:  7.461836338043213 0.9768743515014648
CurrentTrain: epoch  0, batch     2 | loss: 9.4155846Losses:  3.0640640258789062 1.2232272624969482
CurrentTrain: epoch  1, batch     0 | loss: 5.5105186Losses:  3.138641595840454 0.9990935325622559
CurrentTrain: epoch  1, batch     1 | loss: 5.1368284Losses:  2.839184284210205 0.9049809575080872
CurrentTrain: epoch  1, batch     2 | loss: 4.6491461Losses:  2.9693007469177246 0.9874162077903748
CurrentTrain: epoch  2, batch     0 | loss: 4.9441333Losses:  2.4564316272735596 0.691591203212738
CurrentTrain: epoch  2, batch     1 | loss: 3.8396139Losses:  2.5471231937408447 0.8301637768745422
CurrentTrain: epoch  2, batch     2 | loss: 4.2074509Losses:  2.1465907096862793 0.939130425453186
CurrentTrain: epoch  3, batch     0 | loss: 4.0248518Losses:  2.4093985557556152 0.9128884077072144
CurrentTrain: epoch  3, batch     1 | loss: 4.2351751Losses:  2.7860655784606934 0.6248159408569336
CurrentTrain: epoch  3, batch     2 | loss: 4.0356975Losses:  2.533827543258667 0.7023872137069702
CurrentTrain: epoch  4, batch     0 | loss: 3.9386020Losses:  1.9617019891738892 0.81174635887146
CurrentTrain: epoch  4, batch     1 | loss: 3.5851946Losses:  2.096198558807373 0.7243852615356445
CurrentTrain: epoch  4, batch     2 | loss: 3.5449691Losses:  2.2150015830993652 0.7446192502975464
CurrentTrain: epoch  5, batch     0 | loss: 3.7042401Losses:  1.9565980434417725 0.7161058187484741
CurrentTrain: epoch  5, batch     1 | loss: 3.3888097Losses:  2.0159530639648438 0.4089626371860504
CurrentTrain: epoch  5, batch     2 | loss: 2.8338783Losses:  2.0977938175201416 0.6404389142990112
CurrentTrain: epoch  6, batch     0 | loss: 3.3786716Losses:  1.6571261882781982 0.7509324550628662
CurrentTrain: epoch  6, batch     1 | loss: 3.1589911Losses:  2.1400868892669678 0.6793009638786316
CurrentTrain: epoch  6, batch     2 | loss: 3.4986887Losses:  1.9751499891281128 0.6524408459663391
CurrentTrain: epoch  7, batch     0 | loss: 3.2800317Losses:  1.888723373413086 0.7076433897018433
CurrentTrain: epoch  7, batch     1 | loss: 3.3040102Losses:  1.8111839294433594 0.34231528639793396
CurrentTrain: epoch  7, batch     2 | loss: 2.4958146Losses:  1.8173472881317139 0.6053805351257324
CurrentTrain: epoch  8, batch     0 | loss: 3.0281084Losses:  1.6873865127563477 0.6824123859405518
CurrentTrain: epoch  8, batch     1 | loss: 3.0522113Losses:  1.885205626487732 0.3159969747066498
CurrentTrain: epoch  8, batch     2 | loss: 2.5171995Losses:  1.6122938394546509 0.6697824001312256
CurrentTrain: epoch  9, batch     0 | loss: 2.9518585Losses:  1.7119994163513184 0.4921553134918213
CurrentTrain: epoch  9, batch     1 | loss: 2.6963100Losses:  1.8687833547592163 0.31645652651786804
CurrentTrain: epoch  9, batch     2 | loss: 2.5016963
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 80.77%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.95%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.82%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 54.28%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 69.14%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 69.70%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 66.15%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 64.36%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 63.49%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 62.34%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 63.66%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 62.64%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 63.90%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 61.64%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 60.82%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 59.67%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 60.07%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 60.68%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 61.38%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 61.84%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 62.18%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 62.81%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 62.81%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 62.90%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 63.19%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 63.85%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 63.83%   
cur_acc:  ['0.8750', '0.8214', '0.3516', '0.8077']
his_acc:  ['0.8750', '0.8524', '0.6921', '0.6383']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0]
Losses:  6.97621488571167 1.4136278629302979
CurrentTrain: epoch  0, batch     0 | loss: 9.8034706Losses:  10.954827308654785 1.3025894165039062
CurrentTrain: epoch  0, batch     1 | loss: 13.5600061Losses:  10.585490226745605 1.3249924182891846
CurrentTrain: epoch  0, batch     2 | loss: 13.2354755Losses:  3.68721866607666 1.007623553276062
CurrentTrain: epoch  1, batch     0 | loss: 5.7024660Losses:  3.4952163696289062 1.1678578853607178
CurrentTrain: epoch  1, batch     1 | loss: 5.8309321Losses:  4.598372459411621 1.274064540863037
CurrentTrain: epoch  1, batch     2 | loss: 7.1465015Losses:  3.531337261199951 0.9702327251434326
CurrentTrain: epoch  2, batch     0 | loss: 5.4718027Losses:  4.50233268737793 1.065033197402954
CurrentTrain: epoch  2, batch     1 | loss: 6.6323991Losses:  2.9719927310943604 1.0529210567474365
CurrentTrain: epoch  2, batch     2 | loss: 5.0778351Losses:  3.408392906188965 1.157152771949768
CurrentTrain: epoch  3, batch     0 | loss: 5.7226982Losses:  3.6647872924804688 1.236916184425354
CurrentTrain: epoch  3, batch     1 | loss: 6.1386194Losses:  3.39890193939209 1.1062357425689697
CurrentTrain: epoch  3, batch     2 | loss: 5.6113734Losses:  3.02394437789917 1.1204898357391357
CurrentTrain: epoch  4, batch     0 | loss: 5.2649240Losses:  3.225677013397217 1.099489450454712
CurrentTrain: epoch  4, batch     1 | loss: 5.4246559Losses:  3.8317272663116455 1.1637356281280518
CurrentTrain: epoch  4, batch     2 | loss: 6.1591988Losses:  3.7664427757263184 1.1121903657913208
CurrentTrain: epoch  5, batch     0 | loss: 5.9908237Losses:  2.9495348930358887 1.1907157897949219
CurrentTrain: epoch  5, batch     1 | loss: 5.3309665Losses:  2.6546218395233154 0.8439926505088806
CurrentTrain: epoch  5, batch     2 | loss: 4.3426070Losses:  3.1674044132232666 0.848130464553833
CurrentTrain: epoch  6, batch     0 | loss: 4.8636656Losses:  2.382688283920288 1.0862622261047363
CurrentTrain: epoch  6, batch     1 | loss: 4.5552130Losses:  3.564398765563965 0.8592999577522278
CurrentTrain: epoch  6, batch     2 | loss: 5.2829986Losses:  2.606876850128174 0.8511267304420471
CurrentTrain: epoch  7, batch     0 | loss: 4.3091302Losses:  3.339003562927246 1.0593221187591553
CurrentTrain: epoch  7, batch     1 | loss: 5.4576478Losses:  2.789665937423706 0.8772064447402954
CurrentTrain: epoch  7, batch     2 | loss: 4.5440788Losses:  2.836190700531006 0.9880027174949646
CurrentTrain: epoch  8, batch     0 | loss: 4.8121963Losses:  2.6454336643218994 0.8454368114471436
CurrentTrain: epoch  8, batch     1 | loss: 4.3363075Losses:  2.6981775760650635 0.8825170397758484
CurrentTrain: epoch  8, batch     2 | loss: 4.4632115Losses:  2.3813459873199463 0.9413555860519409
CurrentTrain: epoch  9, batch     0 | loss: 4.2640572Losses:  2.5628700256347656 0.9264963865280151
CurrentTrain: epoch  9, batch     1 | loss: 4.4158630Losses:  2.847418785095215 0.781288206577301
CurrentTrain: epoch  9, batch     2 | loss: 4.4099951
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.47%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 53.41%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.39%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.43%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 54.37%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 63.01%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 61.68%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 60.42%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 60.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 60.67%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 60.86%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 60.90%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 60.51%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 60.56%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 59.51%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 59.82%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 58.75%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 57.72%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 57.09%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.13%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 56.60%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 58.77%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 58.19%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 57.63%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 57.08%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 56.76%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 55.65%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 55.66%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 55.96%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 55.87%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 55.97%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 55.79%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 55.62%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 55.09%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 54.58%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 54.08%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 53.85%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 53.72%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 53.70%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 53.90%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 53.93%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 54.11%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 54.37%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 54.55%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 54.80%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 54.97%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 55.13%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 55.22%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 55.45%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 55.60%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 55.18%   
cur_acc:  ['0.8750', '0.8214', '0.3516', '0.8077', '0.5341']
his_acc:  ['0.8750', '0.8524', '0.6921', '0.6383', '0.5518']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0]
Losses:  6.477512836456299 1.0181350708007812
CurrentTrain: epoch  0, batch     0 | loss: 8.5137825Losses:  10.242270469665527 1.2284687757492065
CurrentTrain: epoch  0, batch     1 | loss: 12.6992083Losses:  9.297025680541992 1.2296680212020874
CurrentTrain: epoch  0, batch     2 | loss: 11.7563620Losses:  7.192522048950195 0.20718181133270264
CurrentTrain: epoch  0, batch     3 | loss: 7.6068859Losses:  2.9039978981018066 1.0625083446502686
CurrentTrain: epoch  1, batch     0 | loss: 5.0290146Losses:  2.594322919845581 0.8485062122344971
CurrentTrain: epoch  1, batch     1 | loss: 4.2913351Losses:  2.5929975509643555 1.1692945957183838
CurrentTrain: epoch  1, batch     2 | loss: 4.9315867Losses:  2.3265693187713623 0.27433905005455017
CurrentTrain: epoch  1, batch     3 | loss: 2.8752475Losses:  3.03562068939209 0.7759616374969482
CurrentTrain: epoch  2, batch     0 | loss: 4.5875440Losses:  2.153409719467163 0.9208170771598816
CurrentTrain: epoch  2, batch     1 | loss: 3.9950438Losses:  2.2947487831115723 1.1502463817596436
CurrentTrain: epoch  2, batch     2 | loss: 4.5952415Losses:  1.6804856061935425 0.33888834714889526
CurrentTrain: epoch  2, batch     3 | loss: 2.3582623Losses:  2.399416923522949 1.0342211723327637
CurrentTrain: epoch  3, batch     0 | loss: 4.4678593Losses:  2.707303047180176 0.8277496099472046
CurrentTrain: epoch  3, batch     1 | loss: 4.3628025Losses:  1.9924004077911377 0.8421185612678528
CurrentTrain: epoch  3, batch     2 | loss: 3.6766376Losses:  1.8457696437835693 0.12414507567882538
CurrentTrain: epoch  3, batch     3 | loss: 2.0940597Losses:  2.5898067951202393 0.9332023859024048
CurrentTrain: epoch  4, batch     0 | loss: 4.4562116Losses:  2.149301052093506 0.8299105167388916
CurrentTrain: epoch  4, batch     1 | loss: 3.8091221Losses:  1.877158522605896 0.8712579011917114
CurrentTrain: epoch  4, batch     2 | loss: 3.6196742Losses:  1.8952081203460693 0.5161967277526855
CurrentTrain: epoch  4, batch     3 | loss: 2.9276016Losses:  2.7142770290374756 0.6931040287017822
CurrentTrain: epoch  5, batch     0 | loss: 4.1004848Losses:  2.3085193634033203 0.8217419385910034
CurrentTrain: epoch  5, batch     1 | loss: 3.9520032Losses:  1.583458423614502 0.9292563199996948
CurrentTrain: epoch  5, batch     2 | loss: 3.4419711Losses:  1.5236899852752686 0.16516722738742828
CurrentTrain: epoch  5, batch     3 | loss: 1.8540244Losses:  1.8287577629089355 0.6660782098770142
CurrentTrain: epoch  6, batch     0 | loss: 3.1609142Losses:  2.5721921920776367 0.8969361186027527
CurrentTrain: epoch  6, batch     1 | loss: 4.3660645Losses:  1.7954950332641602 0.6366516947746277
CurrentTrain: epoch  6, batch     2 | loss: 3.0687985Losses:  1.8839619159698486 0.12588545680046082
CurrentTrain: epoch  6, batch     3 | loss: 2.1357329Losses:  1.8809525966644287 0.6702361106872559
CurrentTrain: epoch  7, batch     0 | loss: 3.2214248Losses:  1.79360032081604 0.8152782917022705
CurrentTrain: epoch  7, batch     1 | loss: 3.4241569Losses:  1.8281867504119873 0.6924919486045837
CurrentTrain: epoch  7, batch     2 | loss: 3.2131705Losses:  1.9997422695159912 0.1900053322315216
CurrentTrain: epoch  7, batch     3 | loss: 2.3797529Losses:  1.7178078889846802 0.6917688250541687
CurrentTrain: epoch  8, batch     0 | loss: 3.1013455Losses:  1.7886614799499512 0.6964178681373596
CurrentTrain: epoch  8, batch     1 | loss: 3.1814971Losses:  1.7080999612808228 0.8107340931892395
CurrentTrain: epoch  8, batch     2 | loss: 3.3295681Losses:  1.8183635473251343 0.09823932498693466
CurrentTrain: epoch  8, batch     3 | loss: 2.0148423Losses:  1.6731021404266357 0.7011075019836426
CurrentTrain: epoch  9, batch     0 | loss: 3.0753171Losses:  1.8472776412963867 0.871245801448822
CurrentTrain: epoch  9, batch     1 | loss: 3.5897694Losses:  1.5545556545257568 0.5417557954788208
CurrentTrain: epoch  9, batch     2 | loss: 2.6380672Losses:  1.204876184463501 0.26789557933807373
CurrentTrain: epoch  9, batch     3 | loss: 1.7406673
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.52%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 30.56%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 36.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 63.01%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 62.17%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 61.74%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 61.63%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 61.51%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 61.53%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 60.46%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 60.90%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 60.71%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 59.62%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 58.58%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 57.93%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.96%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 57.41%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 58.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 59.21%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 58.30%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 57.52%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 56.88%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 56.56%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 56.05%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 55.36%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 55.37%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 55.48%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 55.40%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 55.22%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 54.87%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 54.53%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 53.84%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 53.17%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 52.52%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 52.23%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 52.03%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 52.00%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 51.89%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 51.95%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 51.92%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 52.06%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 52.58%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 53.01%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 53.35%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 53.69%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 54.02%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 53.97%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 54.14%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 55.20%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 55.69%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 56.18%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 56.66%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.12%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 57.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.03%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 58.25%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 58.42%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 58.78%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 59.53%   
cur_acc:  ['0.8750', '0.8214', '0.3516', '0.8077', '0.5341', '0.9152']
his_acc:  ['0.8750', '0.8524', '0.6921', '0.6383', '0.5518', '0.5953']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0 1 2 0 0 0]
Losses:  6.845400810241699 1.213389277458191
CurrentTrain: epoch  0, batch     0 | loss: 9.2721796Losses:  9.786280632019043 1.1257107257843018
CurrentTrain: epoch  0, batch     1 | loss: 12.0377026Losses:  9.151116371154785 1.1263298988342285
CurrentTrain: epoch  0, batch     2 | loss: 11.4037762Losses:  9.761054992675781 0.6009606122970581
CurrentTrain: epoch  0, batch     3 | loss: 10.9629765Losses:  3.195880889892578 1.1036133766174316
CurrentTrain: epoch  1, batch     0 | loss: 5.4031076Losses:  2.8882927894592285 0.9545048475265503
CurrentTrain: epoch  1, batch     1 | loss: 4.7973022Losses:  2.5097522735595703 1.2079168558120728
CurrentTrain: epoch  1, batch     2 | loss: 4.9255857Losses:  2.594089984893799 0.7301938533782959
CurrentTrain: epoch  1, batch     3 | loss: 4.0544777Losses:  2.7043018341064453 1.0211117267608643
CurrentTrain: epoch  2, batch     0 | loss: 4.7465253Losses:  2.6755073070526123 0.7980161905288696
CurrentTrain: epoch  2, batch     1 | loss: 4.2715397Losses:  2.995384693145752 1.0860321521759033
CurrentTrain: epoch  2, batch     2 | loss: 5.1674490Losses:  2.098834276199341 0.5429295897483826
CurrentTrain: epoch  2, batch     3 | loss: 3.1846933Losses:  3.2273359298706055 1.0072648525238037
CurrentTrain: epoch  3, batch     0 | loss: 5.2418656Losses:  2.843148708343506 0.8529664278030396
CurrentTrain: epoch  3, batch     1 | loss: 4.5490818Losses:  1.8162089586257935 1.0033000707626343
CurrentTrain: epoch  3, batch     2 | loss: 3.8228092Losses:  1.712197184562683 0.5050163269042969
CurrentTrain: epoch  3, batch     3 | loss: 2.7222300Losses:  2.16951322555542 0.753362774848938
CurrentTrain: epoch  4, batch     0 | loss: 3.6762388Losses:  2.788306713104248 0.7824032306671143
CurrentTrain: epoch  4, batch     1 | loss: 4.3531132Losses:  1.8884037733078003 0.9522975087165833
CurrentTrain: epoch  4, batch     2 | loss: 3.7929988Losses:  2.7511110305786133 0.5049494504928589
CurrentTrain: epoch  4, batch     3 | loss: 3.7610099Losses:  3.0713157653808594 0.6877398490905762
CurrentTrain: epoch  5, batch     0 | loss: 4.4467955Losses:  1.5355920791625977 0.6270859241485596
CurrentTrain: epoch  5, batch     1 | loss: 2.7897639Losses:  1.9863040447235107 0.8334922790527344
CurrentTrain: epoch  5, batch     2 | loss: 3.6532886Losses:  2.504589319229126 0.6880615949630737
CurrentTrain: epoch  5, batch     3 | loss: 3.8807125Losses:  2.5228919982910156 0.9090220928192139
CurrentTrain: epoch  6, batch     0 | loss: 4.3409362Losses:  2.0282459259033203 0.7492018342018127
CurrentTrain: epoch  6, batch     1 | loss: 3.5266495Losses:  1.944145917892456 0.6255627870559692
CurrentTrain: epoch  6, batch     2 | loss: 3.1952715Losses:  1.4369597434997559 0.327104777097702
CurrentTrain: epoch  6, batch     3 | loss: 2.0911694Losses:  2.1759705543518066 0.7600184679031372
CurrentTrain: epoch  7, batch     0 | loss: 3.6960075Losses:  1.690885066986084 0.7465996742248535
CurrentTrain: epoch  7, batch     1 | loss: 3.1840844Losses:  1.8243908882141113 0.8295227885246277
CurrentTrain: epoch  7, batch     2 | loss: 3.4834366Losses:  2.722238302230835 0.6430268883705139
CurrentTrain: epoch  7, batch     3 | loss: 4.0082922Losses:  1.6495493650436401 0.722115159034729
CurrentTrain: epoch  8, batch     0 | loss: 3.0937796Losses:  1.9665136337280273 0.45098841190338135
CurrentTrain: epoch  8, batch     1 | loss: 2.8684905Losses:  2.0164341926574707 0.7213910818099976
CurrentTrain: epoch  8, batch     2 | loss: 3.4592164Losses:  2.0448763370513916 0.5544301271438599
CurrentTrain: epoch  8, batch     3 | loss: 3.1537366Losses:  1.7307872772216797 0.5408592224121094
CurrentTrain: epoch  9, batch     0 | loss: 2.8125057Losses:  1.6270538568496704 0.6732480525970459
CurrentTrain: epoch  9, batch     1 | loss: 2.9735498Losses:  1.722965955734253 0.881481409072876
CurrentTrain: epoch  9, batch     2 | loss: 3.4859288Losses:  2.2087650299072266 0.43570971488952637
CurrentTrain: epoch  9, batch     3 | loss: 3.0801845
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 82.92%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 12.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 15.62%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 53.85%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 53.91%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 65.80%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 64.19%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 62.99%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 61.70%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 62.04%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 62.07%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 62.22%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 61.14%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 61.57%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 62.37%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 61.35%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 60.25%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 59.19%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 58.41%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 57.43%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 57.75%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 58.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 59.10%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 58.73%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 58.05%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 57.07%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.15%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 55.26%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 54.39%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 53.94%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 53.50%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 53.36%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 53.03%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 52.81%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 52.14%   [EVAL] batch:   70 | acc: 12.50%,  total acc: 51.58%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 50.95%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 50.77%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 50.51%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 50.50%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 50.41%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 50.49%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 50.56%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 50.71%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 51.02%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 51.31%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 51.60%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 51.73%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 51.64%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 51.03%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 50.44%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 49.86%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 50.21%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 50.77%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 51.32%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 51.85%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 52.38%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 52.89%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 53.39%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 53.88%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 53.99%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 53.95%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 54.36%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 54.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 55.14%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 55.33%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 55.70%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 55.95%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 56.54%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 56.89%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 57.23%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 57.57%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 57.90%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 57.94%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 58.09%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 58.24%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 58.50%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 58.86%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 58.67%   
cur_acc:  ['0.8750', '0.8214', '0.3516', '0.8077', '0.5341', '0.9152', '0.8292']
his_acc:  ['0.8750', '0.8524', '0.6921', '0.6383', '0.5518', '0.5953', '0.5867']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 1 3 0 0 3 1 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0 1 0 0 3 1 0 0 0 1
 2 1 0 0]
Losses:  6.489499568939209 1.0081992149353027
CurrentTrain: epoch  0, batch     0 | loss: 8.5058975Losses:  9.502779006958008 1.0291733741760254
CurrentTrain: epoch  0, batch     1 | loss: 11.5611258Losses:  9.328475952148438 0.9515414237976074
CurrentTrain: epoch  0, batch     2 | loss: 11.2315588Losses:  9.711516380310059 0.8461865782737732
CurrentTrain: epoch  0, batch     3 | loss: 11.4038897Losses:  2.7013845443725586 0.9881987571716309
CurrentTrain: epoch  1, batch     0 | loss: 4.6777821Losses:  2.33174729347229 0.9816375970840454
CurrentTrain: epoch  1, batch     1 | loss: 4.2950225Losses:  2.818462610244751 0.8444576263427734
CurrentTrain: epoch  1, batch     2 | loss: 4.5073776Losses:  1.477831244468689 0.6106589436531067
CurrentTrain: epoch  1, batch     3 | loss: 2.6991491Losses:  1.9472030401229858 0.7524888515472412
CurrentTrain: epoch  2, batch     0 | loss: 3.4521809Losses:  2.5318846702575684 0.8963546752929688
CurrentTrain: epoch  2, batch     1 | loss: 4.3245940Losses:  2.450162410736084 0.7646605968475342
CurrentTrain: epoch  2, batch     2 | loss: 3.9794836Losses:  2.2129476070404053 0.8123751878738403
CurrentTrain: epoch  2, batch     3 | loss: 3.8376980Losses:  2.0363893508911133 0.8518372774124146
CurrentTrain: epoch  3, batch     0 | loss: 3.7400639Losses:  2.0924248695373535 0.8284568786621094
CurrentTrain: epoch  3, batch     1 | loss: 3.7493386Losses:  2.2605342864990234 0.7548460960388184
CurrentTrain: epoch  3, batch     2 | loss: 3.7702265Losses:  2.0987164974212646 0.6671671271324158
CurrentTrain: epoch  3, batch     3 | loss: 3.4330506Losses:  1.63882315158844 0.8211960792541504
CurrentTrain: epoch  4, batch     0 | loss: 3.2812152Losses:  1.610093593597412 0.7376634478569031
CurrentTrain: epoch  4, batch     1 | loss: 3.0854206Losses:  1.927338719367981 0.6295660138130188
CurrentTrain: epoch  4, batch     2 | loss: 3.1864707Losses:  2.4889824390411377 0.4845065474510193
CurrentTrain: epoch  4, batch     3 | loss: 3.4579954Losses:  2.037716865539551 0.7209211587905884
CurrentTrain: epoch  5, batch     0 | loss: 3.4795592Losses:  1.457529902458191 0.703911542892456
CurrentTrain: epoch  5, batch     1 | loss: 2.8653531Losses:  1.8499925136566162 0.656507134437561
CurrentTrain: epoch  5, batch     2 | loss: 3.1630068Losses:  1.647257924079895 0.7114171385765076
CurrentTrain: epoch  5, batch     3 | loss: 3.0700922Losses:  1.7589741945266724 0.5273168087005615
CurrentTrain: epoch  6, batch     0 | loss: 2.8136077Losses:  1.5126711130142212 0.6862027645111084
CurrentTrain: epoch  6, batch     1 | loss: 2.8850765Losses:  1.8809880018234253 0.6301679015159607
CurrentTrain: epoch  6, batch     2 | loss: 3.1413238Losses:  1.5896557569503784 0.5351074934005737
CurrentTrain: epoch  6, batch     3 | loss: 2.6598706Losses:  1.7223832607269287 0.8039246201515198
CurrentTrain: epoch  7, batch     0 | loss: 3.3302326Losses:  1.4578378200531006 0.5478338003158569
CurrentTrain: epoch  7, batch     1 | loss: 2.5535054Losses:  1.602863073348999 0.6166688203811646
CurrentTrain: epoch  7, batch     2 | loss: 2.8362007Losses:  1.543828010559082 0.5192812085151672
CurrentTrain: epoch  7, batch     3 | loss: 2.5823903Losses:  1.6578147411346436 0.6583881378173828
CurrentTrain: epoch  8, batch     0 | loss: 2.9745910Losses:  1.239328145980835 0.6615378856658936
CurrentTrain: epoch  8, batch     1 | loss: 2.5624039Losses:  1.2950325012207031 0.6041780710220337
CurrentTrain: epoch  8, batch     2 | loss: 2.5033886Losses:  1.7094175815582275 0.4639206528663635
CurrentTrain: epoch  8, batch     3 | loss: 2.6372590Losses:  1.7220432758331299 0.651694655418396
CurrentTrain: epoch  9, batch     0 | loss: 3.0254326Losses:  1.4185600280761719 0.6595133543014526
CurrentTrain: epoch  9, batch     1 | loss: 2.7375867Losses:  1.2123868465423584 0.6419451236724854
CurrentTrain: epoch  9, batch     2 | loss: 2.4962771Losses:  1.4908348321914673 0.4612324833869934
CurrentTrain: epoch  9, batch     3 | loss: 2.4132998
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 90.81%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 87.15%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 49.67%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 50.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.11%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 56.79%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 61.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 66.86%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 64.11%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 61.15%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 59.54%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 58.33%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 57.62%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 57.59%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 57.41%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 56.53%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 56.53%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 55.57%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 55.98%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 55.99%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 55.00%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 54.04%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 53.49%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 52.59%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 52.31%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 52.61%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 52.90%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 53.07%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 52.69%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 52.12%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 51.67%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 50.92%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 50.10%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 49.40%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 48.73%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 47.98%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 47.44%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 47.29%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 47.06%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 46.92%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 46.43%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 46.13%   [EVAL] batch:   71 | acc: 18.75%,  total acc: 45.75%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 45.72%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 45.52%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 45.50%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 45.48%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 45.62%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 45.67%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 45.89%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 46.33%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 46.76%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 47.33%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 47.67%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 47.77%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 47.21%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 46.73%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 46.19%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 46.59%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 47.19%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 47.78%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 48.35%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 48.91%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 49.46%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 50.53%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 50.85%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 50.58%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 50.26%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 50.69%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 51.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 51.61%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 51.35%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 51.15%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 51.32%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 51.49%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 51.36%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 51.69%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 52.08%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 52.47%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 52.78%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 52.87%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 52.57%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 52.32%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 52.14%   [EVAL] batch:  114 | acc: 25.00%,  total acc: 51.90%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 52.10%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 52.46%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 52.65%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 52.99%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 53.23%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 53.41%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 53.69%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 54.07%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 54.28%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 54.45%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 54.66%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 55.02%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 55.37%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 55.72%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 56.06%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 56.39%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 56.72%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 56.77%   
cur_acc:  ['0.8750', '0.8214', '0.3516', '0.8077', '0.5341', '0.9152', '0.8292', '0.8715']
his_acc:  ['0.8750', '0.8524', '0.6921', '0.6383', '0.5518', '0.5953', '0.5867', '0.5677']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.156584739685059 1.8808813095092773
CurrentTrain: epoch  0, batch     0 | loss: 15.9183474Losses:  14.884735107421875 1.9574313163757324
CurrentTrain: epoch  0, batch     1 | loss: 18.7995987Losses:  15.27204704284668 1.726423740386963
CurrentTrain: epoch  0, batch     2 | loss: 18.7248955Losses:  15.284723281860352 1.791212558746338
CurrentTrain: epoch  0, batch     3 | loss: 18.8671494Losses:  14.900794982910156 1.7949960231781006
CurrentTrain: epoch  0, batch     4 | loss: 18.4907875Losses:  15.135725021362305 1.5654537677764893
CurrentTrain: epoch  0, batch     5 | loss: 18.2666321Losses:  14.381654739379883 1.727205514907837
CurrentTrain: epoch  0, batch     6 | loss: 17.8360653Losses:  14.468502044677734 1.5595269203186035
CurrentTrain: epoch  0, batch     7 | loss: 17.5875549Losses:  14.029669761657715 1.5691564083099365
CurrentTrain: epoch  0, batch     8 | loss: 17.1679821Losses:  13.726927757263184 1.185002326965332
CurrentTrain: epoch  0, batch     9 | loss: 16.0969315Losses:  13.485355377197266 1.167504906654358
CurrentTrain: epoch  0, batch    10 | loss: 15.8203650Losses:  13.166057586669922 1.5051792860031128
CurrentTrain: epoch  0, batch    11 | loss: 16.1764164Losses:  12.56920337677002 1.057944893836975
CurrentTrain: epoch  0, batch    12 | loss: 14.6850929Losses:  12.533013343811035 1.4393768310546875
CurrentTrain: epoch  0, batch    13 | loss: 15.4117670Losses:  12.751996994018555 1.4431772232055664
CurrentTrain: epoch  0, batch    14 | loss: 15.6383514Losses:  12.511056900024414 1.6128758192062378
CurrentTrain: epoch  0, batch    15 | loss: 15.7368088Losses:  11.948589324951172 1.2066104412078857
CurrentTrain: epoch  0, batch    16 | loss: 14.3618107Losses:  11.676247596740723 1.4768617153167725
CurrentTrain: epoch  0, batch    17 | loss: 14.6299706Losses:  11.867008209228516 1.2691950798034668
CurrentTrain: epoch  0, batch    18 | loss: 14.4053984Losses:  11.441308975219727 1.170040488243103
CurrentTrain: epoch  0, batch    19 | loss: 13.7813902Losses:  11.592525482177734 1.2851123809814453
CurrentTrain: epoch  0, batch    20 | loss: 14.1627502Losses:  11.639129638671875 1.495710015296936
CurrentTrain: epoch  0, batch    21 | loss: 14.6305494Losses:  11.413122177124023 1.2524036169052124
CurrentTrain: epoch  0, batch    22 | loss: 13.9179296Losses:  11.10977554321289 1.2850420475006104
CurrentTrain: epoch  0, batch    23 | loss: 13.6798592Losses:  10.45311164855957 1.2190234661102295
CurrentTrain: epoch  0, batch    24 | loss: 12.8911591Losses:  10.341976165771484 1.2658673524856567
CurrentTrain: epoch  0, batch    25 | loss: 12.8737106Losses:  10.25174331665039 1.0476343631744385
CurrentTrain: epoch  0, batch    26 | loss: 12.3470116Losses:  10.21628189086914 1.0997010469436646
CurrentTrain: epoch  0, batch    27 | loss: 12.4156837Losses:  9.825590133666992 1.406661033630371
CurrentTrain: epoch  0, batch    28 | loss: 12.6389122Losses:  9.498006820678711 1.2191473245620728
CurrentTrain: epoch  0, batch    29 | loss: 11.9363012Losses:  9.343833923339844 1.3207083940505981
CurrentTrain: epoch  0, batch    30 | loss: 11.9852505Losses:  9.492668151855469 0.7958372235298157
CurrentTrain: epoch  0, batch    31 | loss: 11.0843430Losses:  8.938254356384277 1.0863049030303955
CurrentTrain: epoch  0, batch    32 | loss: 11.1108646Losses:  9.02995491027832 1.0006142854690552
CurrentTrain: epoch  0, batch    33 | loss: 11.0311832Losses:  8.835519790649414 1.0003424882888794
CurrentTrain: epoch  0, batch    34 | loss: 10.8362045Losses:  8.138795852661133 1.2325772047042847
CurrentTrain: epoch  0, batch    35 | loss: 10.6039505Losses:  8.231782913208008 0.9945780038833618
CurrentTrain: epoch  0, batch    36 | loss: 10.2209387Losses:  7.881563186645508 0.7452630996704102
CurrentTrain: epoch  0, batch    37 | loss: 9.3720894Losses:  7.595446586608887 0.9639780521392822
CurrentTrain: epoch  1, batch     0 | loss: 9.5234032Losses:  7.497320175170898 1.0090394020080566
CurrentTrain: epoch  1, batch     1 | loss: 9.5153990Losses:  7.682494640350342 1.0668131113052368
CurrentTrain: epoch  1, batch     2 | loss: 9.8161211Losses:  7.517306804656982 0.9978424310684204
CurrentTrain: epoch  1, batch     3 | loss: 9.5129919Losses:  7.530818462371826 0.9882727265357971
CurrentTrain: epoch  1, batch     4 | loss: 9.5073643Losses:  7.7960615158081055 0.9470367431640625
CurrentTrain: epoch  1, batch     5 | loss: 9.6901350Losses:  7.643475532531738 1.1859936714172363
CurrentTrain: epoch  1, batch     6 | loss: 10.0154629Losses:  7.539185523986816 0.8809654712677002
CurrentTrain: epoch  1, batch     7 | loss: 9.3011169Losses:  7.646587371826172 0.9191446304321289
CurrentTrain: epoch  1, batch     8 | loss: 9.4848766Losses:  7.24598503112793 0.9134503602981567
CurrentTrain: epoch  1, batch     9 | loss: 9.0728855Losses:  7.898076057434082 0.8526701331138611
CurrentTrain: epoch  1, batch    10 | loss: 9.6034164Losses:  7.097714900970459 0.5969358682632446
CurrentTrain: epoch  1, batch    11 | loss: 8.2915869Losses:  7.685116291046143 0.8824317455291748
CurrentTrain: epoch  1, batch    12 | loss: 9.4499798Losses:  7.371580123901367 0.7055286765098572
CurrentTrain: epoch  1, batch    13 | loss: 8.7826376Losses:  7.3485517501831055 0.8025858402252197
CurrentTrain: epoch  1, batch    14 | loss: 8.9537239Losses:  7.2309112548828125 0.8166087865829468
CurrentTrain: epoch  1, batch    15 | loss: 8.8641291Losses:  7.518891334533691 0.9384480118751526
CurrentTrain: epoch  1, batch    16 | loss: 9.3957872Losses:  7.090572834014893 0.8252143859863281
CurrentTrain: epoch  1, batch    17 | loss: 8.7410011Losses:  7.144288539886475 0.8114866614341736
CurrentTrain: epoch  1, batch    18 | loss: 8.7672615Losses:  7.738986015319824 1.138102650642395
CurrentTrain: epoch  1, batch    19 | loss: 10.0151911Losses:  7.168099403381348 0.8246853351593018
CurrentTrain: epoch  1, batch    20 | loss: 8.8174706Losses:  7.001837730407715 0.609994113445282
CurrentTrain: epoch  1, batch    21 | loss: 8.2218256Losses:  6.9203667640686035 0.5480524301528931
CurrentTrain: epoch  1, batch    22 | loss: 8.0164719Losses:  6.798315048217773 0.7385096549987793
CurrentTrain: epoch  1, batch    23 | loss: 8.2753344Losses:  6.841533660888672 0.7534481287002563
CurrentTrain: epoch  1, batch    24 | loss: 8.3484297Losses:  7.256410598754883 0.7571609020233154
CurrentTrain: epoch  1, batch    25 | loss: 8.7707329Losses:  6.866677284240723 0.6714205741882324
CurrentTrain: epoch  1, batch    26 | loss: 8.2095184Losses:  6.983195781707764 0.7257030010223389
CurrentTrain: epoch  1, batch    27 | loss: 8.4346018Losses:  6.244732856750488 0.47606709599494934
CurrentTrain: epoch  1, batch    28 | loss: 7.1968670Losses:  7.149301528930664 0.643821656703949
CurrentTrain: epoch  1, batch    29 | loss: 8.4369450Losses:  7.7257490158081055 0.8253111243247986
CurrentTrain: epoch  1, batch    30 | loss: 9.3763714Losses:  6.993460178375244 0.7701624035835266
CurrentTrain: epoch  1, batch    31 | loss: 8.5337849Losses:  7.15340518951416 0.7279129028320312
CurrentTrain: epoch  1, batch    32 | loss: 8.6092310Losses:  6.872967720031738 0.7931632399559021
CurrentTrain: epoch  1, batch    33 | loss: 8.4592943Losses:  6.847868919372559 0.7150728702545166
CurrentTrain: epoch  1, batch    34 | loss: 8.2780151Losses:  7.465383529663086 0.6227752566337585
CurrentTrain: epoch  1, batch    35 | loss: 8.7109337Losses:  6.873799800872803 0.7260836362838745
CurrentTrain: epoch  1, batch    36 | loss: 8.3259668Losses:  7.356204032897949 0.31815046072006226
CurrentTrain: epoch  1, batch    37 | loss: 7.9925051Losses:  6.495796203613281 0.656437873840332
CurrentTrain: epoch  2, batch     0 | loss: 7.8086720Losses:  6.692227363586426 0.6417778134346008
CurrentTrain: epoch  2, batch     1 | loss: 7.9757829Losses:  6.285589218139648 0.4934616684913635
CurrentTrain: epoch  2, batch     2 | loss: 7.2725124Losses:  6.255724906921387 0.5290545225143433
CurrentTrain: epoch  2, batch     3 | loss: 7.3138342Losses:  6.154748439788818 0.5306203365325928
CurrentTrain: epoch  2, batch     4 | loss: 7.2159891Losses:  6.3123674392700195 0.5382644534111023
CurrentTrain: epoch  2, batch     5 | loss: 7.3888965Losses:  6.638362884521484 0.5930390954017639
CurrentTrain: epoch  2, batch     6 | loss: 7.8244410Losses:  7.128156661987305 0.7041168212890625
CurrentTrain: epoch  2, batch     7 | loss: 8.5363903Losses:  6.330689430236816 0.6004607677459717
CurrentTrain: epoch  2, batch     8 | loss: 7.5316110Losses:  6.190657615661621 0.4484812617301941
CurrentTrain: epoch  2, batch     9 | loss: 7.0876203Losses:  6.383945465087891 0.39466091990470886
CurrentTrain: epoch  2, batch    10 | loss: 7.1732674Losses:  6.218517303466797 0.34444713592529297
CurrentTrain: epoch  2, batch    11 | loss: 6.9074116Losses:  6.326630592346191 0.37248462438583374
CurrentTrain: epoch  2, batch    12 | loss: 7.0716000Losses:  7.327890396118164 0.5131792426109314
CurrentTrain: epoch  2, batch    13 | loss: 8.3542490Losses:  6.596977710723877 0.6831409335136414
CurrentTrain: epoch  2, batch    14 | loss: 7.9632597Losses:  6.632575511932373 0.5697646141052246
CurrentTrain: epoch  2, batch    15 | loss: 7.7721047Losses:  6.9239182472229 0.6066759824752808
CurrentTrain: epoch  2, batch    16 | loss: 8.1372700Losses:  7.074010372161865 0.6958496570587158
CurrentTrain: epoch  2, batch    17 | loss: 8.4657097Losses:  6.62019157409668 0.5002133846282959
CurrentTrain: epoch  2, batch    18 | loss: 7.6206183Losses:  6.230314254760742 0.48750999569892883
CurrentTrain: epoch  2, batch    19 | loss: 7.2053342Losses:  6.1253533363342285 0.4398551285266876
CurrentTrain: epoch  2, batch    20 | loss: 7.0050635Losses:  6.564434051513672 0.4989606738090515
CurrentTrain: epoch  2, batch    21 | loss: 7.5623555Losses:  6.3245649337768555 0.4809384346008301
CurrentTrain: epoch  2, batch    22 | loss: 7.2864418Losses:  6.340251445770264 0.4429206848144531
CurrentTrain: epoch  2, batch    23 | loss: 7.2260928Losses:  6.506311416625977 0.4763581156730652
CurrentTrain: epoch  2, batch    24 | loss: 7.4590278Losses:  6.415855407714844 0.5695345401763916
CurrentTrain: epoch  2, batch    25 | loss: 7.5549245Losses:  6.70365571975708 0.5364847183227539
CurrentTrain: epoch  2, batch    26 | loss: 7.7766252Losses:  6.15365743637085 0.3360009491443634
CurrentTrain: epoch  2, batch    27 | loss: 6.8256593Losses:  5.98573637008667 0.3874867260456085
CurrentTrain: epoch  2, batch    28 | loss: 6.7607098Losses:  6.075783729553223 0.5055801868438721
CurrentTrain: epoch  2, batch    29 | loss: 7.0869441Losses:  6.266922950744629 0.5725229382514954
CurrentTrain: epoch  2, batch    30 | loss: 7.4119687Losses:  6.0396223068237305 0.44292008876800537
CurrentTrain: epoch  2, batch    31 | loss: 6.9254627Losses:  6.295175075531006 0.610702633857727
CurrentTrain: epoch  2, batch    32 | loss: 7.5165806Losses:  6.364052772521973 0.43416470289230347
CurrentTrain: epoch  2, batch    33 | loss: 7.2323823Losses:  5.713250637054443 0.3450905382633209
CurrentTrain: epoch  2, batch    34 | loss: 6.4034319Losses:  6.132965564727783 0.5331891179084778
CurrentTrain: epoch  2, batch    35 | loss: 7.1993437Losses:  6.311293601989746 0.414917528629303
CurrentTrain: epoch  2, batch    36 | loss: 7.1411285Losses:  6.071252822875977 0.26020532846450806
CurrentTrain: epoch  2, batch    37 | loss: 6.5916634Losses:  6.2819414138793945 0.4349210262298584
CurrentTrain: epoch  3, batch     0 | loss: 7.1517835Losses:  6.198705673217773 0.4524744153022766
CurrentTrain: epoch  3, batch     1 | loss: 7.1036544Losses:  5.841757297515869 0.46832194924354553
CurrentTrain: epoch  3, batch     2 | loss: 6.7784014Losses:  5.765172481536865 0.35403674840927124
CurrentTrain: epoch  3, batch     3 | loss: 6.4732461Losses:  5.675443172454834 0.3079988360404968
CurrentTrain: epoch  3, batch     4 | loss: 6.2914410Losses:  5.832350730895996 0.4379817843437195
CurrentTrain: epoch  3, batch     5 | loss: 6.7083144Losses:  5.843099594116211 0.2515353262424469
CurrentTrain: epoch  3, batch     6 | loss: 6.3461704Losses:  5.6798272132873535 0.3078412711620331
CurrentTrain: epoch  3, batch     7 | loss: 6.2955098Losses:  5.990496635437012 0.3668826222419739
CurrentTrain: epoch  3, batch     8 | loss: 6.7242618Losses:  6.105513572692871 0.3287688195705414
CurrentTrain: epoch  3, batch     9 | loss: 6.7630510Losses:  6.280059337615967 0.5777837038040161
CurrentTrain: epoch  3, batch    10 | loss: 7.4356270Losses:  5.71806001663208 0.3159511387348175
CurrentTrain: epoch  3, batch    11 | loss: 6.3499622Losses:  5.986893653869629 0.3606616258621216
CurrentTrain: epoch  3, batch    12 | loss: 6.7082167Losses:  5.499138832092285 0.2782425284385681
CurrentTrain: epoch  3, batch    13 | loss: 6.0556240Losses:  5.720056056976318 0.24330441653728485
CurrentTrain: epoch  3, batch    14 | loss: 6.2066650Losses:  5.8841938972473145 0.3862755298614502
CurrentTrain: epoch  3, batch    15 | loss: 6.6567450Losses:  5.978416919708252 0.3752581477165222
CurrentTrain: epoch  3, batch    16 | loss: 6.7289333Losses:  5.587914943695068 0.22002124786376953
CurrentTrain: epoch  3, batch    17 | loss: 6.0279574Losses:  6.448268890380859 0.4767141342163086
CurrentTrain: epoch  3, batch    18 | loss: 7.4016972Losses:  5.7087931632995605 0.31793856620788574
CurrentTrain: epoch  3, batch    19 | loss: 6.3446703Losses:  6.27079963684082 0.5621104836463928
CurrentTrain: epoch  3, batch    20 | loss: 7.3950205Losses:  5.7271013259887695 0.21390864253044128
CurrentTrain: epoch  3, batch    21 | loss: 6.1549187Losses:  5.782858848571777 0.2942415177822113
CurrentTrain: epoch  3, batch    22 | loss: 6.3713417Losses:  5.838789939880371 0.3195074796676636
CurrentTrain: epoch  3, batch    23 | loss: 6.4778051Losses:  5.548002243041992 0.1537298858165741
CurrentTrain: epoch  3, batch    24 | loss: 5.8554621Losses:  5.474091529846191 0.2814813256263733
CurrentTrain: epoch  3, batch    25 | loss: 6.0370541Losses:  5.558433532714844 0.3451928496360779
CurrentTrain: epoch  3, batch    26 | loss: 6.2488194Losses:  6.372659683227539 0.40595462918281555
CurrentTrain: epoch  3, batch    27 | loss: 7.1845689Losses:  5.860457420349121 0.21457386016845703
CurrentTrain: epoch  3, batch    28 | loss: 6.2896051Losses:  5.49936580657959 0.39159852266311646
CurrentTrain: epoch  3, batch    29 | loss: 6.2825627Losses:  5.324555397033691 0.3928714990615845
CurrentTrain: epoch  3, batch    30 | loss: 6.1102982Losses:  5.99735689163208 0.3924068212509155
CurrentTrain: epoch  3, batch    31 | loss: 6.7821703Losses:  6.026309013366699 0.34465235471725464
CurrentTrain: epoch  3, batch    32 | loss: 6.7156138Losses:  5.289522171020508 0.2872616648674011
CurrentTrain: epoch  3, batch    33 | loss: 5.8640456Losses:  5.109728813171387 0.17140600085258484
CurrentTrain: epoch  3, batch    34 | loss: 5.4525409Losses:  7.003557205200195 0.8635631799697876
CurrentTrain: epoch  3, batch    35 | loss: 8.7306833Losses:  5.664959907531738 0.2265355885028839
CurrentTrain: epoch  3, batch    36 | loss: 6.1180310Losses:  5.379297256469727 0.18752586841583252
CurrentTrain: epoch  3, batch    37 | loss: 5.7543488Losses:  5.769996643066406 0.2731202244758606
CurrentTrain: epoch  4, batch     0 | loss: 6.3162370Losses:  5.606804847717285 0.22618472576141357
CurrentTrain: epoch  4, batch     1 | loss: 6.0591745Losses:  5.190818786621094 0.17283377051353455
CurrentTrain: epoch  4, batch     2 | loss: 5.5364861Losses:  5.47486686706543 0.20006820559501648
CurrentTrain: epoch  4, batch     3 | loss: 5.8750033Losses:  5.653914451599121 0.403891384601593
CurrentTrain: epoch  4, batch     4 | loss: 6.4616971Losses:  5.623011112213135 0.33882007002830505
CurrentTrain: epoch  4, batch     5 | loss: 6.3006511Losses:  5.709738731384277 0.2030879557132721
CurrentTrain: epoch  4, batch     6 | loss: 6.1159148Losses:  5.399164199829102 0.2315741628408432
CurrentTrain: epoch  4, batch     7 | loss: 5.8623123Losses:  5.76770544052124 0.2859005630016327
CurrentTrain: epoch  4, batch     8 | loss: 6.3395066Losses:  6.2162089347839355 0.5208894610404968
CurrentTrain: epoch  4, batch     9 | loss: 7.2579880Losses:  5.697379112243652 0.29886674880981445
CurrentTrain: epoch  4, batch    10 | loss: 6.2951126Losses:  5.416595935821533 0.30103397369384766
CurrentTrain: epoch  4, batch    11 | loss: 6.0186639Losses:  6.233460426330566 0.5672656297683716
CurrentTrain: epoch  4, batch    12 | loss: 7.3679914Losses:  5.572360038757324 0.21006637811660767
CurrentTrain: epoch  4, batch    13 | loss: 5.9924927Losses:  5.572134017944336 0.25697433948516846
CurrentTrain: epoch  4, batch    14 | loss: 6.0860825Losses:  5.346210479736328 0.30563709139823914
CurrentTrain: epoch  4, batch    15 | loss: 5.9574847Losses:  5.639163017272949 0.17359790205955505
CurrentTrain: epoch  4, batch    16 | loss: 5.9863586Losses:  5.271177291870117 0.2525569796562195
CurrentTrain: epoch  4, batch    17 | loss: 5.7762914Losses:  5.150753021240234 0.21636469662189484
CurrentTrain: epoch  4, batch    18 | loss: 5.5834823Losses:  5.421777725219727 0.2572455406188965
CurrentTrain: epoch  4, batch    19 | loss: 5.9362688Losses:  5.70424747467041 0.4513963758945465
CurrentTrain: epoch  4, batch    20 | loss: 6.6070404Losses:  5.211874961853027 0.1891196370124817
CurrentTrain: epoch  4, batch    21 | loss: 5.5901141Losses:  5.2661285400390625 0.2653658986091614
CurrentTrain: epoch  4, batch    22 | loss: 5.7968602Losses:  5.416707992553711 0.24848148226737976
CurrentTrain: epoch  4, batch    23 | loss: 5.9136710Losses:  5.360313892364502 0.32540467381477356
CurrentTrain: epoch  4, batch    24 | loss: 6.0111232Losses:  5.455892562866211 0.30290061235427856
CurrentTrain: epoch  4, batch    25 | loss: 6.0616937Losses:  5.106402397155762 0.18754999339580536
CurrentTrain: epoch  4, batch    26 | loss: 5.4815025Losses:  6.061845779418945 0.5418750047683716
CurrentTrain: epoch  4, batch    27 | loss: 7.1455956Losses:  5.332001209259033 0.34344565868377686
CurrentTrain: epoch  4, batch    28 | loss: 6.0188923Losses:  5.269297122955322 0.17699556052684784
CurrentTrain: epoch  4, batch    29 | loss: 5.6232882Losses:  5.572128772735596 0.3231019377708435
CurrentTrain: epoch  4, batch    30 | loss: 6.2183328Losses:  5.431023120880127 0.27250367403030396
CurrentTrain: epoch  4, batch    31 | loss: 5.9760303Losses:  5.709982872009277 0.25412315130233765
CurrentTrain: epoch  4, batch    32 | loss: 6.2182293Losses:  5.462071418762207 0.17022915184497833
CurrentTrain: epoch  4, batch    33 | loss: 5.8025298Losses:  5.866703033447266 0.29538974165916443
CurrentTrain: epoch  4, batch    34 | loss: 6.4574823Losses:  5.7073893547058105 0.2577601671218872
CurrentTrain: epoch  4, batch    35 | loss: 6.2229099Losses:  5.285858631134033 0.25402697920799255
CurrentTrain: epoch  4, batch    36 | loss: 5.7939124Losses:  5.044229984283447 0.20403048396110535
CurrentTrain: epoch  4, batch    37 | loss: 5.4522910Losses:  5.950953006744385 0.43257856369018555
CurrentTrain: epoch  5, batch     0 | loss: 6.8161101Losses:  5.374690055847168 0.3091598451137543
CurrentTrain: epoch  5, batch     1 | loss: 5.9930096Losses:  5.69903564453125 0.299422025680542
CurrentTrain: epoch  5, batch     2 | loss: 6.2978797Losses:  4.909270286560059 0.13870131969451904
CurrentTrain: epoch  5, batch     3 | loss: 5.1866732Losses:  5.2995758056640625 0.24016259610652924
CurrentTrain: epoch  5, batch     4 | loss: 5.7799010Losses:  5.139754295349121 0.2533053159713745
CurrentTrain: epoch  5, batch     5 | loss: 5.6463652Losses:  5.397572994232178 0.4240252375602722
CurrentTrain: epoch  5, batch     6 | loss: 6.2456236Losses:  5.519406795501709 0.3542047142982483
CurrentTrain: epoch  5, batch     7 | loss: 6.2278161Losses:  5.108419418334961 0.3107987344264984
CurrentTrain: epoch  5, batch     8 | loss: 5.7300167Losses:  5.0790839195251465 0.17588487267494202
CurrentTrain: epoch  5, batch     9 | loss: 5.4308538Losses:  6.289937973022461 0.4156506657600403
CurrentTrain: epoch  5, batch    10 | loss: 7.1212392Losses:  5.792469024658203 0.3197494149208069
CurrentTrain: epoch  5, batch    11 | loss: 6.4319677Losses:  5.1519880294799805 0.1199183538556099
CurrentTrain: epoch  5, batch    12 | loss: 5.3918247Losses:  5.156571388244629 0.1965232789516449
CurrentTrain: epoch  5, batch    13 | loss: 5.5496178Losses:  4.9464521408081055 0.16262134909629822
CurrentTrain: epoch  5, batch    14 | loss: 5.2716947Losses:  5.1047282218933105 0.15217570960521698
CurrentTrain: epoch  5, batch    15 | loss: 5.4090796Losses:  5.4303975105285645 0.21131286025047302
CurrentTrain: epoch  5, batch    16 | loss: 5.8530231Losses:  5.18038272857666 0.21174496412277222
CurrentTrain: epoch  5, batch    17 | loss: 5.6038728Losses:  5.813101768493652 0.3958805203437805
CurrentTrain: epoch  5, batch    18 | loss: 6.6048627Losses:  5.263267517089844 0.1785968840122223
CurrentTrain: epoch  5, batch    19 | loss: 5.6204615Losses:  5.236407279968262 0.18706567585468292
CurrentTrain: epoch  5, batch    20 | loss: 5.6105385Losses:  5.056378364562988 0.18462586402893066
CurrentTrain: epoch  5, batch    21 | loss: 5.4256301Losses:  5.133630275726318 0.15271206200122833
CurrentTrain: epoch  5, batch    22 | loss: 5.4390545Losses:  5.253468036651611 0.1950617879629135
CurrentTrain: epoch  5, batch    23 | loss: 5.6435914Losses:  5.033841133117676 0.18786802887916565
CurrentTrain: epoch  5, batch    24 | loss: 5.4095774Losses:  5.005556106567383 0.13829512894153595
CurrentTrain: epoch  5, batch    25 | loss: 5.2821465Losses:  5.993404388427734 0.31636810302734375
CurrentTrain: epoch  5, batch    26 | loss: 6.6261406Losses:  5.1281843185424805 0.1403411626815796
CurrentTrain: epoch  5, batch    27 | loss: 5.4088669Losses:  4.948165416717529 0.15094013512134552
CurrentTrain: epoch  5, batch    28 | loss: 5.2500458Losses:  5.488435745239258 0.3794495165348053
CurrentTrain: epoch  5, batch    29 | loss: 6.2473350Losses:  5.269174575805664 0.17655789852142334
CurrentTrain: epoch  5, batch    30 | loss: 5.6222906Losses:  5.09293270111084 0.17169630527496338
CurrentTrain: epoch  5, batch    31 | loss: 5.4363251Losses:  5.212887287139893 0.26270824670791626
CurrentTrain: epoch  5, batch    32 | loss: 5.7383037Losses:  5.102309703826904 0.1592499315738678
CurrentTrain: epoch  5, batch    33 | loss: 5.4208097Losses:  5.315907955169678 0.20468434691429138
CurrentTrain: epoch  5, batch    34 | loss: 5.7252765Losses:  5.139031410217285 0.1411532461643219
CurrentTrain: epoch  5, batch    35 | loss: 5.4213381Losses:  4.817958354949951 0.12892219424247742
CurrentTrain: epoch  5, batch    36 | loss: 5.0758028Losses:  5.302298545837402 0.2432498186826706
CurrentTrain: epoch  5, batch    37 | loss: 5.7887983Losses:  4.89503812789917 0.17561136186122894
CurrentTrain: epoch  6, batch     0 | loss: 5.2462606Losses:  5.371471405029297 0.25702589750289917
CurrentTrain: epoch  6, batch     1 | loss: 5.8855233Losses:  5.401885986328125 0.3358137607574463
CurrentTrain: epoch  6, batch     2 | loss: 6.0735135Losses:  5.071078300476074 0.14665058255195618
CurrentTrain: epoch  6, batch     3 | loss: 5.3643794Losses:  4.8298516273498535 0.1735384613275528
CurrentTrain: epoch  6, batch     4 | loss: 5.1769285Losses:  4.94540548324585 0.14787507057189941
CurrentTrain: epoch  6, batch     5 | loss: 5.2411556Losses:  5.868044853210449 0.5305259227752686
CurrentTrain: epoch  6, batch     6 | loss: 6.9290967Losses:  4.8942155838012695 0.1259792000055313
CurrentTrain: epoch  6, batch     7 | loss: 5.1461740Losses:  4.948211193084717 0.16487734019756317
CurrentTrain: epoch  6, batch     8 | loss: 5.2779660Losses:  4.9801154136657715 0.1477261781692505
CurrentTrain: epoch  6, batch     9 | loss: 5.2755680Losses:  5.021246910095215 0.20196948945522308
CurrentTrain: epoch  6, batch    10 | loss: 5.4251857Losses:  4.856745719909668 0.1348627507686615
CurrentTrain: epoch  6, batch    11 | loss: 5.1264710Losses:  4.942591667175293 0.14747631549835205
CurrentTrain: epoch  6, batch    12 | loss: 5.2375441Losses:  4.869074821472168 0.13038353621959686
CurrentTrain: epoch  6, batch    13 | loss: 5.1298418Losses:  5.1714277267456055 0.21761199831962585
CurrentTrain: epoch  6, batch    14 | loss: 5.6066518Losses:  5.014481067657471 0.14789581298828125
CurrentTrain: epoch  6, batch    15 | loss: 5.3102727Losses:  4.902327537536621 0.12636196613311768
CurrentTrain: epoch  6, batch    16 | loss: 5.1550512Losses:  5.008038520812988 0.15791375935077667
CurrentTrain: epoch  6, batch    17 | loss: 5.3238659Losses:  4.889847755432129 0.13556663691997528
CurrentTrain: epoch  6, batch    18 | loss: 5.1609812Losses:  5.279722690582275 0.17555038630962372
CurrentTrain: epoch  6, batch    19 | loss: 5.6308236Losses:  5.51231575012207 0.2734457850456238
CurrentTrain: epoch  6, batch    20 | loss: 6.0592074Losses:  4.999197959899902 0.16104048490524292
CurrentTrain: epoch  6, batch    21 | loss: 5.3212790Losses:  4.759640693664551 0.10497381538152695
CurrentTrain: epoch  6, batch    22 | loss: 4.9695883Losses:  4.91412878036499 0.1342322826385498
CurrentTrain: epoch  6, batch    23 | loss: 5.1825933Losses:  5.145437717437744 0.20827165246009827
CurrentTrain: epoch  6, batch    24 | loss: 5.5619812Losses:  4.99256706237793 0.14131425321102142
CurrentTrain: epoch  6, batch    25 | loss: 5.2751956Losses:  5.278461456298828 0.18364840745925903
CurrentTrain: epoch  6, batch    26 | loss: 5.6457582Losses:  4.844783782958984 0.16486287117004395
CurrentTrain: epoch  6, batch    27 | loss: 5.1745095Losses:  5.005078315734863 0.15656216442584991
CurrentTrain: epoch  6, batch    28 | loss: 5.3182025Losses:  4.678211212158203 0.10443726927042007
CurrentTrain: epoch  6, batch    29 | loss: 4.8870859Losses:  5.106802940368652 0.3070957362651825
CurrentTrain: epoch  6, batch    30 | loss: 5.7209945Losses:  4.869287490844727 0.11982597410678864
CurrentTrain: epoch  6, batch    31 | loss: 5.1089396Losses:  4.852121353149414 0.1314893364906311
CurrentTrain: epoch  6, batch    32 | loss: 5.1150999Losses:  5.1933135986328125 0.25443220138549805
CurrentTrain: epoch  6, batch    33 | loss: 5.7021780Losses:  4.796226501464844 0.18184971809387207
CurrentTrain: epoch  6, batch    34 | loss: 5.1599259Losses:  5.012266159057617 0.23219090700149536
CurrentTrain: epoch  6, batch    35 | loss: 5.4766479Losses:  5.019604682922363 0.1221160888671875
CurrentTrain: epoch  6, batch    36 | loss: 5.2638369Losses:  4.842733383178711 0.21233569085597992
CurrentTrain: epoch  6, batch    37 | loss: 5.2674046Losses:  5.316449165344238 0.21159882843494415
CurrentTrain: epoch  7, batch     0 | loss: 5.7396469Losses:  5.014523506164551 0.12497745454311371
CurrentTrain: epoch  7, batch     1 | loss: 5.2644782Losses:  4.828186988830566 0.13940328359603882
CurrentTrain: epoch  7, batch     2 | loss: 5.1069937Losses:  4.869166374206543 0.15053044259548187
CurrentTrain: epoch  7, batch     3 | loss: 5.1702271Losses:  5.524628639221191 0.2401082068681717
CurrentTrain: epoch  7, batch     4 | loss: 6.0048451Losses:  4.759565353393555 0.13538166880607605
CurrentTrain: epoch  7, batch     5 | loss: 5.0303288Losses:  4.905261516571045 0.13391810655593872
CurrentTrain: epoch  7, batch     6 | loss: 5.1730976Losses:  4.997899055480957 0.15176916122436523
CurrentTrain: epoch  7, batch     7 | loss: 5.3014374Losses:  4.734424114227295 0.12300698459148407
CurrentTrain: epoch  7, batch     8 | loss: 4.9804382Losses:  4.915541648864746 0.153848335146904
CurrentTrain: epoch  7, batch     9 | loss: 5.2232385Losses:  4.975689888000488 0.1732637882232666
CurrentTrain: epoch  7, batch    10 | loss: 5.3222175Losses:  4.71781587600708 0.08752059191465378
CurrentTrain: epoch  7, batch    11 | loss: 4.8928571Losses:  4.983560562133789 0.16067546606063843
CurrentTrain: epoch  7, batch    12 | loss: 5.3049116Losses:  4.891119956970215 0.12936390936374664
CurrentTrain: epoch  7, batch    13 | loss: 5.1498480Losses:  5.177157402038574 0.18410193920135498
CurrentTrain: epoch  7, batch    14 | loss: 5.5453615Losses:  4.809185981750488 0.16945751011371613
CurrentTrain: epoch  7, batch    15 | loss: 5.1481009Losses:  4.724555015563965 0.11684998869895935
CurrentTrain: epoch  7, batch    16 | loss: 4.9582548Losses:  4.835893154144287 0.14696010947227478
CurrentTrain: epoch  7, batch    17 | loss: 5.1298132Losses:  4.765069484710693 0.1309831440448761
CurrentTrain: epoch  7, batch    18 | loss: 5.0270357Losses:  4.889941215515137 0.1677357703447342
CurrentTrain: epoch  7, batch    19 | loss: 5.2254128Losses:  4.833405494689941 0.09993064403533936
CurrentTrain: epoch  7, batch    20 | loss: 5.0332670Losses:  4.700760841369629 0.10768961906433105
CurrentTrain: epoch  7, batch    21 | loss: 4.9161401Losses:  4.855477809906006 0.09766072779893875
CurrentTrain: epoch  7, batch    22 | loss: 5.0507994Losses:  4.69544792175293 0.12039483338594437
CurrentTrain: epoch  7, batch    23 | loss: 4.9362378Losses:  4.660955429077148 0.1225334107875824
CurrentTrain: epoch  7, batch    24 | loss: 4.9060221Losses:  4.6808977127075195 0.13479316234588623
CurrentTrain: epoch  7, batch    25 | loss: 4.9504843Losses:  4.588797092437744 0.08170810341835022
CurrentTrain: epoch  7, batch    26 | loss: 4.7522135Losses:  4.7704057693481445 0.11002606153488159
CurrentTrain: epoch  7, batch    27 | loss: 4.9904580Losses:  4.713512897491455 0.1141815185546875
CurrentTrain: epoch  7, batch    28 | loss: 4.9418759Losses:  4.717987060546875 0.09226478636264801
CurrentTrain: epoch  7, batch    29 | loss: 4.9025168Losses:  4.757431507110596 0.12969204783439636
CurrentTrain: epoch  7, batch    30 | loss: 5.0168157Losses:  4.644519805908203 0.12302704155445099
CurrentTrain: epoch  7, batch    31 | loss: 4.8905740Losses:  4.654016494750977 0.12313609570264816
CurrentTrain: epoch  7, batch    32 | loss: 4.9002886Losses:  4.997849464416504 0.22642965614795685
CurrentTrain: epoch  7, batch    33 | loss: 5.4507089Losses:  4.667223930358887 0.12763917446136475
CurrentTrain: epoch  7, batch    34 | loss: 4.9225025Losses:  4.621384620666504 0.11137038469314575
CurrentTrain: epoch  7, batch    35 | loss: 4.8441253Losses:  4.6468095779418945 0.07545195519924164
CurrentTrain: epoch  7, batch    36 | loss: 4.7977133Losses:  4.610107421875 0.03121720254421234
CurrentTrain: epoch  7, batch    37 | loss: 4.6725416Losses:  4.639962196350098 0.1031547486782074
CurrentTrain: epoch  8, batch     0 | loss: 4.8462715Losses:  4.659436225891113 0.11249511688947678
CurrentTrain: epoch  8, batch     1 | loss: 4.8844266Losses:  4.693371295928955 0.11144304275512695
CurrentTrain: epoch  8, batch     2 | loss: 4.9162574Losses:  4.889665126800537 0.2278149127960205
CurrentTrain: epoch  8, batch     3 | loss: 5.3452950Losses:  4.613104820251465 0.09845130890607834
CurrentTrain: epoch  8, batch     4 | loss: 4.8100076Losses:  4.720581531524658 0.13261283934116364
CurrentTrain: epoch  8, batch     5 | loss: 4.9858074Losses:  4.782326698303223 0.12773452699184418
CurrentTrain: epoch  8, batch     6 | loss: 5.0377955Losses:  4.637905597686768 0.11149007081985474
CurrentTrain: epoch  8, batch     7 | loss: 4.8608856Losses:  4.7059783935546875 0.11601465940475464
CurrentTrain: epoch  8, batch     8 | loss: 4.9380078Losses:  5.019526958465576 0.13814325630664825
CurrentTrain: epoch  8, batch     9 | loss: 5.2958136Losses:  4.6681132316589355 0.10759761929512024
CurrentTrain: epoch  8, batch    10 | loss: 4.8833084Losses:  4.668634414672852 0.10224054753780365
CurrentTrain: epoch  8, batch    11 | loss: 4.8731155Losses:  4.647745609283447 0.10769392549991608
CurrentTrain: epoch  8, batch    12 | loss: 4.8631334Losses:  4.589072227478027 0.09041835367679596
CurrentTrain: epoch  8, batch    13 | loss: 4.7699089Losses:  4.6519575119018555 0.07855574786663055
CurrentTrain: epoch  8, batch    14 | loss: 4.8090692Losses:  4.626614570617676 0.10508126020431519
CurrentTrain: epoch  8, batch    15 | loss: 4.8367772Losses:  4.584836006164551 0.09017598628997803
CurrentTrain: epoch  8, batch    16 | loss: 4.7651882Losses:  4.821598052978516 0.1514434665441513
CurrentTrain: epoch  8, batch    17 | loss: 5.1244850Losses:  4.652837753295898 0.10781749337911606
CurrentTrain: epoch  8, batch    18 | loss: 4.8684726Losses:  4.607639789581299 0.09234952926635742
CurrentTrain: epoch  8, batch    19 | loss: 4.7923388Losses:  4.591675281524658 0.09589391946792603
CurrentTrain: epoch  8, batch    20 | loss: 4.7834630Losses:  4.874112129211426 0.12931323051452637
CurrentTrain: epoch  8, batch    21 | loss: 5.1327386Losses:  4.652816295623779 0.08395080268383026
CurrentTrain: epoch  8, batch    22 | loss: 4.8207178Losses:  4.649243354797363 0.07754360139369965
CurrentTrain: epoch  8, batch    23 | loss: 4.8043303Losses:  4.608340263366699 0.10102220624685287
CurrentTrain: epoch  8, batch    24 | loss: 4.8103848Losses:  4.614294052124023 0.101657435297966
CurrentTrain: epoch  8, batch    25 | loss: 4.8176088Losses:  4.696812629699707 0.0963679701089859
CurrentTrain: epoch  8, batch    26 | loss: 4.8895488Losses:  4.567651271820068 0.1071224957704544
CurrentTrain: epoch  8, batch    27 | loss: 4.7818961Losses:  4.689183712005615 0.1085825115442276
CurrentTrain: epoch  8, batch    28 | loss: 4.9063487Losses:  4.512834548950195 0.0831766426563263
CurrentTrain: epoch  8, batch    29 | loss: 4.6791878Losses:  4.586647987365723 0.09548810124397278
CurrentTrain: epoch  8, batch    30 | loss: 4.7776241Losses:  4.640359401702881 0.11278641223907471
CurrentTrain: epoch  8, batch    31 | loss: 4.8659325Losses:  4.564765930175781 0.06680355966091156
CurrentTrain: epoch  8, batch    32 | loss: 4.6983728Losses:  4.578267574310303 0.0914759486913681
CurrentTrain: epoch  8, batch    33 | loss: 4.7612195Losses:  4.653008460998535 0.10562355816364288
CurrentTrain: epoch  8, batch    34 | loss: 4.8642554Losses:  4.553631782531738 0.08738965541124344
CurrentTrain: epoch  8, batch    35 | loss: 4.7284112Losses:  4.620099067687988 0.09468025714159012
CurrentTrain: epoch  8, batch    36 | loss: 4.8094597Losses:  4.544367790222168 0.06674636900424957
CurrentTrain: epoch  8, batch    37 | loss: 4.6778607Losses:  4.666623115539551 0.1191001832485199
CurrentTrain: epoch  9, batch     0 | loss: 4.9048233Losses:  4.544941425323486 0.08345326036214828
CurrentTrain: epoch  9, batch     1 | loss: 4.7118478Losses:  4.555688381195068 0.0840807855129242
CurrentTrain: epoch  9, batch     2 | loss: 4.7238498Losses:  4.5755109786987305 0.0779426246881485
CurrentTrain: epoch  9, batch     3 | loss: 4.7313962Losses:  4.556427478790283 0.08852821588516235
CurrentTrain: epoch  9, batch     4 | loss: 4.7334838Losses:  4.565776348114014 0.0842033177614212
CurrentTrain: epoch  9, batch     5 | loss: 4.7341828Losses:  4.564094543457031 0.0776371955871582
CurrentTrain: epoch  9, batch     6 | loss: 4.7193689Losses:  4.565174102783203 0.08855098485946655
CurrentTrain: epoch  9, batch     7 | loss: 4.7422762Losses:  4.596354961395264 0.09813419729471207
CurrentTrain: epoch  9, batch     8 | loss: 4.7926235Losses:  4.552464962005615 0.08143270015716553
CurrentTrain: epoch  9, batch     9 | loss: 4.7153301Losses:  4.549931526184082 0.07001209259033203
CurrentTrain: epoch  9, batch    10 | loss: 4.6899557Losses:  4.613870620727539 0.07808838039636612
CurrentTrain: epoch  9, batch    11 | loss: 4.7700472Losses:  4.537528038024902 0.08922501653432846
CurrentTrain: epoch  9, batch    12 | loss: 4.7159781Losses:  4.738980293273926 0.1356707215309143
CurrentTrain: epoch  9, batch    13 | loss: 5.0103216Losses:  4.54866361618042 0.08064672350883484
CurrentTrain: epoch  9, batch    14 | loss: 4.7099571Losses:  4.562855243682861 0.08288590610027313
CurrentTrain: epoch  9, batch    15 | loss: 4.7286272Losses:  4.522615909576416 0.0847700834274292
CurrentTrain: epoch  9, batch    16 | loss: 4.6921558Losses:  4.5281782150268555 0.056718457490205765
CurrentTrain: epoch  9, batch    17 | loss: 4.6416149Losses:  4.5566182136535645 0.07424824684858322
CurrentTrain: epoch  9, batch    18 | loss: 4.7051148Losses:  4.599359512329102 0.09482213854789734
CurrentTrain: epoch  9, batch    19 | loss: 4.7890038Losses:  4.529807090759277 0.05527804046869278
CurrentTrain: epoch  9, batch    20 | loss: 4.6403632Losses:  4.569248199462891 0.09095975756645203
CurrentTrain: epoch  9, batch    21 | loss: 4.7511678Losses:  4.542331695556641 0.08708880096673965
CurrentTrain: epoch  9, batch    22 | loss: 4.7165093Losses:  4.499917984008789 0.07975192368030548
CurrentTrain: epoch  9, batch    23 | loss: 4.6594219Losses:  4.735837936401367 0.08476223051548004
CurrentTrain: epoch  9, batch    24 | loss: 4.9053626Losses:  4.549407958984375 0.07131318747997284
CurrentTrain: epoch  9, batch    25 | loss: 4.6920342Losses:  4.576983451843262 0.08369810879230499
CurrentTrain: epoch  9, batch    26 | loss: 4.7443795Losses:  4.52056884765625 0.06661908328533173
CurrentTrain: epoch  9, batch    27 | loss: 4.6538072Losses:  4.510650157928467 0.055501267313957214
CurrentTrain: epoch  9, batch    28 | loss: 4.6216526Losses:  4.536356449127197 0.06945216655731201
CurrentTrain: epoch  9, batch    29 | loss: 4.6752605Losses:  4.532260894775391 0.0745643824338913
CurrentTrain: epoch  9, batch    30 | loss: 4.6813898Losses:  4.567587852478027 0.07624123245477676
CurrentTrain: epoch  9, batch    31 | loss: 4.7200704Losses:  4.530730247497559 0.07300423085689545
CurrentTrain: epoch  9, batch    32 | loss: 4.6767387Losses:  4.529210090637207 0.08557981252670288
CurrentTrain: epoch  9, batch    33 | loss: 4.7003698Losses:  4.553617477416992 0.09049428999423981
CurrentTrain: epoch  9, batch    34 | loss: 4.7346063Losses:  4.53474235534668 0.07529021054506302
CurrentTrain: epoch  9, batch    35 | loss: 4.6853228Losses:  4.677295684814453 0.10173667967319489
CurrentTrain: epoch  9, batch    36 | loss: 4.8807693Losses:  4.475848197937012 0.04640971124172211
CurrentTrain: epoch  9, batch    37 | loss: 4.5686674
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   
cur_acc:  ['0.8731']
his_acc:  ['0.8731']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 1 0 0 0 0]
Losses:  7.439729690551758 1.4212660789489746
CurrentTrain: epoch  0, batch     0 | loss: 10.2822618Losses:  9.314818382263184 1.1999036073684692
CurrentTrain: epoch  0, batch     1 | loss: 11.7146254Losses:  3.4817028045654297 1.1257715225219727
CurrentTrain: epoch  1, batch     0 | loss: 5.7332458Losses:  3.7086148262023926 0.9844133257865906
CurrentTrain: epoch  1, batch     1 | loss: 5.6774416Losses:  3.586129903793335 1.1782739162445068
CurrentTrain: epoch  2, batch     0 | loss: 5.9426775Losses:  3.519700288772583 0.958548903465271
CurrentTrain: epoch  2, batch     1 | loss: 5.4367981Losses:  3.165315628051758 1.1082477569580078
CurrentTrain: epoch  3, batch     0 | loss: 5.3818111Losses:  4.0440239906311035 0.8233152031898499
CurrentTrain: epoch  3, batch     1 | loss: 5.6906543Losses:  3.7474961280822754 0.9264967441558838
CurrentTrain: epoch  4, batch     0 | loss: 5.6004896Losses:  2.778820037841797 0.8305638432502747
CurrentTrain: epoch  4, batch     1 | loss: 4.4399476Losses:  2.9665770530700684 0.6582576036453247
CurrentTrain: epoch  5, batch     0 | loss: 4.2830925Losses:  3.3338661193847656 0.8686333298683167
CurrentTrain: epoch  5, batch     1 | loss: 5.0711327Losses:  3.337085247039795 0.6936056613922119
CurrentTrain: epoch  6, batch     0 | loss: 4.7242966Losses:  2.4290120601654053 0.8050359487533569
CurrentTrain: epoch  6, batch     1 | loss: 4.0390840Losses:  2.896523952484131 0.7698404788970947
CurrentTrain: epoch  7, batch     0 | loss: 4.4362049Losses:  2.6206185817718506 0.6896557807922363
CurrentTrain: epoch  7, batch     1 | loss: 3.9999301Losses:  2.4628944396972656 0.682909369468689
CurrentTrain: epoch  8, batch     0 | loss: 3.8287132Losses:  2.9710404872894287 0.6597748398780823
CurrentTrain: epoch  8, batch     1 | loss: 4.2905903Losses:  2.5535202026367188 0.6782544851303101
CurrentTrain: epoch  9, batch     0 | loss: 3.9100292Losses:  2.4693586826324463 0.5878407955169678
CurrentTrain: epoch  9, batch     1 | loss: 3.6450403
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 59.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 65.83%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 82.05%   
cur_acc:  ['0.8731', '0.6583']
his_acc:  ['0.8731', '0.8205']
Clustering into  3  clusters
Clusters:  [0 2 2 2 2 2 1 0 2 0 2 0 1 2 2 1]
Losses:  7.114114284515381 1.3326058387756348
CurrentTrain: epoch  0, batch     0 | loss: 9.7793255Losses:  10.229500770568848 1.2564730644226074
CurrentTrain: epoch  0, batch     1 | loss: 12.7424469Losses:  9.267925262451172 1.1375579833984375
CurrentTrain: epoch  0, batch     2 | loss: 11.5430412Losses:  4.845660209655762 1.106095314025879
CurrentTrain: epoch  1, batch     0 | loss: 7.0578508Losses:  5.214623928070068 1.13548743724823
CurrentTrain: epoch  1, batch     1 | loss: 7.4855986Losses:  3.2670652866363525 0.6878241300582886
CurrentTrain: epoch  1, batch     2 | loss: 4.6427135Losses:  4.6169562339782715 0.9589222073554993
CurrentTrain: epoch  2, batch     0 | loss: 6.5348005Losses:  5.151025772094727 1.100698471069336
CurrentTrain: epoch  2, batch     1 | loss: 7.3524227Losses:  3.8942103385925293 0.5654151439666748
CurrentTrain: epoch  2, batch     2 | loss: 5.0250406Losses:  4.111692905426025 1.1907845735549927
CurrentTrain: epoch  3, batch     0 | loss: 6.4932623Losses:  4.2081098556518555 1.0169246196746826
CurrentTrain: epoch  3, batch     1 | loss: 6.2419591Losses:  6.017882823944092 1.0157690048217773
CurrentTrain: epoch  3, batch     2 | loss: 8.0494213Losses:  3.4795355796813965 0.9352366328239441
CurrentTrain: epoch  4, batch     0 | loss: 5.3500090Losses:  4.577341556549072 0.8859215974807739
CurrentTrain: epoch  4, batch     1 | loss: 6.3491850Losses:  3.4046592712402344 0.4742032289505005
CurrentTrain: epoch  4, batch     2 | loss: 4.3530655Losses:  3.9687247276306152 0.9455500841140747
CurrentTrain: epoch  5, batch     0 | loss: 5.8598251Losses:  3.873522996902466 1.1916841268539429
CurrentTrain: epoch  5, batch     1 | loss: 6.2568913Losses:  3.081695795059204 0.3650023341178894
CurrentTrain: epoch  5, batch     2 | loss: 3.8117003Losses:  3.821897029876709 0.8630449771881104
CurrentTrain: epoch  6, batch     0 | loss: 5.5479870Losses:  3.4832801818847656 0.9642865061759949
CurrentTrain: epoch  6, batch     1 | loss: 5.4118533Losses:  3.1314339637756348 0.4642822742462158
CurrentTrain: epoch  6, batch     2 | loss: 4.0599985Losses:  3.3282737731933594 0.9208202958106995
CurrentTrain: epoch  7, batch     0 | loss: 5.1699142Losses:  3.143458366394043 1.0981836318969727
CurrentTrain: epoch  7, batch     1 | loss: 5.3398256Losses:  3.188403606414795 0.24440866708755493
CurrentTrain: epoch  7, batch     2 | loss: 3.6772208Losses:  3.4381942749023438 1.0414588451385498
CurrentTrain: epoch  8, batch     0 | loss: 5.5211120Losses:  2.7425734996795654 0.7326381206512451
CurrentTrain: epoch  8, batch     1 | loss: 4.2078495Losses:  3.4729976654052734 0.3768521249294281
CurrentTrain: epoch  8, batch     2 | loss: 4.2267017Losses:  2.6816041469573975 0.9220949411392212
CurrentTrain: epoch  9, batch     0 | loss: 4.5257940Losses:  2.796510696411133 0.8140894174575806
CurrentTrain: epoch  9, batch     1 | loss: 4.4246893Losses:  2.7038002014160156 0.5239306688308716
CurrentTrain: epoch  9, batch     2 | loss: 3.7516615
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 70.98%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 77.30%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 76.02%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 74.03%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 73.10%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.87%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.79%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 72.41%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 73.49%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 73.31%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 72.13%   
cur_acc:  ['0.8731', '0.6583', '0.7098']
his_acc:  ['0.8731', '0.8205', '0.7213']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0]
Losses:  6.953233242034912 1.2546508312225342
CurrentTrain: epoch  0, batch     0 | loss: 9.4625349Losses:  9.774142265319824 1.3450801372528076
CurrentTrain: epoch  0, batch     1 | loss: 12.4643021Losses:  8.022659301757812 1.0839086771011353
CurrentTrain: epoch  0, batch     2 | loss: 10.1904764Losses:  3.879899024963379 1.1371169090270996
CurrentTrain: epoch  1, batch     0 | loss: 6.1541328Losses:  3.1346676349639893 0.9964108467102051
CurrentTrain: epoch  1, batch     1 | loss: 5.1274891Losses:  2.7900357246398926 0.8661511540412903
CurrentTrain: epoch  1, batch     2 | loss: 4.5223379Losses:  2.7428669929504395 1.1052000522613525
CurrentTrain: epoch  2, batch     0 | loss: 4.9532671Losses:  3.150264263153076 0.8462668657302856
CurrentTrain: epoch  2, batch     1 | loss: 4.8427982Losses:  2.8583459854125977 0.8918889164924622
CurrentTrain: epoch  2, batch     2 | loss: 4.6421237Losses:  3.09859299659729 1.0330421924591064
CurrentTrain: epoch  3, batch     0 | loss: 5.1646776Losses:  2.649930477142334 1.0743510723114014
CurrentTrain: epoch  3, batch     1 | loss: 4.7986326Losses:  2.08652400970459 0.43576690554618835
CurrentTrain: epoch  3, batch     2 | loss: 2.9580579Losses:  2.815591812133789 0.798621416091919
CurrentTrain: epoch  4, batch     0 | loss: 4.4128346Losses:  2.136326789855957 0.918261706829071
CurrentTrain: epoch  4, batch     1 | loss: 3.9728503Losses:  2.451866388320923 0.5468472242355347
CurrentTrain: epoch  4, batch     2 | loss: 3.5455608Losses:  2.055429697036743 0.9249970316886902
CurrentTrain: epoch  5, batch     0 | loss: 3.9054236Losses:  2.4600472450256348 0.9016890525817871
CurrentTrain: epoch  5, batch     1 | loss: 4.2634254Losses:  2.8103573322296143 0.556816816329956
CurrentTrain: epoch  5, batch     2 | loss: 3.9239910Losses:  2.3837974071502686 1.0070022344589233
CurrentTrain: epoch  6, batch     0 | loss: 4.3978019Losses:  1.9007896184921265 0.7349897623062134
CurrentTrain: epoch  6, batch     1 | loss: 3.3707690Losses:  2.8238039016723633 0.6036880612373352
CurrentTrain: epoch  6, batch     2 | loss: 4.0311799Losses:  2.029384136199951 0.8703569173812866
CurrentTrain: epoch  7, batch     0 | loss: 3.7700980Losses:  2.49322509765625 0.5975161194801331
CurrentTrain: epoch  7, batch     1 | loss: 3.6882572Losses:  2.0045671463012695 0.43269452452659607
CurrentTrain: epoch  7, batch     2 | loss: 2.8699563Losses:  1.849979043006897 0.775629997253418
CurrentTrain: epoch  8, batch     0 | loss: 3.4012389Losses:  2.1867475509643555 0.6941055059432983
CurrentTrain: epoch  8, batch     1 | loss: 3.5749586Losses:  1.987937569618225 0.6560961008071899
CurrentTrain: epoch  8, batch     2 | loss: 3.3001299Losses:  1.9580867290496826 0.7006475925445557
CurrentTrain: epoch  9, batch     0 | loss: 3.3593819Losses:  1.5771044492721558 0.6751768589019775
CurrentTrain: epoch  9, batch     1 | loss: 2.9274583Losses:  2.126514196395874 0.46244409680366516
CurrentTrain: epoch  9, batch     2 | loss: 3.0514023
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 82.29%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 73.78%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 71.55%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 0.00%,  total acc: 64.41%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 63.12%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 62.01%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 59.49%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 60.11%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.20%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 60.13%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 59.96%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 59.58%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 59.84%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 59.88%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 60.22%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 60.35%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 60.58%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 61.01%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 61.40%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 61.41%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 61.70%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 62.15%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 65.22%   
cur_acc:  ['0.8731', '0.6583', '0.7098', '0.8229']
his_acc:  ['0.8731', '0.8205', '0.7213', '0.6522']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0]
Losses:  7.449507236480713 1.358640193939209
CurrentTrain: epoch  0, batch     0 | loss: 10.1667881Losses:  11.697075843811035 1.4147533178329468
CurrentTrain: epoch  0, batch     1 | loss: 14.5265827Losses:  9.852035522460938 0.9912548065185547
CurrentTrain: epoch  0, batch     2 | loss: 11.8345451Losses:  4.292539119720459 1.2329058647155762
CurrentTrain: epoch  1, batch     0 | loss: 6.7583508Losses:  3.8092620372772217 1.1746792793273926
CurrentTrain: epoch  1, batch     1 | loss: 6.1586208Losses:  4.4690351486206055 1.128716230392456
CurrentTrain: epoch  1, batch     2 | loss: 6.7264676Losses:  4.847390174865723 1.18858003616333
CurrentTrain: epoch  2, batch     0 | loss: 7.2245502Losses:  3.593557834625244 1.0648149251937866
CurrentTrain: epoch  2, batch     1 | loss: 5.7231874Losses:  3.592799186706543 1.081383228302002
CurrentTrain: epoch  2, batch     2 | loss: 5.7555656Losses:  3.680755615234375 1.098552942276001
CurrentTrain: epoch  3, batch     0 | loss: 5.8778615Losses:  4.247326374053955 1.245224952697754
CurrentTrain: epoch  3, batch     1 | loss: 6.7377763Losses:  3.246363878250122 0.8978272080421448
CurrentTrain: epoch  3, batch     2 | loss: 5.0420184Losses:  4.242604732513428 1.0531725883483887
CurrentTrain: epoch  4, batch     0 | loss: 6.3489499Losses:  3.0843136310577393 1.1696946620941162
CurrentTrain: epoch  4, batch     1 | loss: 5.4237032Losses:  3.2736825942993164 0.957101047039032
CurrentTrain: epoch  4, batch     2 | loss: 5.1878848Losses:  3.3971946239471436 0.684335470199585
CurrentTrain: epoch  5, batch     0 | loss: 4.7658653Losses:  3.331942558288574 1.0249769687652588
CurrentTrain: epoch  5, batch     1 | loss: 5.3818965Losses:  3.155357837677002 0.8710690140724182
CurrentTrain: epoch  5, batch     2 | loss: 4.8974957Losses:  3.1737353801727295 1.049410104751587
CurrentTrain: epoch  6, batch     0 | loss: 5.2725554Losses:  3.103607177734375 0.9510302543640137
CurrentTrain: epoch  6, batch     1 | loss: 5.0056677Losses:  3.3602585792541504 0.8191887140274048
CurrentTrain: epoch  6, batch     2 | loss: 4.9986362Losses:  3.439155101776123 0.9144116640090942
CurrentTrain: epoch  7, batch     0 | loss: 5.2679787Losses:  2.5236268043518066 0.9426162242889404
CurrentTrain: epoch  7, batch     1 | loss: 4.4088593Losses:  3.136542558670044 0.7853111028671265
CurrentTrain: epoch  7, batch     2 | loss: 4.7071648Losses:  2.848925828933716 0.961310625076294
CurrentTrain: epoch  8, batch     0 | loss: 4.7715473Losses:  2.745394706726074 0.9198421835899353
CurrentTrain: epoch  8, batch     1 | loss: 4.5850792Losses:  3.04017972946167 0.9727063179016113
CurrentTrain: epoch  8, batch     2 | loss: 4.9855924Losses:  2.450061082839966 1.0679826736450195
CurrentTrain: epoch  9, batch     0 | loss: 4.5860262Losses:  2.6972732543945312 0.7869772911071777
CurrentTrain: epoch  9, batch     1 | loss: 4.2712278Losses:  2.859549045562744 0.7804557681083679
CurrentTrain: epoch  9, batch     2 | loss: 4.4204607
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 27.34%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 27.78%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 26.88%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 26.14%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 25.52%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 24.52%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 34.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 38.67%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 41.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 44.44%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 46.38%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 48.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 50.30%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 50.00%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 55.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 72.05%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 70.27%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.04%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 63.96%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 63.67%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 61.25%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 60.05%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 58.89%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 57.78%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 57.06%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 57.39%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 57.03%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 57.02%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 56.68%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 56.36%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 55.94%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 56.05%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 56.45%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 57.42%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 57.88%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 58.14%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 58.40%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 58.92%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 58.79%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 58.48%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 58.27%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 58.42%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 58.99%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 59.54%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 60.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 60.61%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 61.12%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 61.38%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 61.08%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 60.70%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 60.42%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 59.91%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 59.56%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 59.15%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 58.68%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 58.21%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 57.90%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 57.53%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 57.02%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 56.53%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 56.32%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 56.79%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.26%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 57.65%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 57.96%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.27%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 58.63%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 58.86%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.22%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 58.81%   
cur_acc:  ['0.8731', '0.6583', '0.7098', '0.8229', '0.5000']
his_acc:  ['0.8731', '0.8205', '0.7213', '0.6522', '0.5881']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0]
Losses:  7.147425651550293 1.1350302696228027
CurrentTrain: epoch  0, batch     0 | loss: 9.4174862Losses:  10.42695140838623 1.2042667865753174
CurrentTrain: epoch  0, batch     1 | loss: 12.8354855Losses:  10.487683296203613 1.132455587387085
CurrentTrain: epoch  0, batch     2 | loss: 12.7525940Losses:  8.007393836975098 0.5070623159408569
CurrentTrain: epoch  0, batch     3 | loss: 9.0215187Losses:  3.217353343963623 1.3197412490844727
CurrentTrain: epoch  1, batch     0 | loss: 5.8568358Losses:  4.006565093994141 0.9019172191619873
CurrentTrain: epoch  1, batch     1 | loss: 5.8103995Losses:  3.0616631507873535 1.1092908382415771
CurrentTrain: epoch  1, batch     2 | loss: 5.2802448Losses:  3.655358076095581 0.4671763777732849
CurrentTrain: epoch  1, batch     3 | loss: 4.5897107Losses:  2.660548686981201 0.8896109461784363
CurrentTrain: epoch  2, batch     0 | loss: 4.4397707Losses:  2.892392158508301 1.148655891418457
CurrentTrain: epoch  2, batch     1 | loss: 5.1897039Losses:  3.8088150024414062 1.1526978015899658
CurrentTrain: epoch  2, batch     2 | loss: 6.1142106Losses:  4.3630876541137695 0.6649894118309021
CurrentTrain: epoch  2, batch     3 | loss: 5.6930666Losses:  3.2739205360412598 0.9307202100753784
CurrentTrain: epoch  3, batch     0 | loss: 5.1353607Losses:  2.531714916229248 1.2185137271881104
CurrentTrain: epoch  3, batch     1 | loss: 4.9687424Losses:  3.0135135650634766 1.0853264331817627
CurrentTrain: epoch  3, batch     2 | loss: 5.1841664Losses:  2.3585948944091797 0.25942331552505493
CurrentTrain: epoch  3, batch     3 | loss: 2.8774414Losses:  2.288250684738159 0.8931504487991333
CurrentTrain: epoch  4, batch     0 | loss: 4.0745516Losses:  2.5130271911621094 1.0828847885131836
CurrentTrain: epoch  4, batch     1 | loss: 4.6787968Losses:  3.7394118309020996 0.9444762468338013
CurrentTrain: epoch  4, batch     2 | loss: 5.6283646Losses:  0.8300991058349609 0.046593137085437775
CurrentTrain: epoch  4, batch     3 | loss: 0.9232854Losses:  2.7271971702575684 0.9158231019973755
CurrentTrain: epoch  5, batch     0 | loss: 4.5588436Losses:  2.987492561340332 0.905823826789856
CurrentTrain: epoch  5, batch     1 | loss: 4.7991400Losses:  2.447406768798828 1.1384106874465942
CurrentTrain: epoch  5, batch     2 | loss: 4.7242279Losses:  1.089806079864502 0.18257682025432587
CurrentTrain: epoch  5, batch     3 | loss: 1.4549598Losses:  2.4525275230407715 0.9553083181381226
CurrentTrain: epoch  6, batch     0 | loss: 4.3631439Losses:  2.323208808898926 0.8917215466499329
CurrentTrain: epoch  6, batch     1 | loss: 4.1066518Losses:  2.674447774887085 0.8743594288825989
CurrentTrain: epoch  6, batch     2 | loss: 4.4231668Losses:  2.7379541397094727 0.3440176844596863
CurrentTrain: epoch  6, batch     3 | loss: 3.4259896Losses:  2.5561299324035645 0.8152121305465698
CurrentTrain: epoch  7, batch     0 | loss: 4.1865540Losses:  2.62229061126709 1.0187671184539795
CurrentTrain: epoch  7, batch     1 | loss: 4.6598248Losses:  2.066359281539917 0.9506912231445312
CurrentTrain: epoch  7, batch     2 | loss: 3.9677417Losses:  2.2439427375793457 0.1415998637676239
CurrentTrain: epoch  7, batch     3 | loss: 2.5271425Losses:  2.0874760150909424 0.8732221126556396
CurrentTrain: epoch  8, batch     0 | loss: 3.8339202Losses:  2.8188538551330566 0.8044068813323975
CurrentTrain: epoch  8, batch     1 | loss: 4.4276676Losses:  1.7037403583526611 0.8551957607269287
CurrentTrain: epoch  8, batch     2 | loss: 3.4141319Losses:  4.033670902252197 0.3047752380371094
CurrentTrain: epoch  8, batch     3 | loss: 4.6432214Losses:  2.299466848373413 0.7636716365814209
CurrentTrain: epoch  9, batch     0 | loss: 3.8268101Losses:  2.383856773376465 0.9276916980743408
CurrentTrain: epoch  9, batch     1 | loss: 4.2392402Losses:  1.4761724472045898 0.810723602771759
CurrentTrain: epoch  9, batch     2 | loss: 3.0976195Losses:  2.855963706970215 0.13228833675384521
CurrentTrain: epoch  9, batch     3 | loss: 3.1205404
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 0.00%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 33.59%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 10.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 40.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 46.63%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 63.93%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 62.15%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 60.64%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 60.53%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 61.77%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 60.37%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 59.31%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 58.02%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 57.18%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 57.16%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 56.12%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 55.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 53.92%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 53.12%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 52.12%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 51.50%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 51.59%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 51.00%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 50.88%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 50.54%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 50.11%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 49.79%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 49.59%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 49.50%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 49.50%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 49.61%   [EVAL] batch:   64 | acc: 50.00%,  total acc: 49.62%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 49.81%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 49.81%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 50.46%   [EVAL] batch:   68 | acc: 43.75%,  total acc: 50.36%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 50.00%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 50.26%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 51.60%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 52.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 52.88%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 53.49%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 53.77%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 53.24%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 52.66%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 52.08%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 51.52%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 51.20%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 50.67%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 50.15%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 49.78%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 49.43%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 49.01%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 48.46%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 48.12%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 47.94%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 48.51%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 49.06%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 49.53%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 49.93%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 50.33%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 50.77%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 51.02%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 51.45%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 51.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 52.29%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 52.57%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 52.18%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 51.74%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 51.25%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 50.77%   [EVAL] batch:  106 | acc: 0.00%,  total acc: 50.29%   
cur_acc:  ['0.8731', '0.6583', '0.7098', '0.8229', '0.5000', '0.3359']
his_acc:  ['0.8731', '0.8205', '0.7213', '0.6522', '0.5881', '0.5029']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0]
Losses:  6.515315055847168 0.9348734617233276
CurrentTrain: epoch  0, batch     0 | loss: 8.3850622Losses:  9.449175834655762 1.172715187072754
CurrentTrain: epoch  0, batch     1 | loss: 11.7946062Losses:  8.8854398727417 1.119398593902588
CurrentTrain: epoch  0, batch     2 | loss: 11.1242371Losses:  9.354934692382812 0.7777483463287354
CurrentTrain: epoch  0, batch     3 | loss: 10.9104309Losses:  2.2085657119750977 0.8350642919540405
CurrentTrain: epoch  1, batch     0 | loss: 3.8786943Losses:  2.756223440170288 1.0363514423370361
CurrentTrain: epoch  1, batch     1 | loss: 4.8289261Losses:  2.5337395668029785 0.9633224010467529
CurrentTrain: epoch  1, batch     2 | loss: 4.4603844Losses:  2.297171115875244 0.6528290510177612
CurrentTrain: epoch  1, batch     3 | loss: 3.6028292Losses:  2.6850430965423584 0.8463537693023682
CurrentTrain: epoch  2, batch     0 | loss: 4.3777504Losses:  1.759345531463623 0.8249549269676208
CurrentTrain: epoch  2, batch     1 | loss: 3.4092555Losses:  2.3129630088806152 1.0474512577056885
CurrentTrain: epoch  2, batch     2 | loss: 4.4078655Losses:  2.182499885559082 0.5159140825271606
CurrentTrain: epoch  2, batch     3 | loss: 3.2143281Losses:  2.4150710105895996 0.9650603532791138
CurrentTrain: epoch  3, batch     0 | loss: 4.3451920Losses:  2.0711395740509033 0.8074047565460205
CurrentTrain: epoch  3, batch     1 | loss: 3.6859491Losses:  1.697727918624878 0.7179403305053711
CurrentTrain: epoch  3, batch     2 | loss: 3.1336086Losses:  2.5659537315368652 0.5295813083648682
CurrentTrain: epoch  3, batch     3 | loss: 3.6251163Losses:  2.048572301864624 0.7792884111404419
CurrentTrain: epoch  4, batch     0 | loss: 3.6071491Losses:  1.739425539970398 0.9358546137809753
CurrentTrain: epoch  4, batch     1 | loss: 3.6111348Losses:  2.4694247245788574 0.8057407140731812
CurrentTrain: epoch  4, batch     2 | loss: 4.0809059Losses:  1.6337931156158447 0.48140472173690796
CurrentTrain: epoch  4, batch     3 | loss: 2.5966024Losses:  1.697646141052246 1.0401930809020996
CurrentTrain: epoch  5, batch     0 | loss: 3.7780323Losses:  2.3954639434814453 0.6469095945358276
CurrentTrain: epoch  5, batch     1 | loss: 3.6892831Losses:  1.7808690071105957 0.5871629118919373
CurrentTrain: epoch  5, batch     2 | loss: 2.9551950Losses:  1.4902567863464355 0.4824666380882263
CurrentTrain: epoch  5, batch     3 | loss: 2.4551902Losses:  1.4214115142822266 0.5774834156036377
CurrentTrain: epoch  6, batch     0 | loss: 2.5763783Losses:  2.1091408729553223 0.722732424736023
CurrentTrain: epoch  6, batch     1 | loss: 3.5546057Losses:  1.879073143005371 0.9450000524520874
CurrentTrain: epoch  6, batch     2 | loss: 3.7690732Losses:  2.4065027236938477 0.6203203201293945
CurrentTrain: epoch  6, batch     3 | loss: 3.6471434Losses:  2.399134397506714 0.6545697450637817
CurrentTrain: epoch  7, batch     0 | loss: 3.7082739Losses:  1.8382396697998047 0.6453022956848145
CurrentTrain: epoch  7, batch     1 | loss: 3.1288443Losses:  1.5740833282470703 0.6572551131248474
CurrentTrain: epoch  7, batch     2 | loss: 2.8885937Losses:  1.398836374282837 0.36955350637435913
CurrentTrain: epoch  7, batch     3 | loss: 2.1379433Losses:  1.2930335998535156 0.6797642707824707
CurrentTrain: epoch  8, batch     0 | loss: 2.6525621Losses:  1.9726018905639648 0.5807873010635376
CurrentTrain: epoch  8, batch     1 | loss: 3.1341765Losses:  1.632314920425415 0.7888273596763611
CurrentTrain: epoch  8, batch     2 | loss: 3.2099695Losses:  2.2456555366516113 0.30963167548179626
CurrentTrain: epoch  8, batch     3 | loss: 2.8649189Losses:  1.478185772895813 0.6365105509757996
CurrentTrain: epoch  9, batch     0 | loss: 2.7512069Losses:  1.5484285354614258 0.6097978949546814
CurrentTrain: epoch  9, batch     1 | loss: 2.7680244Losses:  1.541710376739502 0.528875470161438
CurrentTrain: epoch  9, batch     2 | loss: 2.5994613Losses:  1.9158658981323242 0.44569575786590576
CurrentTrain: epoch  9, batch     3 | loss: 2.8072574
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 48.56%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 50.78%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 55.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 63.36%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 37.50%,  total acc: 61.13%   [EVAL] batch:   32 | acc: 31.25%,  total acc: 60.23%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 58.46%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 56.79%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 55.21%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 53.89%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 53.78%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 54.49%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 55.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 55.79%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 56.70%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 55.52%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.26%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 53.33%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 52.17%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 51.46%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 51.43%   [EVAL] batch:   48 | acc: 6.25%,  total acc: 50.51%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 49.50%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 48.53%   [EVAL] batch:   51 | acc: 0.00%,  total acc: 47.60%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 46.70%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 46.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 46.82%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 46.54%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 46.38%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 46.23%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 45.97%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 45.62%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 45.29%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 44.66%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 44.05%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 43.46%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 43.17%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 42.61%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 42.26%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 43.01%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 43.12%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 42.86%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 42.96%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 43.32%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 44.09%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 44.85%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 45.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 46.30%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 47.00%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 47.36%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 46.76%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 46.17%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 45.60%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 45.12%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 44.80%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 44.42%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 44.04%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 43.53%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 43.10%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 42.76%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 42.35%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 41.94%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 41.62%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 41.85%   [EVAL] batch:   92 | acc: 31.25%,  total acc: 41.73%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 41.56%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 41.51%   [EVAL] batch:   95 | acc: 18.75%,  total acc: 41.28%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 41.17%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 41.14%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 41.10%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 41.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 42.08%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 42.46%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 42.11%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 41.77%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 41.37%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 40.98%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 40.89%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 41.15%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 41.40%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 41.53%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 42.06%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 42.52%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 42.92%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 43.37%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 43.37%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 43.32%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 43.64%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 43.86%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 44.01%   
cur_acc:  ['0.8731', '0.6583', '0.7098', '0.8229', '0.5000', '0.3359', '0.6875']
his_acc:  ['0.8731', '0.8205', '0.7213', '0.6522', '0.5881', '0.5029', '0.4401']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 3 1 0 0 0 1 3 0 0 3 1 2 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 3 0 0 0
 0 1 0 0]
Losses:  6.47346830368042 0.9037926197052002
CurrentTrain: epoch  0, batch     0 | loss: 8.2810535Losses:  9.329927444458008 1.0408635139465332
CurrentTrain: epoch  0, batch     1 | loss: 11.4116545Losses:  9.765348434448242 0.9803304076194763
CurrentTrain: epoch  0, batch     2 | loss: 11.7260094Losses:  9.26225757598877 0.786390483379364
CurrentTrain: epoch  0, batch     3 | loss: 10.8350382Losses:  2.434391975402832 0.9760047793388367
CurrentTrain: epoch  1, batch     0 | loss: 4.3864017Losses:  2.3679046630859375 0.9018470048904419
CurrentTrain: epoch  1, batch     1 | loss: 4.1715984Losses:  3.018533945083618 0.8725181818008423
CurrentTrain: epoch  1, batch     2 | loss: 4.7635703Losses:  2.268394708633423 0.7400665283203125
CurrentTrain: epoch  1, batch     3 | loss: 3.7485278Losses:  2.930786609649658 0.874035120010376
CurrentTrain: epoch  2, batch     0 | loss: 4.6788568Losses:  2.7394094467163086 0.890550971031189
CurrentTrain: epoch  2, batch     1 | loss: 4.5205116Losses:  2.023123264312744 0.8444217443466187
CurrentTrain: epoch  2, batch     2 | loss: 3.7119668Losses:  2.130450963973999 0.7882133722305298
CurrentTrain: epoch  2, batch     3 | loss: 3.7068777Losses:  2.516688108444214 0.8993446826934814
CurrentTrain: epoch  3, batch     0 | loss: 4.3153772Losses:  2.065918445587158 0.7371731996536255
CurrentTrain: epoch  3, batch     1 | loss: 3.5402648Losses:  1.8421220779418945 0.6897306442260742
CurrentTrain: epoch  3, batch     2 | loss: 3.2215834Losses:  2.203287124633789 0.987869381904602
CurrentTrain: epoch  3, batch     3 | loss: 4.1790257Losses:  2.0246803760528564 0.7688288688659668
CurrentTrain: epoch  4, batch     0 | loss: 3.5623381Losses:  2.370496988296509 0.7280470728874207
CurrentTrain: epoch  4, batch     1 | loss: 3.8265910Losses:  1.8807439804077148 0.7936350107192993
CurrentTrain: epoch  4, batch     2 | loss: 3.4680140Losses:  1.6098617315292358 0.7241953611373901
CurrentTrain: epoch  4, batch     3 | loss: 3.0582523Losses:  1.8388253450393677 0.6853560209274292
CurrentTrain: epoch  5, batch     0 | loss: 3.2095375Losses:  1.4732658863067627 0.7904007434844971
CurrentTrain: epoch  5, batch     1 | loss: 3.0540674Losses:  1.6699814796447754 0.7846167683601379
CurrentTrain: epoch  5, batch     2 | loss: 3.2392149Losses:  2.609008550643921 0.5686607956886292
CurrentTrain: epoch  5, batch     3 | loss: 3.7463303Losses:  1.9199811220169067 0.7203993797302246
CurrentTrain: epoch  6, batch     0 | loss: 3.3607798Losses:  2.0150279998779297 0.7032381296157837
CurrentTrain: epoch  6, batch     1 | loss: 3.4215043Losses:  1.4927787780761719 0.6142688989639282
CurrentTrain: epoch  6, batch     2 | loss: 2.7213166Losses:  1.8711562156677246 0.6074987649917603
CurrentTrain: epoch  6, batch     3 | loss: 3.0861537Losses:  1.7021571397781372 0.6935641169548035
CurrentTrain: epoch  7, batch     0 | loss: 3.0892854Losses:  1.4460394382476807 0.6754338145256042
CurrentTrain: epoch  7, batch     1 | loss: 2.7969069Losses:  1.7645046710968018 0.6725387573242188
CurrentTrain: epoch  7, batch     2 | loss: 3.1095822Losses:  1.9849131107330322 0.7066615223884583
CurrentTrain: epoch  7, batch     3 | loss: 3.3982363Losses:  1.4408483505249023 0.6918553709983826
CurrentTrain: epoch  8, batch     0 | loss: 2.8245592Losses:  1.6702992916107178 0.6458619832992554
CurrentTrain: epoch  8, batch     1 | loss: 2.9620233Losses:  1.8664054870605469 0.708362340927124
CurrentTrain: epoch  8, batch     2 | loss: 3.2831302Losses:  1.4581904411315918 0.5979939699172974
CurrentTrain: epoch  8, batch     3 | loss: 2.6541784Losses:  1.375392198562622 0.753250241279602
CurrentTrain: epoch  9, batch     0 | loss: 2.8818927Losses:  1.8218556642532349 0.6215198636054993
CurrentTrain: epoch  9, batch     1 | loss: 3.0648954Losses:  1.6364909410476685 0.5053410530090332
CurrentTrain: epoch  9, batch     2 | loss: 2.6471729Losses:  1.419871211051941 0.676703691482544
CurrentTrain: epoch  9, batch     3 | loss: 2.7732787
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.95%   
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 1.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 12.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 23.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 31.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 48.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.22%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 59.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 63.62%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 62.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.11%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 58.39%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 56.77%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 55.41%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 55.26%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 55.61%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 56.56%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 56.55%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 54.97%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 54.03%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 52.85%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 52.13%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 51.95%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 51.28%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 50.25%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 49.26%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 48.56%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 47.64%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 46.99%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 47.16%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 46.76%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 46.49%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 46.23%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 45.87%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 45.42%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 45.08%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 44.46%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 43.85%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 43.16%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 42.60%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 42.05%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 41.70%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 42.56%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 42.66%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 42.50%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 42.61%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 42.97%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 43.75%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 44.51%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 45.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 45.97%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 46.67%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 47.04%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 46.44%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 45.86%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 45.29%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 44.74%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 44.20%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 43.38%   [EVAL] batch:   85 | acc: 0.00%,  total acc: 42.88%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 42.46%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 42.05%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 41.57%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 41.18%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 40.87%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 41.17%   [EVAL] batch:   92 | acc: 50.00%,  total acc: 41.26%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 41.36%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 41.45%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 41.47%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 41.37%   [EVAL] batch:   97 | acc: 31.25%,  total acc: 41.26%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 41.22%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 41.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 42.14%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 42.03%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 41.69%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 41.29%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 40.89%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 40.51%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 40.42%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 40.77%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 40.91%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 41.39%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 41.85%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 42.26%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 42.65%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 42.55%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 42.51%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 42.68%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 42.69%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 42.70%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 43.07%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 43.49%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 43.95%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 44.41%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 44.86%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 45.30%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 45.73%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 46.16%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 46.48%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 46.37%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 46.54%   [EVAL] batch:  130 | acc: 93.75%,  total acc: 46.90%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 47.25%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 47.37%   
cur_acc:  ['0.8731', '0.6583', '0.7098', '0.8229', '0.5000', '0.3359', '0.6875', '0.8795']
his_acc:  ['0.8731', '0.8205', '0.7213', '0.6522', '0.5881', '0.5029', '0.4401', '0.4737']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.162518501281738 1.6365382671356201
CurrentTrain: epoch  0, batch     0 | loss: 15.4355946Losses:  15.086719512939453 1.7239998579025269
CurrentTrain: epoch  0, batch     1 | loss: 18.5347195Losses:  15.184571266174316 1.7754697799682617
CurrentTrain: epoch  0, batch     2 | loss: 18.7355118Losses:  15.010597229003906 1.8418112993240356
CurrentTrain: epoch  0, batch     3 | loss: 18.6942196Losses:  14.8883056640625 1.6789908409118652
CurrentTrain: epoch  0, batch     4 | loss: 18.2462883Losses:  14.621379852294922 1.576690435409546
CurrentTrain: epoch  0, batch     5 | loss: 17.7747612Losses:  14.538854598999023 1.7048603296279907
CurrentTrain: epoch  0, batch     6 | loss: 17.9485760Losses:  14.41867446899414 1.474698543548584
CurrentTrain: epoch  0, batch     7 | loss: 17.3680725Losses:  13.745244979858398 1.461788535118103
CurrentTrain: epoch  0, batch     8 | loss: 16.6688213Losses:  13.554062843322754 1.3345561027526855
CurrentTrain: epoch  0, batch     9 | loss: 16.2231750Losses:  13.381830215454102 1.1236791610717773
CurrentTrain: epoch  0, batch    10 | loss: 15.6291885Losses:  13.207710266113281 1.3702096939086914
CurrentTrain: epoch  0, batch    11 | loss: 15.9481297Losses:  12.940834045410156 1.4194085597991943
CurrentTrain: epoch  0, batch    12 | loss: 15.7796516Losses:  12.786883354187012 1.4779858589172363
CurrentTrain: epoch  0, batch    13 | loss: 15.7428551Losses:  12.927281379699707 1.6442972421646118
CurrentTrain: epoch  0, batch    14 | loss: 16.2158756Losses:  12.974153518676758 1.7475179433822632
CurrentTrain: epoch  0, batch    15 | loss: 16.4691887Losses:  12.334840774536133 1.5406228303909302
CurrentTrain: epoch  0, batch    16 | loss: 15.4160862Losses:  11.904006004333496 1.346921682357788
CurrentTrain: epoch  0, batch    17 | loss: 14.5978489Losses:  11.985750198364258 1.4579274654388428
CurrentTrain: epoch  0, batch    18 | loss: 14.9016056Losses:  11.676682472229004 1.2844254970550537
CurrentTrain: epoch  0, batch    19 | loss: 14.2455330Losses:  11.804339408874512 1.2789182662963867
CurrentTrain: epoch  0, batch    20 | loss: 14.3621759Losses:  11.213937759399414 1.001906394958496
CurrentTrain: epoch  0, batch    21 | loss: 13.2177505Losses:  11.078109741210938 1.2587906122207642
CurrentTrain: epoch  0, batch    22 | loss: 13.5956907Losses:  10.710346221923828 1.3751875162124634
CurrentTrain: epoch  0, batch    23 | loss: 13.4607210Losses:  10.911280632019043 1.2692818641662598
CurrentTrain: epoch  0, batch    24 | loss: 13.4498444Losses:  10.497505187988281 1.0266345739364624
CurrentTrain: epoch  0, batch    25 | loss: 12.5507746Losses:  10.245299339294434 1.2289793491363525
CurrentTrain: epoch  0, batch    26 | loss: 12.7032585Losses:  9.982054710388184 1.2092159986495972
CurrentTrain: epoch  0, batch    27 | loss: 12.4004869Losses:  9.425301551818848 0.7528855204582214
CurrentTrain: epoch  0, batch    28 | loss: 10.9310722Losses:  9.225059509277344 1.2709174156188965
CurrentTrain: epoch  0, batch    29 | loss: 11.7668943Losses:  9.551996231079102 1.2325682640075684
CurrentTrain: epoch  0, batch    30 | loss: 12.0171328Losses:  9.4401273727417 1.2576502561569214
CurrentTrain: epoch  0, batch    31 | loss: 11.9554281Losses:  8.983610153198242 1.221076250076294
CurrentTrain: epoch  0, batch    32 | loss: 11.4257622Losses:  8.848012924194336 1.2135396003723145
CurrentTrain: epoch  0, batch    33 | loss: 11.2750921Losses:  8.195601463317871 0.9809870719909668
CurrentTrain: epoch  0, batch    34 | loss: 10.1575756Losses:  8.63973331451416 1.2508656978607178
CurrentTrain: epoch  0, batch    35 | loss: 11.1414642Losses:  8.51298713684082 1.0456838607788086
CurrentTrain: epoch  0, batch    36 | loss: 10.6043549Losses:  7.492107391357422 0.8739217519760132
CurrentTrain: epoch  0, batch    37 | loss: 9.2399511Losses:  7.79049015045166 0.8823598623275757
CurrentTrain: epoch  1, batch     0 | loss: 9.5552101Losses:  7.596703052520752 0.8599323034286499
CurrentTrain: epoch  1, batch     1 | loss: 9.3165674Losses:  7.792409896850586 1.0175352096557617
CurrentTrain: epoch  1, batch     2 | loss: 9.8274803Losses:  7.642142295837402 0.8555570840835571
CurrentTrain: epoch  1, batch     3 | loss: 9.3532562Losses:  7.4552321434021 0.8344296813011169
CurrentTrain: epoch  1, batch     4 | loss: 9.1240911Losses:  7.458728790283203 1.0200755596160889
CurrentTrain: epoch  1, batch     5 | loss: 9.4988804Losses:  7.40354061126709 1.0142526626586914
CurrentTrain: epoch  1, batch     6 | loss: 9.4320459Losses:  7.483572959899902 1.1433436870574951
CurrentTrain: epoch  1, batch     7 | loss: 9.7702599Losses:  7.7054901123046875 1.0526970624923706
CurrentTrain: epoch  1, batch     8 | loss: 9.8108845Losses:  7.8805131912231445 0.8324659466743469
CurrentTrain: epoch  1, batch     9 | loss: 9.5454454Losses:  7.250309944152832 0.901910662651062
CurrentTrain: epoch  1, batch    10 | loss: 9.0541315Losses:  7.563047409057617 1.0472723245620728
CurrentTrain: epoch  1, batch    11 | loss: 9.6575918Losses:  7.380584716796875 1.0052621364593506
CurrentTrain: epoch  1, batch    12 | loss: 9.3911095Losses:  7.6917500495910645 0.9196018576622009
CurrentTrain: epoch  1, batch    13 | loss: 9.5309534Losses:  7.473265647888184 0.92931067943573
CurrentTrain: epoch  1, batch    14 | loss: 9.3318872Losses:  7.876761436462402 0.8623254299163818
CurrentTrain: epoch  1, batch    15 | loss: 9.6014118Losses:  7.3703460693359375 0.7839256525039673
CurrentTrain: epoch  1, batch    16 | loss: 8.9381971Losses:  7.809412002563477 0.9237390756607056
CurrentTrain: epoch  1, batch    17 | loss: 9.6568899Losses:  6.844283103942871 0.7816269397735596
CurrentTrain: epoch  1, batch    18 | loss: 8.4075375Losses:  7.278885841369629 0.8227043151855469
CurrentTrain: epoch  1, batch    19 | loss: 8.9242945Losses:  7.348426818847656 0.8854798078536987
CurrentTrain: epoch  1, batch    20 | loss: 9.1193867Losses:  7.570717811584473 0.770942747592926
CurrentTrain: epoch  1, batch    21 | loss: 9.1126032Losses:  6.973989486694336 0.6981878280639648
CurrentTrain: epoch  1, batch    22 | loss: 8.3703651Losses:  7.605893135070801 0.8578213453292847
CurrentTrain: epoch  1, batch    23 | loss: 9.3215361Losses:  7.131624221801758 0.5381746292114258
CurrentTrain: epoch  1, batch    24 | loss: 8.2079735Losses:  7.295068740844727 0.9489513635635376
CurrentTrain: epoch  1, batch    25 | loss: 9.1929712Losses:  6.867711544036865 0.6993787288665771
CurrentTrain: epoch  1, batch    26 | loss: 8.2664690Losses:  6.962025165557861 0.7912017107009888
CurrentTrain: epoch  1, batch    27 | loss: 8.5444288Losses:  7.018468856811523 0.6864381432533264
CurrentTrain: epoch  1, batch    28 | loss: 8.3913450Losses:  7.106693267822266 0.735532283782959
CurrentTrain: epoch  1, batch    29 | loss: 8.5777578Losses:  6.7993879318237305 0.6410449147224426
CurrentTrain: epoch  1, batch    30 | loss: 8.0814781Losses:  6.8163957595825195 0.7094601988792419
CurrentTrain: epoch  1, batch    31 | loss: 8.2353163Losses:  6.607625961303711 0.6798787117004395
CurrentTrain: epoch  1, batch    32 | loss: 7.9673834Losses:  6.9766693115234375 0.6077771186828613
CurrentTrain: epoch  1, batch    33 | loss: 8.1922235Losses:  6.846563816070557 0.7096990346908569
CurrentTrain: epoch  1, batch    34 | loss: 8.2659616Losses:  6.504098892211914 0.6195415258407593
CurrentTrain: epoch  1, batch    35 | loss: 7.7431822Losses:  6.174010753631592 0.38507628440856934
CurrentTrain: epoch  1, batch    36 | loss: 6.9441633Losses:  6.765013694763184 0.4664466977119446
CurrentTrain: epoch  1, batch    37 | loss: 7.6979070Losses:  7.1076202392578125 0.6989893317222595
CurrentTrain: epoch  2, batch     0 | loss: 8.5055990Losses:  6.896193504333496 0.6075853109359741
CurrentTrain: epoch  2, batch     1 | loss: 8.1113644Losses:  6.528161525726318 0.6113611459732056
CurrentTrain: epoch  2, batch     2 | loss: 7.7508841Losses:  6.161284446716309 0.4852537214756012
CurrentTrain: epoch  2, batch     3 | loss: 7.1317921Losses:  6.5870561599731445 0.5514330863952637
CurrentTrain: epoch  2, batch     4 | loss: 7.6899223Losses:  6.474262714385986 0.6056721806526184
CurrentTrain: epoch  2, batch     5 | loss: 7.6856070Losses:  7.100362777709961 0.5932850241661072
CurrentTrain: epoch  2, batch     6 | loss: 8.2869329Losses:  6.466390609741211 0.6367082595825195
CurrentTrain: epoch  2, batch     7 | loss: 7.7398071Losses:  6.576601982116699 0.6241710186004639
CurrentTrain: epoch  2, batch     8 | loss: 7.8249440Losses:  6.653120040893555 0.49286216497421265
CurrentTrain: epoch  2, batch     9 | loss: 7.6388445Losses:  6.208409309387207 0.40091773867607117
CurrentTrain: epoch  2, batch    10 | loss: 7.0102448Losses:  6.133393287658691 0.5012456178665161
CurrentTrain: epoch  2, batch    11 | loss: 7.1358843Losses:  6.006159782409668 0.43334946036338806
CurrentTrain: epoch  2, batch    12 | loss: 6.8728585Losses:  6.71750020980835 0.4524577260017395
CurrentTrain: epoch  2, batch    13 | loss: 7.6224155Losses:  5.741640090942383 0.35784912109375
CurrentTrain: epoch  2, batch    14 | loss: 6.4573383Losses:  6.000133991241455 0.32532215118408203
CurrentTrain: epoch  2, batch    15 | loss: 6.6507783Losses:  6.1018781661987305 0.47797197103500366
CurrentTrain: epoch  2, batch    16 | loss: 7.0578222Losses:  6.503033638000488 0.6054195165634155
CurrentTrain: epoch  2, batch    17 | loss: 7.7138729Losses:  6.289722442626953 0.4619704484939575
CurrentTrain: epoch  2, batch    18 | loss: 7.2136631Losses:  6.287459373474121 0.48152047395706177
CurrentTrain: epoch  2, batch    19 | loss: 7.2505002Losses:  6.556207656860352 0.565254807472229
CurrentTrain: epoch  2, batch    20 | loss: 7.6867170Losses:  5.985384464263916 0.33223408460617065
CurrentTrain: epoch  2, batch    21 | loss: 6.6498528Losses:  6.7088518142700195 0.6731420755386353
CurrentTrain: epoch  2, batch    22 | loss: 8.0551357Losses:  6.556550979614258 0.44637519121170044
CurrentTrain: epoch  2, batch    23 | loss: 7.4493012Losses:  6.360363960266113 0.39591968059539795
CurrentTrain: epoch  2, batch    24 | loss: 7.1522036Losses:  5.846468925476074 0.39293140172958374
CurrentTrain: epoch  2, batch    25 | loss: 6.6323318Losses:  6.241572856903076 0.43961718678474426
CurrentTrain: epoch  2, batch    26 | loss: 7.1208072Losses:  6.177158832550049 0.367910236120224
CurrentTrain: epoch  2, batch    27 | loss: 6.9129791Losses:  6.687183856964111 0.7710239887237549
CurrentTrain: epoch  2, batch    28 | loss: 8.2292318Losses:  6.240176200866699 0.4733358919620514
CurrentTrain: epoch  2, batch    29 | loss: 7.1868482Losses:  7.28749942779541 0.9032596349716187
CurrentTrain: epoch  2, batch    30 | loss: 9.0940189Losses:  6.298192501068115 0.4941786825656891
CurrentTrain: epoch  2, batch    31 | loss: 7.2865500Losses:  6.47046422958374 0.5084062218666077
CurrentTrain: epoch  2, batch    32 | loss: 7.4872766Losses:  6.89968204498291 0.6589864492416382
CurrentTrain: epoch  2, batch    33 | loss: 8.2176552Losses:  6.076058387756348 0.40085694193840027
CurrentTrain: epoch  2, batch    34 | loss: 6.8777723Losses:  6.280979156494141 0.42721933126449585
CurrentTrain: epoch  2, batch    35 | loss: 7.1354179Losses:  6.817188739776611 0.3778204619884491
CurrentTrain: epoch  2, batch    36 | loss: 7.5728297Losses:  6.54233455657959 0.4464762210845947
CurrentTrain: epoch  2, batch    37 | loss: 7.4352870Losses:  6.37049674987793 0.37819573283195496
CurrentTrain: epoch  3, batch     0 | loss: 7.1268883Losses:  5.441451072692871 0.29086291790008545
CurrentTrain: epoch  3, batch     1 | loss: 6.0231771Losses:  6.163418769836426 0.46681368350982666
CurrentTrain: epoch  3, batch     2 | loss: 7.0970459Losses:  5.847897529602051 0.5044416785240173
CurrentTrain: epoch  3, batch     3 | loss: 6.8567810Losses:  6.179067134857178 0.28657299280166626
CurrentTrain: epoch  3, batch     4 | loss: 6.7522130Losses:  6.241123199462891 0.5282406806945801
CurrentTrain: epoch  3, batch     5 | loss: 7.2976046Losses:  6.127503395080566 0.35965558886528015
CurrentTrain: epoch  3, batch     6 | loss: 6.8468146Losses:  6.0581865310668945 0.47290822863578796
CurrentTrain: epoch  3, batch     7 | loss: 7.0040030Losses:  5.765618324279785 0.350972443819046
CurrentTrain: epoch  3, batch     8 | loss: 6.4675632Losses:  6.008861541748047 0.4836072325706482
CurrentTrain: epoch  3, batch     9 | loss: 6.9760761Losses:  6.034027099609375 0.3789360523223877
CurrentTrain: epoch  3, batch    10 | loss: 6.7918992Losses:  5.422065734863281 0.3168785572052002
CurrentTrain: epoch  3, batch    11 | loss: 6.0558228Losses:  5.814493179321289 0.367154061794281
CurrentTrain: epoch  3, batch    12 | loss: 6.5488014Losses:  6.151028633117676 0.3590185046195984
CurrentTrain: epoch  3, batch    13 | loss: 6.8690658Losses:  6.218718528747559 0.3293956220149994
CurrentTrain: epoch  3, batch    14 | loss: 6.8775096Losses:  6.06077241897583 0.349430114030838
CurrentTrain: epoch  3, batch    15 | loss: 6.7596326Losses:  5.9640069007873535 0.3146662712097168
CurrentTrain: epoch  3, batch    16 | loss: 6.5933394Losses:  5.775601387023926 0.285775363445282
CurrentTrain: epoch  3, batch    17 | loss: 6.3471522Losses:  5.988246917724609 0.3689438998699188
CurrentTrain: epoch  3, batch    18 | loss: 6.7261348Losses:  6.2406439781188965 0.5163078308105469
CurrentTrain: epoch  3, batch    19 | loss: 7.2732596Losses:  5.812594413757324 0.4205035865306854
CurrentTrain: epoch  3, batch    20 | loss: 6.6536016Losses:  5.7188873291015625 0.2763559818267822
CurrentTrain: epoch  3, batch    21 | loss: 6.2715993Losses:  5.9291582107543945 0.3876657485961914
CurrentTrain: epoch  3, batch    22 | loss: 6.7044897Losses:  5.677262306213379 0.2898864448070526
CurrentTrain: epoch  3, batch    23 | loss: 6.2570353Losses:  5.698738098144531 0.3481789827346802
CurrentTrain: epoch  3, batch    24 | loss: 6.3950958Losses:  5.999029159545898 0.3515324592590332
CurrentTrain: epoch  3, batch    25 | loss: 6.7020941Losses:  6.091545104980469 0.36556458473205566
CurrentTrain: epoch  3, batch    26 | loss: 6.8226743Losses:  5.558307647705078 0.2964108884334564
CurrentTrain: epoch  3, batch    27 | loss: 6.1511292Losses:  5.949605941772461 0.2750190198421478
#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hardprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.211234092712402 1.7084617614746094
CurrentTrain: epoch  0, batch     0 | loss: 13.0654650Losses:  14.570542335510254 1.6962032318115234
CurrentTrain: epoch  0, batch     1 | loss: 15.4186440Losses:  15.06650161743164 1.6630806922912598
CurrentTrain: epoch  0, batch     2 | loss: 15.8980417Losses:  14.96859073638916 1.8376294374465942
CurrentTrain: epoch  0, batch     3 | loss: 15.8874054Losses:  14.7833251953125 1.8825676441192627
CurrentTrain: epoch  0, batch     4 | loss: 15.7246094Losses:  14.388586044311523 1.7644509077072144
CurrentTrain: epoch  0, batch     5 | loss: 15.2708111Losses:  14.050195693969727 1.4534282684326172
CurrentTrain: epoch  0, batch     6 | loss: 14.7769098Losses:  14.127151489257812 1.5595853328704834
CurrentTrain: epoch  0, batch     7 | loss: 14.9069443Losses:  13.844810485839844 1.5951926708221436
CurrentTrain: epoch  0, batch     8 | loss: 14.6424065Losses:  13.623236656188965 1.3678910732269287
CurrentTrain: epoch  0, batch     9 | loss: 14.3071823Losses:  13.171847343444824 1.4879361391067505
CurrentTrain: epoch  0, batch    10 | loss: 13.9158154Losses:  12.91122817993164 1.3616546392440796
CurrentTrain: epoch  0, batch    11 | loss: 13.5920553Losses:  12.485208511352539 1.4425989389419556
CurrentTrain: epoch  0, batch    12 | loss: 13.2065077Losses:  12.839349746704102 1.780390739440918
CurrentTrain: epoch  0, batch    13 | loss: 13.7295456Losses:  12.517465591430664 1.7687804698944092
CurrentTrain: epoch  0, batch    14 | loss: 13.4018555Losses:  12.793590545654297 1.6175183057785034
CurrentTrain: epoch  0, batch    15 | loss: 13.6023493Losses:  12.596155166625977 1.7276973724365234
CurrentTrain: epoch  0, batch    16 | loss: 13.4600039Losses:  12.111052513122559 1.548889398574829
CurrentTrain: epoch  0, batch    17 | loss: 12.8854971Losses:  12.230708122253418 1.5324065685272217
CurrentTrain: epoch  0, batch    18 | loss: 12.9969110Losses:  11.744256019592285 1.4382245540618896
CurrentTrain: epoch  0, batch    19 | loss: 12.4633684Losses:  11.872535705566406 1.5517593622207642
CurrentTrain: epoch  0, batch    20 | loss: 12.6484156Losses:  11.48039436340332 1.4820384979248047
CurrentTrain: epoch  0, batch    21 | loss: 12.2214136Losses:  10.711366653442383 1.383833885192871
CurrentTrain: epoch  0, batch    22 | loss: 11.4032841Losses:  10.971739768981934 1.4958728551864624
CurrentTrain: epoch  0, batch    23 | loss: 11.7196760Losses:  10.673845291137695 1.3836086988449097
CurrentTrain: epoch  0, batch    24 | loss: 11.3656492Losses:  10.156961441040039 1.394775390625
CurrentTrain: epoch  0, batch    25 | loss: 10.8543491Losses:  9.986894607543945 1.4202563762664795
CurrentTrain: epoch  0, batch    26 | loss: 10.6970224Losses:  9.81794548034668 1.4680931568145752
CurrentTrain: epoch  0, batch    27 | loss: 10.5519924Losses:  9.619546890258789 1.445547342300415
CurrentTrain: epoch  0, batch    28 | loss: 10.3423204Losses:  9.318183898925781 1.2284787893295288
CurrentTrain: epoch  0, batch    29 | loss: 9.9324236Losses:  9.49625301361084 1.502380132675171
CurrentTrain: epoch  0, batch    30 | loss: 10.2474432Losses:  9.102161407470703 1.4082648754119873
CurrentTrain: epoch  0, batch    31 | loss: 9.8062935Losses:  8.8463773727417 1.359487771987915
CurrentTrain: epoch  0, batch    32 | loss: 9.5261211Losses:  9.125592231750488 1.5811169147491455
CurrentTrain: epoch  0, batch    33 | loss: 9.9161510Losses:  8.451828956604004 1.1460609436035156
CurrentTrain: epoch  0, batch    34 | loss: 9.0248594Losses:  8.073752403259277 1.273296594619751
CurrentTrain: epoch  0, batch    35 | loss: 8.7104006Losses:  7.943498611450195 1.2824387550354004
CurrentTrain: epoch  0, batch    36 | loss: 8.5847178Losses:  7.730396270751953 0.8038426637649536
CurrentTrain: epoch  0, batch    37 | loss: 8.1323175Losses:  7.8270158767700195 1.5249748229980469
CurrentTrain: epoch  1, batch     0 | loss: 8.5895033Losses:  7.650745391845703 1.2205287218093872
CurrentTrain: epoch  1, batch     1 | loss: 8.2610102Losses:  7.810871124267578 1.1551861763000488
CurrentTrain: epoch  1, batch     2 | loss: 8.3884640Losses:  7.688255786895752 1.3317515850067139
CurrentTrain: epoch  1, batch     3 | loss: 8.3541317Losses:  7.624208450317383 1.255748987197876
CurrentTrain: epoch  1, batch     4 | loss: 8.2520828Losses:  7.487800598144531 1.394040584564209
CurrentTrain: epoch  1, batch     5 | loss: 8.1848211Losses:  7.268898010253906 1.2959027290344238
CurrentTrain: epoch  1, batch     6 | loss: 7.9168491Losses:  7.577559947967529 1.2417422533035278
CurrentTrain: epoch  1, batch     7 | loss: 8.1984310Losses:  7.7468109130859375 1.3058756589889526
CurrentTrain: epoch  1, batch     8 | loss: 8.3997488Losses:  7.441213607788086 1.2856085300445557
CurrentTrain: epoch  1, batch     9 | loss: 8.0840178Losses:  7.299350261688232 1.2328083515167236
CurrentTrain: epoch  1, batch    10 | loss: 7.9157543Losses:  7.53406286239624 1.1325565576553345
CurrentTrain: epoch  1, batch    11 | loss: 8.1003408Losses:  7.175724506378174 1.236829161643982
CurrentTrain: epoch  1, batch    12 | loss: 7.7941389Losses:  7.699954986572266 1.3204691410064697
CurrentTrain: epoch  1, batch    13 | loss: 8.3601894Losses:  7.190544605255127 1.2410610914230347
CurrentTrain: epoch  1, batch    14 | loss: 7.8110752Losses:  7.348072528839111 1.1554826498031616
CurrentTrain: epoch  1, batch    15 | loss: 7.9258137Losses:  7.087444305419922 1.2148334980010986
CurrentTrain: epoch  1, batch    16 | loss: 7.6948609Losses:  6.889645576477051 1.1511971950531006
CurrentTrain: epoch  1, batch    17 | loss: 7.4652443Losses:  6.973304271697998 1.2317259311676025
CurrentTrain: epoch  1, batch    18 | loss: 7.5891671Losses:  7.325989246368408 1.3917672634124756
CurrentTrain: epoch  1, batch    19 | loss: 8.0218725Losses:  6.711918354034424 1.0144309997558594
CurrentTrain: epoch  1, batch    20 | loss: 7.2191339Losses:  6.956860065460205 0.9644240140914917
CurrentTrain: epoch  1, batch    21 | loss: 7.4390721Losses:  7.033292770385742 0.9996349811553955
CurrentTrain: epoch  1, batch    22 | loss: 7.5331101Losses:  7.359289646148682 1.097365379333496
CurrentTrain: epoch  1, batch    23 | loss: 7.9079723Losses:  7.347617149353027 1.0509836673736572
CurrentTrain: epoch  1, batch    24 | loss: 7.8731089Losses:  7.370314598083496 1.0687193870544434
CurrentTrain: epoch  1, batch    25 | loss: 7.9046745Losses:  6.809694290161133 1.0393399000167847
CurrentTrain: epoch  1, batch    26 | loss: 7.3293643Losses:  6.94901180267334 1.2950267791748047
CurrentTrain: epoch  1, batch    27 | loss: 7.5965252Losses:  6.829732894897461 1.1090482473373413
CurrentTrain: epoch  1, batch    28 | loss: 7.3842568Losses:  6.663043975830078 0.8666461706161499
CurrentTrain: epoch  1, batch    29 | loss: 7.0963669Losses:  6.995729446411133 1.0170269012451172
CurrentTrain: epoch  1, batch    30 | loss: 7.5042429Losses:  6.916308403015137 1.1900827884674072
CurrentTrain: epoch  1, batch    31 | loss: 7.5113497Losses:  6.601003646850586 1.0238239765167236
CurrentTrain: epoch  1, batch    32 | loss: 7.1129155Losses:  6.520966529846191 0.9709702730178833
CurrentTrain: epoch  1, batch    33 | loss: 7.0064516Losses:  6.857132911682129 1.0292890071868896
CurrentTrain: epoch  1, batch    34 | loss: 7.3717775Losses:  6.897306442260742 1.127906084060669
CurrentTrain: epoch  1, batch    35 | loss: 7.4612594Losses:  6.9129438400268555 1.0666186809539795
CurrentTrain: epoch  1, batch    36 | loss: 7.4462533Losses:  6.712950706481934 0.3937045633792877
CurrentTrain: epoch  1, batch    37 | loss: 6.9098029Losses:  6.62269401550293 0.9841067790985107
CurrentTrain: epoch  2, batch     0 | loss: 7.1147475Losses:  6.543026924133301 0.8909566402435303
CurrentTrain: epoch  2, batch     1 | loss: 6.9885054Losses:  5.942367076873779 0.8651716113090515
CurrentTrain: epoch  2, batch     2 | loss: 6.3749528Losses:  6.0244927406311035 0.9316263198852539
CurrentTrain: epoch  2, batch     3 | loss: 6.4903059Losses:  7.215847492218018 0.7461651563644409
CurrentTrain: epoch  2, batch     4 | loss: 7.5889301Losses:  6.372796535491943 0.9741562604904175
CurrentTrain: epoch  2, batch     5 | loss: 6.8598747Losses:  6.406038284301758 0.9635324478149414
CurrentTrain: epoch  2, batch     6 | loss: 6.8878045Losses:  6.170984268188477 0.9429062008857727
CurrentTrain: epoch  2, batch     7 | loss: 6.6424375Losses:  6.570430278778076 0.9383398294448853
CurrentTrain: epoch  2, batch     8 | loss: 7.0396004Losses:  6.39626407623291 0.7329879403114319
CurrentTrain: epoch  2, batch     9 | loss: 6.7627583Losses:  6.290854454040527 0.7038379907608032
CurrentTrain: epoch  2, batch    10 | loss: 6.6427736Losses:  6.38325309753418 0.8903313279151917
CurrentTrain: epoch  2, batch    11 | loss: 6.8284187Losses:  5.973184108734131 0.7084800601005554
CurrentTrain: epoch  2, batch    12 | loss: 6.3274240Losses:  6.337393760681152 0.9361869692802429
CurrentTrain: epoch  2, batch    13 | loss: 6.8054872Losses:  6.959022521972656 0.9087015986442566
CurrentTrain: epoch  2, batch    14 | loss: 7.4133735Losses:  6.66984224319458 0.7137404084205627
CurrentTrain: epoch  2, batch    15 | loss: 7.0267124Losses:  6.659171104431152 0.8354681730270386
CurrentTrain: epoch  2, batch    16 | loss: 7.0769053Losses:  6.187087535858154 0.8113876581192017
CurrentTrain: epoch  2, batch    17 | loss: 6.5927815Losses:  6.09822940826416 0.8185300827026367
CurrentTrain: epoch  2, batch    18 | loss: 6.5074944Losses:  6.179858207702637 0.8036648035049438
CurrentTrain: epoch  2, batch    19 | loss: 6.5816908Losses:  5.858524322509766 0.6252009868621826
CurrentTrain: epoch  2, batch    20 | loss: 6.1711249Losses:  6.218798637390137 0.8682779669761658
CurrentTrain: epoch  2, batch    21 | loss: 6.6529374Losses:  6.508336067199707 0.9023338556289673
CurrentTrain: epoch  2, batch    22 | loss: 6.9595032Losses:  6.518857479095459 0.865114688873291
CurrentTrain: epoch  2, batch    23 | loss: 6.9514151Losses:  6.383453845977783 0.891488790512085
CurrentTrain: epoch  2, batch    24 | loss: 6.8291984Losses:  6.488055229187012 0.8526337742805481
CurrentTrain: epoch  2, batch    25 | loss: 6.9143720Losses:  6.8373212814331055 0.876464307308197
CurrentTrain: epoch  2, batch    26 | loss: 7.2755532Losses:  6.137946128845215 0.7286689281463623
CurrentTrain: epoch  2, batch    27 | loss: 6.5022807Losses:  6.850337505340576 0.900926411151886
CurrentTrain: epoch  2, batch    28 | loss: 7.3008008Losses:  6.20283842086792 0.8176357746124268
CurrentTrain: epoch  2, batch    29 | loss: 6.6116562Losses:  6.213940620422363 0.7257880568504333
CurrentTrain: epoch  2, batch    30 | loss: 6.5768347Losses:  6.6519775390625 0.7899823188781738
CurrentTrain: epoch  2, batch    31 | loss: 7.0469685Losses:  6.281861305236816 0.7587400674819946
CurrentTrain: epoch  2, batch    32 | loss: 6.6612315Losses:  5.998401165008545 0.767353892326355
CurrentTrain: epoch  2, batch    33 | loss: 6.3820782Losses:  6.339864730834961 0.8586210608482361
CurrentTrain: epoch  2, batch    34 | loss: 6.7691751Losses:  6.43612003326416 0.7078114748001099
CurrentTrain: epoch  2, batch    35 | loss: 6.7900257Losses:  6.036134243011475 0.7810471057891846
CurrentTrain: epoch  2, batch    36 | loss: 6.4266577Losses:  6.1038079261779785 0.6174935102462769
CurrentTrain: epoch  2, batch    37 | loss: 6.4125547Losses:  5.905229568481445 0.74916672706604
CurrentTrain: epoch  3, batch     0 | loss: 6.2798128Losses:  5.911188125610352 0.6420609354972839
CurrentTrain: epoch  3, batch     1 | loss: 6.2322187Losses:  6.160517692565918 0.735450029373169
CurrentTrain: epoch  3, batch     2 | loss: 6.5282426Losses:  6.105128288269043 0.8172702193260193
CurrentTrain: epoch  3, batch     3 | loss: 6.5137634Losses:  5.693934440612793 0.6135600805282593
CurrentTrain: epoch  3, batch     4 | loss: 6.0007143Losses:  6.165541648864746 0.807422399520874
CurrentTrain: epoch  3, batch     5 | loss: 6.5692530Losses:  5.606662273406982 0.7259811162948608
CurrentTrain: epoch  3, batch     6 | loss: 5.9696527Losses:  5.884099006652832 0.7002308368682861
CurrentTrain: epoch  3, batch     7 | loss: 6.2342143Losses:  5.817254066467285 0.6578460931777954
CurrentTrain: epoch  3, batch     8 | loss: 6.1461773Losses:  5.792640686035156 0.5159739851951599
CurrentTrain: epoch  3, batch     9 | loss: 6.0506277Losses:  5.746910095214844 0.7168023586273193
CurrentTrain: epoch  3, batch    10 | loss: 6.1053114Losses:  5.7795000076293945 0.6202222108840942
CurrentTrain: epoch  3, batch    11 | loss: 6.0896111Losses:  5.850586414337158 0.7038196325302124
CurrentTrain: epoch  3, batch    12 | loss: 6.2024961Losses:  5.802628040313721 0.6317688226699829
CurrentTrain: epoch  3, batch    13 | loss: 6.1185126Losses:  5.6249895095825195 0.6190206408500671
CurrentTrain: epoch  3, batch    14 | loss: 5.9344997Losses:  5.968517303466797 0.6761018633842468
CurrentTrain: epoch  3, batch    15 | loss: 6.3065681Losses:  5.510501384735107 0.5653991103172302
CurrentTrain: epoch  3, batch    16 | loss: 5.7932010Losses:  6.054366111755371 0.6548638343811035
CurrentTrain: epoch  3, batch    17 | loss: 6.3817978Losses:  5.836250305175781 0.5743312835693359
CurrentTrain: epoch  3, batch    18 | loss: 6.1234159Losses:  5.856491565704346 0.766800045967102
CurrentTrain: epoch  3, batch    19 | loss: 6.2398915Losses:  5.43845272064209 0.521053671836853
CurrentTrain: epoch  3, batch    20 | loss: 5.6989794Losses:  5.829614639282227 0.47574833035469055
CurrentTrain: epoch  3, batch    21 | loss: 6.0674887Losses:  5.8977155685424805 0.7741104960441589
CurrentTrain: epoch  3, batch    22 | loss: 6.2847710Losses:  5.697256088256836 0.5750355124473572
CurrentTrain: epoch  3, batch    23 | loss: 5.9847736Losses:  5.549691200256348 0.5190831422805786
CurrentTrain: epoch  3, batch    24 | loss: 5.8092327Losses:  5.829236030578613 0.5180071592330933
CurrentTrain: epoch  3, batch    25 | loss: 6.0882397Losses:  5.897497177124023 0.6882176399230957
CurrentTrain: epoch  3, batch    26 | loss: 6.2416058Losses:  5.515739440917969 0.6989796161651611
CurrentTrain: epoch  3, batch    27 | loss: 5.8652291Losses:  5.89512825012207 0.6808366775512695
CurrentTrain: epoch  3, batch    28 | loss: 6.2355466Losses:  6.018604278564453 0.6857116222381592
CurrentTrain: epoch  3, batch    29 | loss: 6.3614602Losses:  6.246299743652344 0.4805081784725189
CurrentTrain: epoch  3, batch    30 | loss: 6.4865537Losses:  6.6464080810546875 0.6285989880561829
CurrentTrain: epoch  3, batch    31 | loss: 6.9607077Losses:  5.893117904663086 0.6015799641609192
CurrentTrain: epoch  3, batch    32 | loss: 6.1939077Losses:  6.042591571807861 0.6158833503723145
CurrentTrain: epoch  3, batch    33 | loss: 6.3505335Losses:  5.992928504943848 0.5301851034164429
CurrentTrain: epoch  3, batch    34 | loss: 6.2580209Losses:  5.505147933959961 0.44948703050613403
CurrentTrain: epoch  3, batch    35 | loss: 5.7298913Losses:  5.712087154388428 0.6643539667129517
CurrentTrain: epoch  3, batch    36 | loss: 6.0442643Losses:  4.920137405395508 0.2868765592575073
CurrentTrain: epoch  3, batch    37 | loss: 5.0635757Losses:  5.373750686645508 0.572669506072998
CurrentTrain: epoch  4, batch     0 | loss: 5.6600857Losses:  6.24697732925415 0.665245532989502
CurrentTrain: epoch  4, batch     1 | loss: 6.5796003Losses:  5.609073638916016 0.5330098271369934
CurrentTrain: epoch  4, batch     2 | loss: 5.8755784Losses:  6.120089530944824 0.6578056812286377
CurrentTrain: epoch  4, batch     3 | loss: 6.4489923Losses:  5.760166645050049 0.607198178768158
CurrentTrain: epoch  4, batch     4 | loss: 6.0637655Losses:  5.328060150146484 0.49580198526382446
CurrentTrain: epoch  4, batch     5 | loss: 5.5759611Losses:  5.433297157287598 0.5482140779495239
CurrentTrain: epoch  4, batch     6 | loss: 5.7074041Losses:  5.219788074493408 0.47841954231262207
CurrentTrain: epoch  4, batch     7 | loss: 5.4589977Losses:  5.570662498474121 0.4618784189224243
CurrentTrain: epoch  4, batch     8 | loss: 5.8016019Losses:  5.321840286254883 0.4421808421611786
CurrentTrain: epoch  4, batch     9 | loss: 5.5429306Losses:  5.760851860046387 0.5390810966491699
CurrentTrain: epoch  4, batch    10 | loss: 6.0303926Losses:  5.304880619049072 0.47380688786506653
CurrentTrain: epoch  4, batch    11 | loss: 5.5417843Losses:  5.386407852172852 0.4612461030483246
CurrentTrain: epoch  4, batch    12 | loss: 5.6170311Losses:  5.727514266967773 0.4335235357284546
CurrentTrain: epoch  4, batch    13 | loss: 5.9442759Losses:  5.4059648513793945 0.4495028257369995
CurrentTrain: epoch  4, batch    14 | loss: 5.6307163Losses:  5.866474151611328 0.6180670261383057
CurrentTrain: epoch  4, batch    15 | loss: 6.1755075Losses:  5.613768100738525 0.5414015054702759
CurrentTrain: epoch  4, batch    16 | loss: 5.8844690Losses:  5.344062805175781 0.4701407551765442
CurrentTrain: epoch  4, batch    17 | loss: 5.5791330Losses:  5.363189697265625 0.4474361538887024
CurrentTrain: epoch  4, batch    18 | loss: 5.5869079Losses:  5.324042797088623 0.4322843849658966
CurrentTrain: epoch  4, batch    19 | loss: 5.5401850Losses:  5.5611572265625 0.5080727338790894
CurrentTrain: epoch  4, batch    20 | loss: 5.8151937Losses:  5.395668029785156 0.39118722081184387
CurrentTrain: epoch  4, batch    21 | loss: 5.5912619Losses:  5.981185436248779 0.5969113111495972
CurrentTrain: epoch  4, batch    22 | loss: 6.2796412Losses:  5.319169998168945 0.5006541013717651
CurrentTrain: epoch  4, batch    23 | loss: 5.5694971Losses:  5.599594593048096 0.4618704915046692
CurrentTrain: epoch  4, batch    24 | loss: 5.8305297Losses:  5.390213966369629 0.3688310384750366
CurrentTrain: epoch  4, batch    25 | loss: 5.5746293Losses:  6.062131881713867 0.6887273788452148
CurrentTrain: epoch  4, batch    26 | loss: 6.4064956Losses:  5.413792610168457 0.4451419711112976
CurrentTrain: epoch  4, batch    27 | loss: 5.6363635Losses:  5.200664043426514 0.4370032846927643
CurrentTrain: epoch  4, batch    28 | loss: 5.4191656Losses:  4.925116539001465 0.357148677110672
CurrentTrain: epoch  4, batch    29 | loss: 5.1036911Losses:  5.979778289794922 0.4404783844947815
CurrentTrain: epoch  4, batch    30 | loss: 6.2000175Losses:  5.211989879608154 0.41912055015563965
CurrentTrain: epoch  4, batch    31 | loss: 5.4215503Losses:  5.570554256439209 0.4172021448612213
CurrentTrain: epoch  4, batch    32 | loss: 5.7791553Losses:  5.258700370788574 0.5248241424560547
CurrentTrain: epoch  4, batch    33 | loss: 5.5211124Losses:  5.78270959854126 0.5986037254333496
CurrentTrain: epoch  4, batch    34 | loss: 6.0820112Losses:  5.962089538574219 0.6013784408569336
CurrentTrain: epoch  4, batch    35 | loss: 6.2627788Losses:  5.091091156005859 0.3721024990081787
CurrentTrain: epoch  4, batch    36 | loss: 5.2771425Losses:  5.172433376312256 0.3840159773826599
CurrentTrain: epoch  4, batch    37 | loss: 5.3644414Losses:  4.9438157081604 0.3744727373123169
CurrentTrain: epoch  5, batch     0 | loss: 5.1310520Losses:  5.231444358825684 0.2884080111980438
CurrentTrain: epoch  5, batch     1 | loss: 5.3756485Losses:  5.267605304718018 0.37182098627090454
CurrentTrain: epoch  5, batch     2 | loss: 5.4535160Losses:  5.795188903808594 0.45594650506973267
CurrentTrain: epoch  5, batch     3 | loss: 6.0231624Losses:  5.376169204711914 0.3831339478492737
CurrentTrain: epoch  5, batch     4 | loss: 5.5677361Losses:  5.539205551147461 0.43593186140060425
CurrentTrain: epoch  5, batch     5 | loss: 5.7571716Losses:  5.278634071350098 0.39669933915138245
CurrentTrain: epoch  5, batch     6 | loss: 5.4769835Losses:  5.008429050445557 0.3491542339324951
CurrentTrain: epoch  5, batch     7 | loss: 5.1830063Losses:  5.2360334396362305 0.4248661994934082
CurrentTrain: epoch  5, batch     8 | loss: 5.4484663Losses:  4.944417953491211 0.19012394547462463
CurrentTrain: epoch  5, batch     9 | loss: 5.0394797Losses:  5.337587356567383 0.4414598345756531
CurrentTrain: epoch  5, batch    10 | loss: 5.5583172Losses:  5.330781936645508 0.394595205783844
CurrentTrain: epoch  5, batch    11 | loss: 5.5280795Losses:  5.633286952972412 0.515105128288269
CurrentTrain: epoch  5, batch    12 | loss: 5.8908396Losses:  5.313234806060791 0.44021350145339966
CurrentTrain: epoch  5, batch    13 | loss: 5.5333414Losses:  5.126101493835449 0.36066967248916626
CurrentTrain: epoch  5, batch    14 | loss: 5.3064365Losses:  5.393629550933838 0.3776254951953888
CurrentTrain: epoch  5, batch    15 | loss: 5.5824423Losses:  5.468360900878906 0.35190749168395996
CurrentTrain: epoch  5, batch    16 | loss: 5.6443148Losses:  5.309784889221191 0.370981901884079
CurrentTrain: epoch  5, batch    17 | loss: 5.4952760Losses:  5.261754989624023 0.3066461682319641
CurrentTrain: epoch  5, batch    18 | loss: 5.4150782Losses:  4.976864337921143 0.3106616139411926
CurrentTrain: epoch  5, batch    19 | loss: 5.1321950Losses:  5.420991897583008 0.35411155223846436
CurrentTrain: epoch  5, batch    20 | loss: 5.5980477Losses:  5.3662519454956055 0.3795826733112335
CurrentTrain: epoch  5, batch    21 | loss: 5.5560431Losses:  4.925118923187256 0.366169810295105
CurrentTrain: epoch  5, batch    22 | loss: 5.1082039Losses:  5.467681884765625 0.41369324922561646
CurrentTrain: epoch  5, batch    23 | loss: 5.6745286Losses:  5.477823257446289 0.4437139928340912
CurrentTrain: epoch  5, batch    24 | loss: 5.6996803Losses:  5.473642349243164 0.5046185255050659
CurrentTrain: epoch  5, batch    25 | loss: 5.7259517Losses:  5.059123992919922 0.3152492344379425
CurrentTrain: epoch  5, batch    26 | loss: 5.2167487Losses:  5.066390037536621 0.3227538466453552
CurrentTrain: epoch  5, batch    27 | loss: 5.2277670Losses:  5.343228816986084 0.4571138322353363
CurrentTrain: epoch  5, batch    28 | loss: 5.5717859Losses:  5.225271224975586 0.3667104244232178
CurrentTrain: epoch  5, batch    29 | loss: 5.4086266Losses:  5.008491039276123 0.39123982191085815
CurrentTrain: epoch  5, batch    30 | loss: 5.2041111Losses:  5.547910690307617 0.5103346109390259
CurrentTrain: epoch  5, batch    31 | loss: 5.8030782Losses:  4.736705303192139 0.32267093658447266
CurrentTrain: epoch  5, batch    32 | loss: 4.8980408Losses:  5.357944965362549 0.40611881017684937
CurrentTrain: epoch  5, batch    33 | loss: 5.5610042Losses:  5.320920944213867 0.3652574419975281
CurrentTrain: epoch  5, batch    34 | loss: 5.5035496Losses:  5.329754829406738 0.42708688974380493
CurrentTrain: epoch  5, batch    35 | loss: 5.5432982Losses:  4.91144323348999 0.22287872433662415
CurrentTrain: epoch  5, batch    36 | loss: 5.0228825Losses:  5.38323974609375 0.25325068831443787
CurrentTrain: epoch  5, batch    37 | loss: 5.5098653Losses:  5.049482345581055 0.31925198435783386
CurrentTrain: epoch  6, batch     0 | loss: 5.2091084Losses:  5.014657974243164 0.3409819006919861
CurrentTrain: epoch  6, batch     1 | loss: 5.1851487Losses:  5.123660087585449 0.41305381059646606
CurrentTrain: epoch  6, batch     2 | loss: 5.3301868Losses:  4.996829986572266 0.31363046169281006
CurrentTrain: epoch  6, batch     3 | loss: 5.1536450Losses:  4.881405353546143 0.2908824682235718
CurrentTrain: epoch  6, batch     4 | loss: 5.0268464Losses:  5.011045455932617 0.36407214403152466
CurrentTrain: epoch  6, batch     5 | loss: 5.1930814Losses:  5.196606159210205 0.35511934757232666
CurrentTrain: epoch  6, batch     6 | loss: 5.3741660Losses:  4.867428779602051 0.30664730072021484
CurrentTrain: epoch  6, batch     7 | loss: 5.0207524Losses:  5.440817832946777 0.38318175077438354
CurrentTrain: epoch  6, batch     8 | loss: 5.6324086Losses:  4.915814399719238 0.36382633447647095
CurrentTrain: epoch  6, batch     9 | loss: 5.0977278Losses:  5.123569965362549 0.3652276396751404
CurrentTrain: epoch  6, batch    10 | loss: 5.3061838Losses:  4.985130310058594 0.33146846294403076
CurrentTrain: epoch  6, batch    11 | loss: 5.1508646Losses:  4.787468433380127 0.26863768696784973
CurrentTrain: epoch  6, batch    12 | loss: 4.9217873Losses:  4.930731773376465 0.25014373660087585
CurrentTrain: epoch  6, batch    13 | loss: 5.0558038Losses:  5.153406620025635 0.3116511106491089
CurrentTrain: epoch  6, batch    14 | loss: 5.3092322Losses:  5.150675296783447 0.1899052858352661
CurrentTrain: epoch  6, batch    15 | loss: 5.2456279Losses:  4.879360675811768 0.24363218247890472
CurrentTrain: epoch  6, batch    16 | loss: 5.0011768Losses:  4.974297523498535 0.21537469327449799
CurrentTrain: epoch  6, batch    17 | loss: 5.0819850Losses:  5.119399070739746 0.27180659770965576
CurrentTrain: epoch  6, batch    18 | loss: 5.2553024Losses:  5.342092037200928 0.39572644233703613
CurrentTrain: epoch  6, batch    19 | loss: 5.5399551Losses:  4.940096855163574 0.28479355573654175
CurrentTrain: epoch  6, batch    20 | loss: 5.0824938Losses:  4.970276355743408 0.25469791889190674
CurrentTrain: epoch  6, batch    21 | loss: 5.0976253Losses:  5.153107643127441 0.41717106103897095
CurrentTrain: epoch  6, batch    22 | loss: 5.3616934Losses:  4.857954025268555 0.24461384117603302
CurrentTrain: epoch  6, batch    23 | loss: 4.9802608Losses:  5.255379676818848 0.48521673679351807
CurrentTrain: epoch  6, batch    24 | loss: 5.4979882Losses:  4.7749128341674805 0.2673681080341339
CurrentTrain: epoch  6, batch    25 | loss: 4.9085970Losses:  5.185125827789307 0.5023772716522217
CurrentTrain: epoch  6, batch    26 | loss: 5.4363146Losses:  4.766268730163574 0.17656399309635162
CurrentTrain: epoch  6, batch    27 | loss: 4.8545508Losses:  5.063366889953613 0.24426689743995667
CurrentTrain: epoch  6, batch    28 | loss: 5.1855001Losses:  4.832732200622559 0.3230670392513275
CurrentTrain: epoch  6, batch    29 | loss: 4.9942656Losses:  4.821090221405029 0.221415176987648
CurrentTrain: epoch  6, batch    30 | loss: 4.9317980Losses:  5.310559272766113 0.3458823561668396
CurrentTrain: epoch  6, batch    31 | loss: 5.4835005Losses:  5.043684005737305 0.3269146978855133
CurrentTrain: epoch  6, batch    32 | loss: 5.2071414Losses:  5.1099443435668945 0.3869938254356384
CurrentTrain: epoch  6, batch    33 | loss: 5.3034410Losses:  5.10521936416626 0.3677353262901306
CurrentTrain: epoch  6, batch    34 | loss: 5.2890868Losses:  4.673966407775879 0.20392590761184692
CurrentTrain: epoch  6, batch    35 | loss: 4.7759295Losses:  5.051583290100098 0.2406509667634964
CurrentTrain: epoch  6, batch    36 | loss: 5.1719089Losses:  5.259021759033203 0.36545974016189575
CurrentTrain: epoch  6, batch    37 | loss: 5.4417515Losses:  4.755082607269287 0.2293262779712677
CurrentTrain: epoch  7, batch     0 | loss: 4.8697457Losses:  5.058260440826416 0.2514714300632477
CurrentTrain: epoch  7, batch     1 | loss: 5.1839962Losses:  4.994068145751953 0.31105291843414307
CurrentTrain: epoch  7, batch     2 | loss: 5.1495948Losses:  4.690573692321777 0.22532156109809875
CurrentTrain: epoch  7, batch     3 | loss: 4.8032346Losses:  4.869902610778809 0.2828390896320343
CurrentTrain: epoch  7, batch     4 | loss: 5.0113220Losses:  4.769510269165039 0.22072646021842957
CurrentTrain: epoch  7, batch     5 | loss: 4.8798733Losses:  5.1656999588012695 0.36103391647338867
CurrentTrain: epoch  7, batch     6 | loss: 5.3462172Losses:  4.704080104827881 0.2197767198085785
CurrentTrain: epoch  7, batch     7 | loss: 4.8139687Losses:  5.418507099151611 0.34731438755989075
CurrentTrain: epoch  7, batch     8 | loss: 5.5921645Losses:  4.849348068237305 0.23154260218143463
CurrentTrain: epoch  7, batch     9 | loss: 4.9651194Losses:  4.859644889831543 0.2811068296432495
CurrentTrain: epoch  7, batch    10 | loss: 5.0001984Losses:  4.883184432983398 0.30033862590789795
CurrentTrain: epoch  7, batch    11 | loss: 5.0333538Losses:  4.714540004730225 0.25008735060691833
CurrentTrain: epoch  7, batch    12 | loss: 4.8395839Losses:  4.8524346351623535 0.2467317134141922
CurrentTrain: epoch  7, batch    13 | loss: 4.9758005Losses:  4.880128383636475 0.27255305647850037
CurrentTrain: epoch  7, batch    14 | loss: 5.0164051Losses:  4.781675338745117 0.2624988853931427
CurrentTrain: epoch  7, batch    15 | loss: 4.9129248Losses:  4.871837615966797 0.21739305555820465
CurrentTrain: epoch  7, batch    16 | loss: 4.9805341Losses:  4.696369171142578 0.2376488298177719
CurrentTrain: epoch  7, batch    17 | loss: 4.8151937Losses:  4.8729047775268555 0.21028605103492737
CurrentTrain: epoch  7, batch    18 | loss: 4.9780478Losses:  4.84127140045166 0.26434606313705444
CurrentTrain: epoch  7, batch    19 | loss: 4.9734445Losses:  4.820228576660156 0.21156719326972961
CurrentTrain: epoch  7, batch    20 | loss: 4.9260120Losses:  4.714327812194824 0.24100160598754883
CurrentTrain: epoch  7, batch    21 | loss: 4.8348284Losses:  4.755993843078613 0.1509474217891693
CurrentTrain: epoch  7, batch    22 | loss: 4.8314676Losses:  4.950762748718262 0.3108491599559784
CurrentTrain: epoch  7, batch    23 | loss: 5.1061873Losses:  5.172140121459961 0.27674931287765503
CurrentTrain: epoch  7, batch    24 | loss: 5.3105149Losses:  5.411405563354492 0.5230056643486023
CurrentTrain: epoch  7, batch    25 | loss: 5.6729083Losses:  4.6486592292785645 0.2181728482246399
CurrentTrain: epoch  7, batch    26 | loss: 4.7577457Losses:  5.045201301574707 0.24693620204925537
CurrentTrain: epoch  7, batch    27 | loss: 5.1686692Losses:  5.091695785522461 0.3109859228134155
CurrentTrain: epoch  7, batch    28 | loss: 5.2471886Losses:  4.694878578186035 0.21770305931568146
CurrentTrain: epoch  7, batch    29 | loss: 4.8037300Losses:  4.969700336456299 0.3065812587738037
CurrentTrain: epoch  7, batch    30 | loss: 5.1229911Losses:  4.721492767333984 0.24410422146320343
CurrentTrain: epoch  7, batch    31 | loss: 4.8435450Losses:  4.648465633392334 0.23382557928562164
CurrentTrain: epoch  7, batch    32 | loss: 4.7653785Losses:  4.704819679260254 0.16181688010692596
CurrentTrain: epoch  7, batch    33 | loss: 4.7857280Losses:  5.0457353591918945 0.2717675566673279
CurrentTrain: epoch  7, batch    34 | loss: 5.1816192Losses:  4.705820560455322 0.2158505618572235
CurrentTrain: epoch  7, batch    35 | loss: 4.8137460Losses:  4.718637943267822 0.21916860342025757
CurrentTrain: epoch  7, batch    36 | loss: 4.8282223Losses:  5.146712303161621 0.2787657678127289
CurrentTrain: epoch  7, batch    37 | loss: 5.2860951Losses:  4.649473667144775 0.1714535355567932
CurrentTrain: epoch  8, batch     0 | loss: 4.7352004Losses:  4.690542221069336 0.23991741240024567
CurrentTrain: epoch  8, batch     1 | loss: 4.8105011Losses:  4.689601421356201 0.20526215434074402
CurrentTrain: epoch  8, batch     2 | loss: 4.7922325Losses:  4.9379072189331055 0.2526496648788452
CurrentTrain: epoch  8, batch     3 | loss: 5.0642319Losses:  4.760500907897949 0.13179005682468414
CurrentTrain: epoch  8, batch     4 | loss: 4.8263960Losses:  4.758750915527344 0.1118273064494133
CurrentTrain: epoch  8, batch     5 | loss: 4.8146644Losses:  4.72430944442749 0.2163679003715515
CurrentTrain: epoch  8, batch     6 | loss: 4.8324933Losses:  4.660524368286133 0.23556965589523315
CurrentTrain: epoch  8, batch     7 | loss: 4.7783093Losses:  4.7065887451171875 0.2531268894672394
CurrentTrain: epoch  8, batch     8 | loss: 4.8331523Losses:  4.796231746673584 0.2782154977321625
CurrentTrain: epoch  8, batch     9 | loss: 4.9353395Losses:  4.675697326660156 0.2113315761089325
CurrentTrain: epoch  8, batch    10 | loss: 4.7813630Losses:  4.652587413787842 0.20948949456214905
CurrentTrain: epoch  8, batch    11 | loss: 4.7573323Losses:  4.6921281814575195 0.21378031373023987
CurrentTrain: epoch  8, batch    12 | loss: 4.7990184Losses:  4.669341087341309 0.20154091715812683
CurrentTrain: epoch  8, batch    13 | loss: 4.7701116Losses:  4.768451690673828 0.24360594153404236
CurrentTrain: epoch  8, batch    14 | loss: 4.8902545Losses:  4.669614791870117 0.13510417938232422
CurrentTrain: epoch  8, batch    15 | loss: 4.7371669Losses:  4.651803970336914 0.1503387987613678
CurrentTrain: epoch  8, batch    16 | loss: 4.7269735Losses:  4.895411968231201 0.21082815527915955
CurrentTrain: epoch  8, batch    17 | loss: 5.0008259Losses:  4.7193145751953125 0.2335623800754547
CurrentTrain: epoch  8, batch    18 | loss: 4.8360958Losses:  4.743172645568848 0.23062196373939514
CurrentTrain: epoch  8, batch    19 | loss: 4.8584838Losses:  4.594395637512207 0.19399911165237427
CurrentTrain: epoch  8, batch    20 | loss: 4.6913953Losses:  4.665981292724609 0.2014559507369995
CurrentTrain: epoch  8, batch    21 | loss: 4.7667093Losses:  4.68203592300415 0.13332276046276093
CurrentTrain: epoch  8, batch    22 | loss: 4.7486973Losses:  4.647629737854004 0.2140602171421051
CurrentTrain: epoch  8, batch    23 | loss: 4.7546597Losses:  4.644095420837402 0.17886582016944885
CurrentTrain: epoch  8, batch    24 | loss: 4.7335281Losses:  4.593897819519043 0.17614704370498657
CurrentTrain: epoch  8, batch    25 | loss: 4.6819715Losses:  4.6596174240112305 0.12491922080516815
CurrentTrain: epoch  8, batch    26 | loss: 4.7220769Losses:  4.730759620666504 0.19316916167736053
CurrentTrain: epoch  8, batch    27 | loss: 4.8273444Losses:  4.994789123535156 0.3700341582298279
CurrentTrain: epoch  8, batch    28 | loss: 5.1798062Losses:  4.7105302810668945 0.21476325392723083
CurrentTrain: epoch  8, batch    29 | loss: 4.8179121Losses:  4.828564167022705 0.1732761114835739
CurrentTrain: epoch  8, batch    30 | loss: 4.9152021Losses:  4.646294116973877 0.19145753979682922
CurrentTrain: epoch  8, batch    31 | loss: 4.7420230Losses:  4.691699504852295 0.22739040851593018
CurrentTrain: epoch  8, batch    32 | loss: 4.8053946Losses:  4.645624160766602 0.19829076528549194
CurrentTrain: epoch  8, batch    33 | loss: 4.7447696Losses:  4.757216453552246 0.26262152194976807
CurrentTrain: epoch  8, batch    34 | loss: 4.8885274Losses:  4.7054443359375 0.18829859793186188
CurrentTrain: epoch  8, batch    35 | loss: 4.7995934Losses:  4.918099403381348 0.31515592336654663
CurrentTrain: epoch  8, batch    36 | loss: 5.0756774Losses:  4.762435436248779 0.21823519468307495
CurrentTrain: epoch  8, batch    37 | loss: 4.8715529Losses:  4.7227606773376465 0.14733046293258667
CurrentTrain: epoch  9, batch     0 | loss: 4.7964258Losses:  4.643182754516602 0.21486546099185944
CurrentTrain: epoch  9, batch     1 | loss: 4.7506156Losses:  4.619822025299072 0.1840551197528839
CurrentTrain: epoch  9, batch     2 | loss: 4.7118497Losses:  4.679412841796875 0.20550623536109924
CurrentTrain: epoch  9, batch     3 | loss: 4.7821660Losses:  4.532352447509766 0.1416330337524414
CurrentTrain: epoch  9, batch     4 | loss: 4.6031690Losses:  4.656832218170166 0.18674887716770172
CurrentTrain: epoch  9, batch     5 | loss: 4.7502065Losses:  4.830558776855469 0.14056923985481262
CurrentTrain: epoch  9, batch     6 | loss: 4.9008436Losses:  4.540694236755371 0.16935217380523682
CurrentTrain: epoch  9, batch     7 | loss: 4.6253705Losses:  4.582399368286133 0.20710912346839905
CurrentTrain: epoch  9, batch     8 | loss: 4.6859541Losses:  4.564742088317871 0.1512947976589203
CurrentTrain: epoch  9, batch     9 | loss: 4.6403894Losses:  4.5879974365234375 0.18848222494125366
CurrentTrain: epoch  9, batch    10 | loss: 4.6822386Losses:  4.636627674102783 0.210388645529747
CurrentTrain: epoch  9, batch    11 | loss: 4.7418218Losses:  4.59577751159668 0.12653324007987976
CurrentTrain: epoch  9, batch    12 | loss: 4.6590443Losses:  4.768815040588379 0.25485503673553467
CurrentTrain: epoch  9, batch    13 | loss: 4.8962426Losses:  4.535327911376953 0.18135151267051697
CurrentTrain: epoch  9, batch    14 | loss: 4.6260037Losses:  4.559856414794922 0.15190495550632477
CurrentTrain: epoch  9, batch    15 | loss: 4.6358089Losses:  4.552543640136719 0.1658487617969513
CurrentTrain: epoch  9, batch    16 | loss: 4.6354680Losses:  4.620728969573975 0.2193169891834259
CurrentTrain: epoch  9, batch    17 | loss: 4.7303877Losses:  4.575897216796875 0.18796470761299133
CurrentTrain: epoch  9, batch    18 | loss: 4.6698794Losses:  4.707477569580078 0.17886218428611755
CurrentTrain: epoch  9, batch    19 | loss: 4.7969089Losses:  4.58027982711792 0.17366184294223785
CurrentTrain: epoch  9, batch    20 | loss: 4.6671109Losses:  4.547191619873047 0.1597173810005188
CurrentTrain: epoch  9, batch    21 | loss: 4.6270504Losses:  4.538339614868164 0.152196004986763
CurrentTrain: epoch  9, batch    22 | loss: 4.6144376Losses:  4.593141078948975 0.1694963574409485
CurrentTrain: epoch  9, batch    23 | loss: 4.6778893Losses:  4.571567058563232 0.15907812118530273
CurrentTrain: epoch  9, batch    24 | loss: 4.6511059Losses:  4.550454616546631 0.1528356969356537
CurrentTrain: epoch  9, batch    25 | loss: 4.6268725Losses:  5.057750225067139 0.40705788135528564
CurrentTrain: epoch  9, batch    26 | loss: 5.2612791Losses:  4.528385639190674 0.13039569556713104
CurrentTrain: epoch  9, batch    27 | loss: 4.5935836Losses:  4.6834917068481445 0.14901092648506165
CurrentTrain: epoch  9, batch    28 | loss: 4.7579970Losses:  4.620250701904297 0.16133305430412292
CurrentTrain: epoch  9, batch    29 | loss: 4.7009172Losses:  4.508529186248779 0.14163559675216675
CurrentTrain: epoch  9, batch    30 | loss: 4.5793471Losses:  4.819028854370117 0.19113391637802124
CurrentTrain: epoch  9, batch    31 | loss: 4.9145956Losses:  4.656965255737305 0.1941239833831787
CurrentTrain: epoch  9, batch    32 | loss: 4.7540274Losses:  4.918684005737305 0.2363566756248474
CurrentTrain: epoch  9, batch    33 | loss: 5.0368624Losses:  4.7010979652404785 0.1751391589641571
CurrentTrain: epoch  9, batch    34 | loss: 4.7886677Losses:  4.551403045654297 0.14087194204330444
CurrentTrain: epoch  9, batch    35 | loss: 4.6218390Losses:  4.578707695007324 0.1681734323501587
CurrentTrain: epoch  9, batch    36 | loss: 4.6627946Losses:  4.530102729797363 0.1490117609500885
CurrentTrain: epoch  9, batch    37 | loss: 4.6046085
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.36%   
cur_acc:  ['0.8636']
his_acc:  ['0.8636']
Clustering into  2  clusters
Clusters:  [0 0 0 0 0 0 0 1 0 0 1]
Losses:  7.360393524169922 1.2791755199432373
CurrentTrain: epoch  0, batch     0 | loss: 7.9999814Losses:  9.376605987548828 1.3985850811004639
CurrentTrain: epoch  0, batch     1 | loss: 10.0758982Losses:  4.210651397705078 1.4676570892333984
CurrentTrain: epoch  1, batch     0 | loss: 4.9444799Losses:  4.66901969909668 1.5025502443313599
CurrentTrain: epoch  1, batch     1 | loss: 5.4202948Losses:  4.413917541503906 1.5082380771636963
CurrentTrain: epoch  2, batch     0 | loss: 5.1680365Losses:  3.5102641582489014 1.2093404531478882
CurrentTrain: epoch  2, batch     1 | loss: 4.1149344Losses:  3.785710573196411 1.338261365890503
CurrentTrain: epoch  3, batch     0 | loss: 4.4548411Losses:  3.7088234424591064 1.3515408039093018
CurrentTrain: epoch  3, batch     1 | loss: 4.3845940Losses:  3.587202787399292 1.2552876472473145
CurrentTrain: epoch  4, batch     0 | loss: 4.2148466Losses:  3.2849831581115723 1.4042952060699463
CurrentTrain: epoch  4, batch     1 | loss: 3.9871306Losses:  3.2523889541625977 1.3275730609893799
CurrentTrain: epoch  5, batch     0 | loss: 3.9161754Losses:  3.1216378211975098 1.2192744016647339
CurrentTrain: epoch  5, batch     1 | loss: 3.7312751Losses:  3.2840428352355957 1.2918094396591187
CurrentTrain: epoch  6, batch     0 | loss: 3.9299476Losses:  2.696300983428955 1.0478291511535645
CurrentTrain: epoch  6, batch     1 | loss: 3.2202156Losses:  2.753594398498535 1.1941030025482178
CurrentTrain: epoch  7, batch     0 | loss: 3.3506460Losses:  2.6735589504241943 1.0791606903076172
CurrentTrain: epoch  7, batch     1 | loss: 3.2131393Losses:  2.9780666828155518 1.2252553701400757
CurrentTrain: epoch  8, batch     0 | loss: 3.5906944Losses:  2.146946668624878 1.0443719625473022
CurrentTrain: epoch  8, batch     1 | loss: 2.6691327Losses:  2.375584840774536 1.08038330078125
CurrentTrain: epoch  9, batch     0 | loss: 2.9157765Losses:  2.3798372745513916 0.949137270450592
CurrentTrain: epoch  9, batch     1 | loss: 2.8544059
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 83.04%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 88.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 88.02%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 88.01%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 85.64%   
cur_acc:  ['0.8636', '0.8304']
his_acc:  ['0.8636', '0.8564']
Clustering into  3  clusters
Clusters:  [1 0 0 0 0 0 1 2 0 0 2 1 0 0 0 0]
Losses:  7.3543477058410645 1.2714715003967285
CurrentTrain: epoch  0, batch     0 | loss: 7.9900837Losses:  9.734590530395508 1.3498139381408691
CurrentTrain: epoch  0, batch     1 | loss: 10.4094973Losses:  11.213739395141602 0.4505063593387604
CurrentTrain: epoch  0, batch     2 | loss: 11.4389925Losses:  4.3428497314453125 1.386645793914795
CurrentTrain: epoch  1, batch     0 | loss: 5.0361729Losses:  5.2726030349731445 1.3505629301071167
CurrentTrain: epoch  1, batch     1 | loss: 5.9478846Losses:  3.086535930633545 0.7743476629257202
CurrentTrain: epoch  1, batch     2 | loss: 3.4737098Losses:  4.295979022979736 1.1460049152374268
CurrentTrain: epoch  2, batch     0 | loss: 4.8689814Losses:  4.036065101623535 1.324042558670044
CurrentTrain: epoch  2, batch     1 | loss: 4.6980863Losses:  4.557150840759277 1.2950128316879272
CurrentTrain: epoch  2, batch     2 | loss: 5.2046571Losses:  4.328546047210693 1.291001319885254
CurrentTrain: epoch  3, batch     0 | loss: 4.9740467Losses:  3.4527149200439453 1.2144229412078857
CurrentTrain: epoch  3, batch     1 | loss: 4.0599265Losses:  3.2352380752563477 0.7850401401519775
CurrentTrain: epoch  3, batch     2 | loss: 3.6277580Losses:  3.0666251182556152 1.5129828453063965
CurrentTrain: epoch  4, batch     0 | loss: 3.8231165Losses:  4.40833044052124 1.0717700719833374
CurrentTrain: epoch  4, batch     1 | loss: 4.9442153Losses:  1.5851852893829346 0.5835798382759094
CurrentTrain: epoch  4, batch     2 | loss: 1.8769752Losses:  3.4986062049865723 1.3568581342697144
CurrentTrain: epoch  5, batch     0 | loss: 4.1770353Losses:  2.9217710494995117 1.0417014360427856
CurrentTrain: epoch  5, batch     1 | loss: 3.4426217Losses:  2.803438186645508 0.5496610403060913
CurrentTrain: epoch  5, batch     2 | loss: 3.0782688Losses:  2.8162403106689453 1.2892589569091797
CurrentTrain: epoch  6, batch     0 | loss: 3.4608698Losses:  3.194765567779541 1.3222055435180664
CurrentTrain: epoch  6, batch     1 | loss: 3.8558683Losses:  2.901801347732544 0.49890077114105225
CurrentTrain: epoch  6, batch     2 | loss: 3.1512518Losses:  3.2260165214538574 1.3340442180633545
CurrentTrain: epoch  7, batch     0 | loss: 3.8930387Losses:  2.880669593811035 1.4317313432693481
CurrentTrain: epoch  7, batch     1 | loss: 3.5965352Losses:  2.2353403568267822 0.04163845255970955
CurrentTrain: epoch  7, batch     2 | loss: 2.2561595Losses:  2.475891351699829 1.3932545185089111
CurrentTrain: epoch  8, batch     0 | loss: 3.1725187Losses:  2.733572244644165 1.2014838457107544
CurrentTrain: epoch  8, batch     1 | loss: 3.3343141Losses:  3.290560245513916 0.5763201117515564
CurrentTrain: epoch  8, batch     2 | loss: 3.5787203Losses:  2.39504337310791 1.363278865814209
CurrentTrain: epoch  9, batch     0 | loss: 3.0766828Losses:  2.643735408782959 1.0064318180084229
CurrentTrain: epoch  9, batch     1 | loss: 3.1469512Losses:  2.7760543823242188 0.630458652973175
CurrentTrain: epoch  9, batch     2 | loss: 3.0912838
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 44.53%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 17.19%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.97%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 52.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 53.68%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 76.53%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 75.27%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 74.88%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 73.20%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 72.05%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 70.72%   
cur_acc:  ['0.8636', '0.8304', '0.4453']
his_acc:  ['0.8636', '0.8564', '0.7072']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0]
Losses:  6.610112190246582 1.6227121353149414
CurrentTrain: epoch  0, batch     0 | loss: 7.4214683Losses:  8.726503372192383 1.450059175491333
CurrentTrain: epoch  0, batch     1 | loss: 9.4515333Losses:  7.092818260192871 1.3117141723632812
CurrentTrain: epoch  0, batch     2 | loss: 7.7486753Losses:  2.4299538135528564 1.4429855346679688
CurrentTrain: epoch  1, batch     0 | loss: 3.1514466Losses:  2.7551236152648926 1.4899017810821533
CurrentTrain: epoch  1, batch     1 | loss: 3.5000744Losses:  2.626222848892212 1.0308300256729126
CurrentTrain: epoch  1, batch     2 | loss: 3.1416378Losses:  2.383396625518799 1.1759703159332275
CurrentTrain: epoch  2, batch     0 | loss: 2.9713817Losses:  2.3076210021972656 1.0167415142059326
CurrentTrain: epoch  2, batch     1 | loss: 2.8159919Losses:  2.0242862701416016 1.2190049886703491
CurrentTrain: epoch  2, batch     2 | loss: 2.6337888Losses:  1.9748188257217407 1.4200563430786133
CurrentTrain: epoch  3, batch     0 | loss: 2.6848469Losses:  1.9647551774978638 1.2214828729629517
CurrentTrain: epoch  3, batch     1 | loss: 2.5754967Losses:  2.6415207386016846 0.8951800465583801
CurrentTrain: epoch  3, batch     2 | loss: 3.0891109Losses:  2.0674374103546143 1.1886345148086548
CurrentTrain: epoch  4, batch     0 | loss: 2.6617546Losses:  2.0850958824157715 1.143928050994873
CurrentTrain: epoch  4, batch     1 | loss: 2.6570599Losses:  1.8380635976791382 1.0818943977355957
CurrentTrain: epoch  4, batch     2 | loss: 2.3790107Losses:  1.960556983947754 1.207455039024353
CurrentTrain: epoch  5, batch     0 | loss: 2.5642846Losses:  1.856980562210083 1.2114958763122559
CurrentTrain: epoch  5, batch     1 | loss: 2.4627285Losses:  1.960267424583435 0.7711799144744873
CurrentTrain: epoch  5, batch     2 | loss: 2.3458574Losses:  2.004798173904419 1.0409691333770752
CurrentTrain: epoch  6, batch     0 | loss: 2.5252829Losses:  1.4531704187393188 1.1381216049194336
CurrentTrain: epoch  6, batch     1 | loss: 2.0222311Losses:  1.9360090494155884 0.9657257795333862
CurrentTrain: epoch  6, batch     2 | loss: 2.4188719Losses:  1.8314191102981567 0.9946480393409729
CurrentTrain: epoch  7, batch     0 | loss: 2.3287432Losses:  1.6244841814041138 1.0247217416763306
CurrentTrain: epoch  7, batch     1 | loss: 2.1368451Losses:  1.4432907104492188 0.8635050654411316
CurrentTrain: epoch  7, batch     2 | loss: 1.8750433Losses:  1.3932552337646484 1.063923954963684
CurrentTrain: epoch  8, batch     0 | loss: 1.9252172Losses:  1.5769693851470947 1.1337976455688477
CurrentTrain: epoch  8, batch     1 | loss: 2.1438682Losses:  1.733733892440796 0.5947604775428772
CurrentTrain: epoch  8, batch     2 | loss: 2.0311141Losses:  1.2191107273101807 1.1705129146575928
CurrentTrain: epoch  9, batch     0 | loss: 1.8043672Losses:  1.5495352745056152 0.8828884363174438
CurrentTrain: epoch  9, batch     1 | loss: 1.9909794Losses:  1.6688520908355713 0.6049108505249023
CurrentTrain: epoch  9, batch     2 | loss: 1.9713075
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.52%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 29.17%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 34.82%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 55.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 58.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.18%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 64.74%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 65.32%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 64.66%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 63.68%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 65.36%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 64.92%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 65.81%   
cur_acc:  ['0.8636', '0.8304', '0.4453', '0.7452']
his_acc:  ['0.8636', '0.8564', '0.7072', '0.6581']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0]
Losses:  6.767965316772461 1.6606074571609497
CurrentTrain: epoch  0, batch     0 | loss: 7.5982690Losses:  10.648921966552734 1.670201301574707
CurrentTrain: epoch  0, batch     1 | loss: 11.4840221Losses:  10.481527328491211 1.3450167179107666
CurrentTrain: epoch  0, batch     2 | loss: 11.1540356Losses:  3.694652795791626 1.300830602645874
CurrentTrain: epoch  1, batch     0 | loss: 4.3450680Losses:  3.3504867553710938 1.4952914714813232
CurrentTrain: epoch  1, batch     1 | loss: 4.0981326Losses:  3.9239330291748047 1.3900258541107178
CurrentTrain: epoch  1, batch     2 | loss: 4.6189461Losses:  3.208513021469116 1.3781849145889282
CurrentTrain: epoch  2, batch     0 | loss: 3.8976054Losses:  4.448012351989746 1.340321660041809
CurrentTrain: epoch  2, batch     1 | loss: 5.1181731Losses:  2.4890084266662598 1.323851466178894
CurrentTrain: epoch  2, batch     2 | loss: 3.1509342Losses:  3.184246778488159 1.5446512699127197
CurrentTrain: epoch  3, batch     0 | loss: 3.9565725Losses:  3.4955780506134033 1.5959761142730713
CurrentTrain: epoch  3, batch     1 | loss: 4.2935662Losses:  3.0092573165893555 1.4255529642105103
CurrentTrain: epoch  3, batch     2 | loss: 3.7220337Losses:  2.4185409545898438 1.4419691562652588
CurrentTrain: epoch  4, batch     0 | loss: 3.1395254Losses:  3.237082004547119 1.7074284553527832
CurrentTrain: epoch  4, batch     1 | loss: 4.0907965Losses:  3.4424479007720947 1.3568655252456665
CurrentTrain: epoch  4, batch     2 | loss: 4.1208806Losses:  3.3675501346588135 1.4659552574157715
CurrentTrain: epoch  5, batch     0 | loss: 4.1005278Losses:  2.516191244125366 1.6180007457733154
CurrentTrain: epoch  5, batch     1 | loss: 3.3251915Losses:  2.4826388359069824 1.281256914138794
CurrentTrain: epoch  5, batch     2 | loss: 3.1232672Losses:  2.818241834640503 1.2126328945159912
CurrentTrain: epoch  6, batch     0 | loss: 3.4245582Losses:  1.925213098526001 1.658947467803955
CurrentTrain: epoch  6, batch     1 | loss: 2.7546868Losses:  3.1416420936584473 1.200060486793518
CurrentTrain: epoch  6, batch     2 | loss: 3.7416723Losses:  2.407888412475586 1.4266886711120605
CurrentTrain: epoch  7, batch     0 | loss: 3.1212327Losses:  2.883565664291382 1.3214612007141113
CurrentTrain: epoch  7, batch     1 | loss: 3.5442963Losses:  2.2866034507751465 1.3020970821380615
CurrentTrain: epoch  7, batch     2 | loss: 2.9376521Losses:  2.281116008758545 1.4388164281845093
CurrentTrain: epoch  8, batch     0 | loss: 3.0005243Losses:  2.3783068656921387 1.2897430658340454
CurrentTrain: epoch  8, batch     1 | loss: 3.0231783Losses:  2.2741897106170654 1.2473671436309814
CurrentTrain: epoch  8, batch     2 | loss: 2.8978734Losses:  2.0370945930480957 1.243465781211853
CurrentTrain: epoch  9, batch     0 | loss: 2.6588275Losses:  2.1922504901885986 1.4032841920852661
CurrentTrain: epoch  9, batch     1 | loss: 2.8938925Losses:  2.2489542961120605 1.3126466274261475
CurrentTrain: epoch  9, batch     2 | loss: 2.9052777
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 58.04%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 56.53%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 45.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 57.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 71.17%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 65.13%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 63.94%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 64.86%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 63.86%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 64.10%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 63.90%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 63.25%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 62.38%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 61.90%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 60.97%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 61.46%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 63.27%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 62.39%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 60.42%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 59.94%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 59.27%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 58.63%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 58.59%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 59.47%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 59.51%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 59.47%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 59.06%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 58.21%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 57.39%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 56.77%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 56.59%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 56.59%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 56.58%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 56.74%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 56.90%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 57.13%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 57.36%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 57.89%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 58.18%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 58.38%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 58.66%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 58.97%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 59.08%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 59.12%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 58.59%   
cur_acc:  ['0.8636', '0.8304', '0.4453', '0.7452', '0.5653']
his_acc:  ['0.8636', '0.8564', '0.7072', '0.6581', '0.5859']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0]
Losses:  6.3248209953308105 1.2351067066192627
CurrentTrain: epoch  0, batch     0 | loss: 6.9423742Losses:  9.525435447692871 1.8134080171585083
CurrentTrain: epoch  0, batch     1 | loss: 10.4321394Losses:  9.079381942749023 1.4850318431854248
CurrentTrain: epoch  0, batch     2 | loss: 9.8218975Losses:  7.331969261169434 0.3387755751609802
CurrentTrain: epoch  0, batch     3 | loss: 7.5013571Losses:  2.60703706741333 1.3646295070648193
CurrentTrain: epoch  1, batch     0 | loss: 3.2893519Losses:  2.6127636432647705 1.2268168926239014
CurrentTrain: epoch  1, batch     1 | loss: 3.2261720Losses:  2.246753215789795 1.6012449264526367
CurrentTrain: epoch  1, batch     2 | loss: 3.0473757Losses:  2.646167516708374 0.5666974782943726
CurrentTrain: epoch  1, batch     3 | loss: 2.9295163Losses:  3.2076823711395264 1.2002898454666138
CurrentTrain: epoch  2, batch     0 | loss: 3.8078272Losses:  1.953417181968689 1.3759973049163818
CurrentTrain: epoch  2, batch     1 | loss: 2.6414158Losses:  1.936969518661499 1.4047110080718994
CurrentTrain: epoch  2, batch     2 | loss: 2.6393251Losses:  2.0843684673309326 0.5385765433311462
CurrentTrain: epoch  2, batch     3 | loss: 2.3536568Losses:  2.5040183067321777 1.4227138757705688
CurrentTrain: epoch  3, batch     0 | loss: 3.2153752Losses:  2.416860580444336 1.3899412155151367
CurrentTrain: epoch  3, batch     1 | loss: 3.1118312Losses:  1.4370602369308472 1.4294898509979248
CurrentTrain: epoch  3, batch     2 | loss: 2.1518052Losses:  1.767967700958252 0.46584296226501465
CurrentTrain: epoch  3, batch     3 | loss: 2.0008893Losses:  2.240291118621826 1.3619320392608643
CurrentTrain: epoch  4, batch     0 | loss: 2.9212570Losses:  2.1826446056365967 1.4121623039245605
CurrentTrain: epoch  4, batch     1 | loss: 2.8887258Losses:  1.6492793560028076 1.4481121301651
CurrentTrain: epoch  4, batch     2 | loss: 2.3733354Losses:  1.5064713954925537 0.5400436520576477
CurrentTrain: epoch  4, batch     3 | loss: 1.7764932Losses:  2.595660924911499 1.3620309829711914
CurrentTrain: epoch  5, batch     0 | loss: 3.2766764Losses:  2.0414814949035645 1.2895065546035767
CurrentTrain: epoch  5, batch     1 | loss: 2.6862347Losses:  1.126418113708496 1.3773187398910522
CurrentTrain: epoch  5, batch     2 | loss: 1.8150775Losses:  1.2328095436096191 0.4927182197570801
CurrentTrain: epoch  5, batch     3 | loss: 1.4791687Losses:  1.6647834777832031 1.3203256130218506
CurrentTrain: epoch  6, batch     0 | loss: 2.3249464Losses:  2.0647616386413574 1.3791636228561401
CurrentTrain: epoch  6, batch     1 | loss: 2.7543435Losses:  1.7060158252716064 1.2362382411956787
CurrentTrain: epoch  6, batch     2 | loss: 2.3241348Losses:  1.1851240396499634 0.2765405774116516
CurrentTrain: epoch  6, batch     3 | loss: 1.3233943Losses:  1.6405205726623535 1.2592822313308716
CurrentTrain: epoch  7, batch     0 | loss: 2.2701616Losses:  1.6006890535354614 1.2730025053024292
CurrentTrain: epoch  7, batch     1 | loss: 2.2371902Losses:  1.6525720357894897 1.4360777139663696
CurrentTrain: epoch  7, batch     2 | loss: 2.3706110Losses:  1.801967978477478 0.3330196142196655
CurrentTrain: epoch  7, batch     3 | loss: 1.9684777Losses:  1.5531625747680664 1.2470457553863525
CurrentTrain: epoch  8, batch     0 | loss: 2.1766853Losses:  1.612649917602539 1.3476769924163818
CurrentTrain: epoch  8, batch     1 | loss: 2.2864885Losses:  1.5054421424865723 1.2667453289031982
CurrentTrain: epoch  8, batch     2 | loss: 2.1388149Losses:  1.3664911985397339 0.2068181037902832
CurrentTrain: epoch  8, batch     3 | loss: 1.4699003Losses:  1.5445926189422607 1.2312707901000977
CurrentTrain: epoch  9, batch     0 | loss: 2.1602280Losses:  1.613605260848999 1.3386106491088867
CurrentTrain: epoch  9, batch     1 | loss: 2.2829106Losses:  1.3882275819778442 0.978853166103363
CurrentTrain: epoch  9, batch     2 | loss: 1.8776542Losses:  1.3561122417449951 0.5228419303894043
CurrentTrain: epoch  9, batch     3 | loss: 1.6175332
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 92.41%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.72%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.06%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 64.74%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 65.77%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 65.05%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 64.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 63.24%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 62.74%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 61.79%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 62.15%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 63.15%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 62.40%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 61.99%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 61.29%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 60.81%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 60.84%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 60.48%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 60.23%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 59.79%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 59.38%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 58.79%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 57.95%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 57.13%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 56.34%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 55.99%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 55.83%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 55.83%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 55.51%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 55.44%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 55.45%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 55.46%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 55.94%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 56.10%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 56.33%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 56.48%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 56.62%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 56.40%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 56.32%   [EVAL] batch:   86 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 56.68%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 57.09%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.97%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.87%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.31%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.74%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 60.05%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 60.33%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 60.67%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 61.39%   
cur_acc:  ['0.8636', '0.8304', '0.4453', '0.7452', '0.5653', '0.9241']
his_acc:  ['0.8636', '0.8564', '0.7072', '0.6581', '0.5859', '0.6139']
Clustering into  3  clusters
Clusters:  [2 0 0 0 0 0 2 1 0 0 1 2 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 2 0 0 1 2 0 0 0]
Losses:  6.605298042297363 1.3510557413101196
CurrentTrain: epoch  0, batch     0 | loss: 7.2808261Losses:  9.492670059204102 1.5262991189956665
CurrentTrain: epoch  0, batch     1 | loss: 10.2558193Losses:  8.464885711669922 1.4335895776748657
CurrentTrain: epoch  0, batch     2 | loss: 9.1816807Losses:  9.579387664794922 1.065056324005127
CurrentTrain: epoch  0, batch     3 | loss: 10.1119156Losses:  2.8263723850250244 1.5422582626342773
CurrentTrain: epoch  1, batch     0 | loss: 3.5975015Losses:  2.6412594318389893 1.3392839431762695
CurrentTrain: epoch  1, batch     1 | loss: 3.3109014Losses:  2.421452760696411 1.4188706874847412
CurrentTrain: epoch  1, batch     2 | loss: 3.1308880Losses:  2.1689586639404297 1.0146828889846802
CurrentTrain: epoch  1, batch     3 | loss: 2.6763000Losses:  2.202826738357544 1.4130460023880005
CurrentTrain: epoch  2, batch     0 | loss: 2.9093497Losses:  2.152021884918213 1.289625883102417
CurrentTrain: epoch  2, batch     1 | loss: 2.7968349Losses:  2.999946117401123 1.4818512201309204
CurrentTrain: epoch  2, batch     2 | loss: 3.7408717Losses:  1.835020899772644 0.8601221442222595
CurrentTrain: epoch  2, batch     3 | loss: 2.2650819Losses:  2.8119568824768066 1.4138846397399902
CurrentTrain: epoch  3, batch     0 | loss: 3.5188992Losses:  2.494358777999878 1.327345848083496
CurrentTrain: epoch  3, batch     1 | loss: 3.1580317Losses:  1.535563588142395 1.3760594129562378
CurrentTrain: epoch  3, batch     2 | loss: 2.2235932Losses:  1.823071837425232 0.8278443813323975
CurrentTrain: epoch  3, batch     3 | loss: 2.2369940Losses:  1.7839381694793701 1.1894912719726562
CurrentTrain: epoch  4, batch     0 | loss: 2.3786838Losses:  2.2471671104431152 1.4322022199630737
CurrentTrain: epoch  4, batch     1 | loss: 2.9632683Losses:  1.7518551349639893 1.3996460437774658
CurrentTrain: epoch  4, batch     2 | loss: 2.4516783Losses:  2.491302728652954 0.6453191041946411
CurrentTrain: epoch  4, batch     3 | loss: 2.8139622Losses:  2.703923225402832 1.1760265827178955
CurrentTrain: epoch  5, batch     0 | loss: 3.2919364Losses:  1.3190433979034424 1.0243985652923584
CurrentTrain: epoch  5, batch     1 | loss: 1.8312427Losses:  1.4754563570022583 1.3499391078948975
CurrentTrain: epoch  5, batch     2 | loss: 2.1504259Losses:  1.9112094640731812 0.8415917158126831
CurrentTrain: epoch  5, batch     3 | loss: 2.3320053Losses:  2.068721055984497 1.4281432628631592
CurrentTrain: epoch  6, batch     0 | loss: 2.7827926Losses:  1.9538259506225586 1.308358073234558
CurrentTrain: epoch  6, batch     1 | loss: 2.6080050Losses:  1.7464768886566162 0.9953712821006775
CurrentTrain: epoch  6, batch     2 | loss: 2.2441626Losses:  0.7196377515792847 0.7491439580917358
CurrentTrain: epoch  6, batch     3 | loss: 1.0942097Losses:  1.8035109043121338 1.143404245376587
CurrentTrain: epoch  7, batch     0 | loss: 2.3752131Losses:  1.5832871198654175 1.2591947317123413
CurrentTrain: epoch  7, batch     1 | loss: 2.2128844Losses:  1.3642704486846924 1.4552955627441406
CurrentTrain: epoch  7, batch     2 | loss: 2.0919182Losses:  2.030634880065918 0.7773398160934448
CurrentTrain: epoch  7, batch     3 | loss: 2.4193048Losses:  1.1904704570770264 1.4058175086975098
CurrentTrain: epoch  8, batch     0 | loss: 1.8933792Losses:  1.921604871749878 0.8396962285041809
CurrentTrain: epoch  8, batch     1 | loss: 2.3414531Losses:  1.6903231143951416 1.1420695781707764
CurrentTrain: epoch  8, batch     2 | loss: 2.2613578Losses:  1.527099370956421 0.7312440872192383
CurrentTrain: epoch  8, batch     3 | loss: 1.8927214Losses:  1.6511774063110352 1.1137163639068604
CurrentTrain: epoch  9, batch     0 | loss: 2.2080355Losses:  1.2287651300430298 1.1255414485931396
CurrentTrain: epoch  9, batch     1 | loss: 1.7915359Losses:  1.4128236770629883 1.0772398710250854
CurrentTrain: epoch  9, batch     2 | loss: 1.9514437Losses:  1.8690578937530518 0.7806233167648315
CurrentTrain: epoch  9, batch     3 | loss: 2.2593696
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 80.42%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 21.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 51.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 69.82%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.22%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 63.41%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 63.66%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 63.21%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 62.09%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 62.24%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 60.42%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 59.74%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 58.73%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 59.03%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 59.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 60.53%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 59.81%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 59.00%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 58.33%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 57.38%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.45%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 55.56%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 54.79%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 54.42%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 53.92%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 53.77%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 53.35%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 52.59%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 51.85%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 51.13%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 50.86%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 50.84%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 50.92%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 50.82%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 50.81%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 50.88%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 51.11%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 51.64%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 51.85%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 52.13%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 52.48%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 52.68%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 52.06%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 51.53%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 50.93%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 51.28%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 51.76%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 52.22%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 52.75%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 53.26%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 53.76%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 54.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 54.74%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 55.01%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 54.90%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 55.04%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.43%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 55.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 56.31%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 56.43%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 56.74%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 57.03%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 57.32%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 57.55%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 57.89%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 58.22%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 58.54%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 58.81%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 58.90%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 59.04%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 59.18%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 59.57%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 59.38%   
cur_acc:  ['0.8636', '0.8304', '0.4453', '0.7452', '0.5653', '0.9241', '0.8042']
his_acc:  ['0.8636', '0.8564', '0.7072', '0.6581', '0.5859', '0.6139', '0.5938']
Clustering into  4  clusters
Clusters:  [1 0 0 0 0 0 1 3 0 0 3 1 0 0 0 0 1 1 3 0 0 0 0 0 0 0 0 0 1 0 0 3 1 0 0 0 1
 2 1 0 0]
Losses:  6.298537254333496 1.3016462326049805
CurrentTrain: epoch  0, batch     0 | loss: 6.9493604Losses:  9.28917407989502 1.203273057937622
CurrentTrain: epoch  0, batch     1 | loss: 9.8908110Losses:  8.832551956176758 1.3356218338012695
CurrentTrain: epoch  0, batch     2 | loss: 9.5003624Losses:  9.79379940032959 1.1286320686340332
CurrentTrain: epoch  0, batch     3 | loss: 10.3581152Losses:  2.6825356483459473 1.3250755071640015
CurrentTrain: epoch  1, batch     0 | loss: 3.3450735Losses:  2.2025363445281982 1.2611981630325317
CurrentTrain: epoch  1, batch     1 | loss: 2.8331354Losses:  2.50663423538208 1.16070556640625
CurrentTrain: epoch  1, batch     2 | loss: 3.0869870Losses:  1.5287257432937622 0.9873059988021851
CurrentTrain: epoch  1, batch     3 | loss: 2.0223787Losses:  1.8259646892547607 1.2523863315582275
CurrentTrain: epoch  2, batch     0 | loss: 2.4521580Losses:  2.3166277408599854 1.3774921894073486
CurrentTrain: epoch  2, batch     1 | loss: 3.0053740Losses:  1.7748671770095825 1.0311464071273804
CurrentTrain: epoch  2, batch     2 | loss: 2.2904403Losses:  2.3603944778442383 1.1937272548675537
CurrentTrain: epoch  2, batch     3 | loss: 2.9572582Losses:  1.817938208580017 1.3691651821136475
CurrentTrain: epoch  3, batch     0 | loss: 2.5025208Losses:  2.047565460205078 1.3406891822814941
CurrentTrain: epoch  3, batch     1 | loss: 2.7179101Losses:  1.7936623096466064 1.1691293716430664
CurrentTrain: epoch  3, batch     2 | loss: 2.3782270Losses:  1.703662633895874 1.095012903213501
CurrentTrain: epoch  3, batch     3 | loss: 2.2511692Losses:  1.4765987396240234 1.0998207330703735
CurrentTrain: epoch  4, batch     0 | loss: 2.0265090Losses:  1.194483995437622 1.18691086769104
CurrentTrain: epoch  4, batch     1 | loss: 1.7879394Losses:  1.7579050064086914 1.0760083198547363
CurrentTrain: epoch  4, batch     2 | loss: 2.2959092Losses:  2.3449418544769287 0.9199128150939941
CurrentTrain: epoch  4, batch     3 | loss: 2.8048983Losses:  1.85049307346344 1.1227712631225586
CurrentTrain: epoch  5, batch     0 | loss: 2.4118786Losses:  1.0716286897659302 1.0631824731826782
CurrentTrain: epoch  5, batch     1 | loss: 1.6032200Losses:  1.6227164268493652 1.0918952226638794
CurrentTrain: epoch  5, batch     2 | loss: 2.1686640Losses:  1.5170706510543823 1.2343112230300903
CurrentTrain: epoch  5, batch     3 | loss: 2.1342263Losses:  1.5721887350082397 1.1253790855407715
CurrentTrain: epoch  6, batch     0 | loss: 2.1348782Losses:  1.1670119762420654 1.1325855255126953
CurrentTrain: epoch  6, batch     1 | loss: 1.7333047Losses:  1.6278157234191895 1.072934865951538
CurrentTrain: epoch  6, batch     2 | loss: 2.1642833Losses:  1.2707325220108032 0.8460859656333923
CurrentTrain: epoch  6, batch     3 | loss: 1.6937755Losses:  1.5168055295944214 1.27894127368927
CurrentTrain: epoch  7, batch     0 | loss: 2.1562762Losses:  1.2680405378341675 0.8612809181213379
CurrentTrain: epoch  7, batch     1 | loss: 1.6986810Losses:  1.4454983472824097 1.0344384908676147
CurrentTrain: epoch  7, batch     2 | loss: 1.9627175Losses:  1.212884783744812 1.122941255569458
CurrentTrain: epoch  7, batch     3 | loss: 1.7743554Losses:  1.511018991470337 1.010129451751709
CurrentTrain: epoch  8, batch     0 | loss: 2.0160837Losses:  0.9370912909507751 1.2197086811065674
CurrentTrain: epoch  8, batch     1 | loss: 1.5469456Losses:  1.193238615989685 1.0504298210144043
CurrentTrain: epoch  8, batch     2 | loss: 1.7184535Losses:  1.561017632484436 0.8518320322036743
CurrentTrain: epoch  8, batch     3 | loss: 1.9869337Losses:  1.4383739233016968 1.0931293964385986
CurrentTrain: epoch  9, batch     0 | loss: 1.9849386Losses:  1.2125555276870728 1.1016156673431396
CurrentTrain: epoch  9, batch     1 | loss: 1.7633634Losses:  0.9647475481033325 0.9620593786239624
CurrentTrain: epoch  9, batch     2 | loss: 1.4457772Losses:  1.3796542882919312 0.9207409620285034
CurrentTrain: epoch  9, batch     3 | loss: 1.8400247
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 5.21%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 33.33%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 43.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.04%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 51.97%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 53.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.67%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.03%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 6.25%,  total acc: 67.28%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 65.36%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 63.54%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 60.20%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 58.81%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 58.44%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 57.93%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 58.04%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 57.39%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 57.08%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 55.84%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 55.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 56.77%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 56.12%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 55.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 54.41%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 53.85%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 52.95%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 52.66%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 52.73%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 52.79%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 52.96%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 52.26%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 51.48%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 50.83%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 50.00%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 49.19%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 48.51%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 47.75%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 47.21%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 46.69%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 46.74%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 46.69%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 46.47%   [EVAL] batch:   69 | acc: 0.00%,  total acc: 45.80%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 45.16%   [EVAL] batch:   71 | acc: 0.00%,  total acc: 44.53%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 44.35%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 44.43%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 44.58%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 44.65%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 44.81%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 44.95%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 45.17%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 45.62%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 45.91%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 46.34%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 46.76%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 46.32%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 45.86%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 45.33%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 45.74%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 46.28%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 46.81%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 47.39%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 47.96%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 48.52%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 49.07%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 49.61%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 49.93%   [EVAL] batch:   96 | acc: 43.75%,  total acc: 49.87%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 50.44%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 50.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 51.36%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 51.16%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 50.91%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 50.66%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 50.54%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 50.47%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 50.88%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 51.27%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 51.72%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 52.16%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 52.31%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 51.95%   [EVAL] batch:  112 | acc: 0.00%,  total acc: 51.49%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 51.04%   [EVAL] batch:  114 | acc: 0.00%,  total acc: 50.60%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 50.65%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 51.01%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 51.27%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 51.63%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 51.88%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 52.12%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 52.41%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 52.79%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 52.87%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 53.15%   [EVAL] batch:  125 | acc: 87.50%,  total acc: 53.42%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 53.79%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 54.15%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 54.51%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 54.86%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 55.20%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 55.54%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 55.59%   
cur_acc:  ['0.8636', '0.8304', '0.4453', '0.7452', '0.5653', '0.9241', '0.8042', '0.8750']
his_acc:  ['0.8636', '0.8564', '0.7072', '0.6581', '0.5859', '0.6139', '0.5938', '0.5559']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hardprompt/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 0 0 0]
Losses:  12.156584739685059 1.8808813095092773
CurrentTrain: epoch  0, batch     0 | loss: 13.0970249Losses:  14.847631454467773 1.971010446548462
CurrentTrain: epoch  0, batch     1 | loss: 15.8331366Losses:  15.228837966918945 1.7467725276947021
CurrentTrain: epoch  0, batch     2 | loss: 16.1022243Losses:  15.206698417663574 1.828324317932129
CurrentTrain: epoch  0, batch     3 | loss: 16.1208611Losses:  14.793811798095703 1.8487461805343628
CurrentTrain: epoch  0, batch     4 | loss: 15.7181845Losses:  14.974053382873535 1.6301592588424683
CurrentTrain: epoch  0, batch     5 | loss: 15.7891331Losses:  14.220447540283203 1.7775871753692627
CurrentTrain: epoch  0, batch     6 | loss: 15.1092415Losses:  14.25420093536377 1.6136112213134766
CurrentTrain: epoch  0, batch     7 | loss: 15.0610065Losses:  13.82211971282959 1.6233437061309814
CurrentTrain: epoch  0, batch     8 | loss: 14.6337919Losses:  13.493330001831055 1.2719292640686035
CurrentTrain: epoch  0, batch     9 | loss: 14.1292944Losses:  13.240034103393555 1.2576959133148193
CurrentTrain: epoch  0, batch    10 | loss: 13.8688822Losses:  12.881591796875 1.5688230991363525
CurrentTrain: epoch  0, batch    11 | loss: 13.6660032Losses:  12.27560806274414 1.137808918952942
CurrentTrain: epoch  0, batch    12 | loss: 12.8445129Losses:  12.22422981262207 1.50705087184906
CurrentTrain: epoch  0, batch    13 | loss: 12.9777555Losses:  12.492496490478516 1.574928879737854
CurrentTrain: epoch  0, batch    14 | loss: 13.2799606Losses:  12.37571907043457 1.7500441074371338
CurrentTrain: epoch  0, batch    15 | loss: 13.2507410Losses:  11.830442428588867 1.2945940494537354
CurrentTrain: epoch  0, batch    16 | loss: 12.4777393Losses:  11.620346069335938 1.6667931079864502
CurrentTrain: epoch  0, batch    17 | loss: 12.4537430Losses:  11.707165718078613 1.5097514390945435
CurrentTrain: epoch  0, batch    18 | loss: 12.4620419Losses:  11.33584213256836 1.2718042135238647
CurrentTrain: epoch  0, batch    19 | loss: 11.9717445Losses:  11.698630332946777 1.4177415370941162
CurrentTrain: epoch  0, batch    20 | loss: 12.4075012Losses:  11.66104793548584 1.723785400390625
CurrentTrain: epoch  0, batch    21 | loss: 12.5229406Losses:  11.369648933410645 1.4181634187698364
CurrentTrain: epoch  0, batch    22 | loss: 12.0787306Losses:  11.025092124938965 1.461090087890625
CurrentTrain: epoch  0, batch    23 | loss: 11.7556372Losses:  10.467823028564453 1.3971104621887207
CurrentTrain: epoch  0, batch    24 | loss: 11.1663780Losses:  10.274617195129395 1.4498400688171387
CurrentTrain: epoch  0, batch    25 | loss: 10.9995375Losses:  10.098369598388672 1.2382020950317383
CurrentTrain: epoch  0, batch    26 | loss: 10.7174702Losses:  10.052881240844727 1.2749637365341187
CurrentTrain: epoch  0, batch    27 | loss: 10.6903629Losses:  9.66366195678711 1.5814398527145386
CurrentTrain: epoch  0, batch    28 | loss: 10.4543819Losses:  9.368258476257324 1.3827372789382935
CurrentTrain: epoch  0, batch    29 | loss: 10.0596275Losses:  9.2177734375 1.4897239208221436
CurrentTrain: epoch  0, batch    30 | loss: 9.9626350Losses:  9.387923240661621 0.9917775988578796
CurrentTrain: epoch  0, batch    31 | loss: 9.8838120Losses:  8.813848495483398 1.2735774517059326
CurrentTrain: epoch  0, batch    32 | loss: 9.4506369Losses:  8.965398788452148 1.1736857891082764
CurrentTrain: epoch  0, batch    33 | loss: 9.5522413Losses:  8.771720886230469 1.170572280883789
CurrentTrain: epoch  0, batch    34 | loss: 9.3570070Losses:  8.060331344604492 1.4881172180175781
CurrentTrain: epoch  0, batch    35 | loss: 8.8043900Losses:  8.169495582580566 1.2712106704711914
CurrentTrain: epoch  0, batch    36 | loss: 8.8051014Losses:  7.866621017456055 0.8914621472358704
CurrentTrain: epoch  0, batch    37 | loss: 8.3123522Losses:  7.528365135192871 1.2478636503219604
CurrentTrain: epoch  1, batch     0 | loss: 8.1522970Losses:  7.444260120391846 1.3363865613937378
CurrentTrain: epoch  1, batch     1 | loss: 8.1124535Losses:  7.57747745513916 1.3713643550872803
CurrentTrain: epoch  1, batch     2 | loss: 8.2631598Losses:  7.419025897979736 1.292407512664795
CurrentTrain: epoch  1, batch     3 | loss: 8.0652294Losses:  7.55021858215332 1.2797422409057617
CurrentTrain: epoch  1, batch     4 | loss: 8.1900902Losses:  7.817559242248535 1.2089509963989258
CurrentTrain: epoch  1, batch     5 | loss: 8.4220352Losses:  7.589084625244141 1.430631160736084
CurrentTrain: epoch  1, batch     6 | loss: 8.3044004Losses:  7.486541748046875 1.2129528522491455
CurrentTrain: epoch  1, batch     7 | loss: 8.0930185Losses:  7.5359039306640625 1.1945606470108032
CurrentTrain: epoch  1, batch     8 | loss: 8.1331844Losses:  7.112218856811523 1.2442171573638916
CurrentTrain: epoch  1, batch     9 | loss: 7.7343273Losses:  7.816967964172363 1.0944724082946777
CurrentTrain: epoch  1, batch    10 | loss: 8.3642044Losses:  7.003440856933594 0.9083315134048462
CurrentTrain: epoch  1, batch    11 | loss: 7.4576068Losses:  7.644683837890625 1.1484375
CurrentTrain: epoch  1, batch    12 | loss: 8.2189026Losses:  7.278434753417969 0.9916365742683411
CurrentTrain: epoch  1, batch    13 | loss: 7.7742529Losses:  7.225049018859863 1.0643329620361328
CurrentTrain: epoch  1, batch    14 | loss: 7.7572155Losses:  7.170629501342773 1.1583213806152344
CurrentTrain: epoch  1, batch    15 | loss: 7.7497902Losses:  7.412667274475098 1.2500238418579102
CurrentTrain: epoch  1, batch    16 | loss: 8.0376797Losses:  6.9863667488098145 1.1069672107696533
CurrentTrain: epoch  1, batch    17 | loss: 7.5398502Losses:  6.984384536743164 1.111383318901062
CurrentTrain: epoch  1, batch    18 | loss: 7.5400763Losses:  7.590283393859863 1.3379771709442139
CurrentTrain: epoch  1, batch    19 | loss: 8.2592716Losses:  7.108043670654297 1.1439971923828125
CurrentTrain: epoch  1, batch    20 | loss: 7.6800423Losses:  6.858707427978516 0.9335337281227112
CurrentTrain: epoch  1, batch    21 | loss: 7.3254743Losses:  6.874331951141357 0.9036964178085327
CurrentTrain: epoch  1, batch    22 | loss: 7.3261800Losses:  6.720991134643555 1.1023764610290527
CurrentTrain: epoch  1, batch    23 | loss: 7.2721796Losses:  6.765171527862549 1.0786306858062744
CurrentTrain: epoch  1, batch    24 | loss: 7.3044868Losses:  7.170622825622559 1.068471908569336
CurrentTrain: epoch  1, batch    25 | loss: 7.7048588Losses:  6.7632341384887695 1.047746181488037
CurrentTrain: epoch  1, batch    26 | loss: 7.2871075Losses:  6.857791900634766 0.9769918322563171
CurrentTrain: epoch  1, batch    27 | loss: 7.3462877Losses:  6.153613567352295 0.8057190179824829
CurrentTrain: epoch  1, batch    28 | loss: 6.5564733Losses:  7.021914482116699 0.9101935625076294
CurrentTrain: epoch  1, batch    29 | loss: 7.4770112Losses:  7.476568222045898 1.06878662109375
CurrentTrain: epoch  1, batch    30 | loss: 8.0109615Losses:  6.85300350189209 1.0831685066223145
CurrentTrain: epoch  1, batch    31 | loss: 7.3945875Losses:  6.9925665855407715 1.0866446495056152
CurrentTrain: epoch  1, batch    32 | loss: 7.5358887Losses:  6.793923377990723 1.0700950622558594
CurrentTrain: epoch  1, batch    33 | loss: 7.3289709Losses:  6.717125415802002 1.0012602806091309
CurrentTrain: epoch  1, batch    34 | loss: 7.2177553Losses:  7.300243377685547 0.9585505723953247
CurrentTrain: epoch  1, batch    35 | loss: 7.7795186Losses:  6.720798969268799 1.0310399532318115
CurrentTrain: epoch  1, batch    36 | loss: 7.2363191Losses:  7.056743621826172 0.5756827592849731
CurrentTrain: epoch  1, batch    37 | loss: 7.3445849Losses:  6.34047794342041 1.0009009838104248
CurrentTrain: epoch  2, batch     0 | loss: 6.8409286Losses:  6.540004730224609 0.9122855067253113
CurrentTrain: epoch  2, batch     1 | loss: 6.9961476Losses:  6.202034950256348 0.8427745699882507
CurrentTrain: epoch  2, batch     2 | loss: 6.6234221Losses:  6.195877552032471 0.8634950518608093
CurrentTrain: epoch  2, batch     3 | loss: 6.6276250Losses:  6.100980281829834 0.8866856098175049
CurrentTrain: epoch  2, batch     4 | loss: 6.5443230Losses:  6.22860860824585 0.8937364816665649
CurrentTrain: epoch  2, batch     5 | loss: 6.6754770Losses:  6.469069480895996 0.9488328695297241
CurrentTrain: epoch  2, batch     6 | loss: 6.9434857Losses:  6.935044288635254 1.0190668106079102
CurrentTrain: epoch  2, batch     7 | loss: 7.4445777Losses:  6.2514328956604 0.8858754634857178
CurrentTrain: epoch  2, batch     8 | loss: 6.6943707Losses:  6.039484977722168 0.8506718873977661
CurrentTrain: epoch  2, batch     9 | loss: 6.4648209Losses:  6.220215320587158 0.6957693099975586
CurrentTrain: epoch  2, batch    10 | loss: 6.5681000Losses:  6.161304950714111 0.635439395904541
CurrentTrain: epoch  2, batch    11 | loss: 6.4790249Losses:  6.2072014808654785 0.6418110132217407
CurrentTrain: epoch  2, batch    12 | loss: 6.5281072Losses:  7.169508457183838 0.7205386161804199
CurrentTrain: epoch  2, batch    13 | loss: 7.5297775Losses:  6.549971580505371 0.995236873626709
CurrentTrain: epoch  2, batch    14 | loss: 7.0475903Losses:  6.515315532684326 0.849770188331604
CurrentTrain: epoch  2, batch    15 | loss: 6.9402008Losses:  6.732971668243408 0.8193213939666748
CurrentTrain: epoch  2, batch    16 | loss: 7.1426325Losses:  6.9778032302856445 0.8675941824913025
CurrentTrain: epoch  2, batch    17 | loss: 7.4116001Losses:  6.535677909851074 0.7928215861320496
CurrentTrain: epoch  2, batch    18 | loss: 6.9320889Losses:  6.106664657592773 0.7624121904373169
CurrentTrain: epoch  2, batch    19 | loss: 6.4878707Losses:  6.031296730041504 0.7389335632324219
CurrentTrain: epoch  2, batch    20 | loss: 6.4007635Losses:  6.404669761657715 0.7832514047622681
CurrentTrain: epoch  2, batch    21 | loss: 6.7962956Losses:  6.236029148101807 0.8327199220657349
CurrentTrain: epoch  2, batch    22 | loss: 6.6523890Losses:  6.201347827911377 0.7312961220741272
CurrentTrain: epoch  2, batch    23 | loss: 6.5669961Losses:  6.351865768432617 0.7941631078720093
CurrentTrain: epoch  2, batch    24 | loss: 6.7489471Losses:  6.275223255157471 0.8185166120529175
CurrentTrain: epoch  2, batch    25 | loss: 6.6844816Losses:  6.483577728271484 0.8130267262458801
CurrentTrain: epoch  2, batch    26 | loss: 6.8900909Losses:  5.991440773010254 0.6039295792579651
CurrentTrain: epoch  2, batch    27 | loss: 6.2934055Losses:  5.984537124633789 0.696168839931488
CurrentTrain: epoch  2, batch    28 | loss: 6.3326216Losses:  6.037417411804199 0.7614694833755493
CurrentTrain: epoch  2, batch    29 | loss: 6.4181523Losses:  6.146938800811768 0.8777125477790833
CurrentTrain: epoch  2, batch    30 | loss: 6.5857949Losses:  5.941697120666504 0.7591716051101685
CurrentTrain: epoch  2, batch    31 | loss: 6.3212829Losses:  6.28749942779541 0.8091328144073486
CurrentTrain: epoch  2, batch    32 | loss: 6.6920657Losses:  6.240568161010742 0.6476055383682251
CurrentTrain: epoch  2, batch    33 | loss: 6.5643711Losses:  5.60709285736084 0.6424010992050171
CurrentTrain: epoch  2, batch    34 | loss: 5.9282932Losses:  6.012689590454102 0.812699556350708
CurrentTrain: epoch  2, batch    35 | loss: 6.4190392Losses:  6.177223205566406 0.6649193167686462
CurrentTrain: epoch  2, batch    36 | loss: 6.5096827Losses:  5.998288154602051 0.5325445532798767
CurrentTrain: epoch  2, batch    37 | loss: 6.2645602Losses:  6.143634796142578 0.6629062294960022
CurrentTrain: epoch  3, batch     0 | loss: 6.4750881Losses:  6.07938289642334 0.7642787098884583
CurrentTrain: epoch  3, batch     1 | loss: 6.4615221Losses:  5.787240982055664 0.7143695950508118
CurrentTrain: epoch  3, batch     2 | loss: 6.1444259Losses:  5.646821975708008 0.6754096746444702
CurrentTrain: epoch  3, batch     3 | loss: 5.9845266Losses:  5.542242050170898 0.5655459761619568
CurrentTrain: epoch  3, batch     4 | loss: 5.8250151Losses:  5.673827648162842 0.7048661112785339
CurrentTrain: epoch  3, batch     5 | loss: 6.0262609Losses:  5.713746547698975 0.48270970582962036
CurrentTrain: epoch  3, batch     6 | loss: 5.9551015Losses:  5.590882301330566 0.6117360591888428
CurrentTrain: epoch  3, batch     7 | loss: 5.8967505Losses:  5.839972496032715 0.6554300785064697
CurrentTrain: epoch  3, batch     8 | loss: 6.1676874Losses:  5.950302600860596 0.5616322755813599
CurrentTrain: epoch  3, batch     9 | loss: 6.2311187Losses:  6.140999794006348 0.8384577035903931
CurrentTrain: epoch  3, batch    10 | loss: 6.5602288Losses:  5.627225399017334 0.6187690496444702
CurrentTrain: epoch  3, batch    11 | loss: 5.9366097Losses:  5.894339561462402 0.6725332736968994
CurrentTrain: epoch  3, batch    12 | loss: 6.2306061Losses:  5.47799015045166 0.559292197227478
CurrentTrain: epoch  3, batch    13 | loss: 5.7576361Losses:  5.627408981323242 0.5106053948402405
CurrentTrain: epoch  3, batch    14 | loss: 5.8827119Losses:  5.80918025970459 0.6992436647415161
CurrentTrain: epoch  3, batch    15 | loss: 6.1588020Losses:  5.774743556976318 0.5777216553688049
CurrentTrain: epoch  3, batch    16 | loss: 6.0636044Losses:  5.459203720092773 0.4249994158744812
CurrentTrain: epoch  3, batch    17 | loss: 5.6717033Losses:  6.390071392059326 0.7108021974563599
CurrentTrain: epoch  3, batch    18 | loss: 6.7454724Losses:  5.624515056610107 0.5310838222503662
CurrentTrain: epoch  3, batch    19 | loss: 5.8900571Losses:  6.108386039733887 0.7635124325752258
CurrentTrain: epoch  3, batch    20 | loss: 6.4901423Losses:  5.547707557678223 0.40569016337394714
CurrentTrain: epoch  3, batch    21 | loss: 5.7505527Losses:  5.60272216796875 0.549031138420105
CurrentTrain: epoch  3, batch    22 | loss: 5.8772378Losses:  5.632311820983887 0.6355577707290649
CurrentTrain: epoch  3, batch    23 | loss: 5.9500909Losses:  5.311784744262695 0.31805548071861267
CurrentTrain: epoch  3, batch    24 | loss: 5.4708123Losses:  5.392807483673096 0.5149153470993042
CurrentTrain: epoch  3, batch    25 | loss: 5.6502652Losses:  5.4665021896362305 0.5821419358253479
CurrentTrain: epoch  3, batch    26 | loss: 5.7575731Losses:  6.124499320983887 0.5801342725753784
CurrentTrain: epoch  3, batch    27 | loss: 6.4145665Losses:  5.9199113845825195 0.4134413003921509
CurrentTrain: epoch  3, batch    28 | loss: 6.1266322Losses:  5.443102836608887 0.626958429813385
CurrentTrain: epoch  3, batch    29 | loss: 5.7565823Losses:  5.302257537841797 0.6056501269340515
CurrentTrain: epoch  3, batch    30 | loss: 5.6050825Losses:  5.835172653198242 0.5945096015930176
CurrentTrain: epoch  3, batch    31 | loss: 6.1324272Losses:  5.797654628753662 0.5630146265029907
CurrentTrain: epoch  3, batch    32 | loss: 6.0791621Losses:  5.226659774780273 0.4853978157043457
CurrentTrain: epoch  3, batch    33 | loss: 5.4693584Losses:  4.997920036315918 0.37863537669181824
CurrentTrain: epoch  3, batch    34 | loss: 5.1872377Losses:  6.827360153198242 0.9292190074920654
CurrentTrain: epoch  3, batch    35 | loss: 7.2919698Losses:  5.549117088317871 0.4282822608947754
CurrentTrain: epoch  3, batch    36 | loss: 5.7632580Losses:  5.312455177307129 0.33699098229408264
CurrentTrain: epoch  3, batch    37 | loss: 5.4809508Losses:  5.610405921936035 0.48930245637893677
CurrentTrain: epoch  4, batch     0 | loss: 5.8550572Losses:  5.533524513244629 0.42609912157058716
CurrentTrain: epoch  4, batch     1 | loss: 5.7465739Losses:  5.184103012084961 0.34855398535728455
CurrentTrain: epoch  4, batch     2 | loss: 5.3583798Losses:  5.29566764831543 0.42152974009513855
CurrentTrain: epoch  4, batch     3 | loss: 5.5064325Losses:  5.486649036407471 0.6609283089637756
CurrentTrain: epoch  4, batch     4 | loss: 5.8171134Losses:  5.402248382568359 0.5826794505119324
CurrentTrain: epoch  4, batch     5 | loss: 5.6935883Losses:  5.398488998413086 0.38854625821113586
CurrentTrain: epoch  4, batch     6 | loss: 5.5927620Losses:  5.286110877990723 0.42482906579971313
CurrentTrain: epoch  4, batch     7 | loss: 5.4985256Losses:  5.610998630523682 0.45187079906463623
CurrentTrain: epoch  4, batch     8 | loss: 5.8369341Losses:  5.911149978637695 0.7053252458572388
CurrentTrain: epoch  4, batch     9 | loss: 6.2638125Losses:  5.606924057006836 0.51009601354599
CurrentTrain: epoch  4, batch    10 | loss: 5.8619719Losses:  5.325874328613281 0.4959107041358948
CurrentTrain: epoch  4, batch    11 | loss: 5.5738297Losses:  6.0644659996032715 0.7233967781066895
CurrentTrain: epoch  4, batch    12 | loss: 6.4261646Losses:  5.4891486167907715 0.38116028904914856
CurrentTrain: epoch  4, batch    13 | loss: 5.6797290Losses:  5.464756965637207 0.4136667251586914
CurrentTrain: epoch  4, batch    14 | loss: 5.6715903Losses:  5.2320709228515625 0.5279364585876465
CurrentTrain: epoch  4, batch    15 | loss: 5.4960394Losses:  5.375324249267578 0.33512139320373535
CurrentTrain: epoch  4, batch    16 | loss: 5.5428848Losses:  5.242454528808594 0.5069526433944702
CurrentTrain: epoch  4, batch    17 | loss: 5.4959307Losses:  5.0612664222717285 0.4171496033668518
CurrentTrain: epoch  4, batch    18 | loss: 5.2698412Losses:  5.11934757232666 0.48521560430526733
CurrentTrain: epoch  4, batch    19 | loss: 5.3619552Losses:  5.758655548095703 0.6482430696487427
CurrentTrain: epoch  4, batch    20 | loss: 6.0827770Losses:  5.045128345489502 0.3712373971939087
CurrentTrain: epoch  4, batch    21 | loss: 5.2307472Losses:  5.043456077575684 0.44002431631088257
CurrentTrain: epoch  4, batch    22 | loss: 5.2634683Losses:  5.304997444152832 0.4867479205131531
CurrentTrain: epoch  4, batch    23 | loss: 5.5483713Losses:  5.157463073730469 0.510844349861145
CurrentTrain: epoch  4, batch    24 | loss: 5.4128852Losses:  5.291348457336426 0.43374180793762207
CurrentTrain: epoch  4, batch    25 | loss: 5.5082192Losses:  4.978318214416504 0.3970980644226074
CurrentTrain: epoch  4, batch    26 | loss: 5.1768675Losses:  5.6617631912231445 0.7415168285369873
CurrentTrain: epoch  4, batch    27 | loss: 6.0325217Losses:  5.303264617919922 0.5121515989303589
CurrentTrain: epoch  4, batch    28 | loss: 5.5593405Losses:  5.0868635177612305 0.37491366267204285
CurrentTrain: epoch  4, batch    29 | loss: 5.2743201Losses:  5.278563022613525 0.4296131730079651
CurrentTrain: epoch  4, batch    30 | loss: 5.4933696Losses:  5.509397506713867 0.5779502391815186
CurrentTrain: epoch  4, batch    31 | loss: 5.7983727Losses:  5.401303291320801 0.2891731262207031
CurrentTrain: epoch  4, batch    32 | loss: 5.5458899Losses:  5.451735496520996 0.336179256439209
CurrentTrain: epoch  4, batch    33 | loss: 5.6198254Losses:  5.694824695587158 0.44947385787963867
CurrentTrain: epoch  4, batch    34 | loss: 5.9195614Losses:  5.520873069763184 0.3917153477668762
CurrentTrain: epoch  4, batch    35 | loss: 5.7167306Losses:  5.238568305969238 0.4164974093437195
CurrentTrain: epoch  4, batch    36 | loss: 5.4468169Losses:  5.023712158203125 0.3527598977088928
CurrentTrain: epoch  4, batch    37 | loss: 5.2000923Losses:  5.330611228942871 0.5097903609275818
CurrentTrain: epoch  5, batch     0 | loss: 5.5855064Losses:  5.28255558013916 0.4469834864139557
CurrentTrain: epoch  5, batch     1 | loss: 5.5060472Losses:  5.502995491027832 0.4497002959251404
CurrentTrain: epoch  5, batch     2 | loss: 5.7278457Losses:  4.954132556915283 0.27683544158935547
CurrentTrain: epoch  5, batch     3 | loss: 5.0925503Losses:  5.102492809295654 0.4191001057624817
CurrentTrain: epoch  5, batch     4 | loss: 5.3120427Losses:  4.982746124267578 0.41683152318000793
CurrentTrain: epoch  5, batch     5 | loss: 5.1911621Losses:  5.280755996704102 0.590937614440918
CurrentTrain: epoch  5, batch     6 | loss: 5.5762248Losses:  5.313487529754639 0.5129303932189941
CurrentTrain: epoch  5, batch     7 | loss: 5.5699530Losses:  5.016191005706787 0.3916451930999756
CurrentTrain: epoch  5, batch     8 | loss: 5.2120137Losses:  5.001862049102783 0.3391457796096802
CurrentTrain: epoch  5, batch     9 | loss: 5.1714349Losses:  6.136795997619629 0.45861005783081055
CurrentTrain: epoch  5, batch    10 | loss: 6.3661013Losses:  5.427494049072266 0.3604811131954193
CurrentTrain: epoch  5, batch    11 | loss: 5.6077347Losses:  4.985176086425781 0.2287929654121399
CurrentTrain: epoch  5, batch    12 | loss: 5.0995727Losses:  5.034332752227783 0.3657183051109314
CurrentTrain: epoch  5, batch    13 | loss: 5.2171917Losses:  4.827493667602539 0.33716580271720886
CurrentTrain: epoch  5, batch    14 | loss: 4.9960766Losses:  4.947452068328857 0.29963111877441406
CurrentTrain: epoch  5, batch    15 | loss: 5.0972676Losses:  5.092327117919922 0.30508720874786377
CurrentTrain: epoch  5, batch    16 | loss: 5.2448707Losses:  5.193917274475098 0.39857786893844604
CurrentTrain: epoch  5, batch    17 | loss: 5.3932061Losses:  5.448911666870117 0.5588884353637695
CurrentTrain: epoch  5, batch    18 | loss: 5.7283559Losses:  5.024475574493408 0.3070605397224426
CurrentTrain: epoch  5, batch    19 | loss: 5.1780057Losses:  5.048781394958496 0.3644159436225891
CurrentTrain: epoch  5, batch    20 | loss: 5.2309895Losses:  4.940806865692139 0.36483529210090637
CurrentTrain: epoch  5, batch    21 | loss: 5.1232247Losses:  4.980834007263184 0.28730833530426025
CurrentTrain: epoch  5, batch    22 | loss: 5.1244884Losses:  4.97538948059082 0.3071066737174988
CurrentTrain: epoch  5, batch    23 | loss: 5.1289430Losses:  4.971912384033203 0.38737332820892334
CurrentTrain: epoch  5, batch    24 | loss: 5.1655989Losses:  4.902892112731934 0.26635506749153137
CurrentTrain: epoch  5, batch    25 | loss: 5.0360699Losses:  5.591145038604736 0.425187885761261
CurrentTrain: epoch  5, batch    26 | loss: 5.8037391Losses:  4.941927909851074 0.29293474555015564
CurrentTrain: epoch  5, batch    27 | loss: 5.0883951Losses:  4.8611040115356445 0.30084994435310364
CurrentTrain: epoch  5, batch    28 | loss: 5.0115290Losses:  5.299050331115723 0.5057111978530884
CurrentTrain: epoch  5, batch    29 | loss: 5.5519061Losses:  5.109034538269043 0.35827067494392395
CurrentTrain: epoch  5, batch    30 | loss: 5.2881699Losses:  4.998286247253418 0.33068564534187317
CurrentTrain: epoch  5, batch    31 | loss: 5.1636291Losses:  5.150293827056885 0.40803849697113037
CurrentTrain: epoch  5, batch    32 | loss: 5.3543129Losses:  5.010024547576904 0.2854868173599243
CurrentTrain: epoch  5, batch    33 | loss: 5.1527681Losses:  5.225131988525391 0.3784579336643219
CurrentTrain: epoch  5, batch    34 | loss: 5.4143610Losses:  4.914732456207275 0.26246750354766846
CurrentTrain: epoch  5, batch    35 | loss: 5.0459661Losses:  4.717023849487305 0.25350746512413025
CurrentTrain: epoch  5, batch    36 | loss: 4.8437777Losses:  5.223731994628906 0.44445645809173584
CurrentTrain: epoch  5, batch    37 | loss: 5.4459600Losses:  4.819993495941162 0.29492148756980896
CurrentTrain: epoch  6, batch     0 | loss: 4.9674544Losses:  5.207393646240234 0.3114171028137207
CurrentTrain: epoch  6, batch     1 | loss: 5.3631020Losses:  5.305887699127197 0.4670695662498474
CurrentTrain: epoch  6, batch     2 | loss: 5.5394225Losses:  5.002148151397705 0.2695336937904358
CurrentTrain: epoch  6, batch     3 | loss: 5.1369152Losses:  4.755026817321777 0.29809969663619995
CurrentTrain: epoch  6, batch     4 | loss: 4.9040766Losses:  4.834345817565918 0.27851974964141846
CurrentTrain: epoch  6, batch     5 | loss: 4.9736056Losses:  5.707961082458496 0.6025103330612183
CurrentTrain: epoch  6, batch     6 | loss: 6.0092163Losses:  4.825847625732422 0.26176464557647705
CurrentTrain: epoch  6, batch     7 | loss: 4.9567299Losses:  4.90974235534668 0.2995436489582062
CurrentTrain: epoch  6, batch     8 | loss: 5.0595140Losses:  4.887724876403809 0.22665441036224365
CurrentTrain: epoch  6, batch     9 | loss: 5.0010519Losses:  5.0003862380981445 0.31965264678001404
CurrentTrain: epoch  6, batch    10 | loss: 5.1602125Losses:  4.795855522155762 0.25913530588150024
CurrentTrain: epoch  6, batch    11 | loss: 4.9254231Losses:  4.859081268310547 0.27487751841545105
CurrentTrain: epoch  6, batch    12 | loss: 4.9965200Losses:  4.855725288391113 0.25595128536224365
CurrentTrain: epoch  6, batch    13 | loss: 4.9837008Losses:  5.031322002410889 0.3180406391620636
CurrentTrain: epoch  6, batch    14 | loss: 5.1903424Losses:  4.910426139831543 0.29125329852104187
CurrentTrain: epoch  6, batch    15 | loss: 5.0560527Losses:  4.829183101654053 0.2431844174861908
CurrentTrain: epoch  6, batch    16 | loss: 4.9507751Losses:  4.927264213562012 0.26244819164276123
CurrentTrain: epoch  6, batch    17 | loss: 5.0584884Losses:  4.823300838470459 0.27779901027679443
CurrentTrain: epoch  6, batch    18 | loss: 4.9622002Losses:  4.889340400695801 0.22532612085342407
CurrentTrain: epoch  6, batch    19 | loss: 5.0020037Losses:  5.006771564483643 0.32612526416778564
CurrentTrain: epoch  6, batch    20 | loss: 5.1698341Losses:  5.057948589324951 0.3303596079349518
CurrentTrain: epoch  6, batch    21 | loss: 5.2231283Losses:  4.7467451095581055 0.20644140243530273
CurrentTrain: epoch  6, batch    22 | loss: 4.8499660Losses:  4.800732135772705 0.25777503848075867
CurrentTrain: epoch  6, batch    23 | loss: 4.9296198Losses:  5.014888763427734 0.3521658778190613
CurrentTrain: epoch  6, batch    24 | loss: 5.1909719Losses:  4.780323505401611 0.25768154859542847
CurrentTrain: epoch  6, batch    25 | loss: 4.9091644Losses:  5.235625267028809 0.2953587472438812
CurrentTrain: epoch  6, batch    26 | loss: 5.3833046Losses:  4.749427795410156 0.3197280764579773
CurrentTrain: epoch  6, batch    27 | loss: 4.9092917Losses:  4.725492477416992 0.25300222635269165
CurrentTrain: epoch  6, batch    28 | loss: 4.8519936Losses:  4.678216934204102 0.18924137949943542
CurrentTrain: epoch  6, batch    29 | loss: 4.7728376Losses:  5.016030311584473 0.374210000038147
CurrentTrain: epoch  6, batch    30 | loss: 5.2031355Losses:  4.843737602233887 0.2482161670923233
CurrentTrain: epoch  6, batch    31 | loss: 4.9678459Losses:  4.7839460372924805 0.2747586965560913
CurrentTrain: epoch  6, batch    32 | loss: 4.9213252Losses:  5.042109489440918 0.31954243779182434
CurrentTrain: epoch  6, batch    33 | loss: 5.2018809Losses:  4.75806999206543 0.3234485685825348
CurrentTrain: epoch  6, batch    34 | loss: 4.9197941Losses:  4.9850664138793945 0.2665245234966278
CurrentTrain: epoch  6, batch    35 | loss: 5.1183286Losses:  4.866237640380859 0.22748373448848724
CurrentTrain: epoch  6, batch    36 | loss: 4.9799795Losses:  4.698910713195801 0.2580326199531555
CurrentTrain: epoch  6, batch    37 | loss: 4.8279271Losses:  5.034289360046387 0.3149266839027405
CurrentTrain: epoch  7, batch     0 | loss: 5.1917529Losses:  4.907353401184082 0.2368168979883194
CurrentTrain: epoch  7, batch     1 | loss: 5.0257621Losses:  4.720794200897217 0.2550038993358612
CurrentTrain: epoch  7, batch     2 | loss: 4.8482962Losses:  4.750613689422607 0.2456260323524475
CurrentTrain: epoch  7, batch     3 | loss: 4.8734269Losses:  5.021836280822754 0.24741145968437195
CurrentTrain: epoch  7, batch     4 | loss: 5.1455421Losses:  4.658726215362549 0.24154570698738098
CurrentTrain: epoch  7, batch     5 | loss: 4.7794991Losses:  4.7672529220581055 0.24646452069282532
CurrentTrain: epoch  7, batch     6 | loss: 4.8904853Losses:  4.742995262145996 0.23893144726753235
CurrentTrain: epoch  7, batch     7 | loss: 4.8624611Losses:  4.671825408935547 0.2231065183877945
CurrentTrain: epoch  7, batch     8 | loss: 4.7833786Losses:  4.8468017578125 0.29029059410095215
CurrentTrain: epoch  7, batch     9 | loss: 4.9919472Losses:  4.965808391571045 0.3126589059829712
CurrentTrain: epoch  7, batch    10 | loss: 5.1221380Losses:  4.614089012145996 0.16981476545333862
CurrentTrain: epoch  7, batch    11 | loss: 4.6989965Losses:  4.875020980834961 0.28415194153785706
CurrentTrain: epoch  7, batch    12 | loss: 5.0170970Losses:  4.861351490020752 0.2431657910346985
CurrentTrain: epoch  7, batch    13 | loss: 4.9829345Losses:  5.086827278137207 0.3437211513519287
CurrentTrain: epoch  7, batch    14 | loss: 5.2586880Losses:  4.640600681304932 0.2648405432701111
CurrentTrain: epoch  7, batch    15 | loss: 4.7730207Losses:  4.602528095245361 0.20591969788074493
CurrentTrain: epoch  7, batch    16 | loss: 4.7054877Losses:  4.730156898498535 0.25920212268829346
CurrentTrain: epoch  7, batch    17 | loss: 4.8597579Losses:  4.761144638061523 0.2780672311782837
CurrentTrain: epoch  7, batch    18 | loss: 4.9001784Losses:  4.8878173828125 0.2735980153083801
CurrentTrain: epoch  7, batch    19 | loss: 5.0246162Losses:  4.7196550369262695 0.16935931146144867
CurrentTrain: epoch  7, batch    20 | loss: 4.8043346Losses:  4.6541595458984375 0.18272539973258972
CurrentTrain: epoch  7, batch    21 | loss: 4.7455220Losses:  4.842430114746094 0.17552903294563293
CurrentTrain: epoch  7, batch    22 | loss: 4.9301949Losses:  4.6687726974487305 0.22534753382205963
CurrentTrain: epoch  7, batch    23 | loss: 4.7814465Losses:  4.64533805847168 0.20453521609306335
CurrentTrain: epoch  7, batch    24 | loss: 4.7476058Losses:  4.655961036682129 0.22074174880981445
CurrentTrain: epoch  7, batch    25 | loss: 4.7663317Losses:  4.563828468322754 0.15688326954841614
CurrentTrain: epoch  7, batch    26 | loss: 4.6422701Losses:  4.774075508117676 0.19047191739082336
CurrentTrain: epoch  7, batch    27 | loss: 4.8693113Losses:  4.71754264831543 0.21676163375377655
CurrentTrain: epoch  7, batch    28 | loss: 4.8259234Losses:  4.720296859741211 0.16816847026348114
CurrentTrain: epoch  7, batch    29 | loss: 4.8043809Losses:  4.777175426483154 0.2473551332950592
CurrentTrain: epoch  7, batch    30 | loss: 4.9008532Losses:  4.643531322479248 0.228444904088974
CurrentTrain: epoch  7, batch    31 | loss: 4.7577538Losses:  4.6077656745910645 0.20466521382331848
CurrentTrain: epoch  7, batch    32 | loss: 4.7100983Losses:  4.859836578369141 0.2408381700515747
CurrentTrain: epoch  7, batch    33 | loss: 4.9802556Losses:  4.665979862213135 0.23968791961669922
CurrentTrain: epoch  7, batch    34 | loss: 4.7858238Losses:  4.58905029296875 0.19516050815582275
CurrentTrain: epoch  7, batch    35 | loss: 4.6866307Losses:  4.651291370391846 0.13779383897781372
CurrentTrain: epoch  7, batch    36 | loss: 4.7201881Losses:  4.628119468688965 0.06404247134923935
CurrentTrain: epoch  7, batch    37 | loss: 4.6601405Losses:  4.721247673034668 0.2108297497034073
CurrentTrain: epoch  8, batch     0 | loss: 4.8266625Losses:  4.6687211990356445 0.20804870128631592
CurrentTrain: epoch  8, batch     1 | loss: 4.7727456Losses:  4.829222202301025 0.23514686524868011
CurrentTrain: epoch  8, batch     2 | loss: 4.9467955Losses:  4.885284423828125 0.3375251889228821
CurrentTrain: epoch  8, batch     3 | loss: 5.0540471Losses:  4.604918479919434 0.1735909879207611
CurrentTrain: epoch  8, batch     4 | loss: 4.6917138Losses:  4.651096343994141 0.2102729082107544
CurrentTrain: epoch  8, batch     5 | loss: 4.7562327Losses:  4.759404182434082 0.21536940336227417
CurrentTrain: epoch  8, batch     6 | loss: 4.8670888Losses:  4.603768825531006 0.19648903608322144
CurrentTrain: epoch  8, batch     7 | loss: 4.7020135Losses:  4.707431316375732 0.22598519921302795
CurrentTrain: epoch  8, batch     8 | loss: 4.8204241Losses:  4.8189287185668945 0.20509716868400574
CurrentTrain: epoch  8, batch     9 | loss: 4.9214773Losses:  4.7668561935424805 0.23546597361564636
CurrentTrain: epoch  8, batch    10 | loss: 4.8845892Losses:  4.643391132354736 0.18387936055660248
CurrentTrain: epoch  8, batch    11 | loss: 4.7353306Losses:  4.606127738952637 0.19398623704910278
CurrentTrain: epoch  8, batch    12 | loss: 4.7031207Losses:  4.579174995422363 0.16754934191703796
CurrentTrain: epoch  8, batch    13 | loss: 4.6629496Losses:  4.608902931213379 0.12931299209594727
CurrentTrain: epoch  8, batch    14 | loss: 4.6735592Losses:  4.704257965087891 0.19986660778522491
CurrentTrain: epoch  8, batch    15 | loss: 4.8041911Losses:  4.595221519470215 0.17406585812568665
CurrentTrain: epoch  8, batch    16 | loss: 4.6822543Losses:  5.2996015548706055 0.3285822868347168
CurrentTrain: epoch  8, batch    17 | loss: 5.4638929Losses:  4.8239641189575195 0.20212668180465698
CurrentTrain: epoch  8, batch    18 | loss: 4.9250274Losses:  4.600820541381836 0.17001235485076904
CurrentTrain: epoch  8, batch    19 | loss: 4.6858268Losses:  4.599682331085205 0.1698082983493805
CurrentTrain: epoch  8, batch    20 | loss: 4.6845865Losses:  5.115891456604004 0.2559831142425537
CurrentTrain: epoch  8, batch    21 | loss: 5.2438831Losses:  4.733193397521973 0.14958816766738892
CurrentTrain: epoch  8, batch    22 | loss: 4.8079877Losses:  4.777894020080566 0.14427174627780914
CurrentTrain: epoch  8, batch    23 | loss: 4.8500299Losses:  4.759472846984863 0.20804056525230408
CurrentTrain: epoch  8, batch    24 | loss: 4.8634930Losses:  4.694845199584961 0.18634580075740814
CurrentTrain: epoch  8, batch    25 | loss: 4.7880182Losses:  4.808925628662109 0.171770840883255
CurrentTrain: epoch  8, batch    26 | loss: 4.8948112Losses:  5.092500686645508 0.34400486946105957
CurrentTrain: epoch  8, batch    27 | loss: 5.2645030Losses:  4.5804033279418945 0.16649958491325378
CurrentTrain: epoch  8, batch    28 | loss: 4.6636529Losses:  4.582469463348389 0.14494986832141876
CurrentTrain: epoch  8, batch    29 | loss: 4.6549444Losses:  4.841609477996826 0.19202792644500732
CurrentTrain: epoch  8, batch    30 | loss: 4.9376235Losses:  4.72785758972168 0.1876259446144104
CurrentTrain: epoch  8, batch    31 | loss: 4.8216705Losses:  4.731928825378418 0.13202610611915588
CurrentTrain: epoch  8, batch    32 | loss: 4.7979417Losses:  4.733304977416992 0.1845131516456604
CurrentTrain: epoch  8, batch    33 | loss: 4.8255615Losses:  4.837882041931152 0.20022132992744446
CurrentTrain: epoch  8, batch    34 | loss: 4.9379926Losses:  4.6747636795043945 0.16585926711559296
CurrentTrain: epoch  8, batch    35 | loss: 4.7576933Losses:  4.839719772338867 0.18942509591579437
CurrentTrain: epoch  8, batch    36 | loss: 4.9344325Losses:  4.585596084594727 0.12414473295211792
CurrentTrain: epoch  8, batch    37 | loss: 4.6476684Losses:  4.730900764465332 0.214542418718338
CurrentTrain: epoch  9, batch     0 | loss: 4.8381720Losses:  4.589254856109619 0.16948461532592773
CurrentTrain: epoch  9, batch     1 | loss: 4.6739969Losses:  4.648564338684082 0.15801922976970673
CurrentTrain: epoch  9, batch     2 | loss: 4.7275739Losses:  4.64251184463501 0.16076594591140747
CurrentTrain: epoch  9, batch     3 | loss: 4.7228947Losses:  4.61677360534668 0.16061407327651978
CurrentTrain: epoch  9, batch     4 | loss: 4.6970806Losses:  4.647119998931885 0.16093328595161438
CurrentTrain: epoch  9, batch     5 | loss: 4.7275867Losses:  4.664971351623535 0.15685760974884033
CurrentTrain: epoch  9, batch     6 | loss: 4.7434001